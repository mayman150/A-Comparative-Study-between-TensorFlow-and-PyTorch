Issue Number,Issue Title,Issue Body
41839,"java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: OpenCL library not loaded - dlopen failed: library ""libOpenCL-pixel.so"" not found Falling back to OpenGL TfLiteGpuDelegate Invoke: Write to buffer failed. Source data is larger than buffer. ","**System information**
- Android 9.
- TensorFlow Lite.
- Google Pixel

build.gradle:
```
implementation('org.tensorflow:tensorflow-lite:2.2.0')
implementation ('org.tensorflow:tensorflow-lite-gpu:2.2.0')
```

**Describe the current behavior**

Exception is thrown on Google Pixel while running Blazeface model taken from Mediapipe repo (https://github.com/google/mediapipe/blob/master/mediapipe/models/face_detection_front.tflite). But this model works successfully on Samsung S10 and Xiaomi MI8.

```
 Internal error: Failed to run on the given Interpreter: Following operations are not supported by GPU delegate:
    DEQUANTIZE: 
    164 operations will run on the GPU, and the remaining 0 operations will run on the CPU.
    Can not open OpenCL library on this device - dlopen failed: library ""libOpenCL.so"" not found
    Falling back to OpenGL
    TfLiteGpuDelegate Invoke: GpuDelegate must run on the same thread where it was initialized.
    Node number 164 (TfLiteGpuDelegateV2) failed to invoke.
    
    TfLiteGpuDelegate Invoke: GpuDele [DevelopReportingTree.log:18]
    java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: Following operations are not supported by GPU delegate:
    DEQUANTIZE: 
    164 operations will run on the GPU, and the remaining 0 operations will run on the CPU.
    Can not open OpenCL library on this device - dlopen failed: library ""libOpenCL.so"" not found
    Falling back to OpenGL
    TfLiteGpuDelegate Invoke: GpuDelegate must run on the same thread where it was initialized.
    Node number 164 (TfLiteGpuDelegateV2) failed to invoke.
    
    TfLiteGpuDelegate Invoke: GpuDele
        at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)
        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:158)
        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:343)
```

When I change tflite dependencies from nightly to v. 2.2 and modify interpreter initialization, error will be a bit different for Google Pixel:

```
java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: OpenCL library not loaded - dlopen failed: library ""libOpenCL-pixel.so"" not found
Falling back to OpenGL
TfLiteGpuDelegate Invoke: Write to buffer failed. Source data is larger than buffer. 
```


**Describe the expected behavior**

Successfully recognizes faces without exceptions

**Standalone code to reproduce the issue**

 ```
    private static final int BATCH_SIZE = 1;
    private ByteBuffer imgData;
    private int[] intValues;
    private static final int PIXEL_SIZE = 3;

       Interpreter tfLite = createTfliteInterpreter(assetManager, ""face_detection_front_blazefast.tflite"");
         imgData = ByteBuffer.allocateDirect(BATCH_SIZE * 128 * 128 * PIXEL_SIZE * 4);
        imgData.order(ByteOrder.nativeOrder());
        intValues = new int[inputSize * inputSize];

    public List<Recognition> recognizeImage(Bitmap bitmap) {
        final int previewWidth = bitmap.getWidth();
        final int previewHeight = bitmap.getHeight();

        int sensorOrientation = 0;
        Matrix frameToCropTransform =
                getTransformationMatrix(
                        previewWidth,
                        previewHeight,
                        128, 128,
                        sensorOrientation,
                        false
                );

        // to restore location object
        cropToFrameTransform = new Matrix();
        frameToCropTransform.invert(cropToFrameTransform);

        // crop image
        Canvas canvas = new Canvas(croppedBitmap);
        canvas.drawBitmap(bitmap, frameToCropTransform, null);

        convertBitmapToByteBuffer(
                croppedBitmap,
                imgData,
                intValues,
                128
        );

        Map<Integer, Object> outputs = provideOutput();
        Object[] inputArray = {imgData};
        try {
            tfLite.runForMultipleInputsOutputs(inputArray, outputs);
        } catch (Exception e) {
            Timber.e(e);
        }
        return getDetections(bitmap, outputs);
    }

    public Map<Integer, Object> provideOutput() {
        float[][][] boxesResult = new float[1][896][16];
        float[][][] scoresResult = new float[1][896][1];
        HashMap<Integer, Object> outputs = new HashMap<>();
        outputs.put(0, boxesResult);
        outputs.put(1, scoresResult);
        return outputs;
    }

    Interpreter createTfliteInterpreter(AssetManager assetManager, String modelPath) {
        try {
            final Interpreter.Options options;
                GpuDelegate.Options options1 = new GpuDelegate.Options()
                        .setPrecisionLossAllowed(true)
                        .setInferencePreference(GpuDelegate.Options.INFERENCE_PREFERENCE_FAST_SINGLE_ANSWER);
                GpuDelegate delegate = new GpuDelegate(options1);
                options = new Interpreter.Options()
                        .addDelegate(delegate);
            return new Interpreter(
                    loadModelFile(assetManager, modelPath),
                    options
            );
        } catch (Exception e) {
            throw new RuntimeException(e);
        }
    }

    public static void convertBitmapToByteBuffer(Bitmap bitmap, ByteBuffer imgData, int[] intValues, int mInputSize) {
        bitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());
        imgData.rewind();
        int pixel = 0;
        for (int i = 0; i < mInputSize; ++i) {
            for (int j = 0; j < mInputSize; ++j) {
                final int pixelValue = intValues[pixel++];
                imgData.putFloat(((pixelValue >> 16) & 0xFF) / 255.0f);
                imgData.putFloat(((pixelValue >> 8) & 0xFF) / 255.0f);
                imgData.putFloat((pixelValue & 0xFF) / 255.0f);
            }
        }
    }

public static Matrix getTransformationMatrix(
            final int srcWidth,
            final int srcHeight,
            final int dstWidth,
            final int dstHeight,
            final int applyRotation,
            final boolean maintainAspectRatio) {
        final Matrix matrix = new Matrix();

        if (applyRotation != 0) {
            if (applyRotation % 90 != 0) {
                Timber.w(""Rotation of %d % 90 != 0"", applyRotation);
            }

            // Translate so center of image is at origin.
            matrix.postTranslate(-srcWidth / 2.0f, -srcHeight / 2.0f);

            // Rotate around origin.
            matrix.postRotate(applyRotation);
        }

        // Account for the already applied rotation, if any, and then determine how
        // much scaling is needed for each axis.
        final boolean transpose = (Math.abs(applyRotation) + 90) % 180 == 0;

        final int inWidth = transpose ? srcHeight : srcWidth;
        final int inHeight = transpose ? srcWidth : srcHeight;

        // Apply scaling if necessary.
        if (inWidth != dstWidth || inHeight != dstHeight) {
            final float scaleFactorX = dstWidth / (float) inWidth;
            final float scaleFactorY = dstHeight / (float) inHeight;

            if (maintainAspectRatio) {
                // Scale by minimum factor so that dst is filled completely while
                // maintaining the aspect ratio. Some image may fall off the edge.
                final float scaleFactor = Math.max(scaleFactorX, scaleFactorY);
                matrix.postScale(scaleFactor, scaleFactor);
            } else {
                // Scale exactly to fill dst from src.
                matrix.postScale(scaleFactorX, scaleFactorY);
            }
        }

        if (applyRotation != 0) {
            // Translate back from origin centered reference to destination frame.
            matrix.postTranslate(dstWidth / 2.0f, dstHeight / 2.0f);
        }

        return matrix;
    }
```

"
41838,Segmentation fault: 11,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS X 10.14.6
- TensorFlow installed from (source or binary): Using pip
- TensorFlow version (or github SHA if from source): 2.3.0


**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.



```
python3 tensorflow/lite/python/tflite_convert.py --output_file=model-float.lite --output_format=TFLITE --inference_type=FLOAT --inference_input_type=FLOAT --input_shape=""1,224,224,3"" --input_array=""serving_default_input_1"" --output_array=""StatefulPartitionedCall"" --mean_value=0 --std_dev_value=1 --saved_model_dir=/Users/z004njq/Projects/save_models/mobilenetv4-exp-1004-export/ --experimental_converter=True
```

**The output from the converter invocation**

```
2020-07-28 13:29:05.763703: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-07-28 13:29:05.781147: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14dc421f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-07-28 13:29:05.781174: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-07-28 13:29:14.581147: I tensorflow/core/grappler/devices.cc:78] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)
2020-07-28 13:29:14.581230: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-07-28 13:29:14.677045: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize
2020-07-28 13:29:14.677069: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: Graph size after: 1835 nodes (1614), 2696 edges (2475), time = 64.086ms.
2020-07-28 13:29:14.677092: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 2.02ms.
I0728 13:29:16.315555 140734995711424 lite.py:624] Using experimental converter: If you encountered a problem please file a bug. You can opt-out by setting experimental_new_converter=False
2020-07-28 13:29:16.359263: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:313] Ignored output_format.
2020-07-28 13:29:16.359292: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored drop_control_dependency.
Fatal Python error: Segmentation fault

Current thread 0x00007fff6b6d45c0 (most recent call first):
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/lite/python/wrap_toco.py"", line 38 in wrapped_toco_convert
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/lite/python/convert.py"", line 199 in toco_convert_protos
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/lite/python/convert.py"", line 574 in toco_convert_impl
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/lite/python/lite.py"", line 633 in convert
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/lite/python/lite.py"", line 900 in convert
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/lite/python/lite.py"", line 1076 in convert
  File ""tensorflow/lite/python/tflite_convert.py"", line 239 in _convert_tf2_model
  File ""tensorflow/lite/python/tflite_convert.py"", line 623 in run_main
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/absl/app.py"", line 250 in _run_main
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/absl/app.py"", line 299 in run
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/platform/app.py"", line 40 in run
  File ""tensorflow/lite/python/tflite_convert.py"", line 640 in main
  File ""tensorflow/lite/python/tflite_convert.py"", line 644 in <module>
Segmentation fault: 11
```

**Also, please include a link to the saved model or GraphDef**

[mobilenetv3-exp-1004-export.zip](https://github.com/tensorflow/tensorflow/files/4991052/mobilenetv3-exp-1004-export.zip)

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
- Producing wrong results and/or decrease in accuracy
- Producing correct results, but the model is slower than expected (model generated from old converter)


**RNN conversion support**
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
41836,Is this a functionality that is possible?,"I have created an operation that has a custom gradient. Then I use it to define a new Keras Model, out of layers:

```python
import numpy as np
import numba
import tensorflow as tf

@numba.jit(nopython = True)
def func(param, input):
    return param*input**2

@numba.jit(nopython = True)
def gradfunc(param, input):
    return input**2

@tf.custom_gradient
def func_tf(param, input):
    def grad(dy):
        return tf.numpy_function(gradfunc, (param.numpy(), input.numpy()), tf.float32), 2*param*input
    return tf.numpy_function(func, (param.numpy(), input.numpy()), tf.float32), grad

class myLayer(tf.keras.layers.Layer):
    def __init__(self):
        super().__init__()
        
    def build(self, input_shape):
        self.param = self.add_weight(""param"")
        
    def call(self, input):
        return func_tf(self.param, input)
    
class myModel(tf.keras.Model):
    def __init__(self, num_layers):
        super().__init__(name='')
        self._layers = [myLayer() for _ in range(num_layers)]
        
    def call(self, input_tensor):
        for layer in self._layers:
            input_tensor = layer(input_tensor)
        return input_tensor
    
model = myModel(3)
print(model(1.5)) # <-- this works
```
This, however fails:
```python
def loss(target, output):
    tf.abs(tf.reduce_sum(target - output))**2

model.compile(
    optimizer=tf.keras.optimizers.Adam(),
    loss=loss,
    metrics=[loss])

model.fit([0.1], [0.4], batch_size=None)
```

Because `model.fit` uses `@tf.function` under the hood, so the calls to `.numpy()` in `func` and `gradfunc` are not possible (see #40508)

I have found [this answer](https://stackoverflow.com/questions/41132633/can-numba-be-used-with-tensorflow) on SO, but I think that works only if `param` were not an input to `func` and `gradfunc`.

How do I make it work?"
41835,FailedPreconditionError: Error while reading resource variable block1_conv2_30/kernel from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/block1_conv2_30/kernel/N10tensorflow3VarE does not exist. 	 [[{{node block1_conv2_30/Conv2D/ReadVariableOp}}]],"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.1.1
- Python version: 2.7.17


Trying to model a neural style transfer using VGG16.

Error while reading resource variable block1_conv2_30/kernel from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/block1_conv2_30/kernel/N10tensorflow3VarE does not exist.
	 [[{{node block1_conv2_30/Conv2D/ReadVariableOp}}]]


**Standalone code to reproduce the issue**
https://github.com/nickinack/NST-VGG16/blob/master/nst-vgg.ipynb

**Other info/logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
41833,Keras ImageDataGenerator Preprocessing-Function,"I am quite new to Tensorflow and Keras and not sure if am getting something wrong or if there really is a difference between the documentation and the code. If any futher informations are necessarry I will add them quickly.

## URL(s) with the issue:

https://keras.io/api/preprocessing/image/#imagedatagenerator-class

## Description of issue (what needs changing):

### Clear description

According to the documentation the class `ImageDataGenerator` takes a function `preprocessing_function` when initializing it. In the discription it says that the function is executed AFTER the input has been rescaled:
""...The function will run after the image is resized and augmented...""
When going through the code I realized that in the class `ImageDataGenerator` within the function `standardize` the `preprocessing_function `is applied BEFORE rescaling the image.

### Correct links

Is the link to the source code correct? Yes

### Parameters defined

Are all parameters defined and formatted correctly? yes

### Returns defined

Are return values defined? Yes

### Raises listed and defined

Are the errors defined? Yes

### Submit a pull request? No
"
41831,Issue when trying to import tensorflow in my conda environment right after I downloaded it,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version: 2.3.0
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version: N/A
- GPU model and memory:N/A

Here below is the error message:
  

**Describe the problem**
_It fails to load Tensorflow runtime whenever I try ""import tensorflow"" or ""from tensorflow import keras"" right after I have installed(using pip) tensorflow 2.3.0 from my conda environment.... I used ""pip install tensorflow""
**Provide the exact sequence of commands / steps that you executed before running into the problem**
Installed Anaconda on my machine,
I then successfully setup a new conda environment on Cmd right in the Users/home folder
Onwards, I started downloading libraries I needed for ML/DL, specifically tensorflow
After successful downloaded, I tried importing it, but it _failed__

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
 from tensorflow.python._pywrap_tensorflow_interanl import keras
Traceback (most recent call last):
  File ""C:\Users\celestino\anaconda3\envs\packt_exercises\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

_Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\celestino\anaconda3\envs\packt_exercises\lib\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\celestino\anaconda3\envs\packt_exercises\lib\site-packages\tensorflow\python\__init__.py"", line 40, in <module>
    from tensorflow.python.eager import context
  File ""C:\Users\celestino\anaconda3\envs\packt_exercises\lib\site-packages\tensorflow\python\eager\context.py"", line 35, in <module>
    from tensorflow.python import pywrap_tfe
  File ""C:\Users\celestino\anaconda3\envs\packt_exercises\lib\site-packages\tensorflow\python\pywrap_tfe.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\celestino\anaconda3\envs\packt_exercises\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 83, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\celestino\anaconda3\envs\packt_exercises\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime._"
41830,Tensorflow not working in pycharm,"
**System information**
-   MacOs Version 10.14.6 (18G5033)
- Installed through Pycharm CE 
- TensorFlow version 2.0 but I have also installed Tensorflow 1.14 on python hoping it would work but didn't work.
- Python version: 3.7.4
- Installed using: pip
- 1.6 GHz Intel Core i5



**The problem**

Today is my first time trying learn Tensorflow through LinkedIn Learning. I followed tutorials which uses Pycharm CE. It is through Pycharm where I installed Tensorflow. However, when it came to running the model, the response I got is that **ModuleNotFoundError: No module named 'tensorflow.contrib'**

I have read some debugging related to this question but they were unclear and unhelpful. Any assistance will be highly appreciated. Thank you.  


"
41829," Specified output array ""'TFLite_Detection_PostProcess'"" is not produced by any op in this graph.","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): pip installed
- TensorFlow version (or github SHA if from source): 1.15


**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
# Copy and paste here the exact command
```
tflite_convert --graph_def_file=C:/tensorflow4/models/research/object_detection/TFLite_model/tflite_graph.pb --output_file=tflite/detect.tflite --output_format=TFLITE --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_dev_values=127 --change_concat_input_ranges=false --allow_custom_ops
**The output from the converter invocation**

```
# Copy and paste the output here.
```
2020-07-28 18:33:11.363228: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TFLite_Detection_PostProcess
2020-07-28 18:33:11.531507: F tensorflow/lite/toco/tooling_util.cc:935] Check failed: GetOpWithOutput(model, output_array) Specified output array ""'TFLite_Detection_PostProcess'"" is not produced by any op in this graph. Is it a typo? This should not happen. If you trigger this error please send a bug report (with code to reporduce this error), to the TensorFlow Lite team.
Fatal Python error: Aborted

Current thread 0x000023ec (most recent call first):
  File ""c:\users\sreed\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow_core\lite\toco\python\toco_from_protos.py"", line 52 in execute
  File ""c:\users\sreed\anaconda3\envs\tensorflow2\lib\site-packages\absl\app.py"", line 250 in _run_main
  File ""c:\users\sreed\anaconda3\envs\tensorflow2\lib\site-packages\absl\app.py"", line 299 in run
  File ""c:\users\sreed\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow_core\python\platform\app.py"", line 40 in run
  File ""c:\users\sreed\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow_core\lite\toco\python\toco_from_protos.py"", line 89 in main
  File ""C:\Users\Sreed\Anaconda3\envs\tensorflow2\Scripts\toco_from_protos.exe\__main__.py"", line 7 in <module>
  File ""c:\users\sreed\anaconda3\envs\tensorflow2\lib\runpy.py"", line 85 in _run_code
  File ""c:\users\sreed\anaconda3\envs\tensorflow2\lib\runpy.py"", line 193 in _run_module_as_main

**Also, please include a link to the saved model or GraphDef**
 https://github.com/Dasinator21/Replicate-Error
```
# Put link here or attach to the issue.
```https://github.com/Dasinator21/Replicate-Error

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
- Producing wrong results and/or decrease in accuracy
- Producing correct results, but the model is slower than expected (model generated from old converter)


**RNN conversion support**
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
41828,Please upload tensorflow-gpu-estimator 2.3 to pypi,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): pypi
- TensorFlow version: 2.3
- Python version: 3.7
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1
- GPU model and memory: GTX 1080Ti



**Describe the problem**

Please upload tensorflow-gpu-estimator 2.3 to Pypi.  There's tensorflow-estimator 2.3 but not tensorflow-gpu-estimator 2.3.  Pypi only have 2.2 as of now.

I have TF 2.2, when I upgrade to 2.3, it failed because it could not find co-dependency: tensorflow-gpu-estimator 2.3

**Provide the exact sequence of commands / steps that you executed before running into the problem**

`pip install tensorflow-gpu -U` results in:
```
ERROR: Could not find a version that satisfies the requirement tensorflow-gpu-estimator<2.4.0,>=2.3.0 (from tensorflow-gpu) (from versions: 2.1.0, 2.2.0)
ERROR: No matching distribution found for tensorflow-gpu-estimator<2.4.0,>=2.3.0 (from tensorflow-gpu)
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
41827,TF 2.3 training slowed down by 15% compared to 2.2,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.0, 2.4.0.dev20200728
- Python version: 3.7.8
- CUDA/cuDNN version: 10.1 / 7.6.5.32
- GPU model and memory: NVIDIA V100 on 12 vCPUs, 40 GB memory GCP node

**Describe the current behavior**

When upgrading from TensorFlow 2.2.0 to 2.3.0 we observed a 15 - 18% slow down in training speed for our workloads. Unfortunately I wasn't able to find an easy to reproduce example before the stable release was cut, but below is a code example that illustrates the performance degradation.

When running the training script on a single NVIDIA V100 a 15% performance loss compared to 2.2 can be observed which still is noticable in the latest nightly:

version | epoch time | step time | GPU idle time
--- | ---  | --- | ---
2.2.0 | 34 s | 124.3 ms | 19.7 ms (15.6 %)
2.3.0 | 39 s | 141.9 ms | 37.2 ms (26.1 %)
2.4.0.dev20200728 | 38s | 136.2 ms | 31.6 ms (23.2 %)

**On Device: total self-time (grouped by type)**
2.2.0 | 2.3.0 | 2.4.0.dev20200728
--- | --- | ---
<img width=""395"" alt=""Screenshot 2020-07-28 at 17 45 02"" src=""https://user-images.githubusercontent.com/13285808/88689213-80d3ff80-d0fa-11ea-88ec-3feda0cf9c24.png""> | <img width=""375"" alt=""Screenshot 2020-07-28 at 17 45 15"" src=""https://user-images.githubusercontent.com/13285808/88689219-829dc300-d0fa-11ea-99bb-5d406805a6e8.png""> | <img width=""359"" alt=""Screenshot 2020-07-28 at 17 46 08"" src=""https://user-images.githubusercontent.com/13285808/88689223-83365980-d0fa-11ea-91aa-46ec7659fe64.png"">


The example uses auto mixed precision, but the slowdown can also be observed when running in `float32` or in multi-GPU training. When looking at the [generated execution profile](https://github.com/tensorflow/tensorflow/files/4989125/tb-profile.zip) the slowdown can be explained by an increased idle time of the GPU. Since the training data is cached in memory there should be no IO bottleneck so I am not sure if this performance regression is caused by `tf.data` or by the runtime itself.

**Describe the expected behavior**

TensorFlow 2.3 should show equally fast training performance compared to 2.2.

**Standalone code to reproduce the issue**
```python
import tensorflow as tf
import tensorflow_datasets as tfds

batch_size = 64


def _decode_and_center_crop(image_bytes):
    """"""Crops to center of image with padding then scales image_size.""""""
    shape = tf.image.extract_jpeg_shape(image_bytes)
    image_height, image_width, image_size = shape[0], shape[1], 224

    padded_center_crop_size = tf.cast(
        (
            (image_size / (image_size + 32))
            * tf.cast(tf.minimum(image_height, image_width), tf.float32)
        ),
        tf.int32,
    )

    offset_height = ((image_height - padded_center_crop_size) + 1) // 2
    offset_width = ((image_width - padded_center_crop_size) + 1) // 2
    crop_window = tf.stack(
        [offset_height, offset_width, padded_center_crop_size, padded_center_crop_size]
    )
    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)
    return tf.image.resize(image, [image_size, image_size], method=""bicubic"")


def preprocessing(data):
    return (
        tf.cast(_decode_and_center_crop(data[""image""]), tf.float32),
        data[""label""],
    )


dataset = tfds.load(
    ""imagenette"", decoders={""image"": tfds.decode.SkipDecoding()}, split=""train"",
)

dataset = (
    dataset.cache()
    .repeat(2)  # Artificially increase time per epoch to make it easier to measure
    .map(preprocessing, num_parallel_calls=tf.data.experimental.AUTOTUNE)
    .batch(batch_size)
    .prefetch(1)
)

with tf.distribute.MirroredStrategy().scope():
    model = tf.keras.applications.ResNet50(weights=None)

    model.compile(
        optimizer=tf.train.experimental.enable_mixed_precision_graph_rewrite(
            tf.keras.optimizers.Adam(), loss_scale=""dynamic""
        ),
        loss=""sparse_categorical_crossentropy"",
    )

tb_cbk = tf.keras.callbacks.TensorBoard(f""logs/{tf.__version__}"", profile_batch=300)
model.fit(dataset, verbose=2, epochs=3, callbacks=[tb_cbk])
```

**Other info / logs**

TensorBoard profiles for the runs mentioned above are available at [tb-profile.zip](https://github.com/tensorflow/tensorflow/files/4989125/tb-profile.zip)

@mihaimaruseac @jsimsa @guptapriya do you mind taking a look at this?"
41826,Hadoop Filesystem for C API Modular filesystem,"**System information**
- TensorFlow version (you are using): 2.2.0
- Are you willing to contribute it: Yes

**Describe the feature and the current behavior/state.**
This issue is used to track the implementation of `hadoop` filesystem for c api modular filesystem

**Will this change the current api? How?**
No

**Who will benefit with this feature?**
Users

cc @mihaimaruseac "
41825,Tensorflow-Lite on NDK with C-API fails to provide output - no errors or warnings,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Based on example script
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy S10
- TensorFlow Lite version: 2.2
- Python version: 3.6
- GCC/Compiler version (if compiling from source): clang++ version 7.0

**Describe the current behavior**

Previously I was running TFLite on Android, with the Java API.
Running the .tflite model from Android's Java API works fine, and I get the expected output.

Now I'm running TFLite on NDK, using the C-API.
Invoking the interpreter in the NDK doesn't seem to do anything.

I can verify the the model is loaded (does not return null) and setting everything up (interpreter / options / input and output tensors / etc) seems OK too.

The input and output are both float32 arrays:
- I can verify input array data integrity before `TfLiteTensorCopyFromBuffer` (all float values are correct and none-zero).
- Output array is a float array (in the proper size) initialized with zeros.
Extracting the inference result with `TfLiteTensorCopyToBuffer` does not change the output array - it's still all zeros.


**Describe the expected behavior**

Invoking the interpreter should provide an output array, a result.


**Standalone code to reproduce the issue**

In my native C++ file:

```
// Following the example in c_api.h

// Input
// input is just a single tensor in size
// input is: vector<float> input_float_array;
// accessing input values with input_float_array.at(x), for example,
// provides float32 values - the first few are:
// [ 43.36578, 72.9673, 98.4356, 12.7865, ... ]

// Output
// output is just a single tensor in size
// output is: vector<float> output_float_array;
// output is initialized with zeros:
// [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ... ]
// and is not constant

TfLiteModel * model = TfLiteModelCreateFromFile(
                reinterpret_cast<const char *>(model_file_path));

// checkpoint ""model"":
// if (model == nullptr)
// result: model is not null - model_file_path found
// in model_file_path there's a .tflite file

TfLiteInterpreterOptions * options = TfLiteInterpreterOptionsCreate();
TfLiteInterpreterOptionsSetNumThreads(options, 2);
TfLiteInterpreter * interpreter = TfLiteInterpreterCreate(model, options);
TfLiteInterpreterAllocateTensors(interpreter);
TfLiteTensor * input_tensor =
        TfLiteInterpreterGetInputTensor(interpreter, 0);

// checkpoint ""before""
// input_float_array has legitimate float values, at this point:
// [ 43.36578, 72.9673, 98.4356, 12.7865, ... ]

TfLiteTensorCopyFromBuffer(input_tensor,
        input_float_array.data(),
        input_float_array.size() * sizeof(float));
TfLiteInterpreterInvoke(interpreter);
const TfLiteTensor * output_tensor =
        TfLiteInterpreterGetOutputTensor(interpreter, 0);
TfLiteTensorCopyToBuffer(output_tensor,
        output_float_array.data(),
        output_float_array.size() * sizeof(float));

// checkpoint ""after""
// output_float_array remained all zeros, at this point:
// [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ... ]

TfLiteInterpreterDelete(interpreter);
TfLiteInterpreterOptionsDelete(options);
TfLiteModelDelete(model);

// some alternatives to access data and verify integrity:
//
// replacing ""input_float_array.data()"" with ""&(* input_float_array.begin())""
// and ""output_float_array.data()"" with ""&(* output_float_array.begin())""
//
// initializing output_float_array with ones instead of zeros:
// [ 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ... ]

// I also tried to change a few models
// (all with same input / output and characteristics)
```


**Other info / logs**

No errors of any kind or anything to show.
The code runs smoothly, but nothing happens - the output array remains unchanged.
The exact same models, when run with Java API, work just fine."
41823,Tensorflow Serving 2.2.0 error: output_shape has incorrect number of elements: 2 should be: 1,"I have a SavedModel, which a am serving with Tensorflow Serving. When I load and run it in a separate script, it's working fine. But sending a request to the serving API throws the following error:

    {
        ""error"": ""[_Derived_]{{function_node __inference__wrapped_model_6314}} {{function_node __inference__wrapped_model_6314}} output_shape has incorrect number of elements: 2 should be: 1\n\t [[{{node model/feature_input/Code_trait._indicator/SparseToDense}}]]\n\t [[StatefulPartitionedCall_66/StatefulPartitionedCall]]""
    }

The normal output shape of the model is (1, 1).

The curl call I use is:

    curl -d '{""instances"":[{""Sexe"":""M"",""MCD"":5,""Code_diag."":""I33.5"",""Code_trait."":""b66"",""Med._traitant"":""RickRich"",""DRG"":""x"",""N_med."":""5324"",""Traitement_principal"":[126,178,177,175,162,93,160,165,172,160,93,162,169,162,160,177],""Operation"":[161,205,208,205,204,191,208,205,197,208,191,206,198,199,195,0],""Diag._principal"":[144,179,172,188,52,23,6,7,106,26,106,176,182,191,190,35],""Diagnostic"":[181,186,194,177,191,192,181,179,173,192,181,187,186,191,108,175]}]}' -X POST http://localhost:8501/v1/models/Excel_AI_Model:predict
  

The versions used to create the SavedModel are the following: 

Python version:  3.6.9

Tensorflow version:  2.2.0

The Serving is the docker image tensorflow/serving:2.2.0.

The model metadata while served is this:

        {
    ""model_spec"":{
     ""name"": ""Excel_AI_Model"",
     ""signature_name"": """",
     ""version"": ""4""
    }
    ,
    ""metadata"": {""signature_def"": {
     ""signature_def"": {
      ""serving_default"": {
       ""inputs"": {
        ""Operation"": {
         ""dtype"": ""DT_INT32"",
         ""tensor_shape"": {
          ""dim"": [
           {
            ""size"": ""1"",
            ""name"": """"
           },
           {
            ""size"": ""16"",
            ""name"": """"
           }
          ],
          ""unknown_rank"": false
         },
         ""name"": ""serving_default_Operation:0""
        },
        ""DRG"": {
         ""dtype"": ""DT_STRING"",
         ""tensor_shape"": {
          ""dim"": [
           {
            ""size"": ""1"",
            ""name"": """"
           },
           {
            ""size"": ""1"",
            ""name"": """"
           }
          ],
          ""unknown_rank"": false
         },
         ""name"": ""serving_default_DRG:0""
        },
        ""Code_trait."": {
         ""dtype"": ""DT_STRING"",
         ""tensor_shape"": {
          ""dim"": [
           {
            ""size"": ""1"",
            ""name"": """"
           },
           {
            ""size"": ""1"",
            ""name"": """"
           }
          ],
          ""unknown_rank"": false
         },
         ""name"": ""serving_default_Code_trait.:0""
        },
        ""MCD"": {
         ""dtype"": ""DT_INT32"",
         ""tensor_shape"": {
          ""dim"": [
           {
            ""size"": ""1"",
            ""name"": """"
           },
           {
            ""size"": ""1"",
            ""name"": """"
           }
          ],
          ""unknown_rank"": false
         },
         ""name"": ""serving_default_MCD:0""
        },
        ""Diagnostic"": {
         ""dtype"": ""DT_INT32"",
         ""tensor_shape"": {
          ""dim"": [
           {
            ""size"": ""1"",
            ""name"": """"
           },
           {
            ""size"": ""16"",
            ""name"": """"
           }
          ],
          ""unknown_rank"": false
         },
         ""name"": ""serving_default_Diagnostic:0""
        },
        ""Traitement_principal"": {
         ""dtype"": ""DT_INT32"",
         ""tensor_shape"": {
          ""dim"": [
           {
            ""size"": ""1"",
            ""name"": """"
           },
           {
            ""size"": ""16"",
            ""name"": """"
           }
          ],
          ""unknown_rank"": false
         },
         ""name"": ""serving_default_Traitement_principal:0""
        },
        ""N_med."": {
         ""dtype"": ""DT_STRING"",
         ""tensor_shape"": {
          ""dim"": [
           {
            ""size"": ""1"",
            ""name"": """"
           },
           {
            ""size"": ""1"",
            ""name"": """"
           }
          ],
          ""unknown_rank"": false
         },
         ""name"": ""serving_default_N_med.:0""
        },
        ""Code_diag."": {
         ""dtype"": ""DT_STRING"",
         ""tensor_shape"": {
          ""dim"": [
           {
            ""size"": ""1"",
            ""name"": """"
           },
           {
            ""size"": ""1"",
            ""name"": """"
           }
          ],
          ""unknown_rank"": false
         },
         ""name"": ""serving_default_Code_diag.:0""
        },
        ""Diag._principal"": {
         ""dtype"": ""DT_INT32"",
         ""tensor_shape"": {
          ""dim"": [
           {
            ""size"": ""1"",
            ""name"": """"
           },
           {
            ""size"": ""16"",
            ""name"": """"
           }
          ],
          ""unknown_rank"": false
         },
         ""name"": ""serving_default_Diag._principal:0""
        },
        ""Sexe"": {
         ""dtype"": ""DT_STRING"",
         ""tensor_shape"": {
          ""dim"": [
           {
            ""size"": ""1"",
            ""name"": """"
           },
           {
            ""size"": ""1"",
            ""name"": """"
           }
          ],
          ""unknown_rank"": false
         },
         ""name"": ""serving_default_Sexe:0""
        },
        ""Med._traitant"": {
         ""dtype"": ""DT_STRING"",
         ""tensor_shape"": {
          ""dim"": [
           {
            ""size"": ""1"",
            ""name"": """"
           },
           {
            ""size"": ""1"",
            ""name"": """"
           }
          ],
          ""unknown_rank"": false
         },
         ""name"": ""serving_default_Med._traitant:0""
        }
       },
       ""outputs"": {
        ""result_output"": {
         ""dtype"": ""DT_FLOAT"",
         ""tensor_shape"": {
          ""dim"": [
           {
            ""size"": ""1"",
            ""name"": """"
           },
           {
            ""size"": ""1"",
            ""name"": """"
           }
          ],
          ""unknown_rank"": false
         },
         ""name"": ""StatefulPartitionedCall_66:0""
        }
       },
       ""method_name"": ""tensorflow/serving/predict""
      },
      ""__saved_model_init_op"": {
       ""inputs"": {},
       ""outputs"": {
        ""__saved_model_init_op"": {
         ""dtype"": ""DT_INVALID"",
         ""tensor_shape"": {
          ""dim"": [],
          ""unknown_rank"": true
         },
         ""name"": ""NoOp""
        }
       },
       ""method_name"": """"
      }
     }
    }
    }
    }

Already posted this on stackoverflow, but got no reply:
https://stackoverflow.com/questions/63073490/tensorflow-serving-2-2-0-error-output-shape-has-incorrect-number-of-elements-2"
41821,"Linking error: ""undefined reference to"" - using CMSIS NN","@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): make with Linux Ubuntu 20.04, complied on Atmel Studio on Windows 10, Python 3.7.7
- TensorFlow installed from (source or binary): downloaded from master
- Tensorflow version (commit SHA if source): 2.3.0, e544dce3a3a43631811e0760db5c33fe0a7519ba
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Atmel SAMD51 - Atmel Studio

**Describe the problem**
I have made the projects as described on the tflite for microcontroller webpage specifying TAGS=cmsis-nn to use the optimized backend. I have then moved the files from magic wand project to my project on Atmel Studio, making sure to add all the directories in a correct manner and compiled the project. Compilation is succesfull but I get a bunch of errors when trying to link:
```
Severity	Code	Description	Project	File	Line
Error		recipe for target 'TFLite_SAMD51.elf' failed	TFLite_SAMD51	Debug\Makefile	1314
Error		undefined reference to `tflite::micro::GetTensorShape(TfLiteEvalTensor const*)'	TFLite_SAMD51	tensorflow\lite\kernels\internal\reference\conv.h	69
Error		undefined reference to `tflite::micro::GetTensorShape(TfLiteEvalTensor const*)'	TFLite_SAMD51	tensorflow\lite\kernels\internal\reference\conv.h	149
Error		undefined reference to `tflite::micro::GetTensorShape(TfLiteEvalTensor const*)'	TFLite_SAMD51	tensorflow\lite\kernels\internal\reference\integer_ops\conv.h	72
Error		undefined reference to `tflite::micro::GetTensorShape(TfLiteEvalTensor const*)'	TFLite_SAMD51	tensorflow\lite\kernels\internal\reference\integer_ops\pooling.h	122
Error		undefined reference to `tflite::micro::GetTensorShape(TfLiteEvalTensor const*)'	TFLite_SAMD51	tensorflow\lite\kernels\internal\reference\pooling.h	118
Error		undefined reference to `tflite::micro::GetEvalInput(TfLiteContext const*, TfLiteNode const*, int)'	TFLite_SAMD51	tensorflow\lite\kernels\internal\types.h	390
Error		undefined reference to `tflite::micro::GetEvalOutput(TfLiteContext const*, TfLiteNode const*, int)'	TFLite_SAMD51	tensorflow\lite\kernels\internal\types.h	390
Error		undefined reference to `tflite::micro::GetTensorShape(TfLiteEvalTensor const*)'	TFLite_SAMD51	tensorflow\lite\kernels\internal\types.h	390
Error		undefined reference to `tflite::micro::GetTensorShape(TfLiteEvalTensor const*)'	TFLite_SAMD51	tensorflow\lite\kernels\internal\types.h	391
Error		undefined reference to `tflite::micro::GetEvalInput(TfLiteContext const*, TfLiteNode const*, int)'	TFLite_SAMD51	tensorflow\lite\kernels\internal\types.h	391
Error		undefined reference to `tflite::micro::GetEvalOutput(TfLiteContext const*, TfLiteNode const*, int)'	TFLite_SAMD51	tensorflow\lite\kernels\internal\types.h	391
Error		undefined reference to `tflite::micro::GetTensorShape(TfLiteEvalTensor const*)'	TFLite_SAMD51	tensorflow\lite\kernels\internal\types.h	391
Error		undefined reference to `tflite::micro::GetTensorShape(TfLiteEvalTensor const*)'	TFLite_SAMD51	tensorflow\lite\kernels\internal\types.h	392
Error		undefined reference to `tflite::micro::GetEvalInput(TfLiteContext const*, TfLiteNode const*, int)'	TFLite_SAMD51	tensorflow\lite\micro\kernels\conv.cpp	308
Error		undefined reference to `tflite::micro::GetEvalOutput(TfLiteContext const*, TfLiteNode const*, int)'	TFLite_SAMD51	tensorflow\lite\micro\kernels\conv.cpp	308
Error		undefined reference to `tflite::micro::GetTensorShape(TfLiteEvalTensor const*)'	TFLite_SAMD51	tensorflow\lite\micro\kernels\fully_connected.cpp	178
Error		undefined reference to `tflite::micro::GetTensorShape(TfLiteEvalTensor const*)'	TFLite_SAMD51	tensorflow\lite\micro\kernels\fully_connected.cpp	178
Error		undefined reference to `tflite::micro::GetTensorShape(TfLiteEvalTensor const*)'	TFLite_SAMD51	tensorflow\lite\micro\kernels\fully_connected.cpp	202
Error		undefined reference to `tflite::micro::GetEvalInput(TfLiteContext const*, TfLiteNode const*, int)'	TFLite_SAMD51	tensorflow\lite\micro\kernels\fully_connected.cpp	228
Error		undefined reference to `tflite::micro::GetEvalOutput(TfLiteContext const*, TfLiteNode const*, int)'	TFLite_SAMD51	tensorflow\lite\micro\kernels\fully_connected.cpp	228
Error		undefined reference to `tflite::micro::GetTensorShape(TfLiteEvalTensor const*)'	TFLite_SAMD51	tensorflow\lite\micro\kernels\fully_connected.cpp	228
```
I have adapted the magic wand project made without the CMSIS-NN tag and it runs smoothly.
The working non-cmsis can be found here: https://github.com/Sixaxis9/TFLite-SAMD51

**Please provide the exact sequence of commands/steps when you ran into the problem**
`make -f tensorflow/lite/micro/tools/make/Makefile TAGS=cmsis-nn generate_projects`
go to `tensorflow/lite/micro/tools/make/gen/linux_x86_64/prj/magic_wand/tensorflow_lite/src` and copy the tensorflow and third_parties folders into an Atmel studio project
Added `./` to the directory in compiler settings
Used the same main as the non cmsis variant I have been using succesfully until now (derived anyway from the example)."
41820,windows build error(makedataset),"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.4.0
- Python version: 3.8.3
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): 3.4.1
- GCC/Compiler version (if compiling from source): Visual Studio 2019
- CUDA/cuDNN version: 11.0/8.0.2
- GPU model and memory: RTX2070 GDDR6 8GB



**Describe the problem**

build error ( link error )

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
./configure
bazel build --copt=-nvcc_options=disable-warnings --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
optimize_dataset_op.lo.lib(optimize_dataset_op.obj) : error LNK2019: unresolved external symbol ""class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > __cdecl tensorflow::port::JobName(void)"" (?JobName@port@tensorflow@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@XZ) referenced in function ""protected: virtual void __cdecl tensorflow::data::OptimizeDatasetOp::MakeDataset(class tensorflow::OpKernelContext *,class tensorflow::data::DatasetBase *,class tensorflow::data::DatasetBase * *)"" (?MakeDataset@OptimizeDatasetOp@data@tensorflow@@MEAAXPEAVOpKernelContext@3@PEAVDatasetBase@23@PEAPEAV523@@Z)
bazel-out\x64_windows-opt\bin\tensorflow\python\_pywrap_tensorflow_internal.so : fatal error LNK1120: 1 unresolved externals
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```"
41818,esp - No rule to make target person_detect_model_data.cc,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):

macOS Catalina - 10.15.5 (19F101)

- TensorFlow installed from (source or binary):

source cloned from git - have tried r2.2, r2.3 and master

- Tensorflow version (commit SHA if source):

f295633406569f9a6ee71467a9bb34ef1cc6852b

- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):

ESP32

**Describe the problem**

Trying to generate all projects fails

**Please provide the exact sequence of commands/steps when you ran into the problem**

```
$make -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_projects
tensorflow/lite/micro/tools/make/Makefile:305: warning: overriding recipe for target 'tensorflow/lite/micro/tools/make/downloads/ruy'
tensorflow/lite/micro/tools/make/Makefile:305: warning: ignoring old recipe for target 'tensorflow/lite/micro/tools/make/downloads/ruy'
tensorflow/lite/micro/tools/make/Makefile:305: warning: overriding recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_grayscale'
tensorflow/lite/micro/tools/make/Makefile:305: warning: ignoring old recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_grayscale'
tensorflow/lite/micro/tools/make/Makefile:305: warning: overriding recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'
tensorflow/lite/micro/tools/make/Makefile:305: warning: ignoring old recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'
make: *** No rule to make target 'tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/person_detection/make/tensorflow/lite/micro/examples/person_detection/person_model_grayscale/person_detect_model_data.cc', needed by 'generate_person_detection_make_project'.  Stop.
```
"
41817,"TPU Error: Unable to destroy remote tensor handles. If you are running a tf.function, it usually indicates some op in the graph gets an error: Socket closed","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
https://www.kaggle.com/ratthachat/flickr-image-captioning-tpu-tf2-glove

- TensorFlow installed from (source or binary): installed source
- TensorFlow version (use command below): 2.2
- Python version: 3.7

**Describe the current behavior**

I was running the provided code from Kaggle on a Google cloud TPU.
Hardware: v3-8

It was training fine till when I had 500000 Images. but the moment I increase the amount of data to 1 million. at almost the end of 1st epoch, I am getting the following error. I am new to TPUs so any help will be appreciated.

Traceback (most recent call last):
  File ""flickr-image-captioning-tpu-tf2-glove.py"", line 178, in <module>
    for (batch, inputs) in tqdm_notebook(enumerate(train_dist_dataset)): # by .repeat() this will indefinitely run
  File ""/usr/local/lib/python3.7/dist-packages/tqdm/notebook.py"", line 218, in __iter__
    for obj in super(tqdm_notebook, self).__iter__(*args, **kwargs):
  File ""/usr/local/lib/python3.7/dist-packages/tqdm/std.py"", line 1129, in __iter__
    for obj in iterable:
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py"", line 296, in __next__
    return self.get_next()
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py"", line 328, in get_next
    global_has_value, replicas = _get_next_as_optional(self, self._strategy)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py"", line 192, in _get_next_as_optional
    iterator._iterators[i].get_next_as_list(new_name))  # pylint: disable=protected-access
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py"", line 1150, in get_next_as_list
    strict=True,
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 1204, in cond
    if pred:
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py"", line 884, in __bool__
    return bool(self._numpy())
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py"", line 929, in _numpy
    six.raise_from(core._status_to_exception(e.code, e.message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.UnavailableError: Socket closed
Additional GRPC error information:
{""created"":""@1595426699.706157313"",""description"":""Error received from peer ipv4:10.255.26.210:8470"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}
2020-07-22 14:05:01.237755: W tensorflow/core/distributed_runtime/eager/remote_tensor_handle_data.cc:76] Unable to destroy remote tensor handles. If you are running a tf.function, it usually indicates some op in the graph gets an error: Socket closed
Additional GRPC error information:
{""created"":""@1595426699.706157313"",""description"":""Error received from peer ipv4:10.255.26.210:8470"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
41816,TFLM 2.3 fails assert in CMSIS-NN fully_connected.cc,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Verified on Mbed OS 5.13
- TensorFlow installed from (source or binary): Source
- Tensorflow version (commit SHA if source): 0c43ad89f81b22d81d1894f5b53f9fbdda0b738a
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Mbed OS (ST IoT Discovery Kit)

**Describe the problem**

We have a very simple fully connected network (33x20x10x4 neurons) that fails an assert when being invoked with CMSIS-NN kernels here (in **fully_connected.cc**):

```
    cmsis_nn_dims input_dims;
    input_dims.n = batches;
    input_dims.h = input_shape.Dims(1);
    input_dims.w = input_shape.Dims(2); // <-- here
```

Here `input_shape` is the input layer (33 neurons) which does not have this dimension (`size_` is 2). Setting `input_dims.w` and `input_dims.c` to `1` solves the issue.

**Please provide the exact sequence of commands/steps when you ran into the problem**

See above. Attached the tflite model.
[ei-continuous-gestures-nn-classifier-tensorflow-lite-int8-quantized-model.lite.zip](https://github.com/tensorflow/tensorflow/files/4987903/ei-continuous-gestures-nn-classifier-tensorflow-lite-int8-quantized-model.lite.zip)


"
41815,SavedModel exporting fails on RNNs by setting wrong input dtypes,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0
- Python version: 3.7.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
When trying to export concrete functions in a Keras model to the SavedModel format, if a function uses `tf.keras.layers.RNN`, the dtypes of the RNN cell input arguments are wrong. In particular, actual dtypes seem to be ignored and replaced with tf.float32. This does not happen when calling the same concrete function manually, but only when trying to export it.

**Describe the expected behavior**
The input dtypes should be preserved, allowing to export the model. At the very least the behavior should be consistent with calling the concrete function.

**Standalone code to reproduce the issue**
```python
import tensorflow as tf

class TestRNNCell(tf.keras.layers.Layer):
  def __init__(self):
    super().__init__()
    self.units = 10
    self.state_size = 20

  def call(self, indices, state):
    # This assertion fails.
    assert indices.dtype == tf.int32
    # If the assertion is removed, this line fails with:
    # TypeError: Value passed to parameter 'indices' has DataType float32 not in list of allowed values: int32, int64
    output = tf.gather(tf.range(5), indices)
    return output, state

class TestModel(tf.keras.Model):
  def __init__(self):
    super().__init__()
    self.rnn = tf.keras.layers.RNN(TestRNNCell())

  @tf.function
  def do_stuff(self, indices):
    assert indices.dtype == tf.int32
    return self.rnn(indices)

model = TestModel()
tf.saved_model.save(model, 'test_model', signatures={
  'do_stuff': model.do_stuff.get_concrete_function(
      indices=tf.TensorSpec([None, None, 5], tf.int32))
})
```

Using `model.save` instead of `tf.saved_model.save` does not help either. Here's a version that fails just in the same way.
```python
class TestModel(tf.keras.Model):
  # [Continuing the class above]

  def call(self, indices):
    assert indices.dtype == tf.int32
    return self.rnn(indices)

model = TestModel()

# This works fine.
model(tf.zeros([10, 10, 5], tf.int32))

# This fails because the RNN inputs are now tf.float32.
model.save('test_model', save_format='tf')
```

Any possible workaround for this would be much appreciated. I cannot figure out any since this problem only affects SavedModel exporting and not the actual concrete functions being exported."
41814,Is there any document for running a model which is built by tf.estimator by tf-serving ?,The same [stackoverflow](https://stackoverflow.com/questions/63069621/tensorflow-serving-can-run-a-model-built-by-tensorflow-estimator)
41813,Are break statements in tf.function supported?,"## URL(s) with the issue:

https://www.tensorflow.org/tutorials/distribute/input

## Description of issue (what needs changing):

https://www.tensorflow.org/tutorials/distribute/input#usages states
> However break and return are currently not supported if the loop is placed inside a tf.function

But later the example at https://www.tensorflow.org/tutorials/distribute/input#use_iter_to_create_an_explicit_iterator shows:
```
@tf.function
def train_fn(distributed_iterator):
  for _ in tf.range(steps_per_loop):
    optional_data = distributed_iterator.get_next_as_optional()
    if not optional_data.has_value():
      break
    per_replica_results = strategy.run(lambda x:x, args=(optional_data.get_value(),))
    tf.print(strategy.experimental_local_results(per_replica_results))
```

This shows the usage of a `break` statement in a `tf.function`.

So are those statements supported? Are there conditions under which they are supported?"
41812,[Doc] Are partial batches supported with MultiWorkerMirroredStrategy,"## URL(s) with the issue:

- https://github.com/tensorflow/tensorflow/releases/tag/v2.3.0
- https://www.tensorflow.org/tutorials/distribute/input#partial_batches

## Description of issue (what needs changing):

The release note states 
> tf.distribute.experimental.MultiWorkerMirroredStrategy adds support for partial batches.

However the documentation/tutorial states
> Currently this is supported for all strategies except tf.distribute.experimental.MultiWorkerMirroredStrategy.
> [...] Partial batches are supported for all strategies except tf.distribute.experimental.MultiWorkerMirroredStrategy.

This sounds like those contradict each other. What is the actual reality? Can the documentation or the release notes be updated to clarify?"
41811,Building Model with Attention Layer: Graph disconnected: cannot obtain value for tensor,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04 
- TensorFlow version: 2.2.0
- Python version: 3.8.2


**Describe the problem**
I am trying to build a model with attention layer based on tutorial code as follows. But I am getting the below error.
**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
# Variable-length int sequences.
query_input = tf.keras.Input(shape=(None,), dtype='int32')
value_input = tf.keras.Input(shape=(None,), dtype='int32')

# Embedding lookup.
token_embedding = tf.keras.layers.Embedding(encoder.vocab_size, 64)
# Query embeddings of shape [batch_size, Tq, dimension].
query_embeddings = token_embedding(query_input)
# Value embeddings of shape [batch_size, Tv, dimension].
value_embeddings = token_embedding(value_input)

# CNN layer.
cnn_layer = tf.keras.layers.Conv1D(filters=100, kernel_size=4, padding='same')
# Query encoding of shape [batch_size, Tq, filters].
query_seq_encoding = cnn_layer(query_embeddings)
# Value encoding of shape [batch_size, Tv, filters].
value_seq_encoding = cnn_layer(value_embeddings)

# Query-value attention of shape [batch_size, Tq, filters].
query_value_attention_seq = tf.keras.layers.Attention()([query_seq_encoding, value_seq_encoding])

# Reduce over the sequence axis to produce encodings of shape
# [batch_size, filters].
query_encoding = tf.keras.layers.GlobalAveragePooling1D()(query_seq_encoding)
query_value_attention = tf.keras.layers.GlobalAveragePooling1D()(query_value_attention_seq)

# Concatenate query and document encodings to produce a DNN input layer.
input_layer = tf.keras.layers.Concatenate()([query_encoding, query_value_attention])

output_layer = tf.keras.layers.Dense(3, activation=tf.nn.softmax)(input_layer)
model  = tf.keras.Model(inputs=input_layer, outputs=output_layer)
```


**Any other info / logs**
```
WARNING:tensorflow:Model inputs must come from `tf.keras.Input` (thus holding past layer metadata), they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to ""model_8"" was not an Input tensor, it was generated by layer concatenate_22.
Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.
The tensor that caused the issue was: concatenate_22/Identity:0

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-52-f66b4debcaf8> in <module>
     33 
     34 output_layer = tf.keras.layers.Dense(3, activation=tf.nn.softmax)(input_layer)
---> 35 model  = tf.keras.Model(inputs=input_layer, outputs=output_layer)
     36 
     37 #############################################################################

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in __init__(self, *args, **kwargs)
    165 
    166   def __init__(self, *args, **kwargs):
--> 167     super(Model, self).__init__(*args, **kwargs)
    168     _keras_api_gauge.get_cell('model').set(True)
    169     # Model must be created under scope of DistStrat it will be trained with.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py in __init__(self, *args, **kwargs)
    171         'inputs' in kwargs and 'outputs' in kwargs):
    172       # Graph network
--> 173       self._init_graph_network(*args, **kwargs)
    174     else:
    175       # Subclassed network

/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    454     self._self_setattr_tracking = False  # pylint: disable=protected-access
    455     try:
--> 456       result = method(self, *args, **kwargs)
    457     finally:
    458       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py in _init_graph_network(self, inputs, outputs, name, **kwargs)
    305     # Keep track of the network's nodes and layers.
    306     nodes, nodes_by_depth, layers, _ = _map_graph_network(
--> 307         self.inputs, self.outputs)
    308     self._network_nodes = nodes
    309     self._nodes_by_depth = nodes_by_depth

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py in _map_graph_network(inputs, outputs)
   1790                              'The following previous layers '
   1791                              'were accessed without issue: ' +
-> 1792                              str(layers_with_complete_input))
   1793         for x in nest.flatten(node.output_tensors):
   1794           computable_tensors.add(id(x))

ValueError: Graph disconnected: cannot obtain value for tensor Tensor(""input_50:0"", shape=(None, None), dtype=int32) at layer ""input_50"". The following previous layers were accessed without issue: []
```

"
41810,Support for ragged tensor targets (variable-length y),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.3
- Python version: 3.6.9
- CUDA/cuDNN version: v10.2
- GPU model and memory: GeForce GTX 1070 - 8117MiB

**Describe the current behavior**:
In compiling the model, the loss function fails to capture the shape of y_true as it is a ragged tensor. 
This impedes sequence labelling tasks with both ragged inputs (x) and ragged sequential targets (y).

**Describe the expected behavior**:
I would expect to be able to train with both ragged inputs and targets (following the same internal structure).
In a sequence labelling task, e.g. named entity recognition, sentences and target vectors are variable-length-sequences. 

**Standalone code to reproduce the issue**

Link to the issue and reproducible code:
[LSTM ragged targets sequence labelling](https://colab.research.google.com/drive/18P6gZQUlP6qxBq70UCRfI42RSilRJ2Mx?usp=sharing) "
41809,Is SYCL on Tensorflow officially dead?,"It seems there is no community activity about SYCL for years and there are quite some untested dead code still living in the code base.
I'm wondering is SYCL dead and will it be removed from TF?

Thanks"
41808,Golang Tensorflow v2.3.0 installation fails,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux-4.19.0-9-amd64-x86_64-with-debian-10.4
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: 
-   **TensorFlow installed from (source or binary)**: Not installed (golang go command pull tensorflow from github)
-   **TensorFlow version (use command below)**: 2.3.0 (same bug in 2.2.0)
-   **Python version**:  2.7.16 (but python 3.7 is installed too)
-   **Bazel version (if compiling from source)**: 3.1.0
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**: go get github.com/tensorflow/tensorflow/tensorflow/go


### Describe the problem
BUG --- As in tensorflow-go@v2.2.0, tensorflow-go@v2.3.0 fail at installation.

### Source code / logs
From inside GOPATH:
go: found github.com/tensorflow/tensorflow/tensorflow/go in github.com/tensorflow/tensorflow v2.3.0+incompatible
go: finding module for package github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto
go: finding module for package github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto
../../go/pkg/mod/github.com/tensorflow/tensorflow@v2.3.0+incompatible/tensorflow/go/saved_model.go:25:2: module github.com/tensorflow/tensorflow@latest found (v2.3.0+incompatible), but does not contain package github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto

From inside a go.mod compliant directory:
go: found github.com/tensorflow/tensorflow/tensorflow/go in github.com/tensorflow/tensorflow v2.3.0+incompatible
go: finding module for package github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto
go: finding module for package github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto
../../go/pkg/mod/github.com/tensorflow/tensorflow@v2.3.0+incompatible/tensorflow/go/saved_model.go:25:2: module github.com/tensorflow/tensorflow@latest found (v2.3.0+incompatible), but does not contain package github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto

Looks like exactly the same old bug from tensorflow@v2.2.0 from this issue: https://github.com/tensorflow/tensorflow/issues/39307"
41807,TFLITE Relocate Tensor Fail,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No
- TensorFlow installed from (source or binary): pip install tensorflow-gpu
- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0
- Python version: 3.5.2
- Bazel version (if compiling from source): No 
- GCC/Compiler version (if compiling from source): No
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
Steps to generate .tflite-
1. Train a ckpt
2. Create saved_model.pb using None,None input parameter
3. Generate .TFLITE saved .pb model by specifying default input because None,None doesnt works

Generated .tflite Input Details. 
{'shape': array([  1, 256, 256,   3], dtype=int32), 'quantization': (0.0, 0), 'dtype': <class 'numpy.float32'>, 'index': 0, 'name': 'image'}
{'shape': array([  1, 256, 256,   1], dtype=int32), 'quantization': (0.0, 0), 'dtype': <class 'numpy.float32'>, 'index': 1, 'name': 'mask'}
{'shape': array([ 1, 64, 64,  1], dtype=int32), 'quantization': (0.0, 0), 'dtype': <class 'numpy.float32'>, 'index': 2, 'name': 'mask2'}
{'shape': array([  1, 128, 128,   1], dtype=int32), 'quantization': (0.0, 0), 'dtype': <class 'numpy.float32'>, 'index': 3, 'name': 'mask4'}

Actual Input Detail
(1, 432, 492, 3) (1, 432, 492, 1) (1, 216, 246, 1) (1, 108, 123, 1)

Allocating Tensors based on Actual Input Values
interpreter.resize_tensor_input(input_details[0]['index'], (1,h,w,3))
interpreter.resize_tensor_input(input_details[1]['index'], (1,h,w,1))
interpreter.resize_tensor_input(input_details[2]['index'], (1,int(h/4),int(w/4),1))
interpreter.resize_tensor_input(input_details[3]['index'], (1,int(h/2),int(w/2),1))
interpreter.allocate_tensors()

ERROR - 
File ""testtliteNone.py"", line 141, in <module>
    interpreter.allocate_tensors()
  File ""/homelib/python3.5/site-packages/tensorflow/lite/python/interpreter.py"", line 95, in allocate_tensors
    return self._interpreter.AllocateTensors()
  File ""/home/lib/python3.5/site-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py"", line 106, in AllocateTensors
    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)
RuntimeError: tensorflow/lite/kernels/kernel_util.cc:233 d1 == d2 || d1 == 1 || d2 == 1 was not true.Node number 4 (MUL) failed to prepare.

**Describe the expected behavior**
Allocation Should Be Done


**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
41806,compile tensorflow fails with error: SWIGing tensorflow/python/tensorflow.i ... swig failed: error executing command,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 6
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below):1.14, 1.15, 2.1, 2.2, 2.3
- Bazel version (if compiling from source): 2.0.0

**Describe the current behavior**

Running the build fails with a rather undescriptive:
```
ERROR: /home/username/src/tensorflow/tensorflow/python/BUILD:2974:1: SWIGing tensorflow/python/tensorflow.i failed (Exit 1): swig failed: error executing command
  (cd /tmp/_bazel_username/dcca333cc36b578f4473c754fbbc85ff/execroot/org_tensorflow && \
  exec env - \
  bazel-out/host/bin/external/swig/swig -c++ -python -module pywrap_tensorflow_internal -o bazel-out/local-py3-opt/bin/tensorflow/python/pywrap_tensorflow_internal.cc -outdir bazel-out/local-py3-opt/bin/tensorflow/python -ltensorflow/python/client/device_lib.i -ltensorflow/python/client/events_writer.i -ltensorflow/python/client/tf_session.i -ltensorflow/python/client/tf_sessionrun_wrapper.i -ltensorflow/python/framework/cpp_shape_inference.i -ltensorflow/python/framework/python_op_gen.i -ltensorflow/python/grappler/cluster.i -ltensorflow/python/grappler/cost_analyzer.i -ltensorflow/python/grappler/item.i -ltensorflow/python/grappler/model_analyzer.i -ltensorflow/python/grappler/tf_optimizer.i -ltensorflow/python/lib/core/py_func.i -ltensorflow/python/lib/core/strings.i -ltensorflow/python/lib/io/file_io.i -ltensorflow/python/lib/io/py_record_reader.i -ltensorflow/python/lib/io/py_record_writer.i -ltensorflow/python/platform/base.i -ltensorflow/python/pywrap_tfe.i -ltensorflow/python/training/quantize_training.i -ltensorflow/python/training/server_lib.i -ltensorflow/python/util/kernel_registry.i -ltensorflow/python/util/port.i -ltensorflow/python/util/py_checkpoint_reader.i -ltensorflow/python/util/stat_summarizer.i -ltensorflow/python/util/tfprof.i -ltensorflow/python/util/transform_graph.i -ltensorflow/python/util/util.i -Ibazel-out/local-py3-opt/genfiles -Iexternal/eigen_archive -Iexternal/grpc -Iexternal/protobuf_archive -Iexternal/swig -Iexternal/boringssl -Ibazel-out/local-py3-opt/genfiles/external/local_config_python -Iexternal/nsync -Iexternal/gemmlowp -Iexternal/jpeg -Iexternal/com_googlesource_code_re2 -Iexternal/jsoncpp_git -Iexternal/zlib_archive -Iexternal/highwayhash -Iexternal/gif_archive -Ibazel-out/local-py3-opt/genfiles/external/jpeg -Iexternal/lmdb -Iexternal/png_archive -Iexternal/farmhash_archive -Iexternal/local_config_cuda -Iexternal/sqlite_archive -Iexternal/swig/Lib -Iexternal/swig/Lib/cffi -Iexternal/swig/Lib/python -Iexternal/swig/Lib/std -Iexternal/swig/Lib/typemaps tensorflow/python/tensorflow.i).
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```

This is as reported in #14211, looked like an issue in Bazel (https://github.com/bazelbuild/bazel/issues/4053) but was deemed a Bug in TensorFlow, hence requesting a fix here

**Describe the expected behavior**

Build succeeds without need to apply patch from https://github.com/bazelbuild/bazel/issues/4053#issuecomment-343134886
"
41805,incompatible with expected float_ref.,"Traceback (most recent call last):
  File ""convert_TFLite.py"", line 90, in <module>
    convert_from_savedModel(savedModelDir, TFLiteFile, quan=True, integerOnly=True)
  File ""convert_TFLite.py"", line 50, in convert_from_savedModel
    TFLiteModel = converter.convert()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/lite/python/lite.py"", line 459, in convert
    self._funcs[0], lower_control_flow=False))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/convert_to_constants.py"", line 707, in convert_variables_to_constants_v2_as_graph
    frozen_func = _construct_concrete_function(func, graph_def, converted_inputs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/convert_to_constants.py"", line 406, in _construct_concrete_function
    new_output_names)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/wrap_function.py"", line 633, in function_from_graph_def
    wrapped_import = wrap_function(_imports_graph_def, [])
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/wrap_function.py"", line 611, in wrap_function
    collections={}),
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/func_graph.py"", line 981, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/wrap_function.py"", line 86, in __call__
    return self.call_with_variable_creator_scope(self._fn)(*args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/wrap_function.py"", line 92, in wrapped
    return fn(*args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/wrap_function.py"", line 631, in _imports_graph_def
    importer.import_graph_def(graph_def, name="""")
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/importer.py"", line 405, in import_graph_def
    producer_op_list=producer_op_list)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/importer.py"", line 501, in _import_graph_def_internal
    raise ValueError(str(e))
ValueError: Input 0 of node conv21/pointwise/BatchNorm/cond/Assign/Switch was passed float from conv21/pointwise/BatchNorm/moving_mean:0 incompatible with expected float_ref.

when I use ""tf.lite.TFLiteConverter.from_saved_model()"" to convert checkpoint to tflite，I got above err about BatchNorm, please help me, ths.

_Originally posted by @ZhouKai90 in https://github.com/tensorflow/tensorflow/issues/3628#issuecomment-663382551_"
41804,Error output shape of `tf.gather` in tflite,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 1.15.0

---

`tf.gather` has error behavior in tflite model.

My script:

```python
import os

import tensorflow as tf
from tensorflow.contrib import quantize as contrib_quantize

os.environ['CUDA_VISIBLE_DEVICES'] = ''
MODEL = './test.tflite'


def convert():
    with tf.Graph().as_default() as g:
        with tf.Session() as sess:
            data = tf.placeholder('float32', [1, 512, 2], name='data')
            indices = tf.placeholder('int64', [1, 1], name='indices')
            output = tf.gather(data, indices, batch_dims=1)

            # Tensor(""GatherV2:0"", shape=(1, 1, 2), dtype=float32)
            print(output)
            contrib_quantize.experimental_create_eval_graph(
                input_graph=g)
            converter = tf.lite.TFLiteConverter.from_session(
                sess,
                input_tensors=[data, indices],
                output_tensors=[output])
            tflite_model = converter.convert()
            with open(MODEL, ""wb"") as w:
                w.write(tflite_model)


def load():
    interpreter = tf.lite.Interpreter(MODEL)
    interpreter.allocate_tensors()
    output_details = interpreter.get_output_details()

    # [
    # {'name': 'GatherV2', 'index': 0,
    # 'shape': array([1, 1, 1, 2], dtype=int32),
    # 'dtype': <class 'numpy.float32'>,
    # 'quantization': (0.0, 0)}
    # ]
    print(output_details)


if __name__ == '__main__':
    convert()
    load()

```

The shape of `output` is `[1, 1, 2]` in graph, but its `[1, 1, 1, 2]` in tflite.

**logs**

```bash
2020-07-28 16:41:55.462113: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-07-28 16:41:56.271049: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2020-07-28 16:41:56.271163: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ubun
2020-07-28 16:41:56.271185: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ubun
2020-07-28 16:41:56.271414: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 410.48.0
2020-07-28 16:41:56.271483: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 410.48.0
2020-07-28 16:41:56.271502: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 410.48.0
2020-07-28 16:41:56.272061: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX
2020-07-28 16:41:56.304875: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2394405000 Hz
2020-07-28 16:41:56.308426: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d426dd1c60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-07-28 16:41:56.308484: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version

Tensor(""GatherV2:0"", shape=(1, 1, 2), dtype=float32)
2020-07-28 16:41:56.323565: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2020-07-28 16:41:56.323821: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-07-28 16:41:56.327368: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize
2020-07-28 16:41:56.327398: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: function_optimizer did nothing. time = 0.008ms.
2020-07-28 16:41:56.327411: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2020-07-28 16:41:56.332508: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2020-07-28 16:41:56.332641: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-07-28 16:41:56.342305: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize
2020-07-28 16:41:56.342358: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 4 nodes (0), 3 edges (0), time = 4.117ms.
2020-07-28 16:41:56.342383: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 4 nodes (0), 3 edges (0), time = 0.212ms.
[{'name': 'GatherV2', 'index': 0, 'shape': array([1, 1, 1, 2], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]
```
"
41803,The version of CUB in your include path is not compatible with this release of Thrust,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version: 2.3
- Python version: 3.8.5
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11/8
- GPU model and memory: 1070 ti

 i try
bazel --output_base=c:/bazel/output_dir/ build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package

and have some error:
#error The version of CUB in your include path is not compatible with this release of Thrust. CUB is now included in the CUDA Toolkit, so you no longer need to use your own checkout of CUB. Define THRUST_IGNORE_CUB_VERSION_CHECK to ignore this.
"
41799,RuntimeError: external/org_tensorflow/tensorflow/lite/core/subgraph.cc:1044 required_bytes != bytes (602112 != 150528),"Training and conversion model equipment：ubuntu18.04
tf :2.2.0
tb-nightly               1.14.0a20190603
tf-estimator-nightly     1.14.0.dev2019060501

I use Dev Board to predict：
edgetpu  ： 2.14.0       
tflite-runtime： 2.1.0.post1  
----------------------------------------------
My training and conversion model code:
[2.2.0.ipynb.zip](https://github.com/tensorflow/tensorflow/files/4986035/2.2.0.ipynb.zip)
hdf5 model:
[0.03-0.98.hdf5.zip](https://github.com/tensorflow/tensorflow/files/4986041/0.03-0.98.hdf5.zip)
tflite model:
[st2_io.tflite.zip](https://github.com/tensorflow/tensorflow/files/4986048/st2_io.tflite.zip)

Prediction program running on Dev Board：
[classify_image.py.zip](https://github.com/tensorflow/tensorflow/files/4986066/classify_image.py.zip)
edgetpu model(Model used for prediction）:
[st2_io_edgetpu.tflite.zip](https://github.com/tensorflow/tensorflow/files/4986053/st2_io_edgetpu.tflite.zip)
label:
[sparrow (1).txt](https://github.com/tensorflow/tensorflow/files/4986071/sparrow.1.txt)
Image:
[edgetest.zip](https://github.com/tensorflow/tensorflow/files/4986075/edgetest.zip)
-------------------------------------------------------
I got this error after executing classify_image.py on Dev Board：

mendel@orange-eft:~/sparrow$ python3 classify_image.py --model st2_io_edgetpu.tflite 
Traceback (most recent call last):
  File ""classify_image.py"", line 27, in <module>
    main()
  File ""classify_image.py"", line 21, in main
    for result in engine.classify_with_image(img, top_k=3):
  File ""/usr/lib/python3/dist-packages/edgetpu/classification/engine.py"", line 99, in classify_with_image
    return self.classify_with_input_tensor(input_tensor, threshold, top_k)
  File ""/usr/lib/python3/dist-packages/edgetpu/classification/engine.py"", line 123, in classify_with_input_tensor
    input_tensor)
  File ""/usr/lib/python3/dist-packages/edgetpu/basic/basic_engine.py"", line 136, in run_inference
    result = self._engine.RunInference(input)
  File ""/usr/lib/python3/dist-packages/edgetpu/swig/edgetpu_cpp_wrapper.py"", line 111, in RunInference
    return _edgetpu_cpp_wrapper.BasicEnginePythonWrapper_RunInference(self, input)
RuntimeError: external/org_tensorflow/tensorflow/lite/core/subgraph.cc:1044 required_bytes != bytes (602112 != 150528)















"
41798,S3 ParseURI supporting query parameters,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.0
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
Currently, when I pass query parameters as a part of my request to `TFRecordDataset`  the query becomes a part of the object name. For example:
```
filenames = [""s3://bucketname/path/to/file1.tfrecord?uuid=<UUID>"",
             ""s3://bucketname/path/to/file2.tfrecord?uuid=<UUID>""]
dataset = tf.data.TFRecordDataset(filenames)
```
On the server-side, we then see `path/to/file1.tfrecord?uuid=<UUID>` as the path with the `query params` as empty which should not be the case, as `?` is a query param delimiter and should be treated as such.

**Will this change the current api? How?**

No. But it will eliminate the need for a hack on the storage side to parse/strip URL query from object names. Thus, it will enhance TensorFlow <=> Cloud storage integration - which is a good thing.

**Who will benefit with this feature?**

People who run TensorFlow against datasets hosted by Object/Cloud storages. In our case, we are a team at NVIDIA, working on the open-source https://github.com/NVIDIA/aistore where TensorFlow is one of the critical client apps that we want to support.

It will be of great help, as the feature would be free of hacks.

**Any Other info.**"
41797,Keras saves invalid JSON files containing Infinity,"**Describe the current behavior**

JSON saved by Keras contains `Infinity` which is invalid according to [RFC 7159](https://tools.ietf.org/html/rfc7159):

> ""Numeric values that cannot be represented in the grammar below (such as Infinity and NaN) are not permitted.""

**Describe the expected behavior**

Keras saves correct JSON format.

**Standalone code to reproduce the issue** 

```
from tensorflow import keras
input = keras.Input(shape=(1))
x = keras.backend.sqrt(input)
model = keras.Model(input, x)
model.compile(optimizer='adam', loss='mse')
with open('repro.json', 'w') as json_file:
    json_file.write(model.to_json())
```

```
~ node
> JSON.parse(require('fs').readFileSync('repro.json', 'utf-8'))
Uncaught SyntaxError: Unexpected token I in JSON at position 508
```

[repro.zip](https://github.com/tensorflow/tensorflow/files/4985883/repro.zip)

tensorflow/tensorflow#37196
lutzroeder/netron#553

@goldiegadde @howl-anderson"
41796,the shared network inside a compiled keras model got modified after resetting the trainable attribute of the shared network,"
**System information**
- TensorFlow version (use command below):  2.3.0


**Describe the current behavior**

I would like to do domain-translation with three domains with gan. But I found the shared network got modified although my model has been compiled.


**Standalone code to reproduce the issue**

```python
from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, Dense, Flatten
from tensorflow.keras.models import Model

def dump_model( model, indent=0 ):
    print( '-'*indent, model.name, ('None-Trainable', 'Trainable')[model.trainable] )
    if hasattr(model, 'layers'):
        for layer in model.layers:
            dump_model( layer, indent+2 )

def build_critic():
    input = Input( shape=(28, 28, 1 ) )
    output = Dense(1)(Flatten()(input))
    return Model( input, output )

def generator():
    input = Input( shape=(28, 28, 1 ) )
    output = Conv2D( 1, (3, 3), padding='same' )(input)
    model = Model( input, output )
    model.compile( loss='mae', optimizer='adam' )
    return model

def build_mnist( optimizer = None ):
    input_shape = ( 28, 28, 1 )
    g_m2h = generator()
    g_h2m = generator()
    g_h2r = generator()
    g_r2h = generator()

    m_inputs = Input( shape=input_shape )
    m_output_1 = g_h2m( g_m2h( m_inputs ) )
    m_output_2 = g_h2m( g_r2h( g_h2r( g_m2h( m_inputs ) ) ) )

    r_inputs = Input( shape=input_shape )
    r_output_1 = g_h2r( g_r2h( r_inputs ) )
    r_output_2 = g_h2r( g_m2h (g_h2m( g_r2h( r_inputs ) ) ) )

    mh = g_m2h( m_inputs )
    mmh = g_r2h( g_h2r( mh ) )

    rh = g_r2h( r_inputs )
    rrh = g_m2h( g_h2m( rh ) )

    model_cycle = Model( inputs=[m_inputs, r_inputs], outputs=[m_output_1, m_output_2, r_output_1, r_output_2, mmh, rrh] )
    model_cycle.compile( loss='mae', optimizer='adam' )

    g_m2h.trainable = False
    g_h2m.trainable = False
    g_h2r.trainable = False
    g_r2h.trainable = False
    m_inputs = Input( shape=input_shape )
    g_m2r = g_h2r( g_m2h( m_inputs ) )
    model_m2r = Model( m_inputs, g_m2r, trainable=False )
    model_m2r.compile( loss='mae', optimizer='adam' ) # must compile here?

    critic_r = build_critic()
    critic = critic_r
    generator_model = model_m2r
    real_image = Input(shape=input_shape)
    valid = critic( real_image )
    noisy_image = Input(shape=input_shape)
    fake_image = generator_model(noisy_image)
    fake = critic( fake_image )
    critic_model = Model(inputs=[real_image, noisy_image], outputs=[valid, fake] )
    critic_model.compile(loss='mae', optimizer='adam' )
    model_wgan_r = critic_model
    model_wgan_r.summary()
    dump_model( model_wgan_r )

    r_inputs = Input( shape=input_shape )
    g_r2m = g_h2m( g_r2h( r_inputs ) )
    model_r2m = Model( r_inputs, g_r2m, trainable = False )
    model_r2m.compile( loss='mae', optimizer='adam' )

    critic_m = build_critic()
    critic = critic_m
    generator_model = model_r2m
    generator_model.Trainable = False
    real_image = Input(shape=input_shape)
    valid = critic( real_image )
    noisy_image = Input(shape=input_shape)
    fake_image = generator_model(noisy_image)
    fake = critic( fake_image )
    critic_model = Model(inputs=[real_image, noisy_image], outputs=[valid, fake] )
    critic_model.compile(loss='mae', optimizer='adam' )
    model_wgan_m = critic_model

    g_m2h.trainable = True
    g_h2m.trainable = True
    g_h2r.trainable = True
    g_r2h.trainable = True

    critic_r.trainable = False
    m_inputs = Input( shape=input_shape )
    m2r_critic = critic_r( g_h2r( g_m2h( m_inputs ) ) )
    model_critic_m2r = Model( m_inputs, m2r_critic  )
    model_critic_m2r.compile( loss='mae', optimizer='adam' )

    critic_m.trainable = False
    r_inputs = Input( shape=input_shape )
    r2m_critic = critic_m( g_h2m( g_r2h( r_inputs ) ) )
    model_critic_r2m = Model( r_inputs, r2m_critic )
    model_critic_r2m.compile( loss='mae', optimizer='adam' )

    print( '\n', '*'*80, '\n' )

    model_wgan_r.summary()
    dump_model( model_wgan_r )

    return model_cycle, model_wgan_r, model_wgan_m, model_critic_m2r, model_critic_r2m, model_m2r, model_r2m

if __name__ == ""__main__"":
    model_cycle, model_wgan_r, model_wgan_m, model_critic_m2r, model_critic_r2m, model_m2r, model_r2m = build_mnist()
```

This code produces the following outputs:

```
Model: ""functional_15""
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_10 (InputLayer)           [(None, 28, 28, 1)]  0                                            
__________________________________________________________________________________________________
input_9 (InputLayer)            [(None, 28, 28, 1)]  0                                            
__________________________________________________________________________________________________
functional_11 (Functional)      (None, 28, 28, 1)    20          input_10[0][0]                   
__________________________________________________________________________________________________
functional_13 (Functional)      (None, 1)            785         input_9[0][0]                    
                                                                 functional_11[0][0]              
==================================================================================================
Total params: 805
Trainable params: 785
Non-trainable params: 20
__________________________________________________________________________________________________
 functional_15 Trainable
-- input_10 Trainable
-- input_9 Trainable
-- functional_11 None-Trainable
---- input_7 Trainable
---- functional_1 None-Trainable
------ input_1 None-Trainable
------ conv2d None-Trainable
---- functional_5 None-Trainable
------ input_3 None-Trainable
------ conv2d_2 None-Trainable
-- functional_13 Trainable
---- input_8 Trainable
---- flatten Trainable
---- dense Trainable

 ******************************************************************************** 

Model: ""functional_15""
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_10 (InputLayer)           [(None, 28, 28, 1)]  0                                            
__________________________________________________________________________________________________
input_9 (InputLayer)            [(None, 28, 28, 1)]  0                                            
__________________________________________________________________________________________________
functional_11 (Functional)      (None, 28, 28, 1)    20          input_10[0][0]                   
__________________________________________________________________________________________________
functional_13 (Functional)      (None, 1)            785         input_9[0][0]                    
                                                                 functional_11[0][0]              
==================================================================================================
Total params: 805
Trainable params: 0
Non-trainable params: 805
__________________________________________________________________________________________________
 functional_15 Trainable
-- input_10 Trainable
-- input_9 Trainable
-- functional_11 None-Trainable
---- input_7 Trainable
---- functional_1 Trainable
------ input_1 Trainable
------ conv2d Trainable
---- functional_5 Trainable
------ input_3 Trainable
------ conv2d_2 Trainable
-- functional_13 None-Trainable
---- input_8 None-Trainable
---- flatten None-Trainable
---- dense None-Trainable
```

From the printed model summary information, we can see that even for a compiled composed model  `M = A * B`,  we can modify its layers trainable attribute without recompiling it, just by resetting `A` or `B`'s trainable attribute.



"
41794,Loss follows Learning Rate trend when using tf.keras.optimizers.schedules.LearningRateSchedule,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution : CentOS Linux 7.6.1810 
- TensorFlow installed from : binary (pip)
- TensorFlow version (use command below): 2.2.0-rc3
- Python version: 3.6.4
- CUDA/cuDNN version: CUDA 10.1 / cuDNN 7.6.0
- GPU model and memory: TitanX - 12GB


**Describe the current behavior**
The loss follows the pattern of learning rate when using [tf.keras.optimizers.schedules.LearningRateSchedule](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/LearningRateSchedule). 

Examples showing the issue : 
![learning_rate_sch](https://user-images.githubusercontent.com/16310456/88607865-4f9df580-d04e-11ea-9cbf-233fa8a67d86.png)
These are few learning rate schedulers tested on mnist with different step_size and min-max range.
In all these plots, the trend in loss is similar to that of learning rate.

**Describe the expected behavior**
The loss should be independent of learning rate trend. It should show high variance at higher learning rates and should be relatively stable with low learning rate.

**Standalone code to reproduce the issue**
Find gist [here](https://colab.research.google.com/gist/suraj-maniyar/e9669de59d0694ea6407f310d9923dc7/lrschedule.ipynb)
"
41792,sampled_softmax_loss weights and logits don't get gradients,"**System information**
- OS Platform and Distribution: Linux Ubuntu 20.04
- TensorFlow version: 2.2.0
- Python version: 3.8.2
- CUDA/cuDNN version: 10.2 / 7.6.2 
- GPU model and memory: TITAN X

**Describe the current behavior**

In order to use `tf.nn.sampled_softmax_loss` weights and biases need to be provided as inputs. I believe internally rows from those tensors are selected based on the samples and hte computation is performed.
The problem is that if you create a model with a final Dense layer and provide the weights and biases of that layer as input to `tf.nn.sampled_softmax_loss`, you end up receving a warning that gradients for them are not computed:

```
WARNING:tensorflow:Gradients do not exist for variables ['my_model/dense_1/kernel:0', 'my_model/dense_1/bias:0'] when minimizing the loss.
WARNING:tensorflow:Gradients do not exist for variables ['my_model/dense_1/kernel:0', 'my_model/dense_1/bias:0'] when minimizing the loss.
```
As a consequence, they never get updated during training.


**Describe the expected behavior**

Gradients for those tensors should be computed and they should get updated.

**Standalone code to reproduce the issue**

```python
import numpy as np
import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import SGD

num_classes = 500
num_epochs = 3
num_samples = 10000
batch_size = 10
learning_rate = 0.001

y = np.random.randint(0, num_classes, num_samples, dtype=np.int64)
x = np.expand_dims(y.astype(np.float32), -1)

x_test = x[:10]
y_test = y[:10]


class MyModel(Model):

    def __init__(self, num_classes, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.dense1 = Dense(10)
        self.dense2 = Dense(num_classes)
        self.first_step = True

    def call(self, inputs, training=None, mask=None):
        hidden = self.dense1(inputs)
        if training and not self.first_step:
            return None, hidden
        else:
            logits = self.dense2(hidden)
            return logits, hidden


class SampledSoftmaxCrossEntropyLoss(tf.keras.losses.Loss):
    def __init__(self, decoder_obj=None, num_classes=0):
        super().__init__()
        self.decoder_obj = decoder_obj
        self.num_classes = num_classes

    def call(self, labels, hidden):
        labels = tf.cast(tf.expand_dims(labels, -1), tf.int64)

        weights = tf.transpose(self.decoder_obj.get_weights()[0])
        biases = self.decoder_obj.get_weights()[1]

        sampled_values = tf.random.uniform_candidate_sampler(
            true_classes=labels,
            num_true=1,
            num_sampled=5,
            range_max=self.num_classes,
            unique=False
        )

        loss_val = tf.nn.sampled_softmax_loss(
            weights=weights,
            biases=biases,
            labels=labels,
            inputs=hidden,
            num_sampled=5,
            num_classes=self.num_classes,
            sampled_values=sampled_values)

        return loss_val


my_model = MyModel(num_classes)
optimizer = SGD(learning_rate=learning_rate)
sampled_loss = SampledSoftmaxCrossEntropyLoss(
    decoder_obj=my_model.dense2, num_classes=num_classes)


def train_step(model, loss, optimizer, inputs, targets):
    with tf.GradientTape() as tape:
        logits, hidden = model(inputs, training=True)
        loss_val = loss(targets, hidden)
    grads = tape.gradient(loss_val, model.trainable_weights)
    optimizer.apply_gradients(zip(grads, model.trainable_weights))
    return loss_val


def oredict(model, inputs):
    logits, _ = model(inputs, training=True)
    predictions = tf.argmax(logits, -1)
    return predictions


x_batches = np.split(x, 100)
y_batches = np.split(y, 100)

print(x_test)
print(oredict(my_model, x_test))

first_batch = True
for i in range(num_epochs):
    for x_batch, y_batch in zip(x_batches, y_batches):
        if first_batch:
            print(""Weights and biases after first batch"")
            print(my_model.dense2.get_weights()[0])
            print(my_model.dense2.get_weights()[1])
            first_batch = False

        loss_val = train_step(my_model, sampled_loss, optimizer, x_batch,
                              y_batch)
        print(loss_val)

print(x_test)
print(oredict(my_model, x_test))

print(""Weights and biases after training"")
print(my_model.dense2.get_weights()[0])
print(my_model.dense2.get_weights()[1])

```

"
41789,Error computing gradient of a vectorized scan loop,"This code snippet uses `tf.scan` to compute a sum of squared residuals, wraps it in `tf.vectorized_map`, and attempts to take the gradient:

```python
import numpy as np
import tensorflow as tf

np.random.seed(seed=42)
data = np.random.randn(100).astype(np.float32)

def log_prob(x):
  return tf.reduce_sum(tf.scan(
      lambda _, yi: (x - yi)**2,
      elems=data,
      initializer=tf.convert_to_tensor(0.),))

x = tf.Variable(tf.random.normal([10]))
v_log_prob = lambda x: tf.vectorized_map(log_prob, x)
with tf.GradientTape() as tape:
  lp = tf.reduce_sum(v_log_prob(x))
g = tape.gradient(lp, x)
```

I would expect the gradient to be a vector of the same size as `x`. Instead it raises an exception `InvalidArgumentError: Operation 'loop_body/scan/while/pfor/PartitionedCall' has no attr named '_XlaCompile'.`.

<details><summary>Full stack trace</summary>

```
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in get_attr(self, name)
   2512       with c_api_util.tf_buffer() as buf:
-> 2513         pywrap_tf_session.TF_OperationGetAttrValueProto(self._c_op, name, buf)
   2514         data = pywrap_tf_session.TF_GetBuffer(buf)

InvalidArgumentError: Operation 'loop_body/scan/while/pfor/PartitionedCall' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
67 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)
    330     try:
--> 331       xla_compile = op.get_attr(""_XlaCompile"")
    332       xla_separate_compiled_gradients = op.get_attr(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in get_attr(self, name)
   2516       # Convert to ValueError for backwards compatibility.
-> 2517       raise ValueError(str(e))
   2518     x = attr_value_pb2.AttrValue()

ValueError: Operation 'loop_body/scan/while/pfor/PartitionedCall' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in get_attr(self, name)
   2512       with c_api_util.tf_buffer() as buf:
-> 2513         pywrap_tf_session.TF_OperationGetAttrValueProto(self._c_op, name, buf)
   2514         data = pywrap_tf_session.TF_GetBuffer(buf)

InvalidArgumentError: Operation 'while' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)
    330     try:
--> 331       xla_compile = op.get_attr(""_XlaCompile"")
    332       xla_separate_compiled_gradients = op.get_attr(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in get_attr(self, name)
   2516       # Convert to ValueError for backwards compatibility.
-> 2517       raise ValueError(str(e))
   2518     x = attr_value_pb2.AttrValue()

ValueError: Operation 'while' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in get_attr(self, name)
   2512       with c_api_util.tf_buffer() as buf:
-> 2513         pywrap_tf_session.TF_OperationGetAttrValueProto(self._c_op, name, buf)
   2514         data = pywrap_tf_session.TF_GetBuffer(buf)

InvalidArgumentError: Operation 'while/while_body/cond' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)
    330     try:
--> 331       xla_compile = op.get_attr(""_XlaCompile"")
    332       xla_separate_compiled_gradients = op.get_attr(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in get_attr(self, name)
   2516       # Convert to ValueError for backwards compatibility.
-> 2517       raise ValueError(str(e))
   2518     x = attr_value_pb2.AttrValue()

ValueError: Operation 'while/while_body/cond' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in get_attr(self, name)
   2512       with c_api_util.tf_buffer() as buf:
-> 2513         pywrap_tf_session.TF_OperationGetAttrValueProto(self._c_op, name, buf)
   2514         data = pywrap_tf_session.TF_GetBuffer(buf)

InvalidArgumentError: Operation 'while/while_body/cond/PartitionedCall' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)
    330     try:
--> 331       xla_compile = op.get_attr(""_XlaCompile"")
    332       xla_separate_compiled_gradients = op.get_attr(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in get_attr(self, name)
   2516       # Convert to ValueError for backwards compatibility.
-> 2517       raise ValueError(str(e))
   2518     x = attr_value_pb2.AttrValue()

ValueError: Operation 'while/while_body/cond/PartitionedCall' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in get_attr(self, name)
   2512       with c_api_util.tf_buffer() as buf:
-> 2513         pywrap_tf_session.TF_OperationGetAttrValueProto(self._c_op, name, buf)
   2514         data = pywrap_tf_session.TF_GetBuffer(buf)

InvalidArgumentError: Operation 'loop_body/scan/while/TensorArrayV2Write/TensorListSetItem/pfor/Tile' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)
    330     try:
--> 331       xla_compile = op.get_attr(""_XlaCompile"")
    332       xla_separate_compiled_gradients = op.get_attr(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in get_attr(self, name)
   2516       # Convert to ValueError for backwards compatibility.
-> 2517       raise ValueError(str(e))
   2518     x = attr_value_pb2.AttrValue()

ValueError: Operation 'loop_body/scan/while/TensorArrayV2Write/TensorListSetItem/pfor/Tile' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
<ipython-input-230-367bca75a1b1> in <module>()
      1 with tf.GradientTape() as tape:
----> 2   lp = v_log_prob(x)
      3 g = tape.gradient(lp, x)

<ipython-input-229-89f89e42229e> in <lambda>(x)
      1 x = tf.Variable(tf.random.normal([10]))
----> 2 v_log_prob = lambda x: tf.vectorized_map(log_prob, x)
      3 print(v_log_prob(x))

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py in vectorized_map(fn, elems, fallback_to_while_loop)
    448     batch_size = array_ops.shape(first_elem)[0]
    449   return pfor(loop_fn, batch_size,
--> 450               fallback_to_while_loop=fallback_to_while_loop)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py in pfor(loop_fn, iters, fallback_to_while_loop, parallel_iterations)
    202       def_function.run_functions_eagerly(False)
    203     f = def_function.function(f)
--> 204   outputs = f()
    205   if functions_run_eagerly is not None:
    206     def_function.run_functions_eagerly(functions_run_eagerly)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    794       else:
    795         compiler = ""nonXla""
--> 796         result = self._call(*args, **kwds)
    797 
    798       new_tracing_count = self._get_tracing_count()

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    860               *args, **kwds)
    861       # If we did not create any variables the trace we have is good enough.
--> 862       return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access
    863 
    864     def fn_with_cond(*inner_args, **inner_kwds):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs, cancellation_manager)
   1856                            resource_variable_ops.BaseResourceVariable))],
   1857         captured_inputs=self.captured_inputs,
-> 1858         cancellation_manager=cancellation_manager)
   1859 
   1860   def _call_flat(self, args, captured_inputs, cancellation_manager=None):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1937         possible_gradient_type,
   1938         executing_eagerly)
-> 1939     forward_function, args_with_tangents = forward_backward.forward()
   1940     if executing_eagerly:
   1941       flat_outputs = forward_function.call(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in forward(self)
   1442     """"""Builds or retrieves a forward function for this call.""""""
   1443     forward_function = self._functions.forward(
-> 1444         self._inference_args, self._input_tangents)
   1445     return forward_function, self._inference_args + self._input_tangents
   1446 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in forward(self, inference_args, input_tangents)
   1194       (self._forward, self._forward_graph, self._backward,
   1195        self._forwardprop_output_indices, self._num_forwardprop_outputs) = (
-> 1196            self._forward_and_backward_functions(inference_args, input_tangents))
   1197     return self._forward
   1198 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _forward_and_backward_functions(self, inference_args, input_tangents)
   1346     outputs = self._func_graph.outputs[:self._num_inference_outputs]
   1347     return self._build_functions_for_outputs(
-> 1348         outputs, inference_args, input_tangents)
   1349 
   1350 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _build_functions_for_outputs(self, outputs, inference_args, input_tangents)
    904             self._func_graph.inputs,
    905             grad_ys=gradients_wrt_outputs,
--> 906             src_graph=self._func_graph)
    907 
    908       captures_from_forward = [

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)
    681                 # functions.
    682                 in_grads = _MaybeCompile(grad_scope, op, func_call,
--> 683                                          lambda: grad_fn(op, *out_grads))
    684               else:
    685                 # For function call ops, we add a 'SymbolicGradient'

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)
    334       xla_scope = op.get_attr(""_XlaScope"").decode()
    335     except ValueError:
--> 336       return grad_fn()  # Exit early
    337 
    338   if not xla_compile:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in <lambda>()
    681                 # functions.
    682                 in_grads = _MaybeCompile(grad_scope, op, func_call,
--> 683                                          lambda: grad_fn(op, *out_grads))
    684               else:
    685                 # For function call ops, we add a 'SymbolicGradient'

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _rewrite_forward_and_call_backward(self, op, *doutputs)
    717   def _rewrite_forward_and_call_backward(self, op, *doutputs):
    718     """"""Add outputs to the forward call and feed them to the grad function.""""""
--> 719     forward_function, backwards_function = self.forward_backward(len(doutputs))
    720     if not backwards_function.outputs:
    721       return backwards_function.structured_outputs

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in forward_backward(self, num_doutputs)
    626     if forward_backward is not None:
    627       return forward_backward
--> 628     forward, backward = self._construct_forward_backward(num_doutputs)
    629     self._cached_function_pairs[num_doutputs] = (forward, backward)
    630     return forward, backward

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _construct_forward_backward(self, num_doutputs)
    674           args=[], kwargs={},
    675           signature=signature,
--> 676           func_graph=backwards_graph)
    677       backwards_graph_captures = backwards_graph.external_captures
    678       captures_from_forward = [

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    984         _, original_func = tf_decorator.unwrap(python_func)
    985 
--> 986       func_outputs = python_func(*func_args, **func_kwargs)
    987 
    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _backprop_function(*grad_ys)
    664             self._func_graph.inputs,
    665             grad_ys=grad_ys,
--> 666             src_graph=self._func_graph)
    667 
    668     with self._func_graph.as_default():

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)
    681                 # functions.
    682                 in_grads = _MaybeCompile(grad_scope, op, func_call,
--> 683                                          lambda: grad_fn(op, *out_grads))
    684               else:
    685                 # For function call ops, we add a 'SymbolicGradient'

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)
    334       xla_scope = op.get_attr(""_XlaScope"").decode()
    335     except ValueError:
--> 336       return grad_fn()  # Exit early
    337 
    338   if not xla_compile:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in <lambda>()
    681                 # functions.
    682                 in_grads = _MaybeCompile(grad_scope, op, func_call,
--> 683                                          lambda: grad_fn(op, *out_grads))
    684               else:
    685                 # For function call ops, we add a 'SymbolicGradient'

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/while_v2.py in _WhileGrad(op, *grads)
    352   body_grad_graph, args = _create_grad_func(
    353       ys, xs, non_none_grads, cond_graph, body_graph,
--> 354       util.unique_grad_fn_name(body_graph.name), op, maximum_iterations)
    355 
    356   if body_grad_graph.while_op_needs_rewrite:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/while_v2.py in _create_grad_func(ys, xs, grads, cond_graph, body_graph, name, while_op, maximum_iterations)
    624       func_graph=_WhileBodyGradFuncGraph(name, cond_graph, body_graph,
    625                                          maximum_iterations, while_op,
--> 626                                          body_graph_inputs, body_graph_outputs))
    627 
    628   # Update the list of outputs with tensors corresponding to the captured

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    984         _, original_func = tf_decorator.unwrap(python_func)
    985 
--> 986       func_outputs = python_func(*func_args, **func_kwargs)
    987 
    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/while_v2.py in <lambda>(*args)
    620   grad_func_graph = func_graph_module.func_graph_from_py_func(
    621       name,
--> 622       lambda *args: _grad_fn(ys, xs, args, body_graph),
    623       args, {},
    624       func_graph=_WhileBodyGradFuncGraph(name, cond_graph, body_graph,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/while_v2.py in _grad_fn(ys, xs, args, func_graph)
    680   grad_outs = gradients_util._GradientsHelper(
    681       ys, xs, grad_ys=grad_ys, src_graph=func_graph,
--> 682       unconnected_gradients=""zero"")
    683 
    684   # TODO(b/118712257): Handle the case when grad_outs has None's e.g. when there

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)
    681                 # functions.
    682                 in_grads = _MaybeCompile(grad_scope, op, func_call,
--> 683                                          lambda: grad_fn(op, *out_grads))
    684               else:
    685                 # For function call ops, we add a 'SymbolicGradient'

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)
    334       xla_scope = op.get_attr(""_XlaScope"").decode()
    335     except ValueError:
--> 336       return grad_fn()  # Exit early
    337 
    338   if not xla_compile:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in <lambda>()
    681                 # functions.
    682                 in_grads = _MaybeCompile(grad_scope, op, func_call,
--> 683                                          lambda: grad_fn(op, *out_grads))
    684               else:
    685                 # For function call ops, we add a 'SymbolicGradient'

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/cond_v2.py in _IfGrad(op, *grads)
    119   # functions.
    120   true_grad_graph = _create_grad_func(
--> 121       true_graph, grads, util.unique_grad_fn_name(true_graph.name))
    122   false_grad_graph = _create_grad_func(
    123       false_graph, grads, util.unique_grad_fn_name(false_graph.name))

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/cond_v2.py in _create_grad_func(func_graph, grads, name)
    382       name,
    383       lambda: _grad_fn(func_graph, grads), [], {},
--> 384       func_graph=_CondGradFuncGraph(name, func_graph))
    385 
    386 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    984         _, original_func = tf_decorator.unwrap(python_func)
    985 
--> 986       func_outputs = python_func(*func_args, **func_kwargs)
    987 
    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/cond_v2.py in <lambda>()
    381   return func_graph_module.func_graph_from_py_func(
    382       name,
--> 383       lambda: _grad_fn(func_graph, grads), [], {},
    384       func_graph=_CondGradFuncGraph(name, func_graph))
    385 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/cond_v2.py in _grad_fn(func_graph, grads)
    372   result = gradients_util._GradientsHelper(
    373       ys, func_graph.inputs, grad_ys=grad_ys,
--> 374       src_graph=func_graph)
    375 
    376   return result

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)
    681                 # functions.
    682                 in_grads = _MaybeCompile(grad_scope, op, func_call,
--> 683                                          lambda: grad_fn(op, *out_grads))
    684               else:
    685                 # For function call ops, we add a 'SymbolicGradient'

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)
    334       xla_scope = op.get_attr(""_XlaScope"").decode()
    335     except ValueError:
--> 336       return grad_fn()  # Exit early
    337 
    338   if not xla_compile:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in <lambda>()
    681                 # functions.
    682                 in_grads = _MaybeCompile(grad_scope, op, func_call,
--> 683                                          lambda: grad_fn(op, *out_grads))
    684               else:
    685                 # For function call ops, we add a 'SymbolicGradient'

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _rewrite_forward_and_call_backward(self, op, *doutputs)
    717   def _rewrite_forward_and_call_backward(self, op, *doutputs):
    718     """"""Add outputs to the forward call and feed them to the grad function.""""""
--> 719     forward_function, backwards_function = self.forward_backward(len(doutputs))
    720     if not backwards_function.outputs:
    721       return backwards_function.structured_outputs

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in forward_backward(self, num_doutputs)
    626     if forward_backward is not None:
    627       return forward_backward
--> 628     forward, backward = self._construct_forward_backward(num_doutputs)
    629     self._cached_function_pairs[num_doutputs] = (forward, backward)
    630     return forward, backward

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _construct_forward_backward(self, num_doutputs)
    674           args=[], kwargs={},
    675           signature=signature,
--> 676           func_graph=backwards_graph)
    677       backwards_graph_captures = backwards_graph.external_captures
    678       captures_from_forward = [

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    984         _, original_func = tf_decorator.unwrap(python_func)
    985 
--> 986       func_outputs = python_func(*func_args, **func_kwargs)
    987 
    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _backprop_function(*grad_ys)
    664             self._func_graph.inputs,
    665             grad_ys=grad_ys,
--> 666             src_graph=self._func_graph)
    667 
    668     with self._func_graph.as_default():

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)
    681                 # functions.
    682                 in_grads = _MaybeCompile(grad_scope, op, func_call,
--> 683                                          lambda: grad_fn(op, *out_grads))
    684               else:
    685                 # For function call ops, we add a 'SymbolicGradient'

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)
    334       xla_scope = op.get_attr(""_XlaScope"").decode()
    335     except ValueError:
--> 336       return grad_fn()  # Exit early
    337 
    338   if not xla_compile:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in <lambda>()
    681                 # functions.
    682                 in_grads = _MaybeCompile(grad_scope, op, func_call,
--> 683                                          lambda: grad_fn(op, *out_grads))
    684               else:
    685                 # For function call ops, we add a 'SymbolicGradient'

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_grad.py in _TileGrad(op, grad)
    827         grad.values, math_ops.mod(grad.indices, input_shape_0), input_shape_0)
    828     split_shape = array_ops.concat([[1], split_shape[1:]], axis=0)
--> 829   input_grad = math_ops.reduce_sum(array_ops.reshape(grad, split_shape), axes)
    830   # Fix shape inference
    831   if not context.executing_eagerly():

/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)
    199     """"""Call target, and fall back on dispatchers if there is a TypeError.""""""
    200     try:
--> 201       return target(*args, **kwargs)
    202     except (TypeError, ValueError):
    203       # Note: convert_to_eager_tensor currently raises a ValueError, not a

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py in reduce_sum(input_tensor, axis, keepdims, name)
   2001 
   2002   return reduce_sum_with_dims(input_tensor, axis, keepdims, name,
-> 2003                               _ReductionDims(input_tensor, axis))
   2004 
   2005 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py in reduce_sum_with_dims(input_tensor, axis, keepdims, name, dims)
   2012   return _may_reduce_to_scalar(
   2013       keepdims, axis,
-> 2014       gen_math_ops._sum(input_tensor, dims, keepdims, name=name))
   2015 
   2016 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py in _sum(input, axis, keep_dims, name)
  10537   _, _, _op, _outputs = _op_def_library._apply_op_helper(
  10538         ""Sum"", input=input, reduction_indices=axis, keep_dims=keep_dims,
> 10539                name=name)
  10540   _result = _outputs[:]
  10541   if _execute.must_record_gradient():

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(op_type_name, name, **keywords)
    607             _SatisfiesTypeConstraint(base_type,
    608                                      _Attr(op_def, input_arg.type_attr),
--> 609                                      param_name=input_name)
    610           attrs[input_arg.type_attr] = attr_value
    611           inferred_from[input_arg.type_attr] = input_name

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py in _SatisfiesTypeConstraint(dtype, attr_def, param_name)
     59           ""allowed values: %s"" %
     60           (param_name, dtypes.as_dtype(dtype).name,
---> 61            "", "".join(dtypes.as_dtype(x).name for x in allowed_list)))
     62 
     63 

TypeError: Value passed to parameter 'input' has DataType variant not in list of allowed values: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, float16, uint32, uint64
```
</details>

Here's a colab notebook reproducing the issue with the current tf nightly:
https://colab.sandbox.google.com/drive/14ytDF-74jvDYtJXff0IktJLBin_BuKYJ#scrollTo=GvZjAFbX_-Un

Ashish, any idea where this might be coming from?

"
41787,RuntimeError: tensorflow/lite/kernels/detection_postprocess.cc:158 NumOutputs(node) != 4 (3 != 4)Node number 180 (TFLite_Detection_PostProcess) failed to prepare.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): pip install -ignore-installed --upgrade tensorflow-gpu==1.15
- TensorFlow version (or github SHA if from source):


**Provide the text output from tflite_convert**
FULL ERROR
C:\Users\Sreed\Anaconda3\envs\tensorflow2\lib\site-packages\numpy\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:
C:\Users\Sreed\Anaconda3\envs\tensorflow2\lib\site-packages\numpy\.libs\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll
C:\Users\Sreed\Anaconda3\envs\tensorflow2\lib\site-packages\numpy\.libs\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll
  stacklevel=1)
2020-07-27 23:06:48.543673: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
Traceback (most recent call last):
  File ""TFLite_detection_webcam.py"", line 140, in <module>
    interpreter.allocate_tensors()
  File ""C:\Users\Sreed\Anaconda3\envs\tensorflow2\lib\site-packages\tensorflow_core\lite\python\interpreter.py"", line 244, in allocate_tensors
    return self._interpreter.AllocateTensors()
  File ""C:\Users\Sreed\Anaconda3\envs\tensorflow2\lib\site-packages\tensorflow_core\lite\python\interpreter_wrapper\tensorflow_wrap_interpreter_wrapper.py"", line 106, in AllocateTensors
    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)
RuntimeError: tensorflow/lite/kernels/detection_postprocess.cc:158 NumOutputs(node) != 4 (3 != 4)Node number 180 (TFLite_Detection_PostProcess) failed to prepare.
```
# Copy and paste here
```

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.

Code from https://raw.githubusercontent.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/master/TFLite_detection_webcam.py by someone else. 
Right now have a detect.tflite file as well as a labelmap.txt file"
41786,"Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, FAKE_QUANT, LOGISTIC, RESHAPE. Here is a list of operators for which you will need custom implementations: TFLite_Detection_PostProcess.","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): pip installed
- TensorFlow version (or github SHA if from source): 1.15


**Provide the text output from tflite_convert**
I can make the tflite file by adding --allow_custom_ops at the end of the conversion code. toco --graph_def_file=""C:\tensorflow5\models\research\object_detection\TFLite_model\tflite_graph.pb"" --output_file=tflite_graph.tflite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --input_shape=1,300,300,3 --input_array=normalized_input_image_tensor --output_array=TFLite_Detection_PostProcess:2 --inference_type=FLOAT --input_type=FLOAT --allow_custom_ops 
however when I run the model for inference, I get an runtime error saying node number 180 (TFLite_Detection_PostProcess) failed to prepare.
```
# Copy and paste here
```

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.[


Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
"
41785,failed call to cuInit: UNKNOWN ERROR (-1),"Running the latest docker with:

    docker run -it -p 8888:8888 tensorflow/tensorflow:latest-gpu-jupyter jupyter notebook --notebook-dir=/tf --ip 0.0.0.0 --no-browser --allow-root --NotebookApp.allow_origin='https://colab.research.google.com'

code:

    import tensorflow as tf
    print(""Num GPUs Available: "", len(tf.config.experimental.list_physical_devices('GPU')))

gives me:

    2020-07-27 19:44:03.826149: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
    2020-07-27 19:44:03.826179: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (-1)
    2020-07-27 19:44:03.826201: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist


I'm on Pop_OS 20.04, have tried installing the CUDA drivers from the Pop repository as well as from NVidia. No dice. Any help appreciated.

Running 

    docker run --gpus all nvidia/cuda:10.0-base nvidia-smi

gives me:

    +-----------------------------------------------------------------------------+
    | NVIDIA-SMI 450.51.05    Driver Version: 450.51.05    CUDA Version: 11.0     |
    |-------------------------------+----------------------+----------------------+
    | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
    | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
    |                               |                      |               MIG M. |
    |===============================+======================+======================|
    |   0  GeForce RTX 2080    On   | 00000000:09:00.0  On |                  N/A |
    |  0%   52C    P5    15W / 225W |    513MiB /  7959MiB |     17%      Default |
    |                               |                      |                  N/A |
    +-------------------------------+----------------------+----------------------+
                                                                                   
    +-----------------------------------------------------------------------------+
    | Processes:                                                                  |
    |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
    |        ID   ID                                                   Usage      |
    |=============================================================================|
    +-----------------------------------------------------------------------------+

"
41784,C++ compilation of rule '//tensorflow/core/kernels:strided_slice_op_gpu' failed (Exit 2),"
**System information**
- OS Platform and Distribution (e.g., windows 10):
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.2.0
- Python version: 3.7
- Installed using virtualenv? pip? conda?:pip
- Bazel version (if compiling from source):2.0.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuda 10.1, cudnn 7.6.0
- GPU model and memory:  2060rtx, 16GB



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
python -m pip install --upgrade pip
python -m venv C:\Users\aya\tensorflow-v2.2.0\.venv
C:\Users\aya\tensorflow-v2.2.0\.venv\Scripts\activate.bat
cd C:\Users\aya\tensorflow-v2.2.0\.venv
dir
.\Scripts\activate
cd C:\Program Files (x86)\Microsoft Visual Studio\2019\Community
python -m pip install --upgrade pip
pip install setuptools --upgrade
pip install six numpy wheel   #(numpy==1.17.0 not 1.19.0) pip install numpy==1.17.0
pip install keras_applications==1.0.8 --no-deps
pip install keras_preprocessing==1.1.1 --no-deps
pip install h5py
pip list
cd C:\Users\aya\tensorflow-v2.2.0
git clone https://github.com/tensorflow/tensorflow 
cd tensorflow
git checkout v2.2.0
cd C:\Users\aya\tensorflow-v2.2.0\tensorflow
python ./configure.py
choose: y for cuda    (for GPU) computaion capability for geforce rtx 2060: 7.5
optimization flag:   /arch:AVX2
cd C:\Users\aya\tensorflow-v2.2.0\tensorflow

I run this:
bazel build --config=opt --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package

or

bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

and get the error:


ERROR: C:/users/aya/tensorflow-v2.2.0/tensorflow/tensorflow/core/kernels/BUILD:140:1: C++ compilation of rule '//tensorflow/core/kernels:strided_slice_op_gpu' failed (Exit 2)
C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/util/XprHelper.h(114): warning: __host__ annotation is ignored on a function(""no_assignment_operator"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/util/XprHelper.h(114): warning: __device__ annotation is ignored on a function(""no_assignment_operator"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/util/XprHelper.h(115): warning: __host__ annotation is ignored on a function(""no_assignment_operator"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/util/XprHelper.h(115): warning: __device__ annotation is ignored on a function(""no_assignment_operator"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/util/XprHelper.h(115): warning: __host__ annotation is ignored on a function(""~no_assignment_operator"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/util/XprHelper.h(115): warning: __device__ annotation is ignored on a function(""~no_assignment_operator"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/util/Memory.h(85): warning: ignoring return value from routine declared with ""nodiscard"" attribute

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/DenseBase.h(639): warning: __host__ annotation is ignored on a function(""DenseBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/DenseBase.h(639): warning: __device__ annotation is ignored on a function(""DenseBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/MatrixBase.h(484): warning: __host__ annotation is ignored on a function(""MatrixBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/MatrixBase.h(484): warning: __device__ annotation is ignored on a function(""MatrixBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/MatrixBase.h(485): warning: __host__ annotation is ignored on a function(""MatrixBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/MatrixBase.h(485): warning: __device__ annotation is ignored on a function(""MatrixBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/MatrixBase.h(485): warning: __host__ annotation is ignored on a function(""~MatrixBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/MatrixBase.h(485): warning: __device__ annotation is ignored on a function(""~MatrixBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/ArrayBase.h(156): warning: __host__ annotation is ignored on a function(""ArrayBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/ArrayBase.h(156): warning: __device__ annotation is ignored on a function(""ArrayBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/ArrayBase.h(157): warning: __host__ annotation is ignored on a function(""ArrayBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/ArrayBase.h(157): warning: __device__ annotation is ignored on a function(""ArrayBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/ArrayBase.h(157): warning: __host__ annotation is ignored on a function(""~ArrayBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/ArrayBase.h(157): warning: __device__ annotation is ignored on a function(""~ArrayBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/CwiseBinaryOp.h(105): warning: __host__ annotation is ignored on a function(""CwiseBinaryOp"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/CwiseBinaryOp.h(105): warning: __device__ annotation is ignored on a function(""CwiseBinaryOp"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/CwiseUnaryView.h(70): warning: __host__ annotation is ignored on a function(""CwiseUnaryView"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/CwiseUnaryView.h(70): warning: __device__ annotation is ignored on a function(""CwiseUnaryView"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/CwiseUnaryView.h(110): warning: __host__ annotation is ignored on a function(""CwiseUnaryViewImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/CwiseUnaryView.h(110): warning: __device__ annotation is ignored on a function(""CwiseUnaryViewImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/CwiseUnaryView.h(125): warning: __host__ annotation is ignored on a function(""CwiseUnaryViewImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/CwiseUnaryView.h(125): warning: __device__ annotation is ignored on a function(""CwiseUnaryViewImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/CwiseUnaryView.h(125): warning: __host__ annotation is ignored on a function(""~CwiseUnaryViewImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/CwiseUnaryView.h(125): warning: __device__ annotation is ignored on a function(""~CwiseUnaryViewImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/MapBase.h(185): warning: __host__ annotation is ignored on a function(""MapBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/MapBase.h(185): warning: __device__ annotation is ignored on a function(""MapBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/MapBase.h(186): warning: __host__ annotation is ignored on a function(""MapBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/MapBase.h(186): warning: __device__ annotation is ignored on a function(""MapBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/MapBase.h(186): warning: __host__ annotation is ignored on a function(""~MapBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/MapBase.h(186): warning: __device__ annotation is ignored on a function(""~MapBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/MapBase.h(300): warning: __host__ annotation is ignored on a function(""MapBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/MapBase.h(300): warning: __device__ annotation is ignored on a function(""MapBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/MapBase.h(301): warning: __host__ annotation is ignored on a function(""MapBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/MapBase.h(301): warning: __device__ annotation is ignored on a function(""MapBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/MapBase.h(301): warning: __host__ annotation is ignored on a function(""~MapBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/MapBase.h(301): warning: __device__ annotation is ignored on a function(""~MapBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Map.h(162): warning: __host__ annotation is ignored on a function(""Map"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Map.h(162): warning: __device__ annotation is ignored on a function(""Map"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Ref.h(90): warning: __host__ annotation is ignored on a function(""RefBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Ref.h(90): warning: __device__ annotation is ignored on a function(""RefBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Ref.h(232): warning: __host__ annotation is ignored on a function(""Ref"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Ref.h(232): warning: __device__ annotation is ignored on a function(""Ref"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Block.h(111): warning: __host__ annotation is ignored on a function(""Block"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Block.h(111): warning: __device__ annotation is ignored on a function(""Block"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Block.h(161): warning: __host__ annotation is ignored on a function(""BlockImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Block.h(161): warning: __device__ annotation is ignored on a function(""BlockImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Block.h(181): warning: __host__ annotation is ignored on a function(""BlockImpl_dense"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Block.h(181): warning: __device__ annotation is ignored on a function(""BlockImpl_dense"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Block.h(341): warning: __host__ annotation is ignored on a function(""BlockImpl_dense"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Block.h(341): warning: __device__ annotation is ignored on a function(""BlockImpl_dense"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/IndexedView.h(113): warning: __host__ annotation is ignored on a function(""IndexedView"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/IndexedView.h(113): warning: __device__ annotation is ignored on a function(""IndexedView"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Reshaped.h(103): warning: __host__ annotation is ignored on a function(""Reshaped"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Reshaped.h(103): warning: __device__ annotation is ignored on a function(""Reshaped"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Reshaped.h(137): warning: __host__ annotation is ignored on a function(""ReshapedImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Reshaped.h(137): warning: __device__ annotation is ignored on a function(""ReshapedImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Reshaped.h(155): warning: __host__ annotation is ignored on a function(""ReshapedImpl_dense"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Reshaped.h(155): warning: __device__ annotation is ignored on a function(""ReshapedImpl_dense"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Reshaped.h(215): warning: __host__ annotation is ignored on a function(""ReshapedImpl_dense"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Reshaped.h(215): warning: __device__ annotation is ignored on a function(""ReshapedImpl_dense"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Transpose.h(66): warning: __host__ annotation is ignored on a function(""Transpose"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Transpose.h(66): warning: __device__ annotation is ignored on a function(""Transpose"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Transpose.h(126): warning: __host__ annotation is ignored on a function(""TransposeImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Transpose.h(126): warning: __device__ annotation is ignored on a function(""TransposeImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Transpose.h(157): warning: __host__ annotation is ignored on a function(""TransposeImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Transpose.h(157): warning: __device__ annotation is ignored on a function(""TransposeImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Transpose.h(157): warning: __host__ annotation is ignored on a function(""~TransposeImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Transpose.h(157): warning: __device__ annotation is ignored on a function(""~TransposeImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Diagonal.h(78): warning: __host__ annotation is ignored on a function(""Diagonal"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Diagonal.h(78): warning: __device__ annotation is ignored on a function(""Diagonal"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/TriangularMatrix.h(222): warning: __host__ annotation is ignored on a function(""TriangularView"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/TriangularMatrix.h(222): warning: __device__ annotation is ignored on a function(""TriangularView"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/TriangularMatrix.h(559): warning: __host__ annotation is ignored on a function(""TriangularViewImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/TriangularMatrix.h(559): warning: __device__ annotation is ignored on a function(""TriangularViewImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/TriangularMatrix.h(560): warning: __host__ annotation is ignored on a function(""TriangularViewImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/TriangularMatrix.h(560): warning: __device__ annotation is ignored on a function(""TriangularViewImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/TriangularMatrix.h(560): warning: __host__ annotation is ignored on a function(""~TriangularViewImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/TriangularMatrix.h(560): warning: __device__ annotation is ignored on a function(""~TriangularViewImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Reverse.h(90): warning: __host__ annotation is ignored on a function(""Reverse"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Reverse.h(90): warning: __device__ annotation is ignored on a function(""Reverse"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/ArrayWrapper.h(47): warning: __host__ annotation is ignored on a function(""ArrayWrapper"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/ArrayWrapper.h(47): warning: __device__ annotation is ignored on a function(""ArrayWrapper"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/ArrayWrapper.h(145): warning: __host__ annotation is ignored on a function(""MatrixWrapper"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/ArrayWrapper.h(145): warning: __device__ annotation is ignored on a function(""MatrixWrapper"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\unsupported\Eigen\CXX11\src/Tensor/TensorBlock.h(245): warning: invalid friend declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\unsupported\Eigen\CXX11\src/Tensor/TensorBlock.h(709): warning: invalid friend declaration

external/com_google_protobuf/src\google/protobuf/map.h(1027): warning: invalid friend declaration

external/com_google_absl\absl/strings/string_view.h(501): warning: expression has no effect

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/util/XprHelper.h(114): warning: __host__ annotation is ignored on a function(""no_assignment_operator"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/util/XprHelper.h(114): warning: __device__ annotation is ignored on a function(""no_assignment_operator"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/util/XprHelper.h(115): warning: __host__ annotation is ignored on a function(""no_assignment_operator"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/util/XprHelper.h(115): warning: __device__ annotation is ignored on a function(""no_assignment_operator"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/util/XprHelper.h(115): warning: __host__ annotation is ignored on a function(""~no_assignment_operator"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/util/XprHelper.h(115): warning: __device__ annotation is ignored on a function(""~no_assignment_operator"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/DenseBase.h(639): warning: __host__ annotation is ignored on a function(""DenseBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/DenseBase.h(639): warning: __device__ annotation is ignored on a function(""DenseBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/MatrixBase.h(484): warning: __host__ annotation is ignored on a function(""MatrixBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/MatrixBase.h(484): warning: __device__ annotation is ignored on a function(""MatrixBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/MatrixBase.h(485): warning: __host__ annotation is ignored on a function(""MatrixBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/MatrixBase.h(485): warning: __device__ annotation is ignored on a function(""MatrixBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/MatrixBase.h(485): warning: __host__ annotation is ignored on a function(""~MatrixBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/MatrixBase.h(485): warning: __device__ annotation is ignored on a function(""~MatrixBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/ArrayBase.h(156): warning: __host__ annotation is ignored on a function(""ArrayBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/ArrayBase.h(156): warning: __device__ annotation is ignored on a function(""ArrayBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/ArrayBase.h(157): warning: __host__ annotation is ignored on a function(""ArrayBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/ArrayBase.h(157): warning: __device__ annotation is ignored on a function(""ArrayBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/ArrayBase.h(157): warning: __host__ annotation is ignored on a function(""~ArrayBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/ArrayBase.h(157): warning: __device__ annotation is ignored on a function(""~ArrayBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/CwiseBinaryOp.h(105): warning: __host__ annotation is ignored on a function(""CwiseBinaryOp"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/CwiseBinaryOp.h(105): warning: __device__ annotation is ignored on a function(""CwiseBinaryOp"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/CwiseUnaryView.h(70): warning: __host__ annotation is ignored on a function(""CwiseUnaryView"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/CwiseUnaryView.h(70): warning: __device__ annotation is ignored on a function(""CwiseUnaryView"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/CwiseUnaryView.h(110): warning: __host__ annotation is ignored on a function(""CwiseUnaryViewImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/CwiseUnaryView.h(110): warning: __device__ annotation is ignored on a function(""CwiseUnaryViewImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/CwiseUnaryView.h(125): warning: __host__ annotation is ignored on a function(""CwiseUnaryViewImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/CwiseUnaryView.h(125): warning: __device__ annotation is ignored on a function(""CwiseUnaryViewImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/CwiseUnaryView.h(125): warning: __host__ annotation is ignored on a function(""~CwiseUnaryViewImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/CwiseUnaryView.h(125): warning: __device__ annotation is ignored on a function(""~CwiseUnaryViewImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/MapBase.h(185): warning: __host__ annotation is ignored on a function(""MapBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/MapBase.h(185): warning: __device__ annotation is ignored on a function(""MapBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/MapBase.h(186): warning: __host__ annotation is ignored on a function(""MapBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/MapBase.h(186): warning: __device__ annotation is ignored on a function(""MapBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/MapBase.h(186): warning: __host__ annotation is ignored on a function(""~MapBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/MapBase.h(186): warning: __device__ annotation is ignored on a function(""~MapBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/MapBase.h(300): warning: __host__ annotation is ignored on a function(""MapBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/MapBase.h(300): warning: __device__ annotation is ignored on a function(""MapBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/MapBase.h(301): warning: __host__ annotation is ignored on a function(""MapBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/MapBase.h(301): warning: __device__ annotation is ignored on a function(""MapBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/MapBase.h(301): warning: __host__ annotation is ignored on a function(""~MapBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/MapBase.h(301): warning: __device__ annotation is ignored on a function(""~MapBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Map.h(162): warning: __host__ annotation is ignored on a function(""Map"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Map.h(162): warning: __device__ annotation is ignored on a function(""Map"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Ref.h(90): warning: __host__ annotation is ignored on a function(""RefBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Ref.h(90): warning: __device__ annotation is ignored on a function(""RefBase"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Ref.h(232): warning: __host__ annotation is ignored on a function(""Ref"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Ref.h(232): warning: __device__ annotation is ignored on a function(""Ref"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Block.h(111): warning: __host__ annotation is ignored on a function(""Block"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Block.h(111): warning: __device__ annotation is ignored on a function(""Block"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Block.h(161): warning: __host__ annotation is ignored on a function(""BlockImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Block.h(161): warning: __device__ annotation is ignored on a function(""BlockImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Block.h(181): warning: __host__ annotation is ignored on a function(""BlockImpl_dense"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Block.h(181): warning: __device__ annotation is ignored on a function(""BlockImpl_dense"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Block.h(341): warning: __host__ annotation is ignored on a function(""BlockImpl_dense"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Block.h(341): warning: __device__ annotation is ignored on a function(""BlockImpl_dense"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/IndexedView.h(113): warning: __host__ annotation is ignored on a function(""IndexedView"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/IndexedView.h(113): warning: __device__ annotation is ignored on a function(""IndexedView"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Reshaped.h(103): warning: __host__ annotation is ignored on a function(""Reshaped"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Reshaped.h(103): warning: __device__ annotation is ignored on a function(""Reshaped"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Reshaped.h(137): warning: __host__ annotation is ignored on a function(""ReshapedImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Reshaped.h(137): warning: __device__ annotation is ignored on a function(""ReshapedImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Reshaped.h(155): warning: __host__ annotation is ignored on a function(""ReshapedImpl_dense"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Reshaped.h(155): warning: __device__ annotation is ignored on a function(""ReshapedImpl_dense"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Reshaped.h(215): warning: __host__ annotation is ignored on a function(""ReshapedImpl_dense"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Reshaped.h(215): warning: __device__ annotation is ignored on a function(""ReshapedImpl_dense"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Transpose.h(66): warning: __host__ annotation is ignored on a function(""Transpose"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Transpose.h(66): warning: __device__ annotation is ignored on a function(""Transpose"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Transpose.h(126): warning: __host__ annotation is ignored on a function(""TransposeImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Transpose.h(126): warning: __device__ annotation is ignored on a function(""TransposeImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Transpose.h(157): warning: __host__ annotation is ignored on a function(""TransposeImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Transpose.h(157): warning: __device__ annotation is ignored on a function(""TransposeImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Transpose.h(157): warning: __host__ annotation is ignored on a function(""~TransposeImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Transpose.h(157): warning: __device__ annotation is ignored on a function(""~TransposeImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Diagonal.h(78): warning: __host__ annotation is ignored on a function(""Diagonal"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Diagonal.h(78): warning: __device__ annotation is ignored on a function(""Diagonal"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/TriangularMatrix.h(222): warning: __host__ annotation is ignored on a function(""TriangularView"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/TriangularMatrix.h(222): warning: __device__ annotation is ignored on a function(""TriangularView"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/TriangularMatrix.h(559): warning: __host__ annotation is ignored on a function(""TriangularViewImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/TriangularMatrix.h(559): warning: __device__ annotation is ignored on a function(""TriangularViewImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/TriangularMatrix.h(560): warning: __host__ annotation is ignored on a function(""TriangularViewImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/TriangularMatrix.h(560): warning: __device__ annotation is ignored on a function(""TriangularViewImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/TriangularMatrix.h(560): warning: __host__ annotation is ignored on a function(""~TriangularViewImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/TriangularMatrix.h(560): warning: __device__ annotation is ignored on a function(""~TriangularViewImpl"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Reverse.h(90): warning: __host__ annotation is ignored on a function(""Reverse"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/Reverse.h(90): warning: __device__ annotation is ignored on a function(""Reverse"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/ArrayWrapper.h(47): warning: __host__ annotation is ignored on a function(""ArrayWrapper"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/ArrayWrapper.h(47): warning: __device__ annotation is ignored on a function(""ArrayWrapper"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/ArrayWrapper.h(145): warning: __host__ annotation is ignored on a function(""MatrixWrapper"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/ArrayWrapper.h(145): warning: __device__ annotation is ignored on a function(""MatrixWrapper"") that is explicitly defaulted on its first declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\unsupported\Eigen\CXX11\src/Tensor/TensorBlock.h(245): warning: invalid friend declaration

C:\users\aya\_bazel_aya\apeokjhk\execroot\org_tensorflow\external\eigen_archive\unsupported\Eigen\CXX11\src/Tensor/TensorBlock.h(709): warning: invalid friend declaration

external/com_google_protobuf/src\google/protobuf/map.h(1027): warning: invalid friend declaration

external/com_google_absl\absl/strings/string_view.h(501): warning: expression has no effect

external/com_google_protobuf/src\google/protobuf/repeated_field.h(397): error C2993: 'T': is not a valid type for non-type template parameter '__formal'
external/com_google_protobuf/src\google/protobuf/repeated_field.h(406): note: see reference to class template instantiation 'google::protobuf::internal::TypeImplementsMergeBehaviorProbeForMergeFrom<T>' being compiled
external/com_google_protobuf/src\google/protobuf/repeated_field.h(397): error C2065: 'RetType': undeclared identifier
external/com_google_protobuf/src\google/protobuf/repeated_field.h(397): error C2923: 'std::_Select<__formal>::_Apply': 'RetType' is not a valid template type argument for parameter '<unnamed-symbol>'
external/com_google_protobuf/src\google/protobuf/repeated_field.h(397): error C2062: type 'unknown-type' unexpected
Target //tensorflow/tools/pip_package:build_pip_package failed to build


"
41783,TensorFlow 2.2 using tf.float16 executes only on CPU,"I'm using [this code](https://github.com/vcadillog/PPO-Mario-Bros-Tensorflow-2) as a starting point for one of my projects. The only modification I did so far is switching to OpenAI Gym Atari because I'm running it on Windows.

The problem I'm having is when I use tf.keras.backend.set_floatx(tf.float16) the code gets stuck executing on CPU. I also changed all the explicit casts throughout the code. The idea is to utilize the tensor cores on my GPU. This only happens with this code. All my other projects work fine.

I'm running it on TF 2.2. I tried removing tf.function annotations. I tried messing around with casts. For example, leaving all the data in fp32 and using fp16 weights in the model. Nothing helped. Honestly, I haven't tried much because I can't find any information on the problem and have no idea where to start debugging. There are no errors and everything works fine, except it runs on CPU instead of GPU. The only difference is in the mess that TF spews out when initializing I get this message:

`W tensorflow/compiler/jit/xla_device.cc:398] XLA_GPU and XLA_CPU devices are deprecated and will be removed in subsequent releases. Instead, use either @tf.function(experimental_compile=True) for must-compile semantics, or run with TF_XLA_FLAGS=--tf_xla_auto_jit=2 for auto-clustering best-effort compilation.`

I tried both options proposed in the message and nothing happened.

Also, I have tried [tf.device](https://www.tensorflow.org/api_docs/python/tf/device) but nothing changed

I really need this to work. Not just for the memory and performance optimizations but part of my project is benchmarking the difference between the two.

Any help would be appreciated or at least if someone could point me in the direction where to look for the information.

Thank you."
41781,Timeseries example and warning message with latest code: Executor start aborting: Invalid argument:,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): Pulled from source on 27th EST
- TensorFlow version (use command below): Latest from source
- Python version: 3.8.1
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.0
- GPU model and memory: TRX 2070

**Describe the current behavior**
When i run Timeseries example from Tensorflow.org, i get the following warning messages (a lot)

2020-07-27 14:23:21.829669: W tensorflow/core/common_runtime/executor.cc:1086] [/device:CPU:0] :  You must feed a value for placeholder tensor 'Placeholder/_1' with dtype double and shape [119759,72]
	 [[{{node Placeholder/_1}}]]

**Standalone code to reproduce the issue**
https://www.tensorflow.org/tutorials/structured_data/time_series#part_2_forecast_a_multivariate_time_series
"
41780,ResourceExhaustedError (out of memory) when training a Saved Model,"**System information**
- **OS Platform and Distribution:** Linux Ubuntu 18.04.4 64-bit
- **TensorFlow installed from:** pip
- **TensorFlow version:** 2.3.0-rc2 and 2.4.0-dev20200727
- **Python version:** 3.7.6
- **CUDA/cuDNN version:** CUDA 10.1, cuDNN 7.6.4.38
- **GPU model and memory:** RTX 2060 Super (8 GB)

**Describe the current behavior**
When building a model with a pretrained Xception net as base, than saving it in the Saved Model format, loading and running fit(), ResourceExhaustedError (out of memory) occurs.

**Describe the expected behavior**
When training the model with the same dataset, but without saving or if the Keras H5 format is used, everything is alright, so it looks like there is actually enough GPU memory and the loaded Saved Model should also train without errors. I've also tried replacing Xception with InceptionV3 and this removes the error.

**Standalone code to reproduce the issue**
https://github.com/OlegPonomaryov/xception-saved-model-bug-demo/blob/master/model.ipynb
"
41779,While trying to run 3D Unet of mesh-tensorflow ,"**System information**
- OS Platform : ubuntu 18.04 : x86_64 
- TensorFlow version 2.2.0:
- Python version 3.6:
- Installed using  pip:
- CUDA/cuDNN : **10.1.243** / **7.6** :
- GPU model and memory  :  pciBusID: 0000:1c:00.0 name: TITAN V computeCapability: 7.0
coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 11.75GiB deviceMemoryBandwidth: 607.97GiB/s



I was trying to run 3D Unet model in the experimental/ of mesh-tensorflow on GPUs.     
 **python model_executor.py --use_tpu=False --write_summary=False --train_file_pattern=media/tfrecords64/train/*.tfrecords  --eval_file_pattern=media/tfrecords64/eval/*.tfrecords   --batch_size_train=2  --ct_resolution=64 --mesh_shape=""rows:2, columns:2, cores:2"" --checkpoint_dir=checkpoint_dir_8gpu &**   

I removed tensorflow.contrib etc, in the code as contrib does not belong to 2.2.0. I installed tensorflow_probability. I adjusted/removed a part of code that depends on TPU, to run the above. 


However this is in the following in the logs. I can also see that GPU memory is all being used up as shown by nvidia-smi. The training seems to be slow too.

**tensorflow/core/common_runtime/colocation_graph.cc:1017] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
/job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices:
Root Member(assigned_device_name_index_=-1 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
AssignVariableOp: CPU XLA_CPU XLA_GPU
RandomUniform: CPU XLA_CPU XLA_GPU
VarIsInitializedOp: GPU CPU XLA_CPU XLA_GPU
Const: GPU CPU XLA_CPU XLA_GPU
Mul: CPU XLA_CPU XLA_GPU
ReadVariableOp: GPU CPU XLA_CPU XLA_GPU
Sub: CPU XLA_CPU XLA_GPU
VarHandleOp: CPU XLA_CPU XLA_GPU
Add: CPU XLA_CPU XLA_GPU** 


"
41778,Models with Conv2D layer cause segmentation fault when invoked in C++,"**System information**
- Linux Ubuntu 18.04
- TensorFlow Binary (via `pip`)
- Tested on 2.2.0 & 2.3.0rc2

**Command used to run the converter or code if you’re using the Python API**

```python
import tensorflow as tf
from tensorflow.keras.layers import Conv2D, Flatten, Input, Dense
from tensorflow.keras.models import Model

inputs = Input(shape=[10, 5, 1])
x = inputs
x = Conv2D(32, (3, 3))(x) # *** Functions correctly when removed
x = Flatten()(x)
x = Dense(1)(x)
    
model = Model(inputs, x)
model.compile()

# Convert the model.
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()

# Save the TF Lite model.
with tf.io.gfile.GFile(""min_model.tflite"", ""wb"") as f:
    f.write(tflite_model)
```

**The output from the converter invocation**

None

Models in `SavedModel` & `tflite` formats as well as `minimal` example build for `linux_x86_64`:

```
https://www.dropbox.com/sh/2w67cix39onn28m/AAD0O3OR4Z_y_zMJJJdGBdtQa?dl=0
```

**Failure details**
* Conversion successful
* Causes segmentaion fault on `invoke()` when run via [tflite c++ minimal example](https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_c)
  * Invokes fine without `Conv2D` layer

```c++
#include ""tensorflow/lite/interpreter.h""
#include ""tensorflow/lite/kernels/register.h""
#include ""tensorflow/lite/model.h""
#include ""tensorflow/lite/optional_debug_tools.h""
#include <cstdio>

using namespace tflite;

#define TFLITE_MINIMAL_CHECK(x)                                                \
  if (!(x)) {                                                                  \
    fprintf(stderr, ""Error at %s:%d\n"", __FILE__, __LINE__);                   \
    exit(1);                                                                   \
  }

int main(int argc, char *argv[]) {
  if (argc != 2) {
    fprintf(stderr, ""minimal <tflite model>\n"");
    return 1;
  }
  const char *filename = argv[1];

  // Load model
  std::unique_ptr<tflite::FlatBufferModel> model =
      tflite::FlatBufferModel::BuildFromFile(filename);
  TFLITE_MINIMAL_CHECK(model != nullptr);

  // Build the interpreter
  tflite::ops::builtin::BuiltinOpResolver resolver;
  InterpreterBuilder builder(*model, resolver);
  std::unique_ptr<Interpreter> interpreter;
  builder(&interpreter);
  TFLITE_MINIMAL_CHECK(interpreter != nullptr);

  // Allocate tensor buffers.
  TFLITE_MINIMAL_CHECK(interpreter->AllocateTensors() == kTfLiteOk);
  printf(""=== Pre-invoke Interpreter State ===\n"");
  tflite::PrintInterpreterState(interpreter.get());

  // Run inference
  TFLITE_MINIMAL_CHECK(interpreter->Invoke() == kTfLiteOk); // *** Fails here before return
  printf(""\n\n=== Post-invoke Interpreter State ===\n"");
  tflite::PrintInterpreterState(interpreter.get());

  return 0;
}
```

Called via `minimal min_model.tflite`.


"
41777,Adagrad colocation fitting error,"

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.2.0
- Python version: 3.6.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.0
- GPU model and memory: Nvidia Quadro V100




**Describe the current behavior**
Compilation and fitting of a network that uses tf.keras.layers.DenseFeatures and an Adagrad optimizer results in a co-location error that does not resolve when allowing soft placement and/or turning off XLA JIT. Changing the optimizer to SGD resolves the issue.

**Describe the expected behavior**
Adagrad should not incite a colocation error at model fitting when used with DenseFeatures.


**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```python
import tensorflow as tf
from tensorflow.python.feature_column import feature_column_v2 as fc
metadata = {'m1': tf.ones(shape=(100,1)), 'm2': tf.ones(shape=(100,1)),'label':tf.ones(shape=(100,1))}
num_samples = 100
dnn_optimizer = tf.keras.optimizers.Adagrad(
                learning_rate=1
            )
def meta_dict_gen():
    for i in range(num_samples):
        ls = {}
        for key, val in metadata.items():
            ls[key] = val[i]
        yield ls
# DATASET CREATION
d = tf.data.Dataset.from_generator(
    meta_dict_gen,
    output_types={k: tf.float32 for k in metadata},
    output_shapes={'m1': (1,), 'm2': (1),'label':(1)})
d = d.shuffle(
        buffer_size=10 * 8
    )


features = {'m1':1,'m2':1}
def label_map(d):
    
    label = d.pop('label')
    reshaped_label = tf.reshape(label, [-1, label.shape[-1]])
    reshaped_elem = {
        key: tf.reshape(d[key], [-1, d[key].shape[-1]])
        for key in d if key in features.keys()
    }
    
    return reshaped_elem, reshaped_label
d = d.map(map_func=label_map)

# CREATING DENSE FEATURE LAYER
d_columns = [tf.feature_column.embedding_column(fc.categorical_column_with_hash_bucket(key='m1', hash_bucket_size=2, dtype=tf.int64),dimension=1,combiner='mean'),
tf.feature_column.numeric_column(
                    'm2', shape=(1,))]

d_features = {}
d_features['m1'] = tf.keras.Input(shape=(1,), name='m1', dtype=tf.int64, sparse=False)
d_features['m2'] = tf.keras.Input(shape=(1,), name='m2', dtype=tf.int64, sparse=False)


#CREATING MODEL

d_input = tf.keras.layers.DenseFeatures(d_columns, name='d_embedded')(d_features)
d_output = tf.keras.layers.Dense(1)(d_input)
d_model = tf.keras.Model(d_features,d_output)
d_model.compile()
d_model.compile(optimizer = dnn_optimizer,loss= 'binary_crossentropy',metrics = ['binary_crossentropy'])
d_model.fit(d)
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.


@DEKHTIARJonathan @nluehr 

"
41774,why can i not install tensorflow library at python 3.8?,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:2.2
- Python version:3.8
- Installed using virtualenv? pip? conda?:pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)
ERROR: No matching distribution found for tensorflow

**Provide the exact sequence of commands / steps that you executed before running into the problem**
pip install tensorflow==2.2
pip install tensorflow

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
41773,"MirroredStrategy with train_on_batch: Method requires being in cross-replica context, use get_replica_context().merge_call()","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.4.0-dev20200727
- Python version: 3.8.3
- CUDA/cuDNN version: 10.2
- GPU model and memory: 2x GTX 1060 6Go

**Describe the current behavior**
When using **MirroredStrategy** with **train_on_batch**, I get the error that the **Method requires being in cross-replica context**
I have the same behavior with TF2.2, 2.3

The code is taken from issue #39270, and I added the wrapper `strategy.run`

**Describe the expected behavior**
To have the workload distributed across 2 GPUs

**Standalone code to reproduce the issue**
```
import tensorflow as tf

strategy = tf.distribute.MirroredStrategy()

with strategy.scope():
    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
    x_train, x_test = x_train / 255.0, x_test / 255.0
    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)

    model = tf.keras.models.Sequential([
        tf.keras.layers.Flatten(input_shape=(28, 28)),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(10),
    ])

    model.compile(
        optimizer=tf.keras.optimizers.SGD(),
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=['accuracy'])

def train_step(x, y):
    model.train_on_batch(x, y)
    
for x, y in train_dataset:
    strategy.run(train_step, args=(x, y,))
```


**Other info / logs** 
```
~/anaconda3/envs/distributed/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py in __enter__(self)
    385     # Allow this scope to be entered if this strategy is already in scope.
    386     if distribution_strategy_context.has_strategy():
--> 387       _require_cross_replica_or_default_context_extended(
    388           self._context.strategy.extended)
    389       self._same_scope_again_count += 1

~/anaconda3/envs/distributed/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py in _require_cross_replica_or_default_context_extended(extended)
    309     _wrong_strategy_scope(strategy, context)
    310   assert cross_replica is None
--> 311   raise RuntimeError(""Method requires being in cross-replica context, use ""
    312                      ""get_replica_context().merge_call()"")
    313 

RuntimeError: Method requires being in cross-replica context, use get_replica_context().merge_call()
```

[error.log](https://github.com/tensorflow/tensorflow/files/4983379/error.log)
"
41772,SavedModel with dictionary/list data member,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): tf-nightly
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**

Currently, the SavedModel format doesn't support exporting a customized model with python dict/list (no?) in the graph, possibly due to some restrictions on tf.function tracing. For instance, all models from the object_detection API of model garden have the following workflow:

```python
# training
# save label_dict (key:string, val:tensor) to model.label_dict (a python dict)
model.provide_groundtruth(label_dict)
predict_dict = model.predict(images)
# compute loss based on model.label_dict
loss = model.loss(predict_dict)
```

All involved `dict`s have the same structure, e.g., same key values and value shapes.

Meanwhile, even the feature is not supported now, the current error message is not intuitive. A similar usecase can be found [here](https://colab.research.google.com/drive/1gkG3M_Q4L0J4nmCXtYCyneqTnTe3M49V?usp=sharing).


**Will this change the current api? How?**

**Who will benefit with this feature?**

**Any Other info.**
"
41771,can we convert back tflite model to keras h5 format?,"hi there, thanks for this powerful tool.
can we convert back tflite model to keras h5 format?
i find a solution [here](https://stackoverflow.com/questions/59559289/is-there-any-way-to-convert-the-tflite-file-back-to-keash5), but the weight in bn layer in tflite is null, how can we convert back this?"
41770,Build TFLite with Flex Ops without Bazel ,"Tensorflow Version: `2.4.0-dev20200712`
OS: Ubuntu 18.10 
(tensorflow/tensorflow docker)

Hello

Followed these instructions (C++ section) in the past to build a shared libtensorflowlite.so library
https://www.tensorflow.org/lite/guide/ops_select

```
Add the TensorFlow ops delegate library dependency to the build dependencies: tensorflow/lite/delegates/flex:delegate.
```

Problem is now I'm running into compatibility issues when I upload to the target device..

Few questions:
(1) How can I build a static library using the bazel pipeline? I used the `--config monolithic` flag but upon linking it still requires a newer glibc than my system supports
```bazel build --config=elinux_aarch64 --cpu=aarch64 --config=monolithic --cxxopt=--std=c++14 --define=with_select_tf_ops=true -c opt //tensorflow/lite:libtensorflowlite.so```

(2) How can I add the same flex op dependency to the makefile in lite/tools in order to build the lib without Bazel?


"
41769,Simple TFLite UNet slower on mobile GPU than CPU,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution: Linux Ubuntu 18.04, Android 8.0.0
- Mobile device if the issue happens on mobile device: Huawei P20 Lite (ANE-LX1)
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b
- Python version: 3.6.9
- CUDA/cuDNN version: CUDA 10.1
- GPU model: Mali-T830 MP2

**Describe the current behavior**

I have written a simple image segmentation model based on UNet in Keras, which I want to use on an Android device (Huawei P20 Lite). It consists solely of Conv2D, Conv2DTranspose, and Concatenate layers, which should all be supported on mobile GPUs by TFLite according to the [documentation](https://www.tensorflow.org/lite/performance/gpu_advanced#supported_ops). The model has ~1.9M parameters and expects input tensors of shape [1, 224, 224, 3] and type float32. It is not trained yet, as I want to check its performance first before addressing the accuracy. I exported the model using

```
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
with tf.io.gfile.GFile('trained models/keras_unet.tflite', 'wb') as f:
  f.write(tflite_model)
```
You can find the exported TFLite model attached further down.

On the CPU of my device, inference takes about 1.3s on average using 6 threads. However, when employing the GPU delegate using

```
tfliteModel = FileUtil.loadMappedFile(this, MODEL_NAME_TF);
GpuDelegate gpu_del = new GpuDelegate();
tfliteOptions.addDelegate(gpu_del);
tflite = new Interpreter(tfliteModel, tfliteOptions);
```

the performance drops to about 3.5s per inference, which is more than 2.5 times slower compared to CPU. 

**Describe the expected behavior**
As the UNet model only consists of supported operations, the mobile GPU supports OpenGL ES 3.2, and model export and import as well as GPU delegate creation have been done as suggested in the [TFLite Android Tutorial](https://codelabs.developers.google.com/codelabs/recognize-flowers-with-tensorflow-on-android/#0) and the [GPU delegate Guide](https://www.tensorflow.org/lite/performance/gpu#trying_the_gpu_delegate_on_your_own_model), I would expect the UNet to infer faster on the GPU than on the CPU.

I would be glad if you could point out any implementation issues on my side that could potentially cause the comparably worse performance on the GPU. The guides and tutorials unfortunately do not offer any deeper insight. Is it possible that the model is simply too large for my GPU (and would there be an error if this was the case)?

**Standalone code to reproduce the issue**
The following model exhibits the described behavior: [tflite_model.zip](https://github.com/tensorflow/tensorflow/files/4982040/tflite_model.zip)


**Other info / logs**
I have previously implemented the exact same model in PyTorch, the performance on the CPU is about the same. I then switched to Keras to utilize TFLite's GPU delegate. I used a subclassed approach to develop the UNet-like model in Keras.

I have read [here](https://www.tensorflow.org/lite/performance/gpu#tips_for_optimization) that it might be beneficial to use a Tensor with c=4 instead to avoid memory copies. I tried adding a fourth channel to the model and the input images (shape [1, 224, 224, 4]), which made performance even worse.

The performance has been tested using a custom Android application. The timings mentioned above refer to the following call:
`tflite.run(inputTensorBuffer.getBuffer(), outputProbabilityBuffer.getBuffer().rewind());`
I also attempted to profile the TFLite model using the [TFLite Model Benchmark Tool](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark), but the Tool failed to build correctly using Bazel. If this is of relevance for solving the problem at hand, I will file another issue for that.

Finally, here are the outputs produced by the Python script for exporting the model and the Android test application:
[python_out.txt](https://github.com/tensorflow/tensorflow/files/4982006/python_out.txt)
[android_out.txt](https://github.com/tensorflow/tensorflow/files/4982010/android_out.txt)"
41768,[TF 2.2.0] Error on creating optimizer slot variable under multi GPU training with tf.distribute.MirroredStrategy,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution: Red Hat Enterprise Linux Server release 7.6 (Maipo)
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): tensorflow/2.2.0--cuda--10.1
- Python version: 3.8.2
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): [GCC 4.8.5 20150623 (Red Hat 4.8.5-36)]
- CUDA/cuDNN version: 10.1
- GPU model and memory: 4x Tesla V100-SXM2-16GB


**Describe the current behavior**
I am using Tensorflow 2.2.0 GPU with Tensorflow's Keras API for training a GAN model with multi GPU using Distributed Tensorflow MirroredStrategy. Under the mirrored strategy scope, I've:
- instantiated the model;
- defined the optimizer (tf.keras.optimizer.RMSProp);
- compiled the model with that optimizer.

In the train_step function I've defined the computational graph in Pythonic-way. I'm using Lazy mode (as opposed to the default TF2.0 eager mode and Keras graph mode) because TF warned me that running MirroredStrategy with eager execution still has overhead problems.
When the execution arrives to the apply_gradients call on the optimizer of my model, TF crashes reporting me this error:

`    ValueError: Trying to create optimizer slot variable under the scope for tf.distribute.Strategy (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x20079d15d4f0>), which is different from the scope used for the original variable (MirroredVariable:{
      0: <tf.Variable 'conv2d_20/kernel:0' shape=(3, 3, 2, 2) dtype=float32>,
      1: <tf.Variable 'conv2d_20/kernel/replica_1:0' shape=(3, 3, 2, 2) dtype=float32>,
      2: <tf.Variable 'conv2d_20/kernel/replica_2:0' shape=(3, 3, 2, 2) dtype=float32>,
      3: <tf.Variable 'conv2d_20/kernel/replica_3:0' shape=(3, 3, 2, 2) dtype=float32>
    }). Make sure the slot variables are created under the same strategy scope. This may happen if you're restoring from a checkpoint outside the scope
`

**Describe the expected behavior**
The apply gradient should execute fine, because I've correctly defined the model and the optimizer under the MirroredStrategy object, as reported in: https://www.tensorflow.org/guide/distributed_training

**Standalone code to reproduce the issue**
https://pastebin.com/MyT6p1T8

**Other info / logs**
"
41767,quantize deeplab model trained on PASCAL VOC，TypeError: unsupported operand type(s) for +: 'NoneType' and 'float',"When I change quantize_delay_step=-1 to quantize_delay_step=0, I run the following command, 
python deeplab/train.py \
    --logtostderr \
    --training_number_of_steps=3000 \
    --train_split=""train"" \
    --model_variant=""mobilenet_v2"" \
    --output_stride=16 \
    --train_crop_size=""513,513"" \
    --train_batch_size=8 \
    --base_learning_rate=3e-5 \
    --dataset=""pascal_voc_seg"" \
    --quantize_delay_step=0 \
I get an error：
Traceback (most recent call last):
  File ""train.py"", line 464, in <module>
    tf.app.run()
  File ""/home/sherry/.local/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/home/sherry/.local/lib/python3.6/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/home/sherry/.local/lib/python3.6/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""train.py"", line 391, in main
    quant_delay=FLAGS.quantize_delay_step)
  File ""/home/sherry/.local/lib/python3.6/site-packages/tensorflow/contrib/quantize/python/quantize_graph.py"", line 122, in create_training_graph
    freeze_bn_delay=freeze_bn_delay)
  File ""/home/sherry/.local/lib/python3.6/site-packages/tensorflow/contrib/quantize/python/quantize_graph.py"", line 73, in _create_graph
    is_training=is_training)
  File ""/home/sherry/.local/lib/python3.6/site-packages/tensorflow/contrib/quantize/python/fold_batch_norms.py"", line 53, in FoldBatchNorms
    graph, is_training, freeze_batch_norm_delay=freeze_batch_norm_delay)
  File ""/home/sherry/.local/lib/python3.6/site-packages/tensorflow/contrib/quantize/python/fold_batch_norms.py"", line 98, in _FoldFusedBatchNorms
    freeze_batch_norm_delay=freeze_batch_norm_delay))
  File ""/home/sherry/.local/lib/python3.6/site-packages/tensorflow/contrib/quantize/python/fold_batch_norms.py"", line 384, in _ComputeBatchNormCorrections
    match.moving_variance_tensor + match.batch_epsilon)
TypeError: unsupported operand type(s) for +: 'NoneType' and 'float'
"
41766,TypeError: '<' not supported between instances of 'NoneType' and 'int',"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
![pye1](https://user-images.githubusercontent.com/66160761/88523988-69234b00-d016-11ea-9c74-0b00bb25d371.PNG)
"
41765,Build windows x86 c++ library failed,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):   Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  NA
- TensorFlow installed from (source or binary):  source
- TensorFlow version:    r2.2
- Python version:  3.7.6
- Installed using virtualenv? pip? conda?:   conda 
- Bazel version (if compiling from source):    2.0.0
- GCC/Compiler version (if compiling from source):  vs2019 
- CUDA/cuDNN version:   NA
- GPU model and memory:   NA


**Describe the problem**
I build tensorflow r2.2 branch, I use this commands
> bazel build -c opt  --config=opt --local_ram_resources=2048  //tensorflow/tools/lib_package:libtensorflow
bazel build -c opt //tensorflow/lite:tensorflowlite

this is right, but this is x64 lib. I want to build x86 lib, then I use:
> bazel build --cpu=x86 -c opt  --config=opt --local_ram_resources=2048  //tensorflow/tools/lib_package:libtensorflow

but get tolchain does not contain a toolchain for cpu 'x86'.

> D:\IdeaProjects\tensorflow>bazel build --cpu=x86 -c opt  --config=opt --local_ram_resources=2048  //tensorflow/tools/lib_package:libtensorflow
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=168
INFO: Reading rc options for 'build' from d:\ideaprojects\tensorflow\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=C:/ProgramData/Anaconda3/python.exe
INFO: Reading rc options for 'build' from d:\ideaprojects\tensorflow\.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2
INFO: Reading rc options for 'build' from d:\ideaprojects\tensorflow\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=C:/ProgramData/Anaconda3/python.exe --action_env PYTHON_LIB_PATH=C:/ProgramData/Anaconda3/lib/site-packages --python_path=C:/ProgramData/Anaconda3/python.exe --config=xla --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:v2 in file d:\ideaprojects\tensorflow\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:xla in file d:\ideaprojects\tensorflow\.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true
INFO: Found applicable config definition build:opt in file d:\ideaprojects\tensorflow\.tf_configure.bazelrc: --copt=/arch:AVX --define with_default_optimizations=true
INFO: Found applicable config definition build:windows in file d:\ideaprojects\tensorflow\.bazelrc: --copt=/w --copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file d:\ideaprojects\tensorflow\.bazelrc: --define framework_shared_object=false
INFO: Build options --copt and --define have changed, discarding analysis cache.
ERROR: C:/users/administrator/_bazel_administrator/rqkipcky/external/local_config_cc/BUILD:47:1: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'x86'
ERROR: Analysis of target '//tensorflow/tools/lib_package:libtensorflow' failed; build aborted: Analysis of target '@local_config_cc//:toolchain' failed; build aborted
INFO: Elapsed time: 1.693s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (11 packages loaded, 118 targets configured)
    Fetching @nsync; fetching

"
41764,Run benchmark models get hanged when use grpc+gdr communication.,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: 
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): r1.15
- Python version: python3.6
- Bazel version (if compiling from source): 0.26.0
- GCC/Compiler version (if compiling from source): 7.5.0
- CUDA/cuDNN version: CUDA10.1
- GPU model and memory: V100 32GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
When I compile tensorflow r1.15 version from source code with the gdr support. The compile command line is as following.
`bazel build --config=opt --config=cuda --config=gdr //tensorflow/tools/pip_package:build_pip_package`
After I run the benchmark models, I get the process hang with the following output.

> WARNING:tensorflow:From /home/cengguang/zcg/lib/python3.6/site-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
2020-07-27 15:27:24.230627: I tensorflow/contrib/gdr/gdr_memory_manager.cc:220] RDMA server is listening on 10.0.9.2:10001
2020-07-27 15:27:24.230719: I tensorflow/contrib/gdr/gdr_memory_manager.cc:86] NUMA node for device: mlx5_0 is 0
2020-07-27 15:27:24.230763: I tensorflow/contrib/gdr/gdr_memory_manager.cc:250] Instrumenting CPU allocator(s)
2020-07-27 15:27:24.245919: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2020-07-27 15:27:24.246089: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5f6a630 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-07-27 15:27:24.246112: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-07-27 15:27:24.248568: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-07-27 15:27:26.772045: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5f72cc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-07-27 15:27:26.772110: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2020-07-27 15:27:26.776200: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Tesla V100-SXM2-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:89:00.0
2020-07-27 15:27:26.776621: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-07-27 15:27:26.780467: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-07-27 15:27:26.784188: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-07-27 15:27:26.784806: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-07-27 15:27:26.787654: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-07-27 15:27:26.788934: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-07-27 15:27:26.793107: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-27 15:27:26.795799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2020-07-27 15:27:26.795862: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-07-27 15:27:26.798030: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-27 15:27:26.798050: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0
2020-07-27 15:27:26.798057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N
2020-07-27 15:27:26.800718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:worker/replica:0/task:1/device:GPU:0 with 30584 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:89:00.0, compute capability: 7.0)
2020-07-27 15:27:26.806113: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:258] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.9.2:10009, 1 -> localhost:10001}
begin a gdr server
2020-07-27 15:27:26.810425: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:10001
WARNING:tensorflow:From /home/cengguang/benchmarks/scripts/tf_cnn_benchmarks/convnet_builder.py:134: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0727 15:27:26.848110 139800816277312 deprecation.py:323] From /home/cengguang/benchmarks/scripts/tf_cnn_benchmarks/convnet_builder.py:134: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
WARNING:tensorflow:From /home/cengguang/zcg/lib/python3.6/site-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
W0727 15:27:26.850684 139800816277312 deprecation.py:323] From /home/cengguang/zcg/lib/python3.6/site-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
WARNING:tensorflow:From /home/cengguang/benchmarks/scripts/tf_cnn_benchmarks/convnet_builder.py:266: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling2D instead.
W0727 15:27:26.874744 139800816277312 deprecation.py:323] From /home/cengguang/benchmarks/scripts/tf_cnn_benchmarks/convnet_builder.py:266: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling2D instead.
WARNING:tensorflow:From /home/cengguang/benchmarks/scripts/tf_cnn_benchmarks/convnet_builder.py:408: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dropout instead.
W0727 15:27:26.954734 139800816277312 deprecation.py:323] From /home/cengguang/benchmarks/scripts/tf_cnn_benchmarks/convnet_builder.py:408: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dropout instead.
WARNING:tensorflow:From /home/cengguang/zcg/lib/python3.6/site-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
W0727 15:27:27.005284 139800816277312 deprecation.py:323] From /home/cengguang/zcg/lib/python3.6/site-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /home/cengguang/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py:2268: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.MonitoredTrainingSession
W0727 15:27:27.223294 139800816277312 deprecation.py:323] From /home/cengguang/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py:2268: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.MonitoredTrainingSession
INFO:tensorflow:Running local_init_op.
I0727 15:27:28.615225 139800816277312 session_manager.py:500] Running local_init_op.
2020-07-27 15:27:28.770876: I tensorflow/contrib/gdr/gdr_memory_manager.cc:572] RDMA endpoint connected to rdma://10.0.9.2:10009
INFO:tensorflow:Done running local_init_op.
I0727 15:27:29.256539 139800816277312 session_manager.py:502] Done running local_init_op.
2020-07-27 15:27:29.479763: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-07-27 15:27:29.801486: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-27 15:27:31.455157: I tensorflow/contrib/gdr/gdr_memory_manager.cc:279] Accepted new RDMA connection

**Describe the expected behavior**
Expected behavior should run the code smoothly and exit, BUT get stucked.
**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
After compile the source code with gdr support, you can use the following launch scripts:
`export CUDA_VISIBLE_DEVICES='0'
export LD_LIBRARY_PATH=/usr/local/cuda/lib64/
python tf_cnn_benchmarks.py \
        --controller_host=controller_ip \
        --worker_hosts=worker1_ip,worker2_ip\
        --variable_update=collective_all_reduce \
        --all_reduce_spec=collective \
        --job_name=worker \
        --use_fp16 \
        --batch_size=128 \
        --num_gpus=1 \
        --model=alexnet \
        --task_index=0 \
        --force_gpu_compatible \
        --server_protocol=grpc+gdr 2>&1| tee output1.txt`
**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
41763,Please list XLA flags,"## URL(s) with the issue:

https://www.tensorflow.org/xla

## Description of issue (what needs changing):

### Clear description

It's hard to find a list of XLA environment variables / flags, and examples of how/when to use them, like: 

```
os.environ[""XLA_PYTHON_CLIENT_PREALLOCATE""] = ""false""
os.environ[""XLA_PYTHON_CLIENT_ALLOCATOR""] = ""platform""
```

### Usage example

Usage examples are buried across many different open/closed github issues

Desired usage:

```
def run_agent_on_env(agent, env_name):
  env = gym.make(env_name)
  obs = env.reset()
  action = agent(obs) # how do you make this the only thing to run on XLA?
  obs, reward, done, info = env.step(action)
  # ... loop ... 
  return total_reward

def main():
  agents = get_agents()
  with futures.ProcessPoolExecutor() as pool:
      jobs = [pool.submit(run_agent_on_env, (agent, name)) for agent, name in ...]
      for future in futures.as_completed(jobs):
           # optimize stuff
           # log stuff
           # submit more jobs
```
Problem is, XLA runs OOM

### Request visuals, if applicable

XLA configuration often makes the difference between running, and not running code, given memory issues etc. A simple table of XLA environment variables / ""flags"" would be hugely useful for users 

### Submit a pull request?

I can't"
41761,XLA_CPU_JIT unsupports  NonMaxSuppressionV2 ?,"
- Model: mtcnn downloaded [here](https://github.com/blaueck/tf-mtcnn)
- OS: Linux WSL 4.4.0-19041-Microsoft
- TensorFlow: Source code [@a50de3e](https://github.com/tensorflow/tensorflow/tree/a50de3e381ccb6d0b37a0bfe5d033cf312b53955)
- Python version: 2.7
- Bazel version: 0.11.0 (binary installer installed)

After hacking according to [this repo](https://github.com/nuchi/tf-to-xla-to-wasm)

cmd
```bash

bazel-bin/tensorflow/compiler/aot/tfcompile \
    --target_triple=""wasm32-unknown-unknown-wasm"" \
    --target_cpu=""generic"" \
    --xla_cpu_multi_thread_eigen=false \
    --graph=""./mtcnn.pbtxt"" \
    --config=""./mtcnn_config.pbtxt"" \
    --out_function_object=""out_model.o"" \
    --out_header=""out_header.h"" \
    --out_metadata_object=""out_helper.o"" \
    --cpp_class=""MyClass""
```

log
``` bash
INVALID ARGUMENTS: Detected unsupported operations when trying to compile graph tfcompile on XLA_CPU_JIT:NonMaxSuppressionV2
```


"
41760,Can't change model attribute inside @tf.function with custom training loop.,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2.0
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
@tf.function with custom training loop can't change the attribute of the model. For example, after 10k steps, I want to set some layer to non-trainable by `model.layer[0].trainable = False`, or in case training GAN, after 10k steps, the discriminator start to train. 

**Describe the expected behavior**
Without @tf.function everything is fine but the training will slow compared with graph mode.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Here is a simple example that I take from custom_training_walkthough colab and modify a bit. (https://colab.research.google.com/drive/1Rv0YTBq6qISvt-EFebYr4U_RUdjdSWDe?usp=sharing). 

The main code I changed is here: 
```
    if steps == 1:
      print(""trainable=True"")
      model.layer0.trainable = True
      model.layer1.trainable = True
      model.layer2.trainable = True

    if steps == 100:
      print(""trainable=False"")
      model.layer0.trainable = False
      model.layer1.trainable = False
      model.layer2.trainable = False  
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
```
ValueError: tf.function-decorated function tried to create variables on non-first call.
```

I also have an example in training GAN where I enable discriminator training after N steps but nothing happened, just like the code inside the if condition is not run. See (https://github.com/TensorSpeech/TensorflowTTS/blob/master/examples/multiband_melgan/train_multiband_melgan.py#L157-L166). The workaround is that I MUST to training generator with N steps then resume to training both G and D. "
41759,GPU memory issue with triton server 20.03 ,"I know Tf uses Best-Fit with Coalescing to deal GPU memory. 
Some oom issues always happen when I inferences lots of models at the same time. So I just take some tests to check the issue. 

I have two models A/B trained by tenforflow1.13 and I inference by  triton server 20.03.

When I only load A model with warm up in triton sever, I find GPU memory is 3G.
When I only load B model with warm up in triton sever, I find GPU memory is 4G.
When I load A model and B model with warm up at the same time in triton sever, I find GPU memory is 4.1G. Why not 7G?

If I inference A/B at the same time, should GPU memory increase from 4.1G to 7G?


"
41758,Encountered unresolved custom op: Slice.,"I have converted pb to tflite,When I transplant algorithms,it appears the following error:
java.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: Encountered unresolved custom op: Slice.
    Node number 0 (Slice) failed to prepare.
please help me."
41748,Tensorflow build from source doesn’t see GPUs gets “ failed call to cuInit: UNKNOWN ERROR (-1)”,"System information:
Tensorflow 2.4.0 (built from tensorflow/tensorflow:develgpu and then followed Build from source on 24Jul2020)
Nvidia-cuda 11.0
Nvidia-driver-450

This from nvidia-smi 

nvidia-smi
Sun Jul 26 20:50:35 2020       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.57       Driver Version: 450.57       CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  GeForce GTX 1070    On   | 00000000:01:00.0 Off |                  N/A |
|  0%   28C    P8     4W / 166W |    191MiB /  8116MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GTX 1070    On   | 00000000:05:00.0 Off |                  N/A |
|  0%   29C    P8     5W / 166W |    150MiB /  8119MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  GeForce GTX 107...  On   | 00000000:06:00.0 Off |                  N/A |
|  0%   34C    P8     5W / 180W |    168MiB /  8119MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1010      G   /usr/lib/xorg/Xorg                 36MiB |
|    0   N/A  N/A      1708      G   /usr/bin/gnome-shell                9MiB |
|    0   N/A  N/A      4316      C   nvidia-cuda-mps-server            141MiB |
|    1   N/A  N/A      1010      G   /usr/lib/xorg/Xorg                  4MiB |
|    1   N/A  N/A      4316      C   nvidia-cuda-mps-server            141MiB |
|    2   N/A  N/A      1010      G   /usr/lib/xorg/Xorg                  4MiB |
|    2   N/A  N/A      4316      C   nvidia-cuda-mps-server            159MiB |

I got beyond the error above when I changed the LD_LIBRARY_PATH using:

export LD_LIBRARY_PATH=“/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/include/x64_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64” 

This deleted the stubs library. 

And then I started to get CUDA_ERROR_MAP_FAILED
And in /var/log/nvidia-mps/control.log I see: 

User did not send valid credentials

looking up this error. Someone suggested adding —ipc=“host” to the docker run command and that fixed the problem for me. 

So what your team should do is document that when build from source and trying to execute under that container the “stubs” library needs to be deleted from the LD_LIBRARY_PATH 
AND
the container needs to be run with —ipc=“host” on the docker run command.



<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
41746,Autograph fails to convert nested **if-else** in a for loop,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2.0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 
- GPU model and memory: P100


**Describe the current behavior**

Unexpected behavior of `tf.function` decorator. The decorator fails to convert `if-else` inside a loop and because of this behavior, it can't be used standalone or inside `tf.keras` `train_step` function
 

**Standalone code to reproduce the issue**
```python
import tensorflow as tf
import numpy as np

class Sampler:
    def __init__(self, sample_size=10):
        self.sample_size = tf.Variable(sample_size, dtype=tf.int32)
        self.samples = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)
     
    @tf.function
    def get_new_samples(self, data):
        size = tf.shape(data)[0]
        new_samples = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)
        
        for i in range(size):
            if self.samples.size() < self.sample_size:
                self.samples.write(i, data[i,:])
                new_samples.write(i, data[i, :])
            else:
                if (tf.random.uniform([1]) > 0.5):
                    idx = np.random.randint(0, size)
                    new_sample = self.samples.read(idx)
                    self.samples.write(idx, data[i, :])
                    new_samples.write(i, new_sample)
                else:
                    new_samples.write(i, data[i, :])
        return new_samples.stack()
        
    
    def __call__(self, data):
        return tf.cond(tf.equal(self.sample_size, 0),
                      true_fn=lambda: data,
                      false_fn=self.get_new_samples(data))

s = Sampler()
s(tf.convert_to_tensor(np.random.rand(5, 3).astype(np.float32)))
```

https://colab.research.google.com/drive/1Tf1Pj-_HzpUC8CMG8gS4U2nGV0EPxVd7?usp=sharing

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```python
OperatorNotAllowedInGraphError: in user code:

    <ipython-input-2-b3cdab60c185>:13 get_new_samples  *
        self.samples.write(i, data[i,:])
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py:235 wrapped  **
        return _add_should_use_warning(fn(*args, **kwargs),
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/tensor_array_ops.py:1159 write
        return self._implementation.write(index, value, name=name)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/tensor_array_ops.py:833 write
        self._write(index, value)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/tensor_array_ops.py:796 _write
        if index < 0:
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:778 __bool__
        self._disallow_bool_casting()
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:545 _disallow_bool_casting
        ""using a `tf.Tensor` as a Python `bool`"")
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:532 _disallow_when_autograph_enabled
        "" decorating it directly with @tf.function."".format(task))

    OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.


```
"
41744,Question mark (?) in the model image using plot_model instead of None,"I am using the tf.keras.utils.plot_model function to plot the model. The code is given below. Now instead of getting None for the batch sizes I am getting question mark (?). So can you please address why this is happening and how I can replace None inplace of ? question mark.

tf.keras.utils.plot_model(
    model,
    to_file='model2.png',
    show_shapes=True,
    show_layer_names=True
)

![model image](https://user-images.githubusercontent.com/50369708/88482269-d9ca5900-cf7d-11ea-8a0b-7150d4b056e1.png)
"
41743,Building tensorflow 2.2 fails,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.2
- Python version: 3.8
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): 2.0.0
- GCC/Compiler version (if compiling from source): 7.5
- CUDA/cuDNN version: 10.1/7.6
- GPU model and memory: GeForce GTX 1080 8111MiB



**Describe the problem**

Every time I use the ```bazel``` command to build tensorflow, this error appears:

```
ERROR: /home/amir/Documents/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2734:1: output 'tensorflow/core/kernels/_objs/eye_functor_gpu/eye_functor_gpu.cu.pic.o' was not created
```

or something like above that includes ```_gpu.cu.pic.o```.
I have built previous versions of tensorflow many times including tensorflow 1.9-1.13 with previous versions of cuda without problem

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
bazel build --config=opt --config=mkl //tensorflow/tools/pip_package:build_pip_package
```


**Any other info / logs**

Here is the .tf_configure_bazelrc configurations:

```
build --action_env PYTHON_BIN_PATH=""/home/amir/Documents/tensorflow/py-env/bin/python""
build --action_env PYTHON_LIB_PATH=""/home/amir/Documents/tensorflow/py-env/lib/python3.8/site-packages""
build --python_path=""/home/amir/Documents/tensorflow/py-env/bin/python""
build --config=xla
build --action_env TF_CUDA_VERSION=""10.1""
build --action_env TF_CUDNN_VERSION=""7.6""
build --action_env TF_NCCL_VERSION=""2.7""
build --action_env TF_CUDA_PATHS=""/usr/local/cuda/cuda-10.1/extras/CUPTI,/usr/local/cuda/cuda-10.1/nccl/nccl-2.7,/usr/local/cuda/cuda-10.1/cudnn/cudnn-7.6,/usr/local/cuda/cuda-10.1""
build --action_env CUDA_TOOLKIT_PATH=""/usr/local/cuda/cuda-10.1""
build --action_env TF_CUDA_COMPUTE_CAPABILITIES=""6.1""
build --action_env LD_LIBRARY_PATH=""/usr/local/cuda/cuda-10.1/extras/CUPTI/lib64:/usr/local/cuda/cuda-10.1/nccl/nccl-2.7/lib:/usr/local/cuda/cuda-10.1/cudnn/cudnn-7.6/lib64:/usr/local/cuda/cuda-10.1/lib64""
build --action_env GCC_HOST_COMPILER_PATH=""/usr/bin/x86_64-linux-gnu-gcc-7""
build --config=cuda
build:opt --copt=-march=native
build:opt --copt=-Wno-sign-compare
build:opt --host_copt=-march=native
build:opt --define with_default_optimizations=true
test --flaky_test_attempts=3
test --test_size_filters=small,medium
test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial
test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu
test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only
test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only
build --action_env TF_CONFIGURE_IOS=""0""
```
"
41741,No object detection for panoramic photos Tensorflow,"Good day. I ask for your help.

I tried almost all the models, but I didn't get the desired result.

TF finds objects but only at close proximity, if the photo is panoramic, then the detection does not work or works extremely poorly 
![Аннотация 2020-07-26 090233](https://user-images.githubusercontent.com/32834586/88474754-59323b00-cf32-11ea-87bc-007034ddb758.png)
![Аннотация 2020-07-26 090334](https://user-images.githubusercontent.com/32834586/88474756-5afbfe80-cf32-11ea-905c-89420a937b48.png)
The marked images contain both panoramic and close-up photographs. About the same amount

```model {
  faster_rcnn {
    num_classes: 12
    image_resizer {
      keep_aspect_ratio_resizer {
        min_dimension: 600
        max_dimension: 1024
      }
    }
    feature_extractor {
      type: ""faster_rcnn_resnet50""
      first_stage_features_stride: 16
    }
    first_stage_anchor_generator {
      grid_anchor_generator {
        height_stride: 16
        width_stride: 16
        scales: 0.25
        scales: 0.5
        scales: 1.0
        scales: 2.0
        aspect_ratios: 0.5
        aspect_ratios: 1.0
        aspect_ratios: 2.0
      }
    }
    first_stage_box_predictor_conv_hyperparams {
      op: CONV
      regularizer {
        l2_regularizer {
          weight: 0.0
        }
      }
      initializer {
        truncated_normal_initializer {
          stddev: 0.00999999977648
        }
      }
    }
    first_stage_nms_score_threshold: 0.0
    first_stage_nms_iou_threshold: 0.699999988079
    first_stage_max_proposals: 50
    first_stage_localization_loss_weight: 2.0
    first_stage_objectness_loss_weight: 1.0
    initial_crop_size: 14
    maxpool_kernel_size: 2
    maxpool_stride: 2
    second_stage_box_predictor {
      mask_rcnn_box_predictor {
        fc_hyperparams {
          op: FC
          regularizer {
            l2_regularizer {
              weight: 0.0
            }
          }
          initializer {
            variance_scaling_initializer {
              factor: 1.0
              uniform: true
              mode: FAN_AVG
            }
          }
        }
        use_dropout: false
        dropout_keep_probability: 1.0
      }
    }
    second_stage_post_processing {
      batch_non_max_suppression {
        score_threshold: 0.300000011921
        iou_threshold: 0.600000023842
        max_detections_per_class: 20
        max_total_detections: 20
      }
      score_converter: SOFTMAX
    }
    second_stage_localization_loss_weight: 2.0
    second_stage_classification_loss_weight: 1.0
    second_stage_batch_size: 49
  }
}
train_config {
  batch_size: 1
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
  optimizer {
    momentum_optimizer {
      learning_rate {
        manual_step_learning_rate {
          initial_learning_rate: 0.000300000014249
          
          schedule {
            step: 900000
            learning_rate: 2.99999992421e-05
          }
          schedule {
            step: 1200000
            learning_rate: 3.00000010611e-06
          }
        }
      }
      momentum_optimizer_value: 0.899999976158
    }
    use_moving_average: false
  }
  gradient_clipping_by_norm: 10.0
  fine_tune_checkpoint: ""C:/tensorflow1/models/research/object_detection/faster_rcnn_resnet50_lowproposals_coco_2018_01_28/model.ckpt""
  from_detection_checkpoint: true
  num_steps: 200000
}
train_input_reader {
  label_map_path: ""C:/tensorflow1/models/research/object_detection/training/labelmap.pbtxt""
  tf_record_input_reader {
    input_path: ""C:/tensorflow1/models/research/object_detection/train.record""
  }
}
eval_config {
  num_examples: 316
  max_evals: 10
  use_moving_averages: false
}
eval_input_reader {
  label_map_path: ""C:/tensorflow1/models/research/object_detection/training/labelmap.pbtxt""
  shuffle: false
  num_readers: 1
  tf_record_input_reader {
    input_path: ""C:/tensorflow1/models/research/object_detection/test.record""
  }
}
```

Object_detection_image.py
```
######## Image Object Detection Using Tensorflow-trained Classifier #########
# Import packages
import os
import cv2
import numpy as np
import tensorflow as tf
import sys

# This is needed since the notebook is stored in the object_detection folder.
sys.path.append("".."")

# Import utilites
from utils import label_map_util
from utils import visualization_utils as vis_util

# Name of the directory containing the object detection module we're using
MODEL_NAME = 'inference_graph'
IMAGE_NAME = 'test1.jpeg'

# Grab path to current working directory
CWD_PATH = os.getcwd()

# Path to frozen detection graph .pb file, which contains the model that is used
# for object detection.
PATH_TO_CKPT = os.path.join(CWD_PATH,MODEL_NAME,'frozen_inference_graph.pb')

# Path to label map file
PATH_TO_LABELS = os.path.join(CWD_PATH,'training','labelmap.pbtxt')

# Path to image
PATH_TO_IMAGE = os.path.join(CWD_PATH,IMAGE_NAME)

# Number of classes the object detector can identify
NUM_CLASSES = 12

# Load the label map.
# Label maps map indices to category names, so that when our convolution
# network predicts `5`, we know that this corresponds to `king`.
# Here we use internal utility functions, but anything that returns a
# dictionary mapping integers to appropriate string labels would be fine
label_map = label_map_util.load_labelmap(PATH_TO_LABELS)
categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)
category_index = label_map_util.create_category_index(categories)

# Load the Tensorflow model into memory.
detection_graph = tf.Graph()
with detection_graph.as_default():
    od_graph_def = tf.GraphDef()
    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:
        serialized_graph = fid.read()
        od_graph_def.ParseFromString(serialized_graph)
        tf.import_graph_def(od_graph_def, name='')

    sess = tf.Session(graph=detection_graph)

# Define input and output tensors (i.e. data) for the object detection classifier

# Input tensor is the image
image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')

# Output tensors are the detection boxes, scores, and classes
# Each box represents a part of the image where a particular object was detected
detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')

# Each score represents level of confidence for each of the objects.
# The score is shown on the result image, together with the class label.
detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')
detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')

# Number of objects detected
num_detections = detection_graph.get_tensor_by_name('num_detections:0')

# Load image using OpenCV and
# expand image dimensions to have shape: [1, None, None, 3]
# i.e. a single-column array, where each item in the column has the pixel RGB value
image = cv2.imread(PATH_TO_IMAGE)
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
image_expanded = np.expand_dims(image_rgb, axis=0)

# Perform the actual detection by running the model with the image as input
(boxes, scores, classes, num) = sess.run(
    [detection_boxes, detection_scores, detection_classes, num_detections],
    feed_dict={image_tensor: image_expanded})

# Draw the results of the detection (aka 'visulaize the results')

vis_util.visualize_boxes_and_labels_on_image_array(
    image,
    np.squeeze(boxes),
    np.squeeze(classes).astype(np.int32),
    np.squeeze(scores),
    category_index,
    use_normalized_coordinates=True,
    line_thickness=2,
    min_score_thresh=0.30)

# All the results have been drawn on image. Now display the image.
cv2.imshow('Object detector', image)

# Press any key to close the image
cv2.waitKey(0)

# Clean up
cv2.destroyAllWindows()
```
![tvorog_dop_pon (80)](https://user-images.githubusercontent.com/32834586/88474783-95659b80-cf32-11ea-8bb2-e3ea0047fa95.jpeg)
![tvorog_dop_pon (82)](https://user-images.githubusercontent.com/32834586/88474786-9991b900-cf32-11ea-846b-534eef0262f0.jpeg)
![tvorog_dop_sr (3)](https://user-images.githubusercontent.com/32834586/88474797-add5b600-cf32-11ea-8671-3b31e656e2d3.jpeg)

I have now copied the panoramic photo to the training folder in the folder for object_detection. No matches found that is, a completely identical image from the training set does not find a match"
41740,tf.estimator. adjust learning rate by loss on validation set,"dear all, 

- by using tf.estimator, how to adjust learning rate by loss on validation set ?

> for example, if the loss in not decreasing, i want to half learning rate

thanks"
41739,Confusion regarding TensorFlow licensing,"## URL(s) with the issue: 
https://www.tensorflow.org/install/lang_c
and 
https://www.tensorflow.org

## Description of issue (what needs changing):
- There is no text in the documentation regarding the licensing of the C API.
- There is no text in the documentation regarding the licensing of TensorFlow as a whole (see below).

### Clear description
When downloading the tar.gz file of the C API, or when building that C API, it comes with two license files: LICENSE and THIRD_PARTY_TF_C_LICENSES. The second file contains the GPL license for everything under ./bench/btl.

Yet, when looking at the package, there is no such path. Actually, this path is not available also in the source code of TensorFlow. This is confusing and may lead to the conclusion that this license is not applicable for the C API code or to TensorFlow as a whole and is provided by mistake.  

However, when compiling the TensorFlow code, such a path is created. Yet, still not in the package of the C API.

Could this be clarified: what is the license of the C API libraries? Is it allowed to link these libraries to commercial code without being exposed to GPL? 

And what is the license of TensorFlow code as a whole? Is it allowed to, for example, use TensorFlow code as part of a commercial Python application without being exposed to GPL?

I think that this information should come in the above mentioned URLs - the page about C API and the main page of TensorFlow."
41738,TensorFlow Lite currently doesn't support control flow ops,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 1.15


**Provide the text output from tflite_convert**

```
TensorFlow Lite currently doesn't support control flow ops: Enter, Exit, Merge, Switch. We are working on supporting control flow ops, please see github issue at https://github.com/tensorflow/tensorflow/issues/28485. Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, FULLY_CONNECTED, LESS, LOGICAL_AND, LOGISTIC, MAX_POOL_2D, MUL, RESHAPE, REVERSE_V2, SPLIT, TANH, TRANSPOSE. Here is a list of operators for which you will need custom implementations: CTC_BEAM_SEARCH_DECODER, LoopCond, TensorArrayGatherV3, TensorArrayReadV3, TensorArrayScatterV3, TensorArrayV3, TensorArrayWriteV3.
Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\Scripts\toco_from_protos-script.py"", line 10, in <module>
    sys.exit(main())
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow_core\lite\toco\python\toco_from_protos.py"", line 93, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow_core\python\platform\app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\absl\app.py"", line 299, in run
    _run_main(main, args)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\absl\app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow_core\lite\toco\python\toco_from_protos.py"", line 56, in execute
    enable_mlir_converter)
Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

TensorFlow Lite currently doesn't support control flow ops: Enter, Exit, Merge, Switch. We are working on supporting control flow ops, please see github issue at https://github.com/tensorflow/tensorflow/issues/28485. Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, FULLY_CONNECTED, LESS, LOGICAL_AND, LOGISTIC, MAX_POOL_2D, MUL, RESHAPE, REVERSE_V2, SPLIT, TANH, TRANSPOSE. Here is a list of operators for which you will need custom implementations: CTC_BEAM_SEARCH_DECODER, LoopCond, TensorArrayGatherV3, TensorArrayReadV3, TensorArrayScatterV3, TensorArrayV3, TensorArrayWriteV3.
```

Also, please include a link to a GraphDef or the model if possible.

"
41737,OP_REQUIRES failure in c++ LoadSavedModel with conv2d w/ bias into an Add,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux, latest
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): installed from arch community
- TensorFlow version (use command below): tensorflow-opt-cuda 2.3.0rc2-2
- Python version: 3.8.4
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): 10.1
- CUDA/cuDNN version: cuda 11.0.2-1 cudnn 8.0.0.180-2
- GPU model and memory: nvidia Titan Xp (12GB)

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

2020-07-26 12:25:21.992520: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
unknown 2.3.0-rc2

**Describe the current behavior**

I have successfully trained a little custom keras resnet with skip connections using the python side of things, so (simplified) the portion of network that is erroring during c++ load looks like:

```
a
+------------+
|            |
conv2d       |
|            |
lrelu        |
|            |
conv2d       |
|            |
Add ---------+
|
out
```

when I save this keras model using model.save( 'foo' ), then attempt to load it in and execute it using the c++ layer, during execute, it is unable to run, reporting: Fusion is not implemented: [BiasAdd,Add]
	 [[{{node foo/test_residadd/add}}]]""

**Describe the expected behavior**
I would have expected it to load and optimize correctly so I could use it for inference in a c++ application

**Standalone code to reproduce the issue**
Files attached. you can see the steps in recreate.sh, but basically, run the python file, this will save a model, then compile / run the c++ program to load the model.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

2020-07-26 13:21:36.941101: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2020-07-26 13:21:36.960504: I tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /home/kimball/Development/bugreport/foo
2020-07-26 13:21:36.961289: I tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }
2020-07-26 13:21:36.961304: I tensorflow/cc/saved_model/loader.cc:234] Reading SavedModel debug info (if present) from: /home/kimball/Development/bugreport/foo
2020-07-26 13:21:36.961385: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-07-26 13:21:36.985670: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3601000000 Hz
2020-07-26 13:21:36.986383: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55de6bc0eaa0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-07-26 13:21:36.986394: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-07-26 13:21:36.987511: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-07-26 13:21:37.057104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-26 13:21:37.057480: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55de6bc0e030 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-07-26 13:21:37.057492: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): TITAN Xp, Compute Capability 6.1
2020-07-26 13:21:37.057594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-26 13:21:37.057904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: TITAN Xp computeCapability: 6.1
coreClock: 1.582GHz coreCount: 30 deviceMemorySize: 11.88GiB deviceMemoryBandwidth: 510.07GiB/s
2020-07-26 13:21:37.057932: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2020-07-26 13:21:37.059248: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
2020-07-26 13:21:37.059806: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-07-26 13:21:37.059948: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-07-26 13:21:37.061270: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-07-26 13:21:37.061599: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11
2020-07-26 13:21:37.061666: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2020-07-26 13:21:37.061692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-26 13:21:37.061985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-26 13:21:37.062276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-07-26 13:21:37.062290: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2020-07-26 13:21:37.268129: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-26 13:21:37.268149: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-07-26 13:21:37.268154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2020-07-26 13:21:37.268252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-26 13:21:37.268624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-26 13:21:37.268930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10019 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:01:00.0, compute capability: 6.1)
2020-07-26 13:21:37.269287: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
2020-07-26 13:21:37.275552: I tensorflow/cc/saved_model/loader.cc:199] Restoring SavedModel bundle.
2020-07-26 13:21:37.388514: I tensorflow/cc/saved_model/loader.cc:183] Running initialization op on SavedModel bundle at path: /home/kimball/Development/bugreport/foo
2020-07-26 13:21:37.391535: I tensorflow/cc/saved_model/loader.cc:303] SavedModel load for tags { serve }; Status: success: OK. Took 431036 microseconds.
Loaded model ok...
2020-07-26 13:21:37.422171: W tensorflow/core/framework/op_kernel.cc:1744] OP_REQUIRES failed at conv_ops_fused_impl.h:700 : Unimplemented: Fusion is not implemented: [BiasAdd,Add]
ERROR: Fusion is not implemented: [BiasAdd,Add]
	 [[{{node foo/testresid_add/add}}]]


[bugreport.zip](https://github.com/tensorflow/tensorflow/files/4976996/bugreport.zip)"
41736,In Ubuntu 20.04 not able to register GT 730 GPU - Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file,"**System information**
- OS Platform and Distribution  Linux Ubuntu 20.04

- TensorFlow installed with following command :

`pip3 install --upgrade tensorflow`

- TensorFlow version:

`2.2.0`

- Python version: 3.8.2

- Installed using virtualenv? pip? conda?:
pip3

- GCC/Compiler version (if compiling from source):

- CUDA/cuDNN version:

`nvcc --version` gives me below 

```
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2019 NVIDIA Corporation
Built on Wed_Oct_23_19:24:38_PDT_2019
Cuda compilation tools, release 10.2, V10.2.89

```

And 


```
$ cat /usr/local/cuda/version.txt
CUDA Version 11.0.207
```

And also 

Check for installed CUDA toolkit package:

```
$ dpkg -l | grep cuda-toolkit
ii  cuda-toolkit-11-0                             11.0.2-1                                  amd64        CUDA Toolkit 11.0 meta-package
ii  nvidia-cuda-toolkit                           10.1.243-3                                amd64        NVIDIA CUDA development toolkit

```

And 

```
gcc --version
gcc (Ubuntu 9.3.0-10ubuntu2) 9.3.0
Copyright (C) 2019 Free Software Foundation, Inc.
```


- GPU model and memory:

 GeForce GT 730 computeCapability: 3.5

**Describe the problem**

For installing the cuDnn, I strictly followed [this SO ans](https://askubuntu.com/a/1251052/308359) by creating a developer account - 

## MAIN ISSUE IS - After running Tensorflow code getting

### Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/cuda/include:/usr/lib/cuda/lib64:

And below is the full log of the error in the terminal after a running a single .py file with Tensorflow

```
020-07-26 05:36:40.742760: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-07-26 05:36:40.774143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-26 05:36:40.774446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GT 730 computeCapability: 3.5
coreClock: 0.9015GHz coreCount: 2 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 11.92GiB/s
2020-07-26 05:36:40.774705: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-07-26 05:36:40.776037: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-07-26 05:36:40.776832: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-07-26 05:36:40.777094: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-07-26 05:36:40.779055: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-07-26 05:36:40.779835: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-07-26 05:36:40.779996: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/cuda/include:/usr/lib/cuda/lib64:
2020-07-26 05:36:40.780007: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1598] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-07-26 05:36:40.780309: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-07-26 05:36:40.785339: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3392485000 Hz
2020-07-26 05:36:40.785677: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f71a4000b60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-07-26 05:36:40.785698: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-07-26 05:36:40.787154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-26 05:36:40.787171: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]    
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
41732,BoostedTreesClassifier only supports dictionary-based dataset,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): All
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0
- Python version: 3.7.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
When using `BoostedTreesClassifier` (and likely related classes), the features part of the dataset must be represented as a dictionary of strings to tensors whereas the docs state that the dataset can consist of ""A tuple (features, labels): Where features is a **tf.Tensor or** a dictionary of string feature name to Tensor...""

```python

    772   outputs = {}
    773   with ops.name_scope(
--> 774       None, default_name='transform_features', values=features.values()):
    775     transformation_cache = FeatureTransformationCache(features)
    776     for column in feature_columns:

AttributeError: 'Tensor' object has no attribute 'values'
```

**Describe the expected behavior**
The features part of the dataset should be able to be represented as a tensor (preferred) or the docs should be updated to reflect this constraint.

**Standalone code to reproduce the issue**
```python
import numpy as np
import pandas as pd
import tensorflow as tf

N_OBS = 100
N_DIM = 10
BATCH_SIZE = N_OBS

df = pd.DataFrame(np.random.rand(N_OBS, N_DIM), columns=[f'dim_{i}' for i in range(N_DIM)])
labels = pd.Series(np.random.rand(N_OBS).round())

def input_fn(df, labels, batch_size=BATCH_SIZE):
    # dataset = tf.data.Dataset.from_tensor_slices((dict(df), labels.values))  # works
    dataset = tf.data.Dataset.from_tensor_slices((df.values, labels.values))  # does not work 
    dataset = dataset.repeat().batch(batch_size)
    return dataset

feature_columns = [tf.feature_column.numeric_column(f'dim_{i}') for i in range(N_DIM)]
estimator = tf.estimator.BoostedTreesClassifier(
    feature_columns,
    n_batches_per_layer=N_OBS // BATCH_SIZE
)

estimator.train(lambda: input_fn(df, labels), max_steps=100)
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Feature column source where `features.values()` is referenced: https://github.com/tensorflow/tensorflow/blob/090f260aab3dab00bcdf0232962e753bb9fab696/tensorflow/python/feature_column/feature_column_v2.py#L418
"
41731,ValueError: The first argument to `Layer.call` must always be passed.,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
41729,I'm getting this error. Can you help me out?,tensorflow.python.util.tf_export.SymbolAlreadyExposedError: Symbol Zeros is already exposed as ().
41728,"run tht tfmot.quantization.keras.quantize_model(model) and raise the error ""`model` must be a built model. been built yet"" while model.built is True","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10
- TensorFlow version (use command below):2.1.0
- Python version:3.7.7

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
I write my own `GRUCell` and create a model by `keras.RNN()`.  Here's my model
```python
class GRUCell(keras.layers.Layer):
    def __init__(self, units, **kwargs):
        self.units = units
        self.state_size = tf.TensorShape([units])
        self.output_size = tf.TensorShape([units])
        super(GRUCell, self).__init__(**kwargs)

    def build(self, input_shape):
        self.dim = input_shape[-1]

        self.w_r = self.add_weight(shape=[self.dim+self.units, self.units], initializer='uniform', name='reset_gate', trainable=True)
        self.b_r = self.add_weight(shape=[self.units], initializer='zeros', name='reset gate bias', trainable=True)

        self.w_z = self.add_weight(shape=[self.dim+self.units, self.units], initializer='uniform', name='update_gate', trainable=True)
        self.b_z = self.add_weight(shape=[self.units], initializer='zeros', name='update gate bias', trainable=True) 

        self.w_n = self.add_weight(shape=[self.dim+self.units, self.units], initializer='uniform', name='intetim', trainable=True)
        self.b_n = self.add_weight(shape=[self.units], initializer='zeros', name='interim gate bias', trainable=True)

        self.build = True


    def call(self, inputs, states):
        
        states, = states
        
        r = tf.nn.sigmoid(inputs @ self.w_r[:self.dim] + states @ self.w_r[self.dim:] + self.b_r)
        z = tf.nn.sigmoid(inputs @ self.w_z[:self.dim] + states @ self.w_z[self.dim:] + self.b_z)
        n = tf.nn.tanh(inputs @ self.w_n[:self.dim] + (states * r) @ self.w_n[self.dim:] + self.b_n)
        
        output = (1 - z) * states + z * n

        return output, output

    def get_config(self):
        return {""units"": self.units}

def create_model(units):
    model = keras.Sequential([
            keras.layers.RNN(GRUCell(units)),
            keras.layers.Dense(1)
            ])
    model.compile(optimizer=keras.optimizers.RMSprop(),
                loss='mae', metrics=['mse'])

    return model
```
This model works well.
![](https://raw.githubusercontent.com/ChenHaoHere/PicGo/master/20200725222620.png)
But when I'd like to use the snippet from (https://www.tensorflow.org/model_optimization/guide/quantization/training_example), the error **ValueError: `model` must be a built model. been built yet. Please call `model.build(input_shape)` before quantizing your model.** was raised. But `model.built` is True.

![](https://raw.githubusercontent.com/ChenHaoHere/PicGo/master/20200725223938.png)


"
41727,Tensorflow using Intel GPU instead of NVIDIA GPU,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home 64-bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.2.0
- Python version: 3.8.3
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10.1/7.6
- GPU model and memory: NVIDIA GTX 1050 (4 GB) 

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
TensorFlow, by default is using GPU 0, which on my PC is my built-in Intel GPU while I have a dedicated NVIDIA GPU and have CUDA installed yet TF is using Intel.

**Describe the expected behavior**
TensorFlow should display current device to be NVIDIA GPU

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

![Annotation 2020-07-25 190353](https://user-images.githubusercontent.com/35456031/88458751-b8983880-cea9-11ea-8237-44bcc66eef19.png)
![Annotation 2020-07-25 190430](https://user-images.githubusercontent.com/35456031/88458754-bafa9280-cea9-11ea-9375-de1ba6feec2b.png)

"
41725,Tensorflow serving docker example,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.6.1810 (Core)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14.0
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1/7.6.4
- GPU model and memory: GEForce GT 730, 2GB


**Describe the current behavior**
I am trying out the example code of docker based tensorflow serving basic example as provided  in
https://www.tensorflow.org/tfx/serving/docker
Current behaviour is that the docker throws an error. The error is mentioned at the end of this report.
I suspect that this is docker problem, but i did not find any minimum docker version mentioned in document. However I am trying to upgrade docker on my computer. Current  version is ""Docker version 1.13.1, build 64e9980/1.13.1""

**Describe the expected behavior**
The expected behaviour as per the example is to get an output as described in the tutorial.
# Returns => { ""predictions"": [2.5, 3.0, 4.5] }

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
# Download the TensorFlow Serving Docker image and repo
docker pull tensorflow/serving

git clone https://github.com/tensorflow/serving
# Location of demo models
TESTDATA=""$(pwd)/serving/tensorflow_serving/servables/tensorflow/testdata""

# Start TensorFlow Serving container and open the REST API port
docker run -t --rm -p 8501:8501 \
    -v ""$TESTDATA/saved_model_half_plus_two_cpu:/models/half_plus_two"" \
    -e MODEL_NAME=half_plus_two \
    tensorflow/serving &

# Query the model using the predict API
curl -d '{""instances"": [1.0, 2.0, 5.0]}' \
    -X POST http://localhost:8501/v1/models/half_plus_two:predict

# Returns => { ""predictions"": [2.5, 3.0, 4.5] }

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
The error message i get is

2020-07-25 13:10:40.668221: E tensorflow_serving/sources/storage_path/file_system_storage_path_source.cc:362] FileSystemStoragePathSource encountered a filesystem access error: /models/half_plus_two; Permission denied

I have checked all paths. permission for the serving directory is
drwxr-xr-x. 7 srikrishnan docker       249 Jul 18 22:40 serving"
41724,Tensorflow stuck in training with more than two GPUs,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No, I am using the example presented in https://www.tensorflow.org/guide/gpu#using_multiple_gpus

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 18.04.3 LTS

- TensorFlow installed from (source or binary):
I am using the TensorFlow Docker image tensorflow/tensorflow:latest-gpu-jupyter

- TensorFlow version (use command below):
2.2.0

- Python version:
3.6.9

- CUDA/cuDNN version:
CUDA version: 10.1
- GPU model and memory:
Nvidia Tesla P100-SXM2


**Describe the current behavior**
When I try to run the code below in a jupyter notebook with more than two GPUs it get stuck in the training step. It works fine with two GPUs.

**Standalone code to reproduce the issue**

```
import tensorflow as tf
strategy = tf.distribute.MirroredStrategy()
input_values = tf.random.uniform((256,), minval=0, maxval=1, dtype=tf.dtypes.float32)
with strategy.scope():
    inputs = tf.keras.layers.Input(shape=(1,))
    predictions = tf.keras.layers.Dense(1)(inputs)
    model = tf.keras.models.Model(inputs=inputs, outputs=predictions)
    model.compile(loss='mse', optimizer=tf.keras.optimizers.SGD(learning_rate=0.2))
    
    model.fit(input_values, input_values, batch_size=32)
```
"
41723,Want to see the CNN Filter (image format),"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):
2.2.0
- Are you willing to contribute it (Yes/No):
Yes



**Describe the feature and the current behavior/state.**
There is no such feature for what filter did the model came up while training is completed. Please make a function for it. 

**Will this change the current api? How?**
The API will become more Awosome than before it was. Present every one is getting accuracy on just predefined test and train cases , but after this feature people will know what their model is predecting in the given image

**Who will benefit with this feature?**
Especailly people who are learning like me rather than the people who are just run behind accuracy
**Any Other info.**
Please If u want any help , do tell me , Big fan of Google's Code Works . And also make a function for it like model.show_filter_con2d() ; "
41722,"How to convet ""tf.nn.ctc_greedy_decoder"" output into readable format?","I am using `tf.nn.ctc_greedy_decoder` to decode the text but I am not able to read `decoder[0].indices`. how to read this decoder data (or how to convert it to NumPy)?

code:
`
decoder, _ = tf.compat.v1.nn.ctc_greedy_decoder(inputs=inputs, 
													sequence_length=seqLen, 
													merge_repeated=True)
print(decoder[0].indices)
`


"
41720,golang: array bound is too large at raspberry pi,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspberry PI 3B+ / Buster
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Self build
- TensorFlow version (use command below): 2.3.0-rc2
- Python version:
- Bazel version (if compiling from source): default from cross compile container
- GCC/Compiler version (if compiling from source): default from cross compile container
- CUDA/cuDNN version: No CUDA
- GPU model and memory: No GPU

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
```
$ go test github.com/tensorflow/tensorflow/tensorflow/go
# github.com/tensorflow/tensorflow/tensorflow/go [github.com/tensorflow/tensorflow/tensorflow/go.test]
tensorflow/go/graph.go:123:13: array bound is too large
tensorflow/go/graph.go:123:20: constant 1125899906842623 overflows int
tensorflow/go/tensor.go:349:13: array bound is too large
tensorflow/go/tensor.go:349:20: constant 1125899906842623 overflows int
FAIL    github.com/tensorflow/tensorflow/tensorflow/go [build failed]
```
**Describe the expected behavior**
go test should pass

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
41718,High CPU memory usage when calling GradientTape's gradient() when using multiple threads/cores,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): Binary (pip install tf-nightly)
- TensorFlow version (use command below): tf-nightly 2.4.0.dev20200724
- Python version: 3.6.9
- Bazel version (if compiling from source): None
- GCC/Compiler version (if compiling from source): None
- CUDA/cuDNN version: None (CPU only)
- GPU model and memory: None (CPU only)

**Describe the current behavior**
The memory usage when using multiple threads is ~7GB on the example code provided below.

**Describe the expected behavior**
The memory usage should be around ~850-900MB, which is what you get when you only use 1 thread via the following:
```
tf.config.threading.set_inter_op_parallelism_threads(1)
tf.config.threading.set_intra_op_parallelism_threads(1)
```

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/1XEcTR433ePm4-OTi3i22PshKUk2aFFq-?usp=sharing
Note that the issue is not reproducible in colab as it only runs on 1 core.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.


Memory usage when tf.config.threading.set_*_op_parallelism_threads to 1:
```
2020-07-25 11:22:37.430254: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-07-25 11:23:01.051408: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-07-25 11:23:01.051508: E tensorflow/stream_executor/cuda/cuda_driver.cc:314] failed call to cuInit: UNKNOWN ERROR (-1)
2020-07-25 11:23:01.051596: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (big000): /proc/driver/nvidia/version does not exist
2020-07-25 11:23:01.052370: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-07-25 11:23:01.069108: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2094995000 Hz
2020-07-25 11:23:01.069468: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5732470 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-07-25 11:23:01.069529: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-07-25 11:23:01.638778: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:118] None of the MLIR optimization passes are enabled (registered 1)
-------------------------------------------------------------------- -1.08216429
Memory usage:  867950592
-------------------------------------------------------------------- -1.02312386
Memory usage:  898691072
-------------------------------------------------------------------- -1.02274418
Memory usage:  899932160
-------------------------------------------------------------------- -0.958470643
Memory usage:  900022272
-------------------------------------------------------------------- -0.994939446
Memory usage:  900042752
-------------------------------------------------------------------- -1.04647171
Memory usage:  900050944
-------------------------------------------------------------------- -1.00102437
Memory usage:  900235264
-------------------------------------------------------------------- -0.987627268
Memory usage:  900382720
-------------------------------------------------------------------- -1.00234675
Memory usage:  900382720
-------------------------------------------------------------------- -0.957870305
Memory usage:  900390912
-------------------------------------------------------------------- -1.04384947
Memory usage:  900399104
-------------------------------------------------------------------- -1.05602765
Memory usage:  900399104
-------------------------------------------------------------------- -1.03467119
Memory usage:  900411392
-------------------------------------------------------------------- -1.00377405
Memory usage:  900415488
-------------------------------------------------------------------- -0.925668299
Memory usage:  900415488
-------------------------------------------------------------------- -1.08483529
Memory usage:  900423680
-------------------------------------------------------------------- -1.00288522
Memory usage:  900423680
-------------------------------------------------------------------- -0.909320414
Memory usage:  900427776
-------------------------------------------------------------------- -1.03661454
Memory usage:  900431872
-------------------------------------------------------------------- -0.955467224
...
Memory usage:  900431872
```

Memory usage peaks to around ~850MB.


Memory usage when *not* setting tf.config.threading.set_*_op_parallelism_threads to 1 i.e., when multiple threads/cores are being used:

```
2020-07-25 11:06:52.523261: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-07-25 11:07:17.286000: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-07-25 11:07:17.286099: E tensorflow/stream_executor/cuda/cuda_driver.cc:314] failed call to cuInit: UNKNOWN ERROR (-1)
2020-07-25 11:07:17.286161: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (big000): /proc/driver/nvidia/version does not exist
2020-07-25 11:07:17.287431: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-07-25 11:07:17.325674: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2094995000 Hz
2020-07-25 11:07:17.344702: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x504e590 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-07-25 11:07:17.344800: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-07-25 11:07:18.198284: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:118] None of the MLIR optimization passes are enabled (registered 1)
-------------------------------------------------------------------- -1.08216429
Memory usage:  933933056
-------------------------------------------------------------------- -1.02312386
Memory usage:  1463521280
-------------------------------------------------------------------- -1.02274418
Memory usage:  1981599744
-------------------------------------------------------------------- -0.958470643
Memory usage:  2500599808
-------------------------------------------------------------------- -0.994939446
Memory usage:  3082350592
-------------------------------------------------------------------- -1.04647171
Memory usage:  3530485760
-------------------------------------------------------------------- -1.00102437
Memory usage:  4058849280
-------------------------------------------------------------------- -0.987627268
Memory usage:  4124266496
-------------------------------------------------------------------- -1.00234675
Memory usage:  4574625792
-------------------------------------------------------------------- -0.957870305
Memory usage:  5169975296
-------------------------------------------------------------------- -1.04384947
Memory usage:  5198962688
-------------------------------------------------------------------- -1.05602765
Memory usage:  5294829568
-------------------------------------------------------------------- -1.03467119
Memory usage:  5808898048
-------------------------------------------------------------------- -1.00377405
Memory usage:  6403420160
-------------------------------------------------------------------- -0.925668299
Memory usage:  6428565504
-------------------------------------------------------------------- -1.08483529
Memory usage:  6433001472
-------------------------------------------------------------------- -1.00288522
Memory usage:  6433193984
-------------------------------------------------------------------- -0.909320414
Memory usage:  6462169088
-------------------------------------------------------------------- -1.03661454
Memory usage:  6982217728
-------------------------------------------------------------------- -0.955467224
Memory usage:  6985584640
-------------------------------------------------------------------- -1.02524447
Memory usage:  7088070656
-------------------------------------------------------------------- -1.06649399
Memory usage:  7114588160
-------------------------------------------------------------------- -0.991624892
Memory usage:  7116353536
-------------------------------------------------------------------- -1.01972055
Memory usage:  7143043072
-------------------------------------------------------------------- -0.994572461
Memory usage:  7144923136
-------------------------------------------------------------------- -0.965809107
Memory usage:  7149711360
-------------------------------------------------------------------- -1.02491224
Memory usage:  7181701120
-------------------------------------------------------------------- -1.02480865
Memory usage:  7723294720
-------------------------------------------------------------------- -0.970712662
Memory usage:  7724584960
-------------------------------------------------------------------- -1.02038872
Memory usage:  7726415872
-------------------------------------------------------------------- -1.01687396
Memory usage:  7726661632
-------------------------------------------------------------------- -1.03144
Memory usage:  7726739456
-------------------------------------------------------------------- -0.950338304
Memory usage:  7748882432
-------------------------------------------------------------------- -0.9357301
Memory usage:  7748997120
-------------------------------------------------------------------- -0.932868242
Memory usage:  7781220352
-------------------------------------------------------------------- -0.950198233
Memory usage:  7781298176
-------------------------------------------------------------------- -0.966640532
Memory usage:  7782912000
-------------------------------------------------------------------- -0.97049737
Memory usage:  7783198720
-------------------------------------------------------------------- -1.00543392
Memory usage:  7810494464
-------------------------------------------------------------------- -0.949840665
Memory usage:  7810592768
-------------------------------------------------------------------- -1.0022217
Memory usage:  7810596864
-------------------------------------------------------------------- -1.12232506
Memory usage:  7810686976
-------------------------------------------------------------------- -1.01822805
Memory usage:  7813079040
-------------------------------------------------------------------- -1.04360735
Memory usage:  7813513216
-------------------------------------------------------------------- -0.956581
Memory usage:  7813517312
-------------------------------------------------------------------- -0.981084287
Memory usage:  7813537792
-------------------------------------------------------------------- -0.977599561
Memory usage:  7813615616
-------------------------------------------------------------------- -1.01713431
Memory usage:  8277536768
-------------------------------------------------------------------- -1.03842521
Memory usage:  8276979712
-------------------------------------------------------------------- -0.971421421
Memory usage:  8299802624
-------------------------------------------------------------------- -1.06472087
Memory usage:  8299806720
-------------------------------------------------------------------- -1.00037909
Memory usage:  8299974656
-------------------------------------------------------------------- -1.00314832
Memory usage:  8299995136
-------------------------------------------------------------------- -0.99641335
Memory usage:  8299999232
-------------------------------------------------------------------- -1.07826197
Memory usage:  8300007424
-------------------------------------------------------------------- -0.998241484
Memory usage:  8300498944
-------------------------------------------------------------------- -1.01393604
Memory usage:  8300503040
-------------------------------------------------------------------- -1.04430008
Memory usage:  8301498368
-------------------------------------------------------------------- -0.956604362
Memory usage:  8303042560
-------------------------------------------------------------------- -0.994233251
Memory usage:  8303177728
-------------------------------------------------------------------- -1.0575521
Memory usage:  8303247360
-------------------------------------------------------------------- -0.975697637
Memory usage:  8304357376
-------------------------------------------------------------------- -1.0441494
Memory usage:  8304455680
-------------------------------------------------------------------- -0.949515164
Memory usage:  8304496640
-------------------------------------------------------------------- -0.935756862
Memory usage:  8304496640
-------------------------------------------------------------------- -1.00150812
Memory usage:  8304496640
-------------------------------------------------------------------- -0.983478725
Memory usage:  8304627712
-------------------------------------------------------------------- -0.986330569
Memory usage:  8304635904
-------------------------------------------------------------------- -1.13513744
Memory usage:  8287686656
-------------------------------------------------------------------- -1.03896141
Memory usage:  8287719424
-------------------------------------------------------------------- -0.988004625
Memory usage:  8287723520
-------------------------------------------------------------------- -0.985565066
Memory usage:  8287731712
-------------------------------------------------------------------- -1.0104388
Memory usage:  8304316416
-------------------------------------------------------------------- -1.06218076
Memory usage:  8304353280
-------------------------------------------------------------------- -0.94707495
Memory usage:  8304357376
-------------------------------------------------------------------- -0.998919964
Memory usage:  8304357376
-------------------------------------------------------------------- -0.974167764
Memory usage:  8304361472
-------------------------------------------------------------------- -0.97153908
Memory usage:  8304365568
-------------------------------------------------------------------- -0.964009821
Memory usage:  8304369664
-------------------------------------------------------------------- -0.992211759
Memory usage:  8304373760
-------------------------------------------------------------------- -1.04404831
Memory usage:  8304431104
-------------------------------------------------------------------- -1.02076721
Memory usage:  8304435200
-------------------------------------------------------------------- -0.962560713
Memory usage:  8304439296
-------------------------------------------------------------------- -1.02682006
Memory usage:  8304443392
-------------------------------------------------------------------- -1.03967941
Memory usage:  8304447488
-------------------------------------------------------------------- -1.07996821
Memory usage:  8304480256
-------------------------------------------------------------------- -0.98360759
Memory usage:  8305147904
-------------------------------------------------------------------- -1.0019182
Memory usage:  8305152000
-------------------------------------------------------------------- -0.935099125
Memory usage:  8305205248
-------------------------------------------------------------------- -1.03370762
Memory usage:  8305209344
-------------------------------------------------------------------- -0.98182
Memory usage:  8305213440
-------------------------------------------------------------------- -0.964839816
Memory usage:  8305217536
-------------------------------------------------------------------- -0.926978409
Memory usage:  8305221632
-------------------------------------------------------------------- -0.972101748
...
```

As you can see, memory usage peaks to around ~8GB.

The issue cannot be reproduced on google colab because colab only runs on 1 CPU anyway, so the behavior is the same regardless of whether you call tf.config.threading.set_*_op_parallelism_threads(1) or not."
41715,Keras mixed precision API 50x slower than mixed precision graph rewrite,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.2.0, 2.3.0rc2, 2.4.0.dev2020072401
- Python version: 3.7.8
- CUDA/cuDNN version: 10.1 / 7.6.5.32
- GPU model and memory: NVIDIA V100

I currently use the [mixed precision graph rewrite](https://www.tensorflow.org/api_docs/python/tf/train/experimental/enable_mixed_precision_graph_rewrite) which will apply mixed precision optimizations to the entire TF graph. Since 2.1 there is a [new Keras API](https://www.tensorflow.org/guide/keras/mixed_precision) which replaces the graph rewrite and provides layerwise customization of the dtype policy. I tried switching to the new API and ran into the following issue.

**Describe the current behavior**

The Keras mixed precision API is 50 times slower than the legacy graph rewrite and the float32 baseline. When running the code example below on a NVIDIA V100 I get the following runtimes:

|  | float32 | Keras AMP | Keras AMP (float32 depthwise) | AMP graph rewrite
--- | --- | --- | --- | ---
time / step | 120ms  | **2000ms** | 38ms | 37ms
time / epoch | 6s | **90s** | 2s | 2s

**Describe the expected behavior**

Performance of the Keras mixed precision API should be on par with the graph rewrite and should outperform normal float32 training.

**Standalone code to reproduce the issue**
```python
import tensorflow as tf
import tensorflow_datasets as tfds

policy = tf.keras.mixed_precision.experimental.Policy(""mixed_float16"")
tf.keras.mixed_precision.experimental.set_policy(policy)


def preprocessing(data):
    return tf.cast(data[""image""], tf.float32) / 255.0, data[""label""]


dataset = (
    tfds.load(""cifar10"", split=""train"")
    .map(preprocessing, num_parallel_calls=tf.data.experimental.AUTOTUNE)
    .cache()
    .batch(1024)
    .prefetch(1)
)

model = tf.keras.models.Sequential(
    [
        tf.keras.layers.Conv2D(8, 3, padding=""same"", activation=""relu"", input_shape=(32, 32, 3)),
        tf.keras.layers.DepthwiseConv2D(3, depth_multiplier=8, padding=""same"", activation=""relu""),
        tf.keras.layers.GlobalAveragePooling2D(),
        tf.keras.layers.Dense(10),
        tf.keras.layers.Activation(""softmax"", dtype=""float32""),
    ]
)

model.compile(optimizer=""adam"", loss=""sparse_categorical_crossentropy"")

model.fit(dataset, epochs=3, callbacks=[tf.keras.callbacks.TensorBoard(""logs"")])
```

**Other info / logs**

This big slowdown is cause by the depthwise convolution in the model which takes up the bulk of the computation in the toy model mentioned above. When looking at the generated profile the issue it looks like Keras AMP runs the very slow
```cpp
DepthwiseConv2dBackpropFilterGPUKernelNHWC(DepthwiseArgs, Eigen::half const*, Eigen::half const*, Eigen::half*, int)
```
kernel whereas `float32` runs
```cpp
DepthwiseConv2dBackpropFilterGPUKernelNCHW(DepthwiseArgs, float const*, float const*, float*, int)
```
and the AMP graph rewrite executes
```cpp
DepthwiseConv2dBackpropFilterGPUKernelNHWC(DepthwiseArgs, float const*, float const*, float*, int)
```
It looks like the `float32` versions of the kernels are a lot faster, although I am not sure why the baseline uses `NCHW` instead of `NHWC`.

The underlying issue is definitely the unusable `float16` `DepthwiseConv2dBackpropFilter` kernel (see also #27780), although I think it would be good to automatically workaround this in Keras AMP or grappler as well since this makes Keras AMP not usable for networks with depthwise convolutions.

/cc @reedwm 
"
41712,"Conv1DTranspose Dilation support - Might be a bug, IDK.","Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
[https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1DTranspose]()

## Description of issue (what needs changing):
Conv1DTranspose - Dilation - Does not inform uses that dilation doesn't work for any value  of `Dilation>1` because it isn't implemented yet. 
### Clear description
Currently documentation says:
_""an integer, specifying the dilation rate to use for dilated convolution. Currently, specifying a dilation_rate value != 1 is incompatible with specifying a stride value != 1.""_

This may not be implemented yet in the newest of nightly build, but with my tf-nightly==2.5.0dev20200629 build this didn't work. I fear updating to new nightly builds in case in breaks my research code which relies on nightly builds until Conv1DTranspose is released in a supported build.
```
InvalidArgumentError:  Current libxsmm and customized CPU implementations do not yet support dilation rates larger than 1.
	 [[node test1_AE/decoder/conv1d_transpose/conv1d_transpose (defined at D:\Users\[username]\Desktop\libs_python\nn4n_autoencoder4.py:120) ]] [Op:__inference_train_function_2185]

Function call stack:
train_function
```
This is with stride == 1.
### Correct links

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/convolutional.py

### Parameters defined

Yes, setting my dilation to 1 gets rid of the issue.

### Returns defined

Not necessary. (I'm not sure if you are asking if I have define returns in my code or if my code returns a defined value, or if the documentation claims to return something)

### Raises listed and defined
```
InvalidArgumentError:  Current libxsmm and customized CPU implementations do not yet support dilation rates larger than 1.
	 [[node test1_AE/decoder/conv1d_transpose/conv1d_transpose (defined at D:\Users\[username]\Desktop\libs_python\nn4n_autoencoder4.py:120) ]] [Op:__inference_train_function_2185]

Function call stack:
train_function
```

### Usage example

Nightly build, so no.

### Request visuals, if applicable
 No.

### Submit a pull request?
I will not be doing so.


### Test Code

Note 1: This is built with tf-nightly==2.5.0dev20200626 which was removed from the PyPi archive for unknown reasons.

Note 2: model.fit must be called for the error to occur. Simpy constructing and compiling the network was not enough to reproduce the error.

```
import tensorflow as tf
import tensorflow.keras as krs
import numpy as np

train_data = np.random.uniform(-1,1,(20,20))

inputs = krs.Input((20,1))

x = inputs

x = krs.layers.Conv1D(1,3,strides = 1,padding='same',dilation_rate=2,activation='relu')(x)
x = krs.layers.Flatten()(x)
x = krs.layers.Dense(10,activation='relu')(x)
x = krs.layers.Dense(2,activation='relu')(x)
x = krs.layers.Dense(10,activation='relu')(x)
x = krs.layers.Dense(20,activation='relu')(x)
x = krs.layers.Reshape(target_shape=(20,1))(x)
x = krs.layers.Conv1DTranspose(1,3,strides=1,dilation_rate=2,padding='same',activation='relu',output_padding=0)(x)
output = krs.layers.Flatten()(x)

model = krs.Model(inputs,output,name='test')

model.compile(optimizer='adam',loss='MSE')

model.summary()

model.fit(train_data,train_data)
```"
41710,`--config=c++17` option is incompatible with GCC build,"**System information**
- OS Platform and Distribution **Ubuntu Linux 18.04 LTS**
- TensorFlow installed from (source or binary): **source**
- TensorFlow version: **master** (`e2204b27`)
- Python version: **3.8.3**
- Bazel version (if compiling from source): **3.1.0**
- GCC/Compiler version (if compiling from source): **7.5.0-3ubuntu1~18.04**

**Describe the problem**

The `--config=c++17` build configuration option adds compiler flags which are incompatible with GCC.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
./configure (Accept defaults)
```

```
bazel build --config=c++17 //tensorflow:tensorflow_cc
```

**Any other info / logs**

```
ERROR: /home/andrew/.cache/bazel/_bazel_andrew/05f0c1fc0fd5d93d6530b840ab375f68/external/com_github_grpc_grpc/BUILD:487:1: C++ compilation of rule '@com_github_grpc_grpc//:gpr_base' failed (Exit 1)
gcc: error: unrecognized command line option '-stdlib=libc++'
```

The `c++17` configuration option adds the `-stdlib=libc++` flag to the C++ compiler, but this is a Clang flag, not GCC.

"
41709,Linking cross compiled  libtensorlowlite.so for aarch64 causes reference errors: undefined reference to `fcntl64@GLIBC_2.28',"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18
- TensorFlow installed from (source or binary): pip
- Python version: 3.7
- Installed using virtualenv? pip? conda?:
- GCC/Compiler version (if compiling from source):  aarch64-linux-gnu-g++


**Describe the problem**

I am trying to cross compile a binary to make use of a cross compiled libtensorflowlite.so library. However upon compilation I get the error
```
/home/aarch64/libtensorflowlite.so: undefined reference to `fcntl64@GLIBC_2.28'
```

I've followed the exact same steps using the native compiler (x86_64) and had no issues linking. 

Here is how I build from source:
```
bazel build --config=elinux_aarch64 --config=monolithic --cxxopt=--std=c++14 --define=with_select_tf_ops=true -c opt //tensorflow/lite:libtensorflowlite.so
```

Once I have my shared library, I link it to my inference code via:
```
aarch64-linux-gnu-g++ -std=c++11 hello_world.cpp -I/home/tensorflow/ -I/home/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include   -L/home/aarch64 -ltensorflowlite -lm -lpthread
```

However the above error occurs.
I'm wondering if its due to the steps in which I'm building from source. Specifically the `--config=elinux_aarch64` flag that I add for this cross-compilation case, and didn't have when I compiled natively. Otherwise is there a way to build that missing symbol into the library statically in compilation? Is this a tensorflow build issue? 

FYI: my system itself is running `Ubuntu GLIBC 2.27-3ubuntu1.2`."
41704,tf.ones produces zeros on GPU in an unclear scenario,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2
- Python version: 3.7.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1, 7
- GPU model and memory: 2080Ti 11GB + 1070 8GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
I was using my GPUs for another task, with near full memory taken. 
When using aside ipython, `tf.ones(())` (with any shape/dtype, except tf.int32 working as intented) produced 0s (like tf.zeros). I repeated this multiple times, in multiple instance of ipython. When setting running on CPU, tf.ones correctly produced 1s. After clearing the GPU memory, the issue disappeared, and now I'm unable to reproduce (for instance by filling my GPUs memories).

**Describe the expected behavior**
Output 1s, or an error

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
Cannot provide one, I'm not able to reproduce again

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
41703,Tensorflow C abruptly hangs at absl::synchronization_internal::Waiter::Wait(absl::synchronization_internal::KernelTimeout),"Hi All,

I'm using Tensorflow 1.15 C libraries for running Parallel Image Processing Computations in my cluster. The cluster consists of 8 worker nodes, each running 40 computation threads, where each of them makes use of tensorflow c libraries to process the incoming data.

All works fine, when suddenly the entire cluster gets abruptly hanged. On getting the GDB from my nodes, I'm seeing additional threads (apart from my pre-configured setup of 40 per node), which I'm assuming to be created by Tensorflow. The GDB shows that the threads have been abruptly blocked at **absl::synchronization_internal::Waiter::Wait(absl::synchronization_internal::KernelTimeout)** 

On viewing the system-level logs, I also see one of the nodes having kernel bug log stating

[598428.945633] BUG: kernel NULL pointer dereference, address: 
0000000000000038
  ...
  [598428.945749] Workqueue: cifsoplockd cifs_oplock_break [cifs]
  [598428.945793] RIP: 0010:smb2_push_mandatory_locks+0xd6/0x5a0 [cifs]
  ...
  [598428.945834] Call Trace:
  [598428.945870]  ? cifs_revalidate_mapping+0x45/0x90 [cifs]
  [598428.945901]  cifs_oplock_break+0x13d/0x450 [cifs]
  [598428.945909]  process_one_work+0x1db/0x380
  [598428.945914]  worker_thread+0x4d/0x400
  [598428.945921]  kthread+0x104/0x140
  [598428.945925]  ? process_one_work+0x380/0x380
  [598428.945931]  ? kthread_park+0x80/0x80
  [598428.945937]  ret_from_fork+0x35/0x40

After restarting the cluster, everything goes back to the way it was. But still, this issue keeps on occuring at random.
Can someone help me to understand and resolve this issue? Thanks.


**System information**

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): SLES 12
- TensorFlow installed from (source or binary): Downloaded C Binaries from official website
- TensorFlow version (use command below): 1.15
- GCC/Compiler version (if compiling from source): 6.4.0

**Logs** 
Sample GDB Thread Call Stack

Line 222195: [23] Thread 98 (Thread 0x7ef1b0fb1700 (LWP 36711)):
	Line 222196: [23] #0  0x00007efc2bc9a9f9 in syscall () from /lib64/libc.so.6
	Line 222197: [23] No symbol table info available.
	Line 222198: [23] #1  0x00007efc365aadc2 in absl::synchronization_internal::Waiter::Wait(absl::synchronization_internal::KernelTimeout) () from /usr/local/lib/libtensorflow.so
	Line 222203: [24]         __local = {<std::__shared_ptr<std::thread::_Impl_base, (__gnu_cxx::_Lock_policy)2>> = {_M_ptr = <optimized out>, _M_refcount = {_M_pi = 0x7f46400a7a50}}, <No data fields>}[23] No symbol table info available.
	Line 222204: [23] #2  0x00007efc365aad02 in AbslInternalPerThreadSemWait () from /usr/local/lib/libtensorflow.so
	Line 222209: [23] No symbol table info available.
	Line 222210: [23] #3  0x00007efc365ac2fd in absl::Mutex::Block(absl::base_internal::PerThreadSynch*) () from /usr/local/lib/libtensorflow.so
	Line 222211: [23] No symbol table info available.
	Line 222212: [23] #4  0x00007efc365ad1ed in absl::Mutex::AwaitCommon(absl::Condition const&, absl::synchronization_internal::KernelTimeout) () from /usr/local/lib/libtensorflow.so
	Line 222219: [23] No symbol table info available.
	Line 222220: [23] #5  0x00007efc365ad26d in absl::Mutex::Await(absl::Condition const&) () from /usr/local/lib/libtensorflow.so
	Line 222223: [23] No symbol table info available.
	Line 222224: [23] #6  0x00007efc32120f44 in stream_executor::host::HostStream::WorkLoop() () from /usr/local/lib/libtensorflow.so
	Line 222240: [23] No symbol table info available.
	Line 222241: [23] #7  0x00007efc2c75a810 in std::execute_native_thread_routine_compat (__p=<optimized out>) at ../../../.././libstdc++-v3/src/c++11/thread.cc:110
	Line 222498: [23]         __t = <optimized out>
	Line 222499: [23]         __local = {<std::__shared_ptr<std::thread::_Impl_base, (__gnu_cxx::_Lock_policy)2>> = {_M_ptr = <optimized out>, _M_refcount = {_M_pi = 0x7ef24c0f3270}}, <No data fields>}[23] 
	Line 222499: [23]         __local = {<std::__shared_ptr<std::thread::_Impl_base, (__gnu_cxx::_Lock_policy)2>> = {_M_ptr = <optimized out>, _M_refcount = {_M_pi = 0x7ef24c0f3270}}, <No data fields>}[23] 
	Line 222500: [23] #8  0x00007efc2d05c74a in start_thread () from /lib64/libpthread.so.0
	Line 222501: [23] No symbol table info available.
	Line 222502: [23] #9  0x00007efc2bc9ef6d in clone () from /lib64/libc.so.6
"
41702,I cant install/import tensorflow,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 1904
- TensorFlow version: Trying to install version 2.2
- Python version: 3.8.2
- Installed using virtualenv? pip? conda?: trying to install with pip
- CUDA/cuDNN version: 11.0
- GPU model and memory: Its a laptop, that has both an Intel(R) UHD Graphics 630 and a Nvidia GTX 1060


**Describe the problem**
I get an error when trying to pip install tensorflow, however, I assumed that it was because I am using python 3.8, so I tried doing a special install with: `python -m pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.12.0-py3-none-any.whl` and that installed it, however when running a script that simply imports tensorflow, I get the error: `No module named '_pywrap_tensorflow_internal'` I know that it is tensorflow 1.12, but when I uninstalled and tried to do a special install of 2.2, it didn't work. Sorry if I'm just being stupid here but I cant figure it out.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
ran `python -m pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.12.0-py3-none-any.whl` in the cmd and when I try to import tensorflow I get the error `No module named '_pywrap_tensorflow_internal'`.

**Any other info / logs**
The entire traceback:
```
Traceback (most recent call last):
  File ""C:\Users\LORI\AppData\Local\Programs\Python\Python38-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])
  File ""C:\Users\LORI\AppData\Local\Programs\Python\Python38-32\lib\imp.py"", line 296, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\LORI\AppData\Local\Programs\Python\Python38-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\LORI\AppData\Local\Programs\Python\Python38-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\LORI\AppData\Local\Programs\Python\Python38-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow_internal
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\LORI\Desktop\Python Projects\Tensor Flow Project\main.py"", line 1, in <module>
    import tensorflow
  File ""C:\Users\LORI\AppData\Local\Programs\Python\Python38-32\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\LORI\AppData\Local\Programs\Python\Python38-32\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\LORI\AppData\Local\Programs\Python\Python38-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\LORI\AppData\Local\Programs\Python\Python38-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])
  File ""C:\Users\LORI\AppData\Local\Programs\Python\Python38-32\lib\imp.py"", line 296, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\LORI\AppData\Local\Programs\Python\Python38-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\LORI\AppData\Local\Programs\Python\Python38-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\LORI\AppData\Local\Programs\Python\Python38-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow_internal
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'
```
"
41699,Golang: SessionOptions & ConfigProto opaqueness & inaccessibility,"**System information**
- TensorFlow version (you are using): 1.15.0 (will move to stable 2.3.0 when it is available)
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**
The Go bindings of TensorFlow have a somewhat awkward API when it comes to setting the session options: as per the [docs](https://pkg.go.dev/github.com/tensorflow/tensorflow@v1.15.0/tensorflow/go?tab=doc#SessionOptions), you can configure it, but you have to input a binary-serialized [ConfigProto](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/config.proto) message to do so. 
However, you cannot actually create a ConfigProto by calling the Go TF bindings - you have to create it in another way, [as demonstrated in the source code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/go/session_test.go#L366). 
Creating the ConfigProto message by using Python is extremely impractical and unnecessarily opaque. I would urge the authors to reconsider using protocol buffers in the Go bindings to allow creating the ConfigProto message from Go, thus making this flow much simpler and user-transparent.

**Will this change the current API? How?**
This will introduce a dependency on protocol buffers in the Go bindings and add the generated protobuf structs to the API.

**Who will benefit with this feature?**
Users of the TensorFlow Go implementation with a workload that demands more configuration for effective inference.

"
41698,tf.signal.dct very slow as compared to scipy.dct even on GPU,"Platform: Google Colab with GPU runtime
Tensorflow: version 2.2

`from scipy.fftpack import dct
import tensorflow as tf
import numpy as np
import time

xx=np.random.random_sample((8192,)).astype(np.float32)

start = time.time()
for i in range(1000):
   	x_dct=dct(xx,2,norm='ortho')
print('scipy:{}s'.format(time.time() - start))

start = time.time()
x_tensor = tf.convert_to_tensor(xx)
for i in range(1000):
   	x_dct=tf.signal.dct(x_tensor,2, norm='ortho')
print('tf2:{}s'.format(time.time() - start))`

`scipy: 0.08372211456298828s
tf2: 2.0871903896331787s`

Why TF2 is too slow compared to scipy even on GPU runtime. Is is related to slow backend c++ FFT libraries?




"
41697,converter.inference_input_type = tf.int8 is been ignored,"**System information**
- Docker image tensorflow/tensorflow:2.2.0 
- Same issue with Windows python 3 and tensorflow 2.2.0 installed via pip

**Command used to run the converter or code if you’re using the Python API**

Especially the converter.inference_input_type and converter.inference_output_type is imporant.
```
k_model = tf.keras.models.load_model(model_path)
converter = tf.lite.TFLiteConverter.from_keras_model(k_model)
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8

def representative_data_gen():
    for input_value in data_set:
        yield [input_value.astype(np.float32).reshape((1, 10))]

converter.representative_dataset = representative_data_gen

tf_lite_model_quant = converter.convert()
```

**The output from the converter invocation**

The output of the convertion is the same as without specifying the inference_input and output_type. 
The tflite outputfiles with and without the specification of the inference input type are attached. 
[tflite_conv_test.zip](https://github.com/tensorflow/tensorflow/files/4971633/tflite_conv_test.zip)



**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
- I expect a model without the qunatize and the dequantize layer at the beginning and at the end. 
- The generated model has a infernece_input_type of float32 not the expected int8 

**Any other info / logs**

With tensorflow 1.15 the inference_input was int8 of the generated model when specifing the inference input type. Also no quantization or deqauntization layer were putted in the generated model. "
41696,Not able to create libtensorflow-lite.a from build_ios_universal_lib.sh,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac 10.15.4
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO
- TensorFlow installed from (source or binary): 
- TensorFlow version (use command below): 2.1.1
- Python version: Python 2.7.16 and Python 3.7.3
- Bazel version (if compiling from source): bazel 3.4.1-homebrew
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


**Describe the current behavior**
I was able to create libtensorflow-lite.a on my mac for Tensorflow version 1.8.0 before.
Now, I am trying to do the same for version 2.1.0 but it's giving the following error 

```
clang: error: no such file or directory: 'i386'
clang: warning: no such sysroot directory: '-arch' [-Wmissing-sysroot]
clang: error: no such file or directory: 'i386'
clang: warning: no such sysroot directory: '-arch' [-Wmissing-sysroot]
make: *** [/Users/myMac/TFLite_2_1/tensorflow/lite/tools/make/gen/ios_i386/obj/tensorflow/lite/core/api/flatbuffer_conversions.o] Error 1
make: *** Waiting for unfinished jobs....
make: *** [/Users/myMac/TFLite_2_1/tensorflow/lite/tools/make/gen/ios_i386/obj/tensorflow/lite/allocation.o] Error 1
clang: clang: error: no such file or directory: 'i386'
error: no such file or directory: 'i386'
clang: warning: no such sysroot directory: '-arch' [-Wmissing-sysroot]
clang: warning: no such sysroot directory: '-arch' [-Wmissing-sysroot]
clang: error: no such file or directory: 'i386'
clang: warning: no such sysroot directory: '-arch' [-Wmissing-sysroot]
make: *** [/Users/myMac/TFLite_2_1/tensorflow/lite/tools/make/gen/ios_i386/obj/tensorflow/lite/arena_planner.o] Error 1
make: *** [/Users/myMac/TFLite_2_1/tensorflow/lite/tools/make/gen/ios_i386/obj/tensorflow/lite/core/api/error_reporter.o] Error 1
clang: error: no such file or directory: 'i386'
clang: warning: no such sysroot directory: '-arch' [-Wmissing-sysroot]
clang: error: no such file or directory: 'i386'
clang: warning: no such sysroot directory: '-arch' [-Wmissing-sysroot]
make: *** [/Users/myMac/TFLite_2_1/tensorflow/lite/tools/make/gen/ios_i386/obj/tensorflow/lite/core/api/op_resolver.o] Error 1
clang: error: no such file or directory: 'i386'
clang: warning: no such sysroot directory: '-arch' [-Wmissing-sysroot]
make: *** [/Users/myMac/TFLite_2_1/tensorflow/lite/tools/make/gen/ios_i386/obj/tensorflow/lite/c/c_api_internal.o] Error 1
make: *** [/Users/myMac/TFLite_2_1/tensorflow/lite/tools/make/gen/ios_i386/obj/tensorflow/lite/core/api/tensor_utils.o] Error 1
make: *** [/Users/myMac/TFLite_2_1/tensorflow/lite/tools/make/gen/ios_i386/obj/tensorflow/lite/core/subgraph.o] Error 1
```

**Standalone code to reproduce the issue**
1. Have downloaded the code from https://github.com/tensorflow/tensorflow/tree/r2.1
2. First, run download_dependencies.sh script and then build_ios_universal_lib.sh.

**Facing the same issue in ver 2.2. I have also tried for ver 2.3 but it showed warning ""This build script is deprecated...""**"
41695,Keras Metric Multiple Outputs / Inputs,"**System information**
- TensorFlow version (you are using): 2.2.0
- Are you willing to contribute it (Yes/No): No


**Describe the feature and the current behavior/state.**
Keras custom `metric` should optionally get as input all inputs and all outputs of the same batch.
This will allow creating metrics that are dependent on 2 or more outputs, or dependent on the input, etc.

**Will this change the current api? How?**
Yes, a metric will have a property `use_all_inputs` or something like that

**Who will benefit with this feature?**
Anyone who is looking into custom metrics.

**Any Other info.**
It is possible to do something similar with a callback, but that requires running `predict` twice per sample per epoch, and does not work for `model.evaluate`.


Related issues:

- [https://github.com/keras-team/keras/issues/4506](https://github.com/keras-team/keras/issues/4506)
- [https://stackoverflow.com/questions/63068206/keras-metric-for-multiple-outputs](https://stackoverflow.com/questions/63068206/keras-metric-for-multiple-outputs)
- [https://stackoverflow.com/questions/63058307/tensorflow-dataset-mask-sequence-for-evaluation](https://stackoverflow.com/questions/63058307/tensorflow-dataset-mask-sequence-for-evaluation)
- [https://datascience.stackexchange.com/questions/54443/keras-custom-metric-function-how-to-feed-2-model-outputs-to-a-single-metric-eval](https://datascience.stackexchange.com/questions/54443/keras-custom-metric-function-how-to-feed-2-model-outputs-to-a-single-metric-eval)"
41694,tf.lite.TFLiteConverter crashes when converting Keras model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX 10.15.5- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.12.1-37224-ga6cd18a133 2.4.0-dev20200722
- Python version: 3.7

**Describe the current behavior**
It throws exception
![截圖 2020-07-24 下午3 37 41](https://user-images.githubusercontent.com/4080524/88370104-f6229600-cdc3-11ea-8d7b-4f839cb5ac04.jpg)


**Describe the expected behavior**
It should finish the conversion successfully

**Standalone code to reproduce the issue**
```
import numpy as np
import tensorflow as tf

model = tf.keras.models.Sequential([
    tf.keras.layers.Input(shape=(28, 28), name='input'),
    tf.keras.layers.Bidirectional(
        tf.keras.layers.LSTM(20, return_sequences=True)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(10, activation=tf.nn.softmax, name='output')
])
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
model.summary()

run_model = tf.function(lambda x: model(x))
# This is important, let's fix the input size.
BATCH_SIZE = 1
STEPS = 28
INPUT_SIZE = 28
concrete_func = run_model.get_concrete_function(
    tf.TensorSpec([BATCH_SIZE, STEPS, INPUT_SIZE], model.inputs[0].dtype))

# model directory.
MODEL_DIR = ""keras_lstm""

# converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])
# converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.experimental_new_converter = True
tflite_model = converter.convert()

with tf.io.gfile.GFile('tflite_test.tflite', 'wb') as f:
    f.write(tflite_model)
```"
41800,Converting MediaPipe Handpose model to TFLite model,"HI team,

I am developing an embedded IOT device and I wanted to use the Handpose model for hand-tracking and gesture recognition but I didnt yet find a guide for converting Mediapipe models to TFLite model. Could you please provide a guide if its possible.

Thanks
"
41693,InaccessibleTensorError in custom Model using add_loss and build,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
MacOSX 10.15.5
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
v2.2.0-rc4-8-g2b96f3662b 2.2.0
- Python version:
3.7.6
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**

When running the following code, which uses `add_loss()` *and* creates a layer in the `build()` method, I get an `InaccessibleTensorError`:

```python
import tensorflow as tf
from tensorflow import keras

class MyModel(keras.models.Model):
    def build(self, batch_input_shape):
        self.output_layer = keras.layers.Dense(1)
        super().build(batch_input_shape)

    def call(self, inputs, training=None):
        self.add_loss(1.0)
        return self.output_layer(inputs)

model = MyModel()
model.compile(loss=""mse"", optimizer=""nadam"")

X = tf.random.uniform((100, 10))
y = tf.random.uniform((100, 1))
history = model.fit(X, y, epochs=2)
```

**Describe the expected behavior**

I expect the model to be trained normally, without error.

**Standalone code to reproduce the issue**

See the code above. You can run it in this colab: https://colab.research.google.com/drive/1c_NBNJ2vKt412WSg1HiPUNYjVmf1--YL

**Other info / logs**

Here is the full stacktrace:

```
InaccessibleTensorError: in user code:

    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:571 train_function  *
        outputs = self.distribute_strategy.run(
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:533 train_step  **
        y, y_pred, sample_weight, regularization_losses=self.losses)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:231 __call__
        reg_loss = math_ops.add_n(regularization_losses)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:180 wrapper
        return target(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3239 add_n
        return gen_math_ops.add_n(inputs, name=name)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:420 add_n
        ""AddN"", inputs=inputs, name=name)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:744 _apply_op_helper
        attrs=attr_protos, op_def=op_def)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:591 _create_op_internal
        inp = self.capture(inp)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:641 capture
        % (tensor, tensor.graph, self))

    InaccessibleTensorError: The tensor 'Tensor(""Const:0"", shape=(), dtype=float32)' cannot be accessed here: it is defined in another function or code block. Use return values, explicit Python locals or TensorFlow collections to access it. Defined in: FuncGraph(name=build_graph, id=139933898535488); accessed from: FuncGraph(name=train_function, id=139933898618920).
```
"
41692,"assert padding in {'same', 'valid', 'full'} AssertionError","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): tf-nightly
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

I'm using Conv1DTranspose and I set `padding` to `causal` which is only valid for Conv1D but it fails and gives the assertion above.

**Describe the expected behavior**
 Unless I'm very much mistaken this assertion should be updated.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
41691,ImportError: cannot import name py_checkpoint_reader,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
41690,SPLIT_V for tensorflow lite micro,Currently SPLIT_V is not supported in tensorflow lite micro as a builtin op code. My model uses SPLIT_V. I need help with either preventing tflite conversion from generating SPLIT_V or how can I add it as a builtin op?
41686,'WeightSharedConvolutionalBoxPredictor' object has no attribute 'inputs',"Hello. I have updated tf1 to the second version. Downloaded ssd_resnet152_v1_fpn_1024x1024_coco17_tpu-8 and configured config. But I get the error
`Traceback (most recent call last):
  File ""model_main.py"", line 108, in <module>
    tf.app.run()
  File ""C:\ProgramData\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\platform\app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""C:\ProgramData\Anaconda3\envs\tensorflow1\lib\site-packages\absl\app.py"", line 299, in run
    _run_main(main, args)
  File ""C:\ProgramData\Anaconda3\envs\tensorflow1\lib\site-packages\absl\app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""model_main.py"", line 104, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""C:\ProgramData\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow_estimator\python\estimator\training.py"", line 472, in train_and_evaluate
    return executor.run()
  File ""C:\ProgramData\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow_estimator\python\estimator\training.py"", line 613, in run
    return self.run_local()
  File ""C:\ProgramData\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow_estimator\python\estimator\training.py"", line 714, in run_local
    saving_listeners=saving_listeners)
  File ""C:\ProgramData\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow_estimator\python\estimator\estimator.py"", line 349, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""C:\ProgramData\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow_estimator\python\estimator\estimator.py"", line 1182, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""C:\ProgramData\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow_estimator\python\estimator\estimator.py"", line 1211, in _train_model_default
    self.config)
  File ""C:\ProgramData\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow_estimator\python\estimator\estimator.py"", line 1170, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""C:\tensorflow1\models\research\object_detection\model_lib.py"", line 543, in model_fn
    update_ops=detection_model.updates(),
  File ""C:\tensorflow1\models\research\object_detection\meta_architectures\ssd_meta_arch.py"", line 1350, in updates
    self._box_predictor.inputs))
AttributeError: 'WeightSharedConvolutionalBoxPredictor' object has no attribute 'inputs'`

`model {
  ssd {
    num_classes: 12
    image_resizer {
      fixed_shape_resizer {
        height: 1024
        width: 1024
      }
    }
    feature_extractor {
      type: ""ssd_resnet152_v1_fpn_keras""
      depth_multiplier: 1.0
      min_depth: 16
      conv_hyperparams {
        regularizer {
          l2_regularizer {
            weight: 0.00039999998989515007
          }
        }
        initializer {
          truncated_normal_initializer {
            mean: 0.0
            stddev: 0.029999999329447746
          }
        }
        activation: RELU_6
        batch_norm {
          decay: 0.996999979019165
          scale: true
          epsilon: 0.0010000000474974513
        }
      }
      override_base_feature_extractor_hyperparams: true
      fpn {
        min_level: 3
        max_level: 7
      }
    }
    box_coder {
      faster_rcnn_box_coder {
        y_scale: 10.0
        x_scale: 10.0
        height_scale: 5.0
        width_scale: 5.0
      }
    }
    matcher {
      argmax_matcher {
        matched_threshold: 0.5
        unmatched_threshold: 0.5
        ignore_thresholds: false
        negatives_lower_than_unmatched: true
        force_match_for_each_row: true
        use_matmul_gather: true
      }
    }
    similarity_calculator {
      iou_similarity {
      }
    }
    box_predictor {
      weight_shared_convolutional_box_predictor {
        conv_hyperparams {
          regularizer {
            l2_regularizer {
              weight: 0.00039999998989515007
            }
          }
          initializer {
            random_normal_initializer {
              mean: 0.0
              stddev: 0.009999999776482582
            }
          }
          activation: RELU_6
          batch_norm {
            decay: 0.996999979019165
            scale: true
            epsilon: 0.0010000000474974513
          }
        }
        depth: 256
        num_layers_before_predictor: 4
        kernel_size: 3
        class_prediction_bias_init: -4.599999904632568
      }
    }
    anchor_generator {
      multiscale_anchor_generator {
        min_level: 3
        max_level: 7
        anchor_scale: 4.0
        aspect_ratios: 1.0
        aspect_ratios: 2.0
        aspect_ratios: 0.5
        scales_per_octave: 2
      }
    }
    post_processing {
      batch_non_max_suppression {
        score_threshold: 9.99999993922529e-09
        iou_threshold: 0.6000000238418579
        max_detections_per_class: 100
        max_total_detections: 100
        use_static_shapes: false
      }
      score_converter: SIGMOID
    }
    normalize_loss_by_num_matches: true
    loss {
      localization_loss {
        weighted_smooth_l1 {
        }
      }
      classification_loss {
        weighted_sigmoid_focal {
          gamma: 2.0
          alpha: 0.25
        }
      }
      classification_weight: 1.0
      localization_weight: 1.0
    }
    encode_background_as_zeros: true
    normalize_loc_loss_by_codesize: true
    inplace_batchnorm_update: true
    freeze_batchnorm: false
  }
}
train_config {
  batch_size: 1
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
  data_augmentation_options {
    random_crop_image {
      min_object_covered: 0.0
      min_aspect_ratio: 0.75
      max_aspect_ratio: 3.0
      min_area: 0.75
      max_area: 1.0
      overlap_thresh: 0.0
    }
  }
  sync_replicas: true
  optimizer {
    momentum_optimizer {
      learning_rate {
        cosine_decay_learning_rate {
          learning_rate_base: 0.03999999910593033
          total_steps: 100000
          warmup_learning_rate: 0.013333000242710114
          warmup_steps: 2000
        }
      }
      momentum_optimizer_value: 0.8999999761581421
    }
    use_moving_average: false
  }
  fine_tune_checkpoint: ""C:/tensorflow1/models/research/object_detection/ssd_resnet152_v1_fpn_1024x1024_coco17_tpu-8\checkpoint\ckpt-0""
  num_steps: 100000
  startup_delay_steps: 0.0
  replicas_to_aggregate: 8
  max_number_of_boxes: 100
  unpad_groundtruth_tensors: false
  fine_tune_checkpoint_type: ""classification""
  use_bfloat16: true
  fine_tune_checkpoint_version: V2
}
train_input_reader {
  label_map_path: ""C:/tensorflow1/models/research/object_detection/training/labelmap.pbtxt""
  tf_record_input_reader {
    input_path: ""C:/tensorflow1/models/research/object_detection/train.record""
  }
}
eval_config {
  metrics_set: ""coco_detection_metrics""
  use_moving_averages: false
}
eval_input_reader {
  label_map_path: ""C:/tensorflow1/models/research/object_detection/training/labelmap.pbtxt""
  shuffle: false
  num_epochs: 1
  tf_record_input_reader {
    input_path: ""C:/tensorflow1/models/research/object_detection/test.record""
  }
}
`

`(tensorflow1) C:\tensorflow1\models\research\object_detection>python model_main.py --logtostderr --train_dir=training/ --pipeline_config_path=ssd_resnet152_v1_fpn_1024x1024_coco17_tpu-8/pipeline.config --model_dir=training/`"
41685,image_dataset_from_directory(label_mode=None) dosen't work.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow version (use command below): tf-nightly-gpu 
v1.12.1-37350-g537f3ec52f 
2.4.0-dev20200723
- Python version: 3.8
- GPU model and memory: Nvidia GTX 1080Ti 11GB

**Describe the current behavior**
Using the code provided below results in:

> Found 0 files belonging to 0 classes.
> Using 0 files for training.

And at the end of the output:
> TypeError: Input 'filename' of 'ReadFile' Op has type float32 that does not match expected type of string.

**Describe the expected behavior**
As described in the documentation, expected to return a tensor of shape=[n,h,w,c].

> _If label_mode is None, it yields float32 tensors of shape (batch_size, image_size[0], image_size[1], num_channels), encoding images (see below for rules regarding num_channels)._


**Standalone code to reproduce the issue**
```
from tensorflow.keras.preprocessing import image_dataset_from_directory
train_data = image_dataset_from_directory(directory='/folder/with/images', label_mode=None)
```

**Other info / logs** Include any logs or source code that would be helpful to
[Traceback.txt](https://github.com/tensorflow/tensorflow/files/4969755/Traceback.txt)"
41684,Weighted CE and BCE,"
**System information**
- TensorFlow version (you are using): 2.0
- Are you willing to contribute it (Yes/No):
probably not


**Describe the feature and the current behavior/state.**
For now, loss function such as BCE or CE (Cross Entropy) can not use a specific weights for each class. For instance this feature is possible on pytorch. The way to do it is to use the class_weight option in the .fit method. However, it seems more appropriate to be able to use it through the loss function. 

**Will this change the current api? How?**
I do not know
**Who will benefit with this feature?**
Everyone, especially when using multiple outpu,  it is not possible to use class weight in that specific case.
**Any Other info.**
"
41683,Git Pull doesn't work in building from source for GPU,"**System information**
Ubuntu 20.04
TensorFlow installed from ""docker pull tensorflow/tensorflow:devel-gpu""
- TensorFlow version: 2.4?
- Python version: 3.8.2 (installed on system but what's running in the container I don't know
- Not sure but believe it's Pip
- Bazel version (if compiling from source): Don't know
- GCC/Compiler version (if compiling from source): don't know
- CUDA/cuDNN version: 10.1
- GPU model and memory: 3  GeForce GTX 1070s with 8GB each



**Describe the problem**
In following the instructions on the bottom of the page for compiling TensorFlow from source for GPU. I ran across a problem with the git pull. when I run the docker container I end up in the /tensorflow directory. But there's no .git file there. But there is one in the /tensorflow_src directory. Either you need to change the docker run command to end up in that directory or I suggest you add a ""cd ../tensorflow_src"" command before the ""git pull"" command. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
docker pull tensorflow/tensorflow:devel-gpu
docker run --gpus all -it -w /tensorflow -v $PWD:/mnt -e HOST_PERMS=""$(id -u):$(id -g)"" \
    tensorflow/tensorflow:devel-gpu bash
git pull  # within the container, download the latest source code

**Any other info / logs**
N/A"
41682,"ImportError: dlopen(.../_pywrap_tensorflow_internal.so, 6): Symbol not found: __ZN10tensorflow4data12experimental13LoadDatasetOp11kReaderFuncE","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.2.0
- Python version: 3.7.7
- Installed using virtualenv? pip? conda?: virtualenv, conda
- Bazel version (if compiling from source): 3.4.1
- GCC/Compiler version (if compiling from source): GCC 4.2.1
- CUDA/cuDNN version: NA
- GPU model and memory: Radeon RX 580 8GB installed, but want CPU only version

**Describe the problem**
I am running into the described error approximately 45 minutes into the build of the pip package. I believe the specific problem is: ImportError: dlopen(.../_pywrap_tensorflow_internal.so, 6): Symbol not found: __ZN10tensorflow4data12experimental13LoadDatasetOp11kReaderFuncE. Any help would be greatly appreciated, thanks!

I am trying to build tensorflow from source as my CPU spec is old. My aim is to get a working version of tensorflow 2.x, CPU optimisation would be great but not necessary as I can utilise my GPU for tasks using PlaidML.
My CPU:
Intel Xeon x5670 2.93Ghz
MMX instructions
SSE / Streaming SIMD Extensions
SSE2 / Streaming SIMD Extensions 2
SSE3 / Streaming SIMD Extensions 3
SSSE3 / Supplemental Streaming SIMD Extensions 3
SSE4 / SSE4.1 + SSE4.2 / Streaming SIMD Extensions 4

**Provide the exact sequence of commands / steps that you executed before running into the problem**
# Check whether script is executing in a VirtualEnv or Conda environment
if [ -z ""$VIRTUAL_ENV"" ] && [ -z ""$CONDA_PREFIX"" ] ; then
	echo ""VirtualEnv or Conda env is not activated""
	exit -1
fi

# Set the virtual environment path
if ! [ -z ""$VIRTUAL_ENV"" ] ; then
  VENV_PATH=$VIRTUAL_ENV
elif ! [ -z ""$CONDA_PREFIX"" ] ; then
  VENV_PATH=$CONDA_PREFIX
fi

# Set the bin and lib directories
VENV_BIN=$VENV_PATH/bin
VENV_LIB=$VENV_PATH/lib

# bazel tf needs these env vars
export PYTHON_BIN_PATH=$VENV_BIN/python
export PYTHON_LIB_PATH=`ls -d $VENV_LIB/*/ | grep python`

# Set the native architecture optimization flag, which is a default
COPT=""--copt=-march=native""

# Determine the available features of your CPU
raw_cpu_flags=`sysctl -a | grep machdep.cpu.features | cut -d "":"" -f 2 | tr '[:upper:]' '[:lower:]'`

# Append each of your CPU's features to the list of optimization flags
for cpu_feature in $raw_cpu_flags
do
	case ""$cpu_feature"" in
		""sse4.1"" | ""sse4.2"" | ""ssse3"" | ""fma"" | ""cx16"" | ""popcnt"" | ""maes"")
		    COPT+="" --copt=-m$cpu_feature""
		;;
		""avx1.0"")
		    COPT+="" --copt=-mavx""
		;;
		*)
		;;
	esac
done
bazel clean

# Run TensorFlow configuration (accept defaults unless you have a need)
./configure

# Accepts default options for configuration when prompted

# Build the TensorFlow pip package
bazel build -c opt $COPT -k //tensorflow/tools/pip_package:build_pip_package

**Any other info / logs**
...
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (385 packages loaded, 31018 targets configured).
INFO: Found 1 target...
INFO: Deleting stale sandbox base /private/var/tmp/_bazel_alexanderjenkins/b4dba3332782448704ff09db5b64eb10/sandbox
ERROR: /Users/alexanderjenkins/tensorflow/tensorflow/BUILD:971:1: Couldn't build file tensorflow/_api/v2/v2.py: Executing genrule //tensorflow:tf_python_api_gen_v2 failed (Exit 1)
Traceback (most recent call last):
  File ""/private/var/tmp/_bazel_alexanderjenkins/b4dba3332782448704ff09db5b64eb10/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: dlopen(/private/var/tmp/_bazel_alexanderjenkins/b4dba3332782448704ff09db5b64eb10/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: __ZN10tensorflow4data12experimental13LoadDatasetOp11kReaderFuncE
  Referenced from: /private/var/tmp/_bazel_alexanderjenkins/b4dba3332782448704ff09db5b64eb10/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so
  Expected in: flat namespace
 in /private/var/tmp/_bazel_alexanderjenkins/b4dba3332782448704ff09db5b64eb10/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/private/var/tmp/_bazel_alexanderjenkins/b4dba3332782448704ff09db5b64eb10/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 26, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""/private/var/tmp/_bazel_alexanderjenkins/b4dba3332782448704ff09db5b64eb10/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 40, in <module>
    from tensorflow.python.eager import context
  File ""/private/var/tmp/_bazel_alexanderjenkins/b4dba3332782448704ff09db5b64eb10/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/eager/context.py"", line 35, in <module>
    from tensorflow.python import pywrap_tfe
  File ""/private/var/tmp/_bazel_alexanderjenkins/b4dba3332782448704ff09db5b64eb10/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tfe.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/private/var/tmp/_bazel_alexanderjenkins/b4dba3332782448704ff09db5b64eb10/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 83, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/private/var/tmp/_bazel_alexanderjenkins/b4dba3332782448704ff09db5b64eb10/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: dlopen(/private/var/tmp/_bazel_alexanderjenkins/b4dba3332782448704ff09db5b64eb10/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: __ZN10tensorflow4data12experimental13LoadDatasetOp11kReaderFuncE
  Referenced from: /private/var/tmp/_bazel_alexanderjenkins/b4dba3332782448704ff09db5b64eb10/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so
  Expected in: flat namespace
 in /private/var/tmp/_bazel_alexanderjenkins/b4dba3332782448704ff09db5b64eb10/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so

"
41681,BroadcastTo,"MacBook Pro

**Provide the text output from tflite_convert**

```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, AVERAGE_POOL_2D, CONV_2D, DEPTHWISE_CONV_2D, DIV, FULLY_CONNECTED, HARD_SWISH, MAXIMUM, MEAN, MINIMUM, MUL, PACK, RESHAPE, SHAPE, SOFTMAX, STRIDED_SLICE, SUB. Here is a list of operators for which you will need custom implementations: BroadcastTo.
Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.7/bin/toco_from_protos"", line 10, in <module>
    sys.exit(main())
  File ""/Users/z004njq/Library/Python/3.7/lib/python/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 93, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""/Users/z004njq/Library/Python/3.7/lib/python/site-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/Users/z004njq/Library/Python/3.7/lib/python/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 56, in execute
    enable_mlir_converter)
Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, AVERAGE_POOL_2D, CONV_2D, DEPTHWISE_CONV_2D, DIV, FULLY_CONNECTED, HARD_SWISH, MAXIMUM, MEAN, MINIMUM, MUL, PACK, RESHAPE, SHAPE, SOFTMAX, STRIDED_SLICE, SUB. Here is a list of operators for which you will need custom implementations: BroadcastTo.```

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
"
41679,bug in Using the SavedModel format Guide,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

Ubuntu 18.04, system with two GPUs, tensorflow 2.2

### Describe the problem

The guide, near the top, includes the following:

physical_devices = tf.config.experimental.list_physical_devices('GPU')
if physical_devices:
  tf.config.experimental.set_memory_growth(physical_devices[0], True)

and on a multi-GPU system, this will lead to problems later because memory growth will only be managed on one of the GPUs.  It should instead be:

physical_devices = tf.config.experimental.list_physical_devices('GPU')
for device in physical_devices:
  tf.config.experimental.set_memory_growth(device, True)

I am aware that this is really a minor documentation issue, and would prefer to simply upload the fix myself.  Unfortunately, I am not willing to sign the Contributor License Agreement.  I have many friends at Google with whom I have many technical discussions.  I don't want those discussions to automatically grant licenses to Google if I forget to say ""not a contribution"" at the beginning of them.
### Source code / logs

"
41675,Allow slots to be initialized with different shapes + values than primary var in Keras optimizer,"**System information**
- TensorFlow version (you are using): 2.2.0
- Are you willing to contribute it (Yes/No): probably not

**Describe the feature and the current behavior/state.**
Currently the `add_slot` method in the optimizer class [(link)](https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L694-L730) only allows you to create slots which are the same shape as the original var, and you cannot set it to a custom value (i.e. it forces you to use an initializer).

**Will this change the current api? How?**
Either `add_slot` should be modified to include this functionality, or there should be a tf.keras equivalent to the old `_get_or_make_slots` method which gives the developer the option to pass in a value for the slot to be initialized with (which may be a different shape than the var). 

**Who will benefit with this feature?**
Anyone implementing custom optimizers which require slots which may be custom values or shapes (i.e. the SM3 optimizer creates accumulators which are differently shaped than their corresponding vars).

**Any Other info.**
This feature seems to be present in the base optimizer class via `_get_or_make_slot` which calls [create_slot()](https://github.com/tensorflow/tensorflow/blob/d8dcead44017aa0381ca16254a161099eeb7c2e4/tensorflow/python/training/slot_creator.py#L104), which allows you to pass in an initial value."
41674,TensorFlow Lite for Microcontrollers sigaborts with a MobileNetV2 alpha=0.1 model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.15.5, Linux
- GCC/Compiler version (if compiling from source): Apple clang version 11.0.3 (clang-1103.0.32.62) c++1

**Describe the current behavior**

I am using TensorFlow Lite for Microcontrollers at commit 4f69f62c61ecf3cd23286324af62d00643186ec2.

I've trained two MobileNetV2 models in Keras with 48x48 input size and a single input channel, then converted to int8 quantized.

I am attempting to run both models using TensorFlow Lite for Microcontrollers on x86, built with clang on MacOS and with gcc on Ubuntu.

The first model has a MobileNetV2 filter scaling factor (_a_) of 0.35. This model runs perfectly.

The second model has a scaling factor of 0.1. This model sigaborts during the `Invoke()` call.

Strangely, both models run perfectly when executed from the OpenMV H7+ (Arm Cortex-M7), and the smaller model runs perfectly on the H7. It might be worth noting that on the OpenMV devices the model is stored in dynamic memory. That said, I've tried declaring the model without static on x86 and it has no impact.

I've attached zips containing both model files, plus an example program that exhibits the sigabort.

To build and run the example with an empty input:

```
make -f Makefile.tflite
./build/edge-impulse-standalone """"
```

Within the example code, the call to `Invoke()` is on line 260 of `edge-impulse-sdk/classifier/ei_run_classifier.h`.

To switch to the 0.35 model, which doesn't sigabort, replace `tflite-model`, `model-parameters`, and `edge-impulse-sdk` directories with the versions contained within `0.35 grayscale.zip`.

**Describe the expected behavior**

The _a_=0.1 model should run successfully on x86, the same as the 0.35 does.

**Standalone code to reproduce the issue**
Example code here:

[example-standalone-inferencing.zip](https://github.com/tensorflow/tensorflow/files/4968485/example-standalone-inferencing.zip)

The `.lite` model files are located here:
[models.zip](https://github.com/tensorflow/tensorflow/files/4968496/models.zip)
"
41671,"Problems with Transformations when trying to model.fit() my 2D CNN Model, leading to two exceptions","**System information**
- Have I written custom code [posted below]:
- Microsoft Windows [Version 10.0.18363.959]
- TensorFlow v2.2.0-rc4-8-g2b96f3662b 2.2.0 installed via pip. Cuda 10.1 with CUDNN 10.1. GPU enabled.
- Python version: 3.7
- GPU model and memory: GeForce GTX 1650 with Max-Q Design computeCapability: 7.5
coreClock: 1.245GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.34GiB/s
-Done in Jupyter Notebook

While training my model using the model.fit() method, my generator inputs the first image slice (with corresponding segmentation truth). During that process, I get 2 exceptions, which ill post after my code. I believe the issue has to do during the flatten method, but I do not know how to address that change.

**Standalone code to reproduce the issue**
```
import ctypes
#hllDll = ctypes.WinDLL(""C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\bin\\cudart64_101.dll"")
#incase cudart64_101.dll is unable to be loaded
import os
import numpy as np
from medpy.io import load
import tensorflow as tf
import tensorflow.keras as tfk
from tensorflow.keras import layers
import keras
from keras.utils.vis_utils import plot_model
import sklearn
from scipy.ndimage import rotate
import scipy
from sklearn import datasets, preprocessing, model_selection, metrics
import matplotlib.pyplot as plt
%matplotlib inline
```


```
#Image dict is just a dictionary of paths to .mhd files for images and segmentation truths    

def image_preprocessing(array):
    logging.info(""Beginning Preprocessing"")
    
    logging.info(""Rotating Image into axial plane"")
    array = rotate(array, 90, axes=(0,2), reshape=True) #rotates to axial form sup to inf
    
    logging.info(""Making image to uniform size"")
    
    blank_array = np.full((1,512,512),-3024) #-3024 is the ""background, out of zoom data point""
    
    while len(array) < 69:
        array = np.append(array, blank_array, axis = 0)
        array = np.append(blank_array, array, axis = 0)
        
    if len(array) == 69:
        array = np.append(array,blank_array, axis = 0)
    else:
        pass
    
    logging.info(""Image Preprocessing is complete"")
    
    return array

def image_generator(img_dict, mask):
    
    include_mask = False
    
    logging.info(""creating generator for {}."".format(img_dict))
    
    if mask == True:
        include_mask = True
    
    if include_mask == True:
        
        for path1, path2 in zip(img_dict['image'],img_dict['mask']):
            logging.info(""loading {} {} into generator"".format(""image"",os.path.basename(path1)))
            image, header_img = load(path1)
            print(""Image processing"")
            image = image_preprocessing(image)
            
            
            logging.info(""loading {} {} into generator"".format(""mask"",os.path.basename(path2)))
            mask_img, header_mask = load(path2)
            print(""Mask Processing"")
            mask_img = image_preprocessing(mask_img)
        
            logging.info(""Generator is prepped"")
            
            for img_slice, mask_slice in zip(image,mask_img):
                img_slice = np.reshape(img_slice,(1,512,512,1))
                mask_slice = np.reshape(mask_slice,(1,512,512,1))
                
                yield img_slice, mask_slice
            
    elif include_mask == False:
        for path in dict_obj[""image""]:
            logging.info(""loading {} {} into generator"".format(""image"",os.path.basename(path)))
            image, header  = load(path)
            image = image_preprocessing(image)
            logging.info(""Generator is prepped"")
            
            for slice in image:
                slice = np.reshape(slice,(512,512,1))
                yield slice
    else:
        raise TypeError(""Problem with inputs."")
        logging.error(""Unable to load image into generator"")
```

`tr_data = image_generator(tr_dict,mask = True)
`

```
''' Creating NN'''


if os.path.isdir(Model_path+model_vers)==True:
    logging.info(""Loading Network"")
    model=tfk.models.load_model(""Model_path+model_vers"")
    
else:
    logging.info(""Creating Network"")
    model= tfk.Sequential()
    model.add(layers.Conv2D(32, kernel_size=(3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=input_shape, data_format=""channels_last"", padding='SAME'))
    model.add(layers.MaxPooling2D(pool_size=(3, 3)))
    model.add(layers.BatchNormalization(center=True, scale=True))
    model.add(layers.Dropout(0.5))
    model.add(layers.Conv2D(16, kernel_size=(3, 3), activation='relu', kernel_initializer='he_uniform', padding ='SAME'))
    model.add(layers.MaxPooling2D(pool_size=(3, 3)))
    model.add(layers.BatchNormalization(center=True, scale=True))
    model.add(layers.Dropout(0.5))
    model.add(layers.Flatten())
    model.add(layers.Dense(100, activation='relu', kernel_initializer='he_uniform'))
    model.add(layers.Dense(50, activation='relu', kernel_initializer='he_uniform'))
    model.add(layers.Dense(10, activation='relu'))


'''Hyper Parameters'''
logging.info(""Adding Hyperparameters"")

optimizer=tfk.optimizers.Adadelta() #'Adam' 
loss= ""categorical_crossentropy"" #'sparse_categorical_crossentropy'
metrics = ['accuracy']
model.compile(optimizer = optimizer, loss= loss, metrics= metrics )
```



When I run:

```
""""""Training NN""""""
logging.info(""Training Neural Network"")
tf.autograph.experimental.do_not_convert(func=model.fit(tr_data,epochs = 10,verbose=10))
#model.save(""Model_path+model_vers"")
```
My error is:
```
Image processing
Mask Processing
Epoch 1/10
WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002785034A9D8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: Bad argument number for Name: 4, expecting 3
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002785034A9D8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: Bad argument number for Name: 4, expecting 3
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002785034A9D8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: Bad argument number for Name: 4, expecting 3
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
```

Traceback attached in .txt file
Model Summary in .txt file.
[Tensorflow Traceback.txt](https://github.com/tensorflow/tensorflow/files/4967995/Tensorflow.Traceback.txt)
[Model Summary.txt](https://github.com/tensorflow/tensorflow/files/4967996/Model.Summary.txt)


The 2 exceptions can be found in the tracebook, but summarized here:
```

AssertionError: Bad argument number for Name: 4, expecting 3

During handling of the above exception, another exception occurred:

ValueError: Shapes (None, None, None, None) and (None, 10) are incompatible

```


**Have tried remedy without any change:**
- reisntall tensorflow and keras, as well as gast. 
- restarted computer
- altering input shapes
- rewrote by hand in a new file, no change.
Edit:
- altered size of NN
-altered data shape "
41667,Can't install Tensorflow 1.x,"I am trying to install Tensorflow 1.14 for a package that I am trying to use. I tried:
`pip3 uninstall tensorflow`

Then I tried to install Tensorflow 1.14 using:
`pip3 install tensorflow==1.14`

and I get the following error
`ERROR: Could not find a version that satisfies the requirement tensorflow==1.14 (from versions: 2.2.0rc3, 2.2.0rc4, 2.2.0, 2.3.0rc0, 2.3.0rc1, 2.3.0rc2)
ERROR: No matching distribution found for tensorflow==1.14`

I also tried making a new virtual env and tried the following commands but it didn't work. Is there any way to install Tensorflow 1?
"
41666,"Can't predict bounding boxes with my own trained model, even though Tensorflow returns reduced loss during training.","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
Have I written custom code (as opposed to using a stock example script
provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
happens on a mobile device: No
TensorFlow installed from (source or binary): Source
TensorFlow version (use command below): 1.14.0
Python version: 3.6.7
Bazel version (if compiling from source): Not using Bazel
GCC/Compiler version (if compiling from source): Not using GCC
CUDA/cuDNN version: CUDA/cuDNN: 10.0
GPU model and memory: NVIDIA GeForce GTX 1080, 6G
Exact command to reproduce:

vis_util.visualize_boxes_and_labels_on_image_array(
    image,
    np.squeeze(boxes),
    np.squeeze(classes).astype(np.int32),
    np.squeeze(scores),
    category_index,
    use_normalized_coordinates=True,
    line_thickness=8,
    min_score_thresh=0.20)

**Describe the current behavior**
When trying to predict bounding boxes with my own trained model with the Object Detection API, it won't predict any boxes and returns the original image. The problem is not with my prediction code, as I used the same as in the tutorial at 
https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html
and with a different model, which returns the predicted bounding boxes.

**Describe the expected behavior**
I don't know why this is happening, as the Tensorboard report says that my loss is reducing as I train the model, but when actually using it to predict bounding boxes in the image, it won't work.

**Standalone code to reproduce the issue**
The model:
https://drive.google.com/drive/folders/1SMwSHRR8RLX3RMhYXT_aI9gMAIRLHHlp?usp=sharing

The prediction code:
```
import os
import cv2
import numpy as np
import tensorflow as tf
import sys

from utils import label_map_util
from utils import visualization_utils as vis_util

MODEL_NAME = 'league_model'
IMAGE_NAME = 'test1.jpg'

# Grab path to current working directory
CWD_PATH = os.getcwd()

# Path to frozen detection graph .pb file, which contains the model that is used
# for object detection.
PATH_TO_CKPT = os.path.join(CWD_PATH,MODEL_NAME,'frozen_inference_graph.pb')

# Path to label map file
PATH_TO_LABELS = os.path.join(CWD_PATH,'annotations','label_map.pbtxt')

# Path to image
PATH_TO_IMAGE = os.path.join(CWD_PATH,IMAGE_NAME)

# Number of classes the object detector can identify
NUM_CLASSES = 2

# Load the label map.
# Label maps map indices to category names, so that when our convolution
# network predicts `5`, we know that this corresponds to `king`.
# Here we use internal utility functions, but anything that returns a
# dictionary mapping integers to appropriate string labels would be fine
label_map = label_map_util.load_labelmap(PATH_TO_LABELS)
categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)
category_index = label_map_util.create_category_index(categories)

# Load the Tensorflow model into memory.
detection_graph = tf.Graph()
with detection_graph.as_default():
    od_graph_def = tf.compat.v1.GraphDef()
    with tf.io.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:
        serialized_graph = fid.read()
        od_graph_def.ParseFromString(serialized_graph)
        tf.import_graph_def(od_graph_def, name='')

    sess = tf.Session(graph=detection_graph)

# Define input and output tensors (i.e. data) for the object detection classifier

# Input tensor is the image
image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')

# Output tensors are the detection boxes, scores, and classes
# Each box represents a part of the image where a particular object was detected
detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')

# Each score represents level of confidence for each of the objects.
# The score is shown on the result image, together with the class label.
detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')
detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')

# Number of objects detected
num_detections = detection_graph.get_tensor_by_name('num_detections:0')

# Load image using OpenCV and
# expand image dimensions to have shape: [1, None, None, 3]
# i.e. a single-column array, where each item in the column has the pixel RGB value
image = cv2.imread(PATH_TO_IMAGE)
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
image_expanded = np.expand_dims(image_rgb, axis=0)

# Perform the actual detection by running the model with the image as input
(boxes, scores, classes, num) = sess.run(
    [detection_boxes, detection_scores, detection_classes, num_detections],
    feed_dict={image_tensor: image_expanded})

# Draw the results of the detection (aka 'visulaize the results')

vis_util.visualize_boxes_and_labels_on_image_array(
    image,
    np.squeeze(boxes),
    np.squeeze(classes).astype(np.int32),
    np.squeeze(scores),
    category_index,
    use_normalized_coordinates=True,
    line_thickness=8,
    min_score_thresh=0.20)

# All the results have been drawn on image. Now display the image.
cv2.imshow('Object detector', image)

# Press any key to close the image
cv2.waitKey(0)

# Clean up
cv2.destroyAllWindows()


```"
41664,Maybe incorrect Brier score calculation,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- TensorFlow version (use command below): 1.15.0
- TensorFlow Probability version: 0.8.0

**Describe the current behavior**

I believe that the way the Brier score calculation is implemented in tensorflow_probability.stats.brier_score is incorrect. The formula used is
 
sum_i p[i]* p[i] - 2*p[k]
  
where p is the probability vector over all discrete outcomes, and k is the realized outcome. (Note: This gives element wise Brier scores, and to get the actual Brier score across the dataset, reduce_mean() must be called.)

**Describe the expected behavior**

This formula does not match the reference cited nor [Wikipedia](https://en.wikipedia.org/wiki/Brier_score) nor the definition that [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.brier_score_loss.html) uses. Also, it is stated in the tensorflow_probability docs that the Brier score can be negative, which is not true. 

The reference cited is Brier’s original paper, found [here](https://journals.ametsoc.org/mwr/article/78/1/1/96424/VERIFICATION-OF-FORECASTS-EXPRESSED-IN-TERMS-OF), which states the formula as 

<img width=""182"" alt=""Screen Shot 2020-07-23 at 09 48 34"" src=""https://user-images.githubusercontent.com/41973500/88304392-3277c880-ccd6-11ea-84c3-237be5929f82.png"">

where r is the number of possible classes, n is the number of forecasts, fij is the probability forecast of class j for instance i, and Eij is 0 or 1 depending if the event occurred in class j or not. This is the same formula that is used in sklearn and Wikipedia. By definition, this score cannot be negative.

If we consider the element wise Brier score from this formula, it is

sum_j (p[j] - E[j])2

which is not equivalent to sum_i p[i]* p[i] - 2*p[k]. Tensorflow’s Brier score formula should be corrected to match the commonly accepted definition.

"
41663,Sub-classes of keras.metrics.Metric do not inherit set_model function,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution: Linux
- TensorFlow installed from: Binary
- TensorFlow version: v2.2.0-rc4-8-g2b96f3662b 2.2.0
- Python version: Python 3.8.2

**Describe the current behavior**
When creating a custom callback function for use in training/evaluation I get an error:
```python
AttributeError: 'BinaryTruePositives' object has no attribute 'set_model'
```

**Describe the expected behavior**
Fitting/evaluation to continue normally, using custom callback.

**Standalone code to reproduce the issue**
``` python
import tensorflow as tf
import tensorflow.keras as keras
batch_size = 25
N = 10

# From https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Metric
class BinaryTruePositives(tf.keras.metrics.Metric):
    ...

# Set up model
input = keras.layers.Input(shape=(1))
dense = keras.layers.Dense(100, activation='relu')(input)
output = keras.layers.Dense(1, activation='tanh')(dense)

# Create model
model = keras.Model(inputs=input, outputs=output)
model.compile(loss='MSE')

# Following two lines give the error
model.fit(BinaryTruePositives(), steps_per_epoch=N, callbacks=[BinaryTruePositives()])
model.evaluate(BinaryTruePositives(), steps=N, callbacks=[BinaryTruePositives()])

```

**Other info / logs** 
```python
Traceback (most recent call last):
  File ""up_test.py"", line 47, in <module>
    model.fit(my_generator(), steps_per_epoch=N, callbacks=[BinaryTruePositives()])
  File ""~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py"", line 66, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py"", line 819, in fit
    callbacks = callbacks_module.CallbackList(
  File ""~/.local/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py"", line 219, in __init__
    self.set_model(model)
  File ""~/.local/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py"", line 281, in set_model
    callback.set_model(model)
```
"
41660,Creating model in subprocess consume all memory,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win10
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2.0
- Keras version: 2.3.1
- Python version: 3.7

The code is below.
I expect the model would be created once. but instead it seems being created again and again. And it never got released.

**Standalone code to reproduce the issue**

```
import keras as k
import numpy as np 
import multiprocessing as mp

def createmodel():
    inputs = k.layers.Input(shape=(720,))
    x = k.layers.Dense(256, activation=""relu"")(inputs)
    x = k.layers.Dense(256, activation=""relu"")(x)
    outputs = k.layers.Dense(1, activation=""sigmoid"")(x)
    model = k.models.Model(inputs, outputs)
    model.compile(loss=""binary_crossentropy"",
                optimizer=""adam"")
    return model

class TRAINER:
    def __init__(self):
        self.model = createmodel()

    def play(self, args):
        return (1, np.zeros((720)))

    def train(self):
        mp.set_start_method('spawn')
        p = mp.Pool(4)
        while True:
            res = p.map(self.play, [0] * 128)

trainer = TRAINER()
if __name__ == '__main__':
    trainer.train()
```

"
41659,"Similar problem happened to me: ""tile_functor_gpu.cu.pic.o was not created""","Similar problem happened to me: ""tile_functor_gpu.cu.pic.o was not created""
Tensorflow r1.9
Ubuntu 16.04
gcc 4.8
CUDA 9.0
cuDNN 7.0

_Originally posted by @apepkuss in https://github.com/tensorflow/tensorflow/issues/18839#issuecomment-465826851_"
41658,typo in optimizer_v2.py,"https://github.com/tensorflow/tensorflow/blob/bd6b557c02a5cc1d094a7bb180b9779121a58520/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L285

I was reading through the code to learn how to create a custom optimizer. I think the line should be

```
This class is stateful and thread-compatible.
```"
41657,NaN loss and accuracy when using MirroredStrategy on multiple GPUs (ROCm),"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): 
No, the code is from this example: https://www.tensorflow.org/tutorials/distribute/keras
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: 
No
- TensorFlow installed from (source or binary): 
ROCm 3.5 
- TensorFlow version (use command below): 
v2.2.0-30-g34c3143 2.2.0
- Python version: 
3.6.9
- Bazel version (if compiling from source): 
No
- GCC/Compiler version (if compiling from source): 
No
- CUDA/cuDNN version: 
ROCm
- GPU model and memory: 
2x AMD VEGA 10 XT (Vega 64) 8GB 

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

Mirrored strategy causes the following loss and accuracy values:

Epoch 12/12
468/469 [============================>.] - ETA: 0s - loss: nan - accuracy: nan
469/469 [==============================] - 12s 26ms/step - loss: nan - accuracy: nan - lr: 1.0000e-05
79/79 [==============================] - 1s 12ms/step - loss: nan - accuracy: nan
Eval loss: nan, Eval Accuracy: nan


This seems to happen on Nvidia cards too:
https://github.com/tensorflow/tensorflow/issues/36224

I have tried several other scripts and all do the same.


**Describe the expected behavior**

Running on a single card produces an expected output of loss and accuracy.
I would like that to be the case in multiple GPUs too.

Not sure whether it's just the loss and accuracy calculations screwed up, as the training goes about twice as fast on 2 GPUs.

**This code seems to work flawlessly on both GPUs:**

[mnistmultigpu.txt](https://github.com/tensorflow/tensorflow/files/4965502/mnistmultigpu.txt)
However, it only seems to utilize 1 GPU, which is strange because it did utilize both before...

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Unfortunately I cannot guarantee you will be able to reproduce this as this is tied to having multiple GPUs (Nvidia or not)

<details><summary> The code I used </summary>
# Import TensorFlow and TensorFlow Datasets

import tensorflow_datasets as tfds
import tensorflow as tf
tfds.disable_progress_bar()

import os

datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)

mnist_train, mnist_test = datasets['train'], datasets['test']

strategy = tf.distribute.MirroredStrategy()
print('Number of devices: {}'.format(strategy.num_replicas_in_sync))

# You can also do info.splits.total_num_examples to get the total
# number of examples in the dataset.

num_train_examples = info.splits['train'].num_examples
num_test_examples = info.splits['test'].num_examples

BUFFER_SIZE = 10000

BATCH_SIZE_PER_REPLICA = 64
BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync

def scale(image, label):
  image = tf.cast(image, tf.float32)
  image /= 255

  return image, label

train_dataset = mnist_train.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
eval_dataset = mnist_test.map(scale).batch(BATCH_SIZE)

with strategy.scope():
  model = tf.keras.Sequential([
      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),
      tf.keras.layers.MaxPooling2D(),
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dense(64, activation='relu'),
      tf.keras.layers.Dense(10)
  ])

  model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                optimizer=tf.keras.optimizers.Adam(),
                metrics=['accuracy'])

# Define the checkpoint directory to store the checkpoints

checkpoint_dir = './training_checkpoints'
# Name of the checkpoint files
checkpoint_prefix = os.path.join(checkpoint_dir, ""ckpt_{epoch}"")

# Function for decaying the learning rate.
# You can define any decay function you need.
def decay(epoch):
  if epoch < 3:
    return 1e-3
  elif epoch >= 3 and epoch < 7:
    return 1e-4
  else:
    return 1e-5

# Callback for printing the LR at the end of each epoch.
class PrintLR(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs=None):
    print('\nLearning rate for epoch {} is {}'.format(epoch + 1,
                                                      model.optimizer.lr.numpy()))

callbacks = [
    tf.keras.callbacks.TensorBoard(log_dir='./logs'),
    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,
                                       save_weights_only=True),
    tf.keras.callbacks.LearningRateScheduler(decay),
    PrintLR()
]

model.fit(train_dataset, epochs=12, callbacks=callbacks)

model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))

eval_loss, eval_acc = model.evaluate(eval_dataset)

print('Eval loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))

</details>

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

[runlog.txt](https://github.com/tensorflow/tensorflow/files/4965388/runlog.txt)

My code in a text file:
[mirroredstrategy.txt](https://github.com/tensorflow/tensorflow/files/4965443/mirroredstrategy.txt)


#36224

Any and all help would be appreciated, thank you!"
41656,The first paragraph of the documentation for tf.linalg.inv is cut in half,"## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/api_docs/python/tf/linalg/inv

## Description of issue (what needs changing):

The first paragraph says

> Computes the inverse of one or more square invertible matrices or their

instead of 

> Computes the inverse of one or more square invertible matrices or their adjoints (conjugate transposes).

![image](https://user-images.githubusercontent.com/552629/88276649-b2eef700-ccdf-11ea-9b32-1eb450ffc57a.png)
"
41655,placeholder setting an array with a sequence,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
41654,tensorflow Multi-GPU vs CPU mirroredstrategy  ,"Ask everyone for help..

I am so confused why gpu is slower than cpu on any codition i try...
I want to use six GPU with mirroredstrategy to reduce the training time..
I follow below steps.
https://keras.io/guides/distributed_training/

I got the bad performance with the machine 
GPU: 6*GeForce RTX 2080 TI (10GB) 
CPU: Intel(R) Xeon(R) Silver 4110 CPU @ 2.10GHz

run on the Docker container:  
(1)tensorflow version 2.0.0 
(2)cuda 9.0
(3)cudnn 7.6
(4)nvidia-driver 410.78 - install on server

====Test1=======

**CPU**,batch=64,samples:575478

Epoch 2/500 575478/575478 [===] - **16s** 28us/step - loss: 0.0735 - val_loss: 

====Test2=======

**GPU**,batch=64,samples:575478
Epoch 1/500
1498/1498 [===] - 60s 40ms/step - loss: 0.0907 - val_loss: 0.0619
Epoch 2/500
1498/1498 [===] - **32s** 21ms/step - loss: 0.0592 - val_loss: 0.0522


my_mini_batch = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync(GPU)
384 = 64*6
[enter image description here][1]
**Why so slower than CPU?**



    enter code here
    #import tensorflow_datasets  as ds #for debug
    os.environ[""TF_FORCE_GPU_ALLOW_GROWTH""] = ""true""       
    os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3,4,5'	
    gpu_table = ['/gpu:0','/gpu:1','/gpu:2','/gpu:3','/gpu:4','/gpu:5']
   
    strategy = tf.distribute.MirroredStrategy(devices=gpu_table)

    TrainDataPath = './Train'
    TestDataPath = './Test'

    def Load_Data(InputPath):
	 Data_Total = np.array([])
	 Label_Total = np.array([])
	 folder_content = glob.glob(InputPath+'/'+'*_Label*')
	 print('folder content=',folder_content)
	 for File in folder_content:
		Label = np.load((File))
		Label_Total = np.append(Label_Total, Label)
		Feature = np.load([F for F in glob.glob(InputPath + ""/"" + File.split('/')[-1].split('Label')[0] + '*') if 'Label' not in F][0])
		Data_Total = np.append(Data_Total, Feature)

	Label_Total = Label_Total.reshape(-1, 1)
	Data_Total = Data_Total.reshape(-1, 1,3, 29)
	return Data_Total, Label_Total

    Train_Data, Train_Label = Load_Data(TrainDataPath)
    Test_Data, Test_Label = Load_Data(TestDataPath)

    def get_dataset():#for debug

     print('get_dataset()')
     print(""Training_Data.shpae="", Training_Data.shape)
    
     global my_mini_batch
     my_mini_batch = 1
     BATCH_SIZE_PER_REPLICA = 64 

     my_mini_batch = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync
     print('my_mini_batc=',my_mini_batch)
     print('BATCH_SIZE_PER_REPLICA=',BATCH_SIZE_PER_REPLICA)
     print('strategy.num_replicas_in_sync=',strategy.num_replicas_in_sync)

     Train_Dataset_1 = tf.data.Dataset.from_tensor_slices((Training_Data[:, :,:,0:5], Training_Data[:, :,:,5:15], Training_Data[:, :,:,15:25], Training_Data[:,:,:,25:])).batch(my_mini_batch).repeat()
     Train_Dataset_2 = tf.data.Dataset.from_tensor_slices(Training_Label).batch(my_mini_batch).repeat()

     global train_steps
     global valid_steps
     global test_steps
     train_steps = (int)( Training_Data.shape[0] / my_mini_batch)
     print('train_steps='+str(train_steps))

     valid_steps = (int)(Val_Data.shape[0] / my_mini_batch)
     print('valid_steps=' + str(valid_steps))

     test_steps = (int)(Test_Data.shape[0] / my_mini_batch)
     print('test_steps=' + str(test_steps))

     Valid_Dataset_1 = tf.data.Dataset.from_tensor_slices((Val_Data[:, :,: ,0:5], Val_Data[:, :,:,5:15], 
     Val_Data[:, :,:,15:25], Val_Data[:, :, :,25:])).batch(my_mini_batch).repeat()
     Valid_Dataset_2 = tf.data.Dataset.from_tensor_slices(Val_Label).batch(my_mini_batch).repeat()

     Test_Dataset_1 = tf.data.Dataset.from_tensor_slices((Test_Data[:, :,:,0:5], Test_Data[:, :,:,5:15], 
     Test_Data[:, :,:,15:25], Test_Data[:, :,:,25:])).batch(my_mini_batch).repeat()
     Test_Dataset_2 = tf.data.Dataset.from_tensor_slices(Test_Label).batch(my_mini_batch).repeat()

     print('Replicas: ', strategy.num_replicas_in_sync)
     print(""Num GPUs Available: "", len(tf.config.experimental.list_physical_devices('GPU')))


     Train_Dataset_final = tf.data.Dataset.zip((Train_Dataset_1,Train_Dataset_2))
     Valid_Dataset_final = tf.data.Dataset.zip((Valid_Dataset_1,Valid_Dataset_2))
     Test_Dataset_final = tf.data.Dataset.zip((Test_Dataset_1,Test_Dataset_2))

     return Train_Dataset_final, Valid_Dataset_final, Test_Dataset_final

    with strategy.scope():
	 PalmTh = 0.5
	 A_layer1_Filters= 2
	 B_layer1_Filters = 2
	 C_layer1_Filters = 2
	 D_layer1_Filters = 2

	 L = 0.005
	 F = 12
	 Epochs = 500
	 EarlyStopPatience = 10
	 ChangeLrPatience = 8
	 ChangeLrFactor = 0.9

	 Test_Para = []

	 A_IN_Cell = Input(shape=(1, 3, 5), name = ""Cell"")
	 PX = Input(shape=(1, 3, 10), name = ""X"")
	 PY= Input(shape=(1, 3, 10), name = ""Y"")
	 PZ = Input(shape=(1, 3, 4), name = ""Z"")


	 L1= Convolution2D(filters = A_layer1_Filters, 
		     kernel_size = 3, 
		     strides = 1, 
		     padding = 'valid', 
		     data_format = 'channels_first', 
		     use_bias = True ,
		     name = 'Conv1_Height_Cell', activity_regularizer=regularizers.l2(0.00001)) 
     (A_IN_Cell)

	 LH1 = Activation(custom_HardTanh)(L1)
	 LH1_out= Flatten()(LH1)

	 L2= Convolution2D(filters = B_layer1_Filters, 
		     kernel_size = 3, 
		     strides = 1, 
		     padding = 'valid', 
		     data_format = 'channels_first', 
		     use_bias = True ,
		     name = 'Conv1_ProjectionX', activity_regularizer=regularizers.l2(0.00001)) 
     (PX)

	 LH2= Activation(custom_HardTanh)(L2)
	 LH2_out= Flatten()(LH2)

	 A_Convolution1 = Convolution2D(filters = C_layer1_Filters, 
		     kernel_size = 3, 
		     strides = 1, 
		     padding = 'valid', 
		     data_format = 'channels_first', 
		     use_bias = True ,
		     name = 'Conv1_ProjectionY', activity_regularizer=regularizers.l2(0.00001)) 
     (PY)

	 A_Hidden1 = Activation(custom_HardTanh)(A_Convolution1)
	 A_Out = Flatten()(A_Hidden1)

	 Centroid_Convolution1 = Convolution2D(filters = D_layer1_Filters, 
		     kernel_size = 3, 
		     strides = 1, 
		     padding = 'valid', 
		     data_format = 'channels_first', 
		     use_bias = True ,
		     name = 'Conv1_Centroid', activity_regularizer=regularizers.l2(0.00001))(PZ)

	 Centroid_Hidden1 = Activation(custom_HardTanh)(Centroid_Convolution1)
	 Centroid_Out = Flatten()(Centroid_Hidden1)

	 ConcatentaLayer = concatenate([LH1_out, 
		       LH2_out, 
		       A_Out,
		       Centroid_Out]
		       )

	 DenseLayer1 = Dense(F, use_bias = True, activation=None, 
     activity_regularizer=regularizers.l2(0.00001))(ConcatentaLayer)
	 DenseLayer1 = Activation(custom_HardTanh)(DenseLayer1)
	 Output = Dense(1, use_bias = True, activation='sigmoid')(DenseLayer1)

	 model = Model(inputs=[A_IN_Cell , PX, PY, PZ], 
     outputs=[Output])

	 adam = optimizers.Adam(lr=L)

	 model.compile(optimizer = adam, loss = 'binary_crossentropy')
	 model.summary()

	 change_lr = ReduceLROnPlateau(monitor='val_loss', factor=ChangeLrFactor,
				      patience=ChangeLrPatience, min_lr=0.00005)
	 EarlyStop = EarlyStopping(monitor='loss', patience = EarlyStopPatience, verbose=2, mode='min')

	 Training_Data, Val_Data, Training_Label, Val_Label = train_test_split(Train_Data, Train_Label, 
     test_size=0.1)
	 print('Train_Data follow=',Train_Data.shape)
	 print('Training_Data follow=',Training_Data.shape)

	 train_dataset, val_dataset, test_dataset = get_dataset()
	 print('type(train_dataset)=', train_dataset)

    #beside scope
    history = model.fit(train_dataset, epochs=Epochs,
                    steps_per_epoch=train_steps,
                    validation_steps = valid_steps,
                    validation_data=val_dataset) 


"
41653,tf.lite.TFLiteConverter produces inconsistent converted model ,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX 10.15.5- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.12.1-37224-ga6cd18a133 2.4.0-dev20200722
- Python version: 3.7

**Describe the current behavior**
`tf.lite.TFLiteConverter.from_saved_model` converts LSTM to  fused op, but `tf.lite.TFLiteConverter.from_concrete_functions` doesn't.

**Describe the expected behavior**
The converted result should be the same for the same model.

**Standalone code to reproduce the issue**

```
# produce fused LSTM
import numpy as np
import tensorflow as tf

model = tf.keras.models.Sequential([
    tf.keras.layers.Input(shape=(28, 28), name='input'),
    tf.keras.layers.Bidirectional(
        tf.keras.layers.LSTM(20, return_sequences=True)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(10, activation=tf.nn.softmax, name='output')
])
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
model.summary()

run_model = tf.function(lambda x: model(x))
# This is important, let's fix the input size.
BATCH_SIZE = 1
STEPS = 28
INPUT_SIZE = 28
concrete_func = run_model.get_concrete_function(
    tf.TensorSpec([BATCH_SIZE, STEPS, INPUT_SIZE], model.inputs[0].dtype))

# model directory.
MODEL_DIR = ""keras_lstm""

# converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])
converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)
tflite_model = converter.convert()

with tf.io.gfile.GFile('tflite_test.tflite', 'wb') as f:
    f.write(tflite_model)
```

```
# produce original LSTM
import numpy as np
import tensorflow as tf

model = tf.keras.models.Sequential([
    tf.keras.layers.Input(shape=(28, 28), name='input'),
    tf.keras.layers.Bidirectional(
        tf.keras.layers.LSTM(20, return_sequences=True)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(10, activation=tf.nn.softmax, name='output')
])
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
model.summary()

run_model = tf.function(lambda x: model(x))
# This is important, let's fix the input size.
BATCH_SIZE = 1
STEPS = 28
INPUT_SIZE = 28
concrete_func = run_model.get_concrete_function(
    tf.TensorSpec([BATCH_SIZE, STEPS, INPUT_SIZE], model.inputs[0].dtype))

# model directory.
MODEL_DIR = ""keras_lstm""

converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])
#converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)
tflite_model = converter.convert()

with tf.io.gfile.GFile('tflite_test.tflite', 'wb') as f:
    f.write(tflite_model)
```
"
41652,Unable to load graph from protobuf file on s390x arch (Big Endian),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): IBM Z (s390x arch), Ubuntu 18.04.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Installed Tensorflow C lib from source
- TensorFlow version (use command below): 1.15.3
- Python version:
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): 
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the current behavior**
I installed the TensorFlow C library on an s390x machine. Then tried to do inferencing using a pre-trained model, when I encountered the following error. I'm loading the model graph from a protobuf file.

```
INFO[0000]/root/gopros/src/github.com/rai-project/tensorflow/vendor/github.com/rai-project/dlframework/framework/register.go:76 github.com/rai-project/tensorflow/vendor/github.com/rai-project/dlframework/framework.Register() skipping regitration of hidden model          pkg=dlframework/framework name=SSD_MobileNet_v2_Quantized_300x300_COCO
INFO[0000] running predict urls                          model=MobileNet_v1_1.0_224 pkg=dlframework/framework/cmd/server
Error: unable to create tensorflow model graph: Invalid value in tensor used for shape: -385679360
```



**Describe the expected behavior**
Tensorflow should support the loading of model graphs in protobuf format on a big-endian machine.



**Standalone code to reproduce the issue**
I suspected the error is because the protobuf file for the model graph was originally created on little endian machine and I'm trying to load it on a big-endian machine. In order to confirm that the error is due to the loading of protobuf graph in Tensorflow, I tried out **[this](https://gist.github.com/asimshankar/7c9f8a9b04323e93bb217109da8c7ad2)** tutorial. Using this tutorial, I created a tensorflow model and saved it in protobuf format on an x86 system. When I loaded the graph file on an x86 system, it works but when I tried loading the graph on s390x system, I got the following error -

```
Loading graph
Read GraphDef of 27083 bytes
ERROR: Dimension 0 in both shapes must be equal, but are 1 and 16777216. Shapes are [1,1] and [16777216,16777216]. for 'dense/kernel/Assign' (op: 'Assign') with input shapes: [1,1], [16777216,16777216].
```


**Other info / logs**

The following tests are passing -
```
//tensorflow/c:c_api_test
//tensorflow/c:c_api_function_test
//tensorflow/c:c_test
//tensorflow/c:ops_test
//tensorflow/c:env_test
//tensorflow/c:c_test_util
//tensorflow/cc/saved_model:reader_test
//tensorflow/cc/saved_model:loader_test
```"
41651,TensorFlowLite C++ Mobile,"I am a iOS developer

I know you have  
TensorFlowLiteSwift, 2.2.0
TensorFlowLiteC,2.2.0
TensorFlowLiteOC,2.2.0

but  where i can get TensorFlowLite,2.2.0?  ,this only 1.31.0 verison

1.31.0 not working result right answer

"
41650,Dangerous data format default in SSIM,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Red Hat Linux 7
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.2
- Python version: 3.8
- CUDA/cuDNN version: 10 
- GPU model and memory:

**Describe the current behavior**
[SSIM](https://www.tensorflow.org/api_docs/python/tf/image/ssim) assumes that the image batches have a shape (h, w, batch_size). This is not documented anywhere and the only reason I was aware of it is when I had a batch size smaller than 11, which triggers the following assertion to fail:
``` Python
  checks = [
      control_flow_ops.Assert(
          math_ops.reduce_all(
              math_ops.greater_equal(shape1[-3:-1], filter_size)),
          [shape1, filter_size],
          summarize=8),
      control_flow_ops.Assert(
          math_ops.reduce_all(
              math_ops.greater_equal(shape2[-3:-1], filter_size)),
          [shape2, filter_size],
          summarize=8)
  ]
``` 
Because:
```
>>> shape = (4, 255, 255)
>>> shape[-3:-1]
(4, 255)
```
and `filter_size = 11`

**Describe the expected behavior**
This behaviour should be documented.
The user should have the choice to choose the data format : bath size first or last. By default, I think it should be batch size first as it's the way tf.data.Dataset handles the input pipeline. 

**Standalone code to reproduce the issue**
This works
```
shape = (255, 255, 4)
img1 = tf.zeros(shape)
img2 = tf.zeros(shape)
tf.image.ssim(
    img1, img2, 1
)
```
But this doesn't
```
shape = (4, 255, 255)
img1 = tf.zeros(shape)
img2 = tf.zeros(shape)
tf.image.ssim(
    img1, img2, 1
)
```
"
41649,Help!!!!!!!!!!   tf.keras.layers.Conv2d and slim.conv2d have different result(Though slim.conv2d had been deprecated),"TF 1.15.0
Ubuntu 16.0
python 3.7.6
with same parameter(kernel , bias, padding etc.) tf.keras.layers.Conv2d and slim.conv2d have different output. Though the different is little, about 0.000001,  when i copy the model parameters training with slim.conv2d to the model builded with tf.keras.layers.Conv2d, I got a completely output"
41648,tf.data.Dataset - You must feed a value for placeholder tensor,"**System information**
- OS: Ubuntu 18.04.4 LTS (also tested in Windows 10 obtaining the same behaviour)
- TensorFlow installed from: pypi, pip install tf-nightly
- TensorFlow version (use command below): 2.4.0-dev20200718
- Python version: 3.6.9 
 
2020-07-23 09:58:13.497096: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
v1.12.1-36900-g829256e314 2.4.0-dev20200718

**Describe the current behavior**
Computations involving tf.data.Dataset stall. 
If the dataset is small the execution continues properly but the error is still registered in the server logs.

**Standalone code to reproduce the issue**

Excerpt from: https://www.tensorflow.org/tutorials/load_data/images

```
import tensorflow as tf
import pathlib
dataset_url = ""https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz""
data_dir = tf.keras.utils.get_file(origin=dataset_url, 
                                   fname='flower_photos', 
                                   untar=True)
data_dir = pathlib.Path(data_dir)
list_ds = tf.data.Dataset.list_files(str(data_dir/'*/*'), shuffle=False)
# Triggers the abort message
for f in list_ds.take(5):
    print(f.numpy()) 
```

**Jupyter lab server log**

**2020-07-23 09:52:42.605408: W tensorflow/core/common_runtime/executor.cc:1086] [/device:CPU:0] Executor start aborting: Invalid argument: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [3670]
         [[{{node Placeholder/_0}}]]**
"
41647,Can you optimize the text.text_to_word_sequence API？,"**System information**
- TensorFlow version (you are using):    2.2.0
- Are you willing to contribute it (Yes/No):  Yes


**Describe the feature and the current behavior/state.**
  
   I want to use ```tf.keras.preprocessing.text.text_to_word_sequence(s, filters='{regex}', split='{regex}')  ```  API to split and filter text. but ```filter``` paramter mapping ```split``` paramter. I just want to ```split``` paramter only split and ```filters``` paramter only filter. It best supports ```regex``` from all paramters.eg:
```python
import tensorflow as tf
s = '该文件夹包含，该文件安装了安装了Keras2.1.5环境。该环境可用Docker'
text_to_word_sequence = tf.keras.preprocessing.text.text_to_word_sequence(s, filters='[0-9.]', split='[，。]')
print(text_to_word_sequence)
# ['该文件夹包含','该文件安装了安装了Keras环境','该环境可用Docker']
```"
41646,How to cut off gradient in tensorflow2.0,"I want to cut off gradient when training. For example: 
![image](https://user-images.githubusercontent.com/20520524/88247306-db6ef500-cccf-11ea-9da1-d53e5e27ebc6.png)
In picture, there are six ops. I want to loss7 only influence op5 and op6, so I need to cut off grandient flow between op2 and op5. 
Do you konw how to do it. I am looking forward your answer. thanks
 "
41643,tf-slim ResNet v1 pre-trained models preprocessing,"### Affected Doc URL
https://github.com/tensorflow/models/blob/master/research/slim/README.md#pre-trained-models

### Description

Both tf-slim VGG and Inception preprocessing cause the ResNet v1 models in the above link to have incorrect outputs.  The other ImageNet models run correctly with either the VGG or Inception preprocessing, however under the same code path, ResNet v1 models produce incorrect outputs.  This issue is also documented in [this unresolved issue](https://github.com/tensorflow/tensorflow/issues/17426).

Are we aware of the correct steps to take to correctly run slim's ResNet v1 models and could this be updated in the documentation?

Thank you."
41642,[ TF 2.0 ] tf.function throws error,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): pip install tensorflow==2.0.0
- Python version:3.7.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
I ported n gram computation for truecasing to tf-2.0 and it works fine when I run the function as standalone but fails when I try to wrap the function with tf.function. There is a get_score function that creates unigram/bigram combinations. When the function runs standalone with string tensor as input, everything works as expected

**Describe the expected behavior**

The tf.function wrapper is needed so that the model can be exported to run with tf.serving. The code functions correctly when its provided an input. But the code fails with the error attached when tf.function wrapper is added

**Standalone code to reproduce the issue**
```
    def get_score(self, prev_token, possible_token, next_token):
        possible_token_l = tf.strings.lower(possible_token)
        alternative_tokens = self.get_alternative_tokens(possible_token_l)

        unigram_score = self.compute_unigram_score(possible_token, alternative_tokens)
        result = tf.math.log(unigram_score)

        if prev_token is not None:
            bigram_backward_score = self.compute_bigram_backward_score(possible_token, prev_token, alternative_tokens)
            result += tf.math.log(bigram_backward_score)

        if next_token is not None:
            bigram_forward_score = self.compute_bigram_forward_score(possible_token, next_token, alternative_tokens)
            result += tf.math.log(bigram_forward_score)

        if prev_token is not None and next_token is not None:
            trigram_score = self.compute_trigram_score(possible_token, prev_token, next_token, alternative_tokens)
            result += tf.math.log(trigram_score)
        return result

    @tf.function(input_signature=[tf.TensorSpec(shape=(None), dtype=tf.string, name=""input_text"")])
    def get_true_case(self, tokens_tensor):
        cap_first_token = tf.reshape(tokens_tensor[0], [1])
        trueCasedTokens = cap_first_token
        tokens_tensor = tf.slice(tokens_tensor, [1], [-1])

        condition = lambda tokens_tensor, trueCasedTokens: tf.greater(tf.size(tokens_tensor), 0)

        def body(tokens_tensor, trueCasedTokens):
            cur_tokens_tensor = tf.concat([trueCasedTokens[-1:], tokens_tensor], 0)
            curToken = tf.get_static_value(tf.slice(cur_tokens_tensor, [1], [1]))[0]
            curToken = tf.get_static_value(tf.strings.lower(curToken))

            prevToken = tf.slice(cur_tokens_tensor, [0], [1])
            if tf.get_static_value(tf.greater(tf.size(cur_tokens_tensor), [3]))[0]:
                nextToken = tf.slice(cur_tokens_tensor, [2], [1])
            else:
                nextToken = None

            wordCasingLookup = tf.reshape(self.get_alternative_tokens(tf.constant(curToken)), [-1])
            if tf.get_static_value(tf.equal(tf.size(wordCasingLookup), [0]))[0]:
                trueCasedTokens = tf.concat([trueCasedTokens, tf.constant(curToken)], 0)
            if tf.get_static_value(tf.equal(tf.size(wordCasingLookup), [1]))[0]:
                trueCasedTokens = tf.concat([trueCasedTokens, wordCasingLookup], 0)
            else:
                scores = tf.map_fn(lambda x: self.get_score(prevToken, x, nextToken),
                                   wordCasingLookup, dtype=tf.float32)
                maxElementIndx = tf.get_static_value(tf.argmax(scores))[0]
                trueVariant = tf.slice(wordCasingLookup, [maxElementIndx], [1])
                trueCasedTokens = tf.concat([trueCasedTokens, trueVariant], 0)

            tokens_tensor = tf.slice(tokens_tensor, [1], [-1])
            return tokens_tensor, trueCasedTokens

        res = tf.while_loop(condition,
                            lambda tokens_tensor, trueCasedTokens: body(tokens_tensor, trueCasedTokens),
                            [tokens_tensor, trueCasedTokens])                    
        res = res[1]
        res = tf.gather(res, tf.where(res != b''))

        return res

```
**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
```
  File ""/Users/ayushc/transcript-post-processor/truecaser.py"", line 67, in truecase_tokenize
    res = tf_model.get_true_case(tokens_tensor)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 457, in __call__
    result = self._call(*args, **kwds)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 503, in _call
    self._initialize(args, kwds, add_initializers_to=initializer_map)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 408, in _initialize
    *args, **kwds))
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 1848, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2150, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2041, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py"", line 915, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 358, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2658, in bound_method_wrapper
    return wrapped_fn(*args, **kwargs)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py"", line 905, in wrapper
    raise e.ag_error_metadata.to_exception(e)
TypeError: in converted code:

    ./truecaser/truecaser_tf.py:160 body  *
        curToken = tf.get_static_value(tf.slice(cur_tokens_tensor, [1], [1]))[0]
    /usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/control_flow_ops.py:2478 while_loop_v2
        return_same_structure=True)
    /var/folders/4j/sdgl7s9j3w51c50b5wy4v_p00000gn/T/tmpw4oa0q3i.py:21 body
        curToken = ag__.converted_call(tf.get_static_value, body_scope.callopts, (ag__.converted_call(tf.slice, body_scope.callopts, (cur_tokens_tensor, [1], [1]), None, body_scope),), None, body_scope)[0]

    TypeError: 'NoneType' object is not subscriptable
```"
41639,Using learning-rate decay schedule and verbose=1 creates fatal TypeError,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 and Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO
- TensorFlow installed from (source or binary): Binary (pip3)
- TensorFlow version (use command below): 2.2.0
- Python version: 3.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

In the following minimal example, setting `verbose=1` in the fitting creates a fatal error. I realize that `ReduceLROnPlateau` isn't needed when also using `ExponentialDecay` but the fact that this error happens if `verbose=1` and not when `verbose=0` in training shows there is a deeper problem. 

```
# minimum keras example
import tensorflow as tf
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.optimizers.schedules import ExponentialDecay
from tensorflow.keras.layers import Dense
from tensorflow.keras import Sequential
from tensorflow.keras.callbacks import TerminateOnNaN,EarlyStopping,ReduceLROnPlateau

model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8,input_shape=(16,)))
model.add(tf.keras.layers.Dense(1))
callbacks=[#TerminateOnNaN(),EarlyStopping(monitor='val_loss'),
          ReduceLROnPlateau(monitor='val_loss')]
optimizer=SGD(learning_rate=ExponentialDecay(initial_learning_rate=1.e-3,
                                            decay_steps=2,decay_rate=0.5))
model.compile(optimizer=optimizer, loss='mse')
x=tf.ones((32,16))
y=tf.ones((32,1))
v=tf.ones((8,1))
model.fit(x, y, batch_size=4, epochs=8, callbacks=callbacks,
         validation_split=0.2,steps_per_epoch=4,verbose=1)
```
yields the following error:
```
Epoch 1/8
1/4 [======>.......................] - ETA: 0s - loss: 1.4363

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-138-2120310fffef> in <module>
     19 v=tf.ones((8,1))
     20 model.fit(x, y, batch_size=4, epochs=8, callbacks=callbacks,
---> 21          validation_split=0.2,steps_per_epoch=4,verbose=1)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py in _method_wrapper(self, *args, **kwargs)
     64   def _method_wrapper(self, *args, **kwargs):
     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
---> 66       return method(self, *args, **kwargs)
     67 
     68     # Running inside `run_distribute_coordinator` already.

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
    874           epoch_logs.update(val_logs)
    875 
--> 876         callbacks.on_epoch_end(epoch, epoch_logs)
    877         if self.stop_training:
    878           break

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\keras\callbacks.py in on_epoch_end(self, epoch, logs)
    363     logs = self._process_logs(logs)
    364     for callback in self.callbacks:
--> 365       callback.on_epoch_end(epoch, logs)
    366 
    367   def on_train_batch_begin(self, batch, logs=None):

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\keras\callbacks.py in on_epoch_end(self, epoch, logs)
    892 
    893   def on_epoch_end(self, epoch, logs=None):
--> 894     self._finalize_progbar(logs)
    895 
    896   def on_test_end(self, logs=None):

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\keras\callbacks.py in _finalize_progbar(self, logs)
    933       self.progbar.target = self.seen
    934     logs = logs or {}
--> 935     self.progbar.update(self.seen, list(logs.items()), finalize=True)
    936 
    937 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\keras\utils\generic_utils.py in update(self, current, values, finalize)
    568         value_base = max(current - self._seen_so_far, 1)
    569         if k not in self._values:
--> 570           self._values[k] = [v * value_base, value_base]
    571         else:
    572           self._values[k][0] += v * value_base

TypeError: unsupported operand type(s) for *: 'ExponentialDecay' and 'int'
```

**Describe the expected behavior**

Model should fit without any errors. Here is what happens if I don't use the callbacks:

```
Epoch 1/8
4/4 [==============================] - 0s 20ms/step - loss: 6.3209 - val_loss: 4.6678
Epoch 2/8
4/4 [==============================] - 0s 10ms/step - loss: 4.4087 - val_loss: 4.0870
Epoch 3/8
4/4 [==============================] - 0s 9ms/step - loss: 4.0231 - val_loss: 3.9547
Epoch 4/8
4/4 [==============================] - 0s 10ms/step - loss: 3.9385 - val_loss: 3.9224
Epoch 5/8
4/4 [==============================] - 0s 9ms/step - loss: 3.9185 - val_loss: 3.9143
Epoch 6/8
4/4 [==============================] - 0s 9ms/step - loss: 3.9131 - val_loss: 3.9123
Epoch 7/8
4/4 [==============================] - 0s 9ms/step - loss: 3.9121 - val_loss: 3.9118
Epoch 8/8
4/4 [==============================] - 0s 11ms/step - loss: 3.9117 - val_loss: 3.9117
```

**Standalone code to reproduce the issue**

see above.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
41637,"TFLite quantization silently converts bias tensor to int8 (int32 expected), causing interpreter to crash","**System information**
- OS Platform and Distribution: Linux Ubuntu 20.04
- TensorFlow installed from: source
- TensorFlow version (or github SHA if from source): `tf-nightly`, version `2.4.0-dev20200721`

**Code**
I am converting using the options recommended in the docs:
```
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.target_spec.supported_ops = [
      tf.lite.OpsSet.TFLITE_BUILTINS_INT8
]
converter.target_spec.supported_types = [tf.int8, tf.uint8, tf.int32]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()

# Allocating tensors subsequently fails
interpreter = tf.lite.Interpreter(model_content=tflite_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
```

**The output from the converter invocation**

```
2020-07-22 11:51:55.215847: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:192] None of the MLIR optimization passes are enabled (registered 0 passes)
INFO:tensorflow:The given SavedModel MetaGraphDef contains SignatureDefs with the following keys: {'serving_default'}
I0722 11:51:55.644716 140663456084352 convert_saved_model.py:80] The given SavedModel MetaGraphDef contains SignatureDefs with the following keys: {'serving_default'}
INFO:tensorflow:input tensors info: 
I0722 11:51:55.645021 140663456084352 convert_saved_model.py:99] input tensors info: 
INFO:tensorflow:Tensor's key in saved_model's tensor_map: input-to-image
I0722 11:51:55.645142 140663456084352 convert_saved_model.py:41] Tensor's key in saved_model's tensor_map: input-to-image
INFO:tensorflow: tensor name: Placeholder:0, shape: (1, 640, 640, 3), type: DT_FLOAT
I0722 11:51:55.645210 140663456084352 convert_saved_model.py:43]  tensor name: Placeholder:0, shape: (1, 640, 640, 3), type: DT_FLOAT
INFO:tensorflow:output tensors info: 
I0722 11:51:55.645267 140663456084352 convert_saved_model.py:101] output tensors info: 
INFO:tensorflow:Tensor's key in saved_model's tensor_map: raw_scores
I0722 11:51:55.645350 140663456084352 convert_saved_model.py:41] Tensor's key in saved_model's tensor_map: raw_scores
INFO:tensorflow: tensor name: RawScores:0, shape: (1, 76725, 4), type: DT_FLOAT
I0722 11:51:55.645412 140663456084352 convert_saved_model.py:43]  tensor name: RawScores:0, shape: (1, 76725, 4), type: DT_FLOAT
INFO:tensorflow:Tensor's key in saved_model's tensor_map: raw_boxes
I0722 11:51:55.645479 140663456084352 convert_saved_model.py:41] Tensor's key in saved_model's tensor_map: raw_boxes
INFO:tensorflow: tensor name: RawBoxes:0, shape: (1, 76725, 1, 4), type: DT_FLOAT
I0722 11:51:55.645537 140663456084352 convert_saved_model.py:43]  tensor name: RawBoxes:0, shape: (1, 76725, 1, 4), type: DT_FLOAT
2020-07-22 11:51:55.646430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-22 11:51:55.646453: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      
INFO:tensorflow:Restoring parameters from /home/rsaini/repos/isr/edge-test/tmp/variables/variables
I0722 11:51:56.537452 140663456084352 saver.py:1293] Restoring parameters from /home/rsaini/repos/isr/edge-test/tmp/variables/variables
2020-07-22 11:51:57.199313: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1
2020-07-22 11:51:57.199419: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-07-22 11:51:57.343124: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561781c5b260 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-07-22 11:51:57.343157: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080, Compute Capability 6.1
2020-07-22 11:51:57.343674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:03:00.0 name: GeForce GTX 1080 computeCapability: 6.1
coreClock: 1.7335GHz coreCount: 20 deviceMemorySize: 7.92GiB deviceMemoryBandwidth: 298.32GiB/s
2020-07-22 11:51:57.343771: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory
2020-07-22 11:51:57.343819: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory
2020-07-22 11:51:57.343863: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory
2020-07-22 11:51:57.343906: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory
2020-07-22 11:51:57.343949: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory
2020-07-22 11:51:57.343992: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory
2020-07-22 11:51:57.344035: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory
2020-07-22 11:51:57.344043: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-07-22 11:51:57.344057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-22 11:51:57.344064: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-07-22 11:51:57.344070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2020-07-22 11:51:57.448338: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize
2020-07-22 11:51:57.448368: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.002ms.
2020-07-22 11:51:57.448373: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0ms.
WARNING:tensorflow:From /home/rsaini/.pyenv/versions/edge/lib/python3.7/site-packages/tensorflow/lite/python/util.py:276: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
W0722 11:51:57.642830 140663456084352 deprecation.py:323] From /home/rsaini/.pyenv/versions/edge/lib/python3.7/site-packages/tensorflow/lite/python/util.py:276: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
WARNING:tensorflow:From /home/rsaini/.pyenv/versions/edge/lib/python3.7/site-packages/tensorflow/python/framework/convert_to_constants.py:854: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
W0722 11:51:57.643322 140663456084352 deprecation.py:323] From /home/rsaini/.pyenv/versions/edge/lib/python3.7/site-packages/tensorflow/python/framework/convert_to_constants.py:854: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
2020-07-22 11:51:58.442749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-22 11:51:58.442781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      
INFO:tensorflow:Restoring parameters from /home/rsaini/repos/isr/edge-test/tmp/variables/variables
I0722 11:51:59.415019 140663456084352 saver.py:1293] Restoring parameters from /home/rsaini/repos/isr/edge-test/tmp/variables/variables
INFO:tensorflow:The given SavedModel MetaGraphDef contains SignatureDefs with the following keys: {'serving_default'}
I0722 11:51:59.910989 140663456084352 convert_saved_model.py:80] The given SavedModel MetaGraphDef contains SignatureDefs with the following keys: {'serving_default'}
INFO:tensorflow:input tensors info: 
I0722 11:51:59.911199 140663456084352 convert_saved_model.py:99] input tensors info: 
INFO:tensorflow:Tensor's key in saved_model's tensor_map: input-to-image
I0722 11:51:59.911319 140663456084352 convert_saved_model.py:41] Tensor's key in saved_model's tensor_map: input-to-image
INFO:tensorflow: tensor name: Placeholder:0, shape: (1, 640, 640, 3), type: DT_FLOAT
I0722 11:51:59.911397 140663456084352 convert_saved_model.py:43]  tensor name: Placeholder:0, shape: (1, 640, 640, 3), type: DT_FLOAT
INFO:tensorflow:output tensors info: 
I0722 11:51:59.911462 140663456084352 convert_saved_model.py:101] output tensors info: 
INFO:tensorflow:Tensor's key in saved_model's tensor_map: raw_boxes
I0722 11:51:59.912057 140663456084352 convert_saved_model.py:41] Tensor's key in saved_model's tensor_map: raw_boxes
INFO:tensorflow: tensor name: RawBoxes:0, shape: (1, 76725, 1, 4), type: DT_FLOAT
I0722 11:51:59.912123 140663456084352 convert_saved_model.py:43]  tensor name: RawBoxes:0, shape: (1, 76725, 1, 4), type: DT_FLOAT
INFO:tensorflow:Tensor's key in saved_model's tensor_map: raw_scores
I0722 11:51:59.912191 140663456084352 convert_saved_model.py:41] Tensor's key in saved_model's tensor_map: raw_scores
INFO:tensorflow: tensor name: RawScores:0, shape: (1, 76725, 4), type: DT_FLOAT
I0722 11:51:59.912250 140663456084352 convert_saved_model.py:43]  tensor name: RawScores:0, shape: (1, 76725, 4), type: DT_FLOAT
2020-07-22 11:51:59.913029: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-22 11:51:59.913052: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      
INFO:tensorflow:Restoring parameters from /home/rsaini/repos/isr/edge-test/tmp/variables/variables
I0722 11:52:00.961747 140663456084352 saver.py:1293] Restoring parameters from /home/rsaini/repos/isr/edge-test/tmp/variables/variables
2020-07-22 11:52:01.741302: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1
2020-07-22 11:52:01.741406: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-07-22 11:52:01.742209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:03:00.0 name: GeForce GTX 1080 computeCapability: 6.1
coreClock: 1.7335GHz coreCount: 20 deviceMemorySize: 7.92GiB deviceMemoryBandwidth: 298.32GiB/s
2020-07-22 11:52:01.742323: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory
2020-07-22 11:52:01.742382: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory
2020-07-22 11:52:01.742436: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory
2020-07-22 11:52:01.742488: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory
2020-07-22 11:52:01.742540: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory
2020-07-22 11:52:01.742592: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory
2020-07-22 11:52:01.742644: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory
2020-07-22 11:52:01.742654: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-07-22 11:52:01.742672: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-22 11:52:01.742680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-07-22 11:52:01.742688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2020-07-22 11:52:01.850868: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize
2020-07-22 11:52:01.850904: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.003ms.
2020-07-22 11:52:01.850913: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0ms.
I0722 11:52:03.153840 140663456084352 lite.py:1321] Using experimental converter: If you encountered a problem please file a bug. You can opt-out by setting experimental_new_converter=False
2020-07-22 11:52:03.415416: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:315] Ignored output_format.
2020-07-22 11:52:03.415449: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:318] Ignored drop_control_dependency.
2020-07-22 11:52:03.791157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:03:00.0 name: GeForce GTX 1080 computeCapability: 6.1
coreClock: 1.7335GHz coreCount: 20 deviceMemorySize: 7.92GiB deviceMemoryBandwidth: 298.32GiB/s
2020-07-22 11:52:03.791318: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory
2020-07-22 11:52:03.791376: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory
2020-07-22 11:52:03.791429: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory
2020-07-22 11:52:03.791480: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory
2020-07-22 11:52:03.791544: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory
2020-07-22 11:52:03.791592: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory
2020-07-22 11:52:03.791640: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory
2020-07-22 11:52:03.791649: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-07-22 11:52:03.791666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-22 11:52:03.791673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-07-22 11:52:03.791679: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2020-07-22 11:52:04.930987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-22 11:52:04.931019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      
2020-07-22 11:52:05.066647: W tensorflow/core/common_runtime/executor.cc:1086] [/device:CPU:0] Executor start aborting: Invalid argument: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [34600]
	 [[{{node Placeholder/_0}}]]
2020-07-22 11:52:05.066900: W tensorflow/core/common_runtime/executor.cc:1086] [/device:CPU:0] Executor start aborting: Invalid argument: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [34600]
	 [[{{node Placeholder/_0}}]]
Traceback (most recent call last):
  File ""./export_tflite_model.py"", line 91, in <module>
    tf.app.run(main)
  File ""/home/rsaini/.pyenv/versions/edge/lib/python3.7/site-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/home/rsaini/.pyenv/versions/edge/lib/python3.7/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/home/rsaini/.pyenv/versions/edge/lib/python3.7/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""./export_tflite_model.py"", line 85, in main
    export(FLAGS.saved_model_dir, FLAGS.output_dir)
  File ""./export_tflite_model.py"", line 79, in export
    interpreter.allocate_tensors()
  File ""/home/rsaini/.pyenv/versions/edge/lib/python3.7/site-packages/tensorflow/lite/python/interpreter.py"", line 243, in allocate_tensors
    return self._interpreter.AllocateTensors()
RuntimeError: tensorflow/lite/kernels/conv.cc:334 bias->type != kTfLiteInt32 (INT8 != INT32)Node number 3 (CONV_2D) failed to prepare.
```

**Helpful Files**
Saved model:
[saved.zip](https://github.com/tensorflow/tensorflow/files/4961958/saved.zip)

Graph after transformation:
[after-transformation.zip](https://github.com/tensorflow/tensorflow/files/4961968/after-transformation.zip)


**Failure details**
Interestingly, the actual `TFLiteConverter` quantizes without crashing; reloading the generated model into the interpreter is what causes the crash. I am not sure why the bias tensor is being cast to int8; I am happy to provide any more debug files should they be necessary."
41635,"Fitting sometimes leads to NaN loss on TPU, while on CPU doesn't","**System information**

- TensorFlow version (use command below): 2.2.0 (v2.2.0-0-g2b96f3662b)
- Python version: 3.6.9
- GPU model and memory: Google Colab TPU

I've found that sometimes fitting model on TPU leads to NaN loss, while fitting on CPU doesn't.

I fit Xception network with custom top layers, using only 5 batches, one per epoch. Standalone code:

```
import os
import tensorflow as tf
import numpy as np
from tensorflow.keras.layers import *

tf.get_logger().propagate = False
resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu = 'grpc://' + os.environ['COLAB_TPU_ADDR'])
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
strategy = tf.distribute.experimental.TPUStrategy(resolver)

with strategy.scope():
#with tf.device('/job:localhost/replica:0/task:0/device:CPU:0'):

  xception = tf.keras.applications.Xception(weights = 'imagenet', include_top = False, input_shape = (256, 256, 3))

  x = xception.output
  x = GlobalAveragePooling2D()(x)
  x = Dense(256, activation = 'relu')(x)
  x = Dropout(0.25)(x)
  predictions = Dense(10)(x)

  model = tf.keras.Model(inputs = xception.input, outputs = predictions)

  model.compile(
      loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),
      optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-8),
      metrics = ['accuracy']
  )

  batch_size = 7
  for epoch in range(5):
    X = np.random.rand(batch_size, 256, 256, 3)
    Y = np.zeros(batch_size)
    model.fit(X, Y, batch_size = batch_size, epochs = 1)
```

What i've found?
- if using TPU, batch_size < 8 fitting gives NaN loss after second epoch (or second batch, which is the same)
- if using TPU, batch_size < 8 and X is multiplied by zero, fitting gives NaN loss after first epoch
- if using TPU, batch_size >= 8, fitting doesn't give NaN loss
- if using TPU, batch_size >= 8 and X is multiplied by zero, fitting gives NaN loss after first epoch
- if using CPU, fitting doesn't give NaN loss in the cases described above

So, fitting gives NaN loss if we are using TPU and batch_size < 8 or X is an array of zeros.
I tested batch sizes 1, 7, 8, 64.
To train on CPU i used this command:
```
with tf.device('/job:localhost/replica:0/task:0/device:CPU:0'):
```
or just used Colab version with only CPU, which gives the same results.

Same behaviour occured when I was training this model on real data: fitting immediately led to NaN loss when some batch had less than 8 image-label pairs in it, for example when last batch in dataset was smaller.

I guess that it can be caused by less-precise float on TPU or by some error in parallel calculations on 8 TPU cores (/device:TPU:0, ..., /device:TPU:7)."
41634,batch_to_space & space_to_batch usage.,"This is a request to improve the documentation to add some examples/use cases.

## URL(s) with the issue:
N/A

Please provide a link to the documentation entry, for example:
**https://www.tensorflow.org/api_docs/python/tf/space_to_batch
https://www.tensorflow.org/api_docs/python/tf/batch_to_space**

## Description of issue (what needs changing):
It's not very clear where/how these two (space_to_batch & batch_to_space) operations are used.

### Clear description
Some examples of how/where these operations are used will help.

### Correct links
https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/ops/array_ops.py#L3676-L3678
https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/ops/array_ops.py#L3735-L3849

### Parameters defined
N/A

### Returns defined
N/A

### Raises listed and defined
N/A

### Usage example
This is exactly what's required: usage example

### Request visuals, if applicable
Possibly..

### Submit a pull request?
N/A"
41632,ModelCheckpoint save_freq incompatible with Model.fit validation_freq,"please see [this issue](https://github.com/keras-team/keras/issues/13689) filed on the now unsupported keras repo.  @gattia there has it [figured out](https://github.com/keras-team/keras/issues/13689#issuecomment-583758348) i believe.  would be nice to have a work around.

in brief, when using `ModelCheckpoint(save_freq=<int>...` combined with `model.fit(validation_freq=<int>...` then ""WARNING:tensorflow:Can save best model only with val_loss available, skipping."" is emitted. even when you've carefully calculated the two to be the same number of epochs."
41630,CudnnLSTM variable sequence length sometimes fails with CUDNN_STATUS_EXECUTION_FAILED,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian/Sid (2020-07-01), Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source and binary
- TensorFlow version (use command below): 1.15
- Python version: 3.6, 3.7.8
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): 9.0
- CUDA/cuDNN version: 10.0/7.4.1 ; 10.0/7.4.2.1 ; 10.0/7.5.1.10 ; 10.0/7.6.5.32
- GPU model and memory: 2x RTX 2080 Ti ; 4x GTX 1080 Ti ; 

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`: v1.15.3-0-g4386a6640c


**Describe the current behavior**
Training with some dataset triggers:
```
2020-07-22 16:15:42.108252: E tensorflow/stream_executor/dnn.cc:588] CUDNN_STATUS_EXECUTION_FAILED                                                                                                          
in tensorflow/stream_executor/cuda/cuda_dnn.cc(1778): 'cudnnRNNForwardTrainingEx( cudnn.handle(), rnn_desc.handle(), input_desc.data_handle(), input_data.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), rnn_desc.params_handle(), params.opaque(), output_desc.data_handle(), output_data->opaque(), output_h_desc.handle(), output_h_data->opaque(), output_c_de
sc.handle(), output_c_data->opaque(), nullptr, nullptr, nullptr, nullptr, nullptr, nullptr, nullptr, nullptr, workspace.opaque(), workspace.size(), reserve_space.opaque(), reserve_space.size())'
2020-07-22 16:15:42.108385: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at cudnn_rnn_ops.cc:1527 : Internal: Failed to call ThenRnnForward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 2048, 2048, 1, 75, 2, 2048] 
```

**Describe the expected behavior**
Training should succeed, or TensorFlow or CUDNN should expose a more actionable error

**Standalone code to reproduce the issue**
Will be provided after.

**Other info / logs**
Will be provided after. Some noisy debugging session can be seen at https://github.com/mozilla/DeepSpeech/issues/3088
"
41629,Parallelize and prioritize model inference on ARM CPU with tensorflow lite.,"I have two different neural networks running with tensorflow lite on an embedded device, using an arm cpu. 
My questions are: How can I execute both models at parallel? How can I give one of the two models a higher priority than the other one? The background is: I do not want one model to block or interrept execution of the other model.

"
41627,tf.debugging is not compatible with symbolic Keras tensors,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Ubuntu 16.04
- TensorFlow installed from: binary
- TensorFlow version: 2.2.0
- Python version: 3.6.9

**Describe the current behavior**

Functions from `tf.debugging` (such as `tf.debugging.assert_equal`) raise an exception when passing a Keras tensor as argument.

It appears that the functions run the eager code path even when one of the input is a symbolic tensor.

**Describe the expected behavior**

`tf.debugging` functions should work with Keras tensors.

**Standalone code to reproduce the issue**

```python
import tensorflow as tf
x = tf.keras.Input(shape=[5], batch_size=2)
batch_size = tf.shape(x)[0]
tf.debugging.assert_equal(batch_size, 2)
```

**Other info / logs**

```text
Traceback (most recent call last):
  File ""assert.py"", line 4, in <module>
    tf.debugging.assert_equal(batch_size, 2)
  File ""/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py"", line 648, in assert_equal_v2
    return assert_equal(x=x, y=y, summarize=summarize, message=message, name=name)
  File ""/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py"", line 659, in assert_equal
    data, summarize, message, name)
  File ""/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py"", line 334, in _binary_assert
    if condition:
  File ""/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 778, in __bool__
    self._disallow_bool_casting()
  File ""/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 548, in _disallow_bool_casting
    self._disallow_in_graph_mode(""using a `tf.Tensor` as a Python `bool`"")
  File ""/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 537, in _disallow_in_graph_mode
    "" this function with @tf.function."".format(task))
tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.
```"
41626,[XLA] Unintuitive behavior of jit_scope,"As far as I can tell, the implementation of `MarkForCompilation` relates to `jit_scope` (`_XlaCompile` attribute) does the following steps:
1. Remove the nodes whose `_XlaCompile` attribute is set to false from the compilation candidates (using `CompilationDisallowedByXlaCompileAttr`).
2. Create a cluster for each candidate using `_XlaScope` as the scope (`absl::nullopt` otherwise).
3. Cluster the one-node clusters and set `is_xla_compile_attr_true` to true if any node in the cluster has `_XlaCompile` true.
4. Add the `kXlaClusterAttr` to node if the `is_xla_compile_attr_true` of its cluster is true (as in `ShouldCompileClusterImpl`).

The problem of the above steps is that if we want to only cluster part of the model, for example:
```python
def stage1(input):
  # stage 1 of the model

def stage2(x):
  # stage2 of the model

def loss(x):
  # ...

x = stage1(input)
with tf.xla.experimental.jit_scope():
  x = stage2(x)
l = loss(x)
train_op = optimizer.minimize(l)

# ...
```
The unlabeled part of the model may still be clustered and compiled. In practice, such situation happens a lot in the backprop part of the model, where the unlabeled loss gradient ops are clustered with the gradient ops inside `jit_scope`. I wonder if this is the expected behavior and I think that it is different from the intuitive idea that only the ops in `jit_scope` are compiled.

One walkaround to this problem is adding a global `jit_scope(compile_ops=False)`:

```python
with tf.xla.experimental.jit_scope(compile_ops=False):
  x = stage1(input)
  with tf.xla.experimental.jit_scope():
    x = stage2(x)
  l = loss(x)
  train_op = optimizer.minimize(l)
```

Following are the comparison between the origin result and the walkaround

without the global `jit_scope(compile_ops=False)`:
![image](https://user-images.githubusercontent.com/10428324/88171134-678e0780-cc51-11ea-8b96-32ad29245a35.png)

with the global `jit_scope(compile_ops=False)`:
![image](https://user-images.githubusercontent.com/10428324/88171067-4b8a6600-cc51-11ea-9ad4-7895fc830b22.png)

The red part of the timeline is the `XlaRun` op.

I think that to solve this problem, we only need to remove the unlabeled ops from the compilation candidates when global jit level is not set. It will be great if the XLA team would like to share some idea on the current behavior of `jit_scope`.

Also, is `global_jit_level` designed to be override the `jit_scope`? It seems that if `global_jit_level` is set, it will ignore the `_XlaScope` attribute and use `_XlaInternalScope` instead. In this way, if one want to compile the whole model, is he or she allowed to separate the cluster arbitrarily?

Thank you for your time on this issue :).

Gently ping @sanjoy @cheshire @tpopp ."
41625,"Logging of ""TensorFlow with TPUs"" example in Google Colab is happening Twice","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): NA, as the issue can be replicated in Google Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Available by Default in Google Colab
- TensorFlow version (use command below): 2.2

**Describe the current behavior** :

In the [TensorFlow with TPUs Tutorial](https://colab.research.google.com/notebooks/tpu.ipynb#scrollTo=FpvUOuC3j27n) present in Google Colab, the Logging is happening Twice, as shown below:

```
INFO:tensorflow:Initializing the TPU system: 10.4.82.210:8470
INFO:tensorflow:Initializing the TPU system: 10.4.82.210:8470
INFO:tensorflow:Clearing out eager caches
INFO:tensorflow:Clearing out eager caches
INFO:tensorflow:Finished initializing TPU system.
INFO:tensorflow:Finished initializing TPU system.
INFO:tensorflow:Found TPU system:
INFO:tensorflow:Found TPU system:
INFO:tensorflow:*** Num TPU Cores: 8
INFO:tensorflow:*** Num TPU Cores: 8
INFO:tensorflow:*** Num TPU Workers: 1
INFO:tensorflow:*** Num TPU Workers: 1
INFO:tensorflow:*** Num TPU Cores Per Worker: 8
INFO:tensorflow:*** Num TPU Cores Per Worker: 8
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)
```

**Describe the expected behavior**:

Logging should happen only once. This has been discussed in this [Stack Overflow Question](https://stackoverflow.com/questions/33662648/tensorflow-causes-logging-messages-to-double) but is not fixed yet.

**Standalone code to reproduce the issue**:
Colab link is https://colab.research.google.com/notebooks/tpu.ipynb#scrollTo=FpvUOuC3j27n. Same is the case in the [Tensorflow TPU Tutorial](https://www.tensorflow.org/guide/tpu#tpu_initialization) as well."
41623,tf error : Failed to load the native TensorFlow runtime.,"
![erreur](https://user-images.githubusercontent.com/68639810/88160941-a56a3c00-cc0f-11ea-8416-0a869e5818a1.png)


**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): 
- TensorFlow version: 2.1
- Python version: 3.7
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10
- GPU model and memory: NVIDIA Quattro 2000

I have, at the origins tf2.1, which works very well. For some Reason I have wanted to update to tf2.2. But when I did it ""import tensorflow"" make this error. So I thought that I aven't good composante like GPU, and I try to go back to tf1.15 but nothing changed. so I decided to comme back to the 2.1 version. And now I have also this error. I Don't undestand how a version could worked in the past  but not now.

I precise that I read a lot of issues and try a lot of thinks but nothing works.
Please help me
"
41621,Not found: Key dense/kernel not found in checkpoint,"I am trying to write an encoder/decoder model for a sequence using the TensorFlow estimator and tfa.seq2seq. It is running fine for training and loading the checkpoints when I am training it again but it is not working with the inference. Seems like the encoder is alright but there is some issue with the decoder. 

Here is the training decoder code which estimator.train calls and it is working fine:

```
decoder_emb_inp = tf.compat.v1.nn.embedding_lookup(self.embedding_decoder, target_input)
sampler =  tfa.seq2seq.TrainingSampler(time_major=False)
my_decoder = tfa.seq2seq.BasicDecoder(cell, sampler, output_layer=None)

outputs, final_context_state, _ = tfa.seq2seq.dynamic_decode(decoder = my_decoder,swap_memory = False,scope ='decoder_scope',
	output_time_major=False,decoder_init_input= decoder_emb_inp,
	decoder_init_kwargs= {'initial_state' : decoder_initial_state,'sequence_length': tf.tile([self.length], [self.batch_size])})

sample_id = outputs.sample_id
logits = self.output_layer(outputs.rnn_output) #self.output_layer=tf.keras.layers.Dense(self.vocab_size)
```

This is the code I am using for inference:

```
start_tokens = tf.fill([self.batch_size], tf.constant(0))
end_token = tf.constant(0)

sampler =  tfa.seq2seq.GreedyEmbeddingSampler()
my_decoder = MyDecoder(cell, sampler, output_layer=self.output_layer)        
outputs, final_context_state, _ = tfa.seq2seq.dynamic_decode(decoder=my_decoder,maximum_iterations=self.length, 
	swap_memory=True, scope='decoder_scope',decoder_init_input=self.embedding_decoder,
	decoder_init_kwargs= {'initial_state' : decoder_initial_state,'start_tokens': start_tokens, 'end_token': end_token})
        
logits = outputs.rnn_output
sample_id = outputs.sample_id
```

In inference I'm getting this error:

```
tensorflow.python.framework.errors_impl.NotFoundError: Key BasicDecoderStep/my_dense/kernel not found in checkpoint
         [[{{node save/RestoreV2}}]]
```

I have tried modifying the output layer, but it is not working. Moreover, if I delete all the checkpoints in the model_dir (passed to estimator), the inference code initializes everything and works fine but doesn't make right predictions.

So how do I make inference part read the checkpoints which were created during training and not throw the above error?

I would appreciate any help to fix this error.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): 
- TensorFlow version (use command below): 2.2
- Python version: 3.6
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: 
- GPU model and memory: NA"
41620,Definition of dynamic-shape variable error,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Conda repo
- TensorFlow version (use command below): 1.15
- Python version: 3.7.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1
- GPU model and memory: Tesla V100-SMX3-32GB

**Describe the current behavior**

tensorflow.python.framework.errors_impl.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [] rhs shape= [1,1]
         [[{{node Variable/Assign}}]]

**Describe the expected behavior**

No error

**Standalone code to reproduce the issue**

```
import tensorflow as tf
import numpy as np
import os
os.environ[""CUDA_VISIBLE_DEVICES""]='0'

with tf.Session() as sess:
    v = tf.Variable(np.zeros(shape=[1,1]),shape=tf.TensorShape(None))
    sess.run(tf.global_variables_initializer())
```

Obseration:
The error did not appear when I use eager_execution_mode()

Code:
```
tf.enable_eager_execution()
v = tf.Variable(np.zeros([1,1]),shape=tf.TensorShape(None))
tf.print(v)
v.assign(np.ones([2,2]))
tf.print(v)
```
Output:
```
[[0]]
[[1 1]
 [1 1]]
```


[tensorshape_bug.log](https://github.com/tensorflow/tensorflow/files/4958503/tensorshape_bug.log)"
41619,Error in loading tflite model at runtime from native C++ library.,"**System information**

OS Platform and Distribution (windows 10):
Mobile device (oppo reno2) i am building and running on this device through android studio.
TensorFlow installed from (source or binary): Ubuntu terminal through pip3
TensorFlow version:
Python version: python 3.8.2
Installed using pip
Bazel version (if compiling from source): I have created libtensorflowlite.so using bazle(2.0.0) . Now trying to directly use the lib.
GCC/Compiler version (if compiling from source): gnu++11
CUDA/cuDNN version:
GPU model and memory:

**Describe the current behavior**
Trying to load trained model in native source code but getting an error ""Could not open '/deeplab/deeplabv3_257_mv_gpu.tflite'."" I have built libtensorflowlite.so through bazel for arm64.

Code:-
Java_com_example_myapplication_MainActivity_stringFromJNI(JNIEnv *env, jobject thiz) {
    string model_file = ""/deeplab/deeplabv3_257_mv_gpu.tflite"";
    StderrReporter error_reporter;
    unique_ptr<tflite::FlatBufferModel> model;

    model = FlatBufferModel::BuildFromFile(model_file.c_str(),
                                           &error_reporter);
    ops::builtin::BuiltinOpResolver resolver;
    std::unique_ptr<tflite::Interpreter> interpreter;
    if(model){
        InterpreterBuilder(*model, resolver)(&interpreter);
        interpreter->AllocateTensors();
    }else{
        return env->NewStringUTF(""Model is Null"");
    }
    return env->NewStringUTF(""Hi from JNI LIBS!"");
}
![Code_1](https://user-images.githubusercontent.com/68632656/88143714-5a532780-cc15-11ea-972b-59deda7e0f6b.PNG)

Tried to keep model at different locations like inside cpp folder where source file is present or inside assets folder parallel to cpp folder but still same error is coming.
Also tried to put model in mobile sdcard  and passing that path but still no luck.
**Describe the expected behavior**
Model should load.

**Other info / logs** 

2020-07-22 12:13:07.773 13653-13653/com.example.myapplication D/Main: loaded
2020-07-22 12:13:07.875 13653-13653/com.example.myapplication E/tflite: Could not open '/deeplab/deeplabv3_257_mv_gpu.tflite'.

"
41617,Doe tensorflow has api  similar to pytorch's “masked_fill_”,"pytorch has squeeze , tf also has squeeze.
pytorch has unsqueeze, tf has expand_dims do the same thing with diff name.
pythorh has masked_fill , tf has ?

<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):  2.2.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
   [masked_fill_ api doc in pytorch](https://pytorch.org/docs/stable/tensors.html?highlight=masked_fill_#torch.Tensor.masked_fill_)
**Will this change the current api? How?**
   Maybe not. It will add a new api. 
**Who will benefit with this feature?**
    someone try to migrate from pytorch to tensorflow like me.
**Any Other info.**
   "
41616,training hangs using `tf.keras.model.fit` on `tf.data.Dataset` in GAN,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.4
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2.0
- Python version: 3.6.9
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: CUDA 10.1.243/cuDNN 7.6.5
- GPU model and memory: Tesla K80

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

after training for any number of steps equivalent to processing roughly 200 images, the training hangs and does not progress. reports CPU usage to be at over 100% even though the GPU is available and being used.

**Describe the expected behavior**

the training continues as normal at a fairly consistent pace.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

    ...
    31/Unknown - 12s 380ms/step - psnr: 21.3111 - sharpdiff: 14.1349 - disc_loss: 0.6919 - gen_loss: 363.1030 
    32/Unknown - 12s 379ms/step - psnr: 21.3856 - sharpdiff: 14.1562 - disc_loss: 0.6919 - gen_loss: 355.9599  
    33/Unknown - 12s 379ms/step - psnr: 21.4397 - sharpdiff: 14.1673 - disc_loss: 0.6919 - gen_loss: 348.9806  
    34/Unknown - 13s 380ms/step - psnr: 21.5334 - sharpdiff: 14.1921 - disc_loss: 0.6919 - gen_loss: 341.5822
    <hangs here>

the call to `model.fit`:

    checkpoint = tf.keras.callbacks.ModelCheckpoint \
    (
        filepath = os.path.join(util.get_dir(""save/checkpoints""), ""checkpt""),
        save_weights_only = True,
        save_freq = 25
    );

    image_save = callbacks.ImageSave(util.get_dir(""save/images""));

    model = models.GAN(num_scales = 4, hist_len = 4, pred_len = 1);
    model.compile \
    (
        disc_optim = tf.keras.optimizers.Adam(learning_rate = 1e-6, beta_1 = 0.89, beta_2 = 0.99),
        gen_optim = tf.keras.optimizers.Adam(learning_rate = 1e-4, beta_1 = 0.89, beta_2 = 0.99),
        disc_loss = losses.Adversarial(),
        gen_loss = losses.Combined(),
        metrics = [ metrics.PSNR(), metrics.SharpDiff() ],
        run_eagerly = True
    );

    model.fit \
    (
        dataset,
        epochs = epochs,
        callbacks = [ checkpoint, image_save ]
    );

the losses, metrics, `GAN` model, and the `ImageSave` callback are custom (subclassed in keras), if it matters."
41614,Training with Keras mixed precision policy crashes.,"@DEKHTIARJonathan and @nluehr for visibility.

<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No - Followed stock example to write a CTL

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
NAME=""Ubuntu""
VERSION=""18.04.3 LTS (Bionic Beaver)""

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No

- TensorFlow installed from (source or binary):
container built on tensorflow/tensorflow:nightly-gpu

- TensorFlow version (use command below):
v1.12.1-37099-g3873154276 2.4.0-dev20200721

- Python version:
Python 3.6.9

- CUDA/cuDNN version:
Cuda compilation tools, release 10.1, V10.1.243

- GPU model and memory:
V100 32G

**Describe the current behavior**
Using mixed precision training with keras mixed precision policy, seems like nodes aren't being casted to FP16

```Traceback (most recent call last):
  File ""run_tf_squad.py"", line 615, in <module>
    main()
  File ""run_tf_squad.py"", line 343, in main
    model = TFElectraForQuestionAnswering.from_pretrained(electra_model, config=config, cache_dir=args.cache_dir, args=args)
  File ""/workspace/electra/modeling_utils.py"", line 406, in from_pretrained
    model(model.dummy_inputs, training=False)  # build the network with dummy inputs
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 986, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File ""/workspace/electra/modeling.py"", line 811, in call
    input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, training=training
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 986, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File ""/workspace/electra/modeling.py"", line 278, in call
    hidden_states = self.embeddings([input_ids, position_ids, token_type_ids, inputs_embeds], training=training)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 986, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File ""/workspace/electra/modeling.py"", line 82, in call
    return self._embedding(inputs, training=training)
  File ""/workspace/electra/modeling.py"", line 107, in _embedding
    embeddings = inputs_embeds + position_embeddings + token_type_embeddings
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py"", line 1126, in binary_op_wrapper
    return func(x, y, name=name)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py"", line 201, in wrapper
    return target(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py"", line 1448, in _add_dispatch
    return gen_math_ops.add_v2(x, y, name=name)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py"", line 487, in add_v2
    _ops.raise_from_not_ok_status(e, name)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 6886, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a half tensor but is a float tensor [Op:AddV2]
```


**Describe the expected behavior**
Mixed-precision training should start without any issues.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
1) git clone https://github.com/sharathts/electra.git
2) cd electra
3) bash scripts/docker/build.sh
4) bash scripts/docker/launch.sh
5) python run_tf_squad.py --init_checkpoint=None --do_train --train_batch_size=16    --data_dir /workspace/electra/data/download/squad/v1.1  --do_lower_case --electra_model=google/electra-base-discriminator  --learning_rate=4e-4  --warmup_proportion 0.05  --weight_decay_rate 0.01  --layerwise_lr_decay 0.8  --seed=1  --num_train_epochs=2  --max_seq_length=384  --doc_stride=128  --beam_size 4  --joint_head True  --null_score_diff_threshold -5.6  --output_dir=results/   --amp  --cache_dir=/workspace/electra/data/download/squad/v1.1  --max_steps=-1
```
**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Traceback has been provided in the ""Expected behaviour"" section."
41613,detect.tflite failure converting TF model to TFlite,"I am using my mac, running Catalina 10.15.5, with Python 2.7.17, running Tensorflow 1.15.3 from source, installed using pip, and my bazel version is 0.26.1.
- GCC/Compiler version (if compiling from source): Unsure
- CUDA/cuDNN version: N/A

I am trying to convert my custom model on ssd_mobilenet_v3_small_coco to TFlite, following this tutorial: [Step 3](https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi) but cannot convert my tflite_graph.pb to detect.tflite using this line:
`bazel run --config=opt tensorflow/lite/toco:toco -- --input_file=/Users/jp3spinelli/Desktop/models/research/object_detection/TFLite_model/tflite_graph.pb --output_file=/Users/jp3spinelli/Desktop/models/research/object_detection/TFLite_model/detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays=TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3 --inference_type=FLOAT --allow_custom_ops`

I keep running into this error (I only included the last bit because it's quite lengthy):
```
WARNING: /Users/jp3spinelli/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/strings:proto_text_util.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /Users/jp3spinelli/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/strings:scanner.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /Users/jp3spinelli/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/strings:strcat.cc' directly. You should either move the file to this package or depend on an appropriate rule there
INFO: Analyzed target //tensorflow/lite/toco:toco (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
Target //tensorflow/lite/toco:toco up-to-date:
  bazel-bin/tensorflow/lite/toco/toco
INFO: Elapsed time: 0.472s, Critical Path: 0.00s
INFO: 0 processes.
INFO: Build completed successfully, 1 total action
INFO: Running command line: bazel-bin/tensorflow/lite/toco/toco '--input_file=/Users/jp3spinelli/Desktop/models/research/object_detection/TFLite_model/tflite_graph.pb' '--output_file=/Users/jp3spinelli/Desktop/models/research/object_detection/TFLite_model/detect.tflite' '--input_shapes=1,300,300,3' '--input_arrays=normalized_input_image_tensor' '--output_arrays=TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_DetectioINFO: Build completed successfully, 1 total action
2020-07-21 18:03:47.675656: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TFLite_Detection_PostProcess
2020-07-21 18:03:47.691874: F tensorflow/lite/toco/tooling_util.cc:1669] Check failed: input_array_dims[i] == input_array_proto.shape().dims(i) (320 vs. 300)
Abort trap: 6
```
It is creating a file, but it has zero bytes so I know something is wrong.
I am working out of my tensorflow directory, which is in my home directory. One weird thing I noticed is that the path to my ""models"" folder on my Desktop says it starts in ""iCloud Drive"" not ""Users.""

Please let me know how to fix this, I am new with coding so I need some step-by-step instructions. Thanks!"
41612,Where is Tensorflow Lite Support Library ,"Here is the guide to Tensorflow Lite Support Library:
https://www.tensorflow.org/lite/guide/lite_support

An exert:

> The [TensorFlow Lite Android Support Library](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/support/java) is designed to help process the input and output of TensorFlow Lite models, and make the TensorFlow Lite interpreter easier to use.

Which links to:
https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/support/java

which is 404 Page not found.
 "
41610,Seed for dropout in LSTM - Difference in model(X) and model.predict(X),"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d382ca 2.0.0
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Outputs for LSTM layer in tensorflow when using model(X) and model.predict(X) differ when using dropout.

Let's call the output of model(X) as Fwd Pass and model.predict(X) as Prediction

For a regular dropout layer, we can specify the seed but LSTM layer doesn't have such an argument. I'm guessing this is causing the difference between these Fwd Pass and Prediction.

In the following code sample, if dropout=0.4, these the outputs are different but when dropout=0.0 they match exactly. This makes me believe that every evaluation is using a different operation level seed but there is no way to set that.

PS: I want to use dropout during inference, so that is by design.

**Describe the expected behavior**
**Fwd Pass** and **Prediction** should be exactly the same when using dropout.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
import os
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.initializers import GlorotUniform

SEED = 200
HIDDEN_UNITS = 4
N_OUTPUTS = 1
N_INPUTS = 4
BATCH_SIZE = 4
N_SAMPLES = 4

np.random.seed(SEED)
tf.random.set_seed(SEED)


# Simple LSTM Model
def my_model():
    inputs = x = keras.Input(shape=(N_INPUTS, 1))
    initializer = GlorotUniform(seed=SEED)

    x = layers.LSTM(HIDDEN_UNITS,
                     kernel_initializer=initializer,
                     recurrent_dropout=0.0,
                     dropout=0.4,
                     # return_sequences=True,
                     use_bias=False)(x, training=True)

    output = x
    model = keras.Model(inputs=inputs, outputs=[output])
    return model

# Create Sample Data
# Target Function
def f_x(x):
    y = x[:, 0] + x[:, 1] ** 2 + np.sin(x[:, 2]) + np.sin(x[:, 3] ** 3)
    y = y[:, np.newaxis]
    return y

# Generate random inputs
d = np.linspace(0.1, 1, N_SAMPLES)
X = np.transpose(np.vstack([d*0.25, d*0.5, d*0.75, d]))
X = X[:, :, np.newaxis]
Y = f_x(X)

# PRINT FWD PASS
model = my_model()
n_out = model(X).numpy()
print('FWD PASS:')
print(n_out, '\n')

# PRINT PREDICT OUTPUT
print('PREDICT:')
out = model.predict(X)
print(out)
```

**Output (dropout=0.4) - do not match**

```
FWD PASS:
[[ 0.          0.          0.          0.        ]
 [ 0.          0.          0.          0.        ]
 [ 0.0526864  -0.13284351  0.02326298 -0.30357683]
 [ 0.06297918 -0.14084947  0.02214929 -0.44425806]] 

PREDICT:
[[ 0.00975818 -0.029404    0.00678372 -0.03232396]
 [ 0.0347842  -0.0974849   0.01938616 -0.15696262]
 [ 0.          0.          0.          0.        ]
 [ 0.06297918 -0.14084947  0.02214929 -0.44425806]]
```

**Output (dropout=0.0) - no dropout, outputs match**

```
FWD PASS:
[[ 0.00593475 -0.01799661  0.00424165 -0.01876264]
 [ 0.02226446 -0.06519517  0.01399653 -0.08595844]
 [ 0.03620889 -0.10084937  0.01987283 -0.1663805 ]
 [ 0.0475584  -0.12453148  0.02269932 -0.2541136 ]] 

PREDICT:
[[ 0.00593475 -0.01799661  0.00424165 -0.01876264]
 [ 0.02226446 -0.06519517  0.01399653 -0.08595844]
 [ 0.03620889 -0.10084937  0.01987283 -0.1663805 ]
 [ 0.0475584  -0.12453148  0.02269932 -0.2541136 ]]
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached."
41608,"Converted TFLite-model produces wrong results, while PB-model produces correct result","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS
- TensorFlow installed from (source or binary):  `pip install tf-nightly`
- TensorFlow version (or github SHA if from source): 2.4.0-dev20200721


**Command used to run the converter or code if you’re using the Python API**

Colab (GPU is disabled): https://colab.research.google.com/gist/AlexeyAB/07e7aa3c9ab49f1d733153f64d6fd270/onnx_to_tf_to_tflite.ipynb

```
toco --graph_def_file model-f46da743.pb --output_file model-f46da743.tflite --output_format TFLITE --inference_type FLOAT --inference_input_type FLOAT --input_arrays 0 --output_arrays 1195 --enable_v1_converter --target_ops=SELECT_TF_OPS
```

**The output from the converter invocation**

```
2020-07-21 19:15:30.657556: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-07-21 19:15:32.252326: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-07-21 19:15:32.255365: E tensorflow/stream_executor/cuda/cuda_driver.cc:314] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2020-07-21 19:15:32.255412: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (71fb61714d13): /proc/driver/nvidia/version does not exist
2020-07-21 19:15:32.255749: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-07-21 19:15:32.261434: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2200000000 Hz
2020-07-21 19:15:32.261660: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1aeef40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-07-21 19:15:32.261724: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
I0721 19:15:35.806807 139688388913024 lite.py:1321] Using experimental converter: If you encountered a problem please file a bug. You can opt-out by setting experimental_new_converter=False
2020-07-21 19:15:36.648004: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:315] Ignored output_format.
2020-07-21 19:15:36.648067: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:318] Ignored drop_control_dependency.
```

**Also, please include a link to the saved model or GraphDef**


* [model-f46da743.pb](https://github.com/AlexeyAB/tensorflow-tools/releases/download/tmp_version/model-f46da743.pb)

* [model-f46da743.tflite](https://github.com/AlexeyAB/tensorflow-tools/releases/download/tmp_version/model-f46da743.tflite)


**Failure details**
The conversion is successful, but the generated model is wrong,
- Producing wrong results

**Any other info / logs**

I convert model from ONNX to PB and it works well with `PB-model + TensorFlow`, it shows desired **good result**:
![good](https://user-images.githubusercontent.com/4096485/88097044-43262280-cba0-11ea-8dc9-5b8fff654485.png)

But when I convert PB to TFLITE, it works with `TFLITE-model + TensorFlow`, but it shows unexpected **bad result**:
![bad](https://user-images.githubusercontent.com/4096485/88097079-4f11e480-cba0-11ea-9ee0-600b5899d687.png)


Reprocudable commands and code (GPU is disabled): https://colab.research.google.com/gist/AlexeyAB/07e7aa3c9ab49f1d733153f64d6fd270/onnx_to_tf_to_tflite.ipynb
"
41606,ImportError: cannot import name 'tensorflow' from 'opt_einsum.backends' ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version:2.1
- Python version:3.7.7
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.1 cuda
- GPU model and memory: GTX 1650



**Describe the problem**
Another environment with tensorflow cpu has setup with no issues. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
SET PATH=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1\bin;%PATH%
SET PATH=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1\extras\CUPTI\lib64;%PATH%
SET PATH=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1\include;%PATH%
SET PATH=C:\tools\cuda\bin;%PATH%
I have set these variables too

**Any other info / logs**
>>> import tensorflow
2020-07-22 00:42:00.297809: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Lenovo\anaconda3\envs\tf-gpu-cuda10\lib\site-packages\tensorflow\__init__.py"", line 101, in <module>
    from tensorflow_core import *
  File ""C:\Users\Lenovo\anaconda3\envs\tf-gpu-cuda10\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\Lenovo\anaconda3\envs\tf-gpu-cuda10\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\Lenovo\anaconda3\envs\tf-gpu-cuda10\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\Lenovo\anaconda3\envs\tf-gpu-cuda10\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\Lenovo\anaconda3\envs\tf-gpu-cuda10\lib\site-packages\tensorflow_core\python\__init__.py"", line 85, in <module>
    from tensorflow.python.ops.standard_ops import *
  File ""C:\Users\Lenovo\anaconda3\envs\tf-gpu-cuda10\lib\site-packages\tensorflow_core\python\ops\standard_ops.py"", line 48, in <module>
    from tensorflow.python.ops.special_math_ops import *
  File ""C:\Users\Lenovo\anaconda3\envs\tf-gpu-cuda10\lib\site-packages\tensorflow_core\python\ops\special_math_ops.py"", line 30, in <module>
    import opt_einsum
  File ""C:\Users\Lenovo\AppData\Roaming\Python\Python37\site-packages\opt_einsum\__init__.py"", line 9, in <module>
    from .contract import contract, contract_path, contract_expression
  File ""C:\Users\Lenovo\AppData\Roaming\Python\Python37\site-packages\opt_einsum\contract.py"", line 10, in <module>
    from . import backends, blas, helpers, parser, paths, sharing
  File ""C:\Users\Lenovo\AppData\Roaming\Python\Python37\site-packages\opt_einsum\backends\__init__.py"", line 7, in <module>
    from .dispatch import (get_func, has_einsum, has_tensordot, build_expression, evaluate_constants, has_backend)
  File ""C:\Users\Lenovo\AppData\Roaming\Python\Python37\site-packages\opt_einsum\backends\dispatch.py"", line 13, in <module>
    from . import tensorflow as _tensorflow
ImportError: cannot import name 'tensorflow' from 'opt_einsum.backends' (C:\Users\Lenovo\AppData\Roaming\Python\Python37\site-packages\opt_einsum\backends\__init__.py)
"
41604,"How can I run Tensorflow on one single core, single thread CPP?","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 1.12.0
- Python version: 3.6
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
I am trying to restrict the number of threads that TensorFlow spawns. In python, I understand we need to use the following steps as pointed out [Here](https://stackoverflow.com/questions/38187808/how-can-i-run-tensorflow-on-one-single-core) I was trying to do the same in CPP, but it doesn't seem that straight forward. 
Questions:
How to modify intra_op_parallelism_threads and inter_op_parallelism_threads correctly?
How to modify the device_count to control the core as well?

**Provide the exact sequence of commands / steps that you executed before running into the problem**
SessionOptions options;
    ConfigProto* config = &options.config;
    string key = ""CPU"";
    //not sure if this is the correct way to do it.
    (*config->mutable_device_count())[key] = 1; 
    config->set_inter_op_parallelism_threads(1);
    config->set_intra_op_parallelism_threads(1);


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
41602,Unable to import tensorflow 2.2.0 in conda environment,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.2.0
- Python version:3.7.6
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
Error Details - 

    ***---------------------------------------------------------------------------
    ImportError                               Traceback (most recent call last)
    ~\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
         57 
    ---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
         59 
    
    ~\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in <module>
         27             return _mod
    ---> 28     _pywrap_tensorflow_internal = swig_import_helper()
         29     del swig_import_helper
    
    ~\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
         23             try:
    ---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
         25             finally:
    
    ~\.conda\envs\mldeep\lib\imp.py in load_module(name, file, filename, details)
        241         else:
    --> 242             return load_dynamic(name, filename, file)
        243     elif type_ == PKG_DIRECTORY:
    
    ~\.conda\envs\mldeep\lib\imp.py in load_dynamic(name, path, file)
        341             name=name, loader=loader, origin=path)
    --> 342         return _load(spec)
        343 
    
    ImportError: DLL load failed: The specified module could not be found.
    
    During handling of the above exception, another exception occurred:
    
    ImportError                               Traceback (most recent call last)
    <ipython-input-1-d6579f534729> in <module>
    ----> 1 import tensorflow
    
    ~\AppData\Roaming\Python\Python37\site-packages\tensorflow\__init__.py in <module>
         39 import sys as _sys
         40 
    ---> 41 from tensorflow.python.tools import module_util as _module_util
         42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader
         43 
    
    ~\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\__init__.py in <module>
         48 import numpy as np
         49 
    ---> 50 from tensorflow.python import pywrap_tensorflow
         51 
         52 # Protocol buffers
    
    ~\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
         67 for some common reasons and solutions.  Include the entire stack trace
         68 above this error message when asking for help."""""" % traceback.format_exc()
    ---> 69   raise ImportError(msg)
         70 
         71 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long
    
    ImportError: Traceback (most recent call last):
      File ""C:\Users\Vinayak\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
        from tensorflow.python.pywrap_tensorflow_internal import *
      File ""C:\Users\Vinayak\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
        _pywrap_tensorflow_internal = swig_import_helper()
      File ""C:\Users\Vinayak\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
        _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
      File ""C:\Users\Vinayak\.conda\envs\mldeep\lib\imp.py"", line 242, in load_module
        return load_dynamic(name, filename, file)
      File ""C:\Users\Vinayak\.conda\envs\mldeep\lib\imp.py"", line 342, in load_dynamic
        return _load(spec)
    ImportError: DLL load failed: The specified module could not be found.
    
    
    Failed to load the native TensorFlow runtime.
    
    ***

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I am getting below error on importing tensorflow. The steps I followed were -
> Installed anaconda 
> Created conda env and installed using conda command prompt.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
41600,Pylint sanity check using docker stays in a hung state.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO
- TensorFlow installed from (source or binary):  -
- TensorFlow version (use command below): -
- Python version: -
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: -
- GPU model and memory: -

**Describe the current behavior**

When the following command is executed to perform sanity checks using a docker container, the pylint step just hangs and doesn't complete.

```
$  tensorflow/tools/ci_build/ci_build.sh CPU tensorflow/tools/ci_build/ci_sanity.sh

== omitted logs ==

=== Sanity check step 2 of 15: do_pylint (Python 3 pylint) ===

ERROR_ALLOWLIST=""^tensorflow/python/framework/function_test\.py.*\[E1123.*noinline ^tensorflow/python/platform/default/_gfile\.py.*\[E0301.*non-iterator ^tensorflow/python/platform/default/_googletest\.py.*\[E0102.*function\salready\sdefined ^tensorflow/python/feature_column/feature_column_test\.py.*\[E0110.*abstract-class-instantiated ^tensorflow/contrib/layers/python/layers/feature_column\.py.*\[E0110.*abstract-class-instantiated ^tensorflow/contrib/eager/python/evaluator\.py.*\[E0202.*method-hidden ^tensorflow/contrib/eager/python/metrics_impl\.py.*\[E0202.*method-hidden ^tensorflow/contrib/rate/rate\.py.*\[E0202.*method-hidden ^tensorflow/python/training/tracking/tracking\.py.*\[E0202.*method-hidden ^tensorflow/python/platform/gfile\.py.*\[E0301.*non-iterator ^tensorflow/python/keras/callbacks\.py.*\[E1133.*not-an-iterable ^tensorflow/python/keras/engine/base_layer.py.*\[E0203.*access-member-before-definition ^tensorflow/python/keras/layers/recurrent\.py.*\[E0203.*access-member-before-definition ^tensorflow/python/kernel_tests/constant_op_eager_test.py.*\[E0303.*invalid-length-returned ^tensorflow/python/keras/utils/data_utils.py.*\[E1102.*not-callable ^tensorflow/python/autograph/.*_py3_test\.py.*\[E0001.*syntax-error ^tensorflow/python/keras/preprocessing/image\.py.*\[E0240.*Inconsistent method resolution ""
Running pylint on 2805 files with 8 parallel jobs...

```
It just hangs in this state inside the docker container

**Describe the expected behavior**
However, when I run the check directly on my system (MAC), the stage passes:

```
$  ./tensorflow/tools/ci_build/ci_sanity.sh

== omitted logs ==

=== Sanity check step 2 of 15: do_pylint (Python 3 pylint) ===

ERROR_ALLOWLIST=""^tensorflow/python/framework/function_test\.py.*\[E1123.*noinline ^tensorflow/python/platform/default/_gfile\.py.*\[E0301.*non-iterator ^tensorflow/python/platform/default/_googletest\.py.*\[E0102.*function\salready\sdefined ^tensorflow/python/feature_column/feature_column_test\.py.*\[E0110.*abstract-class-instantiated ^tensorflow/contrib/layers/python/layers/feature_column\.py.*\[E0110.*abstract-class-instantiated ^tensorflow/contrib/eager/python/evaluator\.py.*\[E0202.*method-hidden ^tensorflow/contrib/eager/python/metrics_impl\.py.*\[E0202.*method-hidden ^tensorflow/contrib/rate/rate\.py.*\[E0202.*method-hidden ^tensorflow/python/training/tracking/tracking\.py.*\[E0202.*method-hidden ^tensorflow/python/platform/gfile\.py.*\[E0301.*non-iterator ^tensorflow/python/keras/callbacks\.py.*\[E1133.*not-an-iterable ^tensorflow/python/keras/engine/base_layer.py.*\[E0203.*access-member-before-definition ^tensorflow/python/keras/layers/recurrent\.py.*\[E0203.*access-member-before-definition ^tensorflow/python/kernel_tests/constant_op_eager_test.py.*\[E0303.*invalid-length-returned ^tensorflow/python/keras/utils/data_utils.py.*\[E1102.*not-callable ^tensorflow/python/autograph/.*_py3_test\.py.*\[E0001.*syntax-error ^tensorflow/python/keras/preprocessing/image\.py.*\[E0240.*Inconsistent method resolution ""
Running pylint on     2805 files with 16 parallel jobs...


pylint took 77 s

```

**Standalone code to reproduce the issue**

Running the above-mentioned commands should reproduce the issue

** Supporting logs in attachments **
1. sanity_pass.log -> run on MAC.
2. sanity_fail.log -> run inside docker as per the above-mentioned command where the process hangs.

[sanity_pass.log](https://github.com/tensorflow/tensorflow/files/4955013/sanity_pass.log)
[sanity_fail.log](https://github.com/tensorflow/tensorflow/files/4955015/sanity_fail.log)
"
41598,Failed to load the native TensorFlow runtime,"Python 3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)]
Type ""copyright"", ""credits"" or ""license"" for more information.

IPython 7.12.0 -- An enhanced Interactive Python.

runcell(0, 'C:/Debasish/personal/DS_city/Practice with Dataset_Mine/NLP Code/untitled0.py')
Traceback (most recent call last):

  File ""C:\Users\edebago\Anaconda3\Anaconda_new3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *

  File ""C:\Users\edebago\Anaconda3\Anaconda_new3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()

  File ""C:\Users\edebago\Anaconda3\Anaconda_new3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)

  File ""C:\Users\edebago\Anaconda3\Anaconda_new3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)

  File ""C:\Users\edebago\Anaconda3\Anaconda_new3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)

ImportError: DLL load failed: The specified module could not be found.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):

  File ""C:\Debasish\personal\DS_city\Practice with Dataset_Mine\NLP Code\untitled0.py"", line 8, in <module>
    import tensorflow as tf

  File ""C:\Users\edebago\Anaconda3\Anaconda_new3\lib\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util

  File ""C:\Users\edebago\Anaconda3\Anaconda_new3\lib\site-packages\tensorflow\python\__init__.py"", line 50, in <module>
    from tensorflow.python import pywrap_tensorflow

  File ""C:\Users\edebago\Anaconda3\Anaconda_new3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 69, in <module>
    raise ImportError(msg)

ImportError: Traceback (most recent call last):
  File ""C:\Users\edebago\Anaconda3\Anaconda_new3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\edebago\Anaconda3\Anaconda_new3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\edebago\Anaconda3\Anaconda_new3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\edebago\Anaconda3\Anaconda_new3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\edebago\Anaconda3\Anaconda_new3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
41596,Failed to load the native tensorflow runtime,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version: 2.2
- Python version: 3.5
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1
- GPU model and memory: GTX 1650



**issue while importing**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
installed cuda 10.1
cudnn
following command:
pip install --ignore-installed --upgrade tensorflow-gpu


**Any other info / logs**
>>> import tensorflow
Traceback (most recent call last):
  File ""C:\Users\Lenovo\anaconda3\envs\tf\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Lenovo\anaconda3\envs\tf\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Lenovo\anaconda3\envs\tf\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Lenovo\anaconda3\envs\tf\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Lenovo\anaconda3\envs\tf\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Lenovo\anaconda3\envs\tf\lib\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\Lenovo\anaconda3\envs\tf\lib\site-packages\tensorflow\python\__init__.py"", line 50, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Lenovo\anaconda3\envs\tf\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 69, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Lenovo\anaconda3\envs\tf\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Lenovo\anaconda3\envs\tf\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Lenovo\anaconda3\envs\tf\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Lenovo\anaconda3\envs\tf\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Lenovo\anaconda3\envs\tf\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors"
41594,tf.function breaks the gradient tape when looping over datasets,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): uname -a:
Darwin Daniels-MacBook-Pro.local 18.7.0 Darwin Kernel Version 18.7.0: Tue Aug 20 16:57:14 PDT 2019; root:xnu-4903.271.2~2/RELEASE_X86_64 x86_64

- TensorFlow installed from (source or binary): binary

- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0

- Python version: Python 3.6.8 :: Anaconda, Inc.

Same bug in tf-nightly.

**Describe the current behavior**

The gradient tape is broken in graph mode when a statement is introduced in a loop. 

It works correctly in Eager mode or when the statement is taken out of the loop. 

**Describe the expected behavior**

As detailed in the documentation, a tf.function should either behave as in Eager mode or return an error. Here, none of those happen.

**Standalone code to reproduce the issue**

See [Colab notebook](https://colab.research.google.com/drive/1y__myjOuJEMF4izbJuoLbGqgDItDm7jn?usp=sharing).

```Python
import tensorflow as tf

@tf.function
def training_step(data, par):
    loss = tf.constant(0, dtype=tf.double)
    with tf.GradientTape() as tape:
        for d in data:
            loss = par # works if we take this line out of the loop
    return tape.gradient(loss, par)
    
par = tf.Variable(tf.zeros(1, dtype=tf.double))
data = tf.data.Dataset.from_tensor_slices([1,2,3])

tf.config.experimental_run_functions_eagerly(False)
print(f""graph mode: {training_step(data, par)}"")

tf.config.experimental_run_functions_eagerly(True)
print(f""eager mode: {training_step(data, par)}"")
  
```

Obviously, the same happens if `loss = par` is replaced with `loss += par`, as will typically be the case in a training loop.

**Other info / logs** 
The output is:

```
graph mode: None
eager mode: [1.]
```
"
41593,label_map_util,It was opened by mistake. I haven't problem. 
41591,TF 2.2.0 with cuda 11.0 Gpu test returns False,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10
- TensorFlow installed from (source or binary): pip3
- TensorFlow version: 2.2.0
- Python version: 3.8.4 (3.7 too?)
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.0/8.0
- GPU model and memory: Geforce GTX 980 Ti



I installed the latest Nvidia Driver 451.67, Visual Studio 2019 version 16.6.4, tensorflow-gpu 2.2.0rc2. Then I downloaded and installed cuda 11.0 and cuDNN8.0.1 RC2 for cuda11.0. I'm able to verify the installation and run the sample programs. The Problem arises when I try to test it in anaconda:

```
>>> import tensorflow as tf
2020-07-20 23:29:09.430847: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
>>> tf.__version__
'2.2.0'
>>> print(tf.test.is_gpu_available())
WARNING:tensorflow:From <stdin>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.config.list_physical_devices('GPU')` instead.
2020-07-20 23:29:13.406332: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2020-07-20 23:29:13.418916: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2706c6031b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-07-20 23:29:13.422382: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-07-20 23:29:13.425719: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2020-07-20 23:29:13.456071: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:
pciBusID: 0000:03:00.0 name: GeForce GTX 980 Ti computeCapability: 5.2
coreClock: 1.228GHz coreCount: 22 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 313.37GiB/s
2020-07-20 23:29:13.461916: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-07-20 23:29:13.467883: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cublas64_10.dll'; dlerror: cublas64_10.dll not found
2020-07-20 23:29:13.474696: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-07-20 23:29:13.479956: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-07-20 23:29:13.490763: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-07-20 23:29:13.493807: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusparse64_10.dll'; dlerror: cusparse64_10.dll not found
2020-07-20 23:29:13.498571: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudnn64_7.dll'; dlerror: cudnn64_7.dll not found
2020-07-20 23:29:13.501335: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1598] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-07-20 23:29:13.601860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-20 23:29:13.606065: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0
2020-07-20 23:29:13.609379: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N
2020-07-20 23:29:13.615932: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2707213a850 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-07-20 23:29:13.618721: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 980 Ti, Compute Capability 5.2
False
```
And:
```
>>> tf.config.list_physical_devices('GPU')
2020-07-21 13:18:04.066692: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2020-07-21 13:18:04.096830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:
pciBusID: 0000:03:00.0 name: GeForce GTX 980 Ti computeCapability: 5.2
coreClock: 1.228GHz coreCount: 22 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 313.37GiB/s
2020-07-21 13:18:04.101741: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-07-21 13:18:07.858480: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cublas64_10.dll'; dlerror: cublas64_10.dll not found
2020-07-21 13:18:07.937031: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-07-21 13:18:07.981968: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-07-21 13:18:08.309451: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-07-21 13:18:08.314100: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusparse64_10.dll'; dlerror: cusparse64_10.dll not found
2020-07-21 13:18:08.320636: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudnn64_7.dll'; dlerror: cudnn64_7.dll not found
2020-07-21 13:18:08.325392: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1598] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
```
                     
I uninstalled tensorflow and tensorflow-gpu and installed it again in anaconda (base) but still get the same msg. Can someone tell me why it is looking for outdated dll? I've read that I could manually add dlls but doing so is not advised. Does anyone have a solution to this?

I installed python 3.8.4 from the python website but anaconda gives me this: (base) PS C:\Windows\system32> python --version Python 3.7.6. Probably not related, just wanted to mention it. Thank you!"
41590,Jpeg decoding (for example when loading TFRecords from files) causes error on TPU when trying to fit a model,"**System information**
- TensorFlow version (use command below): 2.2.0 (v2.2.0-0-g2b96f3662b)
- Python version: 3.6.9
- GPU model and memory: Google Colab TPU

I'm not sure that this is a bug, but I've encountered this weird behaviour with my .tfrec dataset and made simple code to reproduce it. This problem only exists at TPU.

Firstly I initialize TPU:
```
import os
import tensorflow as tf
import numpy as np

tf.get_logger().propagate = False
resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu = 'grpc://' + os.environ['COLAB_TPU_ADDR'])
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
strategy = tf.distribute.experimental.TPUStrategy(resolver)
```
```
INFO:tensorflow:Initializing the TPU system: grpc://10.26.115.226:8470
INFO:tensorflow:Clearing out eager caches
INFO:tensorflow:Finished initializing TPU system.
INFO:tensorflow:Found TPU system:
INFO:tensorflow:*** Num TPU Cores: 8
INFO:tensorflow:*** Num TPU Workers: 1
INFO:tensorflow:*** Num TPU Cores Per Worker: 8
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)
```
Then I run the following code, which creates tf.data.Dataset of dummy images, encodes it to jpeg and back, then normalizes to float32 and makes batches.
```
with strategy.scope():

  def encode_jpg(image, class_idx):
    return tf.io.encode_jpeg(image, quality = 95, optimize_size = True, chroma_downsampling = False), class_idx

  def decode_jpg(image, class_idx):
    return tf.image.decode_jpeg(image, channels = 3), class_idx

  def normalize_img(image, class_idx):
    return image / 255 - 0.5, class_idx

  dataset = tf.data.Dataset.from_tensor_slices((
    [tf.cast(np.zeros((256, 256, 3)), dtype = tf.uint8) for _ in range(300)],
    [0 for _ in range(300)]
  ))
  dataset = dataset.map(encode_jpg)
  dataset = dataset.map(decode_jpg)
  dataset = dataset.map(normalize_img)
  dataset = dataset.batch(8)

  print('\nhow does our dataset look like?')
  for i, (image, label) in enumerate(dataset):
    print(image.shape, label.shape)
    if i == 2: break

  model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape = (256, 256, 3)),
    tf.keras.layers.Dense(100, activation = 'relu'),
    tf.keras.layers.Dense(10)
  ])

  print('\nhow does our model model like?')
  model.summary()

  model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam')
  model.fit(dataset, epochs = 1)
```
I receive the following output which ends with exception:
```
how does our dataset look like?
(8, 256, 256, 3) of <dtype: 'float32'> (8,) of <dtype: 'int32'>
(8, 256, 256, 3) of <dtype: 'float32'> (8,) of <dtype: 'int32'>
(8, 256, 256, 3) of <dtype: 'float32'> (8,) of <dtype: 'int32'>

how does our model model like?
Model: ""sequential""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten (Flatten)            (None, 196608)            0         
_________________________________________________________________
dense (Dense)                (None, 100)               19660900  
_________________________________________________________________
dense_1 (Dense)              (None, 10)                1010      
=================================================================
Total params: 19,661,910
Trainable params: 19,661,910
Non-trainable params: 0
_________________________________________________________________
---------------------------------------------------------------------------
UnimplementedError                        Traceback (most recent call last)
<ipython-input-2-9c1762b0cefe> in <module>()
     36 
     37   model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam')
---> 38   model.fit(dataset, epochs = 1)

10 frames
/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

UnimplementedError: {{function_node __inference_train_function_5323}} Compilation failure: Asked to propagate a
dynamic dimension from hlo dot.472@{}@0 to hlo %all-reduce.477 = f32[<=196608,100]{1,0}
all-reduce(f32[<=196608,100]{1,0} %dot.472), replica_groups={{0,1,2,3,4,5,6,7}}, to_apply=%sum.473,
metadata={op_type=""CrossReplicaSum"" op_name=""CrossReplicaSum_2""}, which is not implemented.
	TPU compilation failed
	 [[{{node tpu_compile_succeeded_assert/_4970277850434216321/_5}}]]
```
When I remove these two lines:
```
dataset = dataset.map(encode_jpg)
dataset = dataset.map(decode_jpg)
```
Then it works:
```
38/38 [==============================] - 1s 16ms/step - loss: 16.8563
```
However shapes and types of dataset batches remain the same:
```
how does our dataset look like?
(8, 256, 256, 3) of <dtype: 'float32'> (8,) of <dtype: 'int32'>
(8, 256, 256, 3) of <dtype: 'float32'> (8,) of <dtype: 'int32'>
(8, 256, 256, 3) of <dtype: 'float32'> (8,) of <dtype: 'int32'>
```
To fix this error I tried to case labels to tf.int64, but error still occurs. I tried to run this code on CPU version of Colab (removing `with strategy.scope():`), and then it works perfectly. So I guess the problem is in TPU and jpeg encoding-decoding."
41589,When dose Tensorflow add elasticity function in worker？,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):1.15
- Are you willing to contribute it (Yes/No): with pleasure, but I'm a rookie.



**Describe the feature and the current behavior/state.**
We has lots of jobs and a set number of resource. We hope:
When resource is enough, jobs can add some workers to increase training speed.
When resource is lacking, jobs will delete some workers to release resource.


**Any Other info.**
Does Tensorflow have any plan on elasticity function, and when does TF support elasticity function in workers?"
41588,need to add SparseEmbedding layer support,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.2.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
Hi, can we add a SparseEmbedding layer into tf.keras.layers or make tf.keras.layers.Embedding support sparse embedding? 
The main idea is that in many cases like recommendation or CTR prediction models, there are a huge number of items to be embeded, but in each batch we only use a very small portion of it. Therefore, the SparseEmbedding layer should bring just the parameters that are actually used in this batch into the model parameters to be optimized. This is will provide a huge performance improvement, especially in Multiworker Distributed training as the network cost will be significantly decreased.
BTW, pytorch already have something similar like torch.nn.Embedding(sparse=True) https://pytorch.org/docs/master/generated/torch.nn.Embedding.html
**Will this change the current api? How?**
small refactor only. By adding a new Class or adding a construction parameter
**Who will benefit with this feature?**
in many cases like recommendation or CTR prediction models, there are a huge number of items to be embeded, but in each batch we only use a very small portion of it.
**Any Other info.**
"
41587,pyspark + keras as pandas_udf does not work properly,"In my scenario I wish to train a keras model per each partition on a Spark cluster with Python enabled.
For this I am using Spark PandasUDFs.
Inside the PandasUDF code I have the following command:

curr_input = keras.Input((1,))

This already causes the error below to trigger.
I do not understand why this happens as I use PandasUDFs a lot and I have never seen this error when just creating some small object like this on a worker node.
Any ideas would be highly appreciated.

Thanks,
Roy.


_answer = 'xro123'
gateway_client = <py4j.java_gateway.GatewayClient object at 0x0000027AB538F988>
target_id = 'o122', name = 'count'

    def get_return_value(answer, gateway_client, target_id=None, name=None):
        """"""Converts an answer received from the Java gateway into a Python object.
    
        For example, string representation of integers are converted to Python
        integer, string representation of objects are converted to JavaObject
        instances, etc.
    
        :param answer: the string returned by the Java gateway
        :param gateway_client: the gateway client used to communicate with the Java
            Gateway. Only necessary if the answer is a reference (e.g., object,
            list, map)
        :param target_id: the name of the object from which the answer comes from
            (e.g., *object1* in `object1.hello()`). Optional.
        :param name: the name of the member from which the answer comes from
            (e.g., *hello* in `object1.hello()`). Optional.
        """"""
        if is_error(answer)[0]:
            if len(answer) > 1:
                type = answer[1]
                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)
                if answer[1] == REFERENCE_TYPE:
                    raise Py4JJavaError(
                        ""An error occurred while calling {0}{1}{2}.\n"".
>                       format(target_id, ""."", name), value)
E                   py4j.protocol.Py4JJavaError: An error occurred while calling o122.count.
E                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.net.SocketException: Connection reset
E                   	at java.net.SocketInputStream.read(SocketInputStream.java:210)
E                   	at java.net.SocketInputStream.read(SocketInputStream.java:141)
E                   	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
E                   	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
E                   	at java.io.DataInputStream.readInt(DataInputStream.java:387)
E                   	at org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:159)
E                   	at org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:122)
E                   	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)
E                   	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
E                   	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
E                   	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
E                   	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithoutKey_1$(Unknown Source)
E                   	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithoutKey_0$(Unknown Source)
E                   	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
E                   	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
E                   	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
E                   	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)
E                   	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)
E                   	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)
E                   	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)
E                   	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
E                   	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
E                   	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
E                   	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
E                   	at org.apache.spark.scheduler.Task.run(Task.scala:123)
E                   	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
E                   	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
E                   	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
E                   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
E                   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
E                   	at java.lang.Thread.run(Thread.java:748)
E                   
E                   Driver stacktrace:
E                   	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)
E                   	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)
E                   	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)
E                   	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
E                   	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
E                   	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)
E                   	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)
E                   	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)
E                   	at scala.Option.foreach(Option.scala:257)
E                   	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)
E                   	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)
E                   	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)
E                   	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)
E                   	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
E                   	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)
E                   	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
E                   	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
E                   	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
E                   	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
E                   	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)
E                   	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
E                   	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
E                   	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)
E                   	at org.apache.spark.rdd.RDD.collect(RDD.scala:989)
E                   	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)
E                   	at org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2836)
E                   	at org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2835)
E                   	at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)
E                   	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)
E                   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)
E                   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)
E                   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)
E                   	at org.apache.spark.sql.Dataset.count(Dataset.scala:2835)
E                   	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E                   	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E                   	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E                   	at java.lang.reflect.Method.invoke(Method.java:498)
E                   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E                   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
E                   	at py4j.Gateway.invoke(Gateway.java:282)
E                   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E                   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E                   	at py4j.GatewayConnection.run(GatewayConnection.java:238)
E                   	at java.lang.Thread.run(Thread.java:748)
E                   Caused by: java.net.SocketException: Connection reset
E                   	at java.net.SocketInputStream.read(SocketInputStream.java:210)
E                   	at java.net.SocketInputStream.read(SocketInputStream.java:141)
E                   	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
E                   	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
E                   	at java.io.DataInputStream.readInt(DataInputStream.java:387)
E                   	at org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:159)
E                   	at org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:122)
E                   	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)
E                   	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
E                   	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
E                   	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
E                   	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithoutKey_1$(Unknown Source)
E                   	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithoutKey_0$(Unknown Source)
E                   	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
E                   	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
E                   	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
E                   	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)
E                   	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)
E                   	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)
E                   	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)
E                   	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
E                   	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
E                   	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
E                   	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
E                   	at org.apache.spark.scheduler.Task.run(Task.scala:123)
E                   	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
E                   	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
E                   	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
E                   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
E                   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
E                   	... 1 more

..\..\.env\lib\site-packages\py4j\protocol.py:328: Py4JJavaError_
"
41585,"When use MirroredStrategy for multi-gpu and fit with multi-workers there is a error --""task_done() called too many times""","**System information**

OS: Ubuntu 18.04
Tensorflow 2.20 from pip install
Python version: 3.7.7
CUDA Version: 10.2
cuDNN Version: release 10.2, V10.2.89
GPU : 2070 x 2






**Describe the current behavior**
code:
```python
gpus = tf.config.experimental.list_physical_devices('GPU')
for g in gpus:
    tf.config.experimental.set_virtual_device_configuration(g, [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=7000)])

mirrored_strategy = tf.distribute.MirroredStrategy(devices=[""/gpu:0"", ""/gpu:1""])

train_image_generator = ImageDataGenerator(rescale=1./255, horizontal_flip=True, zoom_range=0.1, rotation_range=45) # Generator for our training data
validation_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our validation data

train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,
                                                           directory=train_dir,
                                                           shuffle=True,
                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),
                                                           class_mode='binary')

val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,
                                                              directory=validation_dir,
                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),
                                                              class_mode='binary')

sample_training_images, _ = next(train_data_gen)


with mirrored_strategy.scope():
    tinydarknet = keras.Sequential([
        keras.layers.Conv2D(16, (3, 3), strides=[1, 1], padding=""same"", input_shape=(IMG_HEIGHT,IMG_WIDTH, 3)),
        keras.layers.BatchNormalization(),
        keras.layers.LeakyReLU(alpha=0.1),
        keras.layers.MaxPooling2D((2, 2), strides=(2, 2)),
        ..........
        keras.layers.BatchNormalization(),
        keras.layers.AveragePooling2D(),
        keras.layers.Flatten(),
        keras.layers.Dense(1)
    ])


    # In[8]:


    tinydarknet.compile(optimizer=""adam"",
                 loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
                 metrics=[""accuracy""])


# In[9]:

history = tinydarknet.fit(#_generator(
    train_data_gen,
    steps_per_epoch=total_train // batch_size,
    epochs=epochs,
    validation_data=val_data_gen,
    validation_steps=total_val // batch_size,
    workers=NUM_WORKERS
)

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss=history.history['loss']
val_loss=history.history['val_loss']

epochs_range = range(epochs)

tinydarknet.save(""keras_model"")
```

**Describe the expected behavior**

When I set workers=1 in fit(), it is normal work, but when I workers more than one , it is got a error:

tensorflow/core/framework/op_kernel.cc:1741] Invalid argument: ValueError: task_done() called too many times
Traceback (most recent call last):

  File ""/data_ssd/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py"", line 243, in __call__
    ret = func(*args)

  File ""/data_ssd/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py"", line 309, in wrapper
    return func(*args, **kwargs)

  File ""/data_ssd/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 785, in generator_py_func
    values = next(generator_state.get_iterator(iterator_id))

  File ""/data_ssd/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 801, in wrapped_generator
    for data in generator_fn():

  File ""/data_ssd/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/utils/data_utils.py"", line 880, in get
    six.reraise(*sys.exc_info())

  File ""/data_ssd/anaconda3/envs/tf2/lib/python3.7/site-packages/six.py"", line 703, in reraise
    raise value

  File ""/data_ssd/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/utils/data_utils.py"", line 875, in get
    self.queue.task_done()

  File ""/data_ssd/anaconda3/envs/tf2/lib/python3.7/queue.py"", line 74, in task_done
    raise ValueError('task_done() called too many times')

ValueError: task_done() called too many times


"
41584,Build did not complete successfully.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): source
- TensorFlow version: r1.15
- Python version: 3.6.9
- Installed using virtualenv? pip? conda?: -
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): 7.5.0
- CUDA/cuDNN version: CUDA=10.0, cuDNN=7.6.5
- GPU model and memory: Tesla P40 , 22919MiB (nvidia-smi)



**Describe the problem**
I tried to build from source for the branch r1.15 but continuously failed to build from source.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
`bazel build --verbose_failures --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package` and

`bazel build --verbose_failures --config=opt --config=cuda --config=monolithic //tensorflow/tools/pip_package:build_pip_package`


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
I got an error message like below by setting the `--verbose_failures` flags.
```bash
ERROR: /root/tensorflow/tensorflow/python/BUILD:329:1: C++ compilation of rule '//tensorflow/python:bfloat16_lib' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command
  (cd /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \
    PATH=/root/.cache/bazelisk/downloads/bazelbuild/bazel-0.26.0-linux-x86_64/bin:/root/bin:/usr/local/cuda-10.0/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/go/bin:/root/go/bin \
    PWD=/proc/self/cwd \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/tensorflow/python/_objs/bfloat16_lib/bfloat16.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/python/_objs/bfloat16_lib/bfloat16.pic.o' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -DTF_USE_SNAPPY -DCURL_STATICLIB -DPLATFORM_LINUX -DENABLE_CURL_CLIENT -DENABLE_NO_ENCRYPTION -DTENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL -DTENSORFLOW_USE_MKLDNN_CONTRACTION_KERNEL -DLLVM_ENABLE_STATS -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -DLLVM_BUILD_GLOBAL_ISEL '-DGRPC_ARES=0' '-DPB_FIELD_32BIT=1' -DSQLITE_OMIT_DEPRECATED -iquote . -iquote bazel-out/host/bin -iquote external/com_google_absl -iquote bazel-out/host/bin/external/com_google_absl -iquote external/eigen_archive -iquote bazel-out/host/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/host/bin/external/local_config_sycl -iquote external/nsync -iquote bazel-out/host/bin/external/nsync -iquote external/gif_archive -iquote bazel-out/host/bin/external/gif_archive -iquote external/jpeg -iquote bazel-out/host/bin/external/jpeg -iquote external/com_google_protobuf -iquote bazel-out/host/bin/external/com_google_protobuf -iquote external/zlib_archive -iquote bazel-out/host/bin/external/zlib_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/bin/external/highwayhash -iquote external/double_conversion -iquote bazel-out/host/bin/external/double_conversion -iquote external/snappy -iquote bazel-out/host/bin/external/snappy -iquote external/hwloc -iquote bazel-out/host/bin/external/hwloc -iquote external/local_config_cuda -iquote bazel-out/host/bin/external/local_config_cuda -iquote external/local_config_tensorrt -iquote bazel-out/host/bin/external/local_config_tensorrt -iquote external/local_config_python -iquote bazel-out/host/bin/external/local_config_python -iquote external/curl -iquote bazel-out/host/bin/external/curl -iquote external/boringssl -iquote bazel-out/host/bin/external/boringssl -iquote external/jsoncpp_git -iquote bazel-out/host/bin/external/jsoncpp_git -iquote external/aws -iquote bazel-out/host/bin/external/aws -iquote external/mkl_dnn -iquote bazel-out/host/bin/external/mkl_dnn -iquote external/llvm -iquote bazel-out/host/bin/external/llvm -iquote external/nccl_archive -iquote bazel-out/host/bin/external/nccl_archive -iquote external/grpc -iquote bazel-out/host/bin/external/grpc -iquote external/com_github_nanopb_nanopb -iquote bazel-out/host/bin/external/com_github_nanopb_nanopb -iquote external/cub_archive -iquote bazel-out/host/bin/external/cub_archive -iquote external/png_archive -iquote bazel-out/host/bin/external/png_archive -iquote external/lmdb -iquote bazel-out/host/bin/external/lmdb -iquote external/icu -iquote bazel-out/host/bin/external/icu -iquote external/org_sqlite -iquote bazel-out/host/bin/external/org_sqlite -iquote external/gemmlowp -iquote bazel-out/host/bin/external/gemmlowp -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/host/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cublas_headers_virtual -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/nccl -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/include_hdrs -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/src_hdrs -Ibazel-out/host/bin/external/cub_archive/_virtual_includes/cub -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cupti_headers_virtual -isystem external/eigen_archive -isystem bazel-out/host/bin/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/host/bin/external/nsync/public -isystem external/gif_archive -isystem bazel-out/host/bin/external/gif_archive -isystem external/com_google_protobuf/src -isystem bazel-out/host/bin/external/com_google_protobuf/src -isystem external/zlib_archive -isystem bazel-out/host/bin/external/zlib_archive -isystem external/farmhash_archive/src -isystem bazel-out/host/bin/external/farmhash_archive/src -isystem external/double_conversion -isystem bazel-out/host/bin/external/double_conversion -isystem external/hwloc/hwloc -isystem bazel-out/host/bin/external/hwloc/hwloc -isystem external/hwloc/include -isystem bazel-out/host/bin/external/hwloc/include -isystem external/local_config_cuda/cuda -isystem bazel-out/host/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_python/numpy_include -isystem bazel-out/host/bin/external/local_config_python/numpy_include -isystem external/local_config_python/python_include -isystem bazel-out/host/bin/external/local_config_python/python_include -isystem external/local_config_cuda/cuda/cublas/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cublas/include -isystem external/curl/include -isystem bazel-out/host/bin/external/curl/include -isystem external/boringssl/src/include -isystem bazel-out/host/bin/external/boringssl/src/include -isystem external/jsoncpp_git/include -isystem bazel-out/host/bin/external/jsoncpp_git/include -isystem external/aws/aws-cpp-sdk-core/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-core/include -isystem external/aws/aws-cpp-sdk-kinesis/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-kinesis/include -isystem external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-s3/include -isystem external/mkl_dnn/include -isystem bazel-out/host/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/host/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/host/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/host/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/host/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/host/bin/external/mkl_dnn/src/cpu/xbyak -isystem external/llvm/include -isystem bazel-out/host/bin/external/llvm/include -isystem external/llvm/lib/IR -isystem bazel-out/host/bin/external/llvm/lib/IR -isystem external/llvm/include/llvm/IR -isystem bazel-out/host/bin/external/llvm/include/llvm/IR -isystem external/llvm/lib/Target/X86 -isystem bazel-out/host/bin/external/llvm/lib/Target/X86 -isystem external/llvm/lib/Transforms/InstCombine -isystem bazel-out/host/bin/external/llvm/lib/Transforms/InstCombine -isystem external/llvm/lib/Target/AArch64 -isystem bazel-out/host/bin/external/llvm/lib/Target/AArch64 -isystem external/llvm/lib/Target/ARM -isystem bazel-out/host/bin/external/llvm/lib/Target/ARM -isystem external/llvm/lib/Target/AMDGPU -isystem bazel-out/host/bin/external/llvm/lib/Target/AMDGPU -isystem external/llvm/lib/Target/NVPTX -isystem bazel-out/host/bin/external/llvm/lib/Target/NVPTX -isystem external/grpc/include -isystem bazel-out/host/bin/external/grpc/include -isystem external/grpc/third_party/address_sorting/include -isystem bazel-out/host/bin/external/grpc/third_party/address_sorting/include -isystem external/png_archive -isystem bazel-out/host/bin/external/png_archive -isystem external/icu/icu4c/source/common -isystem bazel-out/host/bin/external/icu/icu4c/source/common -isystem external/local_config_cuda/cuda/cuda/extras/CUPTI/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/extras/CUPTI/include '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 '-march=native' -g0 -c tensorflow/python/lib/core/bfloat16.cc -o bazel-out/host/bin/tensorflow/python/_objs/bfloat16_lib/bfloat16.pic.o)
Execution platform: @bazel_tools//platforms:host_platform



                       compare_types)) {
                                    ^
tensorflow/python/lib/core/bfloat16.cc:608:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>
                             const std::array<int, 3>& types) {
                                                            ^
tensorflow/python/lib/core/bfloat16.cc:608:60: note:   no known conversion for argument 2 from '<unresolved overloaded function type>' to 'PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}'
tensorflow/python/lib/core/bfloat16.cc:638:36: error: no match for call to '(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [10], <unresolved overloaded function type>, const std::array<int, 3>&)'
                       compare_types)) {
                                    ^
tensorflow/python/lib/core/bfloat16.cc:608:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>
                             const std::array<int, 3>& types) {
                                                            ^
tensorflow/python/lib/core/bfloat16.cc:608:60: note:   no known conversion for argument 2 from '<unresolved overloaded function type>' to 'PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}'
tensorflow/python/lib/core/bfloat16.cc:641:77: error: no match for call to '(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [5], <unresolved overloaded function type>, const std::array<int, 3>&)'
   if (!register_ufunc(""less"", CompareUFunc<Bfloat16LtFunctor>, compare_types)) {
                                                                             ^
tensorflow/python/lib/core/bfloat16.cc:608:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>
                             const std::array<int, 3>& types) {
                                                            ^
tensorflow/python/lib/core/bfloat16.cc:608:60: note:   no known conversion for argument 2 from '<unresolved overloaded function type>' to 'PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}'
tensorflow/python/lib/core/bfloat16.cc:645:36: error: no match for call to '(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [8], <unresolved overloaded function type>, const std::array<int, 3>&)'
                       compare_types)) {
                                    ^
tensorflow/python/lib/core/bfloat16.cc:608:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>
                             const std::array<int, 3>& types) {
                                                            ^
tensorflow/python/lib/core/bfloat16.cc:608:60: note:   no known conversion for argument 2 from '<unresolved overloaded function type>' to 'PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}'
tensorflow/python/lib/core/bfloat16.cc:649:36: error: no match for call to '(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [11], <unresolved overloaded function type>, const std::array<int, 3>&)'
                       compare_types)) {
                                    ^
tensorflow/python/lib/core/bfloat16.cc:608:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>
                             const std::array<int, 3>& types) {
                                                            ^
tensorflow/python/lib/core/bfloat16.cc:608:60: note:   no known conversion for argument 2 from '<unresolved overloaded function type>' to 'PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}'
tensorflow/python/lib/core/bfloat16.cc:653:36: error: no match for call to '(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [14], <unresolved overloaded function type>, const std::array<int, 3>&)'
                       compare_types)) {
                                    ^
tensorflow/python/lib/core/bfloat16.cc:608:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>
                             const std::array<int, 3>& types) {
                                                            ^
tensorflow/python/lib/core/bfloat16.cc:608:60: note:   no known conversion for argument 2 from '<unresolved overloaded function type>' to 'PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 5310.456s, Critical Path: 1039.95s
INFO: 19074 processes: 19074 local.
FAILED: Build did NOT complete successfully
```"
41583,Setting 'name' parameter in keras.layers results in accuracy degradation,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): GCP VM (Debian GNU/Linux 9 Stretch + TF 1-15-3)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): preinstalled
- TensorFlow version (use command below): 1.15.3 (v1.15.2-30-g4386a66)
- Python version: 3.5.3
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version:  -
- GPU model and memory: TPUv2-8

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
When I set name parameter in keras.layers.Conv2D, the training behavior changes (accuracy goes down consistently)

**Describe the expected behavior**
The training behavior should be similar : accuracy have to be similar.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

0. Clone official mnasnet code
```
git clone https://github.com/tensorflow/tpu.git
cd tpu/models/official/mnasnet
export PYTHONPATH=$PYTHONPATH:`pwd`/../../:`pwd`/../efficientnet
```

1. train mnasnet for 50 epochs
```
python mnasnet_main.py --data_dir=IMAGNET_DIR --tpu=TPU_NAME --train_steps=62550 --steps_per_eval=6255 --train_batch_size=1024 --eval_batch_size=1024 --model_dir=SAVE_DIR --model_name=mnasnet-a1
```

2. add name to keras.layers (can be added using diff below)
```
diff --git models/official/mnasnet/mnasnet_model.py models/official/mnasnet/mnasnet_model.py
index 935421e..7909ec2 100644
--- models/official/mnasnet/mnasnet_model.py
+++ models/official/mnasnet/mnasnet_model.py
@@ -117,7 +117,8 @@ def _get_conv2d(filters,
                 padding,
                 use_bias,
                 data_format='channels_last',
-                use_keras=True):
+                use_keras=True,
+                name=None):
   """"""A helper function to create Conv2D layer.""""""
   if use_keras:
     return tf.keras.layers.Conv2D(
@@ -127,7 +128,8 @@ def _get_conv2d(filters,
         kernel_initializer=kernel_initializer,
         padding=padding,
         data_format=data_format,
-        use_bias=use_bias)
+        use_bias=use_bias,
+        name=name)
   else:
     return tf.layers.Conv2D(
         filters=filters,
@@ -136,7 +138,8 @@ def _get_conv2d(filters,
         kernel_initializer=kernel_initializer,
         padding=padding,
         data_format=data_format,
-        use_bias=use_bias)
+        use_bias=use_bias,
+        name=name)
 
 
 class MnasBlock(object):
@@ -190,13 +193,15 @@ class MnasBlock(object):
           padding='same',
           use_bias=False,
           data_format=self._data_format,
-          use_keras=self._use_keras)
+          use_keras=self._use_keras,
+          name='expand_conv')
       # TODO(hongkuny): b/120622234 need to manage update ops directly.
       self._bn0 = tf.layers.BatchNormalization(
           axis=self._channel_axis,
           momentum=self._batch_norm_momentum,
           epsilon=self._batch_norm_epsilon,
-          fused=True)
+          fused=True,
+          name='expand_bn')
 
     kernel_size = self._block_args.kernel_size
     # Depth-wise convolution phase:
@@ -207,7 +212,8 @@ class MnasBlock(object):
           depthwise_initializer=conv_kernel_initializer,
           padding='same',
           data_format=self._data_format,
-          use_bias=False)
+          use_bias=False,
+          name='dw_conv')
     else:
       self._depthwise_conv = mnas_utils.DepthwiseConv2D(
           [kernel_size, kernel_size],
@@ -215,12 +221,14 @@ class MnasBlock(object):
           depthwise_initializer=conv_kernel_initializer,
           padding='same',
           data_format=self._data_format,
-          use_bias=False)
+          use_bias=False,
+          name='dw_conv')
     self._bn1 = tf.layers.BatchNormalization(
         axis=self._channel_axis,
         momentum=self._batch_norm_momentum,
         epsilon=self._batch_norm_epsilon,
-        fused=True)
+        fused=True,
+        name='dw_bn')
 
     if self.has_se:
       num_reduced_filters = max(
@@ -234,7 +242,8 @@ class MnasBlock(object):
           padding='same',
           use_bias=True,
           data_format=self._data_format,
-          use_keras=self._use_keras)
+          use_keras=self._use_keras,
+          name='se_reduce')
       self._se_expand = _get_conv2d(
           filters,
           kernel_size=[1, 1],
@@ -243,7 +252,8 @@ class MnasBlock(object):
           padding='same',
           use_bias=True,
           data_format=self._data_format,
-          use_keras=self._use_keras)
+          use_keras=self._use_keras,
+          name='se_expand')
 
     # Output phase:
     filters = self._block_args.output_filters
@@ -255,12 +265,14 @@ class MnasBlock(object):
         padding='same',
         use_bias=False,
         data_format=self._data_format,
-        use_keras=self._use_keras)
+        use_keras=self._use_keras,
+        name='proj_conv')
     self._bn2 = tf.layers.BatchNormalization(
         axis=self._channel_axis,
         momentum=self._batch_norm_momentum,
         epsilon=self._batch_norm_epsilon,
-        fused=True)
+        fused=True,
+        name='proj_bn')
 
   def _call_se(self, input_tensor):
     """"""Call Squeeze and Excitation layer.

```

3. use same code to train again
```
python mnasnet_main.py --data_dir=IMAGNET_DIR --tpu=TPU_NAME --train_steps=62550 --steps_per_eval=6255 --train_batch_size=1024 --eval_batch_size=1024 --model_dir=DIFFERENT_SAVE_DIR --model_name=mnasnet-a1
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

When I added name, the accuracy consistently degraded.
I repeated 3 times.

![image](https://user-images.githubusercontent.com/8455454/88022457-42729980-cb6a-11ea-9eee-65653ca9aae9.png)"
41582,tf.keras.datasets.imdb.load_data() where path is a local path,"When I download imdb dataset and save it in my disk, say, the **loacl path** is 'D:/datasets/imdb.npz', when I pass the path into load_data() API, remote download will start rather than load data from local path."
41581,Camera does not resume when activity is resumed - TFLite Android flower-classification codelab,"<em>This is a bug as per the [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md)</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 8
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Moto E5

**Describe the current behavior**
Camera is stopped when the activity is paused, but does not resume/reinitialize when the activity is resumed

**Describe the expected behavior**
Camera should reinitialize when the activity is resumed.

**How to reproduce the issue**
Open the app. Go to home through system navigation, i.e, pause the activity. Reopen the app from recents screen, i.e, resume the activity. You'll see that the camera is stuck.

**Other info / logs**
Link to Tensorflow Lite Android flower-classification Codelab project -
https://github.com/tensorflow/examples/tree/master/lite/codelabs/flower_classification/android/start

I've already identified the error and fixed it in my fork. Would be glad to create a pull request here.
"
41580,tensorflow lite performs linear relation between batch size and inference time,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.2.1511
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below):2.3.0.dev2020062301
- Python version:3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):4.8.5
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
  I converted a tiny bert module to tflite and run the  inference with the tensorflow lite c++ api. When batch size=1, tensorflow lite performs average runtime 0.6ms, while tensorflow performs average runtime 1ms(with default threads num); when batch size=10, tensorflow lite performs average runtime 5ms, while tensorflow performs average runtime 3ms.
  It seems tensorflow lite did nothing on multi thread speed up, as i tried to apply SetNumThreads(4).
  And SetNumThreads(4) and SetNumThreads(1) performs same runtime, though cpu usage change from 100% to 200%

**Describe the expected behavior**
  I am wondering is this a normal performance for tflite in the X86 desktop?

**Standalone code to reproduce the issue**
Here is the part of my custom tflite c code
 ```
 class Session {
 public:
    Session() {
      model_ = NULL;
      interpreter_ = NULL;
    }

  bool Open(const std::string &saved_model) {
    model_ = tflite::FlatBufferModel::BuildFromFile(saved_model.c_str());
    if (!model_) {
      return false;
    }

    tflite::InterpreterBuilder(*model_.get(), resolver_)(&interpreter_);

    if (!interpreter_) {
      return false;
    }
    interpreter_->SetNumThreads(4);
    return true;
  }

  bool Run(std::vector<int> &dims, int32_t *tok_id, int32_t *msk_id, int32_t *seg_id, float *output) const {
     int tok_index = interpreter_->inputs()[2];
     int msk_index = interpreter_->inputs()[1];
     int seg_index = interpreter_->inputs()[0];
     interpreter_->ResizeInputTensor(tok_index, dims);
     interpreter_->ResizeInputTensor(msk_index, dims);
     interpreter_->ResizeInputTensor(seg_index, dims);

     if(interpreter_->AllocateTensors() != kTfLiteOk) //remove AllocateTensors() did not change the runtime
         return false;
     int32_t bytes = dims[0] * dims[1] * sizeof(int32_t);
     int32_t* tok_tensor = interpreter_->typed_tensor<int32_t>(tok_index);
     memcpy(tok_tensor, tok_id, bytes);
     int32_t* msk_tensor = interpreter_->typed_tensor<int32_t>(msk_index);
     memcpy(msk_tensor, msk_id, bytes);
     int32_t* seg_tensor = interpreter_->typed_tensor<int32_t>(seg_index);
     memcpy(seg_tensor, seg_id, bytes);
     if(interpreter_->Invoke() != kTfLiteOk)
         return false;
     bytes = dims[0] * sizeof(float);
     float* result = interpreter_->typed_output_tensor<float>(0);
     memcpy(output, result, bytes);
     return true;
  }

private:
  std::unique_ptr<tflite::FlatBufferModel> model_;
  std::unique_ptr<tflite::Interpreter> interpreter_;
  tflite::ops::builtin::BuiltinOpResolver resolver_;
};

```
"
41578,Tensorflow lite make error,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows 10 x64
- TensorFlow installed from (source or binary):source(tensorflow-master) for github
- Tensorflow version (commit SHA if source):
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): keil IDE 5.30



**Describe the problem**
Git Bash 2.270 x64 was used
error of make the generate_projects=>error1
**Please provide the exact sequence of commands/steps when you ran into the problem**
1.start git bash
2. change the file directory to d;\tensorflow-master
3. make -f tensorflow/lite/micro/tools/make/Makefile generate_projects
4. error1 ocurred, process stopped
![tensorflow lite make error](https://user-images.githubusercontent.com/44084579/88012475-8eaee100-cb4c-11ea-8e9b-a0182d91d3a1.PNG)

"
41575,Unable to generate .pbtxt file for Tensorflow 2.0 models.,"**System information**

Running Code on Google Colab
Using Tensorflow 2.2

**Problem**

I'm trying to generate a .pbtxt file from a .pb file that I trained using **TFOD 2 api**  but I'm getting a parsing message error. In fact I'm even getting this error when I try to use a trained SSD mobilenet V2 model from tensorflow model Zoo.

**This is the Code Used to get .pbtxt, you can just paste this code in google colab and the error will be reproduced**

```

!wget 'http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz'
!tar -xf ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz
model_path = 'ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model/saved_model.pb'

import tensorflow as tf

with tf.io.gfile.GFile(model_path, ""rb"") as f:
    graph_def = tf.compat.v1.GraphDef()
    graph_def.ParseFromString(f.read())

    for i in reversed(range(len(graph_def.node))):
        if graph_def.node[i].op == 'Const':
            del graph_def.node[i]

    tf.io.write_graph(graph_def, """", 'graph.pbtxt', as_text=True)
```

Facing this Error while running the code:
```

----> graph_def.ParseFromString(f.read())
DecodeError: Error parsing message
```

**How can I generate a .pbtxt file from Tensorflow Object detection api 2.x models. The end goal is to use these models inside OpenCV DNN module**
"
41574,BroadcastTo,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):


**Provide the text output from tflite_convert**

```
# Copy and paste here
```

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
"
41573,TFDBG doesn't display any tensor ,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.0rc2
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
![Screenshot from 2020-07-21 10-49-27](https://user-images.githubusercontent.com/17592563/88007139-df6c0d00-cb3f-11ea-8309-749d1df03c43.png)

**Describe the expected behavior**
Display record tensors which like tf1.x
**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
```
python -m tensorflow.python.debug.examples.v2.debug_mnist_v2     --dump_dir /tmp/tfdbg2_logdir --dump_tensor_debug_mode FULL_HEALTH
```
```
python -m tensorflow.python.debug.cli.offline_analyzer --dump_dir=""/tmp/tfdbg2_logdir""
```
**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
41572,[TF 2.2.0] tf.io.decode_image caught CUDA_ERROR_NOT_INITIALIZED: initialization error when use_multiprocessing=True,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 
- Python version: 3.6.9
- GCC/Compiler version (if compiling from source): 8.4.0
- CUDA/cuDNN version: 10.1/7.6.5
- GPU model and memory: 6GB 

I try to decode a dataset in *.tfrecord type using ```tf.io.decode_image```
```
def get_item(type, index):
    if type == 'trainval':
        item = images_trainval[index]
    else:
        item = images_val[index]

    image = item['image/encoded']
    image = tf.io.decode_image(image, 3)
    image = image.numpy()

    mask = item['image/segmentation/class/encoded']
    mask = tf.io.decode_image(mask, 1)
    mask = mask.numpy()
    mask = mask.reshape(mask.shape[:2])

    return image, mask

class DLDataset(Dataset):
    def __init__(self, split, dataset_dir):
        self.split = split
        
        self.transformer = data_transforms[split]

    def __getitem__(self, i):
        image, mask = get_item(self.split, i)

        image, mask = safe_crop(image, mask, size=512)
        image = transforms.ToPILImage()(image.copy().astype(np.uint8))
        image = self.transformer(image)

        mask = torch.from_numpy(mask)

        return image, mask

    def __len__(self):
        return get_len(self.split)
```
And it caught error:
```
...
2020-07-21 08:04:06.203528: I tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_NOT_INITIALIZED: initialization error
2020-07-21 08:04:06.203533: I tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_NOT_INITIALIZED: initialization error
2020-07-21 08:04:06.203532: I tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_NOT_INITIALIZED: initialization error
2020-07-21 08:04:06.203538: I tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_NOT_INITIALIZED: initialization error
2020-07-21 08:04:06.203537: I tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_NOT_INITIALIZED: initialization error
2020-07-21 08:04:06.203547: I tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_NOT_INITIALIZED: initialization error
2020-07-21 08:04:06.203559: I tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_NOT_INITIALIZED: initialization error
2020-07-21 08:04:06.203561: I tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_NOT_INITIALIZED: initialization error
2020-07-21 08:04:06.203556: I tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_NOT_INITIALIZED: initialization error
2020-07-21 08:04:06.203564: I tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_NOT_INITIALIZED: initialization error
2020-07-21 08:04:06.203568: I tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_NOT_INITIALIZED: initialization error
2020-07-21 08:04:06.203572: I tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_NOT_INITIALIZED: initialization error
2020-07-21 08:04:06.203571: I tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_NOT_INITIALIZED: initialization error
2020-07-21 08:04:06.203568: I tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_NOT_INITIALIZED: initialization error
2020-07-21 08:04:06.203578: I tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_NOT_INITIALIZED: initialization error
2020-07-21 08:04:06.203577: I tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_NOT_INITIALIZED: initialization error
2020-07-21 08:04:06.203585: I tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_NOT_INITIALIZED: initialization error
2020-07-21 08:04:06.203585: I tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_NOT_INITIALIZED: initialization error
...
```
The error seems to be infinities. I had to press Ctrl + C to stop it. 

But, when I try a clearly function (```tf.io.decode_jpeg``` and ```tf.io.decode_png``` instead of ```tf.io.decode_image```)
```
def get_item(type, index):
    if type == 'trainval':
        item = images_trainval[index]
    else:
        item = images_val[index]

    image = item['image/encoded']
    image = tf.io.decode_jpeg(image, 3)
    image = image.numpy()

    mask = item['image/segmentation/class/encoded']
    mask = tf.io.decode_png(mask, 1)
    mask = mask.numpy()
    mask = mask.reshape(mask.shape[:2])

    return image, mask
```
It work perfectly. 

I noticed this error occurs when use ```workers > 1``` and ```use_multiprocessing = True``` in model fit. If i don't use multi workers, It will work.



"
41571,keras.Model.fit cannot handle variable epoch sizes,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.2.0-0-g2b96f3662b 2.2.0
- Python version: 3.6.9

**Describe the current behavior**
`keras.Model.fit` with finite-length dataset inputs can run without `steps_per_epoch`. However, if the number of batches in the epoch changes between epochs, this can lead to:
* truncation of epoch if subsequence epochs are longer than the initial epoch; and
* truncation of training if subsequence epochsa re shorter than the initial epoch.

While not a common occurance, this happens e.g. in graph neural network applications where it makes more sense to batch according to some maximum number of edges/nodes in a graph as opposed to a fixed batch size. A fixed number of graphs in a dataset can result in a variable number of batches when these are shuffled.

**Describe the expected behavior**
Each epoch of `fit` should iterate over a dataset in it's entirety when `steps_per_epoch` is not provided, and run to the defined number of epochs (unless terminated prior with callbacks), even if the number of steps varies between epochs. Alternatively, a special `steps_per_epoch` value (say, -1) should be able to be provided to specify a dynamic epoch size.
**Standalone code to reproduce the issue**
[Colab](https://colab.research.google.com/drive/1HWPtXa2N_EcSFjhcFDquxBrl8RlZvB9y?usp=sharing)

**Other info / logs**
From colab above, when subsequence epochs are shorter than the first:
```bash
WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 30 batches). You may need to use the repeat() function when building your dataset.
```"
41570,TFLite interpreter.invoke() crashes on GPU despite successful TFLite conversion,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 (Google Colab notebook)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.4.0-dev20200720
- Python version: 3.6.9
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10.1/7 (Google Colab default GPU)
- GPU model and memory: Google Colab default GPU

**Describe the current behavior**
I converted a simple 3DCNN keras model to TFLite. Upon creating an interpreter from that TFLite model and calling `interpreter.invoke()`, the Google Colab notebook crashes. This only happens when using a GPU runtime; `interpreter.invoke()` works fine on a CPU. The converted TFLite model uses both `tf.lite.OpsSet.TFLITE_BUILTINS` and `tf.lite.OpsSet.SELECT_TF_OPS`, and I was converting to TFLite in an attempt to apply quantization-aware training and post-training quantization according to [this guide](https://www.tensorflow.org/model_optimization/guide/quantization/training_comprehensive_guide). 

**Describe the expected behavior**
I expect to be able to call `interpreter.invoke()` successfully on a GPU without any crashing.

**Standalone code to reproduce the issue**

1. Add [this Google Colab notebook](https://colab.research.google.com/drive/19gXOPePKytN-6JptdBqljHfEsLXQHaMj?usp=sharing) and [this dataset](https://drive.google.com/file/d/1F1qxN1HrBbMkkYEQH3Vpa7-LnPF63bEK/view?usp=sharing) to your Google Drive.
2. Open the colab notebook. Set the runtime to GPU (Runtime -> Change Runtime Type -> GPU). 
3. Mount your drive and change ""/content/drive/My Drive/full_dataset_vectors.h5"" to wherever you're storing the dataset.
4. Run all cells of the notebook. The actual crash only happens in the last cell, with `interpreter.invoke()`.

**Other info / logs** 

The logs in the Google Colab notebook didn't provide any information about the cause for the crash. To get more info, I tried running the code on an identical local environment and managed to obtain a gdb backtrace. It seems likely to be related to [this issue](https://github.com/tensorflow/tensorflow/issues/35987), but the backtrace looks different enough to possibly be a separate bug, so I decided to create a new issue.

```
Thread 1 ""python3"" received signal SIGSEGV, Segmentation fault.
__memmove_sse2_unaligned_erms () at ../sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S:370
370    ../sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S: No such file or directory.
(gdb) bt
#0  __memmove_sse2_unaligned_erms () at ../sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S:370
#1  0x00007ff291abd70c in tflite::FlexDelegate::CopyFromBufferHandle(TfLiteContext*, int, TfLiteTensor*) ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00007ff280226f1d in tflite::impl::Subgraph::Invoke() ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so
#3  0x00007ff280229bd0 in tflite::impl::Interpreter::Invoke() ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so
#4  0x00007ff27ffc8a58 in tflite::interpreter_wrapper::InterpreterWrapper::Invoke() ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so
#5  0x00007ff27ffbe6be in void pybind11::cpp_function::initialize<pybind11_init__pywrap_tensorflow_interpreter_wrapper(pybind11::module&)::{lambda(tflite::interpreter_wrapper::InterpreterWrapper&)#4}, pybind11::object, tflite::interpreter_wrapper::InterpreterWrapper&, pybind11::name, pybind11::is_method, pybind11::sibling>(pybind11_init__pywrap_tensorflow_interpreter_wrapper(pybind11::module&)::{lambda(tflite::interpreter_wrapper::InterpreterWrapper&)#4}&&, pybind11::object (*)(tflite::interpreter_wrapper::InterpreterWrapper&), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&)::{lambda(pybind11::detail::function_call&)#3}::_FUN(pybind11::detail::function_call)
    () from /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so
#6  0x00007ff27ffbfc19 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so
#7  0x00000000005669ac in _PyCFunction_FastCallDict () at ../Objects/methodobject.c:231
#8  0x000000000050a5c3 in call_function.lto_priv () at ../Python/ceval.c:4875
#9  0x000000000050bfb4 in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3335
#10 0x0000000000509758 in PyEval_EvalFrameEx (throwflag=0, 
    f=Frame 0x7ff180002e48, for file /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter.py, line 524, in invoke (self=<Interpreter(_custom_op_registerers=[], _interpreter=<tensorflow.lite.python.interpreter_wrapper._pywrap_tensorflow_interpreter_wrapper.InterpreterWrapper at remote 0x7ff2779fd110>, _delegates=[]) at remote 0x7ff2779fd710>)) at ../Python/ceval.c:754
#11 _PyFunction_FastCall (globals=<optimized out>, nargs=140675211341384, args=<optimized out>, co=<optimized out>) at ../Python/ceval.c:4933
#12 fast_function.lto_priv () at ../Python/ceval.c:4968
#13 0x000000000050a48d in call_function.lto_priv () at ../Python/ceval.c:4872
#14 0x000000000050bfb4 in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3335
#15 0x0000000000509758 in PyEval_EvalFrameEx (throwflag=0, 
    f=Frame 0x7ff1b4001df8, for file inference.py, line 80, in tflite_inference (interpreter=<Interpreter(_custom_op_registerers=[], _interpreter=<tensorflow.lite.python.interpreter_wrapper._pywrap_tensorflow_interpreter_wrapper.InterpreterWrapper at remote 0x7ff2779fd110>, _delegates=[]) at remote 0x7ff2779fd710>, test_data=[<tensorflow.python.framework.ops.EagerTensor at remote 0x7ff26c0f4938>, <tensorflow.python.framework.ops.EagerTensor at remote 0x7ff26c0f47d8>, <tensorflow.python.framework.ops.EagerTensor at remote 0x7ff26c0f4678>, <tensorflow.python.framework.ops.EagerTensor at remote 0x7ff26c0f4888>, <tensorflow.python.framework.ops.EagerTensor at remote 0x7ff26c0f4e08>, <tensorflow.python.framework.ops.EagerTensor at remote 0x7ff26c0f4bf8>, <tensorflow.python.framework.ops.EagerTensor at remote 0x7ff26c0f4a98>, <tensorflow.python.framework.ops.EagerTensor at remote 0x7ff26c0f4b48>, <tensorflow.python.framework.ops.EagerTensor at remote 0x7ff21c0ad048>, <tensorflow.python.framework.ops.EagerTens...(truncated)) at ../Python/ceval.c:754
#16 _PyFunction_FastCall (globals=<optimized out>, nargs=140676083752440, args=<optimized out>, co=<optimized out>) at ../Python/ceval.c:4933
#17 fast_function.lto_priv () at ../Python/ceval.c:4968
#18 0x000000000050a48d in call_function.lto_priv () at ../Python/ceval.c:4872
#19 0x000000000050bfb4 in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3335
#20 0x0000000000509758 in PyEval_EvalFrameEx (throwflag=0, 
    f=Frame 0x543b138, for file inference.py, line 120, in run_inference at ../Python/ceval.c:754
#21 _PyFunction_FastCall (globals=<optimized out>, nargs=88322360, args=<optimized out>, co=<optimized out>) at ../Python/ceval.c:4933
#22 fast_function.lto_priv () at ../Python/ceval.c:4968
#23 0x000000000050a48d in call_function.lto_priv () at ../Python/ceval.c:4872
#24 0x000000000050bfb4 in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3335
#25 0x0000000000507d64 in PyEval_EvalFrameEx (throwflag=0, f=Frame 0x18c6bd8, for file inference.py, line 150, in <module> ()) at ../Python/ceval.c:754
---Type <return> to continue, or q <return> to quit---
#26 _PyEval_EvalCodeWithName.lto_priv.1820 () at ../Python/ceval.c:4166
#27 0x000000000050ae13 in PyEval_EvalCodeEx (closure=0x0, kwdefs=0x0, defcount=0, defs=0x0, kwcount=0, kws=0x0, argcount=0, args=0x0, locals=<optimized out>, 
    globals=<optimized out>, _co=<optimized out>) at ../Python/ceval.c:4187
#28 PyEval_EvalCode (co=<optimized out>, globals=<optimized out>, locals=<optimized out>) at ../Python/ceval.c:731
#29 0x0000000000634c82 in run_mod () at ../Python/pythonrun.c:1025
#30 0x0000000000634d37 in PyRun_FileExFlags () at ../Python/pythonrun.c:978
#31 0x00000000006384ef in PyRun_SimpleFileExFlags () at ../Python/pythonrun.c:419
#32 0x00000000006386c5 in PyRun_AnyFileExFlags () at ../Python/pythonrun.c:81
#33 0x0000000000639091 in run_file (p_cf=0x7ffece85e63c, filename=<optimized out>, fp=<optimized out>) at ../Modules/main.c:340
#34 Py_Main () at ../Modules/main.c:810
#35 0x00000000004b0d00 in main (argc=2, argv=0x7ffece85e838) at ../Programs/python.c:69
```
"
41569,ValueError on evaluate with generator.,"```
output = classifier.predict(x=generator)
evaluate = classifier.evaluate(x=generator)
```

This code throws ValueError: Shapes (None, None, None) and (100, 4, 1, 200) are incompatible on the evaluate. The prediction works just fine. 

I don't understand. Why does the predict work when the evaluate gives me a error? Anyone have any ideas?"
41564,"""Introduction to Tensors"" has incorrect example image","## URL(s) with the issue:

https://www.tensorflow.org/guide/tensor
https://www.tensorflow.org/guide/tensor#multi-axis_indexing

## Description of issue (what needs changing):

The left side image in the figure called ""Selecting the last feature across all locations in each example in the batch"" has batches 1 (blue) and 2 (green) swapped. The right side image and the surrounding code example show the correct order.

The image that needs an edit is: https://github.com/tensorflow/docs/blob/master/site/en/guide/images/tensor/index1.png

### Submit a pull request?

No. I do not have a way to edit the image."
41563,AutoGraph fails due to an end-of-line between parentheses,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): uname -a:
Darwin Daniels-MacBook-Pro.local 18.7.0 Darwin Kernel Version 18.7.0: Tue Aug 20 16:57:14 PDT 2019; root:xnu-4903.271.2~2/RELEASE_X86_64 x86_64

- TensorFlow installed from (source or binary): binary

- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0

- Python version: Python 3.6.8 :: Anaconda, Inc.

**Describe the current behavior**

AutoGraph fails if we introduce an end of line in the definition of a lambda function.  

**Describe the expected behavior**

AutoGraph should work the same way as if there is no EOL. An EOL between parentheses is legitimate Python syntax. In my own code, YAPF introduces the EOL that caused the problem. It was difficult to figure out that this was the problem.

**Standalone code to reproduce the issue**

[colab](https://colab.research.google.com/drive/1b_REMiUPxOwIQ3spSzqpLr6HDJ-yY4j-?usp=sharing)

```python
f = tf.function(
    lambda a, 
    b: a + b)
print(f(1, 2))
```
AutoGraph works however if one removes the EOL between `a,` and `b:`.

**Other info / logs** 

WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f1132d6a268> and will run it as-is.
Cause: could not parse the source code:

    lambda a, 

This error may be avoided by creating the lambda in a standalone statement.

To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function <lambda> at 0x7f1132d6a268> and will run it as-is.
Cause: could not parse the source code:

    lambda a, 

This error may be avoided by creating the lambda in a standalone statement.

To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

"
41562,Build tflite_runtime version 2.1.0.post1 for python3.8,"
Hey all!

I´m working on a aarch64 system and inside a ros:foxy docker container.
Because I´m working with the google coral chip I need to use tflite_runtime version 2.1.0.post1.
According to the edge tpu guy I need to build from commit ""d855adfc5a0195788bf5f92c3c7352e638aa1109""  but after building a python 3.8 wheel file and installing that I still get this error:
`ValueError: Model provided has model identifier ' TPU', should be 'TFL3'`
I´m not sure if this is a edge tpu or tflite issue (see [this](https://github.com/google-coral/edgetpu/issues/170) issue for reference).
I´m also not sure whether i build the correct version because tflite_runtime.__version__ displays 2.1.0. 

Here are my two questions:

- Which branch/tag do I need to pull for version 2.1.0.post1?
- Do you have any idea where this error might come from? (I´m 100% sure I´m loading the correct file)"
41561,Allgather gradients in Tensorflow distributed ,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.2
- Are you willing to contribute it (Yes/No): Yes


**Describe the feature and the current behavior/state.**
All gathering gradients in distributed Tensorflow would allow us to directly manipulate gradients in a much more efficient way. We can then encode (compress) gradients at each device(GPU) before cross-communication and then decode (or decompress) the gradients after all gathering. This would also allow us to combine gradients in a non-trivial way. This can allow researchers to develop efficient coding (or compression) schemes for reduced communication (something like Huffman coding). Currently, AFAIK only all reduce with SUM, MEAN option is supported.

**Will this change the current api? How?**

**Who will benefit with this feature?**
Researchers and developers working in coding techniques for distributed learning. 

**Any Other info.**
There seems to be some TODO mentioned over [here](https://github.com/tensorflow/tensorflow/blob/3b5b3cd58acb7b0192d43db444b8a4db66f5bae3/tensorflow/python/distribute/cross_device_ops.py#L928). But I was wondering if we can do the above in a much simpler way.
1. Encode (or compress) gradients in each device after the gradient calculation [here](https://github.com/tensorflow/tensorflow/blob/2f4444c1cff8ac07ab2c31d1ae23d23c66147126/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L434)
2. Let Tensorflow do all the operations across devices till all reduce.
3. Decode (or decompress) gradients just before the all reduce is called in the function [here](https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/distribute/distribute_lib.py#L1905)."
41559,libtensorflowlite.so built with flex delegates too big!,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 x86_64 GNU/Linux Docker (official tensorflow image)
- TensorFlow installed from (source or binary): pip
- TensorFlow version (or github SHA if from source): 2.4.0-dev20200712

Modified BUILD to add following dependency:
```
""//tensorflow/lite/delegates/flex:delegate"",
```

And did a build using the following command:
```
bazel build --config=monolithic --define=with_select_tf_ops=true -c opt //tensorflow/lite:libtensorflowlite.so
```

**The output:**

`libtensorflowlite.so` that is `147104080` bytes (147MB)

Followed the above instructions to generate a libtensorflow with interpreter support for flex ops. Is this file size of 147MB on the library expected? Is this because I added the flex:delegate dependency? How can I reduce this to be <10MB like the lib produced if I follow the lite/tools/make shell scripts (i.e build_lib.sh)"
41558,RuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 1 (FlexRandomStandardNormal) failed to prepare.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 2.2.0


**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

Colab link here: https://colab.research.google.com/gist/Pyrsos/2e1021981a779b0d41ee51de5b3fd321/bayesiannn_tflite_issue.ipynb

And in case the link does not work here is the code:
```
import tensorflow as tf
import tensorflow_probability as tfp
from tensorflow_probability import distributions as tfd

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

x_train = x_train.astype('float32')/255.
x_test = x_test.astype('float32')/255.

kl_divergence_function = (lambda q, p, _: tfd.kl_divergence(q, p) /
                          tf.cast(x_train.shape[0], dtype=tf.float32))

model = tf.keras.Sequential([
    tf.keras.layers.Flatten(),
    tfp.layers.DenseFlipout(
        10, kernel_divergence_fn=kl_divergence_function,
        activation=tf.nn.softmax
    ),
])

optimizer = tf.keras.optimizers.Adam(lr=0.001)
model.compile(optimizer, loss='sparse_categorical_crossentropy')
model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=3)

tflite_converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,
                                              tf.lite.OpsSet.SELECT_TF_OPS]
tflite_model = tflite_converter.convert()

tflite_interpreter = tf.lite.Interpreter(model_content=tflite_model)
tflite_interpreter.allocate_tensors()

```

**The output from the converter invocation**

```
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz
11493376/11490434 [==============================] - 1s 0us/step
2020-07-20 12:34:31.727019: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2020-07-20 12:34:31.727189: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (303)
2020-07-20 12:34:31.727228: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (0effea1c9ee4): /proc/driver/nvidia/version does not exist
2020-07-20 12:34:31.727927: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-07-20 12:34:31.739524: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2192950000 Hz
2020-07-20 12:34:31.739933: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fd2d0000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-07-20 12:34:31.739972: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-07-20 12:34:31.758131: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 188160000 exceeds 10% of free system memory.
Epoch 1/3
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_probability/python/layers/util.py:106: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.add_weight` method instead.
1871/1875 [============================>.] - ETA: 0s - loss: 0.80902020-07-20 12:34:36.954654: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 31360000 exceeds 10% of free system memory.
1875/1875 [==============================] - 5s 3ms/step - loss: 0.8086 - val_loss: 0.5851
Epoch 2/3
1854/1875 [============================>.] - ETA: 0s - loss: 0.55612020-07-20 12:34:42.216506: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 31360000 exceeds 10% of free system memory.
1875/1875 [==============================] - 5s 3ms/step - loss: 0.5546 - val_loss: 0.5113
Epoch 3/3
1863/1875 [============================>.] - ETA: 0s - loss: 0.50292020-07-20 12:34:47.170768: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 31360000 exceeds 10% of free system memory.
1875/1875 [==============================] - 5s 3ms/step - loss: 0.5030 - val_loss: 0.4894
2020-07-20 12:34:47.848009: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2020-07-20 12:34:47.848232: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-07-20 12:34:47.852393: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: graph_to_optimize
2020-07-20 12:34:47.852454: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.003ms.
2020-07-20 12:34:47.852466: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-07-20 12:34:47.946289: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2020-07-20 12:34:47.946632: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-07-20 12:34:47.962972: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: graph_to_optimize
2020-07-20 12:34:47.963044: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 66 nodes (-64), 67 edges (-65), time = 8.269ms.
2020-07-20 12:34:47.963056: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 66 nodes (0), 67 edges (0), time = 1.115ms.
Traceback (most recent call last):
  File ""example.py"", line 31, in <module>
    tflite_interpreter.allocate_tensors()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter.py"", line 242, in allocate_tensors
    return self._interpreter.AllocateTensors()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py"", line 110, in AllocateTensors
    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)
RuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 0 (FlexRandomUniformInt) failed to prepare.
```

**Also, please include a link to the saved model or GraphDef**

[model.zip](https://github.com/tensorflow/tensorflow/files/4947761/model.zip)

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
Conversion completes successfully but when the interpreter is used to invoke the converted tflite model the following error appears:
```
RuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 0 (FlexRandomUniformInt) failed to prepare.
``` 

**Any other info / logs**

I opened a previous issue regarding this #40119, and was giving guidelines to surpass it. I have now applied this recommendation but I receive the error described in this post.
"
41557,Disable Warning for complex numbers,"When running the following code:
```
import tensorflow as tf

x = tf.Variable(tf.complex([2., 2.], [2., 2.]))
with tf.GradientTape() as tape:
    y = tf.abs(tf.reduce_sum(x))**2
dy_dx = tape.gradient(y, x)
```
I get the warning message:
`WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.complex64`

According to [Introduction to Gradients and Automatic Differentiation](https://www.tensorflow.org/guide/autodiff#3_took_gradients_through_an_integer_or_string). This message is intended when the variable is not derivable. However, it does make sense to use the autodiff for complex numbers and this Warning message impacts my code negatively. I don't want to disable ALL warnings just for this.

This issue is related to [#32774](https://github.com/tensorflow/tensorflow/issues/32774) and [#30107 PR](Make tape only watch the tensor with the floating type).


**System information**
- TensorFlow version (you are using): 2.00
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**
There is a warning in my opinion should not exist.

**Will this change the current api? How?**
Complex gradient will no longer display a Warning

**Who will benefit with this feature?**
Whoever is working with complex numbers.
"
41555,TFLite Metal Delegate failed to build on macOS from v2.3.0,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.5
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: v2.3.0-rc2
- Python version: 3.6.1
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): Apple clang version 11.0.3 (clang-1103.0.32.62)
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the problem**

TFLite Metal Delegate for macOS failed to build on v2.3.0-rc2 (bezel 3.1.0)
I could build on v2.2.0 with the same command. (bezel v2.0.0)

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
bazel build -c opt --copt -Os --copt -DTFLITE_GPU_BINARY_RELEASE --copt -fvisibility=default --linkopt -s --strip always --cxxopt=-std=c++14 --apple_platform_type=macos //tensorflow/lite/delegates/gpu:tensorflow_lite_gpu_dylib
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

The command

`bazel build -c opt --copt -Os --copt -DTFLITE_GPU_BINARY_RELEASE --copt -fvisibility=default --linkopt -s --strip always --cxxopt=-std=c++14 --apple_platform_type=macos //tensorflow/lite/delegates/gpu:tensorflow_lite_gpu_dylib`

Bazel Logs

```
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=159
INFO: Reading rc options for 'build' from /Users/ibu/Projects/tf-unity/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /Users/ibu/Projects/tf-unity/tensorflow/.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2
INFO: Reading rc options for 'build' from /Users/ibu/Projects/tf-unity/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/Users/ibu/.pyenv/shims/python --action_env PYTHON_LIB_PATH=/Users/ibu/.pyenv/versions/3.6.1/lib/python3.6/site-packages --python_path=/Users/ibu/.pyenv/shims/python --config=xla --action_env TF_CONFIGURE_IOS=1
INFO: Found applicable config definition build:v2 in file /Users/ibu/Projects/tf-unity/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:xla in file /Users/ibu/Projects/tf-unity/tensorflow/.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true
INFO: Found applicable config definition build:macos in file /Users/ibu/Projects/tf-unity/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14
DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1556410077 -0400""
DEBUG: Repository io_bazel_rules_docker instantiated at:
  no stack (--record_rule_instantiation_callstack not enabled)
Repository rule git_repository defined at:
  /private/var/tmp/_bazel_ibu/7695e3c21358fe2941153284a4ed3f79/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>
ERROR: /private/var/tmp/_bazel_ibu/7695e3c21358fe2941153284a4ed3f79/external/cpuinfo/BUILD.bazel:96:1: Configurable attribute ""srcs"" doesn't match this configuration (would a default condition help?).
Conditions checked:
 @cpuinfo//:linux_x86_64
 @cpuinfo//:linux_arm
 @cpuinfo//:linux_armhf
 @cpuinfo//:linux_armv7a
 @cpuinfo//:linux_armeabi
 @cpuinfo//:linux_aarch64
 @cpuinfo//:macos_x86_64
 @cpuinfo//:windows_x86_64
 @cpuinfo//:android_armv7
 @cpuinfo//:android_arm64
 @cpuinfo//:android_x86
 @cpuinfo//:android_x86_64
 @cpuinfo//:ios_x86_64
 @cpuinfo//:ios_x86
 @cpuinfo//:ios_armv7
 @cpuinfo//:ios_arm64
 @cpuinfo//:ios_arm64e
 @cpuinfo//:watchos_x86_64
 @cpuinfo//:watchos_x86
 @cpuinfo//:watchos_armv7k
 @cpuinfo//:watchos_arm64_32
 @cpuinfo//:tvos_x86_64
 @cpuinfo//:tvos_arm64
INFO: Repository eigen_archive instantiated at:
  no stack (--record_rule_instantiation_callstack not enabled)
Repository rule tf_http_archive defined at:
  /Users/ibu/Projects/tf-unity/tensorflow/third_party/repo.bzl:134:19: in <toplevel>
ERROR: Analysis of target '//tensorflow/lite/delegates/gpu:tensorflow_lite_gpu_dylib' failed; build aborted:

/private/var/tmp/_bazel_ibu/7695e3c21358fe2941153284a4ed3f79/external/cpuinfo/BUILD.bazel:96:1: Configurable attribute ""srcs"" doesn't match this configuration (would a default condition help?).
Conditions checked:
 @cpuinfo//:linux_x86_64
 @cpuinfo//:linux_arm
 @cpuinfo//:linux_armhf
 @cpuinfo//:linux_armv7a
 @cpuinfo//:linux_armeabi
 @cpuinfo//:linux_aarch64
 @cpuinfo//:macos_x86_64
 @cpuinfo//:windows_x86_64
 @cpuinfo//:android_armv7
 @cpuinfo//:android_arm64
 @cpuinfo//:android_x86
 @cpuinfo//:android_x86_64
 @cpuinfo//:ios_x86_64
 @cpuinfo//:ios_x86
 @cpuinfo//:ios_armv7
 @cpuinfo//:ios_arm64
 @cpuinfo//:ios_arm64e
 @cpuinfo//:watchos_x86_64
 @cpuinfo//:watchos_x86
 @cpuinfo//:watchos_armv7k
 @cpuinfo//:watchos_arm64_32
 @cpuinfo//:tvos_x86_64
 @cpuinfo//:tvos_arm64
INFO: Elapsed time: 0.569s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (13 packages loaded, 395 targets configured)
```

similar issue #37472
"
41554,ObjectTracking is returning null,"I implemented tensorflow/example/android.../tracking into my android app and when I am trying to activate and disable ObjectDetection, then I am getting an error : 
'CHECK FAILED (object_tracker != NULL): null object tracker!' 

In ObjectTracker.java there is a documentation like this:
**
 * True object detector/tracker class that tracks objects across consecutive preview frames.
 * It provides a simplified Java interface to the analogous native object defined by
 * jni/client_vision/tracking/object_tracker.*.
 *
 * Currently, the ObjectTracker is a singleton due to native code restrictions, and so must
 * be allocated by ObjectTracker.getInstance(). **In addition, release() should be called**
 * as soon as the ObjectTracker is no longer needed, **and before a new one is created.**
 *
 * nextFrame() should be called as new frames become available, preferably as often as possible.
 *
 * After allocation, new TrackedObjects may be instantiated via trackObject(). TrackedObjects
 * are associated with the ObjectTracker that created them, and are only valid while that
 * ObjectTracker still exists.
 */

I put a Log in release() method and I saw that this method gets called only once when ObjectTracker is no longer needed but not when a new one is created, 
where should I call the release() method before new one is created?  

"
41553,keras.Model.save_weights is overwriting all_model_checkpoint_paths,"**System information**
- Tensorflow 2.2.0

**Describe the current behavior**

`keras.Model.save_weights` does not respect existing checkpoints.

Using `save_weights()` from a Keras model seems to overwrite the `checkpoint` file without preserving existing checkpoints (`all_model_checkpoint_paths`) of that file.

https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/engine/network.py#L1173

**Describe the expected behavior**

`keras.Model.save_weights` should respect existing checkpoints.

**Standalone code to reproduce the issue**

```python
import os
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers


def main():
    nb_checkpoints = 3

    inputs = layers.Input(shape=(8,))
    x = layers.Dense(2)(inputs)
    outputs = layers.Dense(2)(x)
    model = keras.Model(inputs=[inputs], outputs=[outputs])

    model_dir = '/tmp/save-weights'
    for i in range(nb_checkpoints):
        model.save_weights(os.path.join(model_dir, 'ckpt-%04d' % (i + 1)))

    state = tf.train.get_checkpoint_state(model_dir)
    print('')
    print(state.all_model_checkpoint_paths)

    checkpoint_fp = os.path.join(model_dir, 'checkpoint')
    print('\nContent of %s' % checkpoint_fp)
    with open(checkpoint_fp) as f:
        print(f.read())

    assert state.all_model_checkpoint_paths == nb_checkpoints, \
        'Expected %d checkpoints got %d' % (nb_checkpoints, len(state.all_model_checkpoint_paths))


if __name__ == '__main__':
    main()
```

Output:

```none
['/tmp/save-weights/ckpt-0003']

Content of /tmp/save-weights/checkpoint
model_checkpoint_path: ""ckpt-0003""
all_model_checkpoint_paths: ""ckpt-0003""

Traceback (most recent call last):
    ...
    assert state.all_model_checkpoint_paths == nb_checkpoints, \
AssertionError: Expected 3 checkpoints got 1
```
"
41552,CategoryEncoding not working when loading model again,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 20.04 LTS** 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**
- TensorFlow installed from (source or binary): **nightly binary (tf-nightly-gpu)**
- TensorFlow version (use command below): **2.4.0-dev20200719**
- Python version: **3.8.2**
- Bazel version (if compiling from source): **N/A**
- GCC/Compiler version (if compiling from source): **N/A**
- CUDA/cuDNN version: **V10.1.243**
- GPU model and memory: **NVIDIA RTX2070s 8GB VRAM**

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
When running **[this example](https://keras.io/examples/structured_data/structured_data_classification_from_scratch/)** but saving the model at the end and moving the predict code to another file (predict.py), I get the following error while loading the model: **RuntimeError: If you construct a `CategoryEncoding` layer with `max_tokens=None`, you need to call `adapt()` on it before using it**. 

**Describe the expected behavior**
I don't expect the error, but just being able to load the model and call predict on the model.

**Standalone code to reproduce the issue**
/

**Other info / logs** 
```
2020-07-19 19:11:26.244328: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
Invalid MIT-MAGIC-COOKIE-1 key2020-07-19 19:11:27.161486: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-07-19 19:11:27.164500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-19 19:11:27.164743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:03:00.0 name: GeForce RTX 2070 SUPER computeCapability: 7.5
coreClock: 1.8GHz coreCount: 40 deviceMemorySize: 7.77GiB deviceMemoryBandwidth: 417.29GiB/s
2020-07-19 19:11:27.164756: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-07-19 19:11:27.165592: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-07-19 19:11:27.166145: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-07-19 19:11:27.166487: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-07-19 19:11:27.167819: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-07-19 19:11:27.168359: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-07-19 19:11:27.168437: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory
2020-07-19 19:11:27.168457: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-07-19 19:11:27.168681: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-07-19 19:11:27.172281: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3600000000 Hz
2020-07-19 19:11:27.172519: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4b78d40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-07-19 19:11:27.172531: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-07-19 19:11:27.175397: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-19 19:11:27.175415: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      
Traceback (most recent call last):
  File ""predict.py"", line 5, in <module>
    model = load_model(""models/model"")
  File ""/home/jeroen/.local/lib/python3.8/site-packages/tensorflow/python/keras/saving/save.py"", line 187, in load_model
    return saved_model_load.load(filepath, compile, options)
  File ""/home/jeroen/.local/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py"", line 120, in load
    model = tf_load.load_internal(
  File ""/home/jeroen/.local/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py"", line 632, in load_internal
    loader = loader_cls(object_graph_proto, saved_model_proto, export_dir,
  File ""/home/jeroen/.local/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py"", line 194, in __init__
    super(KerasObjectLoader, self).__init__(*args, **kwargs)
  File ""/home/jeroen/.local/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py"", line 130, in __init__
    self._load_all()
  File ""/home/jeroen/.local/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py"", line 221, in _load_all
    self._finalize_objects()
  File ""/home/jeroen/.local/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py"", line 530, in _finalize_objects
    self._reconstruct_all_models()
  File ""/home/jeroen/.local/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py"", line 548, in _reconstruct_all_models
    self._reconstruct_model(model_id, model, layers)
  File ""/home/jeroen/.local/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py"", line 588, in _reconstruct_model
    created_layers) = functional_lib.reconstruct_from_config(
  File ""/home/jeroen/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py"", line 1210, in reconstruct_from_config
    process_node(layer, node_data)
  File ""/home/jeroen/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py"", line 1158, in process_node
    output_tensors = layer(input_tensors, **kwargs)
  File ""/home/jeroen/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 919, in __call__
    return self._functional_construction_call(inputs, args, kwargs,
  File ""/home/jeroen/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1111, in _functional_construction_call
    outputs = call_fn(cast_inputs, *args, **kwargs)
  File ""/home/jeroen/.local/lib/python3.8/site-packages/tensorflow/python/keras/layers/preprocessing/category_encoding.py"", line 282, in call
    raise RuntimeError(
RuntimeError: If you construct a `CategoryEncoding` layer with `max_tokens=None`, you need to call `adapt()` on it before using it
```

"
41550,tensorflow lite for android a bug: Node number 213 (TfLiteGpuDelegateV2) failed to invoke.,"2020-07-20 13:21:08.699 24598-24632/org.gddi.ai.camera E/AndroidRuntime: FATAL EXCEPTION: Thread-5
    Process: org.gddi.ai.camera, PID: 24598
    java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: TfLiteGpuDelegate Invoke: Failed to read data from GPU (clEnqueueReadBuffer) - Execution status error for events in wait list
    Node number 213 (TfLiteGpuDelegateV2) failed to invoke.
    
        at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)
        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:158)
        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:343)
        at org.tensorflow.lite.Interpreter.run(Interpreter.java:304)
        at com.ncnn.demo.tflite.TfLiteFaceRecognitionModel.recognizeImage(TfLiteFaceRecognitionModel.java:77)
        at com.ncnn.demo.loopTask.FaceComparisonLoopTask.run(FaceComparisonLoopTask.java:103)
        at java.lang.Thread.run(Thread.java:764)"
41549,Incorrect processing in tf.image.decode_gif for multiple-frame image,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): n/a
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS or Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2.0 and 2.3.0rc1
- Python version: 3.7
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the current behavior**

While working with `tf.image.decode_gif` for mult-frame image, I noticed the returned value is incorrect after the first image.


**Describe the expected behavior**

All frames should be handled correctly.

**Standalone code to reproduce the issue**

```
import tensorflow as tf
import matplotlib.pyplot as plt

print(tf.version.VERSION)

!curl -OL https://upload.wikimedia.org/wikipedia/commons/thumb/9/90/Animated_GIF_cheloVechek.gif/440px-Animated_GIF_cheloVechek.gif

image = tf.image.decode_gif(tf.io.read_file('440px-Animated_GIF_cheloVechek.gif'))
for i in range(image.shape[0]):
  plt.imshow(image[i])
  plt.figure()
```
**Other info / logs** Include any logs or source code that would be helpful to

The image is downloaded from WIKI page:
https://en.wikipedia.org/wiki/GIF#Animated_GIF


Original picture (gif):

![cradle](https://user-images.githubusercontent.com/6932348/87897007-da537480-c9fe-11ea-848e-258add6a3dd5.gif)

First frame extracted:

![test_0](https://user-images.githubusercontent.com/6932348/87897022-e4757300-c9fe-11ea-9020-9c979e5216db.png)
Second frame extracted:

![test_1](https://user-images.githubusercontent.com/6932348/87897023-e7706380-c9fe-11ea-9cc6-b3e2991a26d5.png)

"
41547,TensorFlow Lite currently doesn't support control flow ops,"TensorFlow Lite currently doesn't support control flow ops: Merge, Switch. We are working on supporting control flow ops, please see github issue at https://github.com/tensorflow/tensorflow/issues/28485. Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, ADD_N, ARG_MAX, CAST, CONCATENATION, CONV_2D, EXPAND_DIMS, FILL, FLOOR, FULLY_CONNECTED, GATHER_ND, GREATER_EQUAL, LESS, LOGICAL_AND, LOGISTIC, MAXIMUM, MAX_POOL_2D, MINIMUM, MUL, PACK, PAD, RANGE, REDUCE_MAX, RESHAPE, RESIZE_BILINEAR, REVERSE_SEQUENCE, SELECT, SHAPE, SLICE, SPARSE_TO_DENSE, SPLIT, SQUEEZE, STRIDED_SLICE, SUB, TANH, TILE, TRANSPOSE, UNPACK. Here is a list of operators for which you will need custom implementations: CTC_BEAM_SEARCH_DECODER, RandomUniform."
41546,Reciprocal is not executed on GPU for complex number,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: colab GPU
- GPU model and memory: colab GPU

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

Reciprocal is not executed on GPU for complex number.

```python3
import tensorflow as tf

tf.debugging.set_log_device_placement(True)
with tf.device(""gpu""):
  tf.math.reciprocal(tf.complex(1.0, 0.0))

""""""
Executing op Complex in device /job:localhost/replica:0/task:0/device:GPU:0
Executing op Reciprocal in device /job:localhost/replica:0/task:0/device:CPU:0
""""""
```

**Describe the expected behavior**

The reciprocal op can be executed on GPU for complex number

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

https://colab.research.google.com/drive/1x0NX11ZBjA95KSeafLg0_LRvVe8L2uyD?usp=sharing

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
41545,model.export(export_dir='.')  does not export labels nor model.,"**System information**
- OSx
- TensorFlow installed from source
-  tf.__version__  ='2.4.0-dev20200719'


**Command used to run the converter or code if you’re using the Python API**
Running the code in [this notebook](https://colab.research.google.com/drive/1_LZxdWIfYFpZorpUkzgx_MPZzdzi8Hhy?usp=sharing), the export is fine. The issue arises when running the same code locally in a virtualenv with py36 and the same packages as per `!pip install git+https://github.com/tensorflow/examples.git#egg=tensorflow-examples[model_maker]`   

```
model.export(export_dir='.')
```

**The output from the converter invocation**

```
WARNING:tensorflow:From /Users/akiva111admin/code/security/venv_tf2/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /Users/akiva111admin/code/security/venv_tf2/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
2020-07-19 22:21:59.233332: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
WARNING:tensorflow:From /Users/akiva111admin/code/security/venv_tf2/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /Users/akiva111admin/code/security/venv_tf2/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
2020-07-19 22:22:11.638306: I tensorflow/core/grappler/devices.cc:78] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)
2020-07-19 22:22:11.638403: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-07-19 22:22:11.689811: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize
2020-07-19 22:22:11.689845: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: Graph size after: 912 nodes (656), 920 edges (664), time = 32.473ms.
2020-07-19 22:22:11.689850: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 1.228ms.
2020-07-19 22:22:14.652608: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:315] Ignored output_format.
2020-07-19 22:22:14.652634: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:318] Ignored drop_control_dependency.
```

**Also, please include a link to the saved model or GraphDef**

```
ipdb> model.summary()                                                                                                                                                                
Model: ""sequential""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
hub_keras_layer_v1v2 (HubKer (None, 1280)              3413024   
_________________________________________________________________
dropout (Dropout)            (None, 1280)              0         
_________________________________________________________________
dense (Dense)                (None, 2)                 2562      
=================================================================
Total params: 3,415,586
Trainable params: 2,562
Non-trainable params: 3,413,024
_________________________________________________________________

type(model)                                                                                                                                                                    
<class 'tensorflow_examples.lite.model_maker.core.task.image_classifier.ImageClassifier'>

model.hparams                                                                                                                                                                  
HParams(train_epochs=5, do_fine_tuning=False, batch_size=32, learning_rate=0.005, momentum=0.9, dropout_rate=0.2)

model.model_spec.name                                                                                                                                                          
'efficientnet_lite0'

model.num_classes                                                                                                                                                              
2

```

**Failure details**

Expected output as per the [example here](https://www.tensorflow.org/lite/tutorials/model_maker_image_classification#step_3_evaluate_the_customized_model) looks like: 
```INFO:tensorflow:Saving labels in ./labels.txt
INFO:tensorflow:Saving labels in ./labels.txt.
INFO:tensorflow:Assets written to: /tmp/tmp886peoe2/assets
INFO:tensorflow:Assets written to: /tmp/tmp886peoe2/assets```



"
41544,How to load tensorflow 2.0 savedbundle file  by using Java,"From Tensorflow 1.x version, Java can handle a pb file after using freeze_graph.pb.
But, from Tensorflow 2, since the saved pb structure is different, I don't think a regular way to export file does not work.

I was wondering how to load Tensorflow 2 saved files by using Java."
41543,subclassed model load_model ValueError?,"I am using tf==2.2.0
Tried the collaborative filtering model
https://keras.io/examples/structured_data/collaborative_filtering_movielens/

for saving and loading the model, these worked
```
tf.keras.models.save_model(model,'./saved_model')
loaded_model = tf.keras.models.load_model('./saved_model')
```
But after loading the model while prediction, I get value error
```
ratings = model.predict(user_movie_array).flatten()

    ValueError: Python inputs incompatible with input_signature:
      inputs: (
        Tensor(""IteratorGetNext:0"", shape=(None, 2), dtype=int32))
      input_signature: (
        TensorSpec(shape=(None, 2), dtype=tf.int64, name='input_1'))
```
May i know how to fix this?

"
41542,tf.io.TFRecordWriter cannot work with gs:// or ram:// at Google Colab TPU,"**System information**
- TensorFlow version (use command below): 2.2.0 (v2.2.0-0-g2b96f3662b)
- Python version: 3.6.9
- GPU model and memory: Google Colab TPU

**Current behavior**

At first, I gave TPU permission to create files on my cloud bucket:
`!gsutil iam ch serviceAccount:service-495559152420@cloud-tpu.iam.gserviceaccount.com:roles/storage.objectCreator gs://oleg-zyablov/`

Now I want to create file and tf.io.write_file does it successfully. Standalone code (except bucket name):
```
path = 'gs://oleg-zyablov/car-classification/train_tfrecords/256x256_square/test'
    tf.io.write_file(path, tf.constant('', dtype = tf.string))
```

But TFRecordWriter fails:
```
with tf.io.TFRecordWriter(path) as out_file:
  out_file.write(tf.constant('', dtype = tf.string))
```

```---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-340-960a1cb6ee89> in <module>()
      5 with tf.io.TFRecordWriter(path) as out_file:
----> 6   out_file.write(tf.constant('', dtype = tf.string))

1 frames
TypeError: write(): incompatible function arguments. The following argument types are supported:
    1. (self: tensorflow.python._pywrap_record_io.RecordWriter, record: str) -> None

Invoked with: <tensorflow.python.lib.io.tf_record.TFRecordWriter object at 0x7feba977aa40>, <tf.Tensor: shape=(), dtype=string, numpy=b''>

During handling of the above exception, another exception occurred:

PermissionDeniedError                     Traceback (most recent call last)
<ipython-input-340-960a1cb6ee89> in <module>()
      4 
      5 with tf.io.TFRecordWriter(path) as out_file:
----> 6   out_file.write(tf.constant('', dtype = tf.string))

PermissionDeniedError: Error executing an HTTP request: HTTP response code 401 with body '{
  ""error"": {
    ""code"": 401,
    ""message"": ""Anonymous caller does not have storage.objects.create access to oleg-zyablov/car-classification/train_tfrecords/256x256_square/test."",
    ""errors"": [
      {
        ""message"": ""Anonymous caller does not have storage.objects.create access to oleg-zyablov/car-classification/train_tfrecords/256x256_square/test."",
        ""domain"": ""global"",
        ""reason"": ""required"",
        ""locationType"": ""header"",
        ""location"": ""Authorization""
      }
  '
	 when initiating an upload to gs://oleg-zyablov/car-classification/train_tfrecords/256x256_square/test
```

When I want to write file to temporary storage (path = 'ram://testfile'), tf.io.write_file also does it successfully, but TFRecordWriter fails again with message ""File system scheme 'ram' not implemented"".

**Expected behavior**

I expected that TFRecordWriter could write file to gs:// just like tf.io.write_file does it. But seems like it does not use proper autentification."
41541,tensorflow_gpu-1.4.0 requires libcudart.so.10.0 rather than libcudart.so.8.0,"**System information**
- OS Platform and Distribution: Linux Ubuntu 16.04
- Mobile device if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.4.0
- Python version: 3.6.10
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 8.0 / 6
- GPU model and memory: Tesla V100-SXM2-16GB

**Describe the problem**
I tried to use openai's [blocksparse](https://github.com/openai/blocksparse), which requires CUDA 8 and Tensorflow 1.4.0. When I `import blocksparse`, the error `tensorflow.python.framework.errors_impl.NotFoundError: libcudart.so.10.0: cannot open shared object file: No such file or directory` is raised. Since tensorflow_gpu-1.4.0 is compatible with CUDA 8, I wonder why `libcudart.so.10.0` is required rather than `libcudart.so.8.0`. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
First I created a conda environment and activated it.
```
conda create -n py36-bs python=3.6
conda activate py36-bs
```
Then I installed TensorFlow 1.4.0.
```
pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.4.0-cp36-cp36m-linux_x86_64.whl
```
Next I installed [blocksparse](https://github.com/openai/blocksparse).
```
pip install blocksparse
```
When I tried to `import bloacksparse`, an error occurred.
```
Python 3.6.10 |Anaconda, Inc.| (default, May  8 2020, 02:54:21) 
[GCC 7.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import blocksparse
/home/ubuntu/anaconda3/envs/py36-bs/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:469: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/home/ubuntu/anaconda3/envs/py36-bs/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:470: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/home/ubuntu/anaconda3/envs/py36-bs/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/home/ubuntu/anaconda3/envs/py36-bs/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/home/ubuntu/anaconda3/envs/py36-bs/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/home/ubuntu/anaconda3/envs/py36-bs/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:476: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
/home/ubuntu/anaconda3/envs/py36-bs/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6
  return f(*args, **kwds)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/ubuntu/anaconda3/envs/py36-bs/lib/python3.6/site-packages/blocksparse/__init__.py"", line 3, in <module>
    from blocksparse.utils import (
  File ""/home/ubuntu/anaconda3/envs/py36-bs/lib/python3.6/site-packages/blocksparse/utils.py"", line 16, in <module>
    _op_module = tf.load_op_library(os.path.join(data_files_path, 'blocksparse_ops.so'))
  File ""/home/ubuntu/anaconda3/envs/py36-bs/lib/python3.6/site-packages/tensorflow/python/framework/load_library.py"", line 56, in load_op_library
    lib_handle = py_tf.TF_LoadLibrary(library_filename, status)
  File ""/home/ubuntu/anaconda3/envs/py36-bs/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: libcudart.so.10.0: cannot open shared object file: No such file or directory
>>>
```

**Any other info / logs**
I checked several things related to CUDA.
```
(py36-bs) ubuntu@xxx:~$ ls -l /usr/local/cuda
lrwxrwxrwx 1 root root 19 Jul 19 13:13 /usr/local/cuda -> /usr/local/cuda-8.0
```
```
(py36-bs) ubuntu@xxx:~$ nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2016 NVIDIA Corporation
Built on Tue_Jan_10_13:22:03_CST_2017
Cuda compilation tools, release 8.0, V8.0.61
```
```
(py36-bs) ubuntu@xxx:~$ echo $PATH
/home/ubuntu/anaconda3/envs/py36-bs/bin:/home/ubuntu/anaconda3/bin:/home/ubuntu/anaconda3/bin:/home/ubuntu/anaconda3/bin:/usr/local/cuda/bin:/usr/local/bin:/opt/aws/bin:/usr/local/mpi/bin:/usr/local/cuda/bin:/usr/local/bin:/opt/aws/bin:/home/ubuntu/.dl_binaries/bin:/usr/local/mpi/bin:/opt/aws/neuron/bin:/home/ubuntu/.vscode-server/bin/5763d909d5f12fe19f215cbfdd29a91c0fa9208a/bin:/home/ubuntu/anaconda3/bin:/home/ubuntu/bin:/home/ubuntu/.local/bin:/home/ubuntu/anaconda3/condabin:/home/ubuntu/anaconda3/bin:/usr/local/cuda/bin:/usr/local/bin:/opt/aws/bin:/usr/local/mpi/bin:/usr/local/cuda/bin:/usr/local/bin:/opt/aws/bin:/home/ubuntu/.dl_binaries/bin:/usr/local/mpi/bin:/opt/aws/neuron/bin:/usr/local/cuda/bin:/usr/local/bin:/opt/aws/bin:/usr/local/mpi/bin:/opt/amazon/openmpi/bin:/opt/amazon/efa/bin:/home/ubuntu/.vscode-server/bin/5763d909d5f12fe19f215cbfdd29a91c0fa9208a/bin:/home/ubuntu/anaconda3/bin:/home/ubuntu/bin:/home/ubuntu/.local/bin:/home/ubuntu/anaconda3/bin:/usr/local/cuda/bin:/usr/local/bin:/opt/aws/bin:/usr/local/mpi/bin:/usr/local/cuda/bin:/usr/local/bin:/opt/aws/bin:/home/ubuntu/.dl_binaries/bin:/usr/local/mpi/bin:/opt/aws/neuron/bin:/usr/local/cuda/bin:/usr/local/bin:/opt/aws/bin:/usr/local/mpi/bin:/opt/amazon/openmpi/bin:/opt/amazon/efa/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
```
```
(py36-bs) ubuntu@xxx:~$ echo $LD_LIBRARY_PATH
/usr/lib64/openmpi/lib/:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/mpi/lib:/lib/:/usr/local/cuda-9.0/lib/:/usr/lib64/openmpi/lib/:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/mpi/lib:/lib/:/usr/local/cuda-9.0/lib/:/usr/lib64/openmpi/lib/:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/mpi/lib:/lib/:/usr/local/cuda-9.0/lib/:/usr/lib64/openmpi/lib/:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/mpi/lib:/lib/:/usr/local/cuda-9.0/lib/:/usr/lib64/openmpi/lib/:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/mpi/lib:/lib/:/usr/local/cuda-9.0/lib/:
```
`libcudart.so.8.0` exists but TensorFlow asked for `libcudart.so.10.0`. 
```
(py36-bs) ubuntu@xxx:~$ ls -l /usr/local/cuda/lib64/libcudart.so.8.0
lrwxrwxrwx 1 root root 19 Jun 17 01:36 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.61
```
Besides, there exist multiple versions of CUDA on my machine.
```
(py36-bs) ubuntu@xxx:~$ ls /usr/local/
bin  cuda  cuda-10.0  cuda-10.1  cuda-10.2  cuda-8.0  cuda-9.0  cuda-9.2  etc  games  include  init  lib  man  mpi  sbin  share  src
```
Is the problem related to GPU driver? When I entered `nvidia-smi`, `CUDA Version` is `10.2`.

Since I met some wired problems when I tried to run [blocksparse](https://github.com/openai/blocksparse) with CUDA 10.0, I want to use CUDA 8.0, which is the version authors of [blocksparse](https://github.com/openai/blocksparse) use. Could anyone help me with the error? Thanks a lot!

tensorflow_gpu-1.4.0 requires libcudart.so.10.0 rather than libcudart.so.8.0
"
41540,Train a tensorflow model using 1 Terabyte swap file,"Hi there,
i want to train a tensorflow model that is huge and require alot of memory.
So i increased the swap file size and swapiness, but in training it also uses the ram so it fails.
Is there a way to train tensorflow with using swap file as memory? "
41539,nccl_ops.all_sum does not correctly reduce gradients,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:n/a
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below): v2.2.0-rc3-33-g70087ab4f4 2.2.0-rc4
- Python version:3.7
- Bazel version (if compiling from source):n/a
- GCC/Compiler version (if compiling from source):n/a
- CUDA/cuDNN version:10.1/7.6.5
- GPU model and memory:P100, V100

**Describe the current behavior**
The allreduce operation `nccl_ops.all_sum` does not correctly sum gradients. The results are __incorrect__.


**Standalone code to reproduce the issue**
```python
#!/usr/bin/env python
import argparse
from tensorflow.compat import v1 as tf
import tqdm

def split_grad_list(grad_list):
    g = []
    v = []
    for tower in grad_list:
        g.append([x[0] for x in tower])
        v.append([x[1] for x in tower])
    return g, v

def allreduce_grads(all_grads):
    # reduce gradients for N variables on K devices
    from tensorflow.python.ops import nccl_ops as nccl
    nr_tower = len(all_grads)
    assert nr_tower > 1
    new_all_grads = []  # N x K
    for grads in zip(*all_grads):
        # k grads
        summed = nccl.all_sum(grads)

        grads_for_devices = []  # K
        true_sum = tf.add_n(grads)
        for g in summed:
            diff = tf.abs(true_sum - g)
            eql = diff < 1e-4
            nccl_res_correct = tf.reduce_all(eql, name=""corr_"" + grads[0].op.name)

            def flat(x):
                x = tf.reshape(x, [-1])
                x = tf.slice(x, [0], [tf.minimum(tf.size(x), 200)])
                return x

            assert_op = tf.debugging.Assert(nccl_res_correct, [
                tf.reduce_max(diff), flat(true_sum), flat(g)], summarize=1000,
                name='assert_' + grads[0].op.name)
            with tf.control_dependencies([assert_op]):
                g = tf.identity(g)
            grads_for_devices.append(g)
        new_all_grads.append(grads_for_devices)
    # transpose to K x N
    ret = list(zip(*new_all_grads))
    return ret

def build_graph(image, label, idx):
    v1 = tf.get_variable('aaa/W', shape=[3, 3, 3, 64], trainable=True)
    v2 = tf.get_variable('bbb/W', shape=[3, 3, 3, 64], trainable=True)
    v = v1 if idx == 0 else v2
    image = tf.nn.conv2d(image, v, 1, padding='SAME', data_format='NCHW')

    def conv(name, x, chan, stride=1):
        with tf.variable_scope(name):
            in_chan = x.shape[1]
            W = tf.get_variable('W', [3, 3, in_chan, chan])
            ret = tf.nn.conv2d(x, W, strides=stride, padding=""SAME"", data_format=""NCHW"")
            return tf.nn.relu(ret)

    x = conv('conv1', image, 64)
    x = conv('conv2', x, 64)
    x = conv('conv3', x, 1280, stride=2)
    x = conv('conv4', x, 1280, stride=2)
    x = conv('conv5', x, 10)
    logits = tf.reduce_mean(x, axis=[2, 3])
    cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)
    cost = tf.reduce_mean(cost, name='cross_entropy_loss')
    return cost


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--gpu', type=int)
    args = parser.parse_args()
    num_gpu = args.gpu

    with tf.Graph().as_default():
        opt = tf.train.GradientDescentOptimizer(0.001)

        grad_list = []
        for k in range(num_gpu):
            with tf.device(""/gpu:{}"".format(k)), tf.variable_scope(""tower{}"".format(k)):
                print(""Building {} ..."".format(k))
                image = tf.random.uniform([32, 3, 30, 30])
                label = tf.random.uniform([32], maxval=9, dtype=tf.int32)
                cost = build_graph(image, label, k)
                varlist = [x for x in tf.trainable_variables() if x.name.startswith(""tower{}"".format(k))]
                print(""Varlist for tower {}: "".format(k), [x.name for x in varlist])
                wd_cost = [tf.reduce_sum(x) * 1e-3 for x in varlist]
                cost = tf.add_n([cost] + wd_cost)
                grads = opt.compute_gradients(cost, var_list=varlist)
                grad_list.append(grads)

        all_grads, all_vars = split_grad_list(grad_list)
        all_grads = allreduce_grads(all_grads)
        grad_list = [list(zip(gs, vs)) for gs, vs in zip(all_grads, all_vars)]

        train_ops = []
        for idx, grad_and_vars in enumerate(grad_list):
            with tf.device('/gpu:{}'.format(idx)):
                train_ops.append(opt.apply_gradients(
                    grad_and_vars, name='apply_grad_{}'.format(idx)))
        train_op = tf.group(*train_ops)

        sess = tf.Session()
        sess.run(tf.global_variables_initializer())
        print(""Training ..."")
        for k in tqdm.trange(5000):
            sess.run(train_op)
```

The above code trains a toy network on random data, and allreduce the gradients using `nccl_ops.all_sum`. It checks the allreduce results against the sum of gradients computed by a naive `add_n`, and asserts that the difference is reasonably small. However, the difference can be quite large sometimes and the assertion usually fails within 100 steps of training.

The code above (written in TF1 style) can be run on a machine with >=2 GPUs using
```
$ TF2_BEHAVIOR=0 python a.py --gpu 2
Building 0 ...
 Varlist for tower 0:  ['tower0/aaa/W:0', 'tower0/bbb/W:0', 'tower0/conv1/W:0', 'tower0/conv2/W:0', 'tower0/conv3/W:0', 'tower0/conv4/W:0', 'tower0/conv5/W:0']                                      
Building 1 ...  
Varlist for tower 1:  ['tower1/aaa/W:0', 'tower1/bbb/W:0', 'tower1/conv1/W:0', 'tower1/conv2/W:0', 'tower1/conv3/W:0', 'tower1/conv4/W:0', 'tower1/conv5/W:0'] 
1%|▉                                                                    | 71/5000 [00:06<07:39, 10.73it/s]    
Traceback (most recent call last):                                                                                                                                                                  
  File ""/private/home/yuxinwu/env/py37-tf2.2v2/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1365, in _do_call                                                             
    return fn(*args)                                                                                                                                                                                
  File ""/private/home/yuxinwu/env/py37-tf2.2v2/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn                                                              
    target_list, run_metadata)                                                                                                                                                                      
  File ""/private/home/yuxinwu/env/py37-tf2.2v2/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun                                                  
    run_metadata)                                                                                                                                                                                   
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.                                                                                                                
  (0) Invalid argument: assertion failed: [0.00100000016] [0.00234295963 0.00230941921 0.00176228327 0.00197261758 0.00213356828 0.00188576151 0.00211580051 0.00221353304 
```

My initial investigation suggests (no proof, just a guess) that the bug might appear because the gradients are computed on each GPU in different order.

The bug was found to exist in TF 1.15 as well. Have not tested earlier versions.
The bug rarely triggers itself if I revert https://github.com/tensorflow/tensorflow/pull/31481, which is a PR that make allreduce ops scheduled as early as possible. 
`collective_ops.all_reduce` with the ring implementation does not seem to have similar issue, but it significantly slows down my training.

cc @dubey @yuefengz @chsigg  who may have context on this issue."
41538,  The memory required for the training model is equivalent to the size of “model.ckpt-0.data”？,"I want to allocate enough memory space in advance for my model.
The size of ""model.ckpt-0.data"" is 1GB,
Does it mean that the memory consumed during training is about 1GB."
41537,From Keras 2.3.1 to 2.4: Input 0 of layer fc1 is incompatible with the layer,"

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): It's taken from a deep learning book.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.5, 2.0, 2.2, 2.3
- Python version: 3.6, 3.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10.2
- GPU model and memory: GTX 1060 6Go

1. Local machine: v1.15.2-30-g4386a66 1.15.3
2. Google Colab: v2.2.0-0-g2b96f3662b 2.2.0

**Describe the current behavior**
I change the input shape of the VGG19 model.but w
On Keras 2.3.1 it works well using Keras imports. When changing to Tensorflow imports, and Keras 2.4.0, I get an error with the same code.

**Describe the expected behavior**
I expect the input shape of VGG19 to change with the new shape as it works on Keras 2.3.1

**Standalone code to reproduce the issue**
Google Colab: https://colab.research.google.com/drive/1IT9e2aYEodW5tO-Z8fEeszgVHA41R4WY?usp=sharing

With Keras 2.4
```
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Input
from tensorflow.keras.applications import VGG19
from tensorflow.keras.models import Model

input_shape = (256, 256, 3)
# Load a pre-trained VGG19 model trained on 'Imagenet' dataset
vgg = VGG19(weights=""imagenet"")
vgg.outputs = [vgg.layers[9].output]

input_layer = Input(shape=input_shape)

# Extract features
features = vgg(input_layer)

# Create a Keras model
model = Model(inputs=[input_layer], outputs=[features])
```
On the `features = vgg(input_layer)` line I get the error:
```
ValueError                                Traceback (most recent call last)
<ipython-input-7-9b16ae481ea8> in <module>
      1 # Extract features
----> 2 features = vgg(input_layer)

~/anaconda3/envs/srgan/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    852                     outputs = base_layer_utils.mark_as_return(outputs, acd)
    853                 else:
--> 854                   outputs = call_fn(cast_inputs, *args, **kwargs)
    855 
    856             except errors.OperatorNotAllowedInGraphError as e:

~/anaconda3/envs/srgan/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py in call(self, inputs, training, mask)
    693                                 ' implement a `call` method.')
    694 
--> 695     return self._run_internal_graph(inputs, training=training, mask=mask)
    696 
    697   def compute_output_shape(self, input_shape):

~/anaconda3/envs/srgan/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py in _run_internal_graph(self, inputs, training, mask)
    842 
    843           # Compute outputs.
--> 844           output_tensors = layer(computed_tensors, **kwargs)
    845 
    846           # Update tensor_dict.

~/anaconda3/envs/srgan/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    817         # are casted, not before.
    818         input_spec.assert_input_compatibility(self.input_spec, inputs,
--> 819                                               self.name)
    820         graph = backend.get_graph()
    821         with graph.as_default(), backend.name_scope(self._name_scope()):

~/anaconda3/envs/srgan/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/input_spec.py in assert_input_compatibility(input_spec, inputs, layer_name)
    211                 ' incompatible with the layer: expected axis ' + str(axis) +
    212                 ' of input shape to have value ' + str(value) +
--> 213                 ' but received input with shape ' + str(shape))
    214     # Check shape.
    215     if spec.shape is not None:

ValueError: Input 0 of layer fc1 is incompatible with the layer: expected axis -1 of input shape to have value 25088 but received input with shape [None, 32768]
```


**Other info / logs** Include any logs or source code that would be helpful to
Working code with Keras 2.3.1:

```
from tensorflow import keras
from keras import Input
from keras.applications import VGG19
from keras.models import Model

input_shape = (256, 256, 3)

# Load a pre-trained VGG19 model trained on 'Imagenet' dataset
vgg = VGG19(weights=""imagenet"")
vgg.outputs = [vgg.layers[9].output]

input_layer = Input(shape=input_shape)

# Extract features
features = vgg(input_layer)

# Create a Keras model
model = Model(inputs=[input_layer], outputs=[features])
```
_Note_: code is the same in both cases, just the imports change.



"
41536,Extending ctc_batch_cost to handle RaggedTensors,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: Yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Colab
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
This is with respect to https://github.com/tensorflow/tensorflow/issues/41276. Basically, I am trying to extend this [official Keras example on OCR](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/captcha_ocr.ipynb) to the [IAM dataset](http://www.fki.inf.unibe.ch/DBs/iamDB/iLogin/index.php) which consists of images of handwritten characters. Here are some samples from the dataset:

![image](https://user-images.githubusercontent.com/22957388/87866184-23261180-c99c-11ea-8c98-1f9e4b6269c5.png)


 In order for the CTC loss to work (calculated with `ctc_batch_cost`) the ground truth labels and the targets should not be variable-length sequences (`RaggedTensors`s) which is not the case here ([reference](https://github.com/tensorflow/tensorflow/issues/41276#issuecomment-659466273)). So my question is how to extend `ctc_batch_cost` in this case. 

### Source code / logs
[Colab Notebook](https://colab.research.google.com/gist/amahendrakar/71ce949e24f2ac3a4c8ad6ad71590a04/41276-2-3.ipynb)

Cc: @alextp 
"
41535,KeyError: 'name' when try to load saved model with tf.keras.models.load_model(),"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on a mobile device: None
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.2.0
- Python version: 3.7.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1
- GPU model and memory: NVIDIA GeForce GTX 750 Ti 10GB (2GB Dedicated Memory; 8GB Shared Memory)

**Describe the current behavior**
I keep getting an `KeyError: 'name'` when trying to load a saved model (.pb) with `tf.keras.models.load_model(path)`

**Describe the expected behavior**
Load the model successfully.

**Standalone code to reproduce the issue**

Snippet where the error happens. 
```python
import tensorflow as tf 

# Load model
model = tf.keras.models.load_model(""models/bs-32-ep-10-1595106737.3691123/"") # Error happens here

# Do stuff
...
```

**Other info / logs**

My model is being saved within a class that inherits from `tf.keras.models.Sequential()`.
I haven't got any errors while training or saving my model.

```python
def train(self):
        self.compile(
            loss=self.loss,
            optimizer=self.optimizer(lr=0.0001, decay=1e-6),
            metrics=[*self.mtcs]
        )

        self.fit(
            x=self.train_data[0],
            y=self.train_data[1],
            batch_size=self.batch_size,
            epochs=self.epochs,
            validation_data=(*self.test_data,),
            callbacks=[*self.callbacks]
        )

        if self.save_after_training:
            self.save(""models/{}"".format(self.NAME)) # Save Model
```
The Error:
```
Traceback (most recent call last):
  File ""RNN.py"", line 63, in <module>
    model = tf.keras.models.load_model(""models/bs-32-ep-10-1595106737.3691123/"")
  File ""C:\Users\EWC\github\GLAI_\venv\lib\site-packages\tensorflow\python\keras\saving\save.py"", line 190, in load_model
    return saved_model_load.load(filepath, compile)
  File ""C:\Users\EWC\github\GLAI_\venv\lib\site-packages\tensorflow\python\keras\saving\saved_model\load.py"", line 116, in load
    model = tf_load.load_internal(path, loader_cls=KerasObjectLoader)
  File ""C:\Users\EWC\github\GLAI_\venv\lib\site-packages\tensorflow\python\saved_model\load.py"", line 604, in load_internal
    export_dir)
  File ""C:\Users\EWC\github\GLAI_\venv\lib\site-packages\tensorflow\python\keras\saving\saved_model\load.py"", line 188, in __init__
    super(KerasObjectLoader, self).__init__(*args, **kwargs)
  File ""C:\Users\EWC\github\GLAI_\venv\lib\site-packages\tensorflow\python\saved_model\load.py"", line 123, in __init__
    self._load_all()
  File ""C:\Users\EWC\github\GLAI_\venv\lib\site-packages\tensorflow\python\keras\saving\saved_model\load.py"", line 215, in _load_all
    self._finalize_objects()
  File ""C:\Users\EWC\github\GLAI_\venv\lib\site-packages\tensorflow\python\keras\saving\saved_model\load.py"", line 510, in _finalize_objects
    self._reconstruct_all_models()
  File ""C:\Users\EWC\github\GLAI_\venv\lib\site-packages\tensorflow\python\keras\saving\saved_model\load.py"", line 528, in _reconstruct_all_models
    self._reconstruct_model(model_id, model, layers)
  File ""C:\Users\EWC\github\GLAI_\venv\lib\site-packages\tensorflow\python\keras\saving\saved_model\load.py"", line 564, in _reconstruct_model
    config, created_layers={layer.name: layer for layer in layers})
  File ""C:\Users\EWC\github\GLAI_\venv\lib\site-packages\tensorflow\python\keras\engine\network.py"", line 2019, in reconstruct_from_config
    process_layer(layer_data)
  File ""C:\Users\EWC\github\GLAI_\venv\lib\site-packages\tensorflow\python\keras\engine\network.py"", line 1993, in process_layer
    layer_name = layer_data['name']
KeyError: 'name'
```

If more code is required, please let me know. Thank you"
41534,"TensorFlow imports into python3 correctly, but I receive unexpected messages","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): NVIDIA Jetpack4.3 Ubuntu 18.0.4
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Jetson Nano
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.1.0+nv20.3
- Python version: 3.6.9
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0
- GPU model and memory:



**Describe the problem**

I followed these instruction to install TensorFlow 2.1.0 on Jetpack4.3 on my Jetson Nano: https://docs.nvidia.com/deeplearning/frameworks/install-tf-jetson-platform/index.html except that I installed TensorFlow in a virtual environment (py3cv4) per Section 3 Installing TensorFlow/3.1 Installing Multiple Versions.

When I am in my virtual environment if I start python3 and import tensorflow, I get the following response before the command line prompt returns:

(py3cv4) thomas@thomas-desktop:~$ python3
Python 3.6.9 (default, Apr 18 2020, 01:56:04)
[GCC 8.4.0] on linux
Type “help”, “copyright”, “credits” or “license” for more information.

import tensorflow
2020-07-18 17:28:48.819958: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-07-18 17:28:51.385750: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-07-18 17:28:51.387403: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6

Is this an issue I should be concerned about or did I miss installing a particular TensorFlow dependency?

Regards,
TCIII"
41533,Can Keras be installed to work with the TensorFlow 1.15.0 for C++ binary or should I build TensorFlow 2.0 for C++ from source to get Keras?,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04 LTS Oracle VM Virtualbox inside Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): Binary https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-linux-x86_64-1.15.0.tar.gz
- TensorFlow version: 1.15.0
- Python version: Actually I'm using the C++ 0.29.0 extension in Visual Studio Code 1.47.1, but my Python version is Python 3.8.2
- Installed using virtualenv? pip? conda?: No installed from TensorFlow C++ binary libtensorflow-gpu-linux-x86_64-1.15.0.tar.gz
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a?
- GPU model and memory: NVIDIA GeForce GTS 965M with 2,048 MB GDDR5 memory. 32 GB RAM.

**Describe the problem**

I do not know how to install Keras in TensorFlow 1.15.0 for C++ even though I have previously installed and used Keras in Python.  The description of Keras on their website suggests that Keras only works in Python and cannot be used in C++, while on the other hand I heard that TensorFlow 2.0 has Keras inside so maybe I could build that from source to work in C++?

> Keras is a deep learning API written in **Python**, running on top of the machine learning platform TensorFlow.

https://keras.io/about/

On the other hand I heard that TensorFlow 2.0 has Keras inside:

> Now with version 2, TensorFlow includes Keras built it.

https://towardsdatascience.com/creating-a-tensorflow-cnn-in-c-part-2-eea0de9dcada

So would it be possible to build TensorFlow 2.0 alpha in C++ to get Keras that way?

> Build TensorFlow 2.0 Alpha from source

https://itnext.io/how-to-use-your-c-muscle-using-tensorflow-2-0-and-xcode-without-using-bazel-builds-9dc82d5e7f80

Is this where the TensorFlow 2.0 source is?
https://github.com/tensorflow/tensorflow

I want to use Keras in C++ because I am doing a Udemy.com C++ online course capstone project to use a Convolutional Neural Network for image classification.  I have successfully installed TensorFlow 1.15.0 for C++ by following the directions here: https://www.tensorflow.org/install/lang_c and some C++ TensorFlow test code worked.

So the installation instructions for Keras say do this but the problem is I think that's for python only not C++:
```
sudo apt install python3-pip
pip install --upgrade pip
pip install keras
pip install --upgrade keras
```
https://linuxize.com/post/how-to-install-pip-on-ubuntu-18.04/
https://www.liquidweb.com/kb/how-to-install-keras/

The alternative is to not use Keras to do the Convolutional Neural Network project and maybe this tutorial would show me how to do that, though they say that Keras is more efficient than doing it without Keras:
https://itnext.io/how-to-use-your-c-muscle-using-tensorflow-2-0-and-xcode-without-using-bazel-builds-9dc82d5e7f80
https://itnext.io/creating-a-tensorflow-dnn-in-c-part-1-54ce69bbd586
https://towardsdatascience.com/creating-a-tensorflow-cnn-in-c-part-2-eea0de9dcada
https://towardsdatascience.com/creating-a-tensorflow-cnn-in-c-part-3-freezing-the-model-and-augmenting-it-59a07c7c4ec6

**Provide the exact sequence of commands / steps that you executed before running into the problem**

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
41532,2.3.0-rc2: Check failed: ret == 0 (11 vs. 0) Thread creation via pthread_create() failed. Aborted,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Red Hat Enterprise Linux Server release 7.6 (Maipo)`
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): `v2.3.0-rc1-15-gbb3c460114 2.3.0-rc2`
- Python version: `3.7.5`
- CUDA/cuDNN version: `10.1.1/7.6.5.32`
- GPU model and memory: `Tesla V100-PCIE-16GB`

**Describe the current behavior**

TensorFlow crashes:

```
2020-07-18 21:29:32.310794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:41:00.0 name: Tesla V100-PCIE-16GB computeCapability: 7.0
coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s
2020-07-18 21:29:32.310851: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-07-18 21:29:32.310887: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-07-18 21:29:32.310898: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-07-18 21:29:32.310908: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-07-18 21:29:32.310918: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-07-18 21:29:32.310929: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-07-18 21:29:32.310943: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-07-18 21:29:32.311936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-07-18 21:29:32.311989: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-07-18 21:29:33.680712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-18 21:29:33.680781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-07-18 21:29:33.680813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2020-07-18 21:29:33.682225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14757 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:41:00.0, compute capability: 7.0)
2020-07-18 21:29:33.694136: F tensorflow/core/platform/default/env.cc:72] Check failed: ret == 0 (11 vs. 0)Thread creation via pthread_create() failed.
Aborted
```

**Standalone code to reproduce the issue**

Just this:

```
import tensorflow as tf
x = tf.constant([1, 2]) 
```

"
41531,tf.ragged.stack issue,"tf.ragged.stack(input) return tf.Tensor if the first dim of the input is of length 1 and tf.RaggedTensor if not.
This is a problem as those types don't share the same interface and if the length of the input is not know in advance (as is often the case when using ragged tensors) one must have in ""if"" condition inside the graph to check the output type.

```
tf.ragged.stack([[1,2,3]]) => tf.Tensor([[1 2 3]], shape=(1, 3), dtype=int32)
tf.ragged.stack([[1,2,3],[1,2,3]]) =>  <tf.RaggedTensor [[1, 2, 3], [1, 2, 3]]>

```"
41530,Dataset not repeating when ignoring errors and shuffling before indefinite repetition,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.2.0
- Python version: Python 3.x
- CUDA/cuDNN version: not using GPU
- GPU model and memory: not using GPU

**Describe the current behavior**

When using `tf.data.experimental.ignore_errors()`, indefinite repetition does not work if an error occurs during getting the elements to fill the shuffle buffer initially. I think it's a easier to see in code:

```
def assert_greater_0(x):
  tf.debugging.assert_greater(x, tf.convert_to_tensor(0))
  return x

dataset = tf.data.Dataset.from_tensor_slices([1, 2, 0, 3, 4])
dataset = dataset.map(assert_greater_0)
dataset = dataset.shuffle(buffer_size=3)
dataset = dataset.repeat()
dataset = tf.data.experimental.ignore_errors()(dataset)
```

yields a dataset that has 4 elements whereas adjusting the numbers in the initial list slightly

```
dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3, 0, 4])
dataset = dataset.map(assert_greater_0)
dataset = dataset.shuffle(buffer_size=3)
dataset = dataset.repeat()
dataset = tf.data.experimental.ignore_errors()(dataset)
```

yields an infinitely repeating dataset.

More data:
* removing the shuffle produces infinite datasets in both cases.
* specifying the number of repetitions as 2 produces the same result (8 elements) in both cases,

**Describe the expected behavior**

I would expect that the two code snippets both produce infinite datasets.

**Standalone code to reproduce the issue**

https://colab.research.google.com/drive/1quYzXz3inEiG6D7tfUhoGjapZbO8-AAA?usp=sharing"
41528,No gradients provided for any variable. TF 2.1.0,"I am trying to build an implicit quantile network. I build a custom loss function but do not get it working. I get the error 'no gradients available' but I belief I only use functions that should provide gradients, like tf.tile and stuff. I dont explicityly cast something in my loss_kv_iq() function.

Below I provide the code for my custom layer ( IQNlayer ) , the network I use (IQN),  and my custom loss function. Also a small piece of code in the __main__  that should be able to reproduce the error. 

TF version: 2.1.0



```
import tensorflow as tf
import tensorflow.keras as keras
import numpy as np
  
class IQN(keras.Model):
    def __init__(self, quantile_dims, fc_dims, n_actions, n_quantiles):
        super(IQN, self).__init__()
        self.n_quantiles = n_quantiles
                
        initializer = keras.initializers.he_uniform()
    
        self.iq = IQNlayer(quantile_dims, n_quantiles)
        self.dense = keras.layers.Dense(fc_dims, activation='relu', kernel_initializer = initializer)
        self.out = keras.layers.Dense(n_actions, activation = None)
    
    def call(self, state, tau):
        batch_size, state_size = state.shape
        
        x = self.iq(state, tau)
        x = self.dense(x)
        x = self.out(x)
        
        x = tf.transpose(tf.split(x, batch_size, axis=0), perm=[0, 2, 1])
        return x
    
      
class IQNlayer(keras.layers.Layer):
    def __init__(self, quantile_dims, n_quantiles):
        super(IQNlayer, self).__init__()
        self.quantile_dims = quantile_dims
        self.n_quantiles = n_quantiles
        
        self.fc1 = keras.layers.Dense(self.quantile_dims, activation = tf.nn.selu)
        self.fc2 = keras.layers.Dense(self.quantile_dims, activation = tf.nn.relu)
        
    def call(self, state, tau):
        batch_size, state_size = state.shape
        
        state_tile = tf.tile(state, [1, self.n_quantiles])
        state_reshape = tf.reshape(state_tile, [-1, state_size])
        state_net = self.fc1(state_reshape)
        
        tau = tf.reshape(tau, [-1, 1])
        pi_mtx = tf.constant(np.expand_dims(np.pi * np.arange(0, 64), axis=0), dtype=tf.float32)
        cos_tau = tf.cos(tf.matmul(tau, pi_mtx))
        phi = self.fc2(cos_tau)
        
        net = tf.multiply(state_net, phi)
        return net
    

def loss_kv_iq(x, tau, action_hot, theta_target):
    expand_dim_action = tf.expand_dims(action_hot, -1)
    main_support = tf.reduce_sum(x * expand_dim_action, axis=1)

    theta_loss_tile = tf.tile(tf.expand_dims(main_support, axis=2), [1, 1, N_QUANTILES])
    logit_valid_tile = tf.tile(tf.expand_dims(theta_target, axis=1), [1, N_QUANTILES, 1])
    Huber_loss = hloss(logit_valid_tile, theta_loss_tile)
    
    inv_tau = 1 - tau
    tau = tf.tile(tf.expand_dims(tau, axis=1), [1, N_QUANTILES, 1])
    inv_tau = tf.tile(tf.expand_dims(inv_tau, axis=1), [1, N_QUANTILES, 1])
    error_loss = logit_valid_tile - theta_loss_tile

    Loss = tf.where(tf.less(error_loss, 0.0), inv_tau * Huber_loss, tau * Huber_loss)
    loss = tf.reduce_mean(tf.reduce_sum(tf.reduce_mean(Loss, axis=2), axis=1))
    return loss
        
if __name__ == '__main__':
    hloss = tf.keras.losses.Huber(reduction = tf.keras.losses.Reduction.NONE)
    
    N_QUANTILES = 10
    BATCH_SIZE = 2
    ACTION_SIZE = 5
    STATE_SIZE = 16
    
    # FOR EXAMPLE: RANDOM BATCH
    cs = np.random.rand(BATCH_SIZE,STATE_SIZE)
    a = np.random.randint(0,5,size=(2))
    r = np.random.randint(0,500,size=(2))
    ns = np.random.rand(BATCH_SIZE,STATE_SIZE)
    
    tau = np.random.uniform(size=(BATCH_SIZE, N_QUANTILES))
    tau = tau.astype('float32')    
    iq = IQN(128,128,ACTION_SIZE,N_QUANTILES)
    
    action_hot = np.zeros((BATCH_SIZE,ACTION_SIZE), dtype = np.float32)
    action_hot[np.arange(BATCH_SIZE), a] = 1
    
    Q = iq(ns, tau)
    theta_target = np.random.rand(BATCH_SIZE,N_QUANTILES)
    theta_target = theta_target.astype('float32')
    
    optimizer = tf.keras.optimizers.Adam(lr = 1e-3)
    
    with tf.GradientTape() as tape:
        loss = loss_kv_iq(Q, tau, action_hot, theta_target)
        grads = tape.gradient(loss, iq.trainable_weights)
        optimizer.apply_gradients(zip(grads,iq.trainable_weights))
```

Error:

```
Traceback (most recent call last):

  File ""C:\Users\rensj\.spyder-py3\Thesis\test.py"", line 106, in <module>
    optimizer.apply_gradients(zip(grads,iq.trainable_weights))

  File ""C:\Users\rensj\Anaconda3\envs\tfnew\lib\site-packages\tensorflow_core\python\keras\optimizer_v2\optimizer_v2.py"", line 426, in apply_gradients
    grads_and_vars = _filter_grads(grads_and_vars)

  File ""C:\Users\rensj\Anaconda3\envs\tfnew\lib\site-packages\tensorflow_core\python\keras\optimizer_v2\optimizer_v2.py"", line 1039, in _filter_grads
    ([v.name for _, v in grads_and_vars],))

ValueError: No gradients provided for any variable: ['iqn_4/iq_nlayer_4/dense_16/kernel:0', 'iqn_4/iq_nlayer_4/dense_16/bias:0', 'iqn_4/iq_nlayer_4/dense_17/kernel:0', 'iqn_4/iq_nlayer_4/dense_17/bias:0', 'iqn_4/dense_18/kernel:0', 'iqn_4/dense_18/bias:0', 'iqn_4/dense_19/kernel:0', 'iqn_4/dense_19/bias:0'].
```

EDIT: Someone pointed out that I used numpy operation in the following line:
```
 pi_mtx = tf.constant(np.expand_dims(np.pi * np.arange(0, 64), axis=0), dtype=tf.float32)
```

I changed this line to 
```
pi_mtx = tf.constant(tf.expand_dims(tf.constant(np.pi) * tf.range(0, 64, dtype=tf.float32), axis=0), dtype=tf.float32)
   ```
But keep getting the same error

--
CORRELATED TO:
https://github.com/tensorflow/tensorflow/issues/1511"
41527,Export frozen inference graph with custom eager mode in tf 2.0,"hello, I am training my custom object detection API with eager mode which is totally working fine but now I want to convert this eager mode model into a frozen inference graph so i can use it in opencv can u guyz help me out with this. I tried the traditional approach for exporting frozen graph but this won't work in our case
here's the link of colab notebook you can try this by your own too
https://colab.research.google.com/github/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tf2_colab.ipynb#scrollTo=YBD6l-E4N71y"
41525,Suggestions For Tensorflow v3,"1.  Rebuild from scratch.  At this point there are too many errors, warnings, and old code to do anything about.  It might be an idea to streamline Tensorflow, by creating a new version with only the most important features, and then adding new ones.

2.  Make updates compatible with previous versions.  Do not change syntax used in previous versions.  For instance, any repository written for Tensorflow 3.2 should be completely compatible with Tensorflow 3.8 .  Currently, if a repository is written in Tensorflow and the version is not specifically mentioned by the repo's author, it is anyone's guess as to which specific version of Tensorflow 1 or 2 will work.  A repo written in Tensorflow 1.4 is completely incompatible with Tensorflow 1.5.  Can you imagine if Microsoft Word worked like this?  For every new version update, all your documents are now incompatible and have to be re-written?

3.  Fewer warnings.  When starting up Tensorflow, a full page of warnings appear.  I would be nice to not have these.

4.  Install correct version of Protoc automatically.  This will avoid the common error ""ImportError: DLL load failed"".

5.  If possible, automatically install the correct versions of Cuda and CUDNN.  The process can be long and fairly difficult.  Even better if this can be done inside a virtual environment so it does not affect the host system's settings.

6.  If tensorflow version is incorrect for the repository, give a hint as to which tensorflow version to install instead, and which commands to enter for that purpose.

7.  No more guessing games: more helpful error messages.  Take the most common stackoverflow errors with tensorflow, and add the solutions to the error messages received from Tensorflow.

8.  Coordinate more closely with Python.org, Nvidia, and other dependencies to ensure compatbility with new versions of Tensorflow.

9.  Make installation process on computers easier and more automatic.  ""pip install tensorflow"" should not install most recent version of Tensorflow, but should install the specific version of Tensorflow compatible with that particular distribution of Python, and compatible with that repository.  This could be accomplished through point #2, making all subsequent versions of Tensorflow compatible with previous versions.  To illustrate, a repository coded in Python 3.7 using Tensorflow 1.4, will not be able to run on Tensorflow 1.5.  It would be wonderful if any repository made in Tensorflow 3 could run on any version of Tensorflow 3: this could be possible by keeping the syntax and commands the same across all versions of Tensorflow 3, and adding features if necessary as the versions go on.

10.  I know you all are geniuses.  But the rest of us need a little help.  It might be an idea to simplify things as much as possible, and make running and installing Tensorflow so easy a child could do it.  Pytorch has been gaining traction over Tensorflow because it is just easier to use.  One helpful reddit thread: https://www.reddit.com/r/MachineLearning/comments/bo0nxh/d_what_are_you_using_tensorflow_vs_pytorch/"
41524,"data input into model.fit() for multiple dense(2,)  layers ","```
def func(img_batch, lb_batch):
  lbs = tf.one_hot(lb_batch,depth=2)
  return img_batch, lbs

train_Data = train_ds.map(func)

model.fit(train_Data,steps_per_epoch=400,validation_steps=40,
                    epochs=50,verbose=1))
```

Error:
The model is expecting a 2 separate arrays , but the custom funtion return single array with shape (2,2).

Here the lbs in func is retrun single array with shape (2,2), how can i make the lb into seperate array with each having shape (1,2)?"
41523,Train with multi-gpu with MirroredStrategy for dataset issue,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:no
- TensorFlow installed from (source or binary):source
- TensorFlow version (use command below):2.2.0
- Python version:3.7
- Bazel version (if compiling from source):no
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuda 10.2 and cuDNN 7.6.5
- GPU model and memory: Tesla T4 15109MB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

v2.2.0-rc4-8-g2b96f3662b 2.2.0
GPU details:

[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),
 PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'),
 PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU'),
 PhysicalDevice(name='/physical_device:XLA_GPU:1', device_type='XLA_GPU'),
 PhysicalDevice(name='/physical_device:XLA_GPU:2', device_type='XLA_GPU'),
 PhysicalDevice(name='/physical_device:XLA_GPU:3', device_type='XLA_GPU'),
 PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),
 PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'),
 PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'),
 PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]

**Describe the current behavior**
I followed the blog for the distributed training with multi gpu.When we train with single gpu this works but with multi gpu i have added mirrored strategy but it throw the error.

**Describe the expected behavior**
Followed this while dev but gpus are not getting used 
https://www.tensorflow.org/tutorials/distribute/multi_worker_with_estimator#train_and_evaluate_the_model

**Standalone code to reproduce the issue**
Notebook and custom transformers are available here:


**Other info / logs** Include any logs or source code that would be helpful to
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-18-285f5b632916> in <module>
      2 eval_spec = tf.estimator.EvalSpec(input_fn = eval_input_fn)
      3 
----> 4 tf.estimator.train_and_evaluate(estimator, train_spec=train_spec, eval_spec=eval_spec)

/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py in train_and_evaluate(estimator, train_spec, eval_spec)
    470         '(with task id 0).  Given task id {}'.format(config.task_id))
    471 
--> 472   return executor.run()
    473 
    474 

/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py in run(self)
    611       tf.compat.v1.logging.info(
    612           'Running training and evaluation locally (non-distributed).')
--> 613       return self.run_local()
    614 
    615     # Distributed case.

/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py in run_local(self)
    712         max_steps=self._train_spec.max_steps,
    713         hooks=train_hooks,
--> 714         saving_listeners=saving_listeners)
    715 
    716     eval_result = listener_for_eval.eval_result or _EvalResult(

/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)
    347 
    348       saving_listeners = _check_listeners_type(saving_listeners)
--> 349       loss = self._train_model(input_fn, hooks, saving_listeners)
    350       logging.info('Loss for final step: %s.', loss)
    351       return self

/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)
   1178   def _train_model(self, input_fn, hooks, saving_listeners):
   1179     if self._train_distribution:
-> 1180       return self._train_model_distributed(input_fn, hooks, saving_listeners)
   1181     else:
   1182       return self._train_model_default(input_fn, hooks, saving_listeners)

/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model_distributed(self, input_fn, hooks, saving_listeners)
   1240       self._config._train_distribute.configure(self._config.session_config)
   1241       return self._actual_train_model_distributed(
-> 1242           self._config._train_distribute, input_fn, hooks, saving_listeners)
   1243     # pylint: enable=protected-access
   1244 

/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py in _actual_train_model_distributed(self, strategy, input_fn, hooks, saving_listeners)
   1324                   labels,  # although this will be None it seems
   1325                   ModeKeys.TRAIN,
-> 1326                   self.config))
   1327           loss = strategy.reduce(
   1328               _get_loss_reduce_op_for_reporting(),

/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py in call_for_each_replica(self, fn, args, kwargs)
   2288       kwargs = {}
   2289     with self._container_strategy().scope():
-> 2290       return self._call_for_each_replica(fn, args, kwargs)
   2291 
   2292   def _call_for_each_replica(self, fn, args, kwargs):

/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py in _call_for_each_replica(self, fn, args, kwargs)
    768 
    769     return _call_for_each_replica(self._container_strategy(), self._devices,
--> 770                                   fn, args, kwargs)
    771 
    772   def _configure(self,

/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py in _call_for_each_replica(distribution, devices, fn, args, kwargs)
    199     for t in threads:
    200       t.should_run.set()
--> 201     coord.join(threads)
    202 
    203   return values.regroup(tuple(t.main_result for t in threads))

/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/training/coordinator.py in join(self, threads, stop_grace_period_secs, ignore_live_threads)
    387       self._registered_threads = set()
    388       if self._exc_info_to_raise:
--> 389         six.reraise(*self._exc_info_to_raise)
    390       elif stragglers:
    391         if ignore_live_threads:

/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/six.py in reraise(tp, value, tb)
    691             if value.__traceback__ is not tb:
    692                 raise value.with_traceback(tb)
--> 693             raise value
    694         finally:
    695             value = None

/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/training/coordinator.py in stop_on_exception(self)
    295     """"""
    296     try:
--> 297       yield
    298     except:  # pylint: disable=bare-except
    299       self.request_stop(ex=sys.exc_info())

/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py in run(self)
    996               self._var_scope, reuse=self.replica_id > 0), \
    997           variable_scope.variable_creator_scope(self.variable_creator_fn):
--> 998         self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)
    999         self.done = True
   1000     finally:

/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)
    263       except Exception as e:  # pylint:disable=broad-except
    264         if hasattr(e, 'ag_error_metadata'):
--> 265           raise e.ag_error_metadata.to_exception(e)
    266         else:
    267           raise

TypeError: in user code:

    /usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py:1170 _call_model_fn  *
        model_fn_results = self._model_fn(features=features, **kwargs)
    /media/ephemeral0/combined_estimator.py:39 model_fn  *
        if spec.train_op:
    /usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/autograph/operators/control_flow.py:924 if_stmt
        basic_symbol_names, composite_symbol_names)
    /usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/autograph/operators/control_flow.py:962 tf_if_stmt
        error_checking_orelse)
    /usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:507 new_func
        return func(*args, **kwargs)
    /usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py:1177 cond
        return cond_v2.cond_v2(pred, true_fn, false_fn, name)
    /usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/ops/cond_v2.py:91 cond_v2
        op_return_value=pred)
    /usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py:981 func_graph_from_py_func
        func_outputs = python_func(*func_args, **func_kwargs)
    /usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/autograph/operators/control_flow.py:958 error_checking_orelse
        basic_symbol_names + composite_symbol_names)
    /usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/autograph/operators/control_flow.py:295 _verify_tf_cond_vars
        ' branches.\n\n{}'.format(name, str(e)))

    TypeError: ""train_op_list"" does not have the same nested structure in the TRUE and FALSE branches.
    
    The two structures don't have the same nested structure.
    
    First structure: type=list str=[<tf.Tensor 'Adam/Identity:0' shape=() dtype=int64>]
    
    Second structure: type=NoneType str=None
    
    More specifically: Substructure ""type=list str=[<tf.Tensor 'Adam/Identity:0' shape=() dtype=int64>]"" is a sequence, while substructure ""type=NoneType str=None"" is not
    Entire first structure:
    [.]
    Entire second structure:
    .
"
41521,Model titled with int8 is not quantized(Artistic Style Transfer with TensorFlow Lite),"I am trying to compile the model [Style transform model (int8)](https://tfhub.dev/google/lite-model/magenta/arbitrary-image-stylization-v1-256/int8/transfer/1?lite-format=tflite) to edgetpu version, but the error message say that model is not quantized.
Can anybody tell me how to fix it?


I have downloaded 2 int8 models at the bottom of https://www.tensorflow.org/lite/models/style_transfer/overview
Style prediction model (int8) : https://tfhub.dev/google/lite-model/magenta/arbitrary-image-stylization-v1-256/int8/prediction/1?lite-format=tflite
Style transform model (int8) : https://tfhub.dev/google/lite-model/magenta/arbitrary-image-stylization-v1-256/int8/transfer/1?lite-format=tflite

I try to convert these 2 model to _edgetpu version by typing following commands in my terminal.
>edgetpu_compiler magenta_arbitrary-image-stylization-v1-256_int8_prediction_1.tflite
>edgetpu_compiler magenta_arbitrary-image-stylization-v1-256_int8_transfer_1.tflite

The compile for first file : *_prediction_1.tflite is OK.
However, the compile for  *_transfer_1.tflite reports error:
**********************************************************************
Edge TPU Compiler version 14.1.317412892
Invalid model: magenta_arbitrary-image-stylization-v1-256_int8_transfer_1.tflite
Model not quantized
**********************************************************************"
41520,Generators not compatible with ragged tensors when using model.fit,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 2.2
- Python version: 3.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: cuDNN 10
- GPU model and memory: V100, 24GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

When providing model.fit with a python generator that creates ragged inputs for a keras model, while the outputs of the generator will allow model.fit to train, if one provides the generator to model.fit, it will not. Below, I have attached a standalone script showing in synthetic data, how when the generator is incorporated into a for loop, the model will fit on the outputs of the generator but if one provides the generator to the model.fit command, it will not train.

**Describe the expected behavior**

The model should train given the python generator according to the docs.

**Standalone code to reproduce the issue**

```python
import tensorflow as tf
import numpy as np

def turn_into_ragged(x):
    return tf.RaggedTensor.from_row_lengths(tf.concat(
        [tf.RaggedTensor.from_row_lengths(np.concatenate(sample, axis=0), list(map(len, sample))) for sample in x],
        axis=0), list(map(len, x)))

def get_batches_m(x,y,batch_size=10,random=False):
    """""" Return a generator that yields batches from vars. """"""
    #batch_size = len(x) // n_batches
    if len(x[0]) % batch_size == 0:
        n_batches = (len(x[0]) // batch_size)
    else:
        n_batches = (len(x[0]) // batch_size) + 1

    sel = np.asarray(list(range(x[0].shape[0])))
    if random is True:
        np.random.shuffle(sel)

    for ii in range(0, n_batches * batch_size, batch_size):
        # If we're not on the last batch, grab data with size batch_size
        if ii != (n_batches - 1) * batch_size:
            sel_ind=sel[ii: ii + batch_size]
        else:
            sel_ind = sel[ii:]

        x_out = [turn_into_ragged(var[sel_ind]) for var in x]
        y_out = [var[sel_ind] for var in y]

        yield tuple(x_out),tuple(y_out)

def generate_syn_data(n_i=2000, n_s=30, n_t=200, shape=(10, 10, 3)):
    values = np.random.uniform(0, 1, (n_i, ) + shape).astype(np.float32)
    idx_t = np.random.choice(n_t, n_i)
    _, l = np.unique(idx_t, return_counts=True)
    rt0 = tf.RaggedTensor.from_row_lengths(values, l)
    idx_s = np.random.choice(n_s, n_t)
    _, l = np.unique(idx_s, return_counts=True)
    rt1 = tf.RaggedTensor.from_row_lengths(rt0, l)
    y = tf.constant(np.eye(2)[np.random.choice(2, n_s)].astype(np.float32))
    return  rt1, y

def basic_ragged_graph(input_shapes):
    ragged_inputs = [tf.keras.layers.Input(shape=(None, None) + shape, dtype=tf.float32, ragged=True) for shape in input_shapes]
    sample_aggregation = tf.concat([tf.keras.layers.Lambda(lambda x: tf.reduce_sum(x, axis=(1, 2)))(ragged_input) for ragged_input in ragged_inputs], axis=1)
    logits = tf.keras.layers.Dense(units=2, activation=None)(tf.keras.layers.Flatten()(sample_aggregation))
    return tf.keras.Model(inputs=ragged_inputs, outputs=[logits])

#create simple model that takes 2 ragged inputs and returns 1 output
tile_shape = (10, 10, 3)
model = basic_ragged_graph([tile_shape,tile_shape])
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True))

#generate synthetic data for inputs and outputs
x1,x2 = generate_syn_data()[0].numpy(),generate_syn_data()[0].numpy()
y = generate_syn_data()[1].numpy()

#create generator objeect to batch and convert data to tf raggged
train_gen = get_batches_m([x1,x2],[y],batch_size=5,random=True)
#this method uses generator and outputs x_train,y_train that work
for x_train, y_train in train_gen:
    model.fit(x_train, y_train)

#however, when one provides this generator to the model.fit, the model will not train
train_gen = get_batches_m([x1,x2],[y],batch_size=5,random=True)
model.fit(train_gen)
```


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
41519,Any way to specific op name when I use saved_model,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):2.2
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
The saved model will override the op name which I specific in graph
**Describe the expected behavior**
Saved model save the op names that I specific
**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.\

https://colab.research.google.com/drive/16kZkauTJwAkUZTsM4UoxtyHz2TuV9rXe

The input op name should be 'input'
The output op name should be 'test'

Current implementation:
input op: serving_default_input
output op: PartitionedCall:0 PartitionedCall:1
**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
41518,Restoring  trained ResNet50v2 model excluding last layer,"I have posted the question on stack overflow[Link](https://stackoverflow.com/questions/62946756/restoring- #tensorflow-resnet50v2-model-without-last-layer) 

I am very confused about the documentation as I did not find how do I load the checkpoint of one model to different model."
41513,"tf 2.2.0 achieves much worse results than tf 2.1.0, same model","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
41512,Tensorflow,"When I try using keras, says that I dont have tensorflow 2.2.0 installed, but i do. Can someone help me? I already uninstalled tensorflow and the same thing happens. Also I reinstalled Anaconda and nothing.


from keras.models import Sequential
Traceback (most recent call last):

  File ""C:\Users\luiz_\anaconda3\lib\site-packages\keras\__init__.py"", line 3, in <module>
    from tensorflow.keras.layers.experimental.preprocessing import RandomRotation

  File ""C:\Users\luiz_\AppData\Roaming\Python\Python37\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util

  File ""C:\Users\luiz_\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\__init__.py"", line 40, in <module>
    from tensorflow.python.eager import context

  File ""C:\Users\luiz_\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\eager\context.py"", line 35, in <module>
    from tensorflow.python import pywrap_tfe

  File ""C:\Users\luiz_\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\pywrap_tfe.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow

ImportError: cannot import name 'pywrap_tensorflow' from 'tensorflow.python' (C:\Users\luiz_\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\__init__.py)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):

  File ""<ipython-input-1-9c5e0a19b646>"", line 1, in <module>
    from keras.models import Sequential

  File ""C:\Users\luiz_\anaconda3\lib\site-packages\keras\__init__.py"", line 6, in <module>
    'Keras requires TensorFlow 2.2 or higher. '

ImportError: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`"
41510,"ValueError: Tape is still recording, This can happen if you try to re-enter an already-active tape.","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): 
_Yes_
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
  _dockerhub container 'latest'  Digest:  08901711826b185136886c7b8271b9fdbe86b8ccb598669781a1f5cb340184eb_
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
   _v2.2.0-rc4-8-g2b96f3662b 2.2.0_
- Python version:
   _Python 3.6.9_
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the current behavior**
on line `grads = tape.jacobian(w, [x, y])`
`ValueError: Tape is still recording, This can happen if you try to re-enter an already-active tape.`

**Describe the expected behavior**

I was expecting to get the jacobian calculation:
`grads == [dwdx, dwdy]`
or similar (not the error)

**Standalone code to reproduce the issue**

```
        import tensorflow as tf

        # x,y,v potentially a large batch
        x = tf.constant([[1.0],[5.0]])
        y = tf.constant([[2.0],[6.0]])
        v = tf.Variable([[1.0],[3.0]])

        with tf.GradientTape(persistent=True) as tape:
            tape.watch(x)
            tape.watch(y)
            tape.watch(v)

            # w1 and w2 a nonlinear function independent for each row
            w1 = (x + y + v )
            w2 = y * y * v
            w = tf.concat([w1, w2], axis=1)

            grads = tape.jacobian(w, [x, y])

        dgradsdv = tape.jacobian(grads, v)

        print(dgradsdv)

```
**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Log
```
    grads = tape.jacobian(w, [x, y])
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py"", line 1113, in jacobian
    self._push_tape()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py"", line 844, in _push_tape
    raise ValueError(""Tape is still recording, This can happen if you try to ""
ValueError: Tape is still recording, This can happen if you try to re-enter an already-active tape.
```
If I substitute `tape.jacobian` by `tape.gradient` there is no error.
I would like to know how to implement these Jacobians efficiently.  Thanks!"
41507,GV100 Tensorflow 2.0.0 not working,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
41506,"C++ TensorFlow Build doesn't find TensorFlow .h files that exist.  Also ""Please update your includePath""","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04 LTS Oracle VM Virtualbox inside Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): Binary https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-1.15.0.tar.gz
- TensorFlow version: 1.15.0
- Python version: Actually I'm using the C++ 0.29.0 extension in Visual Studio Code 1.47.1, but my Python version is Python 3.8.2
- Installed using virtualenv? pip? conda?: No installed from TensorFlow C++ binary libtensorflow-cpu-linux-x86_64-1.15.0.tar.gz
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a?
- GPU model and memory: NVIDIA GeForce GTS 965M with 2,048 MB GDDR5 memory.  32 GB RAM.



**Describe the problem**

Fails to compile in C++ in Visual Studio Code test program, testing for correct installation of TensorFlow, because of errors on two TensorFlow #include.  See the two compile errors below and the three Visual Studio Code warnings after those, which give more information.

I have to use TensorFlow for C++ NOT TensorFlow for Python because I am trying to do a C++ online course project capstone where I have decided to use TensorFlow to do a convolutional neural network to classify dog and cat pictures.  I cannot use Python for that because it is a C++ course.

When I use #include <tensorflow/c/c_api.h>
```
tlroot@tlroot-VirtualBox:~/Documents/C++/Capstone/CppND-Capstone-Hello-World/src$ g++ test.cpp
test.cpp:2:10: fatal error: tensorflow/c/c_api.h: No such file or directory
    2 | #include <tensorflow/c/c_api.h>
      |          ^~~~~~~~~~~~~~~~~~~~~~
compilation terminated.
tlroot@tlroot-VirtualBox:~/Documents/C++/Capstone/CppND-Capstone-Hello-World/src$ 
```
When I use #include ""/usr/local/lib/libtensorflow-cpu-linux-x86_64-1.15.0/include/tensorflow/c/c_api.h""
```
tlroot@tlroot-VirtualBox:~/Documents/C++/Capstone/CppND-Capstone-Hello-World/src$ g++ test.cpp
In file included from test.cpp:2:
/usr/local/lib/libtensorflow-cpu-linux-x86_64-1.15.0/include/tensorflow/c/c_api.h:22:10: fatal error: tensorflow/c/tf_attrtype.h: No such file or directory
   22 | #include ""tensorflow/c/tf_attrtype.h""
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.
tlroot@tlroot-VirtualBox:~/Documents/C++/Capstone/CppND-Capstone-Hello-World/src$
```

I installed TensorFlow for C++ on Ubuntu 20.04 LTS using these instructions: https://www.tensorflow.org/install/lang_c

Below are the three warnings I get from Visual Studio Code before compiling, which give more information than the compile error does:
```
#include <tensorflow/c/c_api.h>
```
WARNING for the above code: cannot open source file ""tensorflow/c/c_api.h""C/C++(1696)

```
#include ""/usr/local/lib/libtensorflow-cpu-linux-x86_64-1.15.0/include/tensorflow/c/c_api.h""
```
WARNING for the above code: #include errors detected. Please update your includePath. Squiggles are disabled for this translation unit (/home/tlroot/Documents/C++/Capstone/CppND-Capstone-Hello-World/src/test.cpp).C/C++(1696)

WARNING for the above code: cannot open source file ""tensorflow/c/tf_attrtype.h"" (dependency of ""/usr/local/lib/libtensorflow-cpu-linux-x86_64-1.15.0/include/tensorflow/c/c_api.h"")C/C++(1696)

Here is the build example program from https://www.tensorflow.org/install/lang_c that tests if the TensorFlow install works:
```
#include <stdio.h>
#include <tensorflow/c/c_api.h>
#include ""/usr/local/lib/libtensorflow-cpu-linux-x86_64-1.15.0/include/tensorflow/c/c_api.h""

int main() {
  printf(""Hello from TensorFlow C library version %s\n"", TF_Version());
  return 0;
}
```
Note: #include <stdio.h> WORKS but neither TensorFlow #include works.  I have run C++ projects in this setup and those programs worked.  See an example: https://github.com/ProfHariSeldon/CppND-System-Monitor.  So I know the C++ works what isn't working is installing TensorFlow.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Follow instructions to install C++ TensorFlow from: https://www.tensorflow.org/install/lang_c.  See below steps:

1. Download https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-1.15.0.tar.gz

2. Untar libtensorflow-cpu-linux-x86_64-1.15.0.tar.gz in cd /usr/local/lib/

3. cd /usr/local/lib/libtensorflow-cpu-linux-x86_64-1.15.0

4. sudo ldconfig
No error messages for this, but I opened Visual Studio Code and ""sudo ldconfig"" did not fix the #include errors

5. cd cd /usr/local/lib/

6. sudo ldconfig
No error messages for this, but I opened Visual Studio Code and ""sudo ldconfig"" did not fix the #include errors

7. Since ""sudo ldconfig"" did not work I tried the other way the installation instructions (https://www.tensorflow.org/install/lang_c) recommended:
```
tlroot@tlroot-VirtualBox:~$ export LIBRARY_PATH=$LIBRARY_PATH:/usr/local/lib/libtensorflow-cpu-linux-x86_64-1.15.0/lib
tlroot@tlroot-VirtualBox:~$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib/libtensorflow-cpu-linux-x86_64-1.15.0/lib
tlroot@tlroot-VirtualBox:~$ echo $LIBRARY_PATH
:/usr/local/lib/libtensorflow-cpu-linux-x86_64-1.15.0/lib
tlroot@tlroot-VirtualBox:~$ echo $LD_LIBRARY_PATH
:/usr/local/lib/libtensorflow-cpu-linux-x86_64-1.15.0/lib
tlroot@tlroot-VirtualBox:~$
```
No error messages for this, but I opened Visual Studio Code and those two export did not fix the #include errors

I also noticed that the LIBRARY_PATH and LD_LIBRARY_PATH are forgotten when I reboot my Ubuntu 20.04 LTS Oracle VM Virtualbox.
```
tlroot@tlroot-VirtualBox:~$ echo $LIBRARY_PATH
tlroot@tlroot-VirtualBox:~$ echo $LD_LIBRARY_PATH
tlroot@tlroot-VirtualBox:~$
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Here is the example program I am using to test my TensorFlow C++ installation in Visual Studio Code:
```
#include <stdio.h>
#include <tensorflow/c/c_api.h>
#include ""/usr/local/lib/libtensorflow-cpu-linux-x86_64-1.15.0/include/tensorflow/c/c_api.h""

int main() {
  printf(""Hello from TensorFlow C library version %s\n"", TF_Version());
  return 0;
}
```
Note: #include <stdio.h> WORKS but neither TensorFlow #include works.  I have run C++ projects in this setup and those programs worked.  See an example: https://github.com/ProfHariSeldon/CppND-System-Monitor.  So I know the C++ works what isn't working is installing TensorFlow.

These both exist so why can't the compiler find them?:
/usr/local/lib/libtensorflow-cpu-linux-x86_64-1.15.0/include/tensorflow/c/tf_attrtype.h
/usr/local/lib/libtensorflow-cpu-linux-x86_64-1.15.0/include/tensorflow/c/c_api.h

See below:

/usr/local/lib/libtensorflow-cpu-linux-x86_64-1.15.0/include/tensorflow/c/c_api.h exists:
```
/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/

#ifndef TENSORFLOW_C_C_API_H_
#define TENSORFLOW_C_C_API_H_

#include <stddef.h>
#include <stdint.h>

#include ""tensorflow/c/tf_attrtype.h""
#include ""tensorflow/c/tf_datatype.h""
#include ""tensorflow/c/tf_status.h""
#include ""tensorflow/c/tf_tensor.h""

// --------------------------------------------------------------------------
// C API for TensorFlow.
//
// The API leans towards simplicity and uniformity instead of convenience
// since most usage will be by language specific wrappers.
//
// Conventions:
// * We use the prefix TF_ for everything in the API.
// * Objects are always passed around as pointers to opaque structs
//   and these structs are allocated/deallocated via the API.
// * TF_Status holds error information.  It is an object type
//   and therefore is passed around as a pointer to an opaque
//   struct as mentioned above.
// * Every call that has a TF_Status* argument clears it on success
//   and fills it with error info on failure.
// * unsigned char is used for booleans (instead of the 'bool' type).
//   In C++ bool is a keyword while in C99 bool is a macro defined
//   in stdbool.h. It is possible for the two to be inconsistent.
//   For example, neither the C99 nor the C++11 standard force a byte
//   size on the bool type, so the macro defined in stdbool.h could
//   be inconsistent with the bool keyword in C++. Thus, the use
//   of stdbool.h is avoided and unsigned char is used instead.
// * size_t is used to represent byte sizes of objects that are
//   materialized in the address space of the calling process.
// * int is used as an index into arrays.
// * Deletion functions are safe to call on nullptr.
//
// Questions left to address:
// * Might at some point need a way for callers to provide their own Env.
// * Maybe add TF_TensorShape that encapsulates dimension info.
//
// Design decisions made:
// * Backing store for tensor memory has an associated deallocation
//   function.  This deallocation function will point to client code
//   for tensors populated by the client.  So the client can do things
//   like shadowing a numpy array.
// * We do not provide TF_OK since it is not strictly necessary and we
//   are not optimizing for convenience.
// * We make assumption that one session has one graph.  This should be
//   fine since we have the ability to run sub-graphs.
// * We could allow NULL for some arguments (e.g., NULL options arg).
//   However since convenience is not a primary goal, we don't do this.
// * Devices are not in this API.  Instead, they are created/used internally
//   and the API just provides high level controls over the number of
//   devices of each type.

// Macro to control visibility of exported symbols in the shared library (.so,
// .dylib, .dll).
// This duplicates the TF_EXPORT macro definition in
// tensorflow/core/platform/macros.h in order to keep this .h file independent
// of any other includes.
#ifdef SWIG
#define TF_CAPI_EXPORT
#else
#if defined(_WIN32)
#ifdef TF_COMPILE_LIBRARY
#define TF_CAPI_EXPORT __declspec(dllexport)
#else
#define TF_CAPI_EXPORT __declspec(dllimport)
#endif  // TF_COMPILE_LIBRARY
#else
#define TF_CAPI_EXPORT __attribute__((visibility(""default"")))
#endif  // _WIN32
#endif  // SWIG

#ifdef __cplusplus
extern ""C"" {
#endif

// --------------------------------------------------------------------------
// TF_Version returns a string describing version information of the
// TensorFlow library. TensorFlow using semantic versioning.
TF_CAPI_EXPORT extern const char* TF_Version(void);

// --------------------------------------------------------------------------
// TF_Buffer holds a pointer to a block of data and its associated length.
// Typically, the data consists of a serialized protocol buffer, but other data
// may also be held in a buffer.
//
// By default, TF_Buffer itself does not do any memory management of the
// pointed-to block.  If need be, users of this struct should specify how to
// deallocate the block by setting the `data_deallocator` function pointer.
typedef struct TF_Buffer {
  const void* data;
  size_t length;
  void (*data_deallocator)(void* data, size_t length);
} TF_Buffer;

// Makes a copy of the input and sets an appropriate deallocator.  Useful for
// passing in read-only, input protobufs.
TF_CAPI_EXPORT extern TF_Buffer* TF_NewBufferFromString(const void* proto,
                                                        size_t proto_len);

// Useful for passing *out* a protobuf.
TF_CAPI_EXPORT extern TF_Buffer* TF_NewBuffer(void);

TF_CAPI_EXPORT extern void TF_DeleteBuffer(TF_Buffer*);

TF_CAPI_EXPORT extern TF_Buffer TF_GetBuffer(TF_Buffer* buffer);

// --------------------------------------------------------------------------
// TF_SessionOptions holds options that can be passed during session creation.
typedef struct TF_SessionOptions TF_SessionOptions;

// Return a new options object.
TF_CAPI_EXPORT extern TF_SessionOptions* TF_NewSessionOptions(void);

// Set the target in TF_SessionOptions.options.
// target can be empty, a single entry, or a comma separated list of entries.
// Each entry is in one of the following formats :
// ""local""
// ip:port
// host:port
TF_CAPI_EXPORT extern void TF_SetTarget(TF_SessionOptions* options,
                                        const char* target);

// Set the config in TF_SessionOptions.options.
// config should be a serialized tensorflow.ConfigProto proto.
// If config was not parsed successfully as a ConfigProto, record the
// error information in *status.
TF_CAPI_EXPORT extern void TF_SetConfig(TF_SessionOptions* options,
                                        const void* proto, size_t proto_len,
                                        TF_Status* status);

// Destroy an options object.
TF_CAPI_EXPORT extern void TF_DeleteSessionOptions(TF_SessionOptions*);

// TODO(jeff,sanjay):
// - export functions to set Config fields

// --------------------------------------------------------------------------
// The new graph construction API, still under development.

// Represents a computation graph.  Graphs may be shared between sessions.
// Graphs are thread-safe when used as directed below.
typedef struct TF_Graph TF_Graph;

// Return a new graph object.
TF_CAPI_EXPORT extern TF_Graph* TF_NewGraph(void);

// Destroy an options object.  Graph will be deleted once no more
// TFSession's are referencing it.
TF_CAPI_EXPORT extern void TF_DeleteGraph(TF_Graph*);

// Operation being built. The underlying graph must outlive this.
typedef struct TF_OperationDescription TF_OperationDescription;

// Operation that has been added to the graph. Valid until the graph is
// deleted -- in particular adding a new operation to the graph does not
// invalidate old TF_Operation* pointers.
typedef struct TF_Operation TF_Operation;

// Represents a specific input of an operation.
typedef struct TF_Input {
  TF_Operation* oper;
  int index;  // The index of the input within oper.
} TF_Input;

// Represents a specific output of an operation.
typedef struct TF_Output {
  TF_Operation* oper;
  int index;  // The index of the output within oper.
} TF_Output;

// TF_Function is a grouping of operations with defined inputs and outputs.
// Once created and added to graphs, functions can be invoked by creating an
// operation whose operation type matches the function name.
typedef struct TF_Function TF_Function;

// Function definition options. TODO(iga): Define and implement
typedef struct TF_FunctionOptions TF_FunctionOptions;

// Sets the shape of the Tensor referenced by `output` in `graph` to
// the shape described by `dims` and `num_dims`.
//
// If the number of dimensions is unknown, `num_dims` must be set to
// -1 and `dims` can be null. If a dimension is unknown, the
// corresponding entry in the `dims` array must be -1.
//
// This does not overwrite the existing shape associated with `output`,
// but merges the input shape with the existing shape.  For example,
// setting a shape of [-1, 2] with an existing shape [2, -1] would set
// a final shape of [2, 2] based on shape merging semantics.
//
// Returns an error into `status` if:
//   * `output` is not in `graph`.
//   * An invalid shape is being set (e.g., the shape being set
//     is incompatible with the existing shape).
TF_CAPI_EXPORT extern void TF_GraphSetTensorShape(TF_Graph* graph,
                                                  TF_Output output,
                                                  const int64_t* dims,
                                                  const int num_dims,
                                                  TF_Status* status);

// Returns the number of dimensions of the Tensor referenced by `output`
// in `graph`.
//
// If the number of dimensions in the shape is unknown, returns -1.
//
// Returns an error into `status` if:
//   * `output` is not in `graph`.
TF_CAPI_EXPORT extern int TF_GraphGetTensorNumDims(TF_Graph* graph,
                                                   TF_Output output,
                                                   TF_Status* status);

// Returns the shape of the Tensor referenced by `output` in `graph`
// into `dims`. `dims` must be an array large enough to hold `num_dims`
// entries (e.g., the return value of TF_GraphGetTensorNumDims).
//
// If the number of dimensions in the shape is unknown or the shape is
// a scalar, `dims` will remain untouched. Otherwise, each element of
// `dims` will be set corresponding to the size of the dimension. An
// unknown dimension is represented by `-1`.
//
// Returns an error into `status` if:
//   * `output` is not in `graph`.
//   * `num_dims` does not match the actual number of dimensions.
TF_CAPI_EXPORT extern void TF_GraphGetTensorShape(TF_Graph* graph,
                                                  TF_Output output,
                                                  int64_t* dims, int num_dims,
                                                  TF_Status* status);

// Operation will only be added to *graph when TF_FinishOperation() is
// called (assuming TF_FinishOperation() does not return an error).
// *graph must not be deleted until after TF_FinishOperation() is
// called.
TF_CAPI_EXPORT extern TF_OperationDescription* TF_NewOperation(
    TF_Graph* graph, const char* op_type, const char* oper_name);

// Specify the device for `desc`.  Defaults to empty, meaning unconstrained.
TF_CAPI_EXPORT extern void TF_SetDevice(TF_OperationDescription* desc,
                                        const char* device);

// The calls to TF_AddInput and TF_AddInputList must match (in number,
// order, and type) the op declaration.  For example, the ""Concat"" op
// has registration:
//   REGISTER_OP(""Concat"")
//       .Input(""concat_dim: int32"")
//       .Input(""values: N * T"")
//       .Output(""output: T"")
//       .Attr(""N: int >= 2"")
//       .Attr(""T: type"");
// that defines two inputs, ""concat_dim"" and ""values"" (in that order).
// You must use TF_AddInput() for the first input (since it takes a
// single tensor), and TF_AddInputList() for the second input (since
// it takes a list, even if you were to pass a list with a single
// tensor), as in:
//   TF_OperationDescription* desc = TF_NewOperation(graph, ""Concat"", ""c"");
//   TF_Output concat_dim_input = {...};
//   TF_AddInput(desc, concat_dim_input);
//   TF_Output values_inputs[5] = {{...}, ..., {...}};
//   TF_AddInputList(desc, values_inputs, 5);

// For inputs that take a single tensor.
TF_CAPI_EXPORT extern void TF_AddInput(TF_OperationDescription* desc,
                                       TF_Output input);

// For inputs that take a list of tensors.
// inputs must point to TF_Output[num_inputs].
TF_CAPI_EXPORT extern void TF_AddInputList(TF_OperationDescription* desc,
                                           const TF_Output* inputs,
                                           int num_inputs);

// Call once per control input to `desc`.
TF_CAPI_EXPORT extern void TF_AddControlInput(TF_OperationDescription* desc,
                                              TF_Operation* input);

// Request that `desc` be co-located on the device where `op`
// is placed.
//
// Use of this is discouraged since the implementation of device placement is
// subject to change. Primarily intended for internal libraries
TF_CAPI_EXPORT extern void TF_ColocateWith(TF_OperationDescription* desc,
                                           TF_Operation* op);

// Call some TF_SetAttr*() function for every attr that is not
// inferred from an input and doesn't have a default value you wish to
// keep.

// `value` must point to a string of length `length` bytes.
TF_CAPI_EXPORT extern void TF_SetAttrString(TF_OperationDescription* desc,
                                            const char* attr_name,
                                            const void* value, size_t length);
// `values` and `lengths` each must have lengths `num_values`.
// `values[i]` must point to a string of length `lengths[i]` bytes.
TF_CAPI_EXPORT extern void TF_SetAttrStringList(TF_OperationDescription* desc,
                                                const char* attr_name,
                                                const void* const* values,
                                                const size_t* lengths,
                                                int num_values);
TF_CAPI_EXPORT extern void TF_SetAttrInt(TF_OperationDescription* desc,
                                         const char* attr_name, int64_t value);
TF_CAPI_EXPORT extern void TF_SetAttrIntList(TF_OperationDescription* desc,
                                             const char* attr_name,
                                             const int64_t* values,
                                             int num_values);
TF_CAPI_EXPORT extern void TF_SetAttrFloat(TF_OperationDescription* desc,
                                           const char* attr_name, float value);
TF_CAPI_EXPORT extern void TF_SetAttrFloatList(TF_OperationDescription* desc,
                                               const char* attr_name,
                                               const float* values,
                                               int num_values);
TF_CAPI_EXPORT extern void TF_SetAttrBool(TF_OperationDescription* desc,
                                          const char* attr_name,
                                          unsigned char value);
TF_CAPI_EXPORT extern void TF_SetAttrBoolList(TF_OperationDescription* desc,
                                              const char* attr_name,
                                              const unsigned char* values,
                                              int num_values);
TF_CAPI_EXPORT extern void TF_SetAttrType(TF_OperationDescription* desc,
                                          const char* attr_name,
                                          TF_DataType value);
TF_CAPI_EXPORT extern void TF_SetAttrTypeList(TF_OperationDescription* desc,
                                              const char* attr_name,
                                              const TF_DataType* values,
                                              int num_values);
TF_CAPI_EXPORT extern void TF_SetAttrPlaceholder(TF_OperationDescription* desc,
                                                 const char* attr_name,
                                                 const char* placeholder);

// Set a 'func' attribute to the specified name.
// `value` must point to a string of length `length` bytes.
TF_CAPI_EXPORT extern void TF_SetAttrFuncName(TF_OperationDescription* desc,
                                              const char* attr_name,
                                              const char* value, size_t length);

// Set `num_dims` to -1 to represent ""unknown rank"".  Otherwise,
// `dims` points to an array of length `num_dims`.  `dims[i]` must be
// >= -1, with -1 meaning ""unknown dimension"".
TF_CAPI_EXPORT extern void TF_SetAttrShape(TF_OperationDescription* desc,
                                           const char* attr_name,
                                           const int64_t* dims, int num_dims);
// `dims` and `num_dims` must point to arrays of length `num_shapes`.
// Set `num_dims[i]` to -1 to represent ""unknown rank"".  Otherwise,
// `dims[i]` points to an array of length `num_dims[i]`.  `dims[i][j]`
// must be >= -1, with -1 meaning ""unknown dimension"".
TF_CAPI_EXPORT extern void TF_SetAttrShapeList(TF_OperationDescription* desc,
                                               const char* attr_name,
                                               const int64_t* const* dims,
                                               const int* num_dims,
                                               int num_shapes);
// `proto` must point to an array of `proto_len` bytes representing a
// binary-serialized TensorShapeProto.
TF_CAPI_EXPORT extern void TF_SetAttrTensorShapeProto(
    TF_OperationDescription* desc, const char* attr_name, const void* proto,
    size_t proto_len, TF_Status* status);
// `protos` and `proto_lens` must point to arrays of length `num_shapes`.
// `protos[i]` must point to an array of `proto_lens[i]` bytes
// representing a binary-serialized TensorShapeProto.
TF_CAPI_EXPORT extern void TF_SetAttrTensorShapeProtoList(
    TF_OperationDescription* desc, const char* attr_name,
    const void* const* protos, const size_t* proto_lens, int num_shapes,
    TF_Status* status);

TF_CAPI_EXPORT extern void TF_SetAttrTensor(TF_OperationDescription* desc,
                                            const char* attr_name,
                                            TF_Tensor* value,
                                            TF_Status* status);
TF_CAPI_EXPORT extern void TF_SetAttrTensorList(TF_OperationDescription* desc,
                                                const char* attr_name,
                                                TF_Tensor* const* values,
                                                int num_values,
                                                TF_Status* status);

// `proto` should point to a sequence of bytes of length `proto_len`
// representing a binary serialization of an AttrValue protocol
// buffer.
TF_CAPI_EXPORT extern void TF_SetAttrValueProto(TF_OperationDescription* desc,
                                                const char* attr_name,
                                                const void* proto,
                                                size_t proto_len,
                                                TF_Status* status);

// If this function succeeds:
//   * *status is set to an OK value,
//   * a TF_Operation is added to the graph,
//   * a non-null value pointing to the added operation is returned --
//     this value is valid until the underlying graph is deleted.
// Otherwise:
//   * *status is set to a non-OK value,
//   * the graph is not modified,
//   * a null value is returned.
// In either case, it deletes `desc`.
TF_CAPI_EXPORT extern TF_Operation* TF_FinishOperation(
    TF_OperationDescription* desc, TF_Status* status);

// TF_Operation functions.  Operations are immutable once created, so
// these are all query functions.

TF_CAPI_EXPORT extern const char* TF_OperationName(TF_Operation* oper);
TF_CAPI_EXPORT extern const char* TF_OperationOpType(TF_Operation* oper);
TF_CAPI_EXPORT extern const char* TF_OperationDevice(TF_Operation* oper);

TF_CAPI_EXPORT extern int TF_OperationNumOutputs(TF_Operation* oper);
TF_CAPI_EXPORT extern TF_DataType TF_OperationOutputType(TF_Output oper_out);
TF_CAPI_EXPORT extern int TF_OperationOutputListLength(TF_Operation* oper,
                                                       const char* arg_name,
                                                       TF_Status* status);

TF_CAPI_EXPORT extern int TF_OperationNumInputs(TF_Operation* oper);
TF_CAPI_EXPORT extern TF_DataType TF_OperationInputType(TF_Input oper_in);
TF_CAPI_EXPORT extern int TF_OperationInputListLength(TF_Operation* oper,
                                                      const char* arg_name,
                                                      TF_Status* status);

// In this code:
//   TF_Output producer = TF_OperationInput(consumer);
// There is an edge from producer.oper's output (given by
// producer.index) to consumer.oper's input (given by consumer.index).
TF_CAPI_EXPORT extern TF_Output TF_OperationInput(TF_Input oper_in);

// Get the number of current consumers of a specific output of an
// operation.  Note that this number can change when new operations
// are added to the graph.
TF_CAPI_EXPORT extern int TF_OperationOutputNumConsumers(TF_Output oper_out);

// Get list of all current consumers of a specific output of an
// operation.  `consumers` must point to an array of length at least
// `max_consumers` (ideally set to
// TF_OperationOutputNumConsumers(oper_out)).  Beware that a concurrent
// modification of the graph can increase the number of consumers of
// an operation.  Returns the number of output consumers (should match
// TF_OperationOutputNumConsumers(oper_out)).
TF_CAPI_EXPORT extern int TF_OperationOutputConsumers(TF_Output oper_out,
                                                      TF_Input* consumers,
                                                      int max_consumers);

// Get the number of control inputs to an operation.
TF_CAPI_EXPORT extern int TF_OperationNumControlInputs(TF_Operation* oper);

// Get list of all control inputs to an operation.  `control_inputs` must
// point to an array of length `max_control_inputs` (ideally set to
// TF_OperationNumControlInputs(oper)).  Returns the number of control
// inputs (should match TF_OperationNumControlInputs(oper)).
TF_CAPI_EXPORT extern int TF_OperationGetControlInputs(
    TF_Operation* oper, TF_Operation** control_inputs, int max_control_inputs);

// Get the number of operations that have `*oper` as a control input.
// Note that this number can change when new operations are added to
// the graph.
TF_CAPI_EXPORT extern int TF_OperationNumControlOutputs(TF_Operation* oper);

// Get the list of operations that have `*oper` as a control input.
// `control_outputs` must point to an array of length at least
// `max_control_outputs` (ideally set to
// TF_OperationNumControlOutputs(oper)). Beware that a concurrent
// modification of the graph can increase the number of control
// outputs.  Returns the number of control outputs (should match
// TF_OperationNumControlOutputs(oper)).
TF_CAPI_EXPORT extern int TF_OperationGetControlOutputs(
    TF_Operation* oper, TF_Operation** control_outputs,
    int max_control_outputs);

// TF_AttrMetadata describes the value of an attribute on an operation.
typedef struct TF_AttrMetadata {
  // A boolean: 1 if the attribute value is a list, 0 otherwise.
  unsigned char is_list;

  // Length of the list if is_list is true. Undefined otherwise.
  int64_t list_size;

  // Type of elements of the list if is_list != 0.
  // Type of the single value stored in the attribute if is_list == 0.
  TF_AttrType type;

  // Total size the attribute value.
  // The units of total_size depend on is_list and type.
  // (1) If type == TF_ATTR_STRING and is_list == 0
  //     then total_size is the byte size of the string
  //     valued attribute.
  // (2) If type == TF_ATTR_STRING and is_list == 1
  //     then total_size is the cumulative byte size
  //     of all the strings in the list.
  // (3) If type == TF_ATTR_SHAPE and is_list == 0
  //     then total_size is the number of dimensions
  //     of the shape valued attribute, or -1
  //     if its rank is unknown.
  // (4) If type == TF_ATTR_SHAPE and is_list == 1
  //     then total_size is the cumulative number
  //     of dimensions of all shapes in the list.
  // (5) Otherwise, total_size is undefined.
  int64_t total_size;
} TF_AttrMetadata;

// Returns metadata about the value of the attribute `attr_name` of `oper`.
TF_CAPI_EXPORT extern TF_AttrMetadata TF_OperationGetAttrMetadata(
    TF_Operation* oper, const char* attr_name, TF_Status* status);

// Fills in `value` with the value of the attribute `attr_name`.  `value` must
// point to an array of length at least `max_length` (ideally set to
// TF_AttrMetadata.total_size from TF_OperationGetAttrMetadata(oper,
// attr_name)).
TF_CAPI_EXPORT extern void TF_OperationGetAttrString(TF_Operation* oper,
                                                     const char* attr_name,
                                                     void* value,
                                                     size_t max_length,
                                                     TF_Status* status);

// Get the list of strings in the value of the attribute `attr_name`.  Fills in
// `values` and `lengths`, each of which must point to an array of length at
// least `max_values`.
//
// The elements of values will point to addresses in `storage` which must be at
// least `storage_size` bytes in length.  Ideally, max_values would be set to
// TF_AttrMetadata.list_size and `storage` would be at least
// TF_AttrMetadata.total_size, obtained from TF_OperationGetAttrMetadata(oper,
// attr_name).
//
// Fails if storage_size is too small to hold the requested number of strings.
TF_CAPI_EXPORT extern void TF_OperationGetAttrStringList(
    TF_Operation* oper, const char* attr_name, void** values, size_t* lengths,
    int max_values, void* storage, size_t storage_size, TF_Status* status);

TF_CAPI_EXPORT extern void TF_OperationGetAttrInt(TF_Operation* oper,
                                                  const char* attr_name,
                                                  int64_t* value,
                                                  TF_Status* status);

// Fills in `values` with the value of the attribute `attr_name` of `oper`.
// `values` must point to an array of length at least `max_values` (ideally set
// TF_AttrMetadata.list_size from TF_OperationGetAttrMetadata(oper,
// attr_name)).
TF_CAPI_EXPORT extern void TF_OperationGetAttrIntList(TF_Operation* oper,
                                                      const char* attr_name,
                                                      int64_t* values,
                                                      int max_values,
                                                      TF_Status* status);

TF_CAPI_EXPORT extern void TF_OperationGetAttrFloat(TF_Operation* oper,
                                                    const char* attr_name,
                                                    float* value,
                                                    TF_Status* status);

// Fills in `values` with the value of the attribute `attr_name` of `oper`.
// `values` must point to an array of length at least `max_values` (ideally set
// to TF_AttrMetadata.list_size from TF_OperationGetAttrMetadata(oper,
// attr_name)).
TF_CAPI_EXPORT extern void TF_OperationGetAttrFloatList(TF_Operation* oper,
                                                        const char* attr_name,
                                                        float* values,
                                                        int max_values,
                                                        TF_Status* status);

TF_CAPI_EXPORT extern void TF_OperationGetAttrBool(TF_Operation* oper,
                                                   const char* attr_name,
                                                   unsigned char* value,
                                                   TF_Status* status);

// Fills in `values` with the value of the attribute `attr_name` of `oper`.
// `values` must point to an array of length at least `max_values` (ideally set
// to TF_AttrMetadata.list_size from TF_OperationGetAttrMetadata(oper,
// attr_name)).
TF_CAPI_EXPORT extern void TF_OperationGetAttrBoolList(TF_Operation* oper,
                                                       const char* attr_name,
                                                       unsigned char* values,
                                                       int max_values,
                                                       TF_Status* status);

TF_CAPI_EXPORT extern void TF_OperationGetAttrType(TF_Operation* oper,
                                                   const char* attr_name,
                                                   TF_DataType* value,
                                                   TF_Status* status);

// Fills in `values` with the value of the attribute `attr_name` of `oper`.
// `values` must point to an array of length at least `max_values` (ideally set
// to TF_AttrMetadata.list_size from TF_OperationGetAttrMetadata(oper,
// attr_name)).
TF_CAPI_EXPORT extern void TF_OperationGetAttrTypeList(TF_Operation* oper,
                                                       const char* attr_name,
                                                       TF_DataType* values,
                                                       int max_values,
                                                       TF_Status* status);

// Fills in `value` with the value of the attribute `attr_name` of `oper`.
// `values` must point to an array of length at least `num_dims` (ideally set to
// TF_Attr_Meta.size from TF_OperationGetAttrMetadata(oper, attr_name)).
TF_CAPI_EXPORT extern void TF_OperationGetAttrShape(TF_Operation* oper,
                                                    const char* attr_name,
                                                    int64_t* value,
                                                    int num_dims,
                                                    TF_Status* status);

// Fills in `dims` with the list of shapes in the attribute `attr_name` of
// `oper` and `num_dims` with the corresponding number of dimensions. On return,
// for every i where `num_dims[i]` > 0, `dims[i]` will be an array of
// `num_dims[i]` elements. A value of -1 for `num_dims[i]` indicates that the
// i-th shape in the list is unknown.
//
// The elements of `dims` will point to addresses in `storage` which must be
// large enough to hold at least `storage_size` int64_ts.  Ideally, `num_shapes`
// would be set to TF_AttrMetadata.list_size and `storage_size` would be set to
// TF_AttrMetadata.total_size from TF_OperationGetAttrMetadata(oper,
// attr_name).
//
// Fails if storage_size is insufficient to hold the requested shapes.
TF_CAPI_EXPORT extern void TF_OperationGetAttrShapeList(
    TF_Operation* oper, const char* attr_name, int64_t** dims, int* num_dims,
    int num_shapes, int64_t* storage, int storage_size, TF_Status* status);

// Sets `value` to the binary-serialized TensorShapeProto of the value of
// `attr_name` attribute of `oper`'.
TF_CAPI_EXPORT extern void TF_OperationGetAttrTensorShapeProto(
    TF_Operation* oper, const char* attr_name, TF_Buffer* value,
    TF_Status* status);

// Fills in `values` with binary-serialized TensorShapeProto values of the
// attribute `attr_name` of `oper`. `values` must point to an array of length at
// least `num_values` (ideally set to TF_AttrMetadata.list_size from
// TF_OperationGetAttrMetadata(oper, attr_name)).
TF_CAPI_EXPORT extern void TF_OperationGetAttrTensorShapeProtoList(
    TF_Operation* oper, const char* attr_name, TF_Buffer** values,
    int max_values, TF_Status* status);

// Gets the TF_Tensor valued attribute of `attr_name` of `oper`.
//
// Allocates a new TF_Tensor which the caller is expected to take
// ownership of (and can deallocate using TF_DeleteTensor).
TF_CAPI_EXPORT extern void TF_OperationGetAttrTensor(TF_Operation* oper,
                                                     const char* attr_name,
                                                     TF_Tensor** value,
                                                     TF_Status* status);

// Fills in `values` with the TF_Tensor values of the attribute `attr_name` of
// `oper`. `values` must point to an array of TF_Tensor* of length at least
// `max_values` (ideally set to TF_AttrMetadata.list_size from
// TF_OperationGetAttrMetadata(oper, attr_name)).
//
// The caller takes ownership of all the non-null TF_Tensor* entries in `values`
// (which can be deleted using TF_DeleteTensor(values[i])).
TF_CAPI_EXPORT extern void TF_OperationGetAttrTensorList(TF_Operation* oper,
                                                         const char* attr_name,
                                                         TF_Tensor** values,
                                                         int max_values,
                                                         TF_Status* status);

// Sets `output_attr_value` to the binary-serialized AttrValue proto
// representation of the value of the `attr_name` attr of `oper`.
TF_CAPI_EXPORT extern void TF_OperationGetAttrValueProto(
    TF_Operation* oper, const char* attr_name, TF_Buffer* output_attr_value,
    TF_Status* status);

// Returns the operation in the graph with `oper_name`. Returns nullptr if
// no operation found.
TF_CAPI_EXPORT extern TF_Operation* TF_GraphOperationByName(
    TF_Graph* graph, const char* oper_name);

// Iterate through the operations of a graph.  To use:
// size_t pos = 0;
// TF_Operation* oper;
// while ((oper = TF_GraphNextOperation(graph, &pos)) != nullptr) {
//   DoSomethingWithOperation(oper);
// }
TF_CAPI_EXPORT extern TF_Operation* TF_GraphNextOperation(TF_Graph* graph,
                                                          size_t* pos);

// Write out a serialized representation of `graph` (as a GraphDef protocol
// message) to `output_graph_def` (allocated by TF_NewBuffer()).
// `output_graph_def`'s underlying buffer will be freed when TF_DeleteBuffer()
// is called.
//
// May fail on very large graphs in the future.
TF_CAPI_EXPORT extern void TF_GraphToGraphDef(TF_Graph* graph,
                                              TF_Buffer* output_graph_def,
                                              TF_Status* status);

// Returns the serialized OpDef proto with name `op_name`, or a bad status if no
// such op exists. This can return OpDefs of functions copied into the graph.
TF_CAPI_EXPORT extern void TF_GraphGetOpDef(TF_Graph* graph,
                                            const char* op_name,
                                            TF_Buffer* output_op_def,
                                            TF_Status* status);

// Returns the serialized VersionDef proto for this graph.
TF_CAPI_EXPORT extern void TF_GraphVersions(TF_Graph* graph,
                                            TF_Buffer* output_version_def,
                                            TF_Status* status);

// TF_ImportGraphDefOptions holds options that can be passed to
// TF_GraphImportGraphDef.
typedef struct TF_ImportGraphDefOptions TF_ImportGraphDefOptions;

TF_CAPI_EXPORT extern TF_ImportGraphDefOptions* TF_NewImportGraphDefOptions(
    void);
TF_CAPI_EXPORT extern void TF_DeleteImportGraphDefOptions(
    TF_ImportGraphDefOptions* opts);

// Set the prefix to be prepended to the names of nodes in `graph_def` that will
// be imported into `graph`. `prefix` is copied and has no lifetime
// requirements.
TF_CAPI_EXPORT extern void TF_ImportGraphDefOptionsSetPrefix(
    TF_ImportGraphDefOptions* opts, const char* prefix);

// Set the execution device for nodes in `graph_def`.
// Only applies to nodes where a device was not already explicitly specified.
// `device` is copied and has no lifetime requirements.
TF_CAPI_EXPORT extern void TF_ImportGraphDefOptionsSetDefaultDevice(
    TF_ImportGraphDefOptions* opts, const char* device);

// Set whether to uniquify imported operation names. If true, imported operation
// names will be modified if their name already exists in the graph. If false,
// conflicting names will be treated as an error. Note that this option has no
// effect if a prefix is set, since the prefix will guarantee all names are
// unique. Defaults to false.
TF_CAPI_EXPORT extern void TF_ImportGraphDefOptionsSetUniquifyNames(
    TF_ImportGraphDefOptions* opts, unsigned char uniquify_names);

// If true, the specified prefix will be modified if it already exists as an
// operation name or prefix in the graph. If false, a conflicting prefix will be
// treated as an error. This option has no effect if no prefix is specified.
TF_CAPI_EXPORT extern void TF_ImportGraphDefOptionsSetUniquifyPrefix(
    TF_ImportGraphDefOptions* opts, unsigned char uniquify_prefix);

// Set any imported nodes with input `src_name:src_index` to have that input
// replaced with `dst`. `src_name` refers to a node in the graph to be imported,
// `dst` references a node already existing in the graph being imported into.
// `src_name` is copied and has no lifetime requirements.
TF_CAPI_EXPORT extern void TF_ImportGraphDefOptionsAddInputMapping(
    TF_ImportGraphDefOptions* opts, const char* src_name, int src_index,
    TF_Output dst);

// Set any imported nodes with control input `src_name` to have that input
// replaced with `dst`. `src_name` refers to a node in the graph to be imported,
// `dst` references an operation already existing in the graph being imported
// into. `src_name` is copied and has no lifetime requirements.
TF_CAPI_EXPORT extern void TF_ImportGraphDefOptionsRemapControlDependency(
    TF_ImportGraphDefOptions* opts, const char* src_name, TF_Operation* dst);

// Cause the imported graph to have a control dependency on `oper`. `oper`
// should exist in the graph being imported into.
TF_CAPI_EXPORT extern void TF_ImportGraphDefOptionsAddControlDependency(
    TF_ImportGraphDefOptions* opts, TF_Operation* oper);

// Add an output in `graph_def` to be returned via the `return_outputs` output
// parameter of TF_GraphImportGraphDef(). If the output is remapped via an input
// mapping, the corresponding existing tensor in `graph` will be returned.
// `oper_name` is copied and has no lifetime requirements.
TF_CAPI_EXPORT extern void TF_ImportGraphDefOptionsAddReturnOutput(
    TF_ImportGraphDefOptions* opts, const char* oper_name, int index);

// Returns the number of return outputs added via
// TF_ImportGraphDefOptionsAddReturnOutput().
TF_CAPI_EXPORT extern int TF_ImportGraphDefOptionsNumReturnOutputs(
    const TF_ImportGraphDefOptions* opts);

// Add an operation in `graph_def` to be returned via the `return_opers` output
// parameter of TF_GraphImportGraphDef(). `oper_name` is copied and has no
// lifetime requirements.
TF_CAPI_EXPORT extern void TF_ImportGraphDefOptionsAddReturnOperation(
    TF_ImportGraphDefOptions* opts, const char* oper_name);

// Returns the number of return operations added via
// TF_ImportGraphDefOptionsAddReturnOperation().
TF_CAPI_EXPORT extern int TF_ImportGraphDefOptionsNumReturnOperations(
    const TF_ImportGraphDefOptions* opts);

// TF_ImportGraphDefResults holds results that are generated by
// TF_GraphImportGraphDefWithResults().
typedef struct TF_ImportGraphDefResults TF_ImportGraphDefResults;

// Fetches the return outputs requested via
// TF_ImportGraphDefOptionsAddReturnOutput(). The number of fetched outputs is
// returned in `num_outputs`. The array of return outputs is returned in
// `outputs`. `*outputs` is owned by and has the lifetime of `results`.
TF_CAPI_EXPORT extern void TF_ImportGraphDefResultsReturnOutputs(
    TF_ImportGraphDefResults* results, int* num_outputs, TF_Output** outputs);

// Fetches the return operations requested via
// TF_ImportGraphDefOptionsAddReturnOperation(). The number of fetched
// operations is returned in `num_opers`. The array of return operations is
// returned in `opers`. `*opers` is owned by and has the lifetime of `results`.
TF_CAPI_EXPORT extern void TF_ImportGraphDefResultsReturnOperations(
    TF_ImportGraphDefResults* results, int* num_opers, TF_Operation*** opers);

// Fetches any input mappings requested via
// TF_ImportGraphDefOptionsAddInputMapping() that didn't appear in the GraphDef
// and weren't used as input to any node in the imported graph def. The number
// of fetched mappings is returned in `num_missing_unused_input_mappings`. The
// array of each mapping's source node name is returned in `src_names`, and the
// array of each mapping's source index is returned in `src_indexes`.
//
// `*src_names`, `*src_indexes`, and the memory backing each string in
// `src_names` are owned by and have the lifetime of `results`.
TF_CAPI_EXPORT extern void TF_ImportGraphDefResultsMissingUnusedInputMappings(
    TF_ImportGraphDefResults* results, int* num_missing_unused_input_mappings,
    const char*** src_names, int** src_indexes);

// Deletes a results object returned by TF_GraphImportGraphDefWithResults().
TF_CAPI_EXPORT extern void TF_DeleteImportGraphDefResults(
    TF_ImportGraphDefResults* results);

// Import the graph serialized in `graph_def` into `graph`.  Returns nullptr and
// a bad status on error. Otherwise, returns a populated
// TF_ImportGraphDefResults instance. The returned instance must be deleted via
// TF_DeleteImportGraphDefResults().
TF_CAPI_EXPORT extern TF_ImportGraphDefResults*
TF_GraphImportGraphDefWithResults(TF_Graph* graph, const TF_Buffer* graph_def,
                                  const TF_ImportGraphDefOptions* options,
                                  TF_Status* status);

// Import the graph serialized in `graph_def` into `graph`.
// Convenience function for when only return outputs are needed.
//
// `num_return_outputs` must be the number of return outputs added (i.e. the
// result of TF_ImportGraphDefOptionsNumReturnOutputs()).  If
// `num_return_outputs` is non-zero, `return_outputs` must be of length
// `num_return_outputs`. Otherwise it can be null.
TF_CAPI_EXPORT extern void TF_GraphImportGraphDefWithReturnOutputs(
    TF_Graph* graph, const TF_Buffer* graph_def,
    const TF_ImportGraphDefOptions* options, TF_Output* return_outputs,
    int num_return_outputs, TF_Status* status);

// Import the graph serialized in `graph_def` into `graph`.
// Convenience function for when no results are needed.
TF_CAPI_EXPORT extern void TF_GraphImportGraphDef(
    TF_Graph* graph, const TF_Buffer* graph_def,
    const TF_ImportGraphDefOptions* options, TF_Status* status);

// Adds a copy of function `func` and optionally its gradient function `grad`
// to `g`. Once `func`/`grad` is added to `g`, it can be called by creating
// an operation using the function's name.
// Any changes to `func`/`grad` (including deleting it) done after this method
// returns, won't affect the copy of `func`/`grad` in `g`.
// If `func` or `grad` are already in `g`, TF_GraphCopyFunction has no
// effect on them, but can establish the function->gradient relationship
// between them if `func` does not already have a gradient. If `func` already
// has a gradient different from `grad`, an error is returned.
//
// `func` must not be null.
// If `grad` is null and `func` is not in `g`, `func` is added without a
// gradient.
// If `grad` is null and `func` is in `g`, TF_GraphCopyFunction is a noop.
// `grad` must have appropriate signature as described in the doc of
// GradientDef in tensorflow/core/framework/function.proto.
//
// If successful, status is set to OK and `func` and `grad` are added to `g`.
// Otherwise, status is set to the encountered error and `g` is unmodified.
TF_CAPI_EXPORT extern void TF_GraphCopyFunction(TF_Graph* g,
                                                const TF_Function* func,
                                                const TF_Function* grad,
                                                TF_Status* status);

// Returns the number of TF_Functions registered in `g`.
TF_CAPI_EXPORT extern int TF_GraphNumFunctions(TF_Graph* g);

// Fills in `funcs` with the TF_Function* registered in `g`.
// `funcs` must point to an array of TF_Function* of length at least
// `max_func`. In usual usage, max_func should be set to the result of
// TF_GraphNumFunctions(g). In this case, all the functions registered in
// `g` will be returned. Else, an unspecified subset.
//
// If successful, returns the number of TF_Function* successfully set in
// `funcs` and sets status to OK. The caller takes ownership of
// all the returned TF_Functions. They must be deleted with TF_DeleteFunction.
// On error, returns 0, sets status to the encountered error, and the contents
// of funcs will be undefined.
TF_CAPI_EXPORT extern int TF_GraphGetFunctions(TF_Graph* g, TF_Function** funcs,
                                               int max_func, TF_Status* status);

// Note: The following function may fail on very large protos in the future.

TF_CAPI_EXPORT extern void TF_OperationToNodeDef(TF_Operation* oper,
                                                 TF_Buffer* output_node_def,
                                                 TF_Status* status);

typedef struct TF_WhileParams {
  // The number of inputs to the while loop, i.e. the number of loop variables.
  // This is the size of cond_inputs, body_inputs, and body_outputs.
  const int ninputs;

  // The while condition graph. The inputs are the current values of the loop
  // variables. The output should be a scalar boolean.
  TF_Graph* const cond_graph;
  const TF_Output* const cond_inputs;
  TF_Output cond_output;

  // The loop body graph. The inputs are the current values of the loop
  // variables. The outputs are the updated values of the loop variables.
  TF_Graph* const body_graph;
  const TF_Output* const body_inputs;
  TF_Output* const body_outputs;

  // Unique null-terminated name for this while loop. This is used as a prefix
  // for created operations.
  const char* name;
} TF_WhileParams;

// Creates a TF_WhileParams for creating a while loop in `g`. `inputs` are
// outputs that already exist in `g` used as initial values for the loop
// variables.
//
// The returned TF_WhileParams will have all fields initialized except
// `cond_output`, `body_outputs`, and `name`. The `body_outputs` buffer will be
// allocated to size `ninputs`. The caller should build `cond_graph` and
// `body_graph` starting from the inputs, and store the final outputs in
// `cond_output` and `body_outputs`.
//
// If `status` is OK, the caller must call either TF_FinishWhile or
// TF_AbortWhile on the returned TF_WhileParams. If `status` isn't OK, the
// returned TF_WhileParams is not valid, and the caller should not call
// TF_FinishWhile() or TF_AbortWhile().
//
// Missing functionality (TODO):
// - Gradients
// - Reference-type inputs
// - Directly referencing external tensors from the cond/body graphs (this is
//   possible in the Python API)
TF_CAPI_EXPORT extern TF_WhileParams TF_NewWhile(TF_Graph* g, TF_Output* inputs,
                                                 int ninputs,
                                                 TF_Status* status);

// Builds the while loop specified by `params` and returns the output tensors of
// the while loop in `outputs`. `outputs` should be allocated to size
// `params.ninputs`.
//
// `params` is no longer valid once this returns.
//
// Either this or TF_AbortWhile() must be called after a successful
// TF_NewWhile() call.
TF_CAPI_EXPORT extern void TF_FinishWhile(const TF_WhileParams* params,
                                          TF_Status* status,
                                          TF_Output* outputs);

// Frees `params`s resources without building a while loop. `params` is no
// longer valid after this returns. Either this or TF_FinishWhile() must be
// called after a successful TF_NewWhile() call.
TF_CAPI_EXPORT extern void TF_AbortWhile(const TF_WhileParams* params);

// Adds operations to compute the partial derivatives of sum of `y`s w.r.t `x`s,
// i.e., d(y_1 + y_2 + ...)/dx_1, d(y_1 + y_2 + ...)/dx_2...
//
// `dx` are used as initial gradients (which represent the symbolic partial
// derivatives of some loss function `L` w.r.t. `y`).
// `dx` must be nullptr or have size `ny`.
// If `dx` is nullptr, the implementation will use dx of `OnesLike` for all
// shapes in `y`.
// The partial derivatives are returned in `dy`. `dy` should be allocated to
// size `nx`.
//
// Gradient nodes are automatically named under the ""gradients/"" prefix. To
// guarantee name uniqueness, subsequent calls to the same graph will
// append an incremental tag to the prefix: ""gradients_1/"", ""gradients_2/"", ...
// See TF_AddGradientsWithPrefix, which provides a means to specify a custom
// name prefix for operations added to a graph to compute the gradients.
//
// WARNING: This function does not yet support all the gradients that python
// supports. See
// https://www.tensorflow.org/code/tensorflow/cc/gradients/README.md
// for instructions on how to add C++ more gradients.
TF_CAPI_EXPORT void TF_AddGradients(TF_Graph* g, TF_Output* y, int ny,
                                    TF_Output* x, int nx, TF_Output* dx,
                                    TF_Status* status, TF_Output* dy);

// Adds operations to compute the partial derivatives of sum of `y`s w.r.t `x`s,
// i.e., d(y_1 + y_2 + ...)/dx_1, d(y_1 + y_2 + ...)/dx_2...
// This is a variant of TF_AddGradients that allows to caller to pass a custom
// name prefix to the operations added to a graph to compute the gradients.
//
// `dx` are used as initial gradients (which represent the symbolic partial
// derivatives of some loss function `L` w.r.t. `y`).
// `dx` must be nullptr or have size `ny`.
// If `dx` is nullptr, the implementation will use dx of `OnesLike` for all
// shapes in `y`.
// The partial derivatives are returned in `dy`. `dy` should be allocated to
// size `nx`.
// `prefix` names the scope into which all gradients operations are being added.
// `prefix` must be unique within the provided graph otherwise this operation
// will fail. If `prefix` is nullptr, the default prefixing behaviour takes
// place, see TF_AddGradients for more details.
//
// WARNING: This function does not yet support all the gradients that python
// supports. See
// https://www.tensorflow.org/code/tensorflow/cc/gradients/README.md
// for instructions on how to add C++ more gradients.
TF_CAPI_EXPORT void TF_AddGradientsWithPrefix(TF_Graph* g, const char* prefix,
                                              TF_Output* y, int ny,
                                              TF_Output* x, int nx,
                                              TF_Output* dx, TF_Status* status,
                                              TF_Output* dy);

// Create a TF_Function from a TF_Graph
//
// Params:
//  fn_body - the graph whose operations (or subset of whose operations) will be
//            converted to TF_Function.
//  fn_name - the name of the new TF_Function. Should match the operation
//            name (OpDef.name) regexp [A-Z][A-Za-z0-9_.\\-/]*.
//            If `append_hash_to_fn_name` is false, `fn_name` must be distinct
//            from other function and operation names (at least those
//            registered in graphs where this function will be used).
//  append_hash_to_fn_name - Must be 0 or 1. If set to 1, the actual name
//                           of the function will be `fn_name` appended with
//                           '_<hash_of_this_function's_definition>'.
//                           If set to 0, the function's name will be `fn_name`.
//  num_opers - `num_opers` contains the number of elements in the `opers` array
//              or a special value of -1 meaning that no array is given.
//              The distinction between an empty array of operations and no
//              array of operations is necessary to distinguish the case of
//              creating a function with no body (e.g. identity or permutation)
//              and the case of creating a function whose body contains all
//              the nodes in the graph (except for the automatic skipping, see
//              below).
//  opers - Array of operations to become the body of the function or null.
//          - If no array is given (`num_opers`  = -1), all the
//          operations in `fn_body` will become part of the function
//          except operations referenced in `inputs`. These operations
//          must have a single output (these operations are typically
//          placeholders created for the sole purpose of representing
//          an input. We can relax this constraint if there are
//          compelling use cases).
//          - If an array is given (`num_opers` >= 0), all operations
//          in it will become part of the function. In particular, no
//          automatic skipping of dummy input operations is performed.
//  ninputs - number of elements in `inputs` array
//  inputs - array of TF_Outputs that specify the inputs to the function.
//           If `ninputs` is zero (the function takes no inputs), `inputs`
//           can be null. The names used for function inputs are normalized
//           names of the operations (usually placeholders) pointed to by
//           `inputs`. These operation names should start with a letter.
//           Normalization will convert all letters to lowercase and
//           non-alphanumeric characters to '_' to make resulting names match
//           the ""[a-z][a-z0-9_]*"" pattern for operation argument names.
//           `inputs` cannot contain the same tensor twice.
//  noutputs - number of elements in `outputs` array
//  outputs - array of TF_Outputs that specify the outputs of the function.
//            If `noutputs` is zero (the function returns no outputs), `outputs`
//            can be null. `outputs` can contain the same tensor more than once.
//  output_names - The names of the function's outputs. `output_names` array
//                 must either have the same length as `outputs`
//                 (i.e. `noutputs`) or be null. In the former case,
//                 the names should match the regular expression for ArgDef
//                 names - ""[a-z][a-z0-9_]*"". In the latter case,
//                 names for outputs will be generated automatically.
//  opts - various options for the function, e.g. XLA's inlining control.
//  description - optional human-readable description of this function.
//  status - Set to OK on success and an appropriate error on failure.
//
// Note that when the same TF_Output is listed as both an input and an output,
// the corresponding function's output will equal to this input,
// instead of the original node's output.
//
// Callers must also satisfy the following constraints:
// - `inputs` cannot refer to TF_Outputs within a control flow context. For
//   example, one cannot use the output of ""switch"" node as input.
// - `inputs` and `outputs` cannot have reference types. Reference types are
//   not exposed through C API and are being replaced with Resources. We support
//   reference types inside function's body to support legacy code. Do not
//   use them in new code.
// - Every node in the function's body must have all of its inputs (including
//   control inputs). In other words, for every node in the body, each input
//   must be either listed in `inputs` or must come from another node in
//   the body. In particular, it is an error to have a control edge going from
//   a node outside of the body into a node in the body. This applies to control
//   edges going from nodes referenced in `inputs` to nodes in the body when
//   the former nodes are not in the body (automatically skipped or not
//   included in explicitly specified body).
//
// Returns:
//  On success, a newly created TF_Function instance. It must be deleted by
//  calling TF_DeleteFunction.
//
//  On failure, null.
TF_CAPI_EXPORT extern TF_Function* TF_GraphToFunction(
    const TF_Graph* fn_body, const char* fn_name,
    unsigned char append_hash_to_fn_name, int num_opers,
    const TF_Operation* const* opers, int ninputs, const TF_Output* inputs,
    int noutputs, const TF_Output* outputs, const char* const* output_names,
    const TF_FunctionOptions* opts, const char* description, TF_Status* status);

// Similar to TF_GraphToFunction but allows specifying control outputs of the
// function.
//
//  The arguments of TF_GraphToFunction have the same meaning, but the new
//  arguments are as follows:
//
//    ncontrol_outputs: Number of control outputs of the function.
//    control_outputs: vector of TF_Operation objects to be marked as control
//      outputs of the function. Operations marked as control outputs are
//      guaranteed to execute.
//    control_output_names: Optional. If not nullptr, vector of strings, one
//      per control output, with their names to be added to the function's
//      OpDef.
TF_CAPI_EXPORT extern TF_Function* TF_GraphToFunctionWithControlOutputs(
    const TF_Graph* fn_body, const char* fn_name,
    unsigned char append_hash_to_fn_name, int num_opers,
    const TF_Operation* const* opers, int ninputs, const TF_Output* inputs,
    int noutputs, const TF_Output* outputs, const char* const* output_names,
    int ncontrol_outputs, const TF_Operation* const* control_outputs,
    const char* const* control_output_names, const TF_FunctionOptions* opts,
    const char* description, TF_Status* status);

// Returns the name of the graph function.
// The return value points to memory that is only usable until the next
// mutation to *func.
TF_CAPI_EXPORT extern const char* TF_FunctionName(TF_Function* func);

// Write out a serialized representation of `func` (as a FunctionDef protocol
// message) to `output_func_def` (allocated by TF_NewBuffer()).
// `output_func_def`'s underlying buffer will be freed when TF_DeleteBuffer()
// is called.
//
// May fail on very large graphs in the future.
TF_CAPI_EXPORT extern void TF_FunctionToFunctionDef(TF_Function* func,
                                                    TF_Buffer* output_func_def,
                                                    TF_Status* status);

// Construct and return the function whose FunctionDef representation is
// serialized in `proto`. `proto_len` must equal the number of bytes
// pointed to by `proto`.
// Returns:
//  On success, a newly created TF_Function instance. It must be deleted by
//  calling TF_DeleteFunction.
//
//  On failure, null.
TF_CAPI_EXPORT extern TF_Function* TF_FunctionImportFunctionDef(
    const void* proto, size_t proto_len, TF_Status* status);

// Sets function attribute named `attr_name` to value stored in `proto`.
// If this attribute is already set to another value, it is overridden.
// `proto` should point to a sequence of bytes of length `proto_len`
// representing a binary serialization of an AttrValue protocol
// buffer.
TF_CAPI_EXPORT extern void TF_FunctionSetAttrValueProto(TF_Function* func,
                                                        const char* attr_name,
                                                        const void* proto,
                                                        size_t proto_len,
                                                        TF_Status* status);

// Sets `output_attr_value` to the binary-serialized AttrValue proto
// representation of the value of the `attr_name` attr of `func`.
// If `attr_name` attribute is not present, status is set to an error.
TF_CAPI_EXPORT extern void TF_FunctionGetAttrValueProto(
    TF_Function* func, const char* attr_name, TF_Buffer* output_attr_value,
    TF_Status* status);

// Frees the memory used by the `func` struct.
// TF_DeleteFunction is a noop if `func` is null.
// Deleting a function does not remove it from any graphs it was copied to.
TF_CAPI_EXPORT extern void TF_DeleteFunction(TF_Function* func);

// Attempts to evaluate `output`. This will only be possible if `output` doesn't
// depend on any graph inputs (this function is safe to call if this isn't the
// case though).
//
// If the evaluation is successful, this function returns true and `output`s
// value is returned in `result`. Otherwise returns false. An error status is
// returned if something is wrong with the graph or input. Note that this may
// return false even if no error status is set.
TF_CAPI_EXPORT extern unsigned char TF_TryEvaluateConstant(TF_Graph* graph,
                                                           TF_Output output,
                                                           TF_Tensor** result,
                                                           TF_Status* status);

// TODO(josh11b): Register OpDef, available to all operations added
// to this graph.

// --------------------------------------------------------------------------
// API for driving Graph execution.

typedef struct TF_Session TF_Session;

// Return a new execution session with the associated graph, or NULL on
// error. Does not take ownership of any input parameters.
//
// *`graph` must be a valid graph (not deleted or nullptr). `graph` will be be
// kept alive for the lifetime of the returned TF_Session. New nodes can still
// be added to `graph` after this call.
TF_CAPI_EXPORT extern TF_Session* TF_NewSession(TF_Graph* graph,
                                                const TF_SessionOptions* opts,
                                                TF_Status* status);

// This function creates a new TF_Session (which is created on success) using
// `session_options`, and then initializes state (restoring tensors and other
// assets) using `run_options`.
//
// Any NULL and non-NULL value combinations for (`run_options, `meta_graph_def`)
// are valid.
//
// - `export_dir` must be set to the path of the exported SavedModel.
// - `tags` must include the set of tags used to identify one MetaGraphDef in
//    the SavedModel.
// - `graph` must be a graph newly allocated with TF_NewGraph().
//
// If successful, populates `graph` with the contents of the Graph and
// `meta_graph_def` with the MetaGraphDef of the loaded model.
TF_CAPI_EXPORT extern TF_Session* TF_LoadSessionFromSavedModel(
    const TF_SessionOptions* session_options, const TF_Buffer* run_options,
    const char* export_dir, const char* const* tags, int tags_len,
    TF_Graph* graph, TF_Buffer* meta_graph_def, TF_Status* status);

// Close a session.
//
// Contacts any other processes associated with the session, if applicable.
// May not be called after TF_DeleteSession().
TF_CAPI_EXPORT extern void TF_CloseSession(TF_Session*, TF_Status* status);

// Destroy a session object.
//
// Even if error information is recorded in *status, this call discards all
// local resources associated with the session.  The session may not be used
// during or after this call (and the session drops its reference to the
// corresponding graph).
TF_CAPI_EXPORT extern void TF_DeleteSession(TF_Session*, TF_Status* status);

// Run the graph associated with the session starting with the supplied inputs
// (inputs[0,ninputs-1] with corresponding values in input_values[0,ninputs-1]).
//
// Any NULL and non-NULL value combinations for (`run_options`,
// `run_metadata`) are valid.
//
//    - `run_options` may be NULL, in which case it will be ignored; or
//      non-NULL, in which case it must point to a `TF_Buffer` containing the
//      serialized representation of a `RunOptions` protocol buffer.
//    - `run_metadata` may be NULL, in which case it will be ignored; or
//      non-NULL, in which case it must point to an empty, freshly allocated
//      `TF_Buffer` that may be updated to contain the serialized representation
//      of a `RunMetadata` protocol buffer.
//
// The caller retains ownership of `input_values` (which can be deleted using
// TF_DeleteTensor). The caller also retains ownership of `run_options` and/or
// `run_metadata` (when not NULL) and should manually call TF_DeleteBuffer on
// them.
//
// On success, the tensors corresponding to outputs[0,noutputs-1] are placed in
// output_values[]. Ownership of the elements of output_values[] is transferred
// to the caller, which must eventually call TF_DeleteTensor on them.
//
// On failure, output_values[] contains NULLs.
TF_CAPI_EXPORT extern void TF_SessionRun(
    TF_Session* session,
    // RunOptions
    const TF_Buffer* run_options,
    // Input tensors
    const TF_Output* inputs, TF_Tensor* const* input_values, int ninputs,
    // Output tensors
    const TF_Output* outputs, TF_Tensor** output_values, int noutputs,
    // Target operations
    const TF_Operation* const* target_opers, int ntargets,
    // RunMetadata
    TF_Buffer* run_metadata,
    // Output status
    TF_Status*);

// Set up the graph with the intended feeds (inputs) and fetches (outputs) for a
// sequence of partial run calls.
//
// On success, returns a handle that is used for subsequent PRun calls. The
// handle should be deleted with TF_DeletePRunHandle when it is no longer
// needed.
//
// On failure, out_status contains a tensorflow::Status with an error
// message. *handle is set to nullptr.
TF_CAPI_EXPORT extern void TF_SessionPRunSetup(
    TF_Session*,
    // Input names
    const TF_Output* inputs, int ninputs,
    // Output names
    const TF_Output* outputs, int noutputs,
    // Target operations
    const TF_Operation* const* target_opers, int ntargets,
    // Output handle
    const char** handle,
    // Output status
    TF_Status*);

// Continue to run the graph with additional feeds and fetches. The
// execution state is uniquely identified by the handle.
TF_CAPI_EXPORT extern void TF_SessionPRun(
    TF_Session*, const char* handle,
    // Input tensors
    const TF_Output* inputs, TF_Tensor* const* input_values, int ninputs,
    // Output tensors
    const TF_Output* outputs, TF_Tensor** output_values, int noutputs,
    // Target operations
    const TF_Operation* const* target_opers, int ntargets,
    // Output status
    TF_Status*);

// Deletes a handle allocated by TF_SessionPRunSetup.
// Once called, no more calls to TF_SessionPRun should be made.
TF_CAPI_EXPORT extern void TF_DeletePRunHandle(const char* handle);

// --------------------------------------------------------------------------
// The deprecated session API.  Please switch to the above instead of
// TF_ExtendGraph(). This deprecated API can be removed at any time without
// notice.

typedef struct TF_DeprecatedSession TF_DeprecatedSession;

TF_CAPI_EXPORT extern TF_DeprecatedSession* TF_NewDeprecatedSession(
    const TF_SessionOptions*, TF_Status* status);
TF_CAPI_EXPORT extern void TF_CloseDeprecatedSession(TF_DeprecatedSession*,
                                                     TF_Status* status);
TF_CAPI_EXPORT extern void TF_DeleteDeprecatedSession(TF_DeprecatedSession*,
                                                      TF_Status* status);
TF_CAPI_EXPORT extern void TF_Reset(const TF_SessionOptions* opt,
                                    const char** containers, int ncontainers,
                                    TF_Status* status);
// Treat the bytes proto[0,proto_len-1] as a serialized GraphDef and
// add the nodes in that GraphDef to the graph for the session.
//
// Prefer use of TF_Session and TF_GraphImportGraphDef over this.
TF_CAPI_EXPORT extern void TF_ExtendGraph(TF_DeprecatedSession*,
                                          const void* proto, size_t proto_len,
                                          TF_Status*);

// See TF_SessionRun() above.
TF_CAPI_EXPORT extern void TF_Run(TF_DeprecatedSession*,
                                  const TF_Buffer* run_options,
                                  const char** input_names, TF_Tensor** inputs,
                                  int ninputs, const char** output_names,
                                  TF_Tensor** outputs, int noutputs,
                                  const char** target_oper_names, int ntargets,
                                  TF_Buffer* run_metadata, TF_Status*);

// See TF_SessionPRunSetup() above.
TF_CAPI_EXPORT extern void TF_PRunSetup(TF_DeprecatedSession*,
                                        const char** input_names, int ninputs,
                                        const char** output_names, int noutputs,
                                        const char** target_oper_names,
                                        int ntargets, const char** handle,
                                        TF_Status*);

// See TF_SessionPRun above.
TF_CAPI_EXPORT extern void TF_PRun(TF_DeprecatedSession*, const char* handle,
                                   const char** input_names, TF_Tensor** inputs,
                                   int ninputs, const char** output_names,
                                   TF_Tensor** outputs, int noutputs,
                                   const char** target_oper_names, int ntargets,
                                   TF_Status*);

typedef struct TF_DeviceList TF_DeviceList;

// Lists all devices in a TF_Session.
//
// Caller takes ownership of the returned TF_DeviceList* which must eventually
// be freed with a call to TF_DeleteDeviceList.
TF_CAPI_EXPORT extern TF_DeviceList* TF_SessionListDevices(TF_Session* session,
                                                           TF_Status* status);

// Lists all devices in a TF_Session.
//
// Caller takes ownership of the returned TF_DeviceList* which must eventually
// be freed with a call to TF_DeleteDeviceList.
TF_CAPI_EXPORT extern TF_DeviceList* TF_DeprecatedSessionListDevices(
    TF_DeprecatedSession* session, TF_Status* status);

// Deallocates the device list.
TF_CAPI_EXPORT extern void TF_DeleteDeviceList(TF_DeviceList* list);

// Counts the number of elements in the device list.
TF_CAPI_EXPORT extern int TF_DeviceListCount(const TF_DeviceList* list);

// Retrieves the full name of the device (e.g. /job:worker/replica:0/...)
// The return value will be a pointer to a null terminated string. The caller
// must not modify or delete the string. It will be deallocated upon a call to
// TF_DeleteDeviceList.
//
// If index is out of bounds, an error code will be set in the status object,
// and a null pointer will be returned.
TF_CAPI_EXPORT extern const char* TF_DeviceListName(const TF_DeviceList* list,
                                                    int index,
                                                    TF_Status* status);

// Retrieves the type of the device at the given index.
//
// The caller must not modify or delete the string. It will be deallocated upon
// a call to TF_DeleteDeviceList.
//
// If index is out of bounds, an error code will be set in the status object,
// and a null pointer will be returned.
TF_CAPI_EXPORT extern const char* TF_DeviceListType(const TF_DeviceList* list,
                                                    int index,
                                                    TF_Status* status);

// Retrieve the amount of memory associated with a given device.
//
// If index is out of bounds, an error code will be set in the status object,
// and -1 will be returned.
TF_CAPI_EXPORT extern int64_t TF_DeviceListMemoryBytes(
    const TF_DeviceList* list, int index, TF_Status* status);

// Retrieve the incarnation number of a given device.
//
// If index is out of bounds, an error code will be set in the status object,
// and 0 will be returned.
TF_CAPI_EXPORT extern uint64_t TF_DeviceListIncarnation(
    const TF_DeviceList* list, int index, TF_Status* status);

// --------------------------------------------------------------------------
// Load plugins containing custom ops and kernels

// TF_Library holds information about dynamically loaded TensorFlow plugins.
typedef struct TF_Library TF_Library;

// Load the library specified by library_filename and register the ops and
// kernels present in that library.
//
// Pass ""library_filename"" to a platform-specific mechanism for dynamically
// loading a library. The rules for determining the exact location of the
// library are platform-specific and are not documented here.
//
// On success, place OK in status and return the newly created library handle.
// The caller owns the library handle.
//
// On failure, place an error status in status and return NULL.
TF_CAPI_EXPORT extern TF_Library* TF_LoadLibrary(const char* library_filename,
                                                 TF_Status* status);

// Get the OpList of OpDefs defined in the library pointed by lib_handle.
//
// Returns a TF_Buffer. The memory pointed to by the result is owned by
// lib_handle. The data in the buffer will be the serialized OpList proto for
// ops defined in the library.
TF_CAPI_EXPORT extern TF_Buffer TF_GetOpList(TF_Library* lib_handle);

// Frees the memory associated with the library handle.
// Does NOT unload the library.
TF_CAPI_EXPORT extern void TF_DeleteLibraryHandle(TF_Library* lib_handle);

// Get the OpList of all OpDefs defined in this address space.
// Returns a TF_Buffer, ownership of which is transferred to the caller
// (and can be freed using TF_DeleteBuffer).
//
// The data in the buffer will be the serialized OpList proto for ops registered
// in this address space.
TF_CAPI_EXPORT extern TF_Buffer* TF_GetAllOpList(void);

// TF_ApiDefMap encapsulates a collection of API definitions for an operation.
//
// This object maps the name of a TensorFlow operation to a description of the
// API to generate for it, as defined by the ApiDef protocol buffer (
// https://www.tensorflow.org/code/tensorflow/core/framework/api_def.proto)
//
// The ApiDef messages are typically used to generate convenience wrapper
// functions for TensorFlow operations in various language bindings.
typedef struct TF_ApiDefMap TF_ApiDefMap;

// Creates a new TF_ApiDefMap instance.
//
// Params:
//  op_list_buffer - TF_Buffer instance containing serialized OpList
//    protocol buffer. (See
//    https://www.tensorflow.org/code/tensorflow/core/framework/op_def.proto
//    for the OpList proto definition).
//  status - Set to OK on success and an appropriate error on failure.
TF_CAPI_EXPORT extern TF_ApiDefMap* TF_NewApiDefMap(TF_Buffer* op_list_buffer,
                                                    TF_Status* status);

// Deallocates a TF_ApiDefMap.
TF_CAPI_EXPORT extern void TF_DeleteApiDefMap(TF_ApiDefMap* apimap);

// Add ApiDefs to the map.
//
// `text` corresponds to a text representation of an ApiDefs protocol message.
// (https://www.tensorflow.org/code/tensorflow/core/framework/api_def.proto).
//
// The provided ApiDefs will be merged with existing ones in the map, with
// precedence given to the newly added version in case of conflicts with
// previous calls to TF_ApiDefMapPut.
TF_CAPI_EXPORT extern void TF_ApiDefMapPut(TF_ApiDefMap* api_def_map,
                                           const char* text, size_t text_len,
                                           TF_Status* status);

// Returns a serialized ApiDef protocol buffer for the TensorFlow operation
// named `name`.
TF_CAPI_EXPORT extern TF_Buffer* TF_ApiDefMapGet(TF_ApiDefMap* api_def_map,
                                                 const char* name,
                                                 size_t name_len,
                                                 TF_Status* status);

// --------------------------------------------------------------------------
// Kernel definition information.

// Returns a serialized KernelList protocol buffer containing KernelDefs for all
// registered kernels.
TF_CAPI_EXPORT extern TF_Buffer* TF_GetAllRegisteredKernels(TF_Status* status);

// Returns a serialized KernelList protocol buffer containing KernelDefs for all
// kernels registered for the operation named `name`.
TF_CAPI_EXPORT extern TF_Buffer* TF_GetRegisteredKernelsForOp(
    const char* name, TF_Status* status);

// --------------------------------------------------------------------------
// In-process TensorFlow server functionality, for use in distributed training.
// A Server instance encapsulates a set of devices and a Session target that
// can participate in distributed training. A server belongs to a cluster
// (specified by a ClusterSpec), and corresponds to a particular task in a
// named job. The server can communicate with any other server in the same
// cluster.

// In-process TensorFlow server.
typedef struct TF_Server TF_Server;

// Creates a new in-process TensorFlow server configured using a serialized
// ServerDef protocol buffer provided via `proto` and `proto_len`.
//
// The server will not serve any requests until TF_ServerStart is invoked.
// The server will stop serving requests once TF_ServerStop or
// TF_DeleteServer is invoked.
TF_CAPI_EXPORT extern TF_Server* TF_NewServer(const void* proto,
                                              size_t proto_len,
                                              TF_Status* status);

// Starts an in-process TensorFlow server.
TF_CAPI_EXPORT extern void TF_ServerStart(TF_Server* server, TF_Status* status);

// Stops an in-process TensorFlow server.
TF_CAPI_EXPORT extern void TF_ServerStop(TF_Server* server, TF_Status* status);

// Blocks until the server has been successfully stopped (via TF_ServerStop or
// TF_ServerClose).
TF_CAPI_EXPORT extern void TF_ServerJoin(TF_Server* server, TF_Status* status);

// Returns the target string that can be provided to TF_SetTarget() to connect
// a TF_Session to `server`.
//
// The returned string is valid only until TF_DeleteServer is invoked.
TF_CAPI_EXPORT extern const char* TF_ServerTarget(TF_Server* server);

// Destroy an in-process TensorFlow server, frees memory. If server is running
// it will be stopped and joined.
TF_CAPI_EXPORT extern void TF_DeleteServer(TF_Server* server);

// Register a listener method that processes printed messages.
//
// If any listeners are registered, the print operator will call all listeners
// with the printed messages and immediately return without writing to the
// logs.
TF_CAPI_EXPORT extern void TF_RegisterLogListener(
    void (*listener)(const char*));

#ifdef __cplusplus
} /* end extern ""C"" */
#endif

#endif  // TENSORFLOW_C_C_API_H_
```

/usr/local/lib/libtensorflow-cpu-linux-x86_64-1.15.0/include/tensorflow/c/tf_attrtype.h exists
```
/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
#ifndef TENSORFLOW_C_TF_ATTRTYPE_H_
#define TENSORFLOW_C_TF_ATTRTYPE_H_

#ifdef __cplusplus
extern ""C"" {
#endif

// TF_AttrType describes the type of the value of an attribute on an operation.
typedef enum TF_AttrType {
  TF_ATTR_STRING = 0,
  TF_ATTR_INT = 1,
  TF_ATTR_FLOAT = 2,
  TF_ATTR_BOOL = 3,
  TF_ATTR_TYPE = 4,
  TF_ATTR_SHAPE = 5,
  TF_ATTR_TENSOR = 6,
  TF_ATTR_PLACEHOLDER = 7,
  TF_ATTR_FUNC = 8,
} TF_AttrType;

#ifdef __cplusplus
} /* end extern ""C"" */
#endif

#endif  // TENSORFLOW_C_TF_ATTRTYPE_H_
```"
41505,Mixed Precision training is ~10 times slower,"Mixed Precision training is too slow when I use `mixed_float16` policy
**System information**
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow):**
- **Ubuntu 18.04:**
- **TensorFlow installed from pip3:**
- **TensorFlow version 2.2.0:**
- **Python version 3.7:**
- **CUDA version 10.1 cuDNN version 7.5:**
- **GPU model and memory RTX 2080ti 11gb:**

I've designed a segmentation model, which consists of different regular layers like BatchNorm, Conv2D, Activation, etc.. Haven't designed and used any custom layer.
I'm using binary cross-entropy as a loss function and also put `float32` dtype on my network's outputs (like to official doc says), which is a sigmoid function. Data loading is with `float32` as well but **when I train with mixed precision, it gets super slow ~6-10 times**. I use the same network with the same batch size, without changing anything else. I've calculated the time between the processes and here is what it looks like

- **Feedforward is slower ~10x**
- **Loss Computing is slower ~6x**
- **Gradients computing is slower ~6x**

I assume it should be faster than the `float32` policy, but it turns out not. It's super slow. At least I'm expecting it should not be slower and the main advantage would be to take a bigger **batch size**."
41504,Misleading error message in tf.broadcast_to,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0
- Python version: 3.7.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
When running `tf.broadcast_to`, the input shown in the error message for an invalid argument is different from the given input.  `[110, 53, 104, 147, 157, 123, 5, 24, 188, 40, 5, 2]` (given) vs `[2,2,2,2,2,2,2,2,2,2,2,2]` (error message).

**Describe the expected behavior**
Correct error message

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
```python
import tensorflow as tf

x = tf.constant([1, 2, 3])
tf.broadcast_to([x, [110, 53, 104, 147, 157, 123, 5, 24, 188, 40, 5, 2])
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
`InvalidArgumentError: Shape [2,2,2,2,2,2,2,2,2,2,2,2] would have more than 2**63 - 1 elements [Op:BroadcastTo]`
"
41503,'ValueError: need more than 0 values to unpack' in Tensorflow object-detection model training,"I've been dealing with an issue training my Tensorflow custom object-detection model, following [this tutorial](https://pythonprogramming.net/training-custom-objects-tensorflow-object-detection-api-tutorial/)

I keep getting `ValueError: need more than 0 values to unpack` I will attached the full error script.

I am using my mac, running Catalina 10.15.5, with Python 2.7.17, and Tensorflow 1.15.0, which was pip installed.

My directory in terms of applicable files and folders:

- Desktop
    - models
        - research 
            - object_detection
                - legacy
                    - train.py
                - training
                    - object-detection.pbtxt
                    - ssdlite_mobilenet_v3_small_320x320_coco.config
    - Object-Detection
        - data/
            - Test_labels.csv
            - Train_labels.csv
        - Camera pictures/
            - Test/
                - testingimages.jpg
            - Train/
                - trainingimages.jpg
            - ...myimages.jpg
        - training
        - xml_to_csv.py
        - generate_TFrecord.py'

Here is my .config script:
```
# SSDLite with Mobilenet v3 large feature extractor.
# Trained on COCO14, initialized from scratch.
# TPU-compatible.
# Users should configure the fine_tune_checkpoint field in the train config as
# well as the label_map_path and input_path fields in the train_input_reader and
# eval_input_reader. Search for ""PATH_TO_BE_CONFIGURED"" to find the fields that
# should be configured.

model {
  ssd {
    inplace_batchnorm_update: true
    freeze_batchnorm: false
    num_classes: 1
    box_coder {
      faster_rcnn_box_coder {
        y_scale: 10.0
        x_scale: 10.0
        height_scale: 5.0
        width_scale: 5.0
      }
    }
    matcher {
      argmax_matcher {
        matched_threshold: 0.5
        unmatched_threshold: 0.5
        ignore_thresholds: false
        negatives_lower_than_unmatched: true
        force_match_for_each_row: true
        use_matmul_gather: true
      }
    }
    similarity_calculator {
      iou_similarity {
      }
    }
    encode_background_as_zeros: true
    anchor_generator {
      ssd_anchor_generator {
        num_layers: 6
        min_scale: 0.2
        max_scale: 0.95
        aspect_ratios: 1.0
        aspect_ratios: 2.0
        aspect_ratios: 0.5
        aspect_ratios: 3.0
        aspect_ratios: 0.3333
      }
    }
    image_resizer {
      fixed_shape_resizer {
        height: 320
        width: 320
      }
    }
    box_predictor {
      convolutional_box_predictor {
        min_depth: 0
        max_depth: 0
        num_layers_before_predictor: 0
        use_dropout: false
        dropout_keep_probability: 0.8
        kernel_size: 3
        use_depthwise: true
        box_code_size: 4
        apply_sigmoid_to_scores: false
        class_prediction_bias_init: -4.6
        conv_hyperparams {
          activation: RELU_6,
          regularizer {
            l2_regularizer {
              weight: 0.00004
            }
          }
          initializer {
            random_normal_initializer {
              stddev: 0.03
              mean: 0.0
            }
          }
          batch_norm {
            train: true,
            scale: true,
            center: true,
            decay: 0.97,
            epsilon: 0.001,
          }
        }
      }
    }
    feature_extractor {
      type: 'ssd_mobilenet_v3_small'
      min_depth: 16
      depth_multiplier: 1.0
      use_depthwise: true
      conv_hyperparams {
        activation: RELU_6,
        regularizer {
          l2_regularizer {
            weight: 0.00004
          }
        }
        initializer {
          truncated_normal_initializer {
            stddev: 0.03
            mean: 0.0
          }
        }
        batch_norm {
          train: true,
          scale: true,
          center: true,
          decay: 0.97,
          epsilon: 0.001,
        }
      }
      override_base_feature_extractor_hyperparams: true
    }
    loss {
      classification_loss {
        weighted_sigmoid_focal {
          alpha: 0.75,
          gamma: 2.0
        }
      }
      localization_loss {
        weighted_smooth_l1 {
          delta: 1.0
        }
      }
      classification_weight: 1.0
      localization_weight: 1.0
    }
    normalize_loss_by_num_matches: true
    normalize_loc_loss_by_codesize: true
    post_processing {
      batch_non_max_suppression {
        score_threshold: 1e-8
        iou_threshold: 0.6
        max_detections_per_class: 100
        max_total_detections: 100
        use_static_shapes: true
      }
      score_converter: SIGMOID
    }
  }
}

train_config: {
  batch_size: 24
  sync_replicas: true
  startup_delay_steps: 0
  replicas_to_aggregate: 32
  num_steps: 800000
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
  data_augmentation_options {
    ssd_random_crop {
    }
  }
  optimizer {
    momentum_optimizer: {
      learning_rate: {
        cosine_decay_learning_rate {
          learning_rate_base: 0.4
          total_steps: 800000
          warmup_learning_rate: 0.13333
          warmup_steps: 2000
        }
      }
      momentum_optimizer_value: 0.9
    }
    use_moving_average: false
  }
  max_number_of_boxes: 100
  unpad_groundtruth_tensors: false
}

train_input_reader: {
  tf_record_input_reader {
    input_path: ""data/train.record""
  }
  label_map_path: ""training/object-detection.pbtxt""
}

eval_config: {
  num_examples: 18
  # Note: The below line limits the evaluation process to 10 evaluations.
  # Remove the below line to evaluate indefinitely.
  max_evals: 10
}

eval_input_reader: {
  tf_record_input_reader {
    input_path: ""data/test.record""
  }
  label_map_path: ""training/object-detection.pbtxt""
  shuffle: false
  num_readers: 1
}
```
Here is my object-detection.pbtxt script:
```
item {
  id: 1
  name: 'Raspi'
}
```
Here is my train.py script
```
import functools
import json
import os
import tensorflow.compat.v1 as tf
from tensorflow.python.util.deprecation import deprecated


from object_detection.builders import dataset_builder
from object_detection.builders import graph_rewriter_builder
from object_detection.builders import model_builder
from object_detection.legacy import trainer
from object_detection.utils import config_util

tf.logging.set_verbosity(tf.logging.INFO)

flags = tf.app.flags
flags.DEFINE_string('master', '', 'Name of the TensorFlow master to use.')
flags.DEFINE_integer('task', 0, 'task id')
flags.DEFINE_integer('num_clones', 1, 'Number of clones to deploy per worker.')
flags.DEFINE_boolean('clone_on_cpu', False,
                     'Force clones to be deployed on CPU.  Note that even if '
                     'set to False (allowing ops to run on gpu), some ops may '
                     'still be run on the CPU if they have no GPU kernel.')
flags.DEFINE_integer('worker_replicas', 1, 'Number of worker+trainer '
                     'replicas.')
flags.DEFINE_integer('ps_tasks', 0,
                     'Number of parameter server tasks. If None, does not use '
                     'a parameter server.')
flags.DEFINE_string('train_dir', '',
                    'Directory to save the checkpoints and training summaries.')

flags.DEFINE_string('pipeline_config_path', '',
                    'Path to a pipeline_pb2.TrainEvalPipelineConfig config '
                    'file. If provided, other configs are ignored')

flags.DEFINE_string('train_config_path', '',
                    'Path to a train_pb2.TrainConfig config file.')
flags.DEFINE_string('input_config_path', '',
                    'Path to an input_reader_pb2.InputReader config file.')
flags.DEFINE_string('model_config_path', '',
                    'Path to a model_pb2.DetectionModel config file.')

FLAGS = flags.FLAGS


@deprecated(None, 'Use object_detection/model_main.py.')
def main(_):
  assert FLAGS.train_dir, '`train_dir` is missing.'
  if FLAGS.task == 0: tf.gfile.MakeDirs(FLAGS.train_dir)
  if FLAGS.pipeline_config_path:
    configs = config_util.get_configs_from_pipeline_file(
        FLAGS.pipeline_config_path)
    if FLAGS.task == 0:
      tf.gfile.Copy(FLAGS.pipeline_config_path,
                    os.path.join(FLAGS.train_dir, 'pipeline.config'),
                    overwrite=True)
  else:
    configs = config_util.get_configs_from_multiple_files(
        model_config_path=FLAGS.model_config_path,
        train_config_path=FLAGS.train_config_path,
        train_input_config_path=FLAGS.input_config_path)
    if FLAGS.task == 0:
      for name, config in [('model.config', FLAGS.model_config_path),
                           ('train.config', FLAGS.train_config_path),
                           ('input.config', FLAGS.input_config_path)]:
        tf.gfile.Copy(config, os.path.join(FLAGS.train_dir, name),
                      overwrite=True)

  model_config = configs['model']
  train_config = configs['train_config']
  input_config = configs['train_input_config']

  model_fn = functools.partial(
      model_builder.build,
      model_config=model_config,
      is_training=True)

  def get_next(config):
    return dataset_builder.make_initializable_iterator(
        dataset_builder.build(config)).get_next()

  create_input_dict_fn = functools.partial(get_next, input_config)

  env = json.loads(os.environ.get('TF_CONFIG', '{}'))
  cluster_data = env.get('cluster', None)
  cluster = tf.train.ClusterSpec(cluster_data) if cluster_data else None
  task_data = env.get('task', None) or {'type': 'master', 'index': 0}
  task_info = type('TaskSpec', (object,), task_data)

  # Parameters for a single worker.
  ps_tasks = 0
  worker_replicas = 1
  worker_job_name = 'lonely_worker'
  task = 0
  is_chief = True
  master = ''

  if cluster_data and 'worker' in cluster_data:
    # Number of total worker replicas include ""worker""s and the ""master"".
    worker_replicas = len(cluster_data['worker']) + 1
  if cluster_data and 'ps' in cluster_data:
    ps_tasks = len(cluster_data['ps'])

  if worker_replicas > 1 and ps_tasks < 1:
    raise ValueError('At least 1 ps task is needed for distributed training.')

  if worker_replicas >= 1 and ps_tasks > 0:
    # Set up distributed training.
    server = tf.train.Server(tf.train.ClusterSpec(cluster), protocol='grpc',
                             job_name=task_info.type,
                             task_index=task_info.index)
    if task_info.type == 'ps':
      server.join()
      return

    worker_job_name = '%s/task:%d' % (task_info.type, task_info.index)
    task = task_info.index
    is_chief = (task_info.type == 'master')
    master = server.target

  graph_rewriter_fn = None
  if 'graph_rewriter_config' in configs:
    graph_rewriter_fn = graph_rewriter_builder.build(
        configs['graph_rewriter_config'], is_training=True)

  trainer.train(
      create_input_dict_fn,
      model_fn,
      train_config,
      master,
      task,
      FLAGS.num_clones,
      worker_replicas,
      FLAGS.clone_on_cpu,
      ps_tasks,
      worker_job_name,
      is_chief,
      FLAGS.train_dir,
      graph_hook_fn=graph_rewriter_fn)


if __name__ == '__main__':
  tf.app.run()
```
Full error script:
```
$object_detection jp3spinelli$ python legacy/train.py --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v3_small_coco.config --logtostderr

WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/absl/app.py:250: main (from __main__) is deprecated and will be removed in a future version.
Instructions for updating:
Use object_detection/model_main.py.
W0717 13:07:39.904257 4548259264 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/absl/app.py:250: main (from __main__) is deprecated and will be removed in a future version.
Instructions for updating:
Use object_detection/model_main.py.
WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/object_detection-0.1-py2.7.egg/object_detection/legacy/trainer.py:265: create_global_step (from tf_slim.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.create_global_step
W0717 13:07:39.919519 4548259264 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/object_detection-0.1-py2.7.egg/object_detection/legacy/trainer.py:265: create_global_step (from tf_slim.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.create_global_step
WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.
W0717 13:07:39.935688 4548259264 dataset_builder.py:83] num_readers has been reduced to 1 to match input file shards.
WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/object_detection-0.1-py2.7.egg/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0717 13:07:39.944289 4548259264 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/object_detection-0.1-py2.7.egg/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/object_detection-0.1-py2.7.egg/object_detection/builders/dataset_builder.py:175: map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.map()
W0717 13:07:40.005249 4548259264 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/object_detection-0.1-py2.7.egg/object_detection/builders/dataset_builder.py:175: map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.map()
WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/api/_v1/estimator/__init__.py:12: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.

W0717 13:07:40.027405 4548259264 module_wrapper.py:139] From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/api/_v1/estimator/__init__.py:12: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.

WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/object_detection-0.1-py2.7.egg/object_detection/builders/dataset_builder.py:48: make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0717 13:07:42.934035 4548259264 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/object_detection-0.1-py2.7.egg/object_detection/builders/dataset_builder.py:48: make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/object_detection-0.1-py2.7.egg/object_detection/core/preprocessor.py:199: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.
Instructions for updating:
`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.
W0717 13:07:42.989105 4548259264 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/object_detection-0.1-py2.7.egg/object_detection/core/preprocessor.py:199: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.
Instructions for updating:
`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.
WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/object_detection-0.1-py2.7.egg/object_detection/core/box_list_ops.py:231: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
W0717 13:07:43.008011 4548259264 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/object_detection-0.1-py2.7.egg/object_detection/core/box_list_ops.py:231: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/object_detection-0.1-py2.7.egg/object_detection/core/batcher.py:101: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).
W0717 13:07:43.557287 4548259264 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/object_detection-0.1-py2.7.egg/object_detection/core/batcher.py:101: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).
WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow_core/python/training/input.py:752: __init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
W0717 13:07:43.561017 4548259264 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow_core/python/training/input.py:752: __init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow_core/python/training/input.py:752: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
W0717 13:07:43.562012 4548259264 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow_core/python/training/input.py:752: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
Traceback (most recent call last):
  File ""legacy/train.py"", line 186, in <module>
    tf.app.run()
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow_core/python/util/deprecation.py"", line 324, in new_func
    return func(*args, **kwargs)
  File ""legacy/train.py"", line 182, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/object_detection-0.1-py2.7.egg/object_detection/legacy/trainer.py"", line 290, in train
    clones = model_deploy.create_clones(deploy_config, model_fn, [input_queue])
  File ""/Users/jp3spinelli/Desktop/models/research/object_detection/legacy/deployment/model_deploy.py"", line 192, in create_clones
    outputs = model_fn(*args, **kwargs)
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/object_detection-0.1-py2.7.egg/object_detection/legacy/trainer.py"", line 180, in _create_losses
    train_config.use_multiclass_scores)
ValueError: need more than 0 values to unpack
```"
41501,smart_resize in keras preprocessing not compatible with Dataset from tf.data,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):2.4.0-dev20200717
- Python version:3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
Using `smart_resize` on Dataset object:
```
size = (200, 200)
ds = ds.map(lambda img: smart_resize(img, size))
``` 
throws error
```
OperatorNotAllowedInGraphError: in user code:

    <ipython-input-4-1b8d37623c29>:4 None  *
        lambda image: tf.keras.preprocessing.image.smart_resize(image, size))
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/preprocessing/image.py:126 smart_resize  **
        if target_ratio < img_ratio:
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:878 __bool__
        self._disallow_bool_casting()
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:491 _disallow_bool_casting
        self._disallow_in_graph_mode(""using a `tf.Tensor` as a Python `bool`"")
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:480 _disallow_in_graph_mode
        "" this function with @tf.function."".format(task))

    OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.
```

**Describe the expected behavior**
This should work according to documentation (https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/keras/preprocessing/image/smart_resize)

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/1_awoHwxurYy0kM2UNQaUvHuOaMHInRxE?usp=sharing

```
import tensorflow as tf
from tensorflow import keras
import numpy as np
IMG_SIZE=224
size = [IMG_SIZE, IMG_SIZE]

np_image = np.random.rand(32, size[0], size[1], 3)
ds_train = tf.data.Dataset.from_tensor_slices(np_image)
ds_train = ds_train.map(lambda image: tf.keras.preprocessing.image.smart_resize(image, size))
```"
41500,"fit the model with features on GPU (AssertionError: Could not compute output Tensor(""dense_3/Identity:0"", shape=(None, 1), dtype=float32)) ","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

TF version 2.2

**Issue occurs when we call model.fit after creating keras model. We are using tfhub bert layer for its word embeddings ,setting trainable =False** as below:-
```
bert_layer = hub.KerasLayer(""https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2"", trainable=False)
pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])

bert_model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=pooled_output)
```

```
X_train_pooled_output.shape(50,000,768)  and X_test_pooled_output.shape(20,000, 768) batch_size = 32
mode3.fit(X_train_pooled_output, y_train, epochs=nb_epoch,batch_size=batch_size,verbose=1,validation_data = (X_test_pooled_output,y_test),callbacks=callback_list)
```

![image](https://user-images.githubusercontent.com/21074002/87811649-9cc6de00-c87c-11ea-9f77-0d705e230c5e.png)

"
41499,Potential overflow in tf.broadcast_to,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0
- Python version: 3.7.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
When passing a large positive value (`2660446024`) to `shape` to `tf.broadcast_to`, it throws an `InvalidArgumentException`  but the error message  `Dimension -1634521272 must be >= 0 ` complains about a number that was not actually given. It seems there is a potential overflow going on internally. 

**Describe the expected behavior**
Correct error message

**Standalone code to reproduce the issue**
```python
import tensorflow as tf

x = tf.constant([1, 2, 3])
tf.broadcast_to(x, [2660446024])
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

`InvalidArgumentError: Dimension -1634521272 must be >= 0 [Op:BroadcastTo]`

"
41496,RGB to YUV: Conflicting information between example and description,"https://www.tensorflow.org/api_docs/python/tf/image/rgb_to_yuv

The documentation says this function is only well defined if the values are between 0 and 1, but the example uses an input with values greater than 1."
41495,Does TensorFlow Profiler step-time consider overlapping?,"In TensorFlow Profiler overview page, the ""step-time graph"" consists of 8 parts of time, such as ""Input"", ""Host compute"", ""Device compute"". Is it from wall-clock time view, or does it consider each part separately?
For example, if I use tensorflow to invoke a matrix multiply operation on GPU which lasts for 3ms in CUDA kernel, at the same time I use tensorflow to invoke another operator on CPU which lasts for 2ms. Then 2ms of the first GPU computing is overlapped(hided) by the CPU computing, and only the last 1ms is observed. In TensorFlow Profiler, will it show ""Device compute"" as ""3ms"" or as ""1ms""?"
41494,Session->Run (c++) not working if launched in a thread,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):source
- TensorFlow version (use command below):1.15.2
- Python version:3.8
- Bazel version (if compiling from source):0.26.1
- GCC/Compiler version (if compiling from source):5.5
- CUDA/cuDNN version:10.1/7.5
- GPU model and memory:Jetson Xavier


**Describe the current behavior**
I'm working on creating an object detection script using Tensorflow in C++. For this, I've used https://github.com/lysukhin/tensorflow-object-detection-cpp as a start and after some changes to make it faster, I'm able to run it at a very good speed on the Jetson Xavier. But today, I would like to separate the capture and inference task to let them run by themself as threads. The problem is that my ""session ->run"" instruction is not giving me any information about what is going on. The only thing that I know is that my session->run is not working properly because it should output a tensorflow::Status and I'm getting nothing (neither an error). Since there is no error, It is pretty hard for me to find why my run instruction is not doing anything.

**Describe the expected behavior**
Described above.

Thanks for the futur help ! 
"
41493,tf.parallel_stack() doesn't support eager execution,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
Bug example:
```python
x = tf.constant([1, 4])
y = tf.constant([2, 5])
z = tf.constant([3, 6])
tf.parallel_stack([x, y, z])
```
Work example
```python
x = tf.constant([1, 4])
y = tf.constant([2, 5])
z = tf.constant([3, 6])
with tf.compat.v1.Session() as sess:
    print(sess.run(tf.parallel_stack([x, y, z])))
```


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
41492,Cannot download celeb_a dataset from tensorflow_datasets :(,"Running this
```python
(train_data, test_data), info = tfds.load(name = 'celeb_a', split = ['train', 'test'], as_supervised = True, shuffle_files = True, with_info = True)
```
Gives this
```
NonMatchingChecksumError: Artifact https://drive.google.com/uc?export=download&id=0B7EVK8r0v71pZjFTYXZWM3FlRnM, downloaded to /root/tensorflow_datasets/downloads/ucexport_download_id_0B7EVK8r0v71pZjFTYXZWM3FlDDaXUAQO8EGH_a7VqGNLRtW52mva1LzDrb-V723OQN8.tmp.4ec0de7ede1541dca88a21190e298882/uc, has wrong checksum.
```

As far as I know, this issue is there because the dataset is on the google drive."
41491,Failed to load the native TensorFlow runtime,"Getting Below error while initializing tensor flow in python:

**Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common reasons and solutions. Include the entire stack trace above this error message when asking for help.**

Also the console part start flickering as in this error message is displaying for an infinite loop"
41490,Building fails at //tensorflow/python/keras/api:keras_python_api_gen_compat_v1,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian ""sid""
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): NA
- TensorFlow version: r2.3
- Python version: 3.8
- Installed using virtualenv? pip? conda?: NA
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): 8.4.0
- CUDA/cuDNN version: 10.1/7.6.5
- GPU model and memory: NA
I should mention that this box has a CPU without AVX support (which is why I need to build from source).

The build fails almost at the end with this stack trace

```ERROR: /home/hans/src/tensorflow.bak/tensorflow/python/keras/api/BUILD:123:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1 failed (Exit 1)
Traceback (most recent call last):
  File ""/home/hans/.cache/bazel/_bazel_hans/2d6565a8de40a1199bd197af102140f1/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: /home/hans/.cache/bazel/_bazel_hans/2d6565a8de40a1199bd197af102140f1/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZTIN6icu_678ByteSinkE

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/hans/.cache/bazel/_bazel_hans/2d6565a8de40a1199bd197af102140f1/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 26, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""/home/hans/.cache/bazel/_bazel_hans/2d6565a8de40a1199bd197af102140f1/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 40, in <module>
    from tensorflow.python.eager import context
  File ""/home/hans/.cache/bazel/_bazel_hans/2d6565a8de40a1199bd197af102140f1/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/eager/context.py"", line 35, in <module>
    from tensorflow.python import pywrap_tfe
  File ""/home/hans/.cache/bazel/_bazel_hans/2d6565a8de40a1199bd197af102140f1/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tfe.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/hans/.cache/bazel/_bazel_hans/2d6565a8de40a1199bd197af102140f1/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 83, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/hans/.cache/bazel/_bazel_hans/2d6565a8de40a1199bd197af102140f1/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: /home/hans/.cache/bazel/_bazel_hans/2d6565a8de40a1199bd197af102140f1/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZTIN6icu_678ByteSinkE


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /home/hans/src/tensorflow.bak/tensorflow/tools/pip_package/BUILD:66:1 Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1 failed (Exit 1)
INFO: Elapsed time: 32684.441s, Critical Path: 205.34s
INFO: 23720 processes: 23720 local.
FAILED: Build did NOT complete successfully
```
The command was a straight 
`bazel build //tensorflow/tools/pip_package:build_pip_package`

The content of `tf_configure.bazel`

```build --action_env PYTHON_BIN_PATH=""/usr/bin/python3""
build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages""
build --python_path=""/usr/bin/python3""
build --config=xla
build --action_env CUDA_TOOLKIT_PATH=""/usr/local/cuda""
build --action_env TF_CUDA_COMPUTE_CAPABILITIES=""6.1""
build --action_env GCC_HOST_COMPILER_PATH=""/usr/bin/x86_64-linux-gnu-gcc-8""
build --config=cuda
build:opt --copt=-march=native
build:opt --host_copt=-march=native
build:opt --define with_default_optimizations=true
test --flaky_test_attempts=3
test --test_size_filters=small,medium
test --test_env=LD_LIBRARY_PATH
test:v1 --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial
test:v1 --build_tag_filters=-benchmark-test,-no_oss,-no_gpu
test:v2 --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial,-v1only
test:v2 --build_tag_filters=-benchmark-test,-no_oss,-no_gpu,-v1only
build --action_env TF_CONFIGURE_IOS=""0""```"
41489,tf.data.Dataset.get_files produces infinite loop,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows Server 2016
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.14.1
- Python version: 3.7
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): MS Visual Studio 2017
- CUDA/cuDNN version: 10.1/7
- GPU model and memory: GeForce GTX 1080 Ti (11 Go)

**Describe the current behavior**
When using tf.data.Dataset to produce a dataset from a set of files (jpg images), it seems that it is unable to find any file or to build the dataset correctly. By looping through the dataset and printing each element, the output is an infinite list of ""Tensor(""IteratorGetNext_N:0"", shape=(), dtype=string)"" where N is the number of iterations. The problem occurs either by using directly Dataset.list_files(""/path/to/*.jpg"") of by using glob independently and giving the result to Dataset.from_tensor_slices. The problem also occurs when iterating through a small number of elements by using Dataset.take().
I know I am supposed to upgrade my TensorFlow version, but I have for now to stick to the 1.14 version because all the software we distribute in my company is linked to this particular version.

**Describe the expected behavior**
It is supposed to produce a dataset of approximately 1000 jpg images.

**Standalone code to reproduce the issue**
`import tensorflow as tf`
`test_dataset = tf.data.Dataset.list_files(""D:/test/*.jpg"")`
`for element in test_dataset:`
`	print(element)`

**Other info / logs**
Attached is a zip file containing 3 jpg images in a ""test"" directory to reproduce the issue. It works with any file anyway.
[test.zip](https://github.com/tensorflow/tensorflow/files/4936851/test.zip)
"
41488,XLA NMS bug,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

https://colab.research.google.com/drive/1IyYxIvXTPdxgMI1Q4F99WEftzXqvv91X?usp=sharing

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
```
InvalidArgumentError: Function invoked by the following node is not compilable: {{node __inference_test_32}} = __inference_test_32[_XlaMustCompile=true, config_proto=""\n\007\n\003CPU\020\001\n\007\n\003GPU\020\0002\002J\0008\001"", executor_type=""""](dummy_input, dummy_input, dummy_input, dummy_input, dummy_input, dummy_input).
Uncompilable nodes:
NonMaxSuppressionV5: unsupported op: No registered 'NonMaxSuppressionV5' OpKernel for XLA_CPU_JIT devices compatible with node {{node NonMaxSuppressionV5}}
	Stacktrace:
		Node: __inference_test_32, function: 
		Node: NonMaxSuppressionV5, function: __inference_test_32
 [Op:__inference_test_32]
```
**Describe the expected behavior**
Tf1.15 doesn't have the bug
**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
41486,"error: the following arguments are required: -p/--prototxt, -m/--model","![python_error](https://user-images.githubusercontent.com/39181530/87752493-20051700-c81e-11ea-828f-fa42efab335e.png)

Sir i am new to this field and would like to resolve this issue as soon as possible i am using as it is i have not done any modification in code and i will be interested to find out solution  if any sir, kindly help ,  I am using preloaded video and using on spyder IDE for processing so kindly guide me about this error. thank you."
41485,FL16 model run on GPU,"**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- Tensorflow version (commit SHA if source): 1.15
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):Android 9 api28 ,Mali-T864 GPU

**Describe the problem**
We tried to run a post-quantized (to float16) model on a robot with GPU delegate according to https://www.tensorflow.org/lite/performance/gpu and https://medium.com/tensorflow/tensorflow-model-optimization-toolkit-float16-quantization-halves-model-size-cc113c75a2fa
 but it fails to run on GPU even after we graph transformed non-GPU supported operators in it. The logs is attached. Interesting thing is if we do not quantize it to fl16, all operators of the model can successfully run on GPU. Netron shows there are lots of 'dequantize' operators added to the graph after we use tflite converter to quantize the model to fl16. So what should we do to let the quantized fl16 model run on GPU entirely?

One more question is we found a parameter SetAllowFp16PrecisionForFp32 in tflite c++. What is the difference between 1).set this to true and use a fl32 model. 2). set this to true and use fl16 model. 3). set this to false and use fl32 model. 4) set this to false and use fl16 model?

Many thanks.

Model is uploaded in:
https://drive.google.com/drive/folders/18B4Wx4BEPxfptsTmIEZySwILLZNXbE2v?usp=sharing
Inputs are image of size 193*321*3

**Please provide the exact sequence of commands/steps when you ran into the problem**
INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite delegate for GPU.
ERROR: Next operations are not supported by GPU delegate:
CONV_2D: Expected 1 input tensor(s), but node has 3 runtime input(s).
DEPTHWISE_CONV_2D: Expected 1 input tensor(s), but node has 3 runtime input(s).
DEQUANTIZE: Operation is not supported.
First 0 operations will run on the GPU, and the remaining 198 on the CPU.







"
41484,I'm having issue when trying to convert keras model to json,"I found an error where it could not find my keras model (input path) and the output path, I think it's because of the space character between ""My Drive"", since I was using google drive to load and save the model. And I think you can't change the name ""My Drive"". 
So Instead of using google drive, I was using the local google colab's drive. So then I uploaded the keras model and convert it and save it in the google colab's drive and downloaded it. 

So If it's a bug, I hope in the future it will be fixed. Thank you.

**Environment**
- Google Colab

**System information**
- TensorFlow 2.2
- tensorflowjs ver2.0.1.post1

**Code**
from google.colab import drive
drive.mount('/content/gdrive')

!tensorflowjs_converter --input_format=keras '/content/gdrive/My Drive/model/model.h5' '/content/gdrive/My Drive/json'

**Error**
usage: TensorFlow.js model converters. [-h]
                                       [--input_format {keras,tf_frozen_model,keras_saved_model,tf_hub,tfjs_layers_model,tf_saved_model}]
                                       [--output_format {keras_saved_model,tfjs_graph_model,tfjs_layers_model,keras}]
                                       [--signature_name SIGNATURE_NAME]
                                       [--saved_model_tags SAVED_MODEL_TAGS]
                                       [--quantize_float16 [QUANTIZE_FLOAT16]]
                                       [--quantize_uint8 [QUANTIZE_UINT8]]
                                       [--quantize_uint16 [QUANTIZE_UINT16]]
                                       [--quantization_bytes {1,2}]
                                       [--split_weights_by_layer] [--version]
                                       [--skip_op_check]
                                       [--strip_debug_ops STRIP_DEBUG_OPS]
                                       [--weight_shard_size_bytes WEIGHT_SHARD_SIZE_BYTES]
                                       [--output_node_names OUTPUT_NODE_NAMES]
                                       [--control_flow_v2 CONTROL_FLOW_V2]
                                       [input_path] [output_path]
TensorFlow.js model converters.: error: unrecognized arguments: /content/gdrive/My Drive/json
"
41482,Access denied when loading frozen inference graph,"------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Home
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: No
-   **TensorFlow installed from (source or binary)**: Source
-   **TensorFlow version (use command below)**: 1.14.0
-   **Python version**: 3.6.7
-   **Bazel version (if compiling from source)**: Not using Bazel
-   **GCC/Compiler version (if compiling from source)**: Not using GCC
-   **CUDA/cuDNN version**: CUDA/cuDNN: 10.0
-   **GPU model and memory**: NVIDIA GeForce GTX 1080, 12G
-   **Exact command to reproduce**:
image_data = tf.gfile.FastGFile('ssd_mobilenet_v1_coco_2017_11_17/output_inference_graph_v1.pb', 'rb').read() 

**Error:**
`Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\rzimm\anaconda3\envs\rapidrzr\lib\site-packages\tensorflow\python\lib\io\file_io.py"", line 122, in read
    self._preread_check()
  File ""C:\Users\rzimm\anaconda3\envs\rapidrzr\lib\site-packages\tensorflow\python\lib\io\file_io.py"", line 84, in _preread_check
    compat.as_bytes(self.__name), 1024 * 512)
tensorflow.python.framework.errors_impl.UnknownError: NewRandomAccessFile failed to Create/Open: ssd_mobilenet_v1_coco_2017_11_17\output_inference_graph_v1.pb : Access is denied.
; Input/output error
>>> image_data = tf.gfile.FastGFile('ssd_mobilenet_v1_coco_2017_11_17/output_inference_graph_v1.pb', 'rb').read()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\rzimm\anaconda3\envs\rapidrzr\lib\site-packages\tensorflow\python\lib\io\file_io.py"", line 122, in read
    self._preread_check()
  File ""C:\Users\rzimm\anaconda3\envs\rapidrzr\lib\site-packages\tensorflow\python\lib\io\file_io.py"", line 84, in _preread_check
    compat.as_bytes(self.__name), 1024 * 512)
tensorflow.python.framework.errors_impl.UnknownError: NewRandomAccessFile failed to Create/Open: ssd_mobilenet_v1_coco_2017_11_17/output_inference_graph_v1.pb : Access is denied.`

### Describe the problem
I tried using the Object Detection Tutorial Notebook to classify some images, but instead of using the downloaded model, I substituted by another model I had trained. When trying to substitute, it returns an error, not allowing me to use the model I had trained with Tensorflow.

### Source code / logs

"
41480,"Server terminated abruptly (error code: 14, error message: 'Socket closed'","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):   Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):  source
- TensorFlow version:  2.4.0
- Python version: 3.5
- Installed using virtualenv? pip? conda?:  docker
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 9.0
- GPU model and memory:  Tesla P100 16G

Here is my Dockerfile :

FROM nvidia/cuda:9.0-cudnn7-devel-ubuntu16.04

RUN apt-get update

RUN apt-get install -y wget \
                vim \
                cmake


RUN apt-get -y install \
    python3 \
    python3-pip \
    python3-setuptools

Here is my configure:
You have bazel 3.1.0 installed.
Please specify the location of python. [Default is /usr/bin/python3]:


Found possible Python library paths:
  /usr/lib/python3/dist-packages
  /usr/local/lib/python3.5/dist-packages
Please input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with TensorRT support? [y/N]: n
No TensorRT support will be enabled for TensorFlow.

Found CUDA 9.0 in:
    /usr/local/cuda-9.0/targets/x86_64-linux/lib
    /usr/local/cuda-9.0/targets/x86_64-linux/include
Found cuDNN 7 in:
    /usr/lib/x86_64-linux-gnu
    /usr/include


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]:


Do you want to use clang as CUDA compiler? [y/N]: n
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]:


Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]:


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=ngraph         # Build with Intel nGraph support.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v2             # Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=nonccl         # Disable NVIDIA NCCL support.
Configuration finished


Here is my compile command:
bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

and then I get the error:
   ^
[15,648 / 26,555] 16 actions running
    Compiling tensorflow/compiler/xla/service/hlo_instructions.cc [for host]; 102s local
    Compiling tensorflow/compiler/xla/service/hlo_computation.cc [for host]; 102s local
    Compiling tensorflow/compiler/xla/service/hlo_instruction.cc [for host]; 101s local
    Compiling tensorflow/compiler/xla/service/hlo_parser.cc [for host]; 97s local
    Compiling tensorflow/compiler/xla/service/hlo_domain_map.cc [for host]; 96s local
    Compiling tensorflow/compiler/xla/service/hlo_query.cc [for host]; 93s local
    Compiling tensorflow/compiler/xla/service/channel_tracker.cc [for host]; 93s local
    Compiling tensorflow/compiler/xla/service/collective_ops_utils.cc [for host]; 92s local ...

Server terminated abruptly (error code: 14, error message: 'Socket closed', log file: '/root/.cache/bazel/_bazel_root/7c482caa6d82184234d890a1494c53ec/server/jvm.out')

I search this error in issue, but I not found the answer. How can I  fix this bug?

By the way, I can compile success in win10 system, use the same tensorflow source, 
and the wheel named ""tensorflow-2.4.0-cp36-cp36m-win_amd64.whl""


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
41478,"tf.ResizeNearestNeighbor not support convert to tensorflow lite ,i dont know how to slove it","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
Exception: <unknown>:0: error: loc(fused[""llyolo/up_sampling2d/resize/ResizeNearestNeighbor@__inference__wrapped_model_15290"", ""StatefulPartitionedCall/llyolo/up_sampling2d/resize/ResizeNearestNeighbor""]): 'tf.ResizeNearestNeighbor' op is neither a custom op nor a flex op
<unknown>:0: error: loc(fused[""llyolo/up_sampling2d_1/resize/ResizeNearestNeighbor@__inference__wrapped_model_15290"", ""StatefulPartitionedCall/llyolo/up_sampling2d_1/resize/ResizeNearestNeighbor""]): 'tf.ResizeNearestNeighbor' op is neither a custom op nor a flex op
<unknown>:0: error: loc(fused[""llyolo/up_sampling2d_2/resize/ResizeNearestNeighbor@__inference__wrapped_model_15290"", ""StatefulPartitionedCall/llyolo/up_sampling2d_2/resize/ResizeNearestNeighbor""]): 'tf.ResizeNearestNeighbor' op is neither a custom op nor a flex op
<unknown>:0: error: loc(fused[""llyolo/up_sampling2d_3/resize/ResizeNearestNeighbor@__inference__wrapped_model_15290"", ""StatefulPartitionedCall/llyolo/up_sampling2d_3/resize/ResizeNearestNeighbor""]): 'tf.ResizeNearestNeighbor' op is neither a custom op nor a flex op
<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag): ResizeNearestNeighbor,ResizeNearestNeighbor,ResizeNearestNeighbor,ResizeNearestNeighbor.
"
41474,CycleGAN tranining pipeline in Documentation failing silently with ipython kernel dying,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2.0
- Python version: 3.6.9
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10.1 /  7.6.4.38-1+cuda10.1
- GPU model and memory: GTX1080Ti / 12GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
When training a custom img2img cycleGAN on 4600+ images in each domain (9300+ images in total), the ipython kernel of the jupyter notebook dies silently some time into training.
**Describe the expected behavior**
Training proceeds normally.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
Due the data problem, it cannot really be reproduced elsewhere.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

The one log I find fishy:

```
 The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the
 partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to 
`dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
```
This is the code I have:
```
train_A = train_A.map(
    preprocess_image_train, num_parallel_calls=AUTOTUNE).cache().shuffle(
    BUFFER_SIZE).batch(1)
```
I think I should change it to:
```
train_A = train_A.map(
    preprocess_image_train, num_parallel_calls=AUTOTUNE).batch(1).cache().shuffle(
    BUFFER_SIZE)
```
Is that correct?

Thank you!"
41470,"Model Maker Outputing Single File, no labels.txt","Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

https://www.tensorflow.org/lite/tutorials/model_maker_image_classification
Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod

## Description of issue (what needs changing):

The tutorial for Tensorflow Model Maker says that on export, there will be a model.tflite file and then a labels.txt file. However, when I export the model using the instructions, it only outputs a single model.tflite. The console output says that it is stored in a temp directory (which appears to be subsequently deleted), and that the labels are merged into the model.tflite file. Would I still be able to use this on mobile or is there any way I can extract labels.txt?

### Clear description

For example, why should someone use this method? How is it useful?

### Correct links

Is the link to the source code correct?

### Parameters defined

Are all parameters defined and formatted correctly?

### Returns defined

Are return values defined?

### Raises listed and defined

Are the errors defined? For example,
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises

### Usage example

Is there a usage example?

See the API guide: https://www.tensorflow.org/community/contribute/docs_ref
on how to write testable usage examples.

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs,
docs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
41468,tf.esitmator.Estimator extremely slow with tf.data.Dataset,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Pip
- TensorFlow version (use command below): 1.15.2
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0
- GPU model and memory: 2080 ti

**Describe the current behavior**
I have constructed a dataset which can read raw waveform and extract acoustic feature. Then I send it to tf.estimator.train_and_evaluate. But I find the GPU utils is 0 most of the time. It will go to peak periodically (but the period is very long, about half an hour). Based on my observation, I find the data reading is always running, and the reading and extraction take about 3 seconds per audios. I use 60 parallel workers , and set the batch size as 4.

**Describe the expected behavior**
Based on my understanding, the dataset read and process batch by batch. So it should feed the data to GPU after it finish processing the batch. In my case, since I use 60 parallel workers and set batch size as 4, it should take only couple of seconds to read the data, and then send the batch to GPU. But now I see data loading is always running, while GPU util is 0 most of time.

**Standalone code to reproduce the issue**
The system is pretty complicated (it is an open source tool called spleeter: https://github.com/deezer/spleeter). I can give some key codes for showing the points that may have problems:

1. The training code is
```
tf.estimator.train_and_evaluate(
        estimator,
        train_spec,
        evaluation_spec)
    ModelProvider.writeProbe(params['model_dir'])
```

2. The estimator is
```

    estimator = tf.estimator.Estimator(
        model_fn=model_fn,
        model_dir=params['model_dir'],
        params=params,
        config=tf.estimator.RunConfig(
            save_checkpoints_steps=params['save_checkpoints_steps'],
            tf_random_seed=params['random_seed'],
            save_summary_steps=params['save_summary_steps'],
            session_config=session_config,
            log_step_count_steps=10,
            keep_checkpoint_max=2))
```

3. The train_spec code:
```
def _create_train_spec(params, audio_adapter, audio_path):
    """""" Creates train spec.

    :param params: TF params to build spec from.
    :returns: Built train spec.
    """"""
    input_fn = partial(get_training_dataset, params, audio_adapter, audio_path)
    train_spec = tf.estimator.TrainSpec(
        input_fn=input_fn,
        max_steps=params['train_max_steps'])
    return train_spec
```

4. The `get_training_dataset` code:
```
def get_training_dataset(audio_params, audio_adapter, audio_path):
    """""" Builds training dataset.

    :param audio_params: Audio parameters.
    :param audio_adapter: Adapter to load audio from.
    :param audio_path: Path of directory containing audio.
    :returns: Built dataset.
    """"""
    builder = DatasetBuilder(
        audio_params,
        audio_adapter,
        audio_path,
        chunk_duration=audio_params.get('chunk_duration', 20.0),
        random_seed=audio_params.get('random_seed', 0))
    return builder.build(
        audio_params.get('train_csv'),
        cache_directory=audio_params.get('training_cache'),
        batch_size=audio_params.get('batch_size'),
        n_chunks_per_song=audio_params.get('n_chunks_per_song', 2),
        random_data_augmentation=False,
        convert_to_uint=True,
        wait_for_cache=False)
```

5. The dataset builder code:
```
    def build(
            self, csv_path,
            batch_size=8, shuffle=True, convert_to_uint=True,
            random_data_augmentation=False, random_time_crop=True,
            infinite_generator=True, cache_directory=None,
            wait_for_cache=False, num_parallel_calls=60, n_chunks_per_song=2,):
        """"""
        TO BE DOCUMENTED.
        """"""
        dataset = dataset_from_csv(csv_path)
        dataset = self.compute_segments(dataset, n_chunks_per_song)
        # Shuffle data
        if shuffle:
            dataset = dataset.shuffle(
                buffer_size=200000,
                seed=self._random_seed,
                # useless since it is cached :
                reshuffle_each_iteration=False)
        # Expand audio path.
        dataset = dataset.map(self.expand_path, num_parallel_calls=AUTOTUNE)
        # Load waveform, compute spectrogram, and filtering error,
        # K bins frequencies, and waveform.
        N = num_parallel_calls
        for instrument in self.instruments:
            dataset = (
                dataset
                .map(instrument.load_waveform, num_parallel_calls=N)
                .filter(self.filter_error)
                .map(instrument.compute_spectrogram, num_parallel_calls=N)
                .map(instrument.filter_frequencies,num_parallel_calls=AUTOTUNE))
        dataset = dataset.map(self.filter_waveform, num_parallel_calls=AUTOTUNE)
        # Convert to uint before caching in order to save space.
        if convert_to_uint:
            for instrument in self.instruments:
                dataset = dataset.map(instrument.convert_to_uint, num_parallel_calls=AUTOTUNE)
        dataset = self.cache(dataset, None, wait_for_cache)
        # Check for INFINITY (should not happen)
        for instrument in self.instruments:
            dataset = dataset.filter(instrument.filter_infinity)
        # Repeat indefinitly
        if infinite_generator:
            dataset = dataset.repeat(count=-1)
        # Ensure same size for vocals and mix spectrograms.
        # NOTE: could be done before caching ?
        dataset = dataset.map(self.harmonize_spectrogram, num_parallel_calls=AUTOTUNE)
        # Filter out too short segment.
        # NOTE: could be done before caching ?
        dataset = dataset.filter(self.filter_short_segments)
        # Random time crop of 11.88s
        if random_time_crop:
            dataset = dataset.map(self.random_time_crop, num_parallel_calls=N)
        else:
            # frame_duration = 11.88/T
            # take central segment (for validation)
            for instrument in self.instruments:
                dataset = dataset.map(instrument.time_crop, num_parallel_calls=AUTOTUNE)
        # Post cache shuffling. Done where the data are the lightest:
        # after croping but before converting back to float.
        if shuffle:
            dataset = dataset.shuffle(
                buffer_size=256, seed=self._random_seed,
                reshuffle_each_iteration=False)
        # Convert back to float32
        if convert_to_uint:
            for instrument in self.instruments:
                dataset = dataset.map(
                    instrument.convert_to_float32, num_parallel_calls=N)
        M = 8  # Parallel call post caching.
        # Must be applied with the same factor on mix and vocals.
        if random_data_augmentation:
            dataset = (
                dataset
                .map(self.random_time_stretch, num_parallel_calls=M)
                .map(self.random_pitch_shift, num_parallel_calls=M))
        # Filter by shape (remove badly shaped tensors).
        for instrument in self.instruments:
            dataset = (
                dataset
                .filter(instrument.filter_shape)
                .map(instrument.reshape_spectrogram, num_parallel_calls=AUTOTUNE))
        # Select features and annotation.
        dataset = dataset.map(self.map_features, num_parallel_calls=AUTOTUNE)
        # Make batch (done after selection to avoid
        # error due to unprocessed instrument spectrogram batching).
        dataset = dataset.batch(batch_size)
        return dataset.prefetch(AUTOTUNE)
```

Thank you very much!"
41467,Implement LSH-based Methods for Enhanced CPU-Only Performance,"I am writing to inquire if the Tensorflow team has any interest or plans for implementing locality sensitive hashing based optimizations for enhanced CPU only performance and reduced overall computational resource consumption as detailed in this paper? 

https://www.cs.rice.edu/~as143/Papers/SLIDE_MLSys.pdf

It would appear that these techniques have reached a level maturity to consider implementation and that this would be a great benefit to reducing the GPU barrier of entry for developers, reduce complexity and expense for both training and deploying large deep learning models, and also reduce waste in terms of not only hardware expense but also energy usage and ecological impact.
"
41466,Error,"Got this error.

ImportError                               Traceback (most recent call last)
/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59 

/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in <module>
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

/opt/anaconda3/lib/python3.7/imp.py in load_module(name, file, filename, details)
    241         else:
--> 242             return load_dynamic(name, filename, file)
    243     elif type_ == PKG_DIRECTORY:

/opt/anaconda3/lib/python3.7/imp.py in load_dynamic(name, path, file)
    341             name=name, loader=loader, origin=path)
--> 342         return _load(spec)
    343 

ImportError: dlopen(/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation
  Referenced from: /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.2.dylib
  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security
 in /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.2.dylib

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-6-dc0837bd74eb> in <module>
----> 1 from batchglm.api.models.tf1.glm_nb import Simulator
      2 
      3 sim = Simulator(num_observations=200, num_features=100)
      4 sim.generate_sample_description(num_batches=0, num_conditions=2)
      5 sim.generate_params(

/opt/anaconda3/lib/python3.7/site-packages/batchglm/api/models/tf1/__init__.py in <module>
----> 1 from . import glm_beta
      2 from . import glm_nb
      3 from . import glm_norm

/opt/anaconda3/lib/python3.7/site-packages/batchglm/api/models/tf1/glm_beta.py in <module>
      1 from batchglm.models.glm_beta import InputDataGLM, Model, Simulator
----> 2 from batchglm.train.tf1.glm_beta import Estimator

/opt/anaconda3/lib/python3.7/site-packages/batchglm/train/tf1/glm_beta/__init__.py in <module>
----> 1 from .estimator import Estimator
      2 from .estimator_graph import EstimatorGraph
      3 from .model import BasicModelGraph, ModelVars, ProcessModel
      4 from .hessians import Hessians
      5 from .fim import FIM

/opt/anaconda3/lib/python3.7/site-packages/batchglm/train/tf1/glm_beta/estimator.py in <module>
      3 
      4 import numpy as np
----> 5 import tensorflow as tf
      6 
      7 from .external import TFEstimatorGLM, InputDataGLM, Model

/opt/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py in <module>
     39 import sys as _sys
     40 
---> 41 from tensorflow.python.tools import module_util as _module_util
     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader
     43 

/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/__init__.py in <module>
     48 import numpy as np
     49 
---> 50 from tensorflow.python import pywrap_tensorflow
     51 
     52 # Protocol buffers

/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>
     67 for some common reasons and solutions.  Include the entire stack trace
     68 above this error message when asking for help."""""" % traceback.format_exc()
---> 69   raise ImportError(msg)
     70 
     71 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/opt/anaconda3/lib/python3.7/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/opt/anaconda3/lib/python3.7/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: dlopen(/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation
  Referenced from: /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.2.dylib
  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security
 in /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.2.dylib
"
41465,Updating labels during online augmentations - ImageGenerator + imgaug,"Hi, 

I have been using imgaug with ImageGenerator's preprocessing_function for augmentations while training. This works well for non-geometric augmentations. 

My labels include location, orientation, etc, and they need to change when I apply geometric transformations like flip/rotate. The preprocessing_function takes in just one argument: ""image"", so I do not have the label information at the time of augmentation, in this callback function.

Is there a way to do this? Other that pre-augmenting the images.

(I am on TensorFlow 2.2, Windows 10)

"
41461,tf.contrib.framework.assign_from_values in v2,"## Description of issue (what needs changing):
I'm running inference on a model that used tf1, and I've converted most of the code to v2 but I can't seem to find anywhere the proper v2 (or even tf.compat.v1) version of framework.assign_from_values. I'm just getting the error
`AttributeError: module 'tensorflow.compat.v1' has no attribute 'contrib'`
"
41460,tf.keras.preprocessing.image.apply_affine_transform shifts tx as height and ty as width,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04 LTS
- TensorFlow version (use command below): 2.2.0
- Python version: 3.x
- CUDA/cuDNN version: 10.1
- GPU model and memory: GeForce GTX 1070 8GB

**Describe the current behavior**
When shifting the image on x axis using `apply_affine_transform` output is shifted on y axis (and vice versa).

![00137](https://user-images.githubusercontent.com/11685462/87704058-11761b80-c79c-11ea-8b07-2ab05fe88fa7.jpg)

```
import tensorflow as tf
import cv2
import matplotlib.pyplot as plt
img = cv2.imread('00137.jpg')
img2 = tf.keras.preprocessing.image.apply_affine_transform(img, tx = 100, row_axis=0, col_axis=1)
plt.imshow(img2)
```

![Screenshot from 2020-07-16 19-14-43](https://user-images.githubusercontent.com/11685462/87704113-29e63600-c79c-11ea-8b33-8644a83ddc4e.png)

Switch in row_axis/col_axis is not doing anything.

**Describe the expected behavior**
Setting `tx` should shift image along `x` axis

**Standalone code to reproduce the issue**
Code in the example above
"
41459,TF to s3a:// access needed,"There is funcionality for basic `s3://` scheme on Linux. Then there is work for tensorflow/community#101 to convert all these filesystem to a plugin based form so they can be used on all platforms, on demand.

Regarding `s3a` and `s3n`, can you file a new issue, feature request type, and assign it to me/mention me? just to keep this issue only for the `s3` not implemented error.

_Originally posted by @mihaimaruseac in https://github.com/tensorflow/tensorflow/issues/40302#issuecomment-659517610_"
41458,tf.random.categorical overflow before OOM on GPU,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 LTS
- TensorFlow version (use command below): 2.2.0 and 2.1.0
- Python version: 3.x
- CUDA/cuDNN version: 10.1
- GPU model and memory: V100 32GB

**Describe the current behavior**

When running the `tf.random.categorical` with a large `num_samples` on a V100 with 32GB we get the following crash:
```bash
2020-07-15 21:58:02.621909: F ./tensorflow/core/util/gpu_launch_config.h:129] Check failed: work_element_count > 0 (-2099511296 vs. 0)
Aborted (core dumped)
```
This looks like an overflow before raising the OOM error.

Here a minimalist code which reproduces the aborted (core dumped) on the V100 with 32GB:
```python
import tensorflow as tf
probs = tf.random.uniform((2 ** 15,), dtype=tf.float64)
probs = probs / tf.reduce_sum(probs)
logits = tf.math.log(probs)
samples = tf.random.categorical(logits[tf.newaxis], 70000, dtype=tf.int64)[0]
```

Here some tests on different cards:
- V100 32GB, cuda 10.1, tf2.2 and tf2.1: Aborted (core dumped)
- V100 16GB, cuda 10.1, tf2.1: OOM
- TitanV 12GB, cuda 10.2, tf2.2: OOM
- RTX2080TI, 12GB, cuda 10.2, tf2.2: OOM

This issue only happens on V100 with 32GB.

**Describe the expected behavior**

The code should raise a Resource Exhausted OOM error instead of being aborted."
41457,macos not raising OOM error,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Catalina 10.15
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2.0
- Python version: 3.x

**Describe the current behavior**

When allocating a large tensor on macos the code is killed by the kernel instead of raising a OOM error.

**Describe the expected behavior**

The code above should raise a Resource Exhausted OOM error, as it does on Linux.

**Standalone code to reproduce the issue**

Here a minimal example:
```python
import tensorflow as tf
tf.ones(2**40, dtype=tf.complex128)
```"
41455,I'm trying to convert a custom model on inception V3 + SSD and running into the same issue. Any workarounds to get the model to tflite format until this is officially fixed?,"I'm trying to convert a custom model on inception V3 + SSD and running into the same issue. Any workarounds to get the model to tflite format until this is officially fixed?

_Originally posted by @divSivasankaran in https://github.com/tensorflow/tensorflow/issues/18731#issuecomment-398985955_

Has someone resolved this issue?, I want to convert ssd_inception_v2_coco and  throws  the OperatorType::kMerge Found StridedSlice as non-selected output from Switch, but only Merge supported."
41453,"After Installing Tensorflow By Using(pip install tensorflow), I got this error while importing.","ImportError                               Traceback (most recent call last)
D:\Unity\Unity\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59 

D:\Unity\Unity\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in <module>
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

D:\Unity\Unity\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

D:\Unity\Unity\lib\imp.py in load_module(name, file, filename, details)
    241         else:
--> 242             return load_dynamic(name, filename, file)
    243     elif type_ == PKG_DIRECTORY:

D:\Unity\Unity\lib\imp.py in load_dynamic(name, path, file)
    341             name=name, loader=loader, origin=path)
--> 342         return _load(spec)
    343 

ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-1-d6579f534729> in <module>
----> 1 import tensorflow

D:\Unity\Unity\lib\site-packages\tensorflow\__init__.py in <module>
     39 import sys as _sys
     40 
---> 41 from tensorflow.python.tools import module_util as _module_util
     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader
     43 

D:\Unity\Unity\lib\site-packages\tensorflow\python\__init__.py in <module>
     48 import numpy as np
     49 
---> 50 from tensorflow.python import pywrap_tensorflow
     51 
     52 # Protocol buffers

D:\Unity\Unity\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     67 for some common reasons and solutions.  Include the entire stack trace
     68 above this error message when asking for help."""""" % traceback.format_exc()
---> 69   raise ImportError(msg)
     70 
     71 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""D:\Unity\Unity\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""D:\Unity\Unity\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""D:\Unity\Unity\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""D:\Unity\Unity\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""D:\Unity\Unity\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
41452,InvalidArgumentError: Only ranks up to 5 supported,"Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS inux release 7.6.1810
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.14.0
Python version: 3.7.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: V9.1.85
GPU model and memory: Tesla K80 24C

I am working with Keras to build the following (simplified) model:
```
class MyModel():
   def __init__(self):
       pass
   
   def build_model(self):
       inp = Input(shape = self.outputs.shape)
        
       x = Dense(128, activation = 'elu')(inp) # We'll start exploring with one hidden layer
       outp = Dense(1, activation = 'elu')(x)
        
       self.model = Model(inputs = inp, outputs = outp)

       self.model.compile(loss = dice_loss,
                            optimizer = Adam(),
                            metrics = [dice_coef])

   def run_model(self):
      # For the sake of simplicity
      self.outputs = np.random.rand(8,64,12,86,98,1)      

      # labels shape = (64, 12, 86, 98) to get (1,64,12,86,98)
      self.labels = np.expand_dims(self.labels, axis = (0,-1))
      # outputs shape (8,64,12,86,98,1) to -> (1,64,12,86,98,8)
      self.outputs = np.swapaxes(self.outputs, 0,-1)
      
      self.build_model()

      self.model.fit([self.outputs, self.labels],
                                batch_size = 1,
                                epochs = 200)
```
However, upon trying to fit the model, as:
```
model = MyModel()
model.run_model()
```
The following error appears:
```
  File ""/home/kdqm927/miniconda3/envs/segment/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_v1.py"", line 785, in fit
    use_multiprocessing=use_multiprocessing)
  File ""/home/kdqm927/miniconda3/envs/segment/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py"", line 634, in fit
    shuffle=shuffle)
  File ""/home/kdqm927/miniconda3/envs/segment/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_v1.py"", line 2308, in _standardize_user_data
    batch_size=batch_size)
  File ""/home/kdqm927/miniconda3/envs/segment/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_v1.py"", line 2335, in _standardize_tensors
    exception_prefix='input')
  File ""/home/kdqm927/miniconda3/envs/segment/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py"", line 573, in standardize_input_data
    'with shape ' + str(data_shape))
ValueError: Error when checking input: expected input_1 to have 7 dimensions, but got array with shape (1, 64, 12, 86, 98, 8)
```
If I tweak the code by adding another dimension to self.outputs, following the exception from the previous error:
```
self.model.fit([np.expand_dims(self.outputs, axis = 0), self.labels],
                                batch_size = 1,
                                epochs = 200)
```
The error I get is: 
```
Traceback (most recent call last):
  File ""./file.py"", line 257, in <module>
    s.fit_metamodel('outputs_validation_arr')
  File ""./file.py"", line 157, in fit_metamodel
    callbacks = callbacks)
  File ""/home/.../miniconda3/envs/segment/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_v1.py"", line 785, in fit
    use_multiprocessing=use_multiprocessing)
  File ""/home/.../miniconda3/envs/segment/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py"", line 666, in fit
    steps_name='steps_per_epoch')
  File ""/home/.../miniconda3/envs/segment/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py"", line 386, in model_iteration
    batch_outs = f(ins_batch)
  File ""/home/.../miniconda3/envs/segment/lib/python3.7/site-packages/tensorflow/python/keras/backend.py"", line 3632, in __call__
    run_metadata=self.run_metadata)
  File ""/home/.../miniconda3/envs/segment/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1472, in __call__
    run_metadata_ptr)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Only ranks up to 5 supported: [1,1,64,12,86,98,128]
	 [[{{node dense/BiasAdd}}]]
```
**EDIT**
TensorFlow version (use command below): 2.2.0
****"
41451,"LUP factorization (tensorflow.linalg.lu) throws ""Input is not invertible"" error.","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Mint 20
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not tested
- TensorFlow installed from (source or binary): binary. Installed tensorflow-2.2.0-cp38-cp38-manylinux2010_x86_64.whl from https://www.wheelodex.org/projects/tensorflow/
- TensorFlow version (use command below):
- Python version: v2.2.0-rc4-8-g2b96f3662b
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: Cuda: 10.1.243 and cuDNN: 7.6.5
- GPU model and memory: GTX 1050 (Notebook) with 4GB

When I am trying to compute the LUP factorization of a particular type of rank deficient square matrix using tensorflow.linalg.lu, I am getting the following error regarding the input being non-invertible:

```python
Traceback (most recent call last):
  File ""LU_bug.py"", line 10, in <module>
    lu, _ = tf.linalg.lu(A_tensor)
  File ""/home/touqir/pyenvs/py3.8/lib/python3.8/site-packages/tensorflow/python/ops/gen_linalg_ops.py"", line 1183, in lu
    _ops.raise_from_not_ok_status(e, name)
  File ""/home/touqir/pyenvs/py3.8/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 6653, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: Input is not invertible. [Op:Lu]
```

The following code reproduces the above error message:
```python
import numpy as np
import tensorflow as tf

A = np.random.rand(4,6)
m, n = A.shape
A_tensor = np.zeros(shape=[m+n, m+n], dtype=A.dtype)
A_tensor[:m, :n] = A
A_tensor[-n:, -m:] = A.T
A_tensor = tf.convert_to_tensor(A_tensor)
lu, _ = tf.linalg.lu(A_tensor)
```

However, LUP factorization does not require the input to be a full rank matrix. ie. not invertible. I have tried with other types of rank deficient square matrices for which tensorflow's LUP factorization works fine.

"
41450,use_multiprocessing=True with data generators gives error `CUDA_ERROR_NOT_INITIALIZED: initialization error`,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux 7 (Core)
- TensorFlow installed from (source or binary): Source (conda)
- TensorFlow version (use command below): 2.2.0
- Python version: 3.7.6
- CUDA/cuDNN version: CUDA 10.1.243/cuDNN 7.6.5
- GPU model and memory: NVIDIA Quadro P4000, 8GB

**Describe the current behaviour**

I'm using a custom data loader to train a U-Net and passing `use_multiprocessing=True` to `model.fit`. Training proceeds as expected, but occasionally freezes mid-training with the error `tensorflow/stream_executor/cuda/cuda_driver.cc:219] Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error`. So far, the error has not appeared when I use `use_multiprocessing=False`, which leads me to believe it's caused by multiprocessing.

I'm not sure I can provide code that reproduces this error on another system, but any possible fixes would be appreciated.
"
41449,'ReadVariableOp:value:0 is not found' when converting savedModel to TFLite,"**System information**
- OS Platform and Distribution: OS X 10.14.6
- TensorFlow installed from: Binary (pip3)
- TensorFlow version: 2.2.0

My initial issue was very similar to this:
https://stackoverflow.com/questions/58499146/how-do-i-convert-tensorflow-2-0-estimator-model-to-tensorflow-lite
so having followed the advice there and at issue #34350 I am running the following code.

```
    saved_model_obj = tf.saved_model.load(export_dir=args.model_dir)
    concrete_func = saved_model_obj.signatures['serving_default']

    converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])

    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.experimental_new_converter = True
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,
                                           tf.lite.OpsSet.SELECT_TF_OPS]

    tflite_model = converter.convert()
```

**The output from the converter invocation**

```
Traceback (most recent call last):
  File ""converter.py"", line 32, in <module>
    main()
  File ""converter.py"", line 26, in main
    tflite_model = converter.convert()
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/lite/python/lite.py"", line 472, in convert
    graph=frozen_func.graph)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/lite/python/util.py"", line 218, in run_graph_optimizations
    return tf_optimizer.OptimizeGraph(config, meta_graph)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/grappler/tf_optimizer.py"", line 58, in OptimizeGraph
    graph_id, strip_default_attributes)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Return readvariableop -> ReadVariableOp:value:0 is not found.
        In function output readvariableop:float
```

The full output -including the function_optimizer lines- and the saved model are attached.  The saved model was exported from an estimator using estimator.export_saved_model().

[full_output.txt](https://github.com/tensorflow/tensorflow/files/4931075/full_output.txt)
[saved_model.pb.zip](https://github.com/tensorflow/tensorflow/files/4931060/saved_model.pb.zip)

Many thanks in advance.



"
41448,tf.keras cannot weight classes when using multiple outputs,"This post is a mirror of https://github.com/keras-team/keras/issues/11735, showing the need to handle class weight for multiple outputs.

Version 2.2.0 used.

------
 
This is a minimal source code, by @GalAvineri, to reproduce the issue (please comment/uncomment the class weight line):

````python3
from tensorflow.python.keras.models import Model
from tensorflow.python.keras.layers import Input, Dense
from tensorflow.python.data import Dataset
import tensorflow as tf
import numpy as np


def preprocess_sample(features, labels):
    label1, label2 = labels
    label1 = tf.one_hot(label1, 2)
    label2 = tf.one_hot(label2, 3)
    return features, (label1, label2)


batch_size = 32

num_samples = 1000
num_features = 10

features = np.random.rand(num_samples, num_features)
labels1 = np.random.randint(2, size=num_samples)
labels2 = np.random.randint(3, size=num_samples)

train = Dataset.from_tensor_slices((features, (labels1, labels2))).map(preprocess_sample).batch(batch_size).repeat()

# Model
inputs = Input(shape=(num_features, ))
output1 = Dense(2, activation='softmax', name='output1')(inputs)
output2 = Dense(3, activation='softmax', name='output2')(inputs)
model = Model(inputs, [output1, output2])

model.compile(loss='categorical_crossentropy', optimizer='adam')
class_weights = {'output1': {0: 1, 1: 10}, 'output2': {0: 5, 1: 1, 2: 10}}
model.fit(train, epochs=10, steps_per_epoch=num_samples // batch_size,
         #  class_weight=class_weights
          )
````

Uncommenting yields this error:
```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-38-d137ff6fb3f9> in <module>
     33 class_weights = {'output1': {0: 1, 1: 10}, 'output2': {0: 5, 1: 1, 2: 10}}
     34 model.fit(train, epochs=10, steps_per_epoch=num_samples // batch_size,
---> 35            class_weight=class_weights
     36           )

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
     64   def _method_wrapper(self, *args, **kwargs):
     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
---> 66       return method(self, *args, **kwargs)
     67 
     68     # Running inside `run_distribute_coordinator` already.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
    813           workers=workers,
    814           use_multiprocessing=use_multiprocessing,
--> 815           model=self)
    816 
    817       # Container that configures and calls `tf.keras.Callback`s.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)
   1115     dataset = self._adapter.get_dataset()
   1116     if class_weight:
-> 1117       dataset = dataset.map(_make_class_weight_map_fn(class_weight))
   1118     self._inferred_steps = self._infer_steps(steps_per_epoch, dataset)
   1119     self._dataset = strategy.experimental_distribute_dataset(dataset)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py in _make_class_weight_map_fn(class_weight)
   1233         ""Expected `class_weight` to be a dict with keys from 0 to one less ""
   1234         ""than the number of classes, found {}"").format(class_weight)
-> 1235     raise ValueError(error_msg)
   1236 
   1237   class_weight_tensor = ops.convert_to_tensor_v2(

ValueError: Expected `class_weight` to be a dict with keys from 0 to one less than the number of classes, found {'output1': {0: 1, 1: 10}, 'output2': {0: 5, 1: 1, 2: 10}}
````"
41446,windows build issues(finally made it),"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10 x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.4.0
- Python version: 3.8.3 x64
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): 3.4.1
- GCC/Compiler version (if compiling from source): visual studio 2019
- CUDA/cuDNN version: 11.0/8.0.1
- GPU model and memory: RTX2070 GDDR6 8GB



**Describe the problem**
many problems with windows build, and I made a few fixes

**Provide the exact sequence of commands / steps that you executed before running into the problem**
1. cudnn_stub  dependency problem ( https://github.com/tensorflow/tensorflow/issues/41057 )
I simply add dependency in that BUILD

2. then thrust version unmatch error
I delete thrust version - cub version comparison codes from
bazel-out\x64_windows-opt\bin\external\local_config_cuda\cuda\cuda\include\thrust\system\cuda\config.h

3. finally, big old numpy problem
this has been very long and unsolvable error, I made same fix as before( simply pip uninstall numpy ( uninstall conda numpy ) and pip install numpy )


and still there are too many ""ignoring unknown options"" from cl


anyway now I succeeded build and have my wheel file


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached."
41445,Errors in tf.Session().run or tf.reduce_max,"Question:
opt = sess.run(pred_DFS, feed_dict = {x: clinic_factors_val, keep_prob: 1.0, treatment: treat)}) 
opt1, Pat_pred = sess.run([output1,pred_DFS], feed_dict = {x: clinic_factors_val), keep_prob: 1.0, treatment: treat)}) 
print(opt1)
print(opt)
print(Pat_pred)
The relationship between pred_DFS and output1: pred_DFS = tf.reduce_max(output1,axis=1). output1 is an intermediate variable.

When pred_DFS is run alone, the result is wrong, but when pred_DFS and output1 are run together, the result of pred_DFS is correct. Why?

the result of a run：
opt1：[[0.23930925 0.36091098 0.61886203]]
opt：[0.72315866]
Pat_pred：[0.61886203]
Note that，the results are consistent when output1 is run alone and when pred_DFS and output1 are run together"
41444, LSTM crash randomly on Windows 10: Failed to call ThenRnnBackward with model config,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.2
- Python version: 3.7
- CUDA/cuDNN version: Cuda 11, cudnn 7
- GPU model and memory: rtx 2070, 8GB

**Describe the current behavior**
Hi, this is a follow up issue for this one : #37942 
The proposed answers might help, but the core if the issue is still not found. The only known solution so far is to switch the OS, which is not sufficient.

When using LSTM networks on windows 10, model training crashes randomly with an error message like:
```
Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 20, 250, 1, 910, 4, 250] 
	 [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]
	 [[PartitionedCall_3]] [Op:__inference_train_function_13679]
```

**Describe the expected behavior**

I would expect the code to either throw an comprehensible error message on a certain error or run through well.
But currently it is throwing error randomly, sometimes after a few steps, sometimes after some epochs.

**Standalone code to reproduce the issue**
@jlebar (thank you!) created a example notebook, which you can find [here](https://gist.github.com/jliebers/995c3c4da4ad2a6f9376d31ee2470ec5)
However, I was able to train that net without problem for 15 epochs, while in the example it crashed after three.
So you can find the code with my repository [here](https://github.com/MichaelJanz/random-LSTM-Crash-TF-Example)


**Other info / logs**

This is my terminal output:
```
[[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]
2020-07-16 08:35:27.443854: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:118] None of the MLIR optimization passes are enabled (registered 1)
2020-07-16 08:35:28.281334: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-07-16 08:35:28.507116: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:118] None of the MLIR optimization passes are enabled (registered 1)
2020-07-16 08:35:28.527048: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
2020-07-16 08:35:29.449350: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:118] None of the MLIR optimization passes are enabled (registered 1)
2020-07-16 08:35:29.482832: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:118] None of the MLIR optimization passes are enabled (registered 1)
2020-07-16 08:35:29.510930: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:118] None of the MLIR optimization passes are enabled (registered 1)
2020-07-16 08:35:30.417794: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:118] None of the MLIR optimization passes are enabled (registered 1)
2020-07-16 08:35:30.459687: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:118] None of the MLIR optimization passes are enabled (registered 1)
2020-07-16 08:35:30.509665: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:118] None of the MLIR optimization passes are enabled (registered 1)
2020-07-16 08:35:30.560799: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:118] None of the MLIR optimization passes are enabled (registered 1)
2020-07-16 08:35:31.524564: E tensorflow/stream_executor/dnn.cc:616] CUDNN_STATUS_EXECUTION_FAILED
in tensorflow/stream_executor/cuda/cuda_dnn.cc(1957): 'cudnnRNNBackwardData( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, output_desc.handles(), output_data.opaque(), output_desc.handles(), output_backprop_data.opaque(), output_h_desc.handle(), output_h_backprop_data.opaque(), output_c_desc.handle(), output_c_backprop_data.opaque(), rnn_desc.params_handle(), params.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), input_desc.handles(), input_backprop_data->opaque(), input_h_desc.handle(), input_h_backprop_data->opaque(), input_c_desc.handle(), input_c_backprop_data->opaque(), workspace.opaque(), workspace.size(), reserve_space_data->opaque(), reserve_space_data->size())'
2020-07-16 08:35:31.525235: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at cudnn_rnn_ops.cc:1922 : Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 20, 250, 1, 910, 4, 250]
2020-07-16 08:35:31.525583: W tensorflow/core/common_runtime/executor.cc:1086] [/job:localhost/replica:0/task:0/device:GPU:0] Executor start aborting: Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 20, 250, 1, 910, 4, 250]
         [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]
2020-07-16 08:35:31.526101: W tensorflow/core/common_runtime/executor.cc:1086] [/job:localhost/replica:0/task:0/device:GPU:0] Executor start aborting: Internal: {{function_node __inference___backward_gpu_lstm_with_fallback_8535_8711_specialized_for_PartitionedCall_3_at___inference_train_function_13679}} {{function_node __inference___backward_gpu_lstm_with_fallback_8535_8711_specialized_for_PartitionedCall_3_at___inference_train_function_13679}} Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 20, 250, 1, 910, 4, 250]
         [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]
         [[PartitionedCall_3]]
```
"
41443,SERLU activation function support,"here, SERLU activation function works better than elu or RELU or leakyRELU

https://www.researchgate.net/figure/Subplot-a-shows-6-different-activation-functions-where-their-main-differences-sit-in_fig1_326646583

I wonder, if tensorflow can officially support SERLU in next update"
41442,"model_from_json return deserialize(config, custom_objects=custom_objects), How to remove this error?","**I am using this code:**

```
import numpy as np
import cv2
from keras.preprocessing import image
import time

#-----------------------------
#opencv initialization

face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')

#-----------------------------
#face expression recognizer initialization
from keras.models import model_from_json
model = model_from_json(open(""facial_expression_model_structure.json"", ""r"").read())
model.load_weights('facial_expression_model_weights.h5') #load weights
#-----------------------------
```
**I am getting this error:**

```
File ""emotion-analysis-from-video.py"", line 15, in <module>
    model = model_from_json(open(""facial_expression_model_structure.json"", ""r"").read())
  File ""C:\Users\DILSHAD\Desktop\lfe\lib\site-packages\tensorflow\python\keras\saving\model_config.py"", line 116, in model_from_json
    return deserialize(config, custom_objects=custom_objects)
  File ""C:\Users\DILSHAD\Desktop\lfe\lib\site-packages\tensorflow\python\keras\layers\serialization.py"", line 105, in deserialize
    return deserialize_keras_object(
  File ""C:\Users\DILSHAD\Desktop\lfe\lib\site-packages\tensorflow\python\keras\utils\generic_utils.py"", line 361, in deserialize_keras_object
    (cls, cls_config) = class_and_config_for_serialized_keras_object(
  File ""C:\Users\DILSHAD\Desktop\lfe\lib\site-packages\tensorflow\python\keras\utils\generic_utils.py"", line 325, in class_and_config_for_serialized_keras_object
    for key, item in cls_config.items():
AttributeError: 'list' object has no attribute 'items'
```
**Here is the result of my pip freeze:**

```
absl-py==0.9.0
astunparse==1.6.3
cachetools==4.1.1
certifi==2020.6.20
chardet==3.0.4
cycler==0.10.0
decorator==4.4.2
efficientnet==1.1.0
gast==0.3.3
google-auth==1.19.1
google-auth-oauthlib==0.4.1
google-pasta==0.2.0
grpcio==1.30.0
h5py==2.10.0
idna==2.10
imageio==2.9.0
Keras==2.4.3
Keras-Applications==1.0.8
Keras-Preprocessing==1.1.2
kiwisolver==1.2.0
Markdown==3.2.2
matplotlib==3.2.2
networkx==2.4
numpy==1.19.0
oauthlib==3.1.0
opencv-contrib-python==4.3.0.36
opt-einsum==3.2.1
Pillow==7.2.0
protobuf==3.12.2
pyasn1==0.4.8
pyasn1-modules==0.2.8
pyparsing==2.4.7
python-dateutil==2.8.1
PyWavelets==1.1.1
PyYAML==5.3.1
requests==2.24.0
requests-oauthlib==1.3.0
rsa==4.6
scikit-image==0.17.2
scipy==1.4.1
six==1.15.0
tensorboard==2.2.2
tensorboard-plugin-wit==1.7.0
tensorflow==2.2.0
tensorflow-estimator==2.2.0
termcolor==1.1.0
tifffile==2020.7.4
urllib3==1.25.9
Werkzeug==1.0.1
wrapt==1.12.1
```
I run this code about a month ago It runs without any error! Now I created a new virtualenv for it and now it is not working fine!!! Any actionable help with be appreciated!!!"
41440,TypeError: Unsupported return value from function passed to Dataset.map(): OrderedDict in TFF (multiple feature inputs),"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Red Hat 7.7
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): TF 2.2
- Python version: 3.7

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.

I use TFF and want to create an element_spec as follows:
```
OrderedDict([('x', OrderedDict([
('a', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), 
('b', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), 
('c', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), 
('d', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), 
('e', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), 
('f', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), 
('g', TensorSpec(shape=(None,), dtype=tf.int64, name=None))])), 
('y', TensorSpec(shape=(None,), dtype=tf.int64, name=None))])
```
To this end, I create a preprocess function below:
```
def preprocess(dataset):

  def batch_format_fn(self):

    return collections.OrderedDict(
        x=collections.OrderedDict(
	    a=tf.TensorSpec(shape=[None,], dtype=tf.int64),
	    b=tf.TensorSpec(shape=[None,], dtype=tf.int64),
            c=tf.TensorSpec(shape=[None,], dtype=tf.int64),
            d=tf.TensorSpec(shape=[None,], dtype=tf.int64),
            e=tf.TensorSpec(shape=[None,], dtype=tf.int64),
            f=tf.TensorSpec(shape=[None,], dtype=tf.int64)),
        y=tf.TensorSpec(shape=[None,], dtype=tf.int64))
  
  return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER).batch(BATCH_SIZE).map(batch_format_fn)
```
And I call this function as follows:
```
sample_dataset = train_dataset.create_tf_dataset_for_client(train_dataset.client_ids[0])
preprocessed_sample_dataset = preprocess(sample_dataset)
```

However, when I run the program, it gives me an error:
```
Traceback (most recent call last):
  File ""/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 3171, in _wrapper_helper
    self._output_structure = structure.type_spec_from_value(ret)
  File ""/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/data/util/structure.py"", line 439, in type_spec_from_value
    return ctor([(k, type_spec_from_value(v)) for k, v in element.items()])
  File ""/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/data/util/structure.py"", line 439, in <listcomp>
    return ctor([(k, type_spec_from_value(v)) for k, v in element.items()])
  File ""/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/data/util/structure.py"", line 439, in type_spec_from_value
    return ctor([(k, type_spec_from_value(v)) for k, v in element.items()])
  File ""/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/data/util/structure.py"", line 439, in <listcomp>
    return ctor([(k, type_spec_from_value(v)) for k, v in element.items()])
  File ""/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/data/util/structure.py"", line 466, in type_spec_from_value
    (element, type(element).__name__))
TypeError: Could not build a TypeSpec for TensorSpec(shape=(None,), dtype=tf.int64, name=None) with type TensorSpec

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""accident_modify_uk_final3b.py"", line 269, in <module>
    preprocessed_sample_dataset = preprocess(sample_dataset)
  File ""accident_modify_uk_final3b.py"", line 267, in preprocess
    return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER).batch(BATCH_SIZE).map(batch_format_fn)
  File ""/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1621, in map
    return MapDataset(self, map_func, preserve_cardinality=True)
  File ""/homeanaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 3981, in __init__
    use_legacy_function=use_legacy_function)
  File ""/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 3221, in __init__
    self._function = wrapper_fn.get_concrete_function()
  File ""/homeanaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 2532, in get_concrete_function
    *args, **kwargs)
  File ""/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 2496, in _get_concrete_function_garbage_collected
    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
  File ""/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 2777, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 2667, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 981, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 3214, in wrapper_fn
    ret = _wrapper_helper(*args)
  File ""/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 3177, in _wrapper_helper
    sys.exc_info()[2])
  File ""/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/six.py"", line 702, in reraise
    raise value.with_traceback(tb)
  File ""/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 3171, in _wrapper_helper
    self._output_structure = structure.type_spec_from_value(ret)
  File ""/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/data/util/structure.py"", line 439, in type_spec_from_value
    return ctor([(k, type_spec_from_value(v)) for k, v in element.items()])
  File ""/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/data/util/structure.py"", line 439, in <listcomp>
    return ctor([(k, type_spec_from_value(v)) for k, v in element.items()])
  File ""/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/data/util/structure.py"", line 439, in type_spec_from_value
    return ctor([(k, type_spec_from_value(v)) for k, v in element.items()])
  File ""/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/data/util/structure.py"", line 439, in <listcomp>
    return ctor([(k, type_spec_from_value(v)) for k, v in element.items()])
  File ""/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/data/util/structure.py"", line 466, in type_spec_from_value
    (element, type(element).__name__))
TypeError: Unsupported return value from function passed to Dataset.map(): OrderedDict([('x', OrderedDict([('a', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('b', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('c', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('d', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('e', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('f', TensorSpec(shape=(None,), dtype=tf.int64, name=None))])), ('y', TensorSpec(shape=(None,), dtype=tf.int64, name=None))]).
(env1_TF2.1) [@phoenix9 accidents UK]$ 
```
What is wrong inside the preprocess function()?"
41439,Tensorflow model benchmarking with keras model file,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): TF2.2
- Are you willing to contribute it (Yes/No): Yes

Add keras model benchmarking in tensorflow benchmarking tool (tensorflow/tools/benchmark)

**Describe the feature and the current behavior/state.**

Currently tensorflow benchmarking tool under tensorflow/tools/benchmark supports only graph files. 

**Will this change the current api? How?**

**Who will benefit with this feature?**

**Any Other info.**
"
41438,Issue with running post quantized TF Lite on ArmNN,"I am have following setup:

Original Model: Retrained Mobilenetv2 with lesser classes (using TF-Keras APIs)
Converted Model: Post quantized- Full integer quantization- uint8 TFLite Model (using TF-Keras APIs)
Tensorflow version: 1.15

I am trying to run this model on a GPU using ArmNN support.

_I faced the following issue:_

Optimize graph and load the optimized graph onto a compute device...
Warning: WARNING: Layer of type Convolution2d is not supported on requested backend GpuAcc for input data type QAsymmS8 and output data type QAsymmS8 (reason: in validate src/runtime/CL/functions/CLGEMMConvolutionLayer.cpp:423: Input data type not compatible with Weights), falling back to the next backend.
Warning: ERROR: Layer of type Convolution2d is not supported on any preferred backend [GpuAcc ]

_I got the following reply from ArmNN git:_

That error occurs if the weights are QSymm8 with per channel quantization (that's QSYMM8_PER_CHANNEL) and the input is not QAsymm8. In this case your input is QAsymmS8 so it's not supported right now.
In order to use the GpuAcc you would have to retrain the model to use QASymm8 instead of QAsymmS8 or not to use per channel quantization on your weights.

**Link**: https://github.com/ARM-software/armnn/issues/425

I don't quite understand how I can do the above mentioned using the TFLite Converter. Please help. Thanks in advance"
41437,tf.math.argmin and tf.math.argmax don't support complex,"**System information**
- OS Platform and Distribution: Windows10 1909
- TensorFlow installed from : binary
- TensorFlow version: 2.2.0rc2
- Python version: 3.8.0
- CUDA/cuDNN version: None

**Describe the current behavior**
The documentation says that input of `argmax` must be one of the following types: float32, float64, int32, uint8, int16, int8, **complex64**, int64, qint8, quint8, qint32, bfloat16, uint16, **complex128** , half, uint32, uint64.
However, I encounter problem when I use complex tensors as input.

**Standalone code to reproduce the issue**
argmin
```python3
a = tf.constant([1j], dtype=tf.complex64)
b = tf.math.argmin(a)
```
argmax
```python3
a = tf.constant([1j], dtype=tf.complex64)
b = tf.math.argmax(a)
```
**Output**
```
2020-07-16 10:27:30.370246: F tensorflow/compiler/xla/literal_util.cc:289] Unhandled primitive type 15
Aborted (core dumped)
```
"
41430,Training hangs when performing an assign op on aggregated gradients in SyncReplicasOptimizer,"I'm trying to fetch the aggregated gradients which are computed after aggregating all the replicas together in SyncReplicasOptimizer. To do this, I added an assign op on a variable that I created and assigned the aggregated gradients value (from the aggregated_grads_and_vars). Basically in sync_replicas_optimizer.py apply_gradients() call I did something like:

```
var1 = tf.Variable(0.0, trainable=False)
def apply_gradients(grads_and_vars, global_step):
   
   *aggregated_grads_and_vars returned based on replicas_to_aggregate*
   aggregated_grads_and_vars = zip(aggregated_grad, var_list)
   
   test_var = tf.assign(var1, aggregated_gradients)

   *execute everything else in the fn and return train_op*
```

Later, I call *session.run(train_op)* to execute a training steps followed by  *session.run(test_var)*.
My training process goes through with the first training and then hangs. The problem is with *session.run(test_var)*. 

My approach works when I use any other asynchronous optimizer (tested on *MomentumOptimizer*, *AdamOptimizer* and *GradientDescentOptimizer*) but not for *SyncReplicasOptimizer*.

Could this have something to do with *SyncReplicasOptimizerHook* ? 

Any lead would be greatly appreciated.

"
41428,Python Keras code out of memory for no apparent reason,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu Server 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): conda
- TensorFlow version (use command below): 2.2.0
- Python version: 3.7
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: 10.2 / 7.6.5
- GPU model and memory: Geforce GTX Titan X 12 GB RAM, 16 GB system RAM

**Describe the current behavior**
See this Stackoverflow question for details: https://stackoverflow.com/questions/62917349/python-keras-code-out-of-memory-for-no-apparent-reason

**Describe the expected behavior**
The code shouldn't cause Python to crash with code 137. I have over 12 GB system RAM available and nothing else is using the GPU.

**Standalone code to reproduce the issue**

Code causes Python to crash with exit code 137 around 1 (sometimes a bit more, sometimes a bit less) iteration of the outer for loop. System memory usage keeps increasing while the program is running.

```
import tensorflow as tf
from sklearn.datasets import fetch_openml
from sklearn.utils import shuffle

data, targets = shuffle(*fetch_openml('CIFAR_10', version=1, return_X_y=True)) # same happens if I force these to be float32s
train_sz = 50000
X_train, X_test, y_train, y_test = data[:train_sz, :], data[train_sz:, :], np.asarray(targets[:train_sz], dtype=np.int), np.asarray(targets[train_sz:], dtype=np.int)

model = tf.keras.Sequential()
model.add(tf.keras.Input(shape=(X_train.shape[1],)))
model.add(tf.keras.layers.Dense(64, activation='relu'))
model.add(tf.keras.layers.Dense(10))
model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer='adam')

s = 0
for _ in range(500):
    for i in range(100):
        layers = []
        for layer in model.get_weights():
            layers.append(np.random.normal(0, 1, layer.shape))
        model.set_weights(layers)
        eval = model.evaluate(X_train, y_train)
        s += eval
        print(f'Done {i}')
print(s)
```"
41425,Cannot Run Tensorflow2.0 saved model with Java ,"In java, cannot feed the input tensor to the loaded model (model which is saved in tf2.0 in .pb file):

### My model: (tf2.0)

```
Model: ""sequential""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
nn_input (Flatten)           (None, 5)                0         
_________________________________________________________________
...
... 
...  
_________________________________________________________________
nn_output (Dense)            (None, 1)                 513       
=================================================================
```

### My java code:

```
        final int NUM_PREDICTIONS = 1;
        final int INPUT_SIZE = 5;
        try (SavedModelBundle b = SavedModelBundle.load(""./nn_visualization/NNRegressor_saved_model/"", ""serve"")) {
            Session sess = b.session();
            Tensor x = Tensor.create(new long[] { INPUT_SIZE }, FloatBuffer.wrap(new float[] {10, 9, 23, 7, 1}));
            float[] y = sess.runner().feed(""nn_input"", x).fetch(""nn_output"").run().get(0).copyTo(new float[NUM_PREDICTIONS]);
            System.out.println(y[0]);
        }

```

### error:

```
Exception in thread ""main"" java.lang.IllegalArgumentException: No Operation named [nn_input] in the Graph
	at org.tensorflow.Session$Runner.operationByName(Session.java:380)
	at org.tensorflow.Session$Runner.parseOutput(Session.java:389)
	at org.tensorflow.Session$Runner.feed(Session.java:131)
	at HelloTensorFlow.main(HelloTensorFlow.java:25)
```
### Instalations

```
$java
jdk version: 1.8

$ tf model save version: 
python3 -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
v2.2.0-rc4-8-g2b96f3662b 2.2.0

$ java used tensorflow version:
TensorFlow.version(): 1.14.0
```

### Assumptions

I assume that the problem arises due to the difference in the tensorflow versions in which the model is saved (tf2.0) and in which the model is loaded (tf1.4)?

I guess I'm wrong with the first argument of the feed method. Any help?

Thanks in advance,
Milan"
41424,[MLIR] HLO to LHLO conversion and fusion,"I see that recently there has been lot of work from @bondhugula for HLO to LHLO conversion. I am wondering what the plan here is w.r.t fusion. Is the fusion being done in HLO or LHLO? I remember a previous PR added fusion to XLA-HLO (using OpInterfaces), is that being used? Is the plan to do fusion in LHLO, which operates on memrefs?

The reason I ask is that in IREE we have been relying heavily on fusion at tensor level rather than fusion at buffer level. For elementwise operations this has worked fairly well so far. These two passes implement the fusion that covers a large number of elementwise-operations and their fusion
1) The conversion from HLO to Linalg on tensors [here](https://github.com/tensorflow/tensorflow/blob/6e7992cae2f4af31ebc9753d7baf21ede69c39b3/tensorflow/compiler/mlir/hlo/lib/Dialect/mhlo/transforms/legalize_to_linalg.cc#L905)
2) Fusion of linalg operations on tensors [here](https://github.com/llvm/llvm-project/blob/4f763b2172c591ab253c8489fcd53af0c544d5cb/mlir/lib/Dialect/Linalg/Transforms/Fusion.cpp#L1025)

The latter covers fusion of generic ops, indexed generic ops and tensor reshape ops with each other. Fusion at tensor level is much easier to do, and the fusion is effectively doing a fixed-point iteration to do maximal fusion. (There might be cases where maximal fusion can be detrimental, but I haven't encountered such a case when element-wise operations are involved). I wanted to see if it would benefit the overall codegen in Tensorflow by using these passes. They aren't complete and might need enhancements to cover additional cases we haven't encountered in IREE, but so far for fusion elementwise operations, these two passes together have worked very well for us. I am trying to see if we can align codegen strategies between IREE and TF so that we can build more shared components. 

@stellaraccident , @sherhut , @joker-eph , @antiagainst , @nicolasvasilache for comments/visiblity.
"
41422,Tensorflow keras timeseries prediction with X and y having different shapes,"
I am trying to do time series prediction with tensorflow and keras with `X` and `y` having different dimensions:

```
X.shape = (5000, 12)
y.shape = (5000, 3, 12)
```
When I do the following

```
n_input = 7
generator = TimeseriesGenerator(X, y, length=n_input, batch_size=1)

for i in range(5):
    x_, y_ = generator[i]
    print(x_.shape)
    print(y_.shape)
```
I get as desired the output

```
(1, 7, 12)
(1, 3, 12)
(1, 7, 12)
(1, 3, 12)
...
```
This is because my data is meteorological, I have 5000 days, for training in the array `X` I use a sliding window of 7 days, with each day containing 12 features (air pressure, temperature, humidity a.o.). And in the target array `y` I have sliding windows of 3 days, trying to predict the next 3 days to each window of 7 days.

But then when I try to fit the model I get an error due to the mismatch in the shape of the `X` and `y` arrays:

```
model = Sequential()
model.add(LSTM(4, input_shape=(None, 12)))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
history = model.fit_generator(generator, epochs=3).history
```

`ValueError: A target array with shape (1, 3, 12) was passed for an output of shape (None, 1) while using as loss ""mean_squared_error"". This loss expects targets to have the same shape as the output.`

So is there a way to adjust the architecture for the mismatch in the dimensions? Or is there a way to reshape `X` and y to make them work with this architecture? I tried the late reshaping `X` into (5000, 7, 12), but this gave also a dimensionality error. Tnx"
41420,Docs: incomplete description of callbacks.ModelCheckpoint's save_best_only parameter,"## URL(s) with the issue:
https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint

## Description of issue (what needs changing):

1. `mode`: ""{auto, min, max}"" should be `{auto, min, max}`, I guess (minor)
2.  `save_best_only`: I believe the description is incomplete. With `save_best_only=True`, not only will ""the latest best model [...] not be overwritten"": but also the current model is not written at all, even if it has another filename than the ""latest best model"". This is kind of implied by the name of the parameter, but the description should include that, too."
41419,Output of tf.keras.utils.Sequence is automatically cast to Tensor even if it is a SparseTensor,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Catalina 10.15.5
- TensorFlow installed from (source or binary): binary 
- TensorFlow version (use command below): 2.2.0
- Python version: 3.8


**Describe the current behavior**
All outputs produced by `tensorflow.keras.utils.Sequence` are automatically cast to `Tensor` when calling `model.fit()`, even if they are of class `SparseTensor`.
This makes it impossible to use sparse operations in a model if the model is trained with the Keras training loop. 

**Describe the expected behavior**
`model.fit()` should not automatically cast the inputs to `Tensor` if the inputs are supported by TensorFlow operations.

**Standalone code to reproduce the issue**
```python
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.utils import Sequence


class Dataset(Sequence):
    def __init__(self):
        pass

    def __len__(self):
        return 1

    def __getitem__(self, idx):
        return tf.SparseTensor([[0, 1], [1, 0]], [1, 1], (2, 2))


class Net(Model):
    def __init__(self):
        super().__init__()

    def call(self, x, **kwargs):
        return x.values


net = Net()
net.compile()
net.fit(Dataset())
```


Causes the following error:
```python
AttributeError: 'Tensor' object has no attribute 'values'
```
"
41418,Built-in Method to check if a GPU is already Reserved,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): tf-nightly==2.3.0dev20200622 & tf-nightly-gpu==2.3.0dev20200622
- Are you willing to contribute it (Yes/No): No - this is far beyond my current abilities



**Describe the feature and the current behavior/state.**
The idea is to have a TF 2.X compatible function that checks to see which GPUs/Resources that are actually available for computation as opposed to simply addressable. An example of this would be running one python script that assigns only 1 gpu out of 2 available gpus, and then another python script, in a completely separate instance, would call said function to check which of the 2 gpus is currently being used and which is available to still be used.

**Will this change the current api? How?**
It would add a new function/method to a module. It shouldn't need to modify currently existing methods.
**Who will benefit with this feature?**
Anyone who wants to run multiple instances of a training script without having to explicitly program which GPU/CPU/TPU to use. There are certainly ways to do this task, but they require the usage of either custom message passing, bash scripted resource management, or an external python script that uses multiprocessing or subprocess packages. Having something that can give a status update on truly available resources would be useful.

**Additional Info**
It is possible that this is done behind the scenes in tensorflow already. If it is, it is not clearly reflected in Tensorflow documentation. However, it would appear that it isn't done behind the scenes because I have network and dataset that causes core dumps on a 2 gpu system if I run two instances but succeeds if I run only one instance. As far as I can tell it's because the GPUs are running out of memory."
41417,Min_delta parameters on callbacks function of Keras model seems to not take effect,"I am using the following callbacks function on a Keras model and I initialize the minimum delta to 0.002, so based on the documentation of Tensorflow/Keras any improvement in the validation loss function less than 0.002 won't be counted for an improvement. However, this seems to not get implemented in my case.

callback function:

```
def callback(folder_path, saved_model_name, patience_value, logdir, hparams):
    
    # Initialize parameters
    monitor_metric = 'val_loss'
    minimum_delta = 0.002
    patience_limit = patience_value
    verbose_value = 1
    mode_value = 'min'
    weights_fname = os.path.join(os.getcwd(), '{0}/{1}.h5'.format(folder_path, saved_model_name))
    
    # Initialize callbacks
    callbacks = [
        
        EarlyStopping(monitor=monitor_metric,
                      min_delta=minimum_delta,
                      patience=patience_limit,
                      verbose=verbose_value,
                      mode=mode_value,
                      restore_best_weights=True),

        ModelCheckpoint(filepath=weights_fname,
                        monitor=monitor_metric,
                        verbose=verbose_value,
                        save_best_only=True,
                        save_weights_only=True),
        
        tf.keras.callbacks.TensorBoard(logdir),
        
        hp.KerasCallback(logdir, hparams)
    ]
    
    return callbacks
```
**Example 1**
```
Epoch 00016: val_loss improved from 0.02129 to 0.02015, saving model to /content/model_one/adam_v2_14072020/multi_input_keras_model_50dim_128batchsize_0.01lr_10decaymultiplier_14072020.h5
```

**Example 2**
```
Epoch 00019: val_loss improved from 0.01880 to 0.01803, saving model to /content/model_one/adam_v2_14072020/multi_input_keras_model_50dim_128batchsize_0.01lr_10decaymultiplier_14072020.h5
```
You can see that in both examples the validation loss improved and the model's weight have been saved, while they shouldn't;t have:

* 0.02129 - 0.02015 = 0.00114 < 0.002 (although it was counted as an improvement)
* 0.01880 - 0.01803 = 0.00077 < 0.002 (also counted as an improvement in validation loss)
What is going wrong?

Apologize for not posting more code since it's a thorough application. So, please check my [colab notebook](https://drive.google.com/file/d/1FqQjjB2xFFQCiAqZacvVsl5SU0kcJwGd/view?usp=sharing) to replicate the issue.

Pickled data saved on my drive [folder](https://drive.google.com/drive/folders/1Z0p2RVpJSn8uhZG5UvZBU1AS3aLqEVqv?usp=sharing)"
41416,generate_TFrecord.py,"I am using my mac, running Catalina 10.15.5, with Python 3.8.3, and TF (2.2.0) which I pip3 installed from github. I have come to the conclusion the issue is using TF 2 instead of 1, but I do not want to downgrade my TF. Additionally, I named some folders differently than him i.e. his ""images"" is my ""Camera pictures,"" and his ""test"" and ""train"" are my ""Test"" and ""Train""

I've been dealing with an issue converting my csv files to TFrecord, following this tutorial https://pythonprogramming.net/creating-tfrecord-files-tensorflow-object-detection-api-tutorial/



Here is the issue I keep running into:
![image](https://user-images.githubusercontent.com/68125365/87552143-46d91700-c67f-11ea-900f-5f80329358e9.png)

My directory looks like:

Desktop

- models
- Object-Detection
        - data/
            -- Test_labels.csv
            -- Train_labels.csv
        - Camera pictures/
            -- Test/
            --- testingimages.jpg
            -- Train/
            --- testingimages.jpg
            -- ...myimages.jpg
        - training
        - xml_to_csv.py
        - generate_TFrecord.py'


Here is my generate_TFrecord.py code: 
    
    """"""   
    Usage:
    # From tensorflow/models/
    # Create train data:
    python3 generate_tfrecord.py --csv_input=data/Test_labels.csv --output_path=data/test.record --image_dir=Camera Pictures/Test
    # Create test data:
    python3 generate_tfrecord.py --csv_input=data/Train_labels.csv --output_path=data/train.record --image_dir=Camera Pictures/Train 
    """"""

    from __future__ import division
    from __future__ import print_function
    from __future__ import absolute_import

    import os
    import io
    import pandas as pd
    import tensorflow.compat.v1 as tf

    from PIL import Image
    from object_detection.utils import dataset_util
    from collections import namedtuple, OrderedDict

    flags = tf.app.flags
    flags.DEFINE_string('csv_input', '', 'Path to the CSV input')
    flags.DEFINE_string('output_path', '', 'Path to output TFRecord')
    flags.DEFINE_string('image_dir', '', 'Path to the images')
    FLAGS = flags.FLAGS


    # TO-DO replace this with label map
    def class_text_to_int(row_label):
        if row_label == 'Raspi':
            return 1
        else:
            None


    def split(df, group):
        data = namedtuple('data', ['filename', 'object'])
        gb = df.groupby(group)
        return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]


    def create_tf_example(group, path):
        with tf.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:
            encoded_jpg = fid.read()
        encoded_jpg_io = io.BytesIO(encoded_jpg)
        image = Image.open(encoded_jpg_io)
        width, height = image.size

        filename = group.filename.encode('utf8')
        image_format = b'jpg'
        xmins = []
        xmaxs = []
        ymins = []
        ymaxs = []
        classes_text = []
        classes = []

        for index, row in group.object.iterrows():
            xmins.append(row['xmin'] / width)
            xmaxs.append(row['xmax'] / width)
            ymins.append(row['ymin'] / height)
            ymaxs.append(row['ymax'] / height)
            classes_text.append(row['class'].encode('utf8'))
            classes.append(class_text_to_int(row['class']))

        tf_example = tf.train.Example(features=tf.train.Features(feature={
            'image/height': dataset_util.int64_feature(height),
            'image/width': dataset_util.int64_feature(width),
            'image/filename': dataset_util.bytes_feature(filename),
            'image/source_id': dataset_util.bytes_feature(filename),
            'image/encoded': dataset_util.bytes_feature(encoded_jpg),
            'image/format': dataset_util.bytes_feature(image_format),
            'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),
            'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),
            'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),
            'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),
            'image/object/class/text': dataset_util.bytes_list_feature(classes_text),
            'image/object/class/label': dataset_util.int64_list_feature(classes),
        }))
        return tf_example


    def main(_):
        writer = tf.python_io.TFRecordWriter(FLAGS.output_path)
        path = os.path.join(FLAGS.image_dir)
        examples = pd.read_csv(FLAGS.csv_input)
        grouped = split(examples, 'filename')
        for group in grouped:
            tf_example = create_tf_example(group, path)
            writer.write(tf_example.SerializeToString())

        writer.close()
        output_path = os.path.join(os.getcwd(), FLAGS.output_path)
        print('Successfully created the TFRecords: {}'.format(output_path))


    if __name__ == '__main__':
        tf.app.run()

Please respond with exactly what needs to be changed in the code, as I am new to this. Thank you!"
41415,Build TensorFlow 2.3 from source for the Raspberry Pi (Python 3.7),"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 LTS
- TensorFlow installed from : Git repository
- TensorFlow version: 2.3.0rc0
- Python version: 3.7
- Installed using virtualenv? pip? conda?: pip




**Describe the problem**

Hello, 

I would like to build a wheel file for my Raspberry Pi 3B (Buster) with TensorFlow version 2.3.0rc0.
So I run the following command : 
```CI_DOCKER_EXTRA_PARAMS=""-e CI_BUILD_PYTHON=python3 -e CROSSTOOL_PYTHON_INCLUDE_PATH=/usr/include/python3.7""     tensorflow/tools/ci_build/ci_build.sh PI-PYTHON37     tensorflow/tools/ci_build/pi/build_raspberry_pi.sh```

However, the output I obtain is the wheel file for Python 3.5 so it won't work for my RPI..
So is my command correct? 

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Output can be found here:
output-artifacts
output-artifacts/tensorflow-2.3.0rc0-cp35-none-linux_armv7l.whl
output-artifacts/benchmark_model
output-artifacts/libtensorflow_framework.so
output-artifacts/libtensorflow.so"
41413,Threshold to be set 0 when using binary accuracy with raw prediction values (from_logits=True),"This is mainly a documentation bug (official tensorflow tutorial), but it is a ""dangerous trap"" and might also happen in general to users, so see below my last sentence this could also be fixed in Tensorflow that it detects this automatically.

In this [tutorial](https://www.tensorflow.org/tutorials/images/transfer_learning) raw prediction values (form_logit=True) are used. So we have negative values and positive values, while 

> 
""prediction will be treated as a logit, or a raw prediction value. **Positive numbers predict class 1, negative numbers predict class 0.**""

However, the model.compile statement is as follows:

```
base_learning_rate = 0.0001
model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate),
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])

```

This is wrong, as per default, threshold value to classify is 0.5:

https://www.tensorflow.org/api_docs/python/tf/keras/metrics/binary_accuracy
https://www.tensorflow.org/api_docs/python/tf/keras/metrics/BinaryAccuracy
```
tf.keras.metrics.binary_accuracy(
    y_true, y_pred, threshold=0.5
)
```
```
tf.keras.metrics.BinaryAccuracy(
    name='binary_accuracy', dtype=None, threshold=0.5
)
```

> threshold | (Optional) Float representing the threshold for deciding whether prediction values are 1 or 0.
> -- | --




This leads to the wrong classifications. model.evaluate will also give false accuracy measure. Reason is that predicted values in range [0,0.49999] are wrongly classified as 0 (I am not sure what happens to a value of exactly 0.5), whereas they actually should be classified as 1!

So it needs to be corrected to:

> model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate),
>               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
>               metrics=tf.keras.metrics.BinaryAccuracy(threshold=0.0))

Would be even better if this is corrected inside Tensorflow that it automatically detects that from_logits=True was set and then assumes that default threshold is not 0.5 anymore, but 0.0 (and maybe additional WARNING output)."
41409,Tensorflow Lite ARM64 build error: inlining failed in call to always_inline 'uint8x16_t vaesmcq_u8(uint8x16_t)': target specific option mismatch,"**System information**
- OS Platform and Distribution: 
  - Linux Ubuntu 18.04
  - Alpine 3.10
- TensorFlow version: v2.2.0
- GCC/Compiler version (if compiling from source): 
  - g++ (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04) 7.5.0
  - g++ (Alpine 8.3.0) 8.3.0

**Describe the problem**

While trying to build natively on ARM64 following the instruction found [here](https://www.tensorflow.org/lite/guide/build_arm64#native_compiling) we encountered the following error during build.

```sh
aarch64-linux-gnu-g++ -O3 -DNDEBUG -fPIC --std=c++11  -DTFLITE_WITH_RUY -march=armv8-a -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/home/ubuntu/tf_lite_build/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/home/ubuntu/tf_lite_build/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/home/ubuntu/tf_lite_build/tensorflow/tensorflow/lite/tools/make/downloads/ -I/home/ubuntu/tf_lite_build/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/home/ubuntu/tf_lite_build/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/home/ubuntu/tf_lite_build/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/ubuntu/tf_lite_build/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/ubuntu/tf_lite_build/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/ubuntu/tf_lite_build/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/home/ubuntu/tf_lite_build/tensorflow/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/tools/make/downloads/absl/absl/random/seed_gen_exception.cc -o /home/ubuntu/tf_lite_build/tensorflow/tensorflow/lite/tools/make/gen/linux_aarch64/obj/tensorflow/lite/tools/make/downloads/absl/absl/random/seed_gen_exception.o
In file included from tensorflow/lite/tools/make/downloads/absl/absl/random/internal/randen_hwaes.cc:225:0:
/usr/lib/gcc/aarch64-linux-gnu/7/include/arm_neon.h: In function ‘Vector128 {anonymous}::AesRound(const Vector128&, const Vector128&)’:
/usr/lib/gcc/aarch64-linux-gnu/7/include/arm_neon.h:12440:1: error: inlining failed in call to always_inline ‘uint8x16_t vaesmcq_u8(uint8x16_t)’: target specific option mismatch
 vaesmcq_u8 (uint8x16_t data)
 ^~~~~~~~~~
tensorflow/lite/tools/make/downloads/absl/absl/random/internal/randen_hwaes.cc:251:20: note: called from here
   return vaesmcq_u8(vaeseq_u8(state, uint8x16_t{})) ^ round_key;
          ~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from tensorflow/lite/tools/make/downloads/absl/absl/random/internal/randen_hwaes.cc:225:0:
/usr/lib/gcc/aarch64-linux-gnu/7/include/arm_neon.h:12426:1: error: inlining failed in call to always_inline ‘uint8x16_t vaeseq_u8(uint8x16_t, uint8x16_t)’: target specific option mismatch
 vaeseq_u8 (uint8x16_t data, uint8x16_t key)
 ^~~~~~~~~
tensorflow/lite/tools/make/downloads/absl/absl/random/internal/randen_hwaes.cc:251:20: note: called from here
   return vaesmcq_u8(vaeseq_u8(state, uint8x16_t{})) ^ round_key;
          ~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
tensorflow/lite/tools/make/Makefile:269: recipe for target '/home/ubuntu/tf_lite_build/tensorflow/tensorflow/lite/tools/make/gen/linux_aarch64/obj/tensorflow/lite/tools/make/downloads/absl/absl/random/internal/randen_hwaes.o' failed
make: *** [/home/ubuntu/tf_lite_build/tensorflow/tensorflow/lite/tools/make/gen/linux_aarch64/obj/tensorflow/lite/tools/make/downloads/absl/absl/random/internal/randen_hwaes.o] Error 1
make: *** Waiting for unfinished jobs....
In file included from tensorflow/lite/tools/make/downloads/absl/absl/random/mocking_bit_gen.cc:16:0:
/home/ubuntu/tf_lite_build/tensorflow/tensorflow/lite/tools/make/downloads/absl/absl/random/mocking_bit_gen.h:40:10: fatal error: gmock/gmock.h: No such file or directory
 #include ""gmock/gmock.h""
          ^~~~~~~~~~~~~~~
compilation terminated.
tensorflow/lite/tools/make/Makefile:269: recipe for target '/home/ubuntu/tf_lite_build/tensorflow/tensorflow/lite/tools/make/gen/linux_aarch64/obj/tensorflow/lite/tools/make/downloads/absl/absl/random/mocking_bit_gen.o' failed
make: *** [/home/ubuntu/tf_lite_build/tensorflow/tensorflow/lite/tools/make/gen/linux_aarch64/obj/tensorflow/lite/tools/make/downloads/absl/absl/random/mocking_bit_gen.o] Error 1
make: Leaving directory '/home/ubuntu/tf_lite_build/tensorflow'

```

This happens both on the Ubuntu machine and inside the Alpine Docker container.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```sh
$ git clone https://github.com/tensorflow/tensorflow.git
$ cd tensorflow/
$ git checkout v2.2.0
$ cd tensorflow/lite/tools/make
$ ./download_dependencies.sh
$ ./build_aarch64_lib.sh
```
"
41406,Ran into this issue while importing the tflearn library,"D:\Softwares\anaconda\lib\site-packages\tflearn\__init__.py in <module>
      2 
      3 # Config
----> 4 from . import config
      5 from .config import is_training, get_training_mode, init_graph
      6 

D:\Softwares\anaconda\lib\site-packages\tflearn\config.py in <module>
      3 import tensorflow as tf
      4 
----> 5 from .variables import variable
      6 
      7 # -------------------

D:\Softwares\anaconda\lib\site-packages\tflearn\variables.py in <module>
      5 import tflearn
      6 
----> 7 from tensorflow.contrib.framework.python.ops import add_arg_scope as contrib_add_arg_scope
      8 from tensorflow.python.framework import ops
      9 from tensorflow.python.ops import variable_scope

ModuleNotFoundError: No module named 'tensorflow.contrib'


I have installed the library"
41405,Error: #include nested too deeply (libtensorflow_cc.so),"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.3.0-rc1
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): 7.5.0

**Describe the problem**
The problem occurs when using libtensorflow_cc.so. When including <tensorflow/core/public/session.h> in the main file of my program that uses libtensorflow_cc.so, it includes <tensorflow/core/framework/tensor.h>, which in turn includes <unsupported/Eigen/CXX11/Tensor>. This file includes itself and results in the error #include nested too deeply.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
`#include <tensorflow/core/public/session.h>` in the main file of my program that uses libtensorflow_cc.so.

**Any other info / logs**

```
In file included from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:0,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/tensorflow/core/framework/tensor.h:22,
                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/tensorflow/core/public/session.h:24,
                 from /media/ssd512/src/main.cpp:2:
/media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:42: error: #include nested too deeply
 #include ""unsupported/Eigen/CXX11/Tensor""
```"
41404,memory leak in tf(2.2).keras.Model.predict function,"pre:
1. train detection model  using tf.keras under tf2.2 version
2. inference using tf.compat.v1.keras 
 config = tf.compat.v1.ConfigProto()
 config.gpu_options.per_process_gpu_memory_fraction = 0.25
  sess = tf.compat.v1.Session(config = config)
  tf.compat.v1.keras.backend.set_session(sess)

3. load model: 
   model = tf.keras.models.load_model(self.modelpath, compile=False)

found memory leak using [model.predict(image_np)] to get prediction

is this a serious bug in tf.keras.model.predict function???"
41403,AttributeError: Tensor.op is meaningless when eager execution is enabled when using multiple feature inputs in Tensorflow federated,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Red Hat 7.7
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**: Binary
-   **TensorFlow version (use command below)**: TF 2.2
-   **Python version**: 3.7
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

### Describe the problem
""AttributeError: Tensor.op is meaningless when eager execution is enabled"" when using multiple feature inputs in Tensorflow federated. 

I have a problem when inputting multiple feature inputs as follows:

```
feature_layer = tf.keras.layers.DenseFeatures(feature_cols)

feature_layer_inputs = {}
feature_layer_inputs['a'] = tf.keras.Input(shape=(1,), name='a', dtype=tf.int32)
feature_layer_inputs['b'] = tf.keras.Input(shape=(1,), name='b', dtype=tf.int32)

model = feature_layer(feature_layer_inputs)

for units in [64, 64]:
    model = tf.keras.layers.Dense(units, activation='relu')(model)
c_pred = tf.keras.layers.Dense(1, activation='sigmoid')(model)

keras_model = tf.keras.Model(inputs=[v for v in feature_layer_inputs.values()], outputs=c_pred)

input_spec = collections.OrderedDict(
        x=collections.OrderedDict(
            a=tf.TensorSpec(shape=[None,], dtype=tf.int32),
            b=tf.TensorSpec(shape=[None,], dtype=tf.int32),
        y=tf.TensorSpec(shape=[None,], dtype=tf.int32))

def model_fn():
  # We _must_ create a new model here, and _not_ capture it from an external
  # scope. TFF will call this within different graph contexts.

  return tff.learning.from_keras_model(
      keras_model,
      #input_spec=preprocessed_sample_dataset.element_spec,
      input_spec=input_spec,
      loss=losses.SparseCategoricalCrossentropy(),
      metrics=[metrics.SparseCategoricalAccuracy()])


iterative_process = tff.learning.build_federated_averaging_process(
    model_fn,
    client_optimizer_fn=lambda: optimizers.Adam(learning_rate=client_lr),
    server_optimizer_fn=lambda: optimizers.SGD(learning_rate=server_lr))

state = iterative_process.initialize()

for round_num in range(1, NUM_ROUNDS+1):
    #state, tff_metrics = iterative_process.next(state, federated_train_data)
    state, tff_metrics = iterative_process.next(state, train_data)
    eval_model = keras_model
    eval_model.compile(optimizer=optimizers.Adam(learning_rate=client_lr),
                       loss=losses.SparseCategoricalCrossentropy(),
                       metrics=[metrics.SparseCategoricalAccuracy()])

    tff.learning.assign_weights_to_keras_model(eval_model, state.model)

    ev_result = eval_model.evaluate(x_test, y_test, verbose=0)
```

However, I got the full traceback as follows. Eventually, I get an error 'AttributeError: Tensor.op is meaningless when eager execution'. It seems that there is something wrong with the built model especially the inputs inside the tf.keras.model function.

```
Traceback (most recent call last):
  File ""accident_modify_uk_final3b.py"", line 323, in <module>
    server_optimizer_fn=lambda: optimizers.SGD(learning_rate=server_lr))
  File ""/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow_federated/python/learning/federated_averaging.py"", line 212, in build_federated_averaging_process
    stateful_delta_aggregate_fn, stateful_model_broadcast_fn)
  File ""/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow_federated/python/learning/framework/optimizer_utils.py"", line 360, in build_model_delta_optimizer_process
    @tff.tf_computation
  File ""/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow_federated/python/core/api/computations.py"", line 152, in tf_computation
    return computation_wrapper_instances.tensorflow_wrapper(*args)
  File ""/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/wrappers/computation_wrapper.py"", line 333, in __call__
    self._wrapper_fn)
  File ""/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/wrappers/computation_wrapper.py"", line 91, in _wrap
    concrete_fn = wrapper_fn(fn, parameter_type, unpack=None)
  File ""/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/wrappers/computation_wrapper_instances.py"", line 52, in _tf_wrapper_fn
    target_fn, parameter_type, ctx_stack)
  File ""/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/tensorflow_serialization.py"", line 275, in serialize_py_fn_as_tf_computation
    result = target(*args)
  File ""/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/utils/function_utils.py"", line 455, in <lambda>
    return lambda: fn()  # pylint: disable=unnecessary-lambda
  File ""/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow_federated/python/learning/framework/optimizer_utils.py"", line 364, in tf_init_fn
    stateful_model_broadcast_fn.initialize())
  File ""/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow_federated/python/learning/framework/optimizer_utils.py"", line 227, in server_init
    _, optimizer_vars = _build_server_optimizer(model, optimizer)
  File ""/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow_federated/python/learning/framework/optimizer_utils.py"", line 123, in _build_server_optimizer
    apply_delta(delta=weights_delta)
  File ""/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 580, in __call__
    result = self._call(*args, **kwds)
  File ""/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 627, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File ""/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 506, in _initialize
    *args, **kwds))
  File ""/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 2446, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 2777, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 2667, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 981, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 441, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 968, in wrapper
    raise e.ag_error_metadata.to_exception(e)
AttributeError: in user code:

    /home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow_federated/python/learning/framework/optimizer_utils.py:112 apply_delta  *
        optimizer.apply_gradients(grads_and_vars, name='server_update')
    /home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:508 apply_gradients  **
        ""name"": name,
    /home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2420 merge_call
        return self._merge_call(merge_fn, args, kwargs)
    /home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2427 _merge_call
        return merge_fn(self._strategy, *args, **kwargs)
    /home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:590 _distributed_apply  **
        ""update_"" + var.op.name, skip_on_eager=True):
    /home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:581 op
        return self._handle.op
    /home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1113 op
        ""Tensor.op is meaningless when eager execution is enabled."")

    AttributeError: Tensor.op is meaningless when eager execution is enabled.
```

How to solve this issue?
"
41402,How to find the number of iterations while using tf.compat.v1.train.AdamOptimizer?,"I am actually. using the VGGish model to train my data and I am using the tf.compat.v1.train.AdamOptimizer to minimize my error. Is there a way in which I can print the number of iterations that the model does before printing the loss. Below is the code that I am using to train the model.

My definition of the optimizer -

```
optimizer = tf.train.AdamOptimizer(
            # time steps
            learning_rate=vggish_params.LEARNING_RATE,
            # tolerance level 
            epsilon=vggish_params.ADAM_EPSILON)
        optimizer.minimize(loss, global_step=global_step, name='train_op')
```

Loop where I am feeding the data each time. I want to print the number of iterations for each time I feed the data.

```
for _ in range(FLAGS.num_batches):
      #for normal batch training
      (features, labels) = _get_examples_batch()
      [num_steps, loss, _] = sess.run(
        # here the first array that we are passing in is called fetches
        # it is a list of graph elements
          [global_step_tensor, loss_tensor, train_op],
          # one has to be of the form of a tensor and the other in the form of a numpy array
          feed_dict={features_tensor: features, labels_tensor: labels})
      print('Step %d: loss %g' % (num_steps, loss))
```
"
41401,RuntimeError: cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version,"I want to run tensorflow with CUDA 10.0 or 10.1 on Windows.

- abstract

-- error message
RuntimeError: cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version

-- system infomeation
tensorflow-gpu 2.2.0
Python 3.7.1
Windows 10 Pro
CUDA 10.1.105
cuDNN 7.6.4
NVIDIA driver 451.48
Visual Studio 2017
GeForce GTX 1080 Ti

- detail

-- NVIDIA driver

nvidia-smi

```
Wed Jul 15 15:14:29 2020
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 451.48       Driver Version: 451.48       CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 108... WDDM  | 00000000:01:00.0 Off |                  N/A |
| 25%   30C    P8     9W / 250W |   1310MiB / 11264MiB |      1%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      2228    C+G   ...es.TextInput.InputApp.exe    N/A      |
|    0   N/A  N/A      9272    C+G   Insufficient Permissions        N/A      |
|    0   N/A  N/A      9372    C+G   Insufficient Permissions        N/A      |
|    0   N/A  N/A     11260    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A     11740    C+G   ...artMenuExperienceHost.exe    N/A      |
|    0   N/A  N/A     12340    C+G   ...w5n1h2txyewy\SearchUI.exe    N/A      |
|    0   N/A  N/A     12952    C+G   ...zf8qxf38zg5c\SkypeApp.exe    N/A      |
|    0   N/A  N/A     13272    C+G   ...y\ShellExperienceHost.exe    N/A      |
|    0   N/A  N/A     15784    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     16300    C+G   ...bbwe\Microsoft.Photos.exe    N/A      |
+-----------------------------------------------------------------------------+
```
-- CUDA

nvcc -V

```
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2019 NVIDIA Corporation Built on Fri_Feb__8_19:08:26_Pacific_Standard_Time_2019
Cuda compilation tools, release 10.1, V10.1.105
```
-- cuDNN

C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1\include\cudnn.h

```
#define CUDNN_MAJOR 7
#define CUDNN_MINOR 6
#define CUDNN_PATCHLEVEL 4

```

-- Environment Variable

```
CUDA_PATH=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1 CUDA_PATH_V10_1=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1 CUDNN_PATH=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1 Path=
    ...
    C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1\bin;
    C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1\libnvvp;
    C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1\extras\CUPTI\libx64;
    C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1\include;
    ...
```

-- python

python -V

```
Python 3.7.1
```

-- pip list

```
Keras                    2.4.3
Keras-Preprocessing      1.1.2
...
tensorboard              2.2.2
tensorboard-plugin-wit   1.7.0
tensorflow-estimator     2.2.0
tensorflow-gpu           2.2.0
tensorflow-gpu-estimator 2.2.0
```

-- Error message

python -c ""from tensorflow.python.client import device_lib; print(device_lib.list_local_devices())""

```
2020-07-15 15:00:36.136510: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-07-15 15:00:37.931078: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2020-07-15 15:00:37.946142: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x148edb04360 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-07-15 15:00:37.957182: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-07-15 15:00:37.965143: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2020-07-15 15:00:37.996562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 451.17GiB/s
2020-07-15 15:00:38.013444: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-07-15 15:00:38.026126: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-07-15 15:00:38.038540: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-07-15 15:00:38.049231: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-07-15 15:00:38.061932: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-07-15 15:00:38.074267: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-07-15 15:00:38.091257: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-07-15 15:00:38.100188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0 Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""D:\****\lib\site-packages\tensorflow\python\client\device_lib.py"", line 43, in list_local_devices
    _convert(s) for s in _pywrap_device_lib.list_devices(serialized_config)
RuntimeError: cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version
```
"
41400,Bigger Tflite micro model,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- Tensorflow version (commit SHA if source): 2.2.0
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): 

**Describe the problem**
I downloaded tflite Inception_v4 model from https://www.tensorflow.org/lite/guide/hosted_models. The model had a size of 42.9MB. I converted to tflite micro using the xxd command and the .cc file I got after conversion had a size of 264MB. Does the model conversion to cc file increase the tflite model size by 5x times? What I understand is that, tflite micro should give a smaller sized model for execution in microcontrollers. Is my understanding correct. Can you help me in solving the problem?

**Please provide the exact sequence of commands/steps when you ran into the problem**
xxd -i inception_v4_299_quant.tflite > model.cc

"
41398,"GPU memory consumption soars up when ""device_record_tensor_accesses_"" in executor.cc .","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): TensorFlow 1.13
- Python version: Python 3.6.10
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: 10.0
- GPU model and memory: NVIDIA Tesla V100, 32GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

There is flag named ""device_record_tensor_accesses_"" which is set by ""RequiresRecordingAccessedTensors()"" function in gpu_device.cc. It sets record accessed tensors and delay tensor release time until actual GPU execution is finished. Comment said it may be useful for multi stream situation which never happened. Hence, it is set to false by default. I need this sort of feature so test it, changing it into true. When I turned it on, weirdly GPU memory consumption soars up. I used Resnet50 with batch 64 from tf_cnn_benchmark. It used around 8GB but when ""device_record_tensor_accesses_"" is turned on, it uses more than 15GB and keeps growing during training. I saw source code quite a long time to figure out where this problem comes from but I still can't.

I looked up why it happens and found out that there are actually more tensor allocation.

I saw that this feature is removed many part of related codes from recent TensorFlow since it has never been used.

Related files: executor.cc, gpu_device.cc, gpu_event_mgr.h/cc, unique_tensor_references.h/cc

**Describe the expected behavior**

I guess it should have stayed using the same amount of GPU memory.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

I simply use tf_cnn_benchmark's Resnet50 model training job.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

This shows tensor allocation from that operator(kernel) which didn't exist when ""device_record_tensor_accesses_"" is turned off.
So, they are newly appearing tensor allocation. (There are more but I just post some of them due to space limit.)

![image](https://user-images.githubusercontent.com/20127356/87491362-e8159c80-c682-11ea-9837-39724853e6be.png)

and I grep one of those newly created operator ""AssignMovingAvg"". And it appears in a strange path.

I just turned on ""device_record_tensor_accesses_"", but why are those operator added??
Is there any reason or is it bug?

![image](https://user-images.githubusercontent.com/20127356/87505594-30918200-c6a4-11ea-9e64-3f4143d1da9c.png)

Is it supposed to change the 

I guess this is bug. I believe there is no reason that the peak GPU memory consumption goes up.

Thank you in advance! 
"
41397,Keras Sequence Generators not compatible with ragged tensors when using mode.fit,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2
- Python version: 3.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: cuDNN 10
- GPU model and memory: V100/24GB RAM

**Describe the current behavior**
I have written the following generator to take numpy arrays and convert to ragged tensors within a Keras Sequence Data generator.

'''python
class get_batches(tf.keras.utils.Sequence):
    def __init__(self,x,y,batch_size=10,random=False):
        self.batch_size = batch_size
        self.random = random
        self.x = x
        self.y = y
        self.on_epoch_end()

    def on_epoch_end(self):
        self.indexes = np.arange(len(self.x[0]))
        if self.random == True:
            np.random.shuffle(self.indexes)

    def __getitem__(self, index):
        'Generate one batch of data'
        # Generate indexes of the batch
        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]

        x_out = [turn_into_ragged(var[indexes]) for var in self.x]
        # x_out = [var[indexes] for var in self.x]
        y_out = [var[indexes] for var in self.y]

        return (x_out, y_out)

    def __len__(self):
        x = self.x
        batch_size = self.batch_size
        if len(x[0]) % batch_size == 0:
            n_batches = (len(x[0]) // batch_size)
        else:
            n_batches = (len(x[0]) // batch_size) + 1
        return n_batches

When I run this generator in a for loop, I am able to take the outputs and feed it to my model.fit command but when I put the generator into the model.fit command, I get the following error:

 TypeError: `generator` yielded an element that could not be converted to the expected type. The expected type was float64, but the yielded element was <tf.RaggedTensor

**Describe the expected behavior**
I would expect the generator to be compatible with model.fit and train.

**Standalone code to reproduce the issue**

'''python
class get_batches(tf.keras.utils.Sequence):
    def __init__(self,x,y,batch_size=10,random=False):
        self.batch_size = batch_size
        self.random = random
        self.x = x
        self.y = y
        self.on_epoch_end()

    def on_epoch_end(self):
        self.indexes = np.arange(len(self.x[0]))
        if self.random == True:
            np.random.shuffle(self.indexes)

    def __getitem__(self, index):
        'Generate one batch of data'
        # Generate indexes of the batch
        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]

        x_out = [turn_into_ragged(var[indexes]) for var in self.x]
        # x_out = [var[indexes] for var in self.x]
        y_out = [var[indexes] for var in self.y]

        return (x_out, y_out)

    def __len__(self):
        x = self.x
        batch_size = self.batch_size
        if len(x[0]) % batch_size == 0:
            n_batches = (len(x[0]) // batch_size)
        else:
            n_batches = (len(x[0]) // batch_size) + 1
        return n_batches


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
41396,pylint errors on master branch,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): N/A
- TensorFlow version (use command below): master branch
- Python version: 3.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
When I run pylint on TF python files, I get a lot of failures. The following is an example (using master branch):

* Is this expected or is a bug?
* Fixing pylint issues in our PRs is hard given all the existing failures. What do you suggest to make the contribution easier? 

```
tensorflow/python~ % pylint --rcfile=../tools/ci_build/pylintrc ./keras/layers/pooling.py
************* Module keras.layers.pooling
keras/layers/pooling.py:167:71: C0303: Trailing whitespace (trailing-whitespace)
keras/layers/pooling.py:168:74: C0303: Trailing whitespace (trailing-whitespace)
keras/layers/pooling.py:213:71: C0303: Trailing whitespace (trailing-whitespace)
keras/layers/pooling.py:214:74: C0303: Trailing whitespace (trailing-whitespace)
keras/layers/pooling.py:377:0: C0303: Trailing whitespace (trailing-whitespace)
keras/layers/pooling.py:379:0: C0303: Trailing whitespace (trailing-whitespace)
keras/layers/pooling.py:383:60: C0303: Trailing whitespace (trailing-whitespace)
keras/layers/pooling.py:385:41: C0303: Trailing whitespace (trailing-whitespace)
keras/layers/pooling.py:387:62: C0303: Trailing whitespace (trailing-whitespace)
keras/layers/pooling.py:426:71: C0303: Trailing whitespace (trailing-whitespace)
keras/layers/pooling.py:427:74: C0303: Trailing whitespace (trailing-whitespace)
keras/layers/pooling.py:483:71: C0303: Trailing whitespace (trailing-whitespace)
keras/layers/pooling.py:484:74: C0303: Trailing whitespace (trailing-whitespace)
keras/layers/pooling.py:628:71: C0303: Trailing whitespace (trailing-whitespace)
keras/layers/pooling.py:629:74: C0303: Trailing whitespace (trailing-whitespace)
keras/layers/pooling.py:681:71: C0303: Trailing whitespace (trailing-whitespace)
keras/layers/pooling.py:682:74: C0303: Trailing whitespace (trailing-whitespace)
keras/layers/pooling.py:72:2: W0221: Parameters differ from overridden 'call' method (arguments-differ)
keras/layers/pooling.py:95:4: R1705: Unnecessary ""else"" after ""return"" (no-else-return)
keras/layers/pooling.py:288:2: W0221: Parameters differ from overridden 'call' method (arguments-differ)
keras/layers/pooling.py:315:4: R1705: Unnecessary ""else"" after ""return"" (no-else-return)
keras/layers/pooling.py:564:2: W0221: Parameters differ from overridden 'call' method (arguments-differ)
keras/layers/pooling.py:600:4: R1705: Unnecessary ""else"" after ""return"" (no-else-return)
keras/layers/pooling.py:734:4: R1705: Unnecessary ""else"" after ""return"" (no-else-return)
keras/layers/pooling.py:739:2: W0221: Parameters differ from overridden 'call' method (arguments-differ)
keras/layers/pooling.py:792:2: W0221: Parameters differ from overridden 'call' method (arguments-differ)
keras/layers/pooling.py:794:4: R1705: Unnecessary ""else"" after ""return"" (no-else-return)
keras/layers/pooling.py:868:4: R1705: Unnecessary ""else"" after ""return"" (no-else-return)
keras/layers/pooling.py:873:2: W0221: Parameters differ from overridden 'call' method (arguments-differ)
keras/layers/pooling.py:918:4: R1705: Unnecessary ""else"" after ""return"" (no-else-return)
keras/layers/pooling.py:959:4: R1705: Unnecessary ""else"" after ""return"" (no-else-return)
keras/layers/pooling.py:975:4: R1705: Unnecessary ""else"" after ""return"" (no-else-return)
keras/layers/pooling.py:980:2: W0221: Parameters differ from overridden 'call' method (arguments-differ)
keras/layers/pooling.py:1019:4: R1705: Unnecessary ""else"" after ""return"" (no-else-return)
keras/layers/pooling.py:1054:4: R1705: Unnecessary ""else"" after ""return"" (no-else-return)

------------------------------------------------------------------
Your code has been rated at 8.54/10 (previous run: 8.81/10, -0.27)
```

"
41395,[TF2.2] tf.data.Dataset memory leak,"**System information**
- Have I written custom code : yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.4
- TensorFlow installed from (source or binary): wheel
- TensorFlow version (use command below):  v2.2.0-rc4-8-g2b96f3662b 2.2.0
- Python version: 3.6.7
- CUDA/cuDNN version: 10.2  
- GPU model and memory: GeForce RTX 2080 Ti


**Describe the current behavior**

I have a memory leak when using `tf.data.Dataset`.
It happens when I use fit on a tf.keras.Model.
It seems to happend when iterating inside a `tf.function` too.

**Describe the expected behavior**

Memory should not be incread

**Standalone code to reproduce the issue**
```python

import numpy as np
import tensorflow as tf
import psutil
from tqdm import tqdm

n = 80
dataset = (
    tf.data.Dataset.from_tensor_slices(
        (
            np.random.rand(n, 416, 416, 3),
            np.zeros((n, 1)),
        )
    )
    .batch(8)
    .repeat()
)

@tf.function
def inspect_dataset(dataset):
    for _ in tqdm(dataset, ""iterating through dataset""):
        pass


inspect_dataset(dataset)
```

![image](https://user-images.githubusercontent.com/17848620/87487333-aadaeb80-c63d-11ea-8d51-8e1c4a794abf.png)

"
41394,InvalidArgumentError: Operation 'TensorListPushBack_489' has no attr named '_XlaCompile',"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
Yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
AWS EC2 instance with image of Deep Learning Base AMI (Amazon Linux)
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
N/A
-   **TensorFlow installed from (source or binary)**:
Source
-   **TensorFlow version (use command below)**:
2.2.0
-   **Python version**:
3.6.6
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
cuda = V10.0.130; cudnn =  CUDNN_MAJOR 7; CUDNN_MINOR 5; CUDNN_PATCHLEVEL 1
-   **GPU model and memory**:
Tesla T4; 8G
-   **Exact command to reproduce**:




### Describe the problem
I am trying to implement a custom model following a paper. This model itself should not matter too much. The problem I encountered is that the code got stuck in ``` y_pred = self(x, training = True)``` in the train_step function in the Dynamic_Loss class. It did not return an error. After running for an hour, I interrupted the kernel and it returns the error message saying ""InvalidArgumentError: Operation 'TensorListPushBack_489' has no attr named '_XlaCompile'"" and ""InvalidArgumentError: Operation 'While' has no attr named '_XlaCompile'"". I am not sure what the issue is because the code worked before I set persistent = True in tf.Gradient_Tape
### Source code / logs


```python

class Tem_Agg:

	""""""
	This class is to aggregate the representations in the temporal dimension using
	an autoencoder architecture
	""""""

	## the length of the time stamp
	max_length = 300

	def __init__(self,  length, timestamp, model = ""baseline""):


		## the number of timestamp in the batch
		self.timestamp = timestamp
		## the dimension of the vectors to be aggregated
		self.vec_length = length
		## the mode to use: baseline model or dynamic model
		assert model in [""baseline"", ""dynamic""]
		self.model = model


	def build_model(self):

		""""""
		This is an autoecoder model where the input is used to reconstruct the input itself
		and predict the future

		for reference: https://arxiv.org/pdf/1502.04681.pdf
		""""""

		## the input layer

		inp = Input(shape = (self.timestamp - 5, self.vec_length))

		if self.model == ""dynamic"":

			weights = Input(shape = (2, ))

			## mean
			mean_weights = Dense(inp.shape[1], activation = ""relu"")(weights)
			mean_weights = tf.expand_dims(mean_weights, axis = -1)
			## standard deviation
			std_weights = Dense(inp.shape[1], activation = ""relu"")(weights)
			std_weights = tf.expand_dims(std_weights, axis = -1)

		## encoder

		enc_first = LSTM(self.vec_length // 2, return_sequences = True)(inp)

		if self.model == ""dynamic"":
			enc_first = Tem_Agg.FiLM(enc_first, mean_weights, std_weights)


		enc_second = LSTM(self.vec_length // 4, return_sequences = True)(enc_first)


		if self.model == ""dynamic"":
			enc_second = Tem_Agg.FiLM(enc_second, mean_weights, std_weights)

		## the full representation has the same dimension as the input so that
		## the hierarchy model can use it in different levels;
		enc_second_full = LSTM(self.vec_length, name = ""representation"")(enc_first)


		## decoder for reconstruction

		dec_first = LSTM(self.vec_length // 2, return_sequences = True)(enc_second)
		recon = LSTM(inp.shape[-1], return_sequences = True)(dec_first)

		## predict the future vectors
		predicted_vec_1 = Dense(inp.shape[-1], activation = ""relu"")(enc_second_full)
		predicted_vec_2 = Dense(inp.shape[-1], activation = ""relu"")(predicted_vec_1)
		predicted_vec_3 = Dense(inp.shape[-1], activation = ""relu"")(predicted_vec_2)
		predicted_vec_4 = Dense(inp.shape[-1], activation = ""relu"")(predicted_vec_3)
		predicted_vec_5 = Dense(inp.shape[-1], activation = ""relu"")(predicted_vec_4)


		if self.model == ""baseline"":
			model = keras.Model(inputs = inp, outputs = [recon, predicted_vec_5])
			model.compile(""adam"", loss = MeanAbsolutePercentageError())
		else:
		
			model = Dynamic_Loss(inputs = [inp, weights], outputs = [recon, predicted_vec_5, weights])
			

			model.compile(""adam"")

		return model


	@staticmethod
	def FiLM(tensor, mean_weights, std_weights):
		""""""
		This is the Feature-wise Linear Modulation (FiLM) operation from the paper

		Inputs:
			tensor: the tensor from the layer
			mean_weight: weights to be multiplied to the tensor
			std_weights: weights to be added to product of tensor and mean_weight

		Output:
			output: the output tensor after the operation

		""""""

		tensor = Multiply()([tensor, mean_weights])
		output = Add()([tensor, std_weights])
		return output


class Dynamic_Loss(keras.Model):

	""""""
	This is the class to learn the distribution of the loss for the multi-tasks
	model. We will pass the weight vector in the training and inference stages

	for reference: https://openreview.net/pdf?id=HyxY6JHKwr

	""""""

	def train_step(self, data):

		""""""
		Overwrite the training loop

		Input:
			data: the data we feed into the fit() function; a tf Dataset object
		""""""

		## unpacking the data
		x, y = data

		## unpacking y
		recon, future = y

		## loss
		mape = MeanAbsolutePercentageError()

		trainable_vars = self.trainable_variables


		with tf.GradientTape(persistent = True) as tape_1:

			## the predicted values
			y_pred = self(x, training = True)
			

			recon_pred, future_pred, weights = y_pred
		

			## the two different losses
			recon_loss = mape(recon, recon_pred)
			#recon_gradients = tape_1.gradient(recon_loss, trainable_vars)



			future_loss = mape(future, future_pred)
			

		
		recon_gradients = tape_1.gradient(recon_loss, trainable_vars)
		
		future_gradients = tape_1.gradient(future_loss, trainable_vars)




		## the gradient of the sum is the sum of the gradient
		gradients = weights.numpy()[0] * recon_gradients + weights.numpy()[1] * future_gradients

		## applying gradients
		self.optimizer.apply_gradients(zip(gradients, trainable_vars))
```"
41392,Debugging predictions step with tf.debugging.experimental.enable_dump_debug_info throws an error.,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I slightly modified a colab notebook from tf ranking package to demonstrate the issue. The modified notebook is  https://colab.research.google.com/drive/1FVkkYCo_ZtN6mu-kmF1IqDYI4C1-jxaw?usp=sharing 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2.0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


**Describe the current behavior**
I get error while trying to log debug information with [tf.debugging.experimental.enable_dump_debug_info](https://www.tensorflow.org/api_docs/python/tf/debugging/experimental/enable_dump_debug_info) .
This is the only change to the notebook is that I added a cell with `tf.debugging.experimental.enable_dump_debug_info(""/tmp/debug"")` right before calling predictions (in the end of the notebook).
The error is 
```
INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-33-81eaf5e40876> in <module>()
----> 1 x = next(predictions)
      2 assert(len(x) == _LIST_SIZE)  ## Note that this includes padding.

13 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/normalization.py in build(self, input_shape)
    374       if axis_to_dim[x] is None:
    375         raise ValueError('Input has undefined `axis` dimension. Input shape: ',
--> 376                          input_shape)
    377     self.input_spec = InputSpec(ndim=ndims, axes=axis_to_dim)
    378 

ValueError: ('Input has undefined `axis` dimension. Input shape: ', TensorShape([None, None]))
```
**Describe the expected behavior**
I expect to get debugging information while calling predictions.

**Standalone code to reproduce the issue**
To reproduce run this notebook https://colab.research.google.com/drive/1FVkkYCo_ZtN6mu-kmF1IqDYI4C1-jxaw?usp=sharing 
"
41391,exponential_avg_factor not in Op FusedBatchNormV3: model deployed using Golang Tensorflow v2.0.2,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Training:
*TensorFlow v2.2.0
*GPU distributed training with 4 GPUs
*Use ```strategy = tf.distribute.MirroredStrategy()``` for multiple GPU training
*MobileNet model is trained using ```model.fit```
*Trained model is saved using ```.keras_model.save(path)```
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
*Model is trained on Machine type ```n1-standard-8 (8 vCPUs, 30 GB memory)```
*GCP VM with image ```c5-deeplearning-tf2-2-2-cu101-v20200706```
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
*Raspberry Pi Golang model deployment
*Model optimization
- TensorFlow installed from (source or binary):
*Training model in GCP VM ( ```c5-deeplearning-tf2-2-2-cu101-v20200706``` )
*Local using ```pip install tensorflow==2.2.0```
- TensorFlow version (use command below):
*Training model in GCP VM ( ```c5-deeplearning-tf2-2-2-cu101-v20200706``` )
*Local using ```pip install tensorflow==2.2.0```
- Python version:
3.7
- Bazel version (if compiling from source):
0.24.1
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:
4 x NVIDIA Tesla K80, 11GB each

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
*Model trained on GCP VM with 4 GPU using TensorFlow version as per google deep learning image ```c5-deeplearning-tf2-2-2-cu101-v20200706``` is v2.2.2
*Model is saved using ```keras_model.save(path)```
*I am facing issue with model freeze and optimization, so that I can deploy that model in Golang Tensorflow
1. Freeze model  using ```tf.compat.v1.graph_util.convert_variables_to_constants``` 
I have tool the keras saved model directory from GCP VM and try to freeze the model in local.
```
from tensorflow.python.framework import graph_io
tf.compat.v1.disable_v2_behavior()
tf.keras.backend.set_learning_phase(0)
tf.compat.v1.keras.backend.set_floatx('float16')
# model_dir: keras model saved dir
model = tf.keras.models.load_model(model_dir, compile=False)
output_node_names = [""input"",""out1/Softmax"",""out2/Softmax""]
sess = tf.compat.v1.keras.backend.get_session()
frozen_graph = tf.compat.v1.graph_util.convert_variables_to_constants(sess, 
                                                                              sess.graph.as_graph_def(), 
                                                                              output_node_names)
input_names = [""input""]
output_names = [""out1/Softmax"",""out2/Softmax""]
frozen_graph = optimize_for_inference_lib.optimize_for_inference(frozen_graph,
                                                                         input_names,
                                                                         output_names,
                                                                         input_node.dtype.as_datatype_enum)
graph_io.write_graph(frozen_graph, output_dir, freezed_model_file, as_text=False)
```
2. The output model .pb (single file), is freezed using above code.
while freezing following is the warning
```
WARNING:tensorflow:Didn't find expected Conv2D or DepthwiseConv2dNative input to 'depth_conv_bn_5/FusedBatchNormV3'
```
3. Then converted the freezed model to builder.SaveModelBuilder
```
tf.compat.v1.disable_v2_behavior()
tf.compat.v1.keras.backend.set_floatx('float16')
with gfile.FastGFile(freezed_model.pb, ""rb"") as f:
    graph_def = tf.compat.v1.GraphDef()
    byte = f.read()
    graph_def.ParseFromString(byte)
    tf.import_graph_def(graph_def, name='')
count_nodes = 0
for node in tf.compat.v1.Session().graph_def.node:
    count_nodes += 1
print(""Number of nodes that not optimized freezed model has: "", count_nodes)
# golang_tf_model_dir: where the golang specific model is prepared
builder = saved_model_builder.SavedModelBuilder(golang_tf_model_dir)
with tf.compat.v1.Session() as sess:
    builder.add_meta_graph_and_variables(sess=sess,
                                    tags=[""model""],
                                    clear_devices=True)
    
builder.save()
``` 
4. After this when I am trying to load this model in golang tensorflow version v2.0.2
I found this error
```
 NodeDef mentions attr 'exponential_avg_factor' not in Op<name=FusedBatchNormV3; signature=x:T, scale:U, offset:U, mean:U, variance:U -> y:T, batch_mean:U, batch_variance:U, reserve_space_1:U, reserve_space_2:U, reserve_space_3:U; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT]; attr=U:type,allowed=[DT_FLOAT]; attr=epsilon:float,default=0.0001; attr=data_format:string,default=""NHWC"",allowed=[""NHWC"", ""NCHW""]; attr=is_training:bool,default=true>; NodeDef: {{node depth_conv_bn_5/FusedBatchNormV3}}. (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).
FAIL
```

**Describe the expected behavior**
Earlier I have used tensorflow version 1.12.0 to my model training, freeze and deployment and I did not see FusedBatchNormV3
but in recent changes in TensorFlow v2.0 BatchNormalization layer is freezed to FusedBatchNormV3 which is causing issue.

Earlier same trained model on Tf1.12.0 has FusedBatchNorm only no FusedBatchNormV3.

My expectation
1. How to exponential_avg_factor will be considered in FusedBatchNormV3
2. GraphDef-generating binary is ```tf.compat.v1.GraphDef()``` 
3. GraphDef-interpreting binary is ```graph_def.pb2``` i think because deployment is in TensorFlow version v2.0.2
Or I am doing something wrong.

**Standalone code to reproduce the issue**
For golang model loading code
```
model := tf.LoadSavedModel(<golang_tf_model_dir>, []string{<model_tag>}, nil /*options*/)
```
model_tag is default ```serve``` is not changed while freezing
golang_tf_model_dir: is mentioned earlier

**Other info / logs**
Attached logs earlier "
41389,Segmentation fault in subgraph.h when running tensorflow lite in ROS Foxy,"### System information 

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: I used example code to write a Ros node
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ROS Foxy Docker Container (Ubuntu 20.04), currently testing on a x86_64 system but will deploy it to a aarch64 system
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: No
-   **TensorFlow installed from (source or binary)**: Source
-   **TensorFlow version (use command below)**: Git tag d855adfc5a0195788bf5f92c3c7352e638aa1109
-   **Python version**: None, I´m using C++
-   **Bazel version (if compiling from source)**: None, I´m using Cmake 3.17
-   **GCC/Compiler version (if compiling from source)**: 4:9.3.0-1ubuntu2
-   **CUDA/cuDNN version**: None
-   **GPU model and memory**: Google Coral Chip
-   **Exact command to reproduce**:


### Describe the problem
I already described the problem [here](https://github.com/Namburger/edgetpu-minimal-example/issues/2)
To make things short: I tried using [these examples](https://github.com/Namburger/edgetpu-minimal-example) in ROS but got a Segmentation fault error when i tried calling interpreter.inputs()[index];
I´ve been told to ask here since it seems to be a Tensorflow API issue.

### Source code / logs
Start a docker container: docker run -it ros:foxy
Execute `source /opt/ros/foxy/setup.bash`
Run 
```
apt update
apt install unzip
apt install curl
apt install libusb-1.0-0
```
Update cmake like this:

```
apt install -y wget
apt-get install libssl-dev
wget https://github.com/Kitware/CMake/releases/download/v3.17.0/cmake-3.17.0.tar.gz
tar xvf cmake-3.17.0.tar.gz
rm cmake-3.17.0.tar.gz
cd cmake-3.17.0/
./configure
make
make install
```

Create a dev_ws directory in the home folder
cd into /home/dev_ws
create a src folder
copy the zip file folder nn_tflite into the src folder
Directory structure should be
home
...|dev_ws
......| src
.........| dev_ws
............| nn_tflite


cd into /home/dev_ws and execute `colcon build --packages-select nn_tflite --symlink-install`
Execute  `. install/setup.bash`
Run the ros node with `ros2 run nn_tflite tflite_nn`

Running gbd:
`cd install/nn_tflite/lib/nn_tflite/`
`gbd --args ./tflite_nn`

[nn_tflite.zip](https://github.com/tensorflow/tensorflow/files/4925761/nn_tflite.zip)


"
41388,2020-07-14 nightlies cannot import `tensorflow`,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): gLinux (like Debian)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version: pip install tf-nightly==2.4.0.dev20200714
- Python version: Python 3.7.7
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**

`import tensorflow` fails.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
$ cd ""$(mktemp -d)""
$ virtualenv -q -p python3.7 ./ve
$ . ./ve/bin/activate
(ve) $ pip install tf-nightly
Collecting tf-nightly
  Using cached tf_nightly-2.4.0.dev20200714-cp37-cp37m-manylinux2010_x86_64.whl (142.9 MB)
Requirement already satisfied: six>=1.12.0 in ./ve/lib/python3.7/site-packages (from tf-nightly) (1.14.0)
Collecting h5py<2.11.0,>=2.10.0
  Using cached h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)
Processing /HOMEDIR/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2/termcolor-1.1.0-py3-none-any.whl
Collecting google-pasta>=0.1.8
  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)
Requirement already satisfied: wheel>=0.26 in ./ve/lib/python3.7/site-packages (from tf-nightly) (0.34.2)
Collecting grpcio>=1.8.6
  Using cached grpcio-1.30.0-cp37-cp37m-manylinux2010_x86_64.whl (3.0 MB)
Collecting gast==0.3.3
  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)
Collecting tf-estimator-nightly
  Using cached tf_estimator_nightly-2.4.0.dev2020071401-py2.py3-none-any.whl (459 kB)
Collecting opt-einsum>=2.3.2
  Using cached opt_einsum-3.2.1-py3-none-any.whl (63 kB)
Collecting tb-nightly<2.4.0a0,>=2.3.0a0
  Using cached tb_nightly-2.3.0a20200713-py3-none-any.whl (6.8 MB)
Collecting protobuf>=3.9.2
  Using cached protobuf-3.12.2-cp37-cp37m-manylinux1_x86_64.whl (1.3 MB)
Processing /HOMEDIR/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6/wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl
Collecting keras-preprocessing<1.2,>=1.1.1
  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)
Processing /HOMEDIR/.cache/pip/wheels/cc/af/1a/498a24d0730ef484019e007bb9e8cef3ac00311a672c049a3e/absl_py-0.9.0-py3-none-any.whl
Collecting numpy<1.19.0,>=1.16.0
  Using cached numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)
Collecting astunparse==1.6.3
  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)
Collecting werkzeug>=0.11.15
  Using cached Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)
Collecting tensorboard-plugin-wit>=1.6.0
  Using cached tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)
Requirement already satisfied: setuptools>=41.0.0 in ./ve/lib/python3.7/site-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (44.0.0)
Collecting google-auth<2,>=1.6.3
  Using cached google_auth-1.19.0-py2.py3-none-any.whl (91 kB)
Collecting google-auth-oauthlib<0.5,>=0.4.1
  Using cached google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)
Requirement already satisfied: requests<3,>=2.21.0 in ./ve/lib/python3.7/site-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (2.23.0)
Collecting markdown>=2.6.8
  Using cached Markdown-3.2.2-py3-none-any.whl (88 kB)
Collecting cachetools<5.0,>=2.0.0
  Using cached cachetools-4.1.1-py3-none-any.whl (10 kB)
Collecting rsa<5,>=3.1.4; python_version >= ""3""
  Using cached rsa-4.6-py3-none-any.whl (47 kB)
Collecting pyasn1-modules>=0.2.1
  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)
Collecting requests-oauthlib>=0.7.0
  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)
Collecting importlib-metadata; python_version < ""3.8""
  Using cached importlib_metadata-1.7.0-py2.py3-none-any.whl (31 kB)
Collecting pyasn1>=0.1.3
  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)
Collecting oauthlib>=3.0.0
  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)
Collecting zipp>=0.5
  Using cached zipp-3.1.0-py3-none-any.whl (4.9 kB)
Installing collected packages: numpy, h5py, termcolor, google-pasta, grpcio, gast, tf-estimator-nightly, opt-einsum, werkzeug, tensorboard-plugin-wit, absl-py, cachetools, pyasn1, rsa, pyasn1-modules, google-auth, protobuf, oauthlib, requests-oauthlib, google-auth-oauthlib, zipp, importlib-metadata, markdown, tb-nightly, wrapt, keras-preprocessing, astunparse, tf-nightly
Successfully installed absl-py-0.9.0 astunparse-1.6.3 cachetools-4.1.1 gast-0.3.3 google-auth-1.19.0 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.30.0 h5py-2.10.0 importlib-metadata-1.7.0 keras-preprocessing-1.1.2 markdown-3.2.2 numpy-1.18.5 oauthlib-3.1.0 opt-einsum-3.2.1 protobuf-3.12.2 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.6 tb-nightly-2.3.0a20200713 tensorboard-plugin-wit-1.7.0 termcolor-1.1.0 tf-estimator-nightly-2.4.0.dev2020071401 tf-nightly-2.4.0.dev20200714 werkzeug-1.0.1 wrapt-1.12.1 zipp-3.1.0
(ve) $ python -c 'import tensorflow'
Traceback (most recent call last):
  File ""/tmp/tmp.VuldV9r4ny/ve/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: libcudart.so.10.1: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/tmp/tmp.VuldV9r4ny/ve/lib/python3.7/site-packages/tensorflow/__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""/tmp/tmp.VuldV9r4ny/ve/lib/python3.7/site-packages/tensorflow/python/__init__.py"", line 40, in <module>
    from tensorflow.python.eager import context
  File ""/tmp/tmp.VuldV9r4ny/ve/lib/python3.7/site-packages/tensorflow/python/eager/context.py"", line 35, in <module>
    from tensorflow.python import pywrap_tfe
  File ""/tmp/tmp.VuldV9r4ny/ve/lib/python3.7/site-packages/tensorflow/python/pywrap_tfe.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/tmp/tmp.VuldV9r4ny/ve/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 83, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/tmp/tmp.VuldV9r4ny/ve/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: libcudart.so.10.1: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
(ve) $ 
```

**Any other info / logs**

Nope.
"
41387,model_server_config requires platform information,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

https://www.tensorflow.org/tfx/serving/serving_config#model_server_config_details

## Description of the issue (what needs changing):

The model configuration file in the TF Serving docs (link above) should include the `model_platform`. If left out, TF Serving stops with the error below:

```
Error: Invalid argument: Illegal setting neither ModelServerConfig::model_type (deprecated) nor ModelServerConfig::model_platform.
```

### Clear description

The configuration example should be updated from 

```
model_config_list {
  config {
    name: 'my_first_model'
    base_path: '/tmp/my_first_model/'
  }
  config {
    name: 'my_second_model'
    base_path: '/tmp/my_second_model/'
  }
}
```

to

```
model_config_list {
  config {
    name: 'my_first_model'
    base_path: '/tmp/my_first_model/'
    model_platform: 'tensorflow'
  }
  config {
    name: 'my_second_model'
    base_path: '/tmp/my_second_model/'
    model_platform: 'tensorflow'
  }
}
```


### Parameters defined

Please see the example above


### Submit a pull request?

PR requested here: https://github.com/tensorflow/serving/pull/1687
"
41382,error trying to link to Tensorflow C library,"After extracting the libraries to my `/usr/local/` directory from

https://www.tensorflow.org/install/lang_c#download

I get the following:

```
/usr/bin/ld: /usr/local/lib/libtensorflow.so: .dynsym local symbol at index 1603 (>= sh_info of 1)
/usr/bin/ld: /usr/local/lib/libtensorflow.so: .dynsym local symbol at index 2710 (>= sh_info of 1)
/usr/bin/ld: /usr/local/lib/libtensorflow.so: .dynsym local symbol at index 2711 (>= sh_info of 1)
/usr/bin/ld: /usr/local/lib/libtensorflow.so: .dynsym local symbol at index 1603 (>= sh_info of 1)
/usr/bin/ld: /usr/local/lib/libtensorflow.so: .dynsym local symbol at index 2710 (>= sh_info of 1)
/usr/bin/ld: /usr/local/lib/libtensorflow.so: .dynsym local symbol at index 2711 (>= sh_info of 1)
```"
41380,Windows - tensorflow.python.framework.errors_impl.UnknownError: Failed to rename: ,"**System information**

OS Name:                   Microsoft Windows 10 Enterprise
OS Version:                10.0.17763 N/A Build 17763
TensorFlow installed using 'conda'.
tensorflow v2.2.0-rc4-8-g2b96f3662b 2.2.0
Python 3.6.10 |Anaconda, Inc.| (default, Jan  7 2020, 15:18:16) [MSC v.1916 64 bit (AMD64)] on win32

**Describe the current behavior**

Saving checkpoint files from tensorflow is failing on Windows 10.

```
Traceback (most recent call last):
  File ""C:\Users\<redacted>\Miniconda3\envs\<redacted>\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""C:\Users\<redacted>\Miniconda3\envs\<redacted>\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Git\<redacted>\tests\integration\validate_train_model.py"", line 216, in <module>
    main()
  File ""C:\Git\<redacted>\tests\integration\validate_train_model.py"", line 176, in main
    fig_save_freq = fig_save_freq)
  File ""c:\git\<redacted>\src\pointnet\model.py"", line 640, in fit
    self.save_best_model()
  File ""c:\git\<redacted>\src\pointnet\model.py"", line 493, in save_best_model
    check_interval = False)
  File ""C:\Users\<redacted>\Miniconda3\envs\<redacted>\lib\site-packages\tensorflow\python\training\checkpoint_management.py"", line 823, in save
    self._record_state()
  File ""C:\Users\<redacted>\Miniconda3\envs\<redacted>\lib\site-packages\tensorflow\python\training\checkpoint_management.py"", line 728, in _record_state
    save_relative_paths=True)
  File ""C:\Users\<redacted>\Miniconda3\envs\<redacted>\lib\site-packages\tensorflow\python\training\checkpoint_management.py"", line 248, in update_checkpoint_state_internal
    text_format.MessageToString(ckpt))
  File ""C:\Users\<redacted>\Miniconda3\envs\<redacted>\lib\site-packages\tensorflow\python\lib\io\file_io.py"", line 532, in atomic_write_string_to_file
    rename(temp_pathname, filename, overwrite)
  File ""C:\Users\<redacted>\Miniconda3\envs\<redacted>\lib\site-packages\tensorflow\python\lib\io\file_io.py"", line 491, in rename
    rename_v2(oldname, newname, overwrite)
  File ""C:\Users\<redacted>\Miniconda3\envs\<redacted>\lib\site-packages\tensorflow\python\lib\io\file_io.py"", line 508, in rename_v2
    compat.as_bytes(src), compat.as_bytes(dst), overwrite)
tensorflow.python.framework.errors_impl.UnknownError: Failed to rename: tests\files\checkpoints\0000_00_00_00_00_00\checkpoint.tmpc6ee5d6bc5a445c884bba8c3acadf01f to: tests\files\checkpoints\0000_00_00_00_00_00\checkpoint : Access is denied.
; Input/output error
```

Problem traced to:
tensorflow.python.lib.io.file_io, line 532, function atomic_write_string_to_file

From debugging, tensorflow attempts to create, then overwrite a file while saving a checkpoint.  For some reason, the 'overwrite' parameter, although set to True, does nothing.  This causes the rename to fail (since the file seems to get created earlier in the checkpoint save process).

We tried deleting the 'checkpoint' file before the 'save', but the checkpoint file that it's trying to overwrite appears to be created as a part of the 'save' call.

I was able to get checkpoint saving working again by modifying atomic_write_string_to_file as follows.  My change checks for existence of the rename target and deletes it using os.remove if overwrite is True, rather than relying on the tensorflow custom machinery that doesn't seem to be working:

```python
def atomic_write_string_to_file(filename, contents, overwrite=True):
  if not has_atomic_move(filename):
    write_string_to_file(filename, contents)
  else:
    temp_pathname = filename + "".tmp"" + uuid.uuid4().hex
    write_string_to_file(temp_pathname, contents)
    try:
      if overwrite and os.path.exists(filename):
        os.remove(filename)
      rename(temp_pathname, filename, overwrite)
    except errors.OpError:
      delete_file(temp_pathname)
      raise
```

The stack trace we got suggested that this is the same issue as someone was reporting for tensorflow.models:
https://github.com/tensorflow/models/issues/4177

**Describe the expected behavior**

We should be able to successfully save a checkpoint on Windows 10.
"
41378,Deprecation of validation_data in tensorflow.keras.Callbacks,"Hi,

why the `validation_data` attribute has been dropped from  *callbacks* ?

The only alternative is to set `validation_data` as a constructor parameter in the `Callback`, I can understand that this is a solution, but why we lost that useful feature? Could be recovered in subsequent releases? Previously callbacks where able to use  validation data generated with `validation_split` on the fit call.

In the documentation  https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback there is any example that uses validation data, it is uncommon to lack such an important example in a flawless overall documentation like Tensorflow has right now.

Thanks.
Kind regards."
41377,Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize,"I am using tensorflow-gpu 1.15.3 and keras 2.3.1, with python 3.7 and cuda 10.0. I confirm that the tensorflow version matches up with the python and cuda (including cudnn).  The tensorflow-gpu works fine when I test it with a demon example on image classification (e.g., https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py). But when I tried a large cnn model such as Yolo-v3 (i.e., https://github.com/qqwweee/keras-yolo3), it gives the error: 
```
UnknownError: 2 root error(s) found.
  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[{{node conv2d_1/convolution}}]]
	 [[Mean/_2311]]
  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[{{node conv2d_1/convolution}}]]
0 successful operations.
0 derived errors ignored.
```

Most often the above error is due to the mismatch between tensorflow-gpu and cuda versions. But it seems not to be that case.

It is very interesting to note if I first run the small image classification test, which would go smoothly without any error, and thereafter run the Yolo-v3, the Yolo training will be fine without the ""Failled to get convolution algorithm"".  But if I directly run the yolo training, it won't proceed because of the error. 

Anyone has similar experiences? What possible reasons for the cuda error?"
41376,More info on context manager/`strategy.scope` in Custom training loops with tf.distribute.MirroredStrategy(),"Hi @lamberta and team,

Just some ideas for the **Custom training loops** tutorial (with `tf.distribute.MirroredStrategy()`):

**URLs:** 

https://www.tensorflow.org/tutorials/distribute/custom_training#define_the_loss_function
https://www.tensorflow.org/tutorials/distribute/custom_training#define_the_metrics_to_track_loss_and_accuracy
https://www.tensorflow.org/tutorials/distribute/custom_training#create_a_strategy_to_distribute_the_variables_and_the_graph

**Description:**

In the tutorial, you create a new strategy (`strategy = tf.distribute.MirroredStrategy()`), which has a context manager—`strategy.scope`. Similar to the **[Distributed training with Keras](https://www.tensorflow.org/tutorials/distribute/keras#define_distribution_strategy)** tutorial, maybe you can also explicitly mention what the scope does in the **Custom training loops** tutorial, because some users may not have studied that in greater depth. And, it would make the tutorial more consistent with some of the other ones under **Tutorials** > **Advanced** > **Distributed training**. 

So, to ensure the users don't have to use search here, especially if some of them are new to this, it may be useful to provide a bit more background on the context manager.

Note that inside the context manager—`strategy.scope`—we have to define a number of things: the loss function, metrics, the model itself, Adam optimizer, and checkpoint. And the tutorial separates these items into different sections and provides detailed explanations of each step. Therefore, I suggest maybe we add a few comments as follows:

**Examples:**

```python
#  <NEW> Enclose the loss function call inside the context manager.
with strategy.scope():
  # <original comment>
  <define a method for loss computation>
```

```python
# <NEW> test loss and training and test accuracy metrics must be created under `strategy.scope`
with strategy.scope():
  test_loss = tf.keras.metrics.Mean(name='test_loss')
  train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(
      name='train_accuracy')
  test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(
      name='test_accuracy')
```

Additionally, this note from the tutorial here may also be improved (the diff is _italicized_) as follows:

> **Note:** You can put all the code below inside a single scope _—the `strategy.scope` context manager that instructs how to split the training among devices_. We are dividing it into several code cells for illustration purposes.

Lastly, similar to this [step](https://www.tensorflow.org/tutorials/distribute/keras#define_distribution_strategy), we can add a short decriptive paragraph (_italicized_) to explain what we're doing when defining `strategy` as `tf.distribute.MirroredStrategy()`:

> _Create a `MirroredStrategy` object to handle distribution, and provides a context manager (`tf.distribute.MirroredStrategy.scope`) to build your model inside:_
>
> ```python
> # If the list of devices is not specified in the
> # `tf.distribute.MirroredStrategy` constructor, it will be auto-detected.
> strategy = tf.distribute.MirroredStrategy()
> ```

---

There is another similar example from the **Multi-worker training with Keras** tutorial:

- [Train the model with MultiWorkerMirroredStrategy](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#train_the_model_with_multiworkermirroredstrategy):

```python
with strategy.scope():
  # Model building/compiling need to be within `strategy.scope()`.
  multi_worker_model = build_and_compile_cnn_model()
```

**PRs:**

Can submit a PR if you OK (some) of the ideas.

Cheers!"
41375,ModuleNotFoundError: No module named 'tensorflow.python.util',"import tensorflow as tf
2020-07-14 21:15:42.824490: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found
2020-07-14 21:15:42.827687: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\廖宇杰\AppData\Roaming\Python\Python37\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\廖宇杰\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\__init__.py"", line 64, in <module>
    from tensorflow.python.framework.framework_lib import *  # pylint: disable=redefined-builtin
  File ""C:\Users\廖宇杰\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\framework\framework_lib.py"", line 24, in <module>
    from tensorflow.python.framework.device import DeviceSpec
  File ""C:\Users\廖宇杰\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\framework\device.py"", line 24, in <module>
    from tensorflow.python.framework import device_spec
  File ""C:\Users\廖宇杰\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\framework\device_spec.py"", line 21, in <module>
    from tensorflow.python.util.tf_export import tf_export
ModuleNotFoundError: No module named 'tensorflow.python.util'"
41374,TensorFlow 2.2.0 doesn't detect GPU with CUDA version 10.2,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 2.2.0
- Python version: 3.6.10
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.2
- GPU model and memory: Tesla K40m

**Describe the current behavior**
Keras (version 2.4.3) with tensorflow as backend doesn't detect the GPU. This is verified by running:
`tf.config.experimental.list_physical_devices('GPU')`
returns emplty list [].

Also, checked as:
`from tensorflow.python.client import device_lib`
`print(device_lib.list_local_devices())`

returns: 
_[name: ""/device:CPU:0""
device_type: ""CPU""
memory_limit: 268435456
locality {
}
incarnation: 11064916497553704899
, name: ""/device:XLA_CPU:0""
device_type: ""XLA_CPU""
memory_limit: 17179869184
locality {
}
incarnation: 5592130336569042773
physical_device_desc: ""device: XLA_CPU device""
]_


**Describe the expected behavior**
It should detect the GPU as detected by PyTorch as:
`torch.cuda.is_available()`
Outputs: True

`torch.cuda.current_device()`
Outputs: 0

`torch.cuda.get_device_name(0)`
Outputs: 'Tesla K40m'

**Standalone code to reproduce the issue**
Not required in this case as it seems to be some version mismatch issue.

**Other info / logs** 
Attaching the tf_env.txt output file for the environment settings.
[tf_env.txt](https://github.com/tensorflow/tensorflow/files/4918521/tf_env.txt)

Thanks!
"
41373,Not able to use libtensorflow-lite.a static library in Android studio for including headers.,"

**System information**
- OS Platform and Distribution (windows 10):
- Mobile device (oppo reno2) i am building and running on this device through android studio.
- TensorFlow installed from (source or binary): Ubuntu terminal through pip3
- TensorFlow version:
- Python version: python 3.8.2
- Installed using pip 
- Bazel version (if compiling from source): I have created libtensorflow-lite.a using bazle . Now trying to directly use the lib. 
- GCC/Compiler version (if compiling from source): gnu++11
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
![image](https://user-images.githubusercontent.com/23177883/87418459-5872de00-c5ef-11ea-8b71-9f1d69a8a161.png)
I am trying to inclue interpreter.h, model.h etc in native c code. But it is showing file not found. If i add tensorflow folder and add the include paths in cmake then build is sucecssfull but include dont work- like I am not able to load model or run interpreter. 
**Provide the exact sequence of commands / steps that you executed before running into the problem**
I have build the libtensorflow-lite.a for arm64 platform and now using it as static library to include the required dependencies of tflite, to code in native cpp in Android studio using NDK.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
41372,freezing the app(.py to exe),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):N/A
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary):  source
- TensorFlow version (use command below): 1.15.0
- Python version:3.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:N/A
- GPU model and memory:

**Describe the current behavior**
I have tried to freeze the custom app with auto-py-to-exe and it is giving mw the error of ""ImportError: No module named 'tensorflow_core.python'""

can anyone please help me with it

Thanks in advance
![tf_error](https://user-images.githubusercontent.com/61754297/87411293-f317f080-c5e0-11ea-99d8-64a6914ace7c.PNG)
"
41370,tf.math.acos raises UnimplementedError for complex tensors,"**System information**
- OS Platform and Distribution: Windows10 1909
- TensorFlow installed from : binary
- TensorFlow version: 2.2.0rc2
- Python version: 3.8.0
- CUDA/cuDNN version: None

**Describe the current behavior**
The documentation says complex inputs are allowed, but tf.math.acos and tf.math.asin raises UnimplementedError for complex64 or complex128 inputs.
I found that tf.math.acos and tf.math.asin use Atan2 which do not support complex inputs. So these ops may need a new implement without Atan2 when the input is complex.

**Standalone code to reproduce the issue**
```python3
import tensorflow as tf 
a = tf.constant([1j], dtype=tf.complex64) 
print(tf.math.acos(a))
```
**Describe the expected behavior**
```
tf.Tensor([1.5708-0.88137j], shape=(1,), dtype=complex64)
```
**Output**
```
2020-07-14 15:50:38.372937: W tensorflow/core/framework/op_kernel.cc:1752] OP_REQUIRES failed at xla_compile_on_demand_op.cc:216 : Unimplemented: binary complex op 'atan2'
Traceback (most recent call last):
  File ""acos_err.py"", line 15, in <module>
    tf.math.acos(a)
  File ""C:\Users\root\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\ops\gen_math_ops.py"", line 193, in acos
    _ops.raise_from_not_ok_status(e, name)
  File ""C:\Users\root\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\framework\ops.py"", line 6653, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.UnimplementedError: binary complex op 'atan2' [Op:Acos]
```"
41369,bazel build tensorflow failed,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**  
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO
- TensorFlow installed from (source or binary):  
- TensorFlow version: latest version
- Python version: 
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 3.1
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 9
- GPU model and memory:


I want to use docker build a tensorflow-gpu container, Here is my Dockerfile:


`FROM nvidia/cuda:9.0-cudnn7-devel-ubuntu16.04

RUN apt-get update

RUN apt-get install -y wget \
                vim \
                cmake


RUN apt-get -y install \
    python3 \
    python3-pip \
    python3-setuptools`

and build command is that:

docker build -t tensorlfow_gpu -f Dockerfile .

docker run -d --name tensorflow-gpu-demo -v ***/tensorflow:tensorflow tensorflow_gpu

**Describe the problem**

And I login the container 
install bazel3.1 and some require tag and start build tensorflow

Then I get these errors:

C++ compilation of rule '//tensorflow/core/kernels:determinant_op_gpu' failed (Exit 1)

C++ compilation of rule '//tensorflow/core/util:einsum_op_util' failed (Killed)

C++ compilation of rule '//tensorflow/core/kernels:roll_op_gpu' failed (Exit 1)

I found every time the error raise, the error is different, What should I do to compile tensorflow successful.


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
41368,"twice embedding_lookup‘ result is the same as once embedding_lookup,but gradients not","<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I have to use twice embedding_lookup to do the same thing as once embedding_lookup,but their gradients dont act the same

`
import tensorflow as tf
import numpy as np

a = tf.convert_to_tensor(np.random.random([2048,32]))
b = tf.convert_to_tensor([1,2,2,3,4,4,5,1,2,3,4,5])
sess = tf.Session()
b_,idx = tf.unique(b)
print(sess.run(b_))
w = tf.nn.embedding_lookup(a,b_)
print(sess.run(w))

perm = tf.argsort(b)
aux = tf.gather_nd(b,tf.reshape(perm,[-1,1]))
mask = tf.not_equal(aux[1:],aux[:-1])
pos = tf.boolean_mask(perm[1:],mask)
pos = tf.concat([[0],pos],axis=0)

print(sess.run(pos))

w_2 = tf.nn.embedding_lookup(a,b)
w_1 = tf.nn.embedding_lookup(w_2,pos)

print(sess.run(w_1))

`

![E5@B5_ XNA U83Q1BJ1 0XO](https://user-images.githubusercontent.com/22000530/87393434-c49a1580-c5e0-11ea-8cfa-9f755ad9818d.png)

**Describe the expected behavior**

Their performance is the same,but when I use them in my models(all is the same except one is once embedding_lookup,one is twice embedding_lookup),they don't perform the same.

use once embedding_lookup:
![MMZZQFLBEKQE8MJHKK8Y5FO](https://user-images.githubusercontent.com/22000530/87393621-0cb93800-c5e1-11ea-8da5-27f363fb7c0c.png)

use twice embedding_lookup:
![X`K9D(FEM_HBM%17UO~5MRS](https://user-images.githubusercontent.com/22000530/87393625-0dea6500-c5e1-11ea-9f5f-28d36a9111fe.png)
We could see before training they perform the same,but after training, they perform different,I assume because their gradients are different,but I don't know how to fix them,by the way I have to use twice embedding_lookup



**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
41367,"""ImportError: DLL load failed: The specified module could not be found."" Windows10 issue with Tensorflow-2.2.0","I have installed Tensorflow 2.2.0 on my Win10 PC:

**System information**
- Windows 10 
- Tensorflow installed in Anaconda using pip
- Python Version : 3.7.6
- Graphics Card: NVIDIA GTX1650 4GB
- Processor: Intel core i5 9th-Gen

I have been facing this issue since a long time. So i uninstalled Anaconda and re-installed it again but the issue still persists\

`ImportError                               Traceback (most recent call last)
C:\ProgramData\Anaconda3\envs\py3-TF2.0\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59 

C:\ProgramData\Anaconda3\envs\py3-TF2.0\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in <module>
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

C:\ProgramData\Anaconda3\envs\py3-TF2.0\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

C:\ProgramData\Anaconda3\envs\py3-TF2.0\lib\imp.py in load_module(name, file, filename, details)
    241         else:
--> 242             return load_dynamic(name, filename, file)
    243     elif type_ == PKG_DIRECTORY:

C:\ProgramData\Anaconda3\envs\py3-TF2.0\lib\imp.py in load_dynamic(name, path, file)
    341             name=name, loader=loader, origin=path)
--> 342         return _load(spec)
    343 

ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-1-4282574fb12a> in <module>
      1 import numpy as np
----> 2 import tensorflow as tf

C:\ProgramData\Anaconda3\envs\py3-TF2.0\lib\site-packages\tensorflow\__init__.py in <module>
     39 import sys as _sys
     40 
---> 41 from tensorflow.python.tools import module_util as _module_util
     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader
     43 

C:\ProgramData\Anaconda3\envs\py3-TF2.0\lib\site-packages\tensorflow\python\__init__.py in <module>
     48 import numpy as np
     49 
---> 50 from tensorflow.python import pywrap_tensorflow
     51 
     52 # Protocol buffers

C:\ProgramData\Anaconda3\envs\py3-TF2.0\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     67 for some common reasons and solutions.  Include the entire stack trace
     68 above this error message when asking for help."""""" % traceback.format_exc()
---> 69   raise ImportError(msg)
     70 
     71 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\envs\py3-TF2.0\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\ProgramData\Anaconda3\envs\py3-TF2.0\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\ProgramData\Anaconda3\envs\py3-TF2.0\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\ProgramData\Anaconda3\envs\py3-TF2.0\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\ProgramData\Anaconda3\envs\py3-TF2.0\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.`


Kindly help to resolve this.
"
41366,pip install tf-nightly (macOS) installs build dated 20200610,"**System information**
- OS Platform: macOS Catalina 13.1.1 (15609.2.9.1.2)
- TensorFlow installed from (source or binary): pip
- TensorFlow version: tf-nightly
- Python version: 3.8.3 (default, Jul  8 2020, 14:27:55) 
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the problem**

Installing `tf-nightly` installs a old build dated from mid-June.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
~: pip3 install tf-nightly
Collecting tf-nightly
  Downloading tf_nightly-2.3.0.dev20200610-cp38-cp38-macosx_10_14_x86_64.whl (165.4 MB)
```
"
41364,how to build tflite2.3 on windows,how to build tflite2.3 on windows and mac
41363,Why TF-OP runs slower than pure API?,"![1188470626](https://user-images.githubusercontent.com/3830256/87379019-d61ff500-c5c1-11ea-8bb0-85a03c98b7aa.jpg)

====================

![483313243](https://user-images.githubusercontent.com/3830256/87379023-d7512200-c5c1-11ea-8830-f6822bc13f72.jpg)

The op is made up of several GPU kernels, but I found it runs slower in TF than in pure api, the above two pictures represents the nv prof result for pure api and TF-OP seperately. Obviously, kernels in the pure api runs one by one, but there are more space between kernels in TF-OP.

Why could this happened?"
41362,c_api.h no such file or directory,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.4.0
- Python version: 3.8.3
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): 3.4.0
- GCC/Compiler version (if compiling from source):  7.5.0
- CUDA/cuDNN version: 11.0 / 8.0.1
- GPU model and memory: GTX1080Ti GDDR5X 11GB

**Describe the problem**
build failed
**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel build //tensorflow/tools/pip_package:build_pip_package

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```
/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/external/llvm-project/llvm/BUILD:712:11: C++ compilation of rule '@llvm-project//llvm:Analysis' failed (Exit 1)
In file included from external/llvm-project/llvm/lib/Analysis/TFUtils.cpp:14:0:
external/llvm-project/llvm/include/llvm/Analysis/Utils/TFUtils.h:12:10: fatal error: tensorflow/c/c_api.h: No such file or directory
 #include ""tensorflow/c/c_api.h""
```

"
41361,"Keras model.compile(..., metrics=[""accuracy""]) no longer introspects loss function in TF 2.2","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS, Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): pip/binary
- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b, 2.2.0
- Python version: 3.7.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A


**Describe the current behavior**

The documentation of [`tf.keras.Model.compile`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile) includes the following for the `metrics` parameter:

> When you pass the strings 'accuracy' or 'acc', we convert this to one of tf.keras.metrics.BinaryAccuracy, tf.keras.metrics.CategoricalAccuracy, tf.keras.metrics.SparseCategoricalAccuracy based on the loss function used and the model output shape. We do a similar conversion for the strings 'crossentropy' and 'ce' as well. 

The code in question only looks at the model shape, and ignores the loss function:

https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/engine/compile_utils.py#L447-L453

This means that using `acc`/`accuracy` for a model doing binary classification with last dimension > 1 (e.g. binary predictions about multiple independent values for each batch element) will incorrectly use categorical accuracy, not binary accuracy. (That is, the `compile` invocation is equivalent to passing `metrics=[""categorical_accuracy""]`.)

**Describe the expected behavior**

The behaviour stated by the documentation is to look at the loss function in addition to the output shape. That is, if the loss function is binary cross-entropy, `metrics=[""accuracy""]` should be equivalent to `metrics=[""binary_accuracy""]`. 

This is behaviour of TF < 2.2, such as TF 2.1.1:

https://github.com/tensorflow/tensorflow/blob/3ffdb91f122f556a74a6e1efd2469bfe1063cb5c/tensorflow/python/keras/engine/training_utils.py#L1114-L1121

**Standalone code to reproduce the issue**

```python
#%pip install tensorflow==2.2.0
#%pip install tensorflow==2.1.1
import tensorflow as tf

inp = tf.keras.Input(3)
model = tf.keras.Model(inp, inp)

model.compile(loss=""bce"", metrics=[""acc""])
model.evaluate(tf.constant([[0.1, 0.6, 0.9]]), tf.constant([[0, 1, 1]]))

if tf.version.VERSION == ""2.2.0"":
    print(model.compiled_metrics.metrics[0]._fn.__name__)
else:
    print(model._per_output_metrics[0][""acc""]._fn.__name__)
```

Output

- TF 2.2.0: `categorical_accuracy`
- TF 2.1.1: `binary_accuracy`

Notebook: https://gist.github.com/huonw/4a95b73e3d8a1c48a8b5fc5297d30772
Colab: https://colab.research.google.com/gist/huonw/4a95b73e3d8a1c48a8b5fc5297d30772


**Other info / logs** N/A"
41360,Using HIP instead of CUDA (work on all graphics card),"Since half of the developers can use an AMD card or an old NVIDIA card, if that append, you cannot use the GPU with CUDA.

**System information**
- TensorFlow version: ALL
- Are you willing to contribute it (Yes/No): YES

**Describe the feature and the current behavior/state.**

HIP is a C++ Runtime API and Kernel Language that allows developers to create portable applications for AMD and NVIDIA GPUs from single source code.

Key features include:

HIP is very thin and has little or no performance impact over coding directly in CUDA or hcc ""HC"" mode.
HIP allows coding in a single-source C++ programming language including features such as templates, C++11 lambdas, classes, namespaces, and more.
HIP allows developers to use the ""best"" development environment and tools on each target platform.

The HIPIFY tools automatically convert source from CUDA to HIP.
Developers can specialize for the platform (CUDA or hcc) to tune for performance or handle tricky cases
New projects can be developed directly in the portable HIP C++ language and can run on either NVIDIA or AMD platforms. 

Additionally, HIP provides porting tools which make it easy to port existing CUDA codes to the HIP layer, with no loss of performance as compared to the original CUDA application. HIP is not intended to be a drop-in replacement for CUDA, and developers should expect to do some manual coding and performance tuning work to complete the port.

[https://github.com/ROCm-Developer-Tools/HIP]

**Will this change the current api? How?** NO

**Who will benefit with this feature?** ALL

"
41354,Can't register an optimizer by name: TensorRTOptimizer,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.15.3+gpu+tensorrt
- Python version:3.6
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): devtoolset7
- CUDA/cuDNN version: 10.0
- GPU model and memory: Tesla V100



**Describe the problem**
We built tensorflow with --config=cuda and --config=tensorrt enabled, but when using TRT converter, it said 
```
2020-07-13 18:15:56.098448: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:247] Registered default graph optimizer: constfold
2020-07-13 18:15:56.098455: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:297] Can't register an optimizer by name: TensorRTOptimizer
2020-07-13 18:15:56.098461: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:292] Registered default graph optimizer: constfold
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```python
import os

os.environ['TF_CPP_MIN_VLOG_LEVEL'] = '2'

import time
import logging
import numpy as np

import tensorflow as tf
print(""TensorFlow version: "", tf.__version__)

from tensorflow.python.compiler.tensorrt import trt_convert as trt

logging.getLogger(""tensorflow"").setLevel(logging.INFO)

SAVED_MODEL_DIR = ""hdfs:///smf1/dw2/user/yinl/resnet_v1_fp32_savedmodel_NHWC""
FP32_SAVED_MODEL_DIR = ""hdfs:///smf1/dw2/user/yinl/resnet_v1_fp32_savedmodel_NHWC_TRT""

converter = trt.TrtGraphConverter(
    input_saved_model_dir=SAVED_MODEL_DIR,
	  precision_mode=trt.TrtPrecisionMode.FP32)
converter.convert()

converter.save(FP32_SAVED_MODEL_DIR)
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Build script
```
BAZEL_LINKLIBS=-l%:libstdc++.a bazel build -c opt \
      ${CC_OPT_FLAGS} \
      ${TF_CONFIG_ARGS} \
      --config=cuda --config=tensorrt \
      --linkopt -ldl \
      //tensorflow/tools/pip_package:build_pip_package \
      //tensorflow/tools/lib_package:libtensorflow_jni \
      //tensorflow/tools/lib_package:libtensorflow
```
"
41353,tf.nn.ctc_loss not returning expected value,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below):  v2.2.0-rc4-8-g2b96f3662b 2.2.0
- Python version: 3.7.7
- Bazel version (if compiling from source): No
- GCC/Compiler version (if compiling from source): No

**Describe the current behavior**

I run the following code:

```
import tensorflow as tf

label = [1, 2, 1]
logits = [[0.0, 1.0, 0.0],
          [0.0, 0.0, 1.0],
          [0.0, 1.0, 0.0]]
labels_length = 3
logits_length = 3

labels_tensor = tf.convert_to_tensor([label], dtype=tf.int32)
logits_tensor = tf.convert_to_tensor([logits], dtype=tf.float32)
labels_length_tensor = tf.convert_to_tensor([labels_length], dtype=tf.int32)
logits_length_tensor = tf.convert_to_tensor([logits_length], dtype=tf.int32)

loss = tf.nn.ctc_loss(labels_tensor, logits_tensor, labels_length_tensor, logits_length_tensor, logits_time_major=False)
print(loss.numpy()[0])
```

Basically there're 3 timestamps, and there're two characters `a, b` and the GT is `aba` (`[1, 2, 1]`) which only has one representation in CTC. And I'm making the logits have the probability of 1 for `aba` only. Therefore I expect the loss is 0, however the loss is some random value 1.6543342.

Either there's a bug, or I seriously misunderstand the documentation - in which case I think the [documentation on this topic](https://www.tensorflow.org/api_docs/python/tf/nn/ctc_loss) isn't very good. It would be better to have some example, as suggested in another issue https://github.com/tensorflow/tensorflow/issues/35063.

**Describe the expected behavior**

Expect the loss to be 0.

"
41352,TensorFlow Lite runtime check doesn't account for bytecode,"This line in TensorFlow Lite doesn't consider the possibility that the name of the file could end with `pyc` instead of `py` if the module is compiled to bytecode (e.g., when used by PyInstaller). I can file a PR but wanted to check first if there's a reason why this is the way that it is. Right now, this causes the TensorFlow Lite runtime to try to import full TensorFlow when it's compiled to bytecode, which of course fails.

https://github.com/tensorflow/tensorflow/blob/df56513aa9b004313c2cff818e138d78cd667b67/tensorflow/lite/python/interpreter.py#L28

I would plan to change this to:

```
import os

if not os.path.splitext(__file__)[0].endswith('tflite_runtime/interpreter')
```

Feedback welcome!"
41351,tf.image.random_saturation cpu implementation crashes on some images,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):   Ubuntu 18.04.3 LTS
- TensorFlow installed from (source or binary): Docker Image 'tensorflow/tensorflow:2.2.0-gpu'
- TensorFlow version (use command below): 2.2.0
- Python version: 3.6.9
- CUDA/cuDNN version: CUDA Version 10.1.243
- GPU model and memory: NVIDIA Titan RTX with 24gb of memory

**Describe the current behavior**

When using tf.image.random_saturation on the cpu (as part of a custom image augmentation pipeline), certain images will cause tf.image.random_saturation to crash. There is no error, the session simply freezes and cannot be exited. Ctr C has no effect and the terminal session needs to be quit. When running the function in GPU this error does not happen. 

The images being fed into this function are imagenet images that have already been augmented with a Gaussian blur and it only seems to happen on less than 1% of the data. The images that cause the crash do not appear to be out of the ordinary, with all the values being float32 values between 0 and 1. The only notable pattern being that they all have at least one pixel equal to exactly 0.0. This alone though is not enough to recreate the failure.

**Describe the expected behavior**
The expected behavior is that this function behaves the same as on the gpu and does not crash from an rgb image where all the values are floats between 0 and 1.

**Standalone code to reproduce the issue**
Download and extract the file provided to get bad_image.npy to use with the code snippet below.
[bad_image.npy.zip](https://github.com/tensorflow/tensorflow/files/4915306/bad_image.npy.zip)


```
import tensorflow as tf
import numpy as np
inp = np.load(""bad_image.npy"")
with tf.device('/cpu:0'):
    tf_inp = tf.convert_to_tensor(inp)
    test = tf.image.random_saturation(tf_inp, .2, 1.8)
print(test)
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Current workaround: wrap the input to tf.image.random_saturation with tf.clip_by_value(x, 1e-6, 1)
"
41350,Switching from Theano 1.0.0 to TensorFlow 2.2 (both using Keras) causes exponentially growing loss and stagnant accuracy,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.2
- Python version: 3.6.10
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10.1, cuDNN 7.6.5
- GPU model and memory: GeForce GTX Titan X, 12 GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

After migrating some Keras code from Theano backend (Keras 2.0.7, Theano 1.0.0) to pure TensorFlow, the model is unable to train with the same success as it did before. I did not change anything about the model architecture, hyper parameters, or parameters in the fit function. All that changed were import statements (ex: keras.layers -> tensorflow.keras.layers)

With TensorFlow:

```
2020-07-10 23:12:37 WARNING  Using a generator with use_multiprocessing=True and multiple workers may duplicate your data. Please consider using the tf.data.Dataset.
Epoch 1/8
2020-07-10 23:12:38.795744: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-07-10 23:12:38 WARNING  multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.
2020-07-10 23:12:39.211689: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
100/100 [==============================] - 10s 103ms/step - loss: 5.1734 - accuracy: 0.1002
Epoch 2/8
100/100 [==============================] - 10s 102ms/step - loss: 6.1694 - accuracy: 0.1079
Epoch 3/8
100/100 [==============================] - 10s 102ms/step - loss: 13.4283 - accuracy: 0.0830
Epoch 4/8
100/100 [==============================] - 10s 101ms/step - loss: 22.0164 - accuracy: 0.0838
Epoch 5/8
100/100 [==============================] - 10s 102ms/step - loss: 44.7066 - accuracy: 0.0575
Epoch 6/8
100/100 [==============================] - 10s 102ms/step - loss: 38.3269 - accuracy: 0.0690
Epoch 7/8
100/100 [==============================] - 10s 102ms/step - loss: 53.5934 - accuracy: 0.0641
Epoch 8/8
100/100 [==============================] - 10s 101ms/step - loss: 66.5487 - accuracy: 0.0673
```

**Describe the expected behavior**

Expected training behavior should be similar to that of Keras with Theano backend:

```
/usr/local/lib/python3.6/site-packages/keras/engine/training.py:1984: UserWarning: Using a generator with use_multiprocessing=True and multiple workers may duplicate your data. Please consider using the keras.utils.Sequence class.
  UserWarning('Using a generator with use_multiprocessing=True')
Epoch 1/8
100/100 [==============================] - 14s - loss: 4.8412 - acc: 0.1197          
Epoch 2/8
100/100 [==============================] - 13s - loss: 3.7391 - acc: 0.2454     
Epoch 3/8
100/100 [==============================] - 13s - loss: 3.2523 - acc: 0.3528     
Epoch 4/8
100/100 [==============================] - 13s - loss: 2.9821 - acc: 0.3923     
Epoch 5/8
100/100 [==============================] - 13s - loss: 2.8871 - acc: 0.4059     
Epoch 6/8
100/100 [==============================] - 13s - loss: 2.8087 - acc: 0.4177     
Epoch 7/8
100/100 [==============================] - 13s - loss: 2.7203 - acc: 0.4290     
Epoch 8/8
100/100 [==============================] - 13s - loss: 2.6507 - acc: 0.4351
```

**Standalone code to reproduce the issue**

Model architecture:

```
import tensorflow.keras.backend as K
from tensorflow.keras import Model, Sequential, Input
from tensorflow.keras.layers import Dense, Lambda, Embedding, LSTM
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import CategoricalCrossentropy
from tensorflow.keras.metrics import Accuracy

epoch = 1
batch_size = 2048
validation_batch_size = 1024
one_hot_dim = 80000
embedding_dim = 128
max_seq_len = 50
hidden_layer_dim = 512
max_q_size = 5000

nb_worker = 5
one_indexing = False
featurizer = ""sequence""
featurizer_parameters = [one_hot_dim, max_seq_len, one_indexing]
def create_model_architecture(num_classes):
    # build model architecture
    model = Sequential(
         [
             Input(shape=(max_seq_len,), name=""query_input""),
             Embedding(output_dim=embedding_dim, input_dim=one_hot_dim, input_length=max_seq_len),
             LSTM(embedding_dim, input_shape=(max_seq_len, embedding_dim)),
             Lambda(lambda x: K.cast(x, ""float32"")),
             Dense(units=hidden_layer_dim, kernel_initializer=""he_normal"", activation=""relu""),
             Dense(units=num_classes, kernel_initializer=""normal"", activation=""softmax"", name=""prediction"")
         ]
    )
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']
    model.summary()
    return model
```

In model.compile, I have tried also using the classes for optimizer, loss, and metrics (such as Adam, CategoricalCrossentropy, Accuracy). It causes the accuracy to be much higher but the loss still increases exponentially.

Model fit call:

```
model.fit(data_generator_train.generate(),
            steps_per_epoch=args.step_log,
            epochs=8,
            validation_data=data_generator_validate.generate() if data_generator_validate else None,
            validation_steps=data_generator_validate.num_batches if data_generator_validate else None,
            max_queue_size=model_module.max_q_size,
            shuffle=True,
            use_multiprocessing=True,
            workers=model_module.nb_worker,
            callbacks=[check_pointer])
```

Data generator function:

```
@threadsafe_generator
def generate(self):
   while True:
      for filename in glob.glob(self.dataset_files):
         with open(filename) as fr:
            while True:
               lines = list(islice(fr, self.batch_size))
               if not lines:
                  break
               x, y, w = self.get_train_vectors(lines)
               yield x, y, w
```

**Other info / logs**

Other things I have tried:

Using tensorflow.keras.utils.Sequence instead of a generator, and including shuffle=True
Not using multiprocessing
Not using Sequential
Using classes instead of names in model.compile()
Changing parameters like batch size, steps_per_epoch, etc.

Other notes:

I am using a custom Docker image to run the model. I have made volume mountings from the local machine, including CUDA, cuDNN, etc. under /usr/local/cuda. TensorFlow 2.2 also has a library (libcublas.so) which is not located there, so I have an additional mapping under /usr/lib/x86_64-linux-gnu. 
"
41347,WARNING:tensorflow:11 out of the last 11 via using Keras library,"Hello all, I'm getting this warning, I wonder why?

>WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x0000023D1D542C80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.

My whole code is [here.](https://github.com/BestSithInEU/Warning)"
41343,Tensorflow 2.2: No gradients provided for any variable,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): pip install
- TensorFlow version (use command below): 2.2
- Python version: 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1/ 7.6.5
- GPU model and memory:  GPU Nvidia 1080Ti / 12GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
I build a custom Unet with custom Densenet encoder. After I compile model and use fit_generator, I got No gradients provided for any variable

**Describe the expected behavior**
Model run the training

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
Link colab https://drive.google.com/file/d/1fyb88Ako0CjoVP9O5NQdy6rfjoBhXaFS/view?usp=sharing

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
41340,Unable to load custom model when using tf.keras.callbacks.ModelCheckpoint,"**System information**

== check python ===================================================
python version: 3.6.9
python branch: 
python build version: ('default', 'Apr 18 2020 01:56:04')
python compiler version: GCC 8.4.0
python implementation: CPython


== check os platform ===============================================
os: Linux
os kernel version: 1 SMP Wed Feb 19 05:26:34 PST 2020
os release version: 4.19.104+
os platform: Linux-4.19.104+-x86_64-with-Ubuntu-18.04-bionic
linux distribution: ('Ubuntu', '18.04', 'bionic')
linux os distribution: ('Ubuntu', '18.04', 'bionic')
mac version: ('', ('', '', ''), '')
uname: uname_result(system='Linux', node='6fd50df0cf80', release='4.19.104+', version='#1 SMP Wed Feb 19 05:26:34 PST 2020', machine='x86_64', processor='x86_64')
architecture: ('64bit', '')
machine: x86_64


== are we in docker =============================================
Yes

== compiler =====================================================
c++ (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Copyright (C) 2017 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== check pips ===================================================
numpy                    1.18.5         
protobuf                 3.10.0         
tensorflow               2.2.0          
tensorflow-addons        0.8.3          
tensorflow-datasets      2.1.0          
tensorflow-estimator     2.2.0          
tensorflow-gcs-config    2.2.0          
tensorflow-hub           0.8.0          
tensorflow-metadata      0.22.2         
tensorflow-privacy       0.2.2          
tensorflow-probability   0.10.0         

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.version.VERSION = 2.2.0
tf.version.GIT_VERSION = v2.2.0-0-g2b96f3662b
tf.version.COMPILER_VERSION = 7.4.0

== env ==========================================================
LD_LIBRARY_PATH /usr/lib64-nvidia
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Mon Jul 13 14:50:08 2020       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |
| N/A   44C    P0    59W / 149W |    157MiB / 11441MiB |      0%      Default |
|                               |                      |                 ERR! |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/lib/python2.7/dist-packages/torch/lib/libcudart-1b201d85.so.10.1
/usr/local/lib/python3.6/dist-packages/torch/lib/libcudart-1b201d85.so.10.1
/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart.so.10.0.130
/usr/local/cuda-10.0/doc/man/man7/libcudart.7
/usr/local/cuda-10.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudart.so.10.1.243
/usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-10.1/doc/man/man7/libcudart.7
/usr/local/cuda-10.1/doc/man/man7/libcudart.so.7

== tensorflow installed from info ==================
Name: tensorflow
Version: 2.2.0
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: /usr/local/lib/python3.6/dist-packages
Required-by: fancyimpute

== python version  ==============================================
(major, minor, micro, releaselevel, serial)
(3, 6, 9, 'final', 0)

- All other system information is contained in a standalone colab notebook as cited section below

**Describe the current behavior**

When training a sub-classed keras model with tf.keras.callbacks.ModelCheckpoint, It is not possible load the saved model. 

> OSError                                   Traceback (most recent call last)
> <ipython-input-7-bd53ee47e8b4> in <module>()
> ----> 1 tf.keras.models.load_model('checkpoints/classifier')>
> 
> 1 frames
> /usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/loader_impl.py in parse_saved_model(export_dir)
>     111                   (export_dir,
>     112                    constants.SAVED_MODEL_FILENAME_PBTXT,
> --> 113                    constants.SAVED_MODEL_FILENAME_PB))
>     114 
>     115 
> OSError: SavedModel file does not exist at: checkpoints/classifier/{saved_model.pbtxt|saved_model.pb}

 
**Describe the expected behavior**
It should be able to use the saved model for inference/deployment or other subsequent tasks

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/1mJnwoTCJOcZKiPxfY5nDfFk14S8r5IOi

**Other info / logs** Include any logs or source code that would be helpful to
 I think there are other issues related like https://github.com/tensorflow/tensorflow/issues/31057
"
41339,ImportError : cannot import name 'context from 'tensorflow.python.eager',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): binary
- TensorFlow version: tensorflow 2.0.0
- Python version: python 3.7.7
- Installed using virtualenv? pip? conda?: pip




**Describe the problem**

cannot import keras. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**

import keras

Using TensorFlow backend.
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-3-88d96843a926> in <module>
----> 1 import keras

C:\Python\anaconda3\lib\site-packages\keras\__init__.py in <module>
      1 from __future__ import absolute_import
      2
----> 3 from . import utils
      4 from . import activations
      5 from . import applications

C:\Python\anaconda3\lib\site-packages\keras\utils\__init__.py in <module>
      4 from . import data_utils
      5 from . import io_utils
----> 6 from . import conv_utils
      7 from . import losses_utils
      8 from . import metrics_utils

C:\Python\anaconda3\lib\site-packages\keras\utils\conv_utils.py in <module>
      7 from six.moves import range
      8 import numpy as np
----> 9 from .. import backend as K
     10
     11

C:\Python\anaconda3\lib\site-packages\keras\backend\__init__.py in <module>
----> 1 from .load_backend import epsilon
      2 from .load_backend import set_epsilon
      3 from .load_backend import floatx
      4 from .load_backend import set_floatx
      5 from .load_backend import cast_to_floatx

C:\Python\anaconda3\lib\site-packages\keras\backend\load_backend.py in <module>
     88 elif _BACKEND == 'tensorflow':
     89     sys.stderr.write('Using TensorFlow backend.\n')
---> 90     from .tensorflow_backend import *
     91 else:
     92     # Try and load external backend.

C:\Python\anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py in <module>
      4
      5 import tensorflow as tf
----> 6 from tensorflow.python.eager import context
      7 from tensorflow.python.framework import device as tfdev
      8 from tensorflow.python.framework import ops as tf_ops

ImportError: cannot import name 'context' from 'tensorflow.python.eager' (unknown location)

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
41338,Keras 2.4.3 does not calculate output shape in reshape layer,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux
- TensorFlow installed from (source or binary): conda
- TensorFlow version (use command below): 2.2.0
- Python version: tested in both 3.7.6 and 3.8.3
- I tested both with a non-gpu version and with cudnn 7.6.5 + cuda10.1_0

**Describe the current behavior**

Output length of a reshape layer with last element of length unknown is marked as None

**Describe the expected behavior**

In previous versions of Keras, it was calculated from the other input shapes

**Standalone code to reproduce the issue**

The following code:
```
import keras
layer = keras.layers.Reshape([5, -1])
k_input = keras.Input((5, 6, 4))
model = keras.Model(inputs = [k_input], outputs = layer(k_input))
model.summary()
```
has different outputs depending on the version. In Keras 2.3.1 the last dimension of output is correctly deduced to be 24:

```
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 5, 6, 4)           0         
_________________________________________________________________
reshape_1 (Reshape)          (None, 5, 24)             0         
=================================================================
```

In Keras 2.4.3 the last dimension of output is None
```
 _________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 5, 6, 4)]         0         
_________________________________________________________________
reshape (Reshape)            (None, 5, None)           0         
=================================================================
```
"
41335,    ValueError: Shape must be rank 0 but is rank 1 for '{{node ReadFile}} = ReadFile[](args_0)' with input shapes: [?].,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 2.2.0
- Python version: 3.7.3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.


Getting the following error 

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-159-9b4a28882f72> in <module>
      1 dataset = tf.data.Dataset.from_tensor_slices((images))
----> 2 train(dataset.batch(2).map(preprocessing))

~/venv/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py in map(self, map_func, num_parallel_calls, deterministic)
   1619     """"""
   1620     if num_parallel_calls is None:
-> 1621       return MapDataset(self, map_func, preserve_cardinality=True)
   1622     else:
   1623       return ParallelMapDataset(

~/venv/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py in __init__(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)
   3979         self._transformation_name(),
   3980         dataset=input_dataset,
-> 3981         use_legacy_function=use_legacy_function)
   3982     variant_tensor = gen_dataset_ops.map_dataset(
   3983         input_dataset._variant_tensor,  # pylint: disable=protected-access

~/venv/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py in __init__(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)
   3219       with tracking.resource_tracker_scope(resource_tracker):
   3220         # TODO(b/141462134): Switch to using garbage collection.
-> 3221         self._function = wrapper_fn.get_concrete_function()
   3222 
   3223         if add_to_graph:

~/venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py in get_concrete_function(self, *args, **kwargs)
   2530     """"""
   2531     graph_function = self._get_concrete_function_garbage_collected(
-> 2532         *args, **kwargs)
   2533     graph_function._garbage_collector.release()  # pylint: disable=protected-access
   2534     return graph_function

~/venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_garbage_collected(self, *args, **kwargs)
   2494       args, kwargs = None, None
   2495     with self._lock:
-> 2496       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
   2497       if self.input_signature:
   2498         args = self.input_signature

~/venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   2775 
   2776       self._function_cache.missed.add(call_context_key)
-> 2777       graph_function = self._create_graph_function(args, kwargs)
   2778       self._function_cache.primary[cache_key] = graph_function
   2779       return graph_function, args, kwargs

~/venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   2665             arg_names=arg_names,
   2666             override_flat_arg_shapes=override_flat_arg_shapes,
-> 2667             capture_by_value=self._capture_by_value),
   2668         self._function_attributes,
   2669         # Tell the ConcreteFunction to clean up its graph once it goes out of

~/venv/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    979         _, original_func = tf_decorator.unwrap(python_func)
    980 
--> 981       func_outputs = python_func(*func_args, **func_kwargs)
    982 
    983       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

~/venv/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py in wrapper_fn(*args)
   3212           attributes=defun_kwargs)
   3213       def wrapper_fn(*args):  # pylint: disable=missing-docstring
-> 3214         ret = _wrapper_helper(*args)
   3215         ret = structure.to_tensor_list(self._output_structure, ret)
   3216         return [ops.convert_to_tensor(t) for t in ret]

~/venv/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py in _wrapper_helper(*args)
   3154         nested_args = (nested_args,)
   3155 
-> 3156       ret = autograph.tf_convert(func, ag_ctx)(*nested_args)
   3157       # If `func` returns a list of tensors, `nest.flatten()` and
   3158       # `ops.convert_to_tensor()` would conspire to attempt to stack

~/venv/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)
    263       except Exception as e:  # pylint:disable=broad-except
    264         if hasattr(e, 'ag_error_metadata'):
--> 265           raise e.ag_error_metadata.to_exception(e)
    266         else:
    267           raise

ValueError: in user code:

    <ipython-input-156-6db31559ad89>:5 preprocessing  *
        image = tf.io.read_file(image)
    /home/jake/venv/lib/python3.7/site-packages/tensorflow/python/ops/gen_io_ops.py:568 read_file  **
        ""ReadFile"", filename=filename, name=name)
    /home/jake/venv/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:744 _apply_op_helper
        attrs=attr_protos, op_def=op_def)
    /home/jake/venv/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py:595 _create_op_internal
        compute_device)
    /home/jake/venv/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:3327 _create_op_internal
        op_def=op_def)
    /home/jake/venv/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1817 __init__
        control_input_ops, op_def)
    /home/jake/venv/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1657 _create_c_op
        raise ValueError(str(e))

    ValueError: Shape must be rank 0 but is rank 1 for '{{node ReadFile}} = ReadFile[](args_0)' with input shapes: [?].
```


the code is 
```
import tensorflow as tf

def preprocessing(image):
    print('preprocessing->',image)
    image = tf.io.read_file(image)
    #image = tf.image.decode_jpeg(img, channels=3)
    #image = tf.cast(img, tf.float32)
    return image

def train(ds):
    for i, batch in enumerate(ds):
        print(""=====================batch{}====================={}"".format(i,batch))
        for x in batch:
            print(x)
            #print(x.numpy())
            #print(x[1].numpy())
images = ['/media/jake/mark-4tb3/Screenshot_from_2020-02-19_23-05-22.png','/media/jake/mark-4tb3/Screenshot_from_2020-02-19_23-05-22.png','/media/jake/mark-4tb3/Screenshot_from_2020-02-19_23-05-22.png','/media/jake/mark-4tb3/Screenshot_from_2020-02-19_23-05-22.png']

images = tf.ragged.constant(images)

dataset = tf.data.Dataset.from_tensor_slices((images))
train(dataset.batch(2).map(preprocessing))
```"
41334,Shuffling then zip tf.data.Dataset,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: None
- GPU model and memory: None

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
When applying shuffling => warped copy => zip, shuffling is applied after the warping. 

**Describe the expected behavior**
I would expect it to happen before the warping.

**Standalone code to reproduce the issue**
```
import tensorflow as tf
master = tf.data.Dataset.range(10)
master = master.shuffle(10)
warped = master.map(lambda x: -x)
dataset = tf.data.Dataset.zip((master, warped))
list(dataset.as_numpy_iterator())
```
We get
```
[(3, -1),
 (7, -9),
 (4, -2),
 (9, -5),
 (5, -4),
 (6, 0),
 (1, -8),
 (0, -3),
 (2, -6),
 (8, -7)]
```
When something like this was expected:
```
[(7, -7),
 (9, -9),
 (6, -6),
 (1, -1),
 (3, -3),
 (5, -5),
 (4, -4),
 (0, 0),
 (2, -2),
 (8, -8)]
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
41333,training,"
Everything works fine, just at the end step, training stops.
I user faster rcnn. As soon as I run train.py with all the parameters provided, it stopped training"
41332,faster rcnn training,"
Everything works fine, just at the end step, training stops.
I user faster rcnn. As soon as I run train.py with all the parameters provided, it stopped training"
41331,faster rcnn stopped training,"
Everything works fine, just at the end step, training stops.
I user faster rcnn. As soon as I run train.py with all the parameters provided, it stopped training"
41330,AttributeError: module 'keras.backend' has no attribute 'slice',"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Windows 10 64 bits
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):pypi.douban.com/simple
- TensorFlow version (use command below):2.2
- Python version:3.7.2
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
AttributeError: module 'keras.backend' has no attribute 'slice'
**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
https://github.com/cyandn/DS/tree/master/NER_Keras

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached."
41329," when using keras, report error: Use Eager execution or decorate this function with @tf.function ","I do not think currently tensorflow supports building a tf.keras.Model which contains tf.cond or tf.while_loop. 

I have built a model based on tf2.0 which contains while loop, but I want to save it as a pb file, so I need to use tf.keras.Model to rebuild my model.

I have tried the code below and it reported the error:
tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.

# the code
import tensorflow as tf
layers = tf.keras.layers
a = layers.Input(shape=tf.TensorShape([]), dtype=tf.bool)
b = a[0]
c = tf.cond(b, lambda: 1, lambda: 0)
print (c)


the tensorflow version I used: 
v2.0.0-69-g765ac8d 2.0.1"
41328,"Error: ""Data adapters should be mutually exclusive for handling inputs. Found multiple adapters to handle"" error when calling `model.fit` with imagenet2012 TFDS.","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2.0 (also tried 2.3 and tf-nightly)
- Python version: 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1 / 7.6.5
- GPU model and memory: RTX 2080 Super, 8 GB

**Describe the current behavior**
When trying to fit a VGG19 model architecture with ImageNet2012 data from TFDS (previously manually downloaded and but with ```download_and_prepare()```), it give the following error:

> RuntimeError: Data adapters should be mutually exclusive for handling inputs. Found multiple adapters [<class 'tensorflow.python.keras.engine.data_adapter.GeneratorDataAdapter'>, <class 'tensorflow.python.keras.engine.data_adapter.CompositeTensorDataAdapter'>] to handle input: <class 'tensorflow.python.data.ops.iterator_ops.OwnedIterator'>, <class 'NoneType'>


This is very similar to issue #33811. However, the error persists even with newer or nightly versions of TF.

**Describe the expected behavior**
Successfully start training.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
batch_size = 12
n_train = 1281167
ds_train,  ds_train_info = tfds.load(name='imagenet2012', download=True, with_info=True,
                                     data_dir='/Data/tfds/', split='train', as_supervised=True,
                                     download_and_prepare_kwargs={'download_dir':'/hdd/Data/tfds/imagenet2012/',})
ds_train = ds_train.map(lambda x, y: (tf.image.resize(x, [224, 224], method='bilinear'), y), num_parallel_calls=tf.data.experimental.AUTOTUNE)
ds_train = ds_train.batch(batch_size)
ds_train = ds_train.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
ds_train = iter(ds_train)

model = tf.keras.applications.VGG19(include_top=True, weights=None, input_tensor=None, input_shape=None,
                                    pooling=None, classes=1000, classifier_activation='softmax')
optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)
loss_instance = SparseCategoricalCrossentropy(from_logits=False)
model.compile(optimizer=optimizer, loss=loss_instance)

epochs = 500
steps_per_epoch = n_train//batch_size
history = model.fit(x=ds_train, epochs=epochs,
                    steps_per_epoch=steps_per_epoch)

```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
```
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
~/scratch/imagenet/train_vgg19.py in <module>
    147 # history = model.fit_generator(data_train, epochs=epochs,
    148 history = model.fit(x=ds_train, epochs=epochs,
--> 149                     steps_per_epoch=steps_per_epoch)
    150                     # callbacks=callbacks,
    151                     # validation_data=ds_val,

~/python_envs/env_p3.6.9_tf2.2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
     64   def _method_wrapper(self, *args, **kwargs):
     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
---> 66       return method(self, *args, **kwargs)
     67 
     68     # Running inside `run_distribute_coordinator` already.

~/python_envs/env_p3.6.9_tf2.2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
    813           workers=workers,
    814           use_multiprocessing=use_multiprocessing,
--> 815           model=self)
    816 
    817       # Container that configures and calls `tf.keras.Callback`s.

~/python_envs/env_p3.6.9_tf2.2/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)
   1097     self._insufficient_data = False
   1098 
-> 1099     adapter_cls = select_data_adapter(x, y)
   1100     self._adapter = adapter_cls(
   1101         x,

~/python_envs/env_p3.6.9_tf2.2/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py in select_data_adapter(x, y)
    967         ""handling inputs. Found multiple adapters {} to handle ""
    968         ""input: {}, {}"".format(
--> 969             adapter_cls, _type_name(x), _type_name(y)))
    970   return adapter_cls[0]
    971 

RuntimeError: Data adapters should be mutually exclusive for handling inputs. Found multiple adapters [<class 'tensorflow.python.keras.engine.data_adapter.GeneratorDataAdapter'>, <class 'tensorflow.python.keras.engine.data_adapter.CompositeTensorDataAdapter'>] to handle input: <class 'tensorflow.python.data.ops.iterator_ops.OwnedIterator'>, <class 'NoneType'>
```"
41327,MethodChannel#tflite(23624): Node number 8 (FlexConv2D) failed to prepare. ,"Hey I've trained a custom object detector model using pytorch and convert it into ONNX format then into tflite format.
When I deploy my model into app using the flutter tflite it shows me this error: 

> MethodChannel#tflite(23624): Node number 8 (FlexConv2D) failed to prepare.

How to handle such error?

Please any ML and Flutter expert could help me to solve it?

This is the code that I have used to done the pipeline conversion:

```
from onnx_tf.backend import prepare
import onnx

model_onnx = onnx.load('/mydrive/YOLOv5_Model/app/weights/best_fit.onnx')
tf_rep = prepare(model_onnx, device='cpu')
tf_rep.export_graph('/content/model_tf.pb')

def wrap_frozen_graph(graph_def, inputs, outputs):
    def _imports_graph_def():
        tf.compat.v1.import_graph_def(graph_def, name="""")

    wrapped_import = tf.compat.v1.wrap_function(_imports_graph_def, [])
    import_graph = wrapped_import.graph

    return wrapped_import.prune(
        tf.nest.map_structure(import_graph.as_graph_element, inputs),
        tf.nest.map_structure(import_graph.as_graph_element, outputs))

with tf.io.gfile.GFile(""/content/model_tf.pb"", ""rb"") as f:
    graph_def = tf.compat.v1.GraphDef()
    loaded = graph_def.ParseFromString(f.read())

frozen_func = wrap_frozen_graph(graph_def=graph_def,
                                inputs=[""images:0""],
                                outputs=['output_0:0','output_1:0', 'output_2:0'])

converter = tf.lite.TFLiteConverter.from_concrete_functions([frozen_func])
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tf_lite_model = converter.convert()
open('/content/best_fit.tflite', 'wb').write(tf_lite_model)
```"
41326,Differencing Time Series Data - How to Invert Differencing ,"I am working with time series data (non-stationary), I have applied .diff(periods=n) for differencing the data to eliminate trends and seasonality factors from data. 

By using .diff(periods=n), the observation from the previous time step (t-1) is subtracted from the current observation (t).

Now I want to invert back the differenced data to its original scale, but I am having issues with that.

Original DataSet before differencing:

[![original dataset][1]][1]


My code for differencing and output:

    data_diff = df.diff(periods=1)     
    
    data_diff.head(5) 

[![output differenced data][2]][2]


Code for inverting data to original scale and output:

    cols = df.columns
    x = []
    for col in cols:
        diff_results = df[col] + data_diff[col].shift(-1)
        x.append(diff_results)
    diff_df_inverted = pd.concat(x, axis=1)
    
    diff_df_inverted

[![data inverted][3]][3]

As you can see from last output, I have successfully inverted my data back to its original scale. However, I do not get the inverted data for row 1. It inverts and shifts the values up a row. My question is, why? What am I missing?

Appreciate your help, thank you!


  [1]: https://i.stack.imgur.com/qMwCZ.png
  [2]: https://i.stack.imgur.com/Z5T6B.png
  [3]: https://i.stack.imgur.com/oCXj2.png"
41325,Performance issue when calling Keras conv2D / gen_nn_ops.conv2d with different sizes,"
### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 18.04
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
NA
    happens on a mobile device**:
NA
-   **TensorFlow installed from (source or binary)**:  binary wheel via PyPI
-   **TensorFlow version (use command below)**:
v2.1.0-rc2-17-ge5bf8de 2.1.0;
v2.2.0-0-g2b96f3662b 2.2.0
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
CUDA 10.0
-   **GPU model and memory**:
V100 32 GB


### Describe the problem
I'm suspecting time performance issue when running a model with different input sizes.
the first time of running a model produce tracing, but I created a model with tf.function with tensor spec and yet when changing the input size (even by 1) the performance decreases a lot.
the model is build with keras layer conv2d.
when running the low level tensorflow API gen_nn_ops.conv2d received the same decrease of performance.

**Describe the expected behavior**
the timing performance shouldn't decrease after the 1st time of calling the model with different size.


Keras code:

````
import tensorflow as tf
import numpy as np
from time import perf_counter
from tensorflow.keras.layers import Conv2D

filters = 8
kernel_size = (3, 3)
inp_shape = (15, 15, filters)

layers = [Conv2D(filters=filters, kernel_size=kernel_size, input_shape=inp_shape)]
layers += [Conv2D(filters=filters, kernel_size=kernel_size) for _ in range(4)]
model = tf.keras.Sequential(layers=layers)

@tf.function(input_signature=[tf.TensorSpec([None, 15, 15, 8])])
def tf_func(inp):
    print('graph tracing')
    return model(inp)

def prepare_inp(n):
    return np.random.rand(n, *inp_shape).astype(""float32"")

out = tf_func(prepare_inp(1)).numpy() # first call - very slow
N = 30000
for n in range(N, N+6):
    inp = prepare_inp(n)
    elapsed = []
    for _ in range(5):
        _start = perf_counter()
        out = tf_func(inp).numpy()
        elapsed.append(perf_counter() - _start)
    print(f'[{n:,} samples] running time (msec): ' + ' '.join(f'{1000*e:6.1f}' for e in elapsed))

""""""
Output:
graph tracing
[30,000 samples] running time (msec):  466.6   77.4   63.0   63.5   77.9
[30,001 samples] running time (msec):  389.0   64.0   49.6   38.8   63.4

Expected something like:
graph tracing
[30,000 samples] running time (msec):  466.6   77.4   63.0   63.5   77.9
[30,001 samples] running time (msec):   60.0   64.0   49.6   38.8   63.4
""""""
````

low level API code:

```
import tensorflow as tf
import numpy as np
from time import perf_counter

filters = 8
kernel_size = (3, 3)
order = 'NHWC' # or 'NCHW'
img_shape = (15, 15)

inp_shape = img_shape + (filters,) if order == 'NHWC' else (filters,) + img_shape
kernel = tf.constant(np.random.rand(*kernel_size, filters, filters), dtype='float32')

from tensorflow.python.ops import gen_nn_ops

@tf.function(input_signature=[tf.TensorSpec([None, 15, 15, 8])])
def tf_func(inp):
    print('graph tracing')
    net = inp
    for _ in range(5):
        net = gen_nn_ops.conv2d(net, kernel, (1, 1, 1, 1), 'VALID', data_format=order)
    return net

def prepare_inp(n):
    return np.random.rand(n, *inp_shape).astype(""float32"")

out = tf_func(prepare_inp(1)).numpy() # first call - very slow
N = 30010
for n in range(N, N+6):
    inp = prepare_inp(n)
    elapsed = []
    for _ in range(5):
        _start = perf_counter()
        out = tf_func(inp).numpy()
        elapsed.append(perf_counter() - _start)
    print(f'[{n:,} samples] running time (msec): ' + ' '.join(f'{1000*e:6.1f}' for e in elapsed))

""""""
Output:
graph tracing
[30,010 samples] running time (msec):  466.6   77.4   63.0   63.5   77.9
[30,011 samples] running time (msec):  389.0   64.0   49.6   38.8   63.4

Expected something like:
graph tracing
[30,010 samples] running time (msec):  466.6   77.4   63.0   63.5   77.9
[30,011 samples] running time (msec):   60.0   64.0   49.6   38.8   63.4
""""""
```
"
41324,"ValueError: quantized_dimension must be in range [0, 1). Was 3.","I am trying to load a tflite model using the following python code:
````
import tensorflow as tf #version=2.0.0
tf.lite.Interpreter(model_path=""model.tflite"")
````
with the following error message:
````
ValueError: quantized_dimension must be in range [0, 1). Was 3.
````
Can anyone help me to solve this problem?"
41323,Doc error for `tf.linalg.trace`,"I think the doc-string [here](https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/ops/math_ops.py#L2758):
```python
  `output[i, j, k, ..., l] = trace(x[i, j, i, ..., l, :, :])`
```

should be:

```python
  `output[i, j, k, ..., l] = trace(x[i, j, k, ..., l, :, :])`
```


"
41322,build problems found at runtime for CUDA related issue on laptop with no GPU,"**System information**
- Linux Ubuntu 20.04LTS
- HP Elitebook 820 G3, Intel® Core™ i7-6600U CPU @ 2.60GHz × 4.  8GB RAM installed
- TF installed in virtualenv with command ""pip3 install tensorflow"" and keras with ""pip3 install keras""
- TensorFlow version: 2.2.0.  Keras version 2.4.2
- Python version: 3.8.2
- Installed using virtualenv? pip? conda?:  installed in virtualenv and used pip
- GPU model and memory:  No GPU, installed on laptop.  using Mesa Intel® HD Graphics 520 (SKL GT2)

**Describe the problem**
Build does not produce an error but fails at runtime, see attached log.  Suspect it is related to the building of tensorflow for this laptop without GPU card.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
trying to train a CNN causes this error

**Any other info / logs**
see attached log file
[error_log.txt](https://github.com/tensorflow/tensorflow/files/4908576/error_log.txt)

"
41321,using 3.2GB ram for the simplest mnist model,"I create the simplest model, but it uses so much ram. Can you help this issue?

**System information**
- Cuda 10.1
- Gtx 1050 ti
- Windows 10 platform
- Tensorflow-gpu 2.2.0
- keras 2.4.3


**Describe the current behavior**
Using ~3.2GB Video Ram

**Describe the expected behavior**
Using ~300MB Video Ram

**Code**
```
import numpy as np
import matplotlib
import matplotlib.pyplot as plt

import keras
from keras.datasets import mnist
from keras.models import Sequential, load_model
from keras.layers.core import Dense, Dropout, Activation, Flatten 
from keras.utils import np_utils
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization

# The first time you run this might be a bit slow, since the
# mnist package has to download and cache the data.
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train = np.array(x_train, dtype=np.float32)
x_test = np.array(x_test, dtype=np.float32)

x_train /= 255
x_test /= 255
y_train = np_utils.to_categorical(y_train, 10)
y_test = np_utils.to_categorical(y_test, 10)

x_train = x_train.reshape((x_train.shape[0], 28, 28, 1))
x_test = x_test.reshape((x_test.shape[0], 28, 28, 1))

print(x_train.shape) # (60000, 28, 28)
print(y_train.shape) # (60000,)

model = Sequential()
#model.add(Conv2D(32, kernel_size=(3, 3), padding='valid', activation='relu', kernel_initializer='he_normal', input_shape=(28,28, 1)))
#model.add(MaxPool2D((2, 2)))
#model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', kernel_initializer='he_normal'))
#model.add(MaxPool2D((2, 2)))
#model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', kernel_initializer='he_normal'))
#model.add(MaxPool2D((2, 2)))
#model.add(Conv2D(256, kernel_size=(1, 1), activation='relu', kernel_initializer='he_normal'))
model.add(Flatten())
model.add(Dense(30, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.25))
model.add(Dense(10, activation='softmax'))
#model.summary()
model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')
```

![image](https://user-images.githubusercontent.com/37745467/87245731-7229ef00-c450-11ea-9bcf-cd5703acfbb3.png)

//EDIT: More info
I had cuda 10.2 installed in my computer. I installed keras and tensorflow. It didn't work at the beginning because tf needs CUDA 10.1. Then I tried to uninstall cuda, reinstalling cuda 10.1. It failed couple times etc. Finally I was able to install cuda 10.1 and tensorflow. I don't know if this is related with the current memory issue. "
41320, Quantization Aware Training with tf.GradientTape gives Error in TensorFlow2.x,"Hey Guys, I am using TensorFlow-2.2, tensorflow_model_optimization and Python 3.8. I am trying to quantize and train a LeNet-300-100 Dense neural network which contains sparsity of 91.3375%. This means that 91.3375% of the weights are zero. I was following the [Quantization TF tutorial][1] and I wanted to train such a sparse network which has been quantized using *tf.GradientTape* rather than *q_aware_model.fit()*.

If you look into the [example code][2], the relevant code snippets are:


    quantize_model = tfmot.quantization.keras.quantize_model
    
    # q_aware stands for for quantization aware.
    q_aware_model = quantize_model(model)
    
    
    # 'quantize_model' requires recompilation-
    q_aware_model.compile(
        optimizer = tf.keras.optimizers.Adam(lr = 0.0012),
        loss=tf.keras.losses.categorical_crossentropy,
        metrics=['accuracy']
    )
    
    
    # Define 'train_one_step()' and 'test_step()' functions here-
    @tf.function
    def train_one_step(model, mask_model, optimizer, x, y):
        '''
        Function to compute one step of gradient descent optimization
        '''
        with tf.GradientTape() as tape:
            # Make predictions using defined model-
            y_pred = model(x)
    
            # Compute loss-
            loss = loss_fn(y, y_pred)
            
        # Compute gradients wrt defined loss and weights and biases-
        grads = tape.gradient(loss, model.trainable_variables)
        
        # type(grads)
        # list
        
        # List to hold element-wise multiplication between-
        # computed gradient and masks-
        grad_mask_mul = []
        
        # Perform element-wise multiplication between computed gradients and masks-
        for grad_layer, mask in zip(grads, mask_model.trainable_weights):
            grad_mask_mul.append(tf.math.multiply(grad_layer, mask))
        
        # Apply computed gradients to model's weights and biases-
        optimizer.apply_gradients(zip(grad_mask_mul, model.trainable_variables))
    
        # Compute accuracy-
        train_loss(loss)
        train_accuracy(y, y_pred)
    
        return None
        
        
    @tf.function
    def test_step(model, optimizer, data, labels):
        """"""
        Function to test model performance
        on testing dataset
        """"""
        
        predictions = model(data)
        t_loss = loss_fn(labels, predictions)
    
        test_loss(t_loss)
        test_accuracy(labels, predictions)
    
        return None
    
    
    
    # Train model using 'GradientTape'-
        
    # Initialize parameters for Early Stopping manual implementation-
    # best_val_loss = 100
    # loc_patience = 0
        
    for epoch in range(num_epochs):
        
        if loc_patience >= patience:
            print(""\n'EarlyStopping' called!\n"")
            break
            
        # Reset the metrics at the start of the next epoch
        train_loss.reset_states()
        train_accuracy.reset_states()
        test_loss.reset_states()
        test_accuracy.reset_states()
                
        
        for x, y in train_dataset:
            train_one_step(q_aware_model, mask_model, optimizer, x, y)
    
    
        for x_t, y_t in test_dataset:
            test_step(q_aware_model, optimizer, x_t, y_t)
    
        template = 'Epoch {0}, Loss: {1:.4f}, Accuracy: {2:.4f}, Test Loss: {3:.4f}, Test Accuracy: {4:4f}'
        
        '''
        # 'i' is the index for number of pruning rounds-
        history_main[i]['accuracy'][epoch] = train_accuracy.result() * 100
        history_main[i]['loss'][epoch] = train_loss.result()
        history_main[i]['val_loss'][epoch] = test_loss.result()
        history_main[i]['val_accuracy'][epoch] = test_accuracy.result() * 100
        ''' 
    
        print(template.format(
            epoch + 1, train_loss.result(),
            train_accuracy.result()*100, test_loss.result(),
            test_accuracy.result()*100)
             )
        
        # Count number of non-zero parameters in each layer and in total-
        # print(""layer-wise manner model, number of nonzero parameters in each layer are: \n"")
        model_sum_params = 0
        
        for layer in winning_ticket_model.trainable_weights:
            # print(tf.math.count_nonzero(layer, axis = None).numpy())
            model_sum_params += tf.math.count_nonzero(layer, axis = None).numpy()
        
        print(""Total number of trainable parameters = {0}\n"".format(model_sum_params))
    
        
        # Code for manual Early Stopping:
        if np.abs(test_loss.result() < best_val_loss) >= minimum_delta:
            # update 'best_val_loss' variable to lowest loss encountered so far-
            best_val_loss = test_loss.result()
            
            # reset 'loc_patience' variable-
            loc_patience = 0
            
        else:  # there is no improvement in monitored metric 'val_loss'
            loc_patience += 1  # number of epochs without any improvement


Gives the following error:


> --------------------------------------------------------------------------- InvalidArgumentError                      Traceback (most recent call
> last) <ipython-input-47-bca851ce138d> in <module>
>      19 
>      20     for x, y in train_dataset:
> ---> 21         train_one_step(q_aware_model, mask_model, optimizer, x, y)
>      22 
>      23 
> 
> ~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py
> in __call__(self, *args, **kwds)
>     578         xla_context.Exit()
>     579     else:
> --> 580       result = self._call(*args, **kwds)
>     581 
>     582     if tracing_count == self._get_tracing_count():
> 
> ~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py
> in _call(self, *args, **kwds)
>     642         # Lifting succeeded, so variables are initialized and we can run the
>     643         # stateless function.
> --> 644         return self._stateless_fn(*args, **kwds)
>     645     else:
>     646       canon_args, canon_kwds = \
> 
> ~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py
> in __call__(self, *args, **kwargs)    2418     with self._lock:   
> 2419       graph_function, args, kwargs =
> self._maybe_define_function(args, kwargs)
> -> 2420     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access    2421     2422   @property
> 
> ~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py
> in _filtered_call(self, args, kwargs)    1659       `args` and
> `kwargs`.    1660     """"""
> -> 1661     return self._call_flat(    1662         (t for t in nest.flatten((args, kwargs), expand_composites=True)    1663         
> if isinstance(t, (ops.Tensor,
> 
> ~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py
> in _call_flat(self, args, captured_inputs, cancellation_manager)   
> 1743         and executing_eagerly):    1744       # No tape is
> watching; skip to running the function.
> -> 1745       return self._build_call_outputs(self._inference_function.call(    1746       
> ctx, args, cancellation_manager=cancellation_manager))    1747    
> forward_backward = self._select_forward_and_backward_functions(
> 
> ~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py
> in call(self, ctx, args, cancellation_manager)
>     591       with _InterpolateFunctionError(self):
>     592         if cancellation_manager is None:
> --> 593           outputs = execute.execute(
>     594               str(self.signature.name),
>     595               num_outputs=self._num_outputs,
> 
> ~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py
> in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
>      57   try:
>      58     ctx.ensure_initialized()
> ---> 59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
>      60                                         inputs, attrs, num_outputs)
>      61   except core._NotOkStatusException as e:
> 
> InvalidArgumentError:  var and grad do not have the same shape[10]
> [100,10] 	 [[node Adam/Adam/update_4/ResourceApplyAdam (defined at
> <ipython-input-37-9c297d161e54>:29) ]]
> [Op:__inference_train_one_step_20360]
> 
> Errors may have originated from an input operation. Input Source
> operations connected to node Adam/Adam/update_4/ResourceApplyAdam: 
> Mul_4 (defined at <ipython-input-37-9c297d161e54>:26)	 
> sequential/quant_dense_2/BiasAdd/ReadVariableOp/resource (defined at
> /home/arjun/.local/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize_wrapper.py:162)
> 
> Function call stack: train_one_step


Is there a way to combine TF model Quantization along with tf.GradientTape?

Thanks!


  [1]: https://www.tensorflow.org/model_optimization/guide/quantization/training_example
  [2]: https://github.com/arjun-majumdar/Lottery_Ticket_Hypothesis-TensorFlow_2/blob/master/Quantization_LTH_LeNet_300_100_MNIST.ipynb
"
41319,Quantization Aware Training with tf.GradientTape gives Error in TensorFlow2.0,"Hey Guys, I am using TensorFlow-2.2, tensorflow_model_optimization and Python 3.8. I am trying to quantize and train a LeNet-300-100 Dense neural network which contains sparsity of 91.3375%. This means that 91.3375% of the weights are zero. I was following the [Quantization TF tutorial][1] and I wanted to train such a sparse network which has been quantized using *tf.GradientTape* rather than *q_aware_model.fit()*.

If you look into the [example code][2], the relevant code snippets are:


    quantize_model = tfmot.quantization.keras.quantize_model
    
    # q_aware stands for for quantization aware.
    q_aware_model = quantize_model(model)
    
    
    # 'quantize_model' requires recompilation-
    q_aware_model.compile(
        optimizer = tf.keras.optimizers.Adam(lr = 0.0012),
        loss=tf.keras.losses.categorical_crossentropy,
        metrics=['accuracy']
    )
    
    
    # Define 'train_one_step()' and 'test_step()' functions here-
    @tf.function
    def train_one_step(model, mask_model, optimizer, x, y):
        '''
        Function to compute one step of gradient descent optimization
        '''
        with tf.GradientTape() as tape:
            # Make predictions using defined model-
            y_pred = model(x)
    
            # Compute loss-
            loss = loss_fn(y, y_pred)
            
        # Compute gradients wrt defined loss and weights and biases-
        grads = tape.gradient(loss, model.trainable_variables)
        
        # type(grads)
        # list
        
        # List to hold element-wise multiplication between-
        # computed gradient and masks-
        grad_mask_mul = []
        
        # Perform element-wise multiplication between computed gradients and masks-
        for grad_layer, mask in zip(grads, mask_model.trainable_weights):
            grad_mask_mul.append(tf.math.multiply(grad_layer, mask))
        
        # Apply computed gradients to model's weights and biases-
        optimizer.apply_gradients(zip(grad_mask_mul, model.trainable_variables))
    
        # Compute accuracy-
        train_loss(loss)
        train_accuracy(y, y_pred)
    
        return None
        
        
    @tf.function
    def test_step(model, optimizer, data, labels):
        """"""
        Function to test model performance
        on testing dataset
        """"""
        
        predictions = model(data)
        t_loss = loss_fn(labels, predictions)
    
        test_loss(t_loss)
        test_accuracy(labels, predictions)
    
        return None
    
    
    
    # Train model using 'GradientTape'-
        
    # Initialize parameters for Early Stopping manual implementation-
    # best_val_loss = 100
    # loc_patience = 0
        
    for epoch in range(num_epochs):
        
        if loc_patience >= patience:
            print(""\n'EarlyStopping' called!\n"")
            break
            
        # Reset the metrics at the start of the next epoch
        train_loss.reset_states()
        train_accuracy.reset_states()
        test_loss.reset_states()
        test_accuracy.reset_states()
                
        
        for x, y in train_dataset:
            train_one_step(q_aware_model, mask_model, optimizer, x, y)
    
    
        for x_t, y_t in test_dataset:
            test_step(q_aware_model, optimizer, x_t, y_t)
    
        template = 'Epoch {0}, Loss: {1:.4f}, Accuracy: {2:.4f}, Test Loss: {3:.4f}, Test Accuracy: {4:4f}'
        
        '''
        # 'i' is the index for number of pruning rounds-
        history_main[i]['accuracy'][epoch] = train_accuracy.result() * 100
        history_main[i]['loss'][epoch] = train_loss.result()
        history_main[i]['val_loss'][epoch] = test_loss.result()
        history_main[i]['val_accuracy'][epoch] = test_accuracy.result() * 100
        ''' 
    
        print(template.format(
            epoch + 1, train_loss.result(),
            train_accuracy.result()*100, test_loss.result(),
            test_accuracy.result()*100)
             )
        
        # Count number of non-zero parameters in each layer and in total-
        # print(""layer-wise manner model, number of nonzero parameters in each layer are: \n"")
        model_sum_params = 0
        
        for layer in winning_ticket_model.trainable_weights:
            # print(tf.math.count_nonzero(layer, axis = None).numpy())
            model_sum_params += tf.math.count_nonzero(layer, axis = None).numpy()
        
        print(""Total number of trainable parameters = {0}\n"".format(model_sum_params))
    
        
        # Code for manual Early Stopping:
        if np.abs(test_loss.result() < best_val_loss) >= minimum_delta:
            # update 'best_val_loss' variable to lowest loss encountered so far-
            best_val_loss = test_loss.result()
            
            # reset 'loc_patience' variable-
            loc_patience = 0
            
        else:  # there is no improvement in monitored metric 'val_loss'
            loc_patience += 1  # number of epochs without any improvement


Gives the following error:


> --------------------------------------------------------------------------- InvalidArgumentError                      Traceback (most recent call
> last) <ipython-input-47-bca851ce138d> in <module>
>      19 
>      20     for x, y in train_dataset:
> ---> 21         train_one_step(q_aware_model, mask_model, optimizer, x, y)
>      22 
>      23 
> 
> ~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py
> in __call__(self, *args, **kwds)
>     578         xla_context.Exit()
>     579     else:
> --> 580       result = self._call(*args, **kwds)
>     581 
>     582     if tracing_count == self._get_tracing_count():
> 
> ~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py
> in _call(self, *args, **kwds)
>     642         # Lifting succeeded, so variables are initialized and we can run the
>     643         # stateless function.
> --> 644         return self._stateless_fn(*args, **kwds)
>     645     else:
>     646       canon_args, canon_kwds = \
> 
> ~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py
> in __call__(self, *args, **kwargs)    2418     with self._lock:   
> 2419       graph_function, args, kwargs =
> self._maybe_define_function(args, kwargs)
> -> 2420     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access    2421     2422   @property
> 
> ~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py
> in _filtered_call(self, args, kwargs)    1659       `args` and
> `kwargs`.    1660     """"""
> -> 1661     return self._call_flat(    1662         (t for t in nest.flatten((args, kwargs), expand_composites=True)    1663         
> if isinstance(t, (ops.Tensor,
> 
> ~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py
> in _call_flat(self, args, captured_inputs, cancellation_manager)   
> 1743         and executing_eagerly):    1744       # No tape is
> watching; skip to running the function.
> -> 1745       return self._build_call_outputs(self._inference_function.call(    1746       
> ctx, args, cancellation_manager=cancellation_manager))    1747    
> forward_backward = self._select_forward_and_backward_functions(
> 
> ~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py
> in call(self, ctx, args, cancellation_manager)
>     591       with _InterpolateFunctionError(self):
>     592         if cancellation_manager is None:
> --> 593           outputs = execute.execute(
>     594               str(self.signature.name),
>     595               num_outputs=self._num_outputs,
> 
> ~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py
> in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
>      57   try:
>      58     ctx.ensure_initialized()
> ---> 59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
>      60                                         inputs, attrs, num_outputs)
>      61   except core._NotOkStatusException as e:
> 
> InvalidArgumentError:  var and grad do not have the same shape[10]
> [100,10] 	 [[node Adam/Adam/update_4/ResourceApplyAdam (defined at
> <ipython-input-37-9c297d161e54>:29) ]]
> [Op:__inference_train_one_step_20360]
> 
> Errors may have originated from an input operation. Input Source
> operations connected to node Adam/Adam/update_4/ResourceApplyAdam: 
> Mul_4 (defined at <ipython-input-37-9c297d161e54>:26)	 
> sequential/quant_dense_2/BiasAdd/ReadVariableOp/resource (defined at
> /home/arjun/.local/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize_wrapper.py:162)
> 
> Function call stack: train_one_step


Is there a way to combine TF model Quantization along with tf.GradientTape?

Thanks!

The .h5 model file used in this code can be accessed in: [sub-model](https://github.com/arjun-majumdar/Lottery_Ticket_Hypothesis-TensorFlow_2/blob/master/LeNet_300_MNIST_Magnitude_Winning_Ticket_Distribution_91.18900266306589.h5)

Please let me know if I can help with anything else?

  [1]: https://www.tensorflow.org/model_optimization/guide/quantization/training_example
  [2]: https://github.com/arjun-majumdar/Lottery_Ticket_Hypothesis-TensorFlow_2/blob/master/Quantization_LTH_LeNet_300_100_MNIST.ipynb
"
41318,tensorflow lite example is not working converting file to tflite model. ,"what I'm facing is as same as below link

https://github.com/tensorflow/tensorflow/issues/30005

it worked a couple of month ago, but now, it's not working. 
there's gesture calssification listed on example on github and I opened index.html from web folder then I trained to get file. 
but then, how can I convert it to tflite model without collab? 
collab code is not working as link says 
"
41317,Undefined behavior in tf.multiply,"**System information**
- OS Platform and Distribution: Linux 5.7.8-5, Manjaro KDE
- TensorFlow installed from: conda-forge
- TensorFlow version: 2.2.0
- Python version: 3.8.3
- GCC/Compiler version: 10.1.0
- CUDA/cuDNN version: 10.2
- GPU model and memory:  GeForce RTX 2070, 7982 MiB

**Current behavior**
```python
In [1]: a = np.random.randint(0, 10, (2, 2))                                                                                                                                                                            

In [2]: w = np.array([2, 5])                                                                                                                                                                                            

In [3]: x = tf.convert_to_tensor(a)                                                                                                                                                                                     

In [4]: y = tf.convert_to_tensor(w)                                                                                                                                                                                     

In [5]: x                                                                                                                                                                                                               
Out[5]: 
<tf.Tensor: shape=(2, 2), dtype=int64, numpy=
array([[8, 0],
       [5, 7]])>

In [6]: y                                                                                                                                                                                                               
Out[6]: <tf.Tensor: shape=(2,), dtype=int64, numpy=array([2, 5])>

In [7]: x * y                                                                                                                                                                                                           
Out[7]: 
<tf.Tensor: shape=(2, 2), dtype=int64, numpy=
array([[9, 6],
       [0, 0]])>
In [8]: tf.multiply(x, y)                                                                                                                                                                                               
Out[8]: 
<tf.Tensor: shape=(2, 2), dtype=int64, numpy=
array([[0, 6],
       [7, 3]])>
```
Each time the multiplication operation is performed without any modification to `x` and `y` the results were different. I could not find any plausible explanation of this behavior.

**Expected behavior**
When I performed same multiplication in `numpy` it computed the expected result.
```python
In [9]: a                                                                                                                                                                                                               
Out[9]: 
array([[8, 0],
       [5, 7]])

In [10]:  w                                                                                                                                                                                                              
Out[10]: array([2, 5])

In [11]: a * w                                                                                                                                                                                                           
Out[11]: 
array([[16,  0],
       [10, 35]])
``` 

Is there any explanation behind this unexpected behavior? If it is intended behavior then how do I compute the result I expect in this case?
"
41316,"tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [2], [batch]: [5]","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 2.2
- Python version: 3.7.3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

`tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [2], [batch]: [5]`

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```

from datasets import PascalVOCDataset

class PascalVOCDataset(Dataset):
    """"""
    A PyTorch Dataset class to be used in a PyTorch DataLoader to create batches.
    """"""

    def __init__(self, data_folder, split, keep_difficult=False):
        """"""
        :param data_folder: folder where data files are stored
        :param split: split, one of 'TRAIN' or 'TEST'
        :param keep_difficult: keep or discard objects that are considered difficult to detect?
        """"""
        self.split = split.upper()

        assert self.split in {'TRAIN', 'TEST'}

        self.data_folder = data_folder
        self.keep_difficult = keep_difficult

        # Read data files
        with open(os.path.join(data_folder, self.split + '_images.json'), 'r') as j:
            self.images = json.load(j)
        with open(os.path.join(data_folder, self.split + '_objects.json'), 'r') as j:
            self.objects = json.load(j)
        print('dataset_path->',os.path.join(data_folder, self.split + '_images.json'))

        assert len(self.images) == len(self.objects)

    def __getitem__(self, i):
        # Read image
        #print('get_item')
        image = Image.open(self.images[i], mode='r')
        image = image.convert('RGB')

        #print('images_shape->',np.array(image).shape)

        # Read objects in this image (bounding boxes, labels, difficulties)
        objects = self.objects[i]
        #print('image->',image)
        #print('objects[boxes]->',objects['boxes'])
        boxes = torch.FloatTensor(objects['boxes'])  # (n_objects, 4)
        labels = torch.LongTensor(objects['labels'])  # (n_objects)
        difficulties = torch.ByteTensor(objects['difficulties'])  # (n_objects)

        # Discard difficult objects, if desired
        if not self.keep_difficult:
            boxes = boxes[1 - difficulties]
            labels = labels[1 - difficulties]
            difficulties = difficulties[1 - difficulties]

        # Apply transformations
        #print('dataset_boxes->',boxes)
        image, boxes, labels, difficulties = transform(image, boxes, labels, difficulties, split=self.split)

        return image, boxes, labels, difficulties

 # Custom dataloaders
    train_dataset = PascalVOCDataset(data_folder,
                                     split='train',
                                     keep_difficult=keep_difficult)

def run_train(dataset, num_epochs=1):
    start_time = time.perf_counter()
    print('run_train')
    model = SSD(n_classes=20)
    criterion = MultiBoxLoss(priors_cxcy=model.priors_cxcy)
    print('dataset->',dataset)

    for _ in tf.data.Dataset.range(num_epochs):
        for idx,(images,boxes,labels) in enumerate(dataset): # (batch_size (N), 300, 300, 3)
            print('=========================================================')
            images = np.array(images)
            labels = np.array(labels)
            boxes = np.array(list(boxes))
            print('image_shape->', images.shape)
            print('labels_shape->',labels.shape)
            print('boxes_shape->',boxes.shape)

            if isprint: tf.print(type(images), type(labels),images.shape,labels.shape)
            predicted_locs, predicted_socres = model(images)# (N, 8732, 4), (N, 8732, n_classes)
            loss = criterion(predicted_locs,predicted_socres,boxes,labels)
            print('loss->',loss)
            if idx ==10: break
        pass
    tf.print(""실행 시간:"", time.perf_counter() - start_time)
def train():
    print('train')
    print(tf.__version__)
    batch_size= 8
    images,boxes,labels,difficulties,new_boxes= PascalVOCDataset()
    new_boxes = list(new_boxes)


    boxes = tf.ragged.constant(boxes)
    labels = tf.ragged.constant(labels)
    new_boxes = tf.ragged.constant(new_boxes)

    print('boxes->',boxes.shape)
    print('labels->',labels.shape)
    print('images->', np.array(images).shape)

    dataset = tf.data.Dataset.from_tensor_slices((images,new_boxes,labels))
    run_train(dataset.map(resize_image_bbox, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE))

def main():
    train()
if __name__ =='__main__':
    main()

```



"
41315,AttributeError: module 'tensorflow.python.ops.special_math_ops' has no attribute 'bessel_i0e',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64-bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version: 2.2.0
- Python version: 3.7.6
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: AMD RX-580



**Describe the problem**
AttributeError: module 'tensorflow.python.ops.special_math_ops' has no attribute 'bessel_i0e' arises when trying to use tensorflow's object detection API to fine-tune EfficientDet to detect a custom class I created a dataset and matching test/train TFrecord files.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

1. I followed the instructions on https://github.com/tensorflow/models/tree/master/official#running-the-models under ""How to get started with the official models"" to install dependencies, clone the models repo, and add the models folder to my python path. The last part I accomplished by adding a sys.path.append(r'C:\Users\user\models') line to the beginning of model_main_tf2.py, which is the file I was under the impression runs the training loop. I also installed protoc-3.12.3-win64.zip and ran ""C:/Program Files/protoc/bin/protoc"" object_detection/protos/*.proto --python_out=. in the models directory.

2. I downloaded and configured the config files for EfficientDet D7 from https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md to point to my TFrecord test/train files, the checkpoint/ckpt-0 file and label_map.txt files.

3. I ran the following command in an anaconda powershell which prompted the AttributeError: module 'tensorflow.python.ops.special_math_ops' has no attribute 'bessel_i0e':

python model_main_tf2.py --model_dir=C:\Users\user\efficientdet_d7_coco17_tpu-32\training --num_train_steps=10000 --sample_1_of_n_eval_examples=1 --pipeline_config_path=C:\Users\user\efficientdet_d7_coco17_tpu-32\pipeline.config --alsologtostderr 

I also tried it with and without the --checkpoint_dir = C:\Users\user\efficientdet_d7_coco17_tpu-32\checkpoint flag

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

(base) C:\Users\user\models\research\object_detection>python model_main_tf2.py --model_dir=C:\Users\user\efficientdet_d7_coco17_tpu-32\training --num_train_steps=10000 --sample_1_of_n_eval_examples=1 --pipeline_config_path=C:\Users\user\efficientdet_d7_coco17_tpu-32\pipeline.config --alsologtostderr
2020-07-12 02:03:31.351897: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found
2020-07-12 02:03:31.356039: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Traceback (most recent call last):
  File ""model_main_tf2.py"", line 35, in <module>
    import tensorflow.compat.v2 as tf
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\__init__.py"", line 74, in <module>
    from tensorflow.python.ops.standard_ops import *
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\standard_ops.py"", line 27, in <module>
    from tensorflow.python.training.experimental import loss_scaling_gradient_tape
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\experimental\loss_scaling_gradient_tape.py"", line 21, in <module>
    from tensorflow.python.distribute import distribution_strategy_context
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\distribute\__init__.py"", line 28, in <module>
    from tensorflow.python.distribute.experimental import collective_all_reduce_strategy
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\distribute\experimental\__init__.py"", line 25, in <module>
    from tensorflow.python.distribute import tpu_strategy
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\distribute\tpu_strategy.py"", line 28, in <module>
    from tensorflow.compiler.xla.experimental.xla_sharding import xla_sharding
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\compiler\xla\experimental\xla_sharding\xla_sharding.py"", line 23, in <module>
    from tensorflow.compiler.tf2xla.python import xla as tf2xla
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\compiler\tf2xla\python\xla.py"", line 107, in <module>
    bessel_i0e = _unary_op(special_math_ops.bessel_i0e)
AttributeError: module 'tensorflow.python.ops.special_math_ops' has no attribute 'bessel_i0e'

Please let me know if there is any other info I can provide.
"
41314,"python program spitting out a tensorflow error of ""ImportError: DLL Load failed while importing"" ","**System information**
- Windows 10 pro 64bit ver 2004
- TensorFlow version: 2.2.0
- Python version: 3.8.3
- Installed using PIP
- CUDA ver: 10.0
- GPU model and memory: NVIDIA GTX 1070 8GB
-My CPU does not support the AVX Instruction set


when attempting to use a python program, I am greeted with this error (Copied Verbatim below)
     (side note, I'm not great with python or most of the dev tools I've attempted to use to download all of the required libraries, so forgive me if this has a relatively simple solution.)

This occurs when I use the CMD prompt to execute the main.py file for this program. to do this, I typed this command
Python main.py
after a few moments the error is triggered. the prompt remains blank until the error traceback appears.
(I can give more specifics to the best of my ability, but this is all of the information that I am aware that I have to help diagnose)

(this is the entire error verbatim)

Error when importing libraries:  Traceback (most recent call last):
  File ""G:\APPLICATIONS\Python\3.8.3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""G:\APPLICATIONS\Python\3.8.3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""G:\APPLICATIONS\Python\3.8.3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""G:\APPLICATIONS\Python\3.8.3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""G:\APPLICATIONS\Python\3.8.3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
Some Python libraries are missing. You can install all required libraries by running in the command line 'pip install -r requirements.txt'
"
41313,"Colab TPU not working in 2.3.0-rc1, ""InvalidArgumentError: NodeDef expected inputs 'string' do not match 0 inputs specified""","

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab

- TensorFlow installed from (source or binary): Installed using `!pip install --upgrade tensorflow==2.3.0-rc1`

- TensorFlow version (use command below): 2.3.0-rc1
- Python version: 3, colab default 

- GPU model and memory: Colab TPU 


**Describe the current behavior**

When I try to initialize the TPU using 

```
tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
tf.config.experimental_connect_to_cluster(tpu)
tf.tpu.experimental.initialize_tpu_system(tpu)
```

I get this error 

```
INFO:tensorflow:2.3.0-rc1
INFO:absl:Entering into master device scope: /job:worker/replica:0/task:0/device:CPU:0
INFO:tensorflow:Initializing the TPU system: grpc://10.6.63.186:8470
INFO:tensorflow:Initializing the TPU system: grpc://10.6.63.186:8470
INFO:tensorflow:Clearing out eager caches
INFO:tensorflow:Clearing out eager caches
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-4-5aaeec42239c> in <module>()
     19 tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
     20 tf.config.experimental_connect_to_cluster(tpu)
---> 21 tf.tpu.experimental.initialize_tpu_system(tpu)
     22 strategy = tf.distribute.experimental.TPUStrategy(tpu)
     23 print('strategy.num_replicas_in_sync', strategy.num_replicas_in_sync)

3 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/tpu/tpu_strategy_util.py in initialize_tpu_system(cluster_resolver)
    109     context.context()._clear_caches()  # pylint: disable=protected-access
    110 
--> 111     serialized_topology = output.numpy()
    112 
    113     # TODO(b/134094971): Remove this when lazy tensor copy in multi-device

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in numpy(self)
   1061     """"""
   1062     # TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.
-> 1063     maybe_arr = self._numpy()  # pylint: disable=protected-access
   1064     return maybe_arr.copy() if isinstance(maybe_arr, np.ndarray) else maybe_arr
   1065 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _numpy(self)
   1029       return self._numpy_internal()
   1030     except core._NotOkStatusException as e:  # pylint: disable=protected-access
-> 1031       six.raise_from(core._status_to_exception(e.code, e.message), None)  # pylint: disable=protected-access
   1032 
   1033   @property

/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

InvalidArgumentError: NodeDef expected inputs 'string' do not match 0 inputs specified; Op<name=_Send; signature=tensor:T -> ; attr=T:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>; NodeDef: {{node _Send}}
```

**Describe the expected behavior**

TPU should initialize like in 2.2.0

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.


Here is a github colab gist 

https://colab.research.google.com/gist/Santosh-Gupta/a4d5459c13b4bb54ace08993f5b174da/tpu_try_2-3-0-rc1.ipynb


"
41312,"model.fit() InvalidArgumentError:  indices[28,13] = -2147483648 is not in [0, 1193514) ","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Conda
- TensorFlow version (use command below): 2.1.0
- Python version: 3.6.10
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: Disabled (Hardcoding TensorFlow without GPU)

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
I try to fit my model and face this problem, for some reason, the largest negative number is being looked up.
**Describe the expected behavior**
The model should proceed through ftting seamlessly
**Standalone code to reproduce the issue**

**->Embedding layer**
    

```
def pretrained_embedding_layer(word_to_vec_map, word_to_index):
   

    vocab_len = len(word_to_index) + 1            #1193514      
    emb_matrix = np.zeros((vocab_len,embedding_dim))
    for word, idx in word_to_index.items():
        emb_matrix[idx, :] = word_to_vec_map[word]

    # Definning a pre-trained Embedding layer
    embedding_layer = layers.Embedding(
                        vocab_len,
                        embedding_dim,
                        trainable = False
                        )

    # Build the embedding layer, it is required before setting the weights of the embedding layer. 
    embedding_layer.build((None,))
    
    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.
    embedding_layer.set_weights([emb_matrix])
    
    return embedding_layer
```



**->Model**

```
def sentiment_model(input_shape, word_to_vec_map, word_to_index):


    sentence_indices =layers.Input(shape=input_shape, dtype='float32')
    
    # Create the embedding layer pretrained with GloVe Vectors
    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)
    
    # Propagate sentence_indices through your embedding layer
    # (See additional hints in the instructions).
    embeddings = embedding_layer(sentence_indices)   

    x = layers.LSTM(128)(embeddings)
    x = layers.Dropout(0.5)(x)
    predictions = layers.Dense(2, activation=""sigmoid"", name=""predictions"")(x)
    
    # Create Model instance which converts sentence_indices into X.
    model = keras.Model(inputs=sentence_indices,outputs=predictions)   
    return model
```



```
def sentences_to_indices(X, word_to_index, max_len):

    X_indices = np.zeros((m,max_len))
    
    # Assign indices to words
    for i,sentence in enumerate(X):        
        sentence_words = sentence.lower().split()
        for j,word in enumerate(sentence_words):
            X_indices[i, j] = word_to_index[word]
    return X_indices
```


```
X_train_indices = sentences_to_indices(X_train, word_to_index, max_features)
Y_train_OH = to_categorical(Y_train)
model.fit(X_train_indices, Y_train_OH, epochs = 10, batch_size = 32)
```


Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

**Train on 28624 samples
Epoch 1/10
   32/28624 [..............................] - ETA: 15:20**

**InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-25-4679097c6578> in <module>
----> 1 model.fit(X_train_indices, Y_train_OH, epochs = 10, batch_size = 32)**

~\Anaconda3\envs\sentiment_analysis\lib\site-packages\tensorflow_core\python\keras\engine\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    817         max_queue_size=max_queue_size,
    818         workers=workers,
--> 819         use_multiprocessing=use_multiprocessing)
    820 
    821   def evaluate(self,

~\Anaconda3\envs\sentiment_analysis\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    340                 mode=ModeKeys.TRAIN,
    341                 training_context=training_context,
--> 342                 total_epochs=epochs)
    343             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)
    344 

~\Anaconda3\envs\sentiment_analysis\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)
    126         step=step, mode=mode, size=current_batch_size) as batch_logs:
    127       try:
--> 128         batch_outs = execution_function(iterator)
    129       except (StopIteration, errors.OutOfRangeError):
    130         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?

~\Anaconda3\envs\sentiment_analysis\lib\site-packages\tensorflow_core\python\keras\engine\training_v2_utils.py in execution_function(input_fn)
     96     # `numpy` translates Tensors to values in Eager mode.
     97     return nest.map_structure(_non_none_constant_value,
---> 98                               distributed_function(input_fn))
     99 
    100   return execution_function

~\Anaconda3\envs\sentiment_analysis\lib\site-packages\tensorflow_core\python\eager\def_function.py in __call__(self, *args, **kwds)
    566         xla_context.Exit()
    567     else:
--> 568       result = self._call(*args, **kwds)
    569 
    570     if tracing_count == self._get_tracing_count():

~\Anaconda3\envs\sentiment_analysis\lib\site-packages\tensorflow_core\python\eager\def_function.py in _call(self, *args, **kwds)
    630         # Lifting succeeded, so variables are initialized and we can run the
    631         # stateless function.
--> 632         return self._stateless_fn(*args, **kwds)
    633     else:
    634       canon_args, canon_kwds = \

~\Anaconda3\envs\sentiment_analysis\lib\site-packages\tensorflow_core\python\eager\function.py in __call__(self, *args, **kwargs)
   2361     with self._lock:
   2362       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
-> 2363     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
   2364 
   2365   @property

~\Anaconda3\envs\sentiment_analysis\lib\site-packages\tensorflow_core\python\eager\function.py in _filtered_call(self, args, kwargs)
   1609          if isinstance(t, (ops.Tensor,
   1610                            resource_variable_ops.BaseResourceVariable))),
-> 1611         self.captured_inputs)
   1612 
   1613   def _call_flat(self, args, captured_inputs, cancellation_manager=None):

~\Anaconda3\envs\sentiment_analysis\lib\site-packages\tensorflow_core\python\eager\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1690       # No tape is watching; skip to running the function.
   1691       return self._build_call_outputs(self._inference_function.call(
-> 1692           ctx, args, cancellation_manager=cancellation_manager))
   1693     forward_backward = self._select_forward_and_backward_functions(
   1694         args,

~\Anaconda3\envs\sentiment_analysis\lib\site-packages\tensorflow_core\python\eager\function.py in call(self, ctx, args, cancellation_manager)
    543               inputs=args,
    544               attrs=(""executor_type"", executor_type, ""config_proto"", config),
--> 545               ctx=ctx)
    546         else:
    547           outputs = execute.execute_with_cancellation(

~\Anaconda3\envs\sentiment_analysis\lib\site-packages\tensorflow_core\python\eager\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     65     else:
     66       message = e.message
---> 67     six.raise_from(core._status_to_exception(e.code, message), None)
     68   except TypeError as e:
     69     keras_symbolic_tensors = [

~\Anaconda3\envs\sentiment_analysis\lib\site-packages\six.py in raise_from(value, from_value)

**InvalidArgumentError:  indices[15,2] = -2147483648 is not in [0, 1193514)
	 [[node model_1/embedding_1/embedding_lookup (defined at <ipython-input-25-4679097c6578>:1) ]] [Op:__inference_distributed_function_6120]**

Errors may have originated from an input operation.
Input Source operations connected to node model_1/embedding_1/embedding_lookup:
 model_1/embedding_1/embedding_lookup/4992 (defined at C:\Users\shash\Anaconda3\envs\sentiment_analysis\lib\contextlib.py:81)

Function call stack:
distributed_function



"
41311,Tensorflow default supports dynamic preprocessing (e.g. masking).,"# Environment
- tensorflow 2.x
- google colab

# Reproducible code
https://colab.research.google.com/gist/MokkeMeguru/8c867b2bbc941cc82e115b9044276afc/test.ipynb

# Problem
if text data preprocessing using the map function, each epoch resets previous preprocessing information.

```python
def encode(text):
    encoded_text = encoder.encode(text.numpy())
    tf.print(""raw"", encoded_text)
    
    def random_masking(s):
        s = [word if tf.random.uniform([]) > mask_rate else mask_idx for word in s]
        return s

    encoded_text = random_masking(encoded_text)
    tf.print(""masked"", encoded_text)
    return [encoded_text

def encode_map_fn(text):
    encoded_text = tf.py_function(encode, inp=[text], Tout=(tf.int64))
    encoded_text.set_shape([None])
    return encoded_text

encoder.encode(next(iter(ds)).numpy()

encoded_ds = ds.map(encode_map_fn).padded_batch(3)

print(""mask is"", mask_idx)

for epoch in range(3):
    print(""epoch :"", epoch)
    for batch in encoded_ds:
        print(batch)
```
```
mask is 12
epoch : 0
raw [11, 6, 1, 5, 9]
masked [11, 6, 1, 12, 9] # <---------!
...
epoch : 1
raw [11, 6, 1, 5, 9]
masked [11, 6, 12, 12, 9] # <------!
...
```

In some NLP methods, we know static preprocessing (like BERT) and dynamic preprocessing (like RoBERTa) don't have the same performance.

**I want to know whether this dynamic preprocessing is the correct attribute or not.**
And also
**How to keep the preprocessing information**"
41310,Create a Simple Speech Recognition example using TensorFlow Version 2,A tracking bug for migrating the [TF1 Speech Recognition example](https://github.com/tensorflow/docs/blob/master/site/en/r1/tutorials/sequences/audio_recognition.md) to TF 2
41309,    AttributeError: 'OwnedIterator' object has no attribute '_get_trainable_state',"Getting  the error AttributeError: in user code:

Does anybody know what is causing the issue ?

The code was working fine a couple of days back. 


```

new_input = Input(shape=(128 , 128 , 3))
base = EfficientNetB0(include_top=False ,
                input_tensor=new_input )

for layer in base.layers :
    layer.trainable = False
    
x = base.output
x = Conv2D (64 , (3,3) , activation = 'relu')(x)
x = MaxPooling2D((2,2))(x)
x = GlobalAveragePooling2D ()(x)

out = Dense (6 , activation = 'softmax')(x)

model = Model (inputs = base.input , outputs = out)

cce = tf.keras.losses.CategoricalCrossentropy()

model.compile(loss=cce,
              optimizer=Adam(),
              metrics=kappa_score)

history = model.fit_generator(train_generator,
                    validation_data=val_generator, 
                    epochs = 100)

```

Log is as below - 

**---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-49-3e58d15894cd> in <module>
     24 history = model.fit_generator(train_generator,
     25                     validation_data=val_generator,
---> 26                     epochs = 100)

/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)
    322               'in a future version' if date is None else ('after %s' % date),
    323               instructions)
--> 324       return func(*args, **kwargs)
    325     return tf_decorator.make_decorator(
    326         func, new_func, 'deprecated',

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1477                                 for t in nest.flatten(outputs)])
   1478           step_outputs = step_function(self, iterator)
-> 1479           outputs = nest.map_structure(lambda t1, t2: concat([t1, t2]), outputs,
   1480                                        step_outputs)
   1481         return outputs

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
     64 from tensorflow.python.ops import math_ops
     65 from tensorflow.python.ops import sparse_ops
---> 66 from tensorflow.python.ops import summary_ops_v2
     67 from tensorflow.python.ops import variables
     68 from tensorflow.python.ops.ragged import ragged_concat_ops

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
    846 
    847     Arguments:
--> 848         x: Input data. It could be:
    849           - A Numpy array (or array-like), or a list of arrays
    850             (in case the model has multiple inputs).

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    578       target_function = target_function.__func__
    579 
--> 580     if hasattr(target_function, ""__code__""):
    581       return target_function.__code__
    582 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    625         implements_attr = six.ensure_text(self._implements, ""utf-8"")
    626         attr_value = attr_value_pb2.AttrValue()
--> 627         nameattrlist = attr_value_pb2.NameAttrList()
    628         _text_format.Merge(implements_attr, nameattrlist)
    629         attr_value.func.CopyFrom(nameattrlist)

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    504         conversion options when autograph is set to True.
    505       experimental_relax_shapes: When true, argument shapes may be relaxed to
--> 506         avoid unnecessary retracing.
    507       experimental_compile: If `True`, compiles the function using XLA
    508         (see https://tensorflow.org/xla). XLA performs compiler optimizations,

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   2444     self._args_to_indices = {arg: i for i, arg in enumerate(args)}
   2445     self._arg_names = args
-> 2446 
   2447     # A cache mapping from arg index to default value, for canonicalization.
   2448     default_values = fullargspec.defaults

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   2775 
   2776   def all_values(self):
-> 2777     """"""A set of all `ConcreteFunction` instances held by this cache.""""""
   2778     return set(self.primary.values()) | set(self.arg_relaxed.values())
   2779 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   2665       # CompositeTensors should be flattened instead.
   2666       or isinstance(value, composite_tensor.CompositeTensor))
-> 2667 
   2668 
   2669 def _convert_numpy_inputs(inputs):

/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    979         converted_func = tf_decorator.make_decorator(original_func, wrapper)
    980         python_func = tf_decorator.rewrap(python_func, original_func,
--> 981                                           converted_func)
    982 
    983       else:

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    439 
    440   def __del__(self):
--> 441     try:
    442       func_graph_module.dismantle_func_graph(self.func_graph)
    443     except:  # pylint: disable=bare-except

/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    966                 options=autograph.ConversionOptions(
    967                     recursive=True,
--> 968                     optional_features=autograph_options,
    969                     user_requested=True,
    970                 ))

AttributeError: in user code:

    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:576 _reset_compile_cache  *
        self._compiled_trainable_state = self._get_trainable_state()

    AttributeError: 'OwnedIterator' object has no attribute '_get_trainable_state'
** "
41308,[TFLite] Two consecutive Dequantize OPs are generated in the structure of the model after integer quantization,"## 1. System information
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 x86_64
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): tensorflow==2.3.0-rc1

## 2. Command used to run the converter, and code using the Python API
**[Here's a minimal Google Colaboratory](https://colab.research.google.com/drive/1Nrf_zZjUgrlp4yi6vJFVj2yBZIDwGxYZ?usp=sharing)** that can reproduce the situation.

### (1) Integer Quantization
Performs integer quantization. The resources used for quantization are obtained at **[Google Colaboratory](https://colab.research.google.com/drive/1Nrf_zZjUgrlp4yi6vJFVj2yBZIDwGxYZ?usp=sharing)**. This step will successfully generate an integer quantized .tflite file.
```python
### tensorflow==2.3.0-rc1

import tensorflow as tf
import numpy as np

def representative_dataset_gen():
  for image in raw_test_data:
    image = tf.image.resize(image, (512, 512))
    image = image[np.newaxis,:,:,:]
    image = image - 127.5
    image = image * 0.007843
    yield [image]

raw_test_data = np.load('calibration_data_img_coco_512x512.npy', allow_pickle=True)

# Integer Quantization - Input/Output=float32
converter = tf.lite.TFLiteConverter.from_saved_model('saved_model')
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset_gen
tflite_quant_model = converter.convert()
with open('efficientdet_d0_512x512_integer_quant.tflite', 'wb') as w:
    w.write(tflite_quant_model)
print(""Integer Quantization complete! - efficientdet_d0_512x512_integer_quant.tflite"")
```
```console
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
  :
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Issue encountered when serializing global_step.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
to_proto not supported in EAGER mode.
Integer Quantization complete! - efficientdet_d0_512x512_integer_quant.tflite
```

### (2) Checking the operation of the model
The next step is to check the execution of the model with a minimal amount of test code. When you run this test code, you'll see an error in the input geometry mismatch for the Dequantize OP occurs.
```python
import numpy as np
import tensorflow as tf

interpreter = tf.lite.Interpreter(model_path=""efficientdet_d0_512x512_integer_quant.tflite"")
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

print('input:', input_details)
print('')
print('output:', output_details)

input_shape = input_details[0]['shape']
input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
interpreter.set_tensor(input_details[0]['index'], input_data)
interpreter.invoke()
output_data = interpreter.get_tensor(output_details[0]['index'])
print('output_data.shape:', output_data.shape)
```
```console
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-7-c9ae37dddd9b> in <module>()
      3 
      4 interpreter = tf.lite.Interpreter(model_path=""efficientdet_d0_512x512_integer_quant.tflite"")
----> 5 interpreter.allocate_tensors()
      6 input_details = interpreter.get_input_details()
      7 output_details = interpreter.get_output_details()

/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter.py in allocate_tensors(self)
    241   def allocate_tensors(self):
    242     self._ensure_safe()
--> 243     return self._interpreter.AllocateTensors()
    244 
    245   def _safe_to_run(self):

RuntimeError: tensorflow/lite/kernels/dequantize.cc:61 op_context.input->type == kTfLiteUInt8 || op_context.input->type == kTfLiteInt8 || op_context.input->type == kTfLiteInt16 || op_context.input->type == kTfLiteFloat16 was not true.Node number 782 (DEQUANTIZE) failed to prepare.
```

## 3. Links to saved models, GraphDef, and test datasets
The links below contain EfficientDet D0's GraphDef and saved_model and test datasets.
**https://drive.google.com/file/d/1ymLnGUebTPTRtGLPy539w0P0r3UZfcA_/view?usp=sharing**
The integer quantized .tflite file can be obtained from the link below.
**https://drive.google.com/file/d/12S6GgRn4I2jl1c5omP023i4zgKACyDe5/view?usp=sharing**
## 4. Failure details
In some places, a double Dequantize OP is generated in duplicate. Due to this issue, running the Python API **`interpreter.allocate_tensors()`** causes an error indicating that the structure of the model is flawed.
```console
RuntimeError: tensorflow/lite/kernels/dequantize.cc:61 op_context.input->type == kTfLiteUInt8 || op_context.input->type == kTfLiteInt8 || op_context.input->type == kTfLiteInt16 || op_context.input->type == kTfLiteFloat16 was not true.Node number 782 (DEQUANTIZE) failed to prepare.
```
![Screenshot 2020-07-11 21:57:04](https://user-images.githubusercontent.com/33194443/87227676-9eedf000-c3d7-11ea-952c-5375a67b4f36.png)
The content of the error message and the INDEX number of the OP indicates that it is the second Dequantize OP that is causing this problem.
![Screenshot 2020-07-12 00:38:44](https://user-images.githubusercontent.com/33194443/87227799-276c9080-c3d8-11ea-948a-ce7799cdc3a5.png)
## 5. Related Issues
- [tflite: Slicing isn't compatible with quantisation #29571](https://github.com/tensorflow/tensorflow/issues/29571)
- [TFLite Interpreter, allocate_tensors() failed to prepare, not kTFLiteInt8/Uint8 #31053](https://github.com/tensorflow/tensorflow/issues/31053)"
41307,[DLPack] DLPack doesn't work with int32 on gpu,"Reproduce code: https://colab.research.google.com/drive/1kwR_ZVutSot7yA7FqPUvCsKgXcirmNLh?pli=1#scrollTo=UI-LuV7EUick

Error message on my machine:
```python
2020-07-11 14:42:25.598880: E tensorflow/stream_executor/cuda/cuda_driver.cc:1037] failed to enqueue async memcpy from device to host: CUDA_ERROR_INVALID_VALUE: invalid argument; host dst: 0x1abfd2c0; GPU src: 0x1a027e80; size: 12=0xc
2020-07-11 14:42:25.598954: F tensorflow/core/common_runtime/gpu/gpu_util.cc:291] GPU->CPU Memcpy failed
```

It seems the int32 tensor's data pointer is on cpu instead of being on gpu, althought its device is ""gpu"". Previously as I know there's special case handling for int32 tensor(https://github.com/tensorflow/tensorflow/issues/34071), not sure whether this is related.

And I just found I used constant_op.constant to test dlpacks, which means now it only test the case on cpu since tf.constant always place data on host memory

Do you have any idea on this issue? @sanjoy @alextp "
41306,"ValueError: Tensor-typed variable initializers must either be wrapped in an init_scope or callable (e.g., `tf.Variable(lambda : tf.truncated_normal([10, 40]))`) when building functions.","### Version ###

OS Platform
Python 3.7.6
tensorflow 2.2.0
keras 2.3.0-tf
mem 64247.95703125
cpu 16

### Try to reproduce this work ###
https://www.kaggle.com/devang/transfer-learning-with-keras-and-mobilenet-v2

### Issues ###
## The error occurs when I try to define layers and compile model using: ##
---------------------------------------------------------------------------------------------------------------------------------------------
epochs = 100
batch_size = 150
testsplit = .2
targetx = 224
targety = 224
learning_rate = 0.0001
classes = 120
seed = random.randint(1, 1000)

shape=(targetx, targety, 3))

x = base_model.output
x = GlobalAveragePooling2D()(x)

x = Dropout(rate = .2)(x)
x = BatchNormalization()(x)
x = Dense(1280, activation='relu', kernel_initializer=glorot_uniform(seed), bias_initializer='zeros')(x)

x = Dropout(rate = .2)(x)
x = BatchNormalization()(x)
predictions = Dense(classes, activation='softmax', kernel_initializer='random_uniform', bias_initializer='zeros')(x)

model = Model(inputs=base_model.input, outputs=predictions)

---------------------------------------------------------------------------------------------------------------------------------------------
### Error report ###

---------------------------------------------------------------------------------------------------------------------------------------------
ValueError Traceback (most recent call last)
in
12 """"""
13
---> 14 x = Dense(1280, activation='relu', kernel_initializer=glorot_uniform(seed), bias_initializer='zeros')(x)
15 x = Dropout(rate = .2)(x)
16 x = BatchNormalization()(x)

/apps/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py in symbolic_fn_wrapper(*args, **kwargs)
73 if _SYMBOLIC_SCOPE.value:
74 with get_graph().as_default():
---> 75 return func(*args, **kwargs)
76 else:
77 return func(*args, **kwargs)

/apps/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/keras/engine/base_layer.py in call(self, inputs, **kwargs)
461 'You can build it manually via: '
462 'layer.build(batch_input_shape)')
--> 463 self.build(unpack_singleton(input_shapes))
464 self.built = True
465

/apps/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/keras/layers/core.py in build(self, input_shape)
893 name='kernel',
894 regularizer=self.kernel_regularizer,
--> 895 constraint=self.kernel_constraint)
896 if self.use_bias:
897 self.bias = self.add_weight(shape=(self.units,),

/apps/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/keras/engine/base_layer.py in add_weight(self, name, shape, dtype, initializer, regularizer, trainable, constraint)
280 dtype=dtype,
281 name=name,
--> 282 constraint=constraint)
283 if regularizer is not None:
284 with K.name_scope('weight_regularizer'):

/apps/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py in variable(value, dtype, name, constraint)
618 """"""
619 v = tf_keras_backend.variable(
--> 620 value, dtype=dtype, name=name, constraint=constraint)
621 if hasattr(value, 'tocoo'):
622 v._keras_shape = value.tocoo().shape

/apps/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/keras/backend.py in variable(value, dtype, name, constraint)
843 dtype=dtypes_module.as_dtype(dtype),
844 name=name,
--> 845 constraint=constraint)
846 if isinstance(value, np.ndarray):
847 v._keras_shape = value.shape

/apps/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/variables.py in call(cls, *args, **kwargs)
259 return cls._variable_v1_call(*args, **kwargs)
260 elif cls is Variable:
--> 261 return cls._variable_v2_call(*args, **kwargs)
262 else:
263 return super(VariableMetaclass, cls).call(*args, **kwargs)

/apps/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/variables.py in _variable_v2_call(cls, initial_value, trainable, validate_shape, caching_device, name, variable_def, dtype, import_scope, constraint, synchronization, aggregation, shape)
253 synchronization=synchronization,
254 aggregation=aggregation,
--> 255 shape=shape)
256
257 def call(cls, *args, **kwargs):

/apps/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/variables.py in (**kws)
234 shape=None):
235 """"""Call on Variable class. Useful to force the signature.""""""
--> 236 previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)
237 for _, getter in ops.get_default_graph()._variable_creator_stack: # pylint: disable=protected-access
238 previous_getter = _make_getter(getter, previous_getter)

/apps/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py in default_variable_creator_v2(next_creator, **kwargs)
2645 synchronization=synchronization,
2646 aggregation=aggregation,
-> 2647 shape=shape)
2648
2649

/apps/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/variables.py in call(cls, *args, **kwargs)
261 return cls._variable_v2_call(*args, **kwargs)
262 else:
--> 263 return super(VariableMetaclass, cls).call(*args, **kwargs)
264
265

/apps/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py in init(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)
1432 aggregation=aggregation,
1433 shape=shape,
-> 1434 distribute_strategy=distribute_strategy)
1435
1436 def _init_from_args(self,

/apps/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py in _init_from_args(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)
1515 if isinstance(initial_value, ops.Tensor) and hasattr(
1516 initial_value, ""graph"") and initial_value.graph.building_function:
-> 1517 raise ValueError(""Tensor-typed variable initializers must either be ""
1518 ""wrapped in an init_scope or callable ""
1519 ""(e.g., `tf.Variable(lambda : ""

ValueError: Tensor-typed variable initializers must either be wrapped in an init_scope or callable (e.g., tf.Variable(lambda : tf.truncated_normal([10, 40]))) when building functions. Please file a feature request if this restriction inconveniences you.

---------------------------------------------------------------------------------------------------------------------------------------------
I had a few go on changing the 'seed' to the following:

- [1, seed]
- tf.constant(np.random.rand(2, 2))
- tf.keras.Variable(lambda : tf.truncated_normal([1, seed]))

However, I still can't manage to convert 'seed' to a tensor.

Can anyone help me please?

Any suggestions/feedback will be much appreciated!"
41305,[RNN] Converting network with LSTM layer to int8 sefaults whole python,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu or Google Colab
- TensorFlow installed from (source or binary): pip install tf-nightly
- TensorFlow version (or github SHA if from source): 2.4.0-dev20200710


Trying out conversion, I managed to reproducibly segfault. I am not sure whether I am supplying wrong inputs to representative_dataset but in any case, the code should not be segfaulting and at max throwing python errors. The issue is reproducible also on colab.

```
!pip install tf-nightly
```

```
import numpy as np
import tensorflow as tf
tf.__version__
```

```
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
x_train = x_train.astype(np.float32)
x_test = x_test.astype(np.float32)

_EPOCHS = 1
_TRAINING_DATA_COUNT = 1000
x_train = x_train[:_TRAINING_DATA_COUNT]
y_train = y_train[:_TRAINING_DATA_COUNT]

model.fit(x_train, y_train, epochs=_EPOCHS)
model.evaluate(x_test, y_test, verbose=0)
```
> [1.8708466291427612, 0.531000018119812]

```
run_model = tf.function(lambda x: model(x))
# This is important, let's fix the input size.
BATCH_SIZE = 1
STEPS = 28
INPUT_SIZE = 28
concrete_func = run_model.get_concrete_function(
    tf.TensorSpec([BATCH_SIZE, STEPS, INPUT_SIZE], model.inputs[0].dtype))

# model directory.
MODEL_DIR = ""keras_lstm""
model.save(MODEL_DIR, save_format=""tf"", signatures=concrete_func)
```
WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f69aa50b730> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: <module '__main__'> is a built-in module
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f69aa50b730> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: <module '__main__'> is a built-in module
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function <lambda> at 0x7f69aa50b730> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: <module '__main__'> is a built-in module
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
INFO:tensorflow:Assets written to: keras_lstm/assets
INFO:tensorflow:Assets written to: keras_lstm/assets

```
def representative_dataset_gen():
    return [[x_train[:1]]] # Not sure why I need to wrap this in two lists (otherwise converter complains)

converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)
converter.experimental_new_converter = True
converter.optimizations = [tf.lite.Optimize.DEFAULT]
        
converter.representative_dataset = representative_dataset_gen
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8
```

**Running this last cell now segfaults python interpreter / crashes jupyter kernel**
```
# This segfaults
tflite_model = converter.convert()
```

"
41304,tf.py_function fails on the tutorial code when running tensorflow-gpu 2.2 on Windows 10,"**System information**
- OS Platform: Windows 10
- TensorFlow installed: using pip
- TensorFlow version: tensorflow gpu 2.2
- Python version: 3.8.3
- CUDA/cuDNN version: CUDA 10.1
- GPU model and memory: GeForce GTX 1080, 8GB

**Describe the current behavior**
The serialized result without tf.py_function was normal.
But there were exception errors from eager_py_func when trying to get the serialized result with tf.py_function.
**Describe the expected behavior**
The serialized result without/with tf.py_fuction should be the same.

**Standalone code to reproduce the issue**
The issue can be reproduced by following code (from tensoflow tutorial, ""TFRecord and tf.Example"", https://www.tensorflow.org/tutorials/load_data/tfrecord). 
The code is running well on  Colab on the tutorial website but there are exception errors when running on Windows 10 with cmd or pycharm.
 Here is the code:
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

import tensorflow as tf

def _bytes_feature(value):
  """"""Returns a bytes_list from a string / byte.""""""
  if isinstance(value, type(tf.constant(0))):
    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.
  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))

def _float_feature(value):
  """"""Returns a float_list from a float / double.""""""
  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))

def _int64_feature(value):
  """"""Returns an int64_list from a bool / enum / int / uint.""""""
  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))

def serialize_example(feature0, feature1, feature2, feature3):
  feature = {
      'feature0': _int64_feature(feature0),
      'feature1': _int64_feature(feature1),
      'feature2': _bytes_feature(feature2),
      'feature3': _float_feature(feature3),
  }
  example_proto = tf.train.Example(features=tf.train.Features(feature=feature))
  return example_proto.SerializeToString()

def tf_serialize_example(f0,f1,f2,f3):
  tf_string = tf.py_function(
    serialize_example,
    (f0,f1,f2,f3),
    tf.string)
  return tf.reshape(tf_string, ())

result_without_py_function = serialize_example(False, 4, b'goat', 0.9876)
print(""result_without_py_function ="",result_without_py_function)

result_with_py_function = tf_serialize_example(False, 4, b'goat', 0.9876)
print(""result_with_py_function="",result_with_py_function)
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

I only wrote extra lines to print out results from serialize_example() and tf_serialize_example() for comparison.
Running the above code on windows cmd or pycharm will get the exception errors when executing:
result_with_py_function = tf_serialize_example(False, 4, b'goat', 0.9876)

All the output including error messages:

020-07-10 20:36:22.730441: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-07-10 20:36:25.159732: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2020-07-10 20:36:25.298922: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1080 computeCapability: 6.1
coreClock: 1.847GHz coreCount: 20 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 298.32GiB/s
2020-07-10 20:36:25.299515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties: 
pciBusID: 0000:02:00.0 name: GeForce GTX 1080 computeCapability: 6.1
coreClock: 1.847GHz coreCount: 20 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 298.32GiB/s
2020-07-10 20:36:25.299855: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-07-10 20:36:25.330094: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-07-10 20:36:25.345517: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-07-10 20:36:25.375238: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-07-10 20:36:25.392428: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-07-10 20:36:25.412082: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-07-10 20:36:25.430892: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-07-10 20:36:25.432276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1
2020-07-10 20:36:25.432747: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2020-07-10 20:36:25.439470: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x18e19723790 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-07-10 20:36:25.439669: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-07-10 20:36:25.594336: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1080 computeCapability: 6.1
coreClock: 1.847GHz coreCount: 20 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 298.32GiB/s
2020-07-10 20:36:25.594839: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties: 
pciBusID: 0000:02:00.0 name: GeForce GTX 1080 computeCapability: 6.1
coreClock: 1.847GHz coreCount: 20 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 298.32GiB/s
2020-07-10 20:36:25.595149: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-07-10 20:36:25.595311: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-07-10 20:36:25.595476: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-07-10 20:36:25.595637: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-07-10 20:36:25.595793: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-07-10 20:36:25.595953: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-07-10 20:36:25.596121: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-07-10 20:36:25.596974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1
2020-07-10 20:36:26.501843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-10 20:36:26.502016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 1 
2020-07-10 20:36:26.502118: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N N 
2020-07-10 20:36:26.502217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 1:   N N 
2020-07-10 20:36:26.503174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6280 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
2020-07-10 20:36:26.504471: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 6280 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080, pci bus id: 0000:02:00.0, compute capability: 6.1)
2020-07-10 20:36:26.507161: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x18e8a13a5e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-07-10 20:36:26.507331: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080, Compute Capability 6.1
2020-07-10 20:36:26.507460: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): GeForce GTX 1080, Compute Capability 6.1
result_without_py_function = b'\nR\n\x11\n\x08feature0\x12\x05\x1a\x03\n\x01\x00\n\x11\n\x08feature1\x12\x05\x1a\x03\n\x01\x04\n\x14\n\x08feature2\x12\x08\n\x06\n\x04goat\n\x14\n\x08feature3\x12\x08\x12\x06\n\x04[\xd3|?'

2020-07-10 20:36:26.534597: W tensorflow/core/framework/op_kernel.cc:1741] Invalid argument: TypeError: <tf.Tensor: shape=(), dtype=bool, numpy=False> has type <class 'tensorflow.python.framework.ops.EagerTensor'>, but expected one of: (<class 'int'>,)
Traceback (most recent call last):

  File ""C:\Program Files\Python38\lib\site-packages\tensorflow\python\ops\gen_script_ops.py"", line 43, in eager_py_func
    _result = pywrap_tfe.TFE_Py_FastPathExecute(

tensorflow.python.eager.core._FallbackException: This function does not handle the case of the path where all inputs are not already EagerTensors.


During handling of the above exception, another exception occurred:


Traceback (most recent call last):

  File ""C:\Program Files\Python38\lib\site-packages\tensorflow\python\ops\script_ops.py"", line 241, in __call__
    return func(device, token, args)

  File ""C:\Program Files\Python38\lib\site-packages\tensorflow\python\ops\script_ops.py"", line 130, in __call__
    ret = self._func(*args)

  File ""C:\Program Files\Python38\lib\site-packages\tensorflow\python\autograph\impl\api.py"", line 309, in wrapper
    return func(*args, **kwargs)

  File ""F:/python_projects/deep_learning/workspace/programs/dlcls/tf_py_function_test.py"", line 20, in serialize_example
    'feature0': _int64_feature(feature0),

  File ""F:/python_projects/deep_learning/workspace/programs/dlcls/tf_py_function_test.py"", line 15, in _int64_feature
    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))

  File ""C:\Program Files\Python38\lib\site-packages\google\protobuf\internal\python_message.py"", line 542, in init
    copy.extend(field_value)

  File ""C:\Program Files\Python38\lib\site-packages\google\protobuf\internal\containers.py"", line 282, in extend
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]

  File ""C:\Program Files\Python38\lib\site-packages\google\protobuf\internal\containers.py"", line 282, in <listcomp>
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]

  File ""C:\Program Files\Python38\lib\site-packages\google\protobuf\internal\type_checkers.py"", line 171, in CheckValue
    raise TypeError(message)

TypeError: <tf.Tensor: shape=(), dtype=bool, numpy=False> has type <class 'tensorflow.python.framework.ops.EagerTensor'>, but expected one of: (<class 'int'>,)


Traceback (most recent call last):
  File ""C:\Program Files\Python38\lib\site-packages\tensorflow\python\ops\gen_script_ops.py"", line 43, in eager_py_func
    _result = pywrap_tfe.TFE_Py_FastPathExecute(
tensorflow.python.eager.core._FallbackException: This function does not handle the case of the path where all inputs are not already EagerTensors.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""F:/python_projects/deep_learning/workspace/programs/dlcls/tf_py_function_test.py"", line 40, in <module>
    result_with_py_function = tf_serialize_example(False, 4, b'goat', 0.9876)
  File ""F:/python_projects/deep_learning/workspace/programs/dlcls/tf_py_function_test.py"", line 30, in tf_serialize_example
    tf_string = tf.py_function(
  File ""C:\Program Files\Python38\lib\site-packages\tensorflow\python\ops\script_ops.py"", line 454, in eager_py_func
    return _internal_py_func(
  File ""C:\Program Files\Python38\lib\site-packages\tensorflow\python\ops\script_ops.py"", line 336, in _internal_py_func
    result = gen_script_ops.eager_py_func(
  File ""C:\Program Files\Python38\lib\site-packages\tensorflow\python\ops\gen_script_ops.py"", line 50, in eager_py_func
    return eager_py_func_eager_fallback(
  File ""C:\Program Files\Python38\lib\site-packages\tensorflow\python\ops\gen_script_ops.py"", line 99, in eager_py_func_eager_fallback
    _result = _execute.execute(b""EagerPyFunc"", len(Tout), inputs=_inputs_flat,
  File ""C:\Program Files\Python38\lib\site-packages\tensorflow\python\eager\execute.py"", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError: TypeError: <tf.Tensor: shape=(), dtype=bool, numpy=False> has type <class 'tensorflow.python.framework.ops.EagerTensor'>, but expected one of: (<class 'int'>,)
Traceback (most recent call last):

  File ""C:\Program Files\Python38\lib\site-packages\tensorflow\python\ops\gen_script_ops.py"", line 43, in eager_py_func
    _result = pywrap_tfe.TFE_Py_FastPathExecute(

tensorflow.python.eager.core._FallbackException: This function does not handle the case of the path where all inputs are not already EagerTensors.


During handling of the above exception, another exception occurred:


Traceback (most recent call last):

  File ""C:\Program Files\Python38\lib\site-packages\tensorflow\python\ops\script_ops.py"", line 241, in __call__
    return func(device, token, args)

  File ""C:\Program Files\Python38\lib\site-packages\tensorflow\python\ops\script_ops.py"", line 130, in __call__
    ret = self._func(*args)

  File ""C:\Program Files\Python38\lib\site-packages\tensorflow\python\autograph\impl\api.py"", line 309, in wrapper
    return func(*args, **kwargs)

  File ""F:/python_projects/deep_learning/workspace/programs/dlcls/tf_py_function_test.py"", line 20, in serialize_example
    'feature0': _int64_feature(feature0),

  File ""F:/python_projects/deep_learning/workspace/programs/dlcls/tf_py_function_test.py"", line 15, in _int64_feature
    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))

  File ""C:\Program Files\Python38\lib\site-packages\google\protobuf\internal\python_message.py"", line 542, in init
    copy.extend(field_value)

  File ""C:\Program Files\Python38\lib\site-packages\google\protobuf\internal\containers.py"", line 282, in extend
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]

  File ""C:\Program Files\Python38\lib\site-packages\google\protobuf\internal\containers.py"", line 282, in <listcomp>
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]

  File ""C:\Program Files\Python38\lib\site-packages\google\protobuf\internal\type_checkers.py"", line 171, in CheckValue
    raise TypeError(message)

TypeError: <tf.Tensor: shape=(), dtype=bool, numpy=False> has type <class 'tensorflow.python.framework.ops.EagerTensor'>, but expected one of: (<class 'int'>,)

 [Op:EagerPyFunc]

Process finished with exit code 1

"
41301,Resnet weights link ,"Where can I find the Resnet50 pretrained weights to download ?

Thanks"
41300,Confusing documentation for tf.image.rgb_to_yuv,"The documentation for tf.image.rgb_to_yuv says ""Outputs a tensor of the same shape as the images tensor, containing the YUV value of the pixels. The output is only well defined if the value in images are in [0,1]."" Does that mean the RGB values should be [0,1]?

If so, the usage example added confusion:
```
x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.rgb_to_yuv(x)
```

Clearly, x does not lie in [0,1]
"
41299,How to disable mkl_dnn when compile TensorFlow 1.15?,"Compile TensorFlow 1.15 from source code will use more than 1.5 hours, and during compile I find mkl_cudnn is compiled by default.  What I need is TensorFlow 1.15 with CUDA support, how to reduce the compile time?"
41298,"tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [3], [batch]: [5]","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 2.2 
- Python version: 3.7.3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0
- GPU model and memory: titan xp 

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.




getting error  

`tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [3], [batch]: [5]`

```
def train():
    print('train')
    print(tf.__version__)
    batch_size= 2
    images,boxes,labels,difficulties,new_boxes= PascalVOCDataset()
    new_boxes = list(new_boxes)


    boxes = tf.ragged.constant(boxes)
    labels = tf.ragged.constant(labels)
    new_boxes = tf.ragged.constant(new_boxes)

    print('boxes->',boxes.shape)
    print('labels->',labels.shape)
    print('images->', np.array(images).shape)

    dataset = tf.data.Dataset.from_tensor_slices((images,new_boxes,labels))
    run_train(dataset.map(resize_image_bbox, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE))

def main():
    train()
if __name__ =='__main__':
    main()
boxes-> (5717, None, None)
labels-> (5717, None)
images-> (5717,)
===============================================resize_image_bbox
new_image-> (300, 300, 3)
new_boxes-> (None, None)
labels-> (None,)
run_train
dataset-> <PrefetchDataset shapes: ((None, 300, 300, 3), (None, None, None), (None, None)), types: (tf.float32, tf.float32, tf.int32)>
=========================================================
image_shape-> (2, 300, 300, 3)
labels_shape-> (2, 2)
boxes_shape-> (2,)
```

when I change the batch_size 1-> any number it gives this error 

```
Traceback (most recent call last):
  File ""/home/jake/Gits/ssd_tensorflow/train.py"", line 64, in <module>
    main()
  File ""/home/jake/Gits/ssd_tensorflow/train.py"", line 62, in main
    train()
  File ""/home/jake/Gits/ssd_tensorflow/train.py"", line 59, in train
    run_train(dataset.map(resize_image_bbox, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE))
  File ""/home/jake/Gits/ssd_tensorflow/train.py"", line 26, in run_train
    for idx,(images,boxes,labels) in enumerate(dataset): # (batch_size (N), 300, 300, 3)
  File ""/home/jake/venv/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 631, in __next__
    return self.next()
  File ""/home/jake/venv/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 670, in next
    return self._next_internal()
  File ""/home/jake/venv/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 661, in _next_internal
    return structure.from_compatible_tensor_list(self._element_spec, ret)
  File ""/usr/lib/python3.7/contextlib.py"", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/home/jake/venv/lib/python3.7/site-packages/tensorflow/python/eager/context.py"", line 1989, in execution_mode
    executor_new.wait()
  File ""/home/jake/venv/lib/python3.7/site-packages/tensorflow/python/eager/executor.py"", line 67, in wait
    pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [3], [batch]: [5]

Process finished with exit code 1
```

"
41294,"Let Attention layer accept a layer mask of list of length 3, namely [query_mask, value_mask, key_mask]","<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (2.2.0):
- Are you willing to contribute it (Yes):



**Describe the feature and the current behavior/state.**
for tf.keras.layers.Attention currently the layer mask must be a list of length 2, namely [query_mask, value_mask]
change it to also accept a layer mask of list of length 3, namely [query_mask, value_mask, key_mask]

currently def compute_mask() returns the query_mask and ignores the value_mask. If passed a key_mask it could also just be ignored

**Will this change the current api? How?**
No
"
41292,Can't compile the C code in smart_reply,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Archlinux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: irrelevant
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 3.0.0
- GCC/Compiler version (if compiling from source): 10.1.0
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I am trying to compile the C code in smart_reply from source. I was able to compile and run the app with the pre-compiled binaries, but I want to modify the C code.
I think I am missing BUILD file in the root folder (same as WORKSPACE) because if I add an empty BUILD file, I get a different error.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.


~/tflite/examples/lite/examples/smart_reply/android/app$ ./bazelisk-linux-amd64 build libs/cc:smartreply_runtime_aar
ERROR: /home/mini/workspace/tflite/examples/lite/examples/smart_reply/android/app/libs/cc/BUILD:159:1: no such package '': BUILD file not found in any of the following directories. Add a BUILD file to a directory to mark it as a package.    - /home/mini/workspace/tflite/examples/lite/examples/smart_reply/android/app and referenced by '//libs/cc:smartreply_runtime_aar'
ERROR: Analysis of target '//libs/cc:smartreply_runtime_aar' failed; build aborted: no such package '': BUILD file not found in any of the following directories. Add a BUILD file to a directory to mark it as a package.                       - /home/mini/workspace/tflite/examples/lite/examples/smart_reply/android/app
INFO: Elapsed time: 0.170s
INFO: 0 processes.                                                                                                                                                                                                                              FAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)   


~/tflite/examples/lite/examples/smart_reply/android/app$ touch BUILD
~/tflite/examples/lite/examples/smart_reply/android/app$ ./bazelisk-linux-amd64 build libs/cc:smartreply_runtime_aar
WARNING: The major revision of the Android NDK referenced by android_ndk_repository rule 'androidndk' is 21. The major revisions supported by Bazel are [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]. Bazel will attempt to treat the NDK as if it was r20. This may cause compilation and linkage problems. Please download a supported NDK version.
WARNING: API level 30 specified by android_ndk_repository 'androidndk' is not available. Using latest known API level 28                                                                                                                        ERROR: /home/mini/workspace/tflite/examples/lite/examples/smart_reply/android/app/libs/cc/BUILD:159:1: no such target '//:LICENSE': target 'LICENSE' not declared in package '' defined by /home/mini/workspace/tflite/examples/lite/examples/smart_reply/android/app/BUILD and referenced by '//libs/cc:smartreply_runtime_aar'
ERROR: Analysis of target '//libs/cc:smartreply_runtime_aar' failed; build aborted: Analysis failed
INFO: Elapsed time: 5.450s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (93 packages loaded, 10616 targets configured)   "
41291,tf.keras.model.evaluate() not working as expected,"
**System information**
Code is running locally on:

MacBook Pro
2.8 GHz Quad-Core Intel Core i7
Radeon Pro 555 2 GB
Intel HD Graphics 630 1536 MB

**current behavior**


In Jupyter

`
import tensorflow as tf

mnist = tf.keras.datasets.fashion_mnist
(training_images, training_labels), (test_images, test_labels) = mnist.load_data()
training_images = training_images / 255.0
test_images = test_images / 255.0
model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation=tf.nn.relu),
    tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(training_images, training_labels, epochs=5)

test_loss = model.evaluate(test_images, test_labels)
`

The output for the evaluate method  is currently ""10000/1"" followed by thousands of ""=="". I also believe the fraction should be ""10000/10000""

**expected behavior**

something like: ""10000/10000 [==========]""


** code to reproduce**

pasted above, exactly as is in a jupyter notebook 
"
41290,Model.fit(): class_weight leads to error with distributed Dataset ,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): effectively tf-nightly 20200710
- Python version:
- Bazel version (if compiling from source):


**Describe the current behavior**

When I call tf.keras.Model.fit() with a distributed dataset, one obtained from 
`strategy.experimental_distribute_dataset()`, in conjunction with class_weight, 
I get the following error:

```
    dataset = dataset.map(_make_class_weight_map_fn(class_weight))
AttributeError: 'DistributedDataset' object has no attribute 'map'
```

The model fit code looks like:

```py
    model.fit(
      train_ds,
      epochs=FLAGS.epochs,
      steps_per_epoch=train_steps_per_epoch,
      validation_data=val_ds,
      validation_steps=val_steps_per_epoch,
      class_weight={
          0: 1.0,
          1: 5.0,
      },
      callbacks=[
          tf.keras.callbacks.TensorBoard(FLAGS.logdir, profile_batch=50)
      ])
```

However, if I don't call `strategy.experimental_distribute_dataset()` on train_ds
and val_ds, the fit() works correctly.

The `strategy` here is simpley a OneDeviceStrategy with my GPU.

**Describe the expected behavior**

The expected behavior is that class_weight should work regardless of 
whether the Dataset is distributed or not.
"
41289,"Expecting float value for attr dropout, got int  in gen_cudnn_rnn_ops.py","I was trying to build a chatbot on Google Colab. The training model worked fine. However, when I tried to run the inference model, I got the following error. 

```
_FallbackException                        Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_cudnn_rnn_ops.py in cudnn_rnnv3(input, input_h, input_c, params, sequence_lengths, rnn_mode, input_mode, direction, dropout, seed, seed2, num_proj, is_training, time_major, name)
   1895         ""num_proj"", num_proj, ""is_training"", is_training, ""time_major"",
-> 1896         time_major)
   1897       _result = _CudnnRNNV3Output._make(_result)

_FallbackException: Expecting float value for attr dropout, got int

During handling of the above exception, another exception occurred:

UnknownError                              Traceback (most recent call last)

```

Here's the google colab link
https://drive.google.com/file/d/17qBe-pN68_HeGNAEyobDW-Ph34EEjsO1/view?usp=sharing

It follow the script that raised the Exception and it seems to me that the error was from inside the script. I tried to fix it by setting my LSTM layer dropout parameter but it still didn't work."
41288,ValueError: Could not find matching function to call loaded from the SavedModel,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2.0
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Standalone code to reproduce the issue**

I am getting the error from the title when I try to predict from saved model/

I am providing the code to reproduce the problem

```
##### IMPORT ANDR PREPARE DATA #######


import pandas as pd
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow import keras
import numpy as np

url = 'https://raw.githubusercontent.com/MislavSag/trademl/master/trademl/modeling/random_forest/X_TEST.csv'
X_TEST = pd.read_csv(url, sep=',')
url = 'https://raw.githubusercontent.com/MislavSag/trademl/master/trademl/modeling/random_forest/labeling_info_TEST.csv'
labeling_info_TEST = pd.read_csv(url, sep=',')


# TRAIN TEST SPLIT
X_train, X_test, y_train, y_test = train_test_split(
    X_TEST.drop(columns=['close_orig']), labeling_info_TEST['bin'],
    test_size=0.10, shuffle=False, stratify=None)


### PREPARE LSTM
x = X_train['close'].values.reshape(-1, 1)
y = y_train.values.reshape(-1, 1)
x_test = X_test['close'].values.reshape(-1, 1)
y_test = y_test.values.reshape(-1, 1)
train_val_index_split = 0.75
train_generator = keras.preprocessing.sequence.TimeseriesGenerator(
    data=x,
    targets=y,
    length=30,
    sampling_rate=1,
    stride=1,
    start_index=0,
    end_index=int(train_val_index_split*X_TEST.shape[0]),
    shuffle=False,
    reverse=False,
    batch_size=128
)
validation_generator = keras.preprocessing.sequence.TimeseriesGenerator(
    data=x,
    targets=y,
    length=30,
    sampling_rate=1,
    stride=1,
    start_index=int((train_val_index_split*X_TEST.shape[0] + 1)),
    end_index=None,  #int(train_test_index_split*X.shape[0])
    shuffle=False,
    reverse=False,
    batch_size=128
)
test_generator = keras.preprocessing.sequence.TimeseriesGenerator(
    data=x_test,
    targets=y_test,
    length=30,
    sampling_rate=1,
    stride=1,
    start_index=0,
    end_index=None,
    shuffle=False,
    reverse=False,
    batch_size=128
)

# convert generator to inmemory 3D series (if enough RAM)
def generator_to_obj(generator):
    xlist = []
    ylist = []
    for i in range(len(generator)):
        x, y = train_generator[i]
        xlist.append(x)
        ylist.append(y)
    X_train = np.concatenate(xlist, axis=0)
    y_train = np.concatenate(ylist, axis=0)
    return X_train, y_train

X_train_lstm, y_train_lstm = generator_to_obj(train_generator)
X_val_lstm, y_val_lstm = generator_to_obj(validation_generator)
X_test_lstm, y_test_lstm = generator_to_obj(test_generator)

# test for shapes
print('X and y shape train: ', X_train_lstm.shape, y_train_lstm.shape)
print('X and y shape validate: ', X_val_lstm.shape, y_val_lstm.shape)
print('X and y shape test: ', X_test_lstm.shape, y_test_lstm.shape)


##### TRAIN  MODEL #######


model = keras.models.Sequential([
        keras.layers.LSTM(258, return_sequences=True, input_shape=[None, x.shape[1]]),
        
        keras.layers.LSTM(124, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),
        keras.layers.LSTM(32, dropout=0.2, recurrent_dropout=0.2),
        keras.layers.Dense(1, activation='sigmoid')
        
])
model.compile(loss='binary_crossentropy', optimizer='adam',
              metrics=['accuracy', 
                       keras.metrics.AUC(),
                       keras.metrics.Precision(),
                       keras.metrics.Recall()])
# fit the model
history = model.fit(X_train_lstm, y_train_lstm, epochs=5, batch_size=128,
                    validation_data=(X_val_lstm, y_val_lstm))



##### SAVE AND LOAD MODEL (WORKS) #######

model.save('my_model_lstm.h5')
model = keras.models.load_model('my_model_lstm.h5')
model.predict(X_test_lstm)

##### SAVE AND LOAD MODEL (DOESNT WORK) #######


model_version = ""0001""
model_name = ""lstm_cloud""
model_path = os.path.join(model_name, model_version)
tf.saved_model.save(model, model_path)

saved_model = tf.saved_model.load(model_path)
y_pred = saved_model(X_test_lstm, training=False)
```



**Other info / logs** Include any logs or source code that would be helpful to
I am geeting this error:

```
y_pred = saved_model(X_test_lstm, training=False)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
c:\Users\Mislav\Documents\GitHub\trademl\trademl\modeling\train_nn.py in 
----> 525 y_pred = saved_model(X_test_lstm, training=False)

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\saved_model\load.py in _call_attribute(instance, *args, **kwargs)
    484 
    485 def _call_attribute(instance, *args, **kwargs):
--> 486   return instance.__call__(*args, **kwargs)
    487 
    488 

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\eager\def_function.py in __call__(self, *args, **kwds)
    578         xla_context.Exit()
    579     else:
--> 580       result = self._call(*args, **kwds)
    581 
    582     if tracing_count == self._get_tracing_count():

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\eager\def_function.py in _call(self, *args, **kwds)
    625       # This is the first call of __call__, so we have to initialize.
    626       initializers = []
--> 627       self._initialize(args, kwds, add_initializers_to=initializers)
    628     finally:
    629       # At this point we know that the initialization is complete (or less

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\eager\def_function.py in _initialize(self, args, kwds, add_initializers_to)
    504     self._concrete_stateful_fn = (
    505         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--> 506             *args, **kwds))
    507 
    508     def invalid_creator_scope(*unused_args, **unused_kwds):

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\eager\function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   2444       args, kwargs = None, None
   2445     with self._lock:
-> 2446       graph_function, _, _ = self._maybe_define_function(args, kwargs)
   2447     return graph_function
   2448 

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\eager\function.py in _maybe_define_function(self, args, kwargs)
   2775 
   2776       self._function_cache.missed.add(call_context_key)
-> 2777       graph_function = self._create_graph_function(args, kwargs)
   2778       self._function_cache.primary[cache_key] = graph_function
   2779       return graph_function, args, kwargs

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\eager\function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   2665             arg_names=arg_names,
   2666             override_flat_arg_shapes=override_flat_arg_shapes,
-> 2667             capture_by_value=self._capture_by_value),
   2668         self._function_attributes,
   2669         # Tell the ConcreteFunction to clean up its graph once it goes out of

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    979         _, original_func = tf_decorator.unwrap(python_func)
    980 
--> 981       func_outputs = python_func(*func_args, **func_kwargs)
    982 
    983       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\eager\def_function.py in wrapped_fn(*args, **kwds)
    439         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    440         # the function a weak reference to itself to avoid a reference cycle.
--> 441         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    442     weak_wrapped_fn = weakref.ref(wrapped_fn)
    443 

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\saved_model\function_deserialization.py in restored_function_body(*args, **kwargs)
    259         .format(_pretty_format_positional(args), kwargs,
    260                 len(saved_function.concrete_functions),
--> 261                 ""\n\n"".join(signature_descriptions)))
    262 
    263   concrete_function_objects = []

ValueError: Could not find matching function to call loaded from the SavedModel. Got:
  Positional arguments (3 total):
    * Tensor(""inputs:0"", shape=(256, 30, 1), dtype=float64)
    * False
    * None
  Keyword arguments: {}

Expected these arguments to match one of the following 4 option(s):

Option 1:
  Positional arguments (3 total):
    * TensorSpec(shape=(None, None, 1), dtype=tf.float32, name='inputs')
    * False
    * None
  Keyword arguments: {}

Option 2:
  Positional arguments (3 total):
    * TensorSpec(shape=(None, None, 1), dtype=tf.float32, name='inputs')
    * True
    * None
  Keyword arguments: {}

Option 3:
  Positional arguments (3 total):
    * TensorSpec(shape=(None, None, 1), dtype=tf.float32, name='lstm_6_input')
    * True
    * None
  Keyword arguments: {}

Option 4:
  Positional arguments (3 total):
    * TensorSpec(shape=(None, None, 1), dtype=tf.float32, name='lstm_6_input')
    * False
    * None
  Keyword arguments: {}
```"
41287,Compile failure in TF 2.3 rc0 on python 3.5,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 16.04**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version: **2.3**
- Python version: **3.5**
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): **3.1.0**
- GCC/Compiler version (if compiling from source): **gcc version: 8.2.1**
- CUDA/cuDNN version:
- GPU model and memory:



**I am trying to build Tensorflow 2.3 on python 3.5 and getting following compile errors:
ERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_google_protobuf/BUILD:795:1: C++ compilation of rule '@com_google_protobuf//:python/google/protobuf/internal/_api_implementation.so' failed (Exit 1)
In file included from bazel-out/k8-opt/bin/external/local_config_python/python_include/Python.h:8,
                 from external/com_google_protobuf/python/google/protobuf/internal/api_implementation.cc:31:
bazel-out/k8-opt/bin/external/local_config_python/python_include/pyconfig.h:3:12: fatal error: x86_64-linux-gnu/python3.5m/pyconfig.h: No such file or directory
 #  include <x86_64-linux-gnu/python3.5m/pyconfig.h>
            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.
**

**I ran with the following bazel options:
'build' options: --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 --copt=-O3 --copt=-Wformat --copt=-Wformat-security --copt=-fstack-protector --copt=-fPIC --copt=-fpic --linkopt=-znoexecstack --linkopt=-zrelro --linkopt=-znow --linkopt=-fstack-protector --config=mkl --copt=-march=sandybridge
**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
41286,model.fit(...) throws NotImplementedError under tf.distribute.MirroredStrategy,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: ---
- TensorFlow installed from (source or binary): Pip
- TensorFlow version (use command below): 2.1.0
- Python version: 3.7.5
- Bazel version (if compiling from source): ---
- GCC/Compiler version (if compiling from source): ---
- CUDA/cuDNN version: 10.1
- GPU model and memory: U-Net

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
I am trying to train my U-Net model using `tf.distribute.MirroredStrategy()` on my dataset but I get `NotImplementedError` without any details about the error from stack trace, and this error does not happen when the same code is executed without using the distributed strategy.
This problem happens whether using GPU or CPU and keeps as long as I use this strategy.
Full stack trace is linked [here](https://gist.github.com/orwa-te/e0d92cc83d2ddd1d3a6be075a2161ef8)
**Describe the expected behavior**

**Standalone code to reproduce the issue**
Here is my code [link](https://gist.github.com/orwa-te/4887067323fb32bc5c2e62e29d4afc81)
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
41284,Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
Ubuntu 18.04
- TensorFlow version (you are using): 2.3.0-r0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**

Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.

**Will this change the current api? How?**No

**Who will benefit with this feature?** networks with dynamic tensors.

**Any Other info.**
"
41283,Problem TensorFlow on Jupyter,"Hi! I had the following error running MaskRCNN. I have macos Yosemite 10.10.5. I'm using conda virtual environment, Tensorflow version 2.0.1.

ERROR:root:Internal Python error in the inspect module.
Below is the traceback from this internal error.

ERROR:root:Internal Python error in the inspect module.
Below is the traceback from this internal error.

ERROR:root:Internal Python error in the inspect module.
Below is the traceback from this internal error.

Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: dlopen(/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation
  Referenced from: /opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib
  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security
 in /opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3343, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-1-ebe7095df7bb>"", line 15, in <module>
    from mrcnn import utils
  File ""/Users/Maria/Documents/GitHub/Mask_RCNN/mrcnn/utils.py"", line 16, in <module>
    import tensorflow as tf
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/__init__.py"", line 98, in <module>
    from tensorflow_core import *
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: dlopen(/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation
  Referenced from: /opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib
  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security
 in /opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2044, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'ImportError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: dlopen(/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation
  Referenced from: /opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib
  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security
 in /opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/ultratb.py"", line 1169, in get_records
    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/ultratb.py"", line 316, in wrapped
    return f(*args, **kwargs)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/ultratb.py"", line 350, in _fixed_getinnerframes
    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/inspect.py"", line 1490, in getinnerframes
    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/inspect.py"", line 1448, in getframeinfo
    filename = getsourcefile(frame) or getfile(frame)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/inspect.py"", line 696, in getsourcefile
    if getattr(getmodule(object, filename), '__loader__', None) is not None:
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/inspect.py"", line 733, in getmodule
    if ismodule(module) and hasattr(module, '__file__'):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 941, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 665, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 678, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 42, in <module>
    from . _api.v2 import audio
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/_api/v2/audio/__init__.py"", line 10, in <module>
    from tensorflow.python.ops.gen_audio_ops import decode_wav
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_audio_ops.py"", line 10, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: dlopen(/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation
  Referenced from: /opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib
  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security
 in /opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3343, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-1-ebe7095df7bb>"", line 15, in <module>
    from mrcnn import utils
  File ""/Users/Maria/Documents/GitHub/Mask_RCNN/mrcnn/utils.py"", line 16, in <module>
    import tensorflow as tf
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/__init__.py"", line 98, in <module>
    from tensorflow_core import *
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: dlopen(/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation
  Referenced from: /opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib
  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security
 in /opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2044, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'ImportError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: dlopen(/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation
  Referenced from: /opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib
  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security
 in /opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: dlopen(/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation
  Referenced from: /opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib
  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security
 in /opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3343, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-1-ebe7095df7bb>"", line 15, in <module>
    from mrcnn import utils
  File ""/Users/Maria/Documents/GitHub/Mask_RCNN/mrcnn/utils.py"", line 16, in <module>
    import tensorflow as tf
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/__init__.py"", line 98, in <module>
    from tensorflow_core import *
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: dlopen(/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation
  Referenced from: /opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib
  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security
 in /opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2044, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'ImportError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3263, in run_ast_nodes
    if (await self.run_code(code, result,  async_=asy)):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3360, in run_code
    self.showtraceback(running_compiled_code=True)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2047, in showtraceback
    value, tb, tb_offset=tb_offset)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/ultratb.py"", line 1436, in structured_traceback
    self, etype, value, tb, tb_offset, number_of_lines_of_context)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/ultratb.py"", line 1336, in structured_traceback
    self, etype, value, tb, tb_offset, number_of_lines_of_context
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/ultratb.py"", line 1193, in structured_traceback
    tb_offset)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/ultratb.py"", line 1150, in format_exception_as_a_whole
    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/ultratb.py"", line 451, in find_recursion
    return len(records), 0
TypeError: object of type 'NoneType' has no len()

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2044, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'TypeError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: dlopen(/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation
  Referenced from: /opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib
  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security
 in /opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/ultratb.py"", line 1169, in get_records
    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/ultratb.py"", line 316, in wrapped
    return f(*args, **kwargs)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/ultratb.py"", line 350, in _fixed_getinnerframes
    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/inspect.py"", line 1490, in getinnerframes
    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/inspect.py"", line 1448, in getframeinfo
    filename = getsourcefile(frame) or getfile(frame)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/inspect.py"", line 696, in getsourcefile
    if getattr(getmodule(object, filename), '__loader__', None) is not None:
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/inspect.py"", line 733, in getmodule
    if ismodule(module) and hasattr(module, '__file__'):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 941, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 665, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 678, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 42, in <module>
    from . _api.v2 import audio
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/_api/v2/audio/__init__.py"", line 10, in <module>
    from tensorflow.python.ops.gen_audio_ops import decode_wav
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_audio_ops.py"", line 10, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: dlopen(/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation
  Referenced from: /opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib
  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security
 in /opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3343, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-1-ebe7095df7bb>"", line 15, in <module>
    from mrcnn import utils
  File ""/Users/Maria/Documents/GitHub/Mask_RCNN/mrcnn/utils.py"", line 16, in <module>
    import tensorflow as tf
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/__init__.py"", line 98, in <module>
    from tensorflow_core import *
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: dlopen(/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation
  Referenced from: /opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib
  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security
 in /opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2044, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'ImportError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3263, in run_ast_nodes
    if (await self.run_code(code, result,  async_=asy)):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3360, in run_code
    self.showtraceback(running_compiled_code=True)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2047, in showtraceback
    value, tb, tb_offset=tb_offset)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/ultratb.py"", line 1436, in structured_traceback
    self, etype, value, tb, tb_offset, number_of_lines_of_context)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/ultratb.py"", line 1336, in structured_traceback
    self, etype, value, tb, tb_offset, number_of_lines_of_context
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/ultratb.py"", line 1193, in structured_traceback
    tb_offset)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/ultratb.py"", line 1150, in format_exception_as_a_whole
    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/ultratb.py"", line 451, in find_recursion
    return len(records), 0
TypeError: object of type 'NoneType' has no len()

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2044, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'TypeError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: dlopen(/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation
  Referenced from: /opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib
  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security
 in /opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: dlopen(/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation
  Referenced from: /opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib
  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security
 in /opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3343, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-1-ebe7095df7bb>"", line 15, in <module>
    from mrcnn import utils
  File ""/Users/Maria/Documents/GitHub/Mask_RCNN/mrcnn/utils.py"", line 16, in <module>
    import tensorflow as tf
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/__init__.py"", line 98, in <module>
    from tensorflow_core import *
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: dlopen(/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation
  Referenced from: /opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib
  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security
 in /opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2044, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'ImportError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3263, in run_ast_nodes
    if (await self.run_code(code, result,  async_=asy)):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3360, in run_code
    self.showtraceback(running_compiled_code=True)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2047, in showtraceback
    value, tb, tb_offset=tb_offset)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/ultratb.py"", line 1436, in structured_traceback
    self, etype, value, tb, tb_offset, number_of_lines_of_context)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/ultratb.py"", line 1336, in structured_traceback
    self, etype, value, tb, tb_offset, number_of_lines_of_context
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/ultratb.py"", line 1193, in structured_traceback
    tb_offset)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/ultratb.py"", line 1150, in format_exception_as_a_whole
    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/ultratb.py"", line 451, in find_recursion
    return len(records), 0
TypeError: object of type 'NoneType' has no len()

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2044, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'TypeError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2895, in _run_cell
    return runner(coro)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/async_helpers.py"", line 68, in _pseudo_sync_runner
    coro.send(None)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3072, in run_cell_async
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3282, in run_ast_nodes
    self.showtraceback()
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2047, in showtraceback
    value, tb, tb_offset=tb_offset)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/ultratb.py"", line 1436, in structured_traceback
    self, etype, value, tb, tb_offset, number_of_lines_of_context)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/ultratb.py"", line 1336, in structured_traceback
    self, etype, value, tb, tb_offset, number_of_lines_of_context
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/ultratb.py"", line 1211, in structured_traceback
    chained_exceptions_tb_offset)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/ultratb.py"", line 1150, in format_exception_as_a_whole
    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/ultratb.py"", line 451, in find_recursion
    return len(records), 0
TypeError: object of type 'NoneType' has no len()

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2044, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'TypeError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: dlopen(/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation
  Referenced from: /opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib
  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security
 in /opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/ultratb.py"", line 1169, in get_records
    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/ultratb.py"", line 316, in wrapped
    return f(*args, **kwargs)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/ultratb.py"", line 350, in _fixed_getinnerframes
    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/inspect.py"", line 1490, in getinnerframes
    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/inspect.py"", line 1448, in getframeinfo
    filename = getsourcefile(frame) or getfile(frame)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/inspect.py"", line 696, in getsourcefile
    if getattr(getmodule(object, filename), '__loader__', None) is not None:
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/inspect.py"", line 733, in getmodule
    if ismodule(module) and hasattr(module, '__file__'):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 941, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 665, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 678, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 42, in <module>
    from . _api.v2 import audio
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/_api/v2/audio/__init__.py"", line 10, in <module>
    from tensorflow.python.ops.gen_audio_ops import decode_wav
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_audio_ops.py"", line 10, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: dlopen(/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation
  Referenced from: /opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib
  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security
 in /opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3343, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-1-ebe7095df7bb>"", line 15, in <module>
    from mrcnn import utils
  File ""/Users/Maria/Documents/GitHub/Mask_RCNN/mrcnn/utils.py"", line 16, in <module>
    import tensorflow as tf
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/__init__.py"", line 98, in <module>
    from tensorflow_core import *
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: dlopen(/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation
  Referenced from: /opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib
  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security
 in /opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2044, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'ImportError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3263, in run_ast_nodes
    if (await self.run_code(code, result,  async_=asy)):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3360, in run_code
    self.showtraceback(running_compiled_code=True)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2047, in showtraceback
    value, tb, tb_offset=tb_offset)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/ultratb.py"", line 1436, in structured_traceback
    self, etype, value, tb, tb_offset, number_of_lines_of_context)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/ultratb.py"", line 1336, in structured_traceback
    self, etype, value, tb, tb_offset, number_of_lines_of_context
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/ultratb.py"", line 1193, in structured_traceback
    tb_offset)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/ultratb.py"", line 1150, in format_exception_as_a_whole
    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/ultratb.py"", line 451, in find_recursion
    return len(records), 0
TypeError: object of type 'NoneType' has no len()

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2044, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'TypeError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2895, in _run_cell
    return runner(coro)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/async_helpers.py"", line 68, in _pseudo_sync_runner
    coro.send(None)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3072, in run_cell_async
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3282, in run_ast_nodes
    self.showtraceback()
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2047, in showtraceback
    value, tb, tb_offset=tb_offset)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/ultratb.py"", line 1436, in structured_traceback
    self, etype, value, tb, tb_offset, number_of_lines_of_context)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/ultratb.py"", line 1336, in structured_traceback
    self, etype, value, tb, tb_offset, number_of_lines_of_context
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/ultratb.py"", line 1211, in structured_traceback
    chained_exceptions_tb_offset)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/ultratb.py"", line 1150, in format_exception_as_a_whole
    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/ultratb.py"", line 451, in find_recursion
    return len(records), 0
TypeError: object of type 'NoneType' has no len()

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2044, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'TypeError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/opt/anaconda3/envs/MaskRCNN/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: dlopen(/opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation
  Referenced from: /opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib
  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security
 in /opt/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
41281,device:GPU missing from the list not accepting XLA_GPU,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.2.0
- Python version: 3.7.6
- CUDA/cuDNN version: 10.1 update 2 / 7.5
- GPU model and memory: RTX 2060, dedicated 6 GB and shared 8 GB


**Describe the current behavior**
>>> tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None)                                                                                                                                    WARNING:tensorflow:From <stdin>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.                                                            Instructions for updating:                                                                                                                                                                                         Use `tf.config.list_physical_devices('GPU')` instead.                                                                                                                                                              2020-07-10 19:37:38.026206: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2                                          2020-07-10 19:37:38.077466: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2311a9eb5d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:                    2020-07-10 19:37:38.085075: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version                                                                                   2020-07-10 19:37:38.099271: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll                                                                         2020-07-10 19:37:39.351124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:                                                                                               pciBusID: 0000:01:00.0 name: GeForce RTX 2060 computeCapability: 7.5                                                                                                                                               coreClock: 1.335GHz coreCount: 30 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 312.97GiB/s                                                                                                                     2020-07-10 19:37:39.369392: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found                                 2020-07-10 19:37:39.375813: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cublas64_10.dll'; dlerror: cublas64_10.dll not found                                   2020-07-10 19:37:39.381862: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cufft64_10.dll'; dlerror: cufft64_10.dll not found                                     2020-07-10 19:37:39.388101: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'curand64_10.dll'; dlerror: curand64_10.dll not found                                   2020-07-10 19:37:39.392995: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusolver64_10.dll'; dlerror: cusolver64_10.dll not found                               2020-07-10 19:37:39.398977: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusparse64_10.dll'; dlerror: cusparse64_10.dll not found                               2020-07-10 19:37:39.403336: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudnn64_7.dll'; dlerror: cudnn64_7.dll not found                                       2020-07-10 19:37:39.408632: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1598] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.                                                                 Skipping registering GPU devices...                                                                                                                                                                                2020-07-10 19:37:39.596047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:                                                               2020-07-10 19:37:39.599475: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0                                                                                                                        2020-07-10 19:37:39.601116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N                                                                                                                        2020-07-10 19:37:39.607957: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x23112e31a20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:                    2020-07-10 19:37:39.611013: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2060, Compute Capability 7.5                                                                False  


>>> tf.config.list_physical_devices('GPU')                                                                                                                                                                         2020-07-10 19:40:03.191437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:                                                                                               pciBusID: 0000:01:00.0 name: GeForce RTX 2060 computeCapability: 7.5                                                                                                                                               coreClock: 1.335GHz coreCount: 30 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 312.97GiB/s                                                                                                                     2020-07-10 19:40:03.220558: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found                                 2020-07-10 19:40:03.234827: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cublas64_10.dll'; dlerror: cublas64_10.dll not found                                   2020-07-10 19:40:03.244975: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cufft64_10.dll'; dlerror: cufft64_10.dll not found                                     2020-07-10 19:40:03.254710: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'curand64_10.dll'; dlerror: curand64_10.dll not found                                   2020-07-10 19:40:03.263915: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusolver64_10.dll'; dlerror: cusolver64_10.dll not found                               2020-07-10 19:40:03.274163: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusparse64_10.dll'; dlerror: cusparse64_10.dll not found                               2020-07-10 19:40:03.283131: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudnn64_7.dll'; dlerror: cudnn64_7.dll not found                                       2020-07-10 19:40:03.291143: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1598] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.                                                                 Skipping registering GPU devices...                                                                                                                                                                                [] 

>>> tf.config.list_physical_devices()                                                                                                                                                                              [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'), PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')]  

**Describe the expected behavior**
Cuda available and GPU also should be available 
**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
41280,Incorrect gradient for ctc_loss on GPU when using logit_length,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 9.12 (TF2.2 DeepLearning image on GCP)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Preinstalled
- TensorFlow version (use command below): v2.2.0-0-g2b96f36 2.2.0-dlenv
- Python version: 3.7.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: V10.1.243
- GPU model and memory: Nvidia tesla P100

**Describe the current behavior**

I have experienced inconsistencies in the computation of the gradient of `tf.nn.ctc_loss` between the CPU and GPU implementations when the `logit_length` argument contains something else than `[num_frames]*batch_size`.
Mostly I observe that the gradient relative to `logits` for the GPU implementation does not contain zeros after the end of the sequence as given by `logit_length`. Whereas this is the case for the CPU implementation which seems to work correctly.

I have noticed that the unit tests for this op do not test this case in particular (see https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/ctc_loss_op_test.py#L993).

**Standalone code to reproduce the issue**

```python
import tensorflow as tf

use_logits_lengths = True

batch_size = 8
num_labels = 27
max_labels_length = 32
max_logits_length = 128

labels = []
labels_lengths = []
logits = []
logits_lengths = []
for i in range(batch_size):
    labels_lengths.append(tf.random.uniform([], 1, max_labels_length, tf.int32))
    labels.extend(tf.random.uniform([labels_lengths[-1]], 0, num_labels-1, tf.int32))

    # I multiply label_length by 2 to make sure there are enough frames
    logits_lengths.append(tf.random.uniform([], labels_lengths[-1].numpy()*2, max_logits_length+1, tf.int32))

labels = tf.RaggedTensor.from_row_lengths(labels, labels_lengths).to_sparse()
labels_lengths = tf.concat(labels_lengths, 0)
logits = tf.random.uniform([batch_size, max_logits_length, num_labels])
logits_lengths = tf.concat(logits_lengths, 0)
logits_lengths_full = tf.constant([max_logits_length]*batch_size)

def ctc_compare_cpu_gpu(logits_lengths):
    print(""logits_lengths"", logits_lengths.numpy())

    with tf.device(""/gpu:0""):
        with tf.GradientTape() as t:
            t.watch(logits)
            gpu_loss = tf.nn.ctc_loss(labels, logits, labels_lengths, logits_lengths, logits_time_major=False, blank_index=-1)
        gpu_grad = t.gradient(gpu_loss, [logits])[0]

    with tf.device(""/cpu:0""):
        with tf.GradientTape() as t:
            t.watch(logits)
            cpu_loss = tf.nn.ctc_loss(labels, logits, labels_lengths, logits_lengths, logits_time_major=False, blank_index=-1)
        cpu_grad = t.gradient(cpu_loss, [logits])[0]

    print(""Max loss error"", tf.math.abs(gpu_loss - cpu_loss).numpy().max())
    print(""Max grad error"", tf.math.abs(gpu_grad - cpu_grad).numpy().max())
    print()
    return cpu_loss, gpu_loss, cpu_grad, gpu_grad

ctc_compare_cpu_gpu(logits_lengths_full)
ctc_compare_cpu_gpu(logits_lengths)
```
Output:
```
logits_lengths [128 128 128 128 128 128 128 128]
Max loss error 0.00012207031
Max grad error 0.00014734268

logits_lengths [ 70  86  22  74 112 121 103 123]
Max loss error 6.1035156e-05
Max grad error 0.9669469
```
"
41279,tf_doctest.py fails on import ,"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 20.04 LTS**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**
- TensorFlow installed from (source or binary): **pip package**
- TensorFlow version: **2.2.0**
- Python version: **3.8.2**
- Installed using virtualenv? pip? conda?: **pip**
- Bazel version (if compiling from source): **N/A**
- GCC/Compiler version (if compiling from source): **N/A**
- CUDA/cuDNN version: **N/A**
- GPU model and memory: **N/A**


I was testing my changes to the documentation, specifically the doctests as specified in #40619, using the `tf_doctest.py` as described in [documentation](https://www.tensorflow.org/community/contribute/docs_ref).

Unfortunately the command: `python tf_doctest.py --file=../../python/module/module.py` failed producing following error.
```
Traceback (most recent call last):
  File ""tf_doctest.py"", line 30, in <module>
    import tensorflow.compat.v2 as tf

ModuleNotFoundError: No module named 'tensorflow.compat'
```
I tried the same in container spun from devel image with similar results. In order to identify the issue I reinstalled tensorflow on my machine and for the good measure removed nightly packages. The result wasn't any better.

Please note that I only altered module.py file, as described in #40619"
41276,TypeError: Failed to convert object of type <class 'tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor'> to Tensor.,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: Yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Colab
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**: 2.4.0-dev20200709
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

### Describe the problem
I am referring to [this official Keras example](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/captcha_ocr.ipynb) that shows how to train an OCR model using the CTC loss. I am trying to extend it to the [**IAM dataset**](http://www.fki.inf.unibe.ch/DBs/iamDB/iLogin/index.php) which is rawer in terms of its quality and the images are of handwritten characters. 

I was able to construct the dataset in the way expected by the model used in the example. However, the labels, in this case, are of variable length. This is why they are getting converted to RaggedTensors. This is producing the following error - 

```
TypeError: in user code:

    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:806 train_function  *
        return step_function(self, iterator)
    <ipython-input-42-e0e4c73f8c9f>:10 call  *
        batch_len = tf.cast(tf.shape(y_true)[0], dtype=""int64"")
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper  **
        return target(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:617 shape_v2
        return shape(input, name, out_type)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper
        return target(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:644 shape
        return shape_internal(input, name, optimize=True, out_type=out_type)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:668 shape_internal
        input = ops.convert_to_tensor(input)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1525 convert_to_tensor
        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py:338 _constant_tensor_conversion_function
        return constant(v, dtype=dtype, name=name)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py:264 constant
        allow_broadcast=True)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py:282 _constant_impl
        allow_broadcast=allow_broadcast))
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:553 make_tensor_proto
        ""supported type."" % (type(values), values))

    TypeError: Failed to convert object of type <class 'tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor'> to Tensor. Contents: tf.RaggedTensor(values=tf.RaggedTensor(values=Tensor(""ocr_model_v1/Cast_1:0"", shape=(None,), dtype=float32), row_splits=Tensor(""RaggedFromVariant/RaggedTensorFromVariant:1"", shape=(None,), dtype=int64)), row_splits=Tensor(""RaggedFromVariant/RaggedTensorFromVariant:0"", shape=(None,), dtype=int64)). Consider casting elements to a supported type.
```

What I am looking for is a way to bypass this so that the ground truth labels and the predicted labels could be sent to the CTC loss. 

### Source code / logs
Here's the [Colab Notebook](https://colab.research.google.com/gist/sayakpaul/529500ca05252277f1c1f390ef37e4d3/scratchpad.ipynb). 

The commands that download the dataset are as follows:
```
$ wget http://www.fki.inf.unibe.ch/DBs/iamDB/data/words/words.tgz --user=**user** --password=**password**
$ wget http://www.fki.inf.unibe.ch/DBs/iamDB/data/ascii/words.txt --user=**user** --password=**password**
```

**_Please register here first: http://www.fki.inf.unibe.ch/DBs/iamDB/iLogin/index.php and then replace the username and password accordingly. This is a requirement._** "
41274,Real-time prediction using Keras,"THi! I would like to ask, does Keras Neural Networks do predictions in real time?
I see, that one of your parameters in prediction is array of data. But when you need to predict future data, you don't have an array of data - you have only previous data and time interval for prediction and nothing more.
As I undestand, you simply checked the prediction because to predict future data I need to give to a function predict a time interval in future, not a previous data trainX as necessary in this function predict

Here is my Neuro Network:
look_back = 3
model = Sequential()
model.add(LSTM(4, input_shape=(look_back, 1)))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(trainX, trainY, epochs=20, batch_size=1, verbose=2)
trainPredict = model.predict(trainX)

I would like to predict future values of EMA oscillator using previous values and a value in the current time moment

def create_dataset(dataset, look_back=1):
dataX, dataY = [], []
for i in range(len(dataset)-look_back-1):
a = dataset[i:(i+look_back), 0]
dataX.append(a)
dataY.append(dataset[i + look_back, 0])
return np.array(dataX), np.array(dataY)

I would like to predict future values of EMA oscillator using previous values and a value in the current time moment
data2 = pandas.read_csv('ratevalues.csv')[::-1]
data2.columns = ['date','high','low','open','close','volume','quoteVolume','weightedAverage']
dataset = pandas.DataFrame(data2.close.ewm(span=14).mean())
train_size = int(len(dataset) * 0.67)
train = dataset[0:train_size,:]
trainX, trainY = create_dataset(train, look_back)

Can you show an example of predicting in real time?"
41272,User defined color in tf.image.resize_with_crop_or_pad,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): tf 2.2.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**

Currently, the `tf.image.resize_with_crop_or_pad` ops pads with 0. It would be helpful to be able to give another value because in some application black is not the right color for padding (it messes with real data).

**Will this change the current api? How?**

Adding a kwarg padding_color Union[int, iterable] with:
 - int, a value to be used on all channels (currently 0)
 - iterable: with len(iterable) == number of channel to describe any rgb color

Example of how to achieve the expected result currently:

```
        output_tensor = tf.image.resize_with_crop_or_pad(input_tensor, 450, 450)
        padding = (1 - tf.image.resize_with_crop_or_pad(tf.ones_like(input_tensor), 450, 450)) * tf.constant(
            [255, 0, 255], dtype=tf.uint8
        )
        output_tensor += padding
```

**Who will benefit with this feature?**

Users who need to pad images with another color because black is not suitable for their dataset

**Any Other info.**
"
41271,TFLite micro: Is the flatbuffer allowed to be placed in flash in all cases?,"@tensorflow/micro

**System information**

- TensorFlow installed from (source or binary):

Source.

- Tensorflow version (commit SHA if source):

b2f092894012e9c2612cb46c332140f28d91ced2

- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):

Arduino Nano 33 BLE Sense

**Describe the problem**

We received a bug report from a user that is experiencing a hard fault when initializing their TFLite model when also adding the ArduinoBLE library on the Arduino Nano 33 BLE Sense. So far we have always placed the flatbuffer model in flash, and this has not been a problem - but when digging into this bug report I traced the error back to the first call to the `InitializeTfLiteTensorFromFlatbuffer`. Is it possible that the flatbuffer is manipulated here? When moving the flatbuffer byte array into RAM the problem disappears. It's hard to debug on my end as I haven't figured out how to connect my J-Link to this board and build with debug symbols enabled, and because the way the serial communications are handled I can't see the error log either.

**Please provide the exact sequence of commands/steps when you ran into the problem**

I've reproduced the problem with:

* Arduino Nano 33 BLE Sense
* Arduino Core for nRF528x (Mbed OS) 1.1.4
* Arduino BLE 1.1.3
* The attached library (add via **Sketch > Include library > From ZIP file**)

And the following sketch:

```cpp
#include <ArduinoBLE.h>
#include <continuous_gestures_inference.h>

BLEService soundsService(""180C"");  // User defined service

static const float features[] = {
19.9700, 8.7300, 17.5700, 19.9700, 8.7300, 17.5700, 19.9700, 9.8200, 14.0300, 15.9400, 10.0700, 14.1400, 5.1400, 10.3500, 14.0700, -3.6600, 8.7700, 13.4100, -14.0000, 4.9900, 8.9200, -19.9800, -1.5200, 5.5700, -19.9800, -1.5200, 5.5700, -19.9800, -8.7800, -7.7400, -19.9800, -8.9900, -19.5300, -19.9800, -4.7900, -6.8400, -19.9800, 1.8400, 9.2000, -15.4000, 9.3800, 15.2200, -7.5900, 9.4600, 19.9500, -7.5900, 9.4600, 19.9500, 0.3900, 14.4600, 19.9800, 16.8500, 19.5300, 19.9800, 19.9700, 18.9900, 19.9800, 19.9700, 19.8700, 19.9500, 19.9700, 19.9700, 18.2000, 19.9700, 19.9700, 16.2600, 19.9700, 19.9700, 16.2600, 19.9700, 16.1000, 17.7500, 19.9700, 8.9900, 17.6500, 15.7300, 3.6000, 15.1800, 11.4500, -0.7100, 17.0700, 7.3700, -3.4600, 19.4300, 7.3700, -3.4600, 19.9800, 1.4400, -4.7400, 19.9800, -5.8700, -4.6500, 18.4300, -17.6300, -2.4500, 13.9000, -19.9800, -2.8000, 12.0600, -19.9800, 0.3900, 6.8400, -19.9800, 5.3700, -12.8800, -19.9800, 5.3700, -12.8800, -19.9800, 6.8900, -19.8900, -19.9800, 3.8400, -13.2000, -19.9800, 0.6700, -8.7200, -19.9800, 0.3600, -8.4200, -19.9800, 0.1200, -4.4400, -19.9800, 0.1300, 5.5800, -19.9800, 0.1300, 5.5800, -14.2500, -7.5100, 19.4800, -4.9700, -7.8700, 19.9800, 7.2300, -5.0500, 19.9800, 19.8600, 5.9300, 19.9800, 19.9700, 10.6000, 19.9800, 19.9700, 7.5400, 19.9800, 19.9700, 7.5400, 19.9800, 19.9700, 10.1600, 19.9800, 19.9700, 10.1500, 19.7000, 19.9700, 6.8200, 16.7800, 19.8700, 3.9500, 15.8000, 12.7500, 0.4500, 5.5200, 6.5800, -1.7100, 12.2200, 6.5800, -1.7100, 12.2200, -0.4100, -3.4200, 11.1400, -10.6100, -0.6800, 4.3500, -19.1800, -0.6900, 1.0500, -19.9800, -2.2000, -2.9700, -19.9800, -3.8700, -12.7700, -19.9800, -3.8700, -12.7700, -19.9800, -2.6600, -15.4700, -19.9800, 0.3700, -5.8800, -19.9800, 5.4000, 3.2500, -18.2600, 9.8800, 11.9400, -11.9500, 9.2100, 19.5900, -5.1000, 11.2400, 19.9800, -5.1000, 11.2400, 19.9800, 8.0700, 15.0100, 19.9800, 18.6900, 16.8800, 19.9800, 19.9700, 15.9200, 19.9800, 19.9700, 15.8600, 19.9800, 19.9700, 16.9700, 19.9800, 19.9700, 16.3800, 19.9800, 19.9700, 16.3800, 19.9800, 19.9700, 14.5000, 19.9800, 19.9700, 11.1800, 19.9800, 17.8900, 5.6400, 19.9800, 10.8300, 1.3800, 19.4300, 4.3300, 2.1100, 19.6200, -4.2300, 4.2900, 17.2100, -4.2300, 4.2900, 17.2100, -13.5500, 5.6800, 16.4400, -19.9800, 3.3700, 10.1400, -19.9800, 3.0800, 0.5100, -19.9800, 4.1200, -17.0600, -19.9800, 8.4400, -19.9700, -19.9800, 10.6300, -19.6100, -19.9800, 10.6300, -19.6100, -19.9800, 5.5000, -9.4300, -19.9800, 1.9500, -3.7400, -19.9800, -1.8000, 4.2200, -19.9800, -4.5600, 13.1500, -19.1300, -4.0500, 19.9800, -19.1300, -4.0500, 19.9800, -5.3800, -3.8200, 19.9800, 15.9100, -7.5500, 19.9800, 19.9700, -12.1300, 19.9800, 19.9700, -10.4600, 19.9800, 19.9700, -9.2700, 19.9800, 19.9700, -6.1800, 19.9800, 19.9700, -6.1800, 19.9800, 19.9700, -4.6800, 19.5600, 15.6500, -3.6800, 12.9200, 8.4000, -5.7900, 9.6900, 3.8200, -5.5500, 10.9100, -3.9100, -4.0800, 6.0200, -12.0400, -2.9300, 1.6000, -12.0400, -2.9300, 1.6000, -17.2900, -0.8900, -3.1300, -19.9800, 0.7700, -7.0300, -19.9800, 1.9800, -11.8500, -19.9800, 0.8300, -18.0000, -19.9800, -1.3800, -11.1400, -19.9800, 2.2700, -1.8800, -19.9800, 2.2700, -1.8800, -19.9800, 5.4800, 9.4500, -15.5700, 8.2800, 17.1300, -8.2900, 9.7100, 19.9700, -4.7900, 9.4900, 19.9800, 1.6000, 10.4000, 19.9800, 1.6000, 10.4000, 19.9800, 16.2600, 6.8500, 19.9800
};

int raw_feature_get_data(size_t offset, size_t length, float *out_ptr) {
    memcpy(out_ptr, features + offset, length * sizeof(float));
    return 0;
}

void setup()
{
    // put your setup code here, to run once:
    Serial.begin(115200);

    Serial.println(""Edge Impulse Inferencing Demo"");
}

void loop()
{
    ei_printf(""Edge Impulse standalone inferencing (Arduino)\n"");

    if (sizeof(features) / sizeof(float) != EI_CLASSIFIER_DSP_INPUT_FRAME_SIZE) {
        ei_printf(""The size of your 'features' array is not correct. Expected %lu items, but had %lu\n"",
            EI_CLASSIFIER_DSP_INPUT_FRAME_SIZE, sizeof(features) / sizeof(float));
        delay(1000);
        return;
    }

    ei_impulse_result_t result = { 0 };

    // the features are stored into flash, and we don't want to load everything into RAM
    signal_t features_signal;
    features_signal.total_length = sizeof(features) / sizeof(features[0]);
    features_signal.get_data = &raw_feature_get_data;

    // invoke the impulse
    EI_IMPULSE_ERROR res = run_classifier(&features_signal, &result, false /* debug */);
    ei_printf(""run_classifier returned: %d\n"", res);

    if (res != 0) return;

    ei_printf(""Predictions (DSP: %d ms., Classification: %d ms., Anomaly: %d ms.): \n"",
        result.timing.dsp, result.timing.classification, result.timing.anomaly);

    // print the predictions
    ei_printf(""["");
    for (size_t ix = 0; ix < EI_CLASSIFIER_LABEL_COUNT; ix++) {
        ei_printf(""%.5f"", result.classification[ix].value);
#if EI_CLASSIFIER_HAS_ANOMALY == 1
        ei_printf("", "");
#else
        if (ix != EI_CLASSIFIER_LABEL_COUNT - 1) {
            ei_printf("", "");
        }
#endif
    }
#if EI_CLASSIFIER_HAS_ANOMALY == 1
    ei_printf(""%.3f"", result.anomaly);
#endif
    ei_printf(""]\n"");

    delay(1000);
}

void ei_printf(const char *format, ...) {
    static char print_buf[1024] = { 0 };

    va_list args;
    va_start(args, format);
    int r = vsnprintf(print_buf, sizeof(print_buf), format, args);
    va_end(args);

    if (r > 0) {
        Serial.write(print_buf);
    }
}
```

When you run this the board will hardfault. If you open the library (in `~/Documents/Arduino/library`) and edit the `tflite-trained.h` file and remove the `const` declaration on `trained_tflite` this will run fine.

We're using a very simple network (just four operations: ['DEQUANTIZE', 'FULLY_CONNECTED', 'SOFTMAX', 'QUANTIZE']). I've attached the quantized tflite model and the TF SavedModel in the library ZIP file.

## Attached

* [Arduino library, tflite file and TensorFlow SavedModel file](https://github.com/tensorflow/tensorflow/files/4902006/ei-continuous-gestures-arduino-1.0.23.zip)"
41270,"GPU delegate library is libtensorflowlite_gpu_delegate.so, not libtensorflowlite_gpu_gl.so.","Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod


https://www.tensorflow.org/lite/performance/gpu_advanced


## Description of issue (what needs changing):

    bazel build -c opt --config android_arm64 tensorflow/lite/delegates/gpu:gl_delegate                  # for static library
    bazel build -c opt --config android_arm64 tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_gl.so  # for dynamic library

should be changed to,

    bazel build -c opt --config android_arm64 tensorflow/lite/delegates/gpu:delegate                  # for static library
    bazel build -c opt --config android_arm64 tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so  # for dynamic library

because gl_delegate is not GPU delegate runtime library, it is for OpenGL delegate, right?


### Clear description

For example, why should someone use this method? How is it useful?

### Correct links

Is the link to the source code correct?

### Parameters defined

Are all parameters defined and formatted correctly?

### Returns defined

Are return values defined?

### Raises listed and defined

Are the errors defined? For example,
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises

### Usage example

Is there a usage example?

See the API guide: https://www.tensorflow.org/community/contribute/docs_ref
on how to write testable usage examples.

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs,
docs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
41269,AttributeError: module 'tensorflow' has no attribute 'gfile',"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
41268,"ValueError: Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [<tf.Tensor 'cond_8/Identity_1:0' shape=(?, 25) dtype=float32>]...","I have a tf.keras model with one encoder and two decoders. I'm attempting to train the model with tf dataset generated from 

tf.data.Dataset.from_generator

Model -
`
# encoder
inp = tf.keras.Input(shape=(None, 25))
x = tf.keras.layers.Dense(64)(inp)
enc = tf.keras.layers.Dense(64)(x)

# decoder1
latentInputs1 = tf.keras.Input(shape=(64,))
 x1 = tf.keras.layers.Dense(64)(latentInputs1)
 x1 = tf.keras.layers.Dense(64)(x1)
dec1 = tf.keras.layers.Dense(25, activation='softmax', name='dec1')(x1)

# decoder2
latentInputs2 = tf.keras.Input(shape=(64,))
x1 = tf.keras.layers.Dense(64)(x1)
x1 = tf.keras.layers.Dense(64)(x1)
dec2 = tf.keras.layers.Dense(5, activation='softmax', name='dec2')(x1)

#model
encoder = tf.keras.Model(inputs = inp, outputs = enc)
decode1 = tf.keras.Model(inputs = latentInputs, outputs = dec, name='dec1')
decode2 = tf.keras.Model(inputs = latentInputs1, outputs = classif_dec, name='dec2')

model = tf.keras.Model( inputs = inp, outputs = [decode1(encoder(inp)), decode2(encoder(inp))] )

#dataset definition
train_data = tf.data.Dataset.from_generator(
       lambda: generator_fn(file_path),
       output_types=(tf.float32, tf.float32, f.float32}),
       output_shapes=(tf.TensorShape([None]), tf.TensorShape([None]), tf.TensorShape([None])})
       )

#compile
losses = {
                    ""dec1"": ""mean_absolute_error"",
                    ""dec2"": ""mean_absolute_error"",
                }

model.compile(optimizer=keras.optimizers.Adam(learning_rate=experiment_params['learning_rate'])
                          loss=losses,
                          metrics=[keras.metrics.MeanSquaredError()]
                          )

model.fit(train_data, epochs=data_params.epochs,
                        steps_per_epoch=train_steps,
                        callbacks=callbacks,)
`

should I make changes in the dataset shape ? what's the course of action ?










"
41267,tensorflow2.3 android cannot open armv7-a/libatomic.a,"/tensorflow-r2.3/tensorflow/lite/java/BUILD:404:18: Linking of rule '//tensorflow/lite/java:libtensorflowlite_jni.so' failed (Exit 1)
external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/windows-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin\ld: error: cannot open external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/windows-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/lib/../lib/armv7-a/libatomic.a: No such file or directory
clang.exe: error: linker command failed with exit code 1 (use -v to see invocation)"
41266,Training with Coco dataset 2017 for a pretrained model,"I am training with Coco dataset to have a pretrained model for a new network architecture.
2017 Coco dataset was used. dataset_tools/create_coco_tf_record.py was used to create tfrecords.

I have error at 
```
object_detection_evaluation.py"", line 285, in add_single_ground_truth_image_info
    raise ValueError('Image with id {} already added.'.format(image_id))
ValueError: Image with id b'559842' already added.
```
Then comment out the lines 285 and 286 in `object_detection_evaluation.py` and can continue the evaluation.
```
if image_id in self._image_ids:
      raise ValueError('Image with id {} already added.'.format(image_id))
```

The eval results are
```
I0710 15:06:50.361298 140470232598272 eval_util.py:82] Losses/Loss/BoxClassifierLoss/classification_loss: 0.426311
I0710 15:06:50.361555 140470232598272 eval_util.py:82] Losses/Loss/BoxClassifierLoss/localization_loss: 0.273654
I0710 15:06:50.361709 140470232598272 eval_util.py:82] Losses/Loss/RPNLoss/localization_loss: 0.571312
I0710 15:06:50.361852 140470232598272 eval_util.py:82] Losses/Loss/RPNLoss/objectness_loss: 0.382609
```
Is that result acceptable?"
41263,How to check programmatically TF-Lite Ops supported,"How to check programmatically TF-Lite Ops supported?

Is there a way to check the list of all operators available per platform CPU ([BuiltinOperator](https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/lite/tools/versioning/runtime_version.cc#L53)), GPU ([OperationType](https://github.com/tensorflow/tensorflow/blob/9e54b4b299f27dfdd2cb8d9d00c7186eb7d9826f/tensorflow/lite/delegates/gpu/common/operations.h#L33)) programmatically?  "
41262,pip install error : unable to install tensorflow on py 3.7 64 bit version,"unable to install tensorflow on python 3.7  64 bit version

 (tensor) C:\Users\Saifi's\Desktop\tensor_env>pip install tensorflow
ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)
ERROR: No matching distribution found for tensorflow

even tried python 3.8 still same issue.
 pip version pip 20.1.1 (installed)"
41260,Could the argument alpha in leakyrelu be minus?,"I have tried to set alpha <0 in my model, the model did not work. Some document said the argument should >0, is it really? How can I make it?"
41258,problem with importing modules,"---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
~\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59 

~\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in <module>
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

~\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

~\anaconda3\lib\imp.py in load_module(name, file, filename, details)
    241         else:
--> 242             return load_dynamic(name, filename, file)
    243     elif type_ == PKG_DIRECTORY:

~\anaconda3\lib\imp.py in load_dynamic(name, path, file)
    341             name=name, loader=loader, origin=path)
--> 342         return _load(spec)
    343 

ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-1-a5c78dab8445> in <module>
      1 import pandas as pd
----> 2 import tensorflow as tf
      3 from tensorflow import keras
      4 import numpy as np

~\anaconda3\lib\site-packages\tensorflow\__init__.py in <module>
     39 import sys as _sys
     40 
---> 41 from tensorflow.python.tools import module_util as _module_util
     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader
     43 

~\anaconda3\lib\site-packages\tensorflow\python\__init__.py in <module>
     48 import numpy as np
     49 
---> 50 from tensorflow.python import pywrap_tensorflow
     51 
     52 # Protocol buffers

~\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     67 for some common reasons and solutions.  Include the entire stack trace
     68 above this error message when asking for help."""""" % traceback.format_exc()
---> 69   raise ImportError(msg)
     70 
     71 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""C:\Users\acer\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\acer\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\acer\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\acer\anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\acer\anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
41257,"Configuring `TF_CONFIG` in practice + a small ""bug"" in the Multi-worker training with Keras tutorial","Hey @lamberta and team 👋

A couple of things here related to the **[Multi-worker training with Keras](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras)** tutorial (with `tf.distribute.experimental.MultiWorkerMirroredStrategy()`):

### 1. Configuring the `TF_CONFIG` environment variable for multi-worker training in practice

**URL:** https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#multi-worker_configuration

**Description:**
There is an important section dedicated to [Multi-worker Configuration](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#multi-worker_configuration), which states that outside of the free Colab environment:

> In practice, users would create multiple workers on external IP addresses/ports, and set `TF_CONFIG` on each worker appropriately.

The current example snippet shows that in the `task` component of `TF_CONFIG`, the `index` value of the worker is set to `0` (`task` is different on each worker):

```python
# Import necessary libraries (not currently included in the tutorial)
import json
import os

os.environ['TF_CONFIG'] = json.dumps({
    'cluster': {
        'worker': [""localhost:12345"", ""localhost:23456""]
    },
    'task': {'type': 'worker', 'index': 0}
})
```

However, this may create some confusion, as demonstrated in https://github.com/tensorflow/tensorflow/issues/39922. The user tried setting the `TF_CONFIG` environment variable (with `index: 0`), as shown in the tutorial, but that wouldn't let the rest of the script proceed with execution. If the user runs the experiment with `index: 1`, all is OK.

As mentioned in https://github.com/tensorflow/tensorflow/issues/39922, you should be following this TensorFlow distributed training [guide](https://www.tensorflow.org/guide/distributed_training#setting_up_tf_config_environment_variable), where it says:

> One example of TF_CONFIG is:
> 
> ```python
> os.environ[""TF_CONFIG""] = json.dumps({
>     ""cluster"": {
>         ""worker"": [""host1:port"", ""host2:port"", ""host3:port""],
>         ""ps"": [""host4:port"", ""host5:port""]
>     },
>    ""task"": {""type"": ""worker"", ""index"": 1}
> })
> ```

**Proposed solution:** 

1) Borrow the sample code from the [Setting up TF_CONFIG environment variable](https://www.tensorflow.org/guide/distributed_training#setting_up_tf_config_environment_variable
) section from the [Distributed training with TensorFlow](https://www.tensorflow.org/guide/distributed_training) guide into; and
2) Add this code sample to the [Multi-worker Configuration](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#multi-worker_configuration) section of the [Multi-worker training with Keras](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras) tutorial.

For example:
> In this example, we set the task `type` to `""worker""` and the task `index` to `0`. This means the machine that has such setting is the first worker...
>...
> [ORIGINAL CODE SNIPPET]
>...
> In your use case, you may also set the task `type` to `""worker""` and the task `index` to `1` instead of `0`.
> 
> ```python
> # Import necessary libraries (not currently included in the tutorial)
> import json
> import os
> os.environ[""TF_CONFIG""] = json.dumps({
>     ""cluster"": {
>         ""worker"": [""host1:port"", ""host2:port"", ""host3:port""],
>         ""ps"": [""host4:port"", ""host5:port""]
>     },
>    ""task"": {""type"": ""worker"", ""index"": 1}
> })
> ```

### 2. A small ""bug"" in the Dataset sharding and batch size section

**URL:** https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#dataset_sharding_and_batch_size

**Description:** The following sentence in **Dataset sharding and batch size** looks a bit odd:

> If you prefer manual sharding for your training, automatic sharding can be turned off via `tf.data.experimental.DistributeOptions` api. Concretely,

(this is followed by a code snippet)

### Submit a pull request?

Yes for (1), once this has been clarified.
Regarding (2), I think it's up to the team."
41255,"MLIR Converter crashes because ""tf.DecodeJpeg is not an op""","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installed from (source or binary): Source
- TensorFlow version (or github SHA if from source): 2.4.0-dev20200709


**Command used to run the converter or code if you’re using the Python API**

```
# Convert to tflite
mkdir -p tflite
SAVED_MODEL_DIR=""$REPO_PATH/tmp""
OUTPUT_DIR=""$REPO_PATH/tflite""
python $TPU_REPO_PATH/models/official/detection/export_tflite_model.py \
  --saved_model_dir=""${SAVED_MODEL_DIR?}"" \
  --output_dir=""${OUTPUT_DIR?}""
```

**The output from the converter invocation**

```
loc(callsite(""DecodeJpeg""(""/home/rsaini/.pyenv/versions/edge/lib/python3.7/site-packages/tensorflow/python/saved_model/loader_impl.py"":299:0) at callsite(""/home/rsaini/.pyenv/versions/edge/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py"":324:0 at callsite(""/home/rsaini/.pyenv/versions/edge/lib/python3.7/site-packages/tensorflow/lite/python/convert_saved_model.py"":198:0 at callsite(""/home/rsaini/.pyenv/versions/edge/lib/python3.7/site-packages/tensorflow/lite/python/lite.py"":1923:0 at callsite(""/home/rsaini/repos/applications/tpu/models/official/detection/export_tflite_model.py"":35:0 at callsite(""/home/rsaini/repos/applications/tpu/models/official/detection/export_tflite_model.py"":49:0 at callsite(""/home/rsaini/.pyenv/versions/edge/lib/python3.7/site-packages/absl/app.py"":250:0 at callsite(""/home/rsaini/.pyenv/versions/edge/lib/python3.7/site-packages/absl/app.py"":299:0 at callsite(""/home/rsaini/.pyenv/versions/edge/lib/python3.7/site-packages/tensorflow/python/platform/app.py"":40:0 at ""/home/rsaini/repos/applications/tpu/models/official/detection/export_tflite_model.py"":55:0)))))))))): error: 'tf.DecodeJpeg' op is neither a custom op nor a flex op
error: failed while converting: 'main': Ops that need custom implementation (enabled via setting the -emit-custom-ops flag):
	tf.DecodeJpeg {acceptable_fraction = 1.000000e+00 : f32, channels = 0 : i64, dct_method = """", device = """", fancy_upscaling = true, ratio = 1 : i64, try_recover_truncated = false}
Traceback (most recent call last):
  File ""/home/rsaini/.pyenv/versions/edge/lib/python3.7/site-packages/tensorflow/lite/python/convert.py"", line 199, in toco_convert_protos
    enable_mlir_converter)
  File ""/home/rsaini/.pyenv/versions/edge/lib/python3.7/site-packages/tensorflow/lite/python/wrap_toco.py"", line 38, in wrapped_toco_convert
    enable_mlir_converter)
 %cst_9 = ""std.constant""() {value = dense<""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
```

Note: The output was an endless string of random hex characters. Unfortunately, I missed the last few stack frames as I was unable to perfectly time pausing the process (the hex characters filled the terminal's buffer more quickly than I could react :) ). 

**Also, please include a link to the saved model or GraphDef**
[model.zip](https://github.com/tensorflow/tensorflow/files/4900048/model.zip)

**Failure details**

No model is produced"
41254,Error building basic AOT library,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Debian GNU/Linux rodete**
- TensorFlow installed from (source or binary): **source**
- TensorFlow version (use command below): **v2.2.0-rc4-8-g2b96f3662b 2.2.0**
- Python version: **Python3**
- Bazel version (if compiling from source): **3.1.0**

**Background**

I'm following [this](https://www.tensorflow.org/xla/tfcompile) tutorial. I can't build the sample library. I've already read [this](https://github.com/tensorflow/tensorflow/issues/13482) issue, which seems similar to mine, but it didn't help.

**Steps followed**

1. Clone tensorflow and go to aot's directory

`git clone https://github.com/tensorflow/tensorflow.git`
`cd tensorflow/tensorflow/compiler/aot`

2. Run scipt to generate protobuf

`python3 tests/make_test_graphs.py --out_dir=.`

3. Run test_graph_tfmatmul rule.

`bazel build :test_graph_tfmatmul`

**Describe the current behavior**

Step 3 errors out with the following message:

ERROR: /usr/local/google/home/adanlopez/tfcompile/tensorflow/tensorflow/python/BUILD:873:1: undeclared inclusion(s) in rule '//tensorflow/python:_pywrap_checkpoint_reader.so':
this rule is missing dependency declarations for the following files included by 'tensorflow/python/util/py_checkpoint_reader_wrapper.cc':
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/arrayobject.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/ndarrayobject.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/ndarraytypes.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/npy_common.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/numpyconfig.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/_numpyconfig.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/npy_endian.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/npy_cpu.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/utils.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/_neighborhood_iterator_imp.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/__multiarray_api.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/npy_interrupt.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/ufuncobject.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/npy_math.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/npy_common.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/arrayobject.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/__ufunc_api.h'
Target //tensorflow/compiler/aot/tests:test_graph_tfmatmul failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 135.764s, Critical Path: 47.52s
INFO: 336 processes: 336 local.
FAILED: Build did NOT complete successfully

**Describe the expected behavior**

Step 3 should not error out."
41251,The `**kwargs` in `tf.keras.Model` does not accept any argument,"The `**kwargs` in `tf.keras.Model` does not accept any argument.

I am writing derivative class of `tf.keras.Model`, namely `Foo` class, which would like to create an instance of another class `Boo`. Hence I would like to pass arguments of `Boo`, say `booarg`, together to the derived class constructor. However, since `**kwargs` in `tf.keras.Model` does not accept any argument, I have to filter out arguments by myself, which is complicated, especially when the argument list of `Boo` is long. A toy code is attached.

```python
import tensorflow as tf

class Boo():
    def __init__(self, booarg1, booarg2):
        print(""Boo Class: "", booarg1)
        print(""Boo Class: "", booarg2)

class FooModel(tf.keras.Model):
    def __init__(self, fooarg, **kwargs):
        super(FooModel, self).__init__(**kwargs)
        print(""Foo Class: "", fooarg)
        boo = Boo(**kwargs)

foo = FooModel(fooarg=1, booarg1=2, booarg2=3)
```


**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2.0
- Python version: 3.8.2
- GCC/Compiler version (if compiling from source): 9.3.0
"
41250,Unknown call to ResizeNearestNeighbor?,"I am trying to convert a model to tflite format and run inferencing. I am aware that the ResizeNearestNeighbor is supported on tf_nightly, but unfortunately I am not able to use tf_nightly for my application. I am trying to rework the model so as to not call ResizeNearestNeighbor, but I am not sure where it is being called. I am not using image.resize anywhere. How can I find where the model is calling this operation?

Thank you!"
41248,target_tensor argument from tf.keras.Model compile() method has dissapeared,"I am not entirely sure whether this is a bug or not.

I have a piece of code which was relying in the `target_tensors` argument of the `compile` method. This code works fine in TF 2.1, however when updating to TF 2.2 it stopped working.
Looking in the documentation I've noticed that indeed, the argument `target_tensors` has dissapeared [link](https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile) however I haven't seen any mentions to this in the changelog.

Is this a bug or is it intentional? The error message just says
`
ValueError: target_tensors argument is not supported when executing eagerly
`

But looking at the source code https://github.com/tensorflow/tensorflow/blob/ae975ee7445b448abb40a8916336dc5646e8b46a/tensorflow/python/keras/engine/training.py#L2496 this argument is clearly not accepted anymore.

If it is intentional, is there a recommended workaround? I can think of several not-very-intrusive ways around it, but all them feel a bit hacky."
41247,"TensorFlow 2.2, No progress of distributed training for a TensorFlow example code on GPU clusters","I am running the example code at 
     https://www.tensorflow.org/tutorials/distribute/multi_worker_with_estimator

on GPU clusters (1 master 2 workers) from databricks py3 notebook. 

      Tensorflow version : 2.2.0
      Num XLA_GPUs Available:  1
      Num GPUs Available:  1
      [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'), PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]

But, this training makes no progress:


        tf.estimator.train_and_evaluate(
        classifier,
       train_spec=tf.estimator.TrainSpec(input_fn=input_fn),
       eval_spec=tf.estimator.EvalSpec(input_fn=input_fn)

The log : 

     Instructions for updating:
     Use the iterator's `initializer` property instead.
     INFO:tensorflow:Graph was finalized.
     INFO:tensorflow:Graph was finalized.

From GPU cluster Ganglia: all GPUs and CPUs are idle.

I have tried all comms:
 
     communication=tf.distribute.experimental.CollectiveCommunication.NCCL/RING/AUTO

 but the same thing.

Could anybody help me about this ?



"
41246,Support for DepthwiseConv2D with output channel less than input channel,"**System information**
- TensorFlow version (you are using):2.2 and nightly
- Are you willing to contribute it (Yes/No):No



**Describe the feature and the current behavior/state.**
DepthwiseConv2D use depth_multiplier (int), which makes number of output channel always greater than or  equal to number of input channel. But for unet use case, we need to reduce the number of output channels after doing upsampling using depthwise convolution. There is currently no way to support that in tensorflow. pytorch is much more dynamic since it supports grouping in convolution2D which tensorflow does not support and use a delicate  DepthwiseConv2D API.


"
41245,Difference between result of code and description of guide,"## URL(s) with the issue:
https://www.tensorflow.org/guide/keras/train_and_evaluate#passing_data_to_multi-input_multi-output_models

## Description of issue (what needs changing):

### Clear description
When I debug my code according to the guide(""Passing data to multi-input, multi-output models,Training & evaluation with the built-in methods""), I found it is difference between result of code and description of guide.
The guide told us that we could choose not to compute a loss for certain outputs, if these outputs meant for prediction but not for training.
like follows:
```
model.compile(
    optimizer=keras.optimizers.RMSprop(1e-3),
    loss=[None, keras.losses.CategoricalCrossentropy()],
) 
```
But, in Linux OS and version of tensorflow is 2.1.0, ""None"" can not be used to there, and  pull a error.
like：
```
model.compile(
        optimizer=op,
        loss=[None,LossForTask(),LossForLMP()],
        metrics=[[F1()],[],[]]
        # loss_weights=[1,1,0]
    )
```
error：
ValueError: Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), for inputs ['output_2', 'output_3'] but instead got the following list of 3 arrays

I use same code in win10 OS，it is fine without any error。
"
41244,Tensorflow profiler crashes when using string categorical layer,"Tensorflow profiler crashes when using string categorical layer, not allowing to profile models with those layers.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):('v1.12.1-35610-gd8c49c2fde', '2.4.0-dev20200701')
  tf-estimator-nightly       2.4.0.dev2020063001
  tf-nightly                 2.4.0.dev20200701
- Python version: 3.7
- CUDA/cuDNN version: 


- GPU model and memory: 
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:00:1B.0 Off |                    0 |
| N/A   38C    P0    50W / 300W |  15450MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-SXM2...  On   | 00000000:00:1C.0 Off |                    0 |
| N/A   37C    P0    54W / 300W |  15450MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-SXM2...  On   | 00000000:00:1D.0 Off |                    0 |
| N/A   36C    P0    55W / 300W |  15522MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |
| N/A   37C    P0    55W / 300W |  15522MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+


== cuda libs  ===================================================
/usr/local/cuda-9.2/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.2/doc/man/man7/libcudart.7
/usr/local/cuda-9.2/lib64/libcudart.so.9.2.148
/usr/local/cuda-9.2/lib64/libcudart_static.a
/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.0/doc/man/man7/libcudart.7
/usr/local/cuda-9.0/lib64/libcudart.so.9.0.176
/usr/local/cuda-9.0/lib64/libcudart_static.a
/usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudart.so.10.1.243
/usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-10.1/doc/man/man7/libcudart.so.7
/usr/local/cuda-10.1/doc/man/man7/libcudart.7
/usr/local/cuda-10.2/targets/x86_64-linux/lib/libcudart.so.10.2.89
/usr/local/cuda-10.2/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-10.2/doc/man/man7/libcudart.so.7
/usr/local/cuda-10.2/doc/man/man7/libcudart.7
/usr/local/cuda-10.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-10.0/doc/man/man7/libcudart.7
/usr/local/cuda-10.0/lib64/libcudart.so.10.0.130
/usr/local/cuda-10.0/lib64/libcudart_static.a
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61
/usr/local/cuda-8.0/lib64/libcudart_static.a

== tensorflow installed from info ==================

== python version  ==============================================
(major, minor, micro, releaselevel, serial)
(3, 6, 10, 'final', 0)


**Describe the current behavior**
Profiler crashes, doesn't allow to profile a model

**Describe the expected behavior**
I expect to be able to profile models with string categorical encoding

**Standalone code to reproduce the issue**
```
import tensorflow as tf
import numpy as np
import pandas as pd
from tensorflow import keras
from tensorflow.keras import layers

from datetime import datetime
from packaging import version
import os

from tensorflow.keras.layers.experimental.preprocessing import Normalization
from tensorflow.keras.layers.experimental.preprocessing import CategoryEncoding
from tensorflow.keras.layers.experimental.preprocessing import StringLookup

#tf.debugging.set_log_device_placement(True)

file_url = ""https://storage.googleapis.com/applied-dl/heart.csv""
dataframe = pd.read_csv(file_url)

val_dataframe = dataframe.sample(frac=0.2, random_state=1337)
train_dataframe = dataframe.drop(val_dataframe.index)

print(
    ""Using %d samples for training and %d for validation""
    % (len(train_dataframe), len(val_dataframe))
)

def dataframe_to_dataset(dataframe):
    dataframe = dataframe.copy()
    labels = dataframe.pop(""target"")
    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))
    ds = ds.shuffle(buffer_size=len(dataframe))
    return ds


train_ds = dataframe_to_dataset(train_dataframe)
val_ds = dataframe_to_dataset(val_dataframe)

for x, y in train_ds.take(1):
    print(""Input:"", x)
    print(""Target:"", y)


def encode_numerical_feature(feature, name, dataset):
    normalizer = Normalization()

    feature_ds = dataset.map(lambda x, y: x[name])
    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))

    normalizer.adapt(feature_ds)

    encoded_feature = normalizer(feature)
    return encoded_feature


def encode_string_categorical_feature(feature, name, dataset):
    index = StringLookup()

    feature_ds = dataset.map(lambda x, y: x[name])
    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))

    index.adapt(feature_ds)

    encoded_feature = index(feature)

    encoder = CategoryEncoding(output_mode=""binary"")

    feature_ds = feature_ds.map(index)

    encoder.adapt(feature_ds)

    encoded_feature = encoder(encoded_feature)
    return encoded_feature


def encode_integer_categorical_feature(feature, name, dataset):
    encoder = CategoryEncoding(output_mode=""binary"")

    feature_ds = dataset.map(lambda x, y: x[name])
    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))

    encoder.adapt(feature_ds)

    encoded_feature = encoder(feature)
    return encoded_feature

def build_models():
    sex = keras.Input(shape=(1,), name=""sex"", dtype=""int64"")
    cp = keras.Input(shape=(1,), name=""cp"", dtype=""int64"")
    fbs = keras.Input(shape=(1,), name=""fbs"", dtype=""int64"")
    restecg = keras.Input(shape=(1,), name=""restecg"", dtype=""int64"")
    exang = keras.Input(shape=(1,), name=""exang"", dtype=""int64"")
    ca = keras.Input(shape=(1,), name=""ca"", dtype=""int64"")

    thal = keras.Input(shape=(1,), name=""thal"", dtype=""string"")

    age = keras.Input(shape=(1,), name=""age"")
    trestbps = keras.Input(shape=(1,), name=""trestbps"")
    chol = keras.Input(shape=(1,), name=""chol"")
    thalach = keras.Input(shape=(1,), name=""thalach"")
    oldpeak = keras.Input(shape=(1,), name=""oldpeak"")
    slope = keras.Input(shape=(1,), name=""slope"")

    all_inputs = [
        sex,
        cp,
        fbs,
        restecg,
        exang,
        ca,
        thal,
        age,
        trestbps,
        chol,
        thalach,
        oldpeak,
        slope,
    ]

    numerical_inputs = [
        age,
        trestbps,
        chol,
        thalach,
        oldpeak,
        slope,
    ]


    sex_encoded = encode_integer_categorical_feature(sex, ""sex"", train_ds)
    cp_encoded = encode_integer_categorical_feature(cp, ""cp"", train_ds)
    fbs_encoded = encode_integer_categorical_feature(fbs, ""fbs"", train_ds)
    restecg_encoded = encode_integer_categorical_feature(restecg, ""restecg"", train_ds)
    exang_encoded = encode_integer_categorical_feature(exang, ""exang"", train_ds)
    ca_encoded = encode_integer_categorical_feature(ca, ""ca"", train_ds)

    thal_encoded = encode_string_categorical_feature(thal, ""thal"", train_ds)

    age_encoded = encode_numerical_feature(age, ""age"", train_ds)
    trestbps_encoded = encode_numerical_feature(trestbps, ""trestbps"", train_ds)
    chol_encoded = encode_numerical_feature(chol, ""chol"", train_ds)
    thalach_encoded = encode_numerical_feature(thalach, ""thalach"", train_ds)
    oldpeak_encoded = encode_numerical_feature(oldpeak, ""oldpeak"", train_ds)
    slope_encoded = encode_numerical_feature(slope, ""slope"", train_ds)

    all_features = layers.concatenate(
        [
            sex_encoded,
            cp_encoded,
            fbs_encoded,
            restecg_encoded,
            exang_encoded,
            slope_encoded,
            ca_encoded,
            thal_encoded,
            age_encoded,
            trestbps_encoded,
            chol_encoded,
            thalach_encoded,
            oldpeak_encoded,
        ]
    )

    numerical_features = layers.concatenate(
        [
            age_encoded,
            trestbps_encoded,
            chol_encoded,
            thalach_encoded,
            oldpeak_encoded,
            slope_encoded
        ]
    )


    x = layers.Dense(32, activation=""relu"")(all_features)
    x = layers.Dropout(0.5)(x)
    output = layers.Dense(1, activation=""sigmoid"")(x)
    model_cat_string = keras.Model(all_inputs, output)
    model_cat_string.compile(""adam"", ""binary_crossentropy"", metrics=[""accuracy""])
    
    x = layers.Dense(32, activation=""relu"")(numerical_features)
    x = layers.Dropout(0.5)(x)
    output = layers.Dense(1, activation=""sigmoid"")(x)
    model_only_num = keras.Model(numerical_inputs, output)
    model_only_num.compile(""adam"", ""binary_crossentropy"", metrics=[""accuracy""])

    return (model_only_num, model_cat_string)



model_only_numerical, model_with_categorical_string_layers = build_models()


logs = ""logs/"" + datetime.now().strftime(""%Y%m%d-%H%M%S"")
tboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logs, histogram_freq=1, profile_batch='1,20')


#model with only numerical features, profiler works fine
model_only_numerical.summary()
model_only_numerical.fit(train_ds.batch(32), 
                     epochs=50, 
                     validation_data=val_ds.batch(32),
                    callbacks=[tboard_callback]
                    )

#model with categorical string encoding layers, profiler crashes!
model_with_categorical_string_layers.summary()
model_with_categorical_string_layers.fit(train_ds.batch(32), 
                     epochs=50, 
                     validation_data=val_ds.batch(32),
                    callbacks=[tboard_callback]
                    )
```"
41243,Why my Xception model is very very slow on TPU compared to large models like efficientnet-b6 or b7?,"i am having this while trying Xception model on tpu :

WARNING: AutoGraph could not transform <function at 0x7f1e447a1950> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, export AUTOGRAPH_VERBOSITY=10) and attach the full output.
Cause: Unable to locate the source code of <function at 0x7f1e447a1950>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

and also it is taking a lot of time
for 2 epochs it takes 59 minutes on TPU with Xception model
but same code where if i change Xception with efficientnet-b6 or b7 then i can run 13 epochs which will finish within 1 hour!!

here is my code : https://www.kaggle.com/mobassir/in-depth-melanoma-and-modeling?scriptVersionId=38354798

and here you can see : https://www.kaggle.com/agentauers/incredible-tpus-finetune-effnetb0-b6-at-once

same code with efficientnet-b6 or b7 is extremely fast compared to my simple Xception model
please help?"
41242,Tensorflow Install Error: Internal Python error in the inspect module,"**The problem**
I have been trying to install tensorflow for some days now. I have tried the following various install instructions but remain unable to use tensorflow in Jupyter notebook and am at an impasse. I followed https://365datascience.com/install-tensorflow-2-anaconda/ to install.

Can someone please advise how to fix this? Your help is really appreciated.
The traceback is below:


**System information**
- OS Platform: Windows 10 
- TensorFlow installed from (source or binary):  conda install tensorflow
- TensorFlow version: https://repo.anaconda.com/pkgs/main/win-64/tensorflow-2.1.0-eigen_py37hd727fc0_0.conda"",
  ""version"": ""2.1.0
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: Installed using virtual env, pip and conda per  https://365datascience.com/install-tensorflow-2-anaconda/
- CUDA/cuDNN version:  #define TF_CUDA_CAPABILITIES CudaVersion(""3.5""),CudaVersion(""5.2"")
- GPU model and memory: TENSORFLOW_CORE_KERNELS_CUDNN_POOLING_GPU_H_

**Traceback**
ERROR:root:Internal Python error in the inspect module.
Below is the traceback from this internal error.

Traceback (most recent call last):
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\willi\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\willi\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\willi\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 3325, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-1-2acf5319ccae>"", line 1, in <module>
    import tensorflow as ts
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 98, in <module>
    from tensorflow_core import *
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\willi\Anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\willi\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\willi\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\willi\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2039, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'ImportError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\willi\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\willi\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\willi\Anaconda3\lib\site-packages\IPython\core\ultratb.py"", line 1101, in get_records
    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)
  File ""C:\Users\willi\Anaconda3\lib\site-packages\IPython\core\ultratb.py"", line 319, in wrapped
    return f(*args, **kwargs)
  File ""C:\Users\willi\Anaconda3\lib\site-packages\IPython\core\ultratb.py"", line 353, in _fixed_getinnerframes
    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))
  File ""C:\Users\willi\Anaconda3\lib\inspect.py"", line 1502, in getinnerframes
    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)
  File ""C:\Users\willi\Anaconda3\lib\inspect.py"", line 1460, in getframeinfo
    filename = getsourcefile(frame) or getfile(frame)
  File ""C:\Users\willi\Anaconda3\lib\inspect.py"", line 696, in getsourcefile
    if getattr(getmodule(object, filename), '__loader__', None) is not None:
  File ""C:\Users\willi\Anaconda3\lib\inspect.py"", line 733, in getmodule
    if ismodule(module) and hasattr(module, '__file__'):
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\willi\Anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 953, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 728, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\__init__.py"", line 42, in <module>
    from . _api.v2 import audio
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\_api\v2\audio\__init__.py"", line 10, in <module>
    from tensorflow.python.ops.gen_audio_ops import decode_wav
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\ops\gen_audio_ops.py"", line 9, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\willi\Anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\willi\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\willi\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\willi\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 3325, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-1-2acf5319ccae>"", line 1, in <module>
    import tensorflow as ts
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 98, in <module>
    from tensorflow_core import *
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\willi\Anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\willi\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\willi\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\willi\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2039, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'ImportError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\willi\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\willi\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
ERROR:root:Internal Python error in the inspect module.
Below is the traceback from this internal error.

Traceback (most recent call last):
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\willi\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\willi\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\willi\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 3325, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-1-2acf5319ccae>"", line 1, in <module>
    import tensorflow as ts
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 98, in <module>
    from tensorflow_core import *
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\willi\Anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\willi\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\willi\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\willi\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2039, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'ImportError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\willi\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 3248, in run_ast_nodes
    if (await self.run_code(code, result,  async_=asy)):
  File ""C:\Users\willi\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 3342, in run_code
    self.showtraceback(running_compiled_code=True)
  File ""C:\Users\willi\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2042, in showtraceback
    value, tb, tb_offset=tb_offset)
  File ""C:\Users\willi\Anaconda3\lib\site-packages\IPython\core\ultratb.py"", line 1385, in structured_traceback
    self, etype, value, tb, tb_offset, number_of_lines_of_context)
  File ""C:\Users\willi\Anaconda3\lib\site-packages\IPython\core\ultratb.py"", line 1288, in structured_traceback
    self, etype, value, tb, tb_offset, number_of_lines_of_context
  File ""C:\Users\willi\Anaconda3\lib\site-packages\IPython\core\ultratb.py"", line 1150, in structured_traceback
    formatted_exceptions += self.prepare_chained_exception_message(evalue.__cause__)
TypeError: can only concatenate str (not ""list"") to str

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\willi\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2039, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'TypeError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\willi\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\willi\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\willi\Anaconda3\lib\site-packages\IPython\core\ultratb.py"", line 1101, in get_records
    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)
  File ""C:\Users\willi\Anaconda3\lib\site-packages\IPython\core\ultratb.py"", line 319, in wrapped
    return f(*args, **kwargs)
  File ""C:\Users\willi\Anaconda3\lib\site-packages\IPython\core\ultratb.py"", line 353, in _fixed_getinnerframes
    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))
  File ""C:\Users\willi\Anaconda3\lib\inspect.py"", line 1502, in getinnerframes
    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)
  File ""C:\Users\willi\Anaconda3\lib\inspect.py"", line 1460, in getframeinfo
    filename = getsourcefile(frame) or getfile(frame)
  File ""C:\Users\willi\Anaconda3\lib\inspect.py"", line 696, in getsourcefile
    if getattr(getmodule(object, filename), '__loader__', None) is not None:
  File ""C:\Users\willi\Anaconda3\lib\inspect.py"", line 733, in getmodule
    if ismodule(module) and hasattr(module, '__file__'):
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\willi\Anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 953, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 728, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\__init__.py"", line 42, in <module>
    from . _api.v2 import audio
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\_api\v2\audio\__init__.py"", line 10, in <module>
    from tensorflow.python.ops.gen_audio_ops import decode_wav
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\ops\gen_audio_ops.py"", line 9, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\willi\Anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\willi\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\willi\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\willi\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 3325, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-1-2acf5319ccae>"", line 1, in <module>
    import tensorflow as ts
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 98, in <module>
    from tensorflow_core import *
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\willi\Anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\willi\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\willi\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\willi\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2039, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'ImportError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\willi\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 3248, in run_ast_nodes
    if (await self.run_code(code, result,  async_=asy)):
  File ""C:\Users\willi\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 3342, in run_code
    self.showtraceback(running_compiled_code=True)
  File ""C:\Users\willi\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2042, in showtraceback
    value, tb, tb_offset=tb_offset)
  File ""C:\Users\willi\Anaconda3\lib\site-packages\IPython\core\ultratb.py"", line 1385, in structured_traceback
    self, etype, value, tb, tb_offset, number_of_lines_of_context)
  File ""C:\Users\willi\Anaconda3\lib\site-packages\IPython\core\ultratb.py"", line 1288, in structured_traceback
    self, etype, value, tb, tb_offset, number_of_lines_of_context
  File ""C:\Users\willi\Anaconda3\lib\site-packages\IPython\core\ultratb.py"", line 1150, in structured_traceback
    formatted_exceptions += self.prepare_chained_exception_message(evalue.__cause__)
TypeError: can only concatenate str (not ""list"") to str

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\willi\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2039, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'TypeError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\willi\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\willi\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\willi\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
~\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py in <module>
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

~\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py in <module>
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

~\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

~\Anaconda3\lib\imp.py in load_module(name, file, filename, details)
    241         else:
--> 242             return load_dynamic(name, filename, file)
    243     elif type_ == PKG_DIRECTORY:

~\Anaconda3\lib\imp.py in load_dynamic(name, path, file)
    341             name=name, loader=loader, origin=path)
--> 342         return _load(spec)
    343 

ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:


During handling of the above exception, another exception occurred:

AttributeError                            Traceback (most recent call last)
~\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py in showtraceback(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)
   2038                         # in the engines. This should return a list of strings.
-> 2039                         stb = value._render_traceback_()
   2040                     except Exception:

AttributeError: 'ImportError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
~\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py in run_code(self, code_obj, result, async_)
   3340             if result is not None:
   3341                 result.error_in_exec = sys.exc_info()[1]
-> 3342             self.showtraceback(running_compiled_code=True)
   3343         else:
   3344             outflag = False

~\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py in showtraceback(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)
   2040                     except Exception:
   2041                         stb = self.InteractiveTB.structured_traceback(etype,
-> 2042                                             value, tb, tb_offset=tb_offset)
   2043 
   2044                     self._showtraceback(etype, value, stb)

~\Anaconda3\lib\site-packages\IPython\core\ultratb.py in structured_traceback(self, etype, value, tb, tb_offset, number_of_lines_of_context)
   1383         self.tb = tb
   1384         return FormattedTB.structured_traceback(
-> 1385             self, etype, value, tb, tb_offset, number_of_lines_of_context)
   1386 
   1387 

~\Anaconda3\lib\site-packages\IPython\core\ultratb.py in structured_traceback(self, etype, value, tb, tb_offset, number_of_lines_of_context)
   1286             # Verbose modes need a full traceback
   1287             return VerboseTB.structured_traceback(
-> 1288                 self, etype, value, tb, tb_offset, number_of_lines_of_context
   1289             )
   1290         elif mode == 'Minimal':

~\Anaconda3\lib\site-packages\IPython\core\ultratb.py in structured_traceback(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)
   1148         exception = self.get_parts_of_chained_exception(evalue)
   1149         if exception:
-> 1150             formatted_exceptions += self.prepare_chained_exception_message(evalue.__cause__)
   1151             etype, evalue, etb = exception
   1152         else:

TypeError: can only concatenate str (not ""list"") to str
"
41241,Anaconda Jupyter Notebook - Tensorflow ImportError: DLL load failed: The specified module could not be found,"Hello Team, 

Please see my issue below. I'm working in Jupyter Notebooks launched within Anaconda to work on a computer vision project. 

Would it be helpful for me to share all the packages I have in my particular environment just in case there's a conflict that's not immediately obvious? I have 137 in total. I'm a casual user.

![image](https://user-images.githubusercontent.com/44557668/87065048-266c1100-c1d6-11ea-941c-7f1295275cd2.png)
![image](https://user-images.githubusercontent.com/44557668/87065138-44397600-c1d6-11ea-9a1c-e9dd47aab698.png)
![image](https://user-images.githubusercontent.com/44557668/87065204-5b786380-c1d6-11ea-8d2b-5be360e9d448.png)
![image](https://user-images.githubusercontent.com/44557668/87065294-79de5f00-c1d6-11ea-8169-98f302e3b0b4.png)
![image](https://user-images.githubusercontent.com/44557668/87065335-882c7b00-c1d6-11ea-8c2d-6eb701e45667.png)
![image](https://user-images.githubusercontent.com/44557668/87065422-9da1a500-c1d6-11ea-8fb6-e367db18bb5b.png)
![image](https://user-images.githubusercontent.com/44557668/87065483-b0b47500-c1d6-11ea-95a1-7a4430730d88.png)
![image](https://user-images.githubusercontent.com/44557668/87065521-bdd16400-c1d6-11ea-9773-ed034cbea9ba.png)


### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: None
-   **TensorFlow installed from (source or binary)**: pip install
-   **TensorFlow version (use command below)**: 2.2.0
-   **Python version**: 3.7.7
-   **Bazel version (if compiling from source)**: Not installed
-   **GCC/Compiler version (if compiling from source)**: Unsure
-   **CUDA/cuDNN version**: unsure
-   **GPU model and memory**: Intel(R) HD Graphics 520
-   **Exact command to reproduce**: import tensorflow as tf

### Describe the problem
I cannot import tensorflow in my jupyter notebook. I have a specific environment created to within Anaconda Navigator to handle computer vision packages

### Source code / logs

>%pylab notebook
>import cv2
>import tensorflow as tf
>import pytesseract as ptess

---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
~\anaconda3\envs\Tigers Football Image Processin\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59 

~\anaconda3\envs\Tigers Football Image Processin\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in <module>
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

~\anaconda3\envs\Tigers Football Image Processin\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

~\anaconda3\envs\Tigers Football Image Processin\lib\imp.py in load_module(name, file, filename, details)
    241         else:
--> 242             return load_dynamic(name, filename, file)
    243     elif type_ == PKG_DIRECTORY:

~\anaconda3\envs\Tigers Football Image Processin\lib\imp.py in load_dynamic(name, path, file)
    341             name=name, loader=loader, origin=path)
--> 342         return _load(spec)
    343 

ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-5-fe1814e2e27b> in <module>
      1 import cv2
----> 2 import tensorflow as tf
      3 import dlib
      4 import pytesseract as ptess

~\anaconda3\envs\Tigers Football Image Processin\lib\site-packages\tensorflow\__init__.py in <module>
     39 import sys as _sys
     40 
---> 41 from tensorflow.python.tools import module_util as _module_util
     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader
     43 

~\anaconda3\envs\Tigers Football Image Processin\lib\site-packages\tensorflow\python\__init__.py in <module>
     48 import numpy as np
     49 
---> 50 from tensorflow.python import pywrap_tensorflow
     51 
     52 # Protocol buffers

~\anaconda3\envs\Tigers Football Image Processin\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     67 for some common reasons and solutions.  Include the entire stack trace
     68 above this error message when asking for help."""""" % traceback.format_exc()
---> 69   raise ImportError(msg)
     70 
     71 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""C:\Users\jimmy\anaconda3\envs\Tigers Football Image Processin\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\jimmy\anaconda3\envs\Tigers Football Image Processin\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\jimmy\anaconda3\envs\Tigers Football Image Processin\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\jimmy\anaconda3\envs\Tigers Football Image Processin\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\jimmy\anaconda3\envs\Tigers Football Image Processin\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

Thank you, 

Jimmy "
41240,Hard Swish support in TFLite Micro,"@tensorflow/micro

While support for the HARD_SWISH operation has been added to TFLite Micro, there's no corresponding layer in TensorFlow's and Keras's APIs. 
After converting a TensorFlow/Keras model containing HARD_SWISH layers to TFLite, how do I instruct the TFLite interpreter to use the custom implementation of HARD_SWISH available in [tensorflow/lite/micro/kernels/hard_swish.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/kernels/hard_swish.cc)?
I know that `hard_swish(x)` could be implemented using Keras's ReLU layer as `x * (relu6(x + 3) / 6)`, but I guess it wouldn't take advantage of its dedicated implementation for microcontrollers."
41239,Error in TF 2.3.0rc0/1 when mixing eager and non-eager Keras models,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.0rc1
- Python version: 3.6
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the current behavior**

Mixing eager and non-eager models in TensorFlow 2.3.0 results in an error.

**Describe the expected behavior**

There should be no error, as in TensorFlow<2.3.

**Standalone code to reproduce the issue**

``` python
import tensorflow as tf
import numpy as np

DO_BUG = True

inputs = tf.keras.Input((1,))
outputs = tf.keras.layers.Dense(10)(inputs)
model0 = tf.keras.Model(inputs=inputs, outputs=outputs)

if DO_BUG:
    with tf.Graph().as_default():
        inputs = tf.keras.Input((1,))
        outputs = tf.keras.layers.Dense(10)(inputs)
        model1 = tf.keras.Model(inputs=inputs, outputs=outputs)

model0.compile(optimizer=tf.optimizers.SGD(0.1), loss=tf.losses.mse)
model0.fit(np.zeros((4, 1)), np.zeros((4, 10)))
```

**Other info / logs**
```
Traceback (most recent call last):
  File "".../tmp.py"", line 15, in <module>
    model0.fit(np.zeros((4, 1)), np.zeros((4, 10)))
  File ""...\tensorflow\python\keras\engine\training_v1.py"", line 807, in fit
    use_multiprocessing=use_multiprocessing)
  File ""...\tensorflow\python\keras\engine\training_arrays.py"", line 666, in fit
    steps_name='steps_per_epoch')
  File ""...\tensorflow\python\keras\engine\training_arrays.py"", line 189, in model_iteration
    f = _make_execution_function(model, mode)
  File ""...\tensorflow\python\keras\engine\training_arrays.py"", line 557, in _make_execution_function
    return model._make_execution_function(mode)
  File ""...\tensorflow\python\keras\engine\training_v1.py"", line 2072, in _make_execution_function
    self._make_train_function()
  File ""...\tensorflow\python\keras\engine\training_v1.py"", line 2021, in _make_train_function
    **self._function_kwargs)
  File ""...\tensorflow\python\keras\backend.py"", line 3933, in function
    'eager execution. You passed: %s' % (updates,))
ValueError: `updates` argument is not supported during eager execution. You passed: [<tf.Operation 'training/SGD/SGD/AssignAddVariableOp' type=AssignAddVariableOp>]
```

"
41238,ModuleNotFoundError: No module named 'tensorflow' on windows 8.1,"Tensorflow is successfully installed on my laptop.
`Name: tensorflow
Version: 2.2.0
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author: Google Inc.
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: c:\users\my name\anaconda3\lib\site-packages
Requires: protobuf, astunparse, h5py, opt-einsum, wrapt, grpcio, termcolor, gast, tensorflow-estimator, scipy, six, tensorboard, wheel, google-pasta, numpy, keras-preprocessing, absl-py
Required-by:`

The environment is Windows 8.1, python 3.7.1, pip 20.1.1.

When I run this test file
`import tensorflow as tf
hello = tf.constant(""hello TensorFlow!"")
sess=tf.Session()
print(sess.run(hello))`

, I get the following error message:
`ModuleNotFoundError: No module named 'tensorflow'.`
How can I resolve this problem?"
41237,tflite_with_xnnpack=true,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. aarch64 ) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version:2.2
- Python version:3.6.9
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):2.0.0
- GCC/Compiler version (if compiling from source):
 gcc --version
gcc (Ubuntu 9.3.0-11ubuntu0~18.04.1) 9.3.0
Copyright (C) 2019 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


- CUDA/cuDNN version: No
- GPU model and memory: No



**Describe the problem**

bazel-out/aarch64-opt-exec-50AE0418/bin/_solib_aarch64/_U_S_Stensorflow_Spython_Cgen_Ustate_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so.2: error: undefined reference to 'aws_checksums_do_cpu_id'
bazel-out/aarch64-opt-exec-50AE0418/bin/_solib_aarch64/_U_S_Stensorflow_Spython_Cgen_Ustate_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so.2: error: undefined reference to 'aws_checksums_crc32c_hw'
collect2: error: ld returned 1 exit status
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 8709.447s, Critical Path: 152.23s
INFO: 2105 processes: 2105 local.
FAILED: Build did NOT complete successfully

**Provide the exact sequence of commands / steps that you executed before running into the problem**

bazel build --define tflite_with_xnnpack=true //tensorflow/tools/pip_package:build_pip_package --discard_analysis_cache --notrack_incremental_state --jobs=1

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
bazel-out/aarch64-opt-exec-50AE0418/bin/_solib_aarch64/_U_S_Stensorflow_Spython_Cgen_Ustate_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so.2: error: undefined reference to 'aws_checksums_do_cpu_id'
bazel-out/aarch64-opt-exec-50AE0418/bin/_solib_aarch64/_U_S_Stensorflow_Spython_Cgen_Ustate_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so.2: error: undefined reference to 'aws_checksums_crc32c_hw'
collect2: error: ld returned 1 exit status
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 8709.447s, Critical Path: 152.23s
INFO: 2105 processes: 2105 local.
FAILED: Build did NOT complete successfully
"
41234,TF2 add a reduce or aggregation key to tf.scatter_nd,"**System information**
- TensorFlow version (you are using): `2.2.0`
- Are you willing to contribute it (Yes/No): maybe

**Describe the feature and the current behavior/state.**
`tf.scatter_nd` currently sum the updates having the same indices.

I would like to be able to provide a different tensor reduce or aggregation function.
Keeping the current default to a sum :
```python
import tensorflow as tf

indices = tf.constant([[0], [0]], dtype=tf.int32)
updates = tf.constant([[0, 1], [1, 2]])
tf.assert_equal(
    tf.scatter_nd(indices, updates, (3, 2)), # default
    tf.scatter_nd(indices, updates, (3, 2), reduce=lambda current, new: current + new),
)
tf.assert_equal(
    tf.scatter_nd(indices, updates, (3, 2)), # default
    tf.scatter_nd(indices, updates, (3, 2), agg=lambda batched_update: tf.reduce_sum(batched_update, axis=0)
)
```


**Will this change the current api? How?**
It will add new key to `scatter_nd` and `scatter_nd_update`


**Who will benefit with this feature?**
People which will use `tf.scatter_nd`.

I think it will be especially useful to implement some SotA detection algorithms.
For example, for yolo boxes `[N_boxes, (x, y, h, w, o, c1, c2, ..., cn)]` are scattered in a kind of sparse Tensor to mimic the network output `[batch_size, y_grid_index, x_grid_index, anchor_n, (x, y, h, w, o, c1, c2, ..., cn)]`.
In the current situation, if 2 objects / boxes are similar (same anchor) and positionned nearby, they will hit the same position on the output grid and be summed.
The sum does not make any sense in that case :
- x, y, h, w are ~ summed. This signifies moving the boxe after positioning it : first or a mean will be better
- o is summed. It should stay 1 : the value is a bolean indicating if there is one element or not. First or reduce_any will be better.
- c1...cn are similar to o.
This happens for overlapping elements and may greatly improve the handling of this situation.

**Any Other info.**

Referenced here : https://github.com/tensorflow/tensorflow/issues/32235, without a need.
`tf.scatter_nd_update` is probably impacted"
41233,Keras 'Model' object has no attribute '_callable_losses',"- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS inux release 7.6.1810
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14.0
- Python version: 3.7.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: V9.1.85
- GPU model and memory: Tesla K80 24C

I have a class with a model and I want to add a custom loss with three arguments.  Upon building the model, the following error is raised:
```
<ipython-input-124-46b9e7f916ce> in _build(self)
     50 
     51         self.get_optimiser()
---> 52         model = self.get_loss(inputs, x)
     53 
     54         return model

<ipython-input-124-46b9e7f916ce> in get_loss(self, inputs, outputs)
    151             model = Model(inputs=[inputs, y_true, is_weight], outputs=[outputs])
    152 
--> 153             model.add_loss(weighted_dice_loss(y_true, outputs, is_weight))
    154             self._model.compile(optimizer = self._optimiser, loss = None, metrics = [dice_coef])
    155 

~/miniconda3/envs/segment/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in add_loss(self, losses, inputs)
    899         eager_losses.append(_tag_unconditional(loss))
    900 
--> 901     self._callable_losses += callable_losses
    902 
    903     call_context = base_layer_utils.is_in_call_context()

AttributeError: 'Model' object has no attribute '_callable_losses'
```
The expected behaviour is, thus, that the model accepts this custom loss, since it has three arguments instead of two when calling unet._build() without getting thrown an error:
```
unet = UNet()
unet._build()

```

The minimum reproducible code is as follows:
```
class UNet():

    def __init__(self, **kwargs):
        
        self._input_shape = kwargs.get('input_shape', (12, 86, 98,1))
        self._blocks = kwargs.get('blocks', 2)
        self._layers = kwargs.get('layers', 8)
        self._n_filters = kwargs.get('n_filters', 16)
        self._patch = kwargs.get('patch', (3,3,3))
        self._activation = kwargs.get('activation', 'elu')
        self._activation_last = kwargs.get('activation_last', 'sigmoid')
        self._kernel_initializer = kwargs.get('kernel_initializer', 'glorot_normal')
        self._padding = kwargs.get('padding', 'same')
        self._learnrate = kwargs.get('learnrate',0.001)
        self._momentum = kwargs.get('momentum',0.99)
        self._decay = kwargs.get('decay',0.0)
        self._mode = kwargs.get('mode','train')
           
    def _build(self):
        
        # Initialise array to keep skip connections
        self.skips = []
        
        inputs = Input(shape = self._input_shape)
        
        x = self.first_layers(inputs)
        x = self.contractive_path(x)
        x = self.middle_path(x)
        x = self.expansive_path(x)
        
        self.get_optimiser()
        model = self.get_loss(inputs, x)
            
        return model
    
    def first_layers(self, inputs):
       
        layer = Conv3D(filters = self._n_filters, kernel_size = self._patch, activation = self._activation, kernel_initializer = self._kernel_initializer,
                padding = self._padding)(inputs)
        
        return layer
    
    def contractive_path(self, layer):

        for b in range(0,self._blocks):
            for i in range(0,self._layers):
                layer = Conv3D(filters = self._n_filters, kernel_size = self._patch, activation = self._activation, kernel_initializer = self._kernel_initializer,
                padding = self._padding)(layer)
            
            #append for later use in up-sampling
            self.skips.append(layer)
            
            #downsampling (using patch(2,2,2) and stride of 2, similar to MaxPooling3D but uses less parameters)
            layer = Conv3D(filters = self._n_filters, kernel_size = (2,2,2), strides = (2, 2, 2), activation = self._activation, kernel_initializer = self._kernel_initializer,
                padding = self._padding)(layer)

            #post-pooling, double number of filters
            self._n_filters = int(self._n_filters*2)
     
        return layer 
        
    def middle_path(self, layer):
   
        for i in range(0,self._layers):
            layer = Conv3D(filters = self._n_filters, kernel_size = self._patch, activation = self._activation, kernel_initializer = self._kernel_initializer,
                padding = self._padding)(layer)
        
        return layer

    def expansive_path(self, layer):

        for u in range(0,self._blocks):
            layer = UpSampling3D(size = (2,2,2), data_format = None)(layer) 
            
            # skip connection from DOWN_PATH
            concat_lr = self.skips[-1]

            layer, concat_lr = cropping_tensor(layer, concat_lr)
            layer = Concatenate()([layer, concat_lr])
            
            for i in range(0,(self._layers)):
                layer = Conv3D(filters = self._n_filters, kernel_size = self._patch, activation = self._activation, kernel_initializer = self._kernel_initializer,
                padding = self._padding)(layer)
                
            self._n_filters = int(self._n_filters/2)
            #print('UpBlock',str(u),' : end_n_filters ', str(n_filters))
            
            self.skips = self.skips[:-1] # get rid of last skip connection 
            
        ### output layer 
        y_pred = Conv3D(1, (1,1,1), activation = self._activation_last)(layer)    
        
        return y_pred
    
    def get_optimiser(self):
            self._optimiser = SGD(lr = self._learnrate, momentum = self._momentum, decay = self._decay, nesterov = False)
    
    def get_loss(self, inputs, outputs):
            
            y_true = Input(self._input_shape, name = 'y_true')
            is_weight = Input(self._input_shape, name = 'is_weight')

            model = Model(inputs=[inputs, y_true, is_weight], outputs=[outputs])
            
            model.add_loss(weighted_dice_loss(y_true, outputs, is_weight))
            self._model.compile(optimizer = self._optimiser, loss = None, metrics = [dice_coef])
        
        return self._model
```
The weighted_dice_loss function is defined as follows:
```
def weighted_dice_loss(y_true, y_pred, w):
    
    return -weighted_dice_coef(y_true, y_pred, w)
```"
41231,missing comma in script,"## URL(s) with the issue:

https://www.tensorflow.org/tfx/guide/build_tfx_pipeline#build_a_custom_pipeline

## Description of issue (what needs changing):

A comma needs to be added to the script used in step 3 at the end of line: data_path=DATA_PATH

### Submit a pull request?
YES

https://github.com/seyedrezamirkhani/tfx/pull/1

"
41230,Error transforming entity <function Model.make_predict_function.<locals>.predict_function at 0x0000018A825705E8>,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, but reproducible on example
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):                           Win 10
- TensorFlow installed from (source or binary):                                             pip install
- TensorFlow version (use command below):                                                v2.2.0-rc4-8-g2b96f3662b 2.2.0
- Python version:                                                                                         3.7.7
- Bazel version (if compiling from source):                                                    N/A
- GCC/Compiler version (if compiling from source):                                      N/A
- CUDA/cuDNN version:                                                                              V10.0.130 / v7.6.5.32
- GPU model and memory:                                                                         GTX1050Ti, 4Gb

**Describe the current behavior**
During training and prediction I get an ""Error transforming entity"". The error is not fatal. Training completes and prediction accuracy on a validation set is comparable to prediction in TF2.1. Reporting this because tensorflow requests it in trace.

**Describe the expected behavior**
During training and prediction I get no ""Error transforming entity"". I get no errors in TF1.15 nor TF2.1.

**Standalone code to reproduce the issue**
The cats and dogs code example at https://www.tensorflow.org/tutorials/images/classification produces the message as well, I just changed epochs to 1

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import os
import numpy as np
import matplotlib.pyplot as plt

os.environ['AUTOGRAPH_VERBOSITY'] = '10'

def plotImages(images_arr):
    fig, axes = plt.subplots(1, 5, figsize=(20,20))
    axes = axes.flatten()
    for img, ax in zip( images_arr, axes):
        ax.imshow(img)
        ax.axis('off')
    plt.tight_layout()
    plt.show()


_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'
path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)
PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')
train_cats_dir = os.path.join(train_dir, 'cats')  # directory with our training cat pictures
train_dogs_dir = os.path.join(train_dir, 'dogs')  # directory with our training dog pictures
validation_cats_dir = os.path.join(validation_dir, 'cats')  # directory with our validation cat pictures
validation_dogs_dir = os.path.join(validation_dir, 'dogs')  # directory with our validation dog pictures
num_cats_tr = len(os.listdir(train_cats_dir))
num_dogs_tr = len(os.listdir(train_dogs_dir))

num_cats_val = len(os.listdir(validation_cats_dir))
num_dogs_val = len(os.listdir(validation_dogs_dir))

total_train = num_cats_tr + num_dogs_tr
total_val = num_cats_val + num_dogs_val
print('total training cat images:', num_cats_tr)
print('total training dog images:', num_dogs_tr)

print('total validation cat images:', num_cats_val)
print('total validation dog images:', num_dogs_val)
print(""--"")
print(""Total training images:"", total_train)
print(""Total validation images:"", total_val)
batch_size = 128
epochs = 15
IMG_HEIGHT = 150
IMG_WIDTH = 150
train_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our training data
validation_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our validation data

train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,
                                                           directory=train_dir,
                                                           shuffle=True,
                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),
                                                           class_mode='binary')
val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,
                                                              directory=validation_dir,
                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),
                                                              class_mode='binary')

sample_training_images, _ = next(train_data_gen)

model = Sequential([
    Conv2D(16, 3, padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),
    MaxPooling2D(),
    Conv2D(32, 3, padding='same', activation='relu'),
    MaxPooling2D(),
    Conv2D(64, 3, padding='same', activation='relu'),
    MaxPooling2D(),
    Flatten(),
    Dense(512, activation='relu'),
    Dense(1)
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])

model.summary()

history = model.fit_generator(
    train_data_gen,
    steps_per_epoch=total_train // batch_size,
    epochs=epochs,
    validation_data=val_data_gen,
    validation_steps=total_val // batch_size
)

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss=history.history['loss']
val_loss=history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()



**Other info / logs, error in bold** 
INFO:tensorflow:Converted call: <function DatasetV2.from_generator.<locals>.flat_map_fn at 0x0000020244B005E8>
    args: (<tf.Tensor 'args_0:0' shape=() dtype=int32>,)
    kwargs: {}

Converted call: <function DatasetV2.from_generator.<locals>.flat_map_fn at 0x0000020244B005E8>
    args: (<tf.Tensor 'args_0:0' shape=() dtype=int32>,)
    kwargs: {}

INFO:tensorflow:Whitelisted: <function DatasetV2.from_generator.<locals>.flat_map_fn at 0x0000020244B005E8>: DoNotConvert rule for tensorflow
Whitelisted: <function DatasetV2.from_generator.<locals>.flat_map_fn at 0x0000020244B005E8>: DoNotConvert rule for tensorflow
INFO:tensorflow:Converted call: <function DatasetV2.from_generator.<locals>.get_iterator_id_fn at 0x0000020244B00438>
    args: (<tf.Tensor 'args_0:0' shape=() dtype=int32>,)
    kwargs: {}

Converted call: <function DatasetV2.from_generator.<locals>.get_iterator_id_fn at 0x0000020244B00438>
    args: (<tf.Tensor 'args_0:0' shape=() dtype=int32>,)
    kwargs: {}

INFO:tensorflow:Whitelisted: <function DatasetV2.from_generator.<locals>.get_iterator_id_fn at 0x0000020244B00438>: DoNotConvert rule for tensorflow
Whitelisted: <function DatasetV2.from_generator.<locals>.get_iterator_id_fn at 0x0000020244B00438>: DoNotConvert rule for tensorflow
INFO:tensorflow:Converted call: <function DatasetV2.from_generator.<locals>.generator_next_fn at 0x0000020244B004C8>
    args: (<tf.Tensor 'args_0:0' shape=<unknown> dtype=int64>,)
    kwargs: {}

Converted call: <function DatasetV2.from_generator.<locals>.generator_next_fn at 0x0000020244B004C8>
    args: (<tf.Tensor 'args_0:0' shape=<unknown> dtype=int64>,)
    kwargs: {}

INFO:tensorflow:Whitelisted: <function DatasetV2.from_generator.<locals>.generator_next_fn at 0x0000020244B004C8>: DoNotConvert rule for tensorflow
Whitelisted: <function DatasetV2.from_generator.<locals>.generator_next_fn at 0x0000020244B004C8>: DoNotConvert rule for tensorflow
INFO:tensorflow:Converted call: <function DatasetV2.from_generator.<locals>.finalize_fn at 0x0000020244B00558>
    args: (<tf.Tensor 'args_0:0' shape=<unknown> dtype=int64>,)
    kwargs: {}

Converted call: <function DatasetV2.from_generator.<locals>.finalize_fn at 0x0000020244B00558>
    args: (<tf.Tensor 'args_0:0' shape=<unknown> dtype=int64>,)
    kwargs: {}

INFO:tensorflow:Whitelisted: <function DatasetV2.from_generator.<locals>.finalize_fn at 0x0000020244B00558>: DoNotConvert rule for tensorflow
Whitelisted: <function DatasetV2.from_generator.<locals>.finalize_fn at 0x0000020244B00558>: DoNotConvert rule for tensorflow
INFO:tensorflow:Converted call: <function Model.make_train_function.<locals>.train_function at 0x0000020244B00798>
    args: (<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x00000203638FB208>,)
    kwargs: {}

Converted call: <function Model.make_train_function.<locals>.train_function at 0x0000020244B00798>
    args: (<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x00000203638FB208>,)
    kwargs: {}

INFO:tensorflow:Cache hit for entity <function Model.make_train_function.<locals>.train_function at 0x0000020244B00798> subkey (<tensorflow.python.autograph.core.converter.ConversionOptions object at 0x00000203638FBCC8>, frozenset({'self'})): _ConvertedEntityFactoryInfo(tf__train_function in tmproefnuih)
Cache hit for entity <function Model.make_train_function.<locals>.train_function at 0x0000020244B00798> subkey (<tensorflow.python.autograph.core.converter.ConversionOptions object at 0x00000203638FBCC8>, frozenset({'self'})): _ConvertedEntityFactoryInfo(tf__train_function in tmproefnuih)
INFO:tensorflow:Error transforming entity <function Model.make_train_function.<locals>.train_function at 0x0000020244B00798>
Traceback (most recent call last):
  File ""C:\Users\coumansFAW\Miniconda3\envs\TF220\lib\site-packages\tensorflow\python\autograph\impl\api.py"", line 538, in converted_call
    converted_f = conversion.convert(target_entity, program_ctx)
  File ""C:\Users\coumansFAW\Miniconda3\envs\TF220\lib\site-packages\tensorflow\python\autograph\impl\conversion.py"", line 362, in convert
    return _instantiate(entity, converted_entity_info, free_nonglobal_var_names)
  File ""C:\Users\coumansFAW\Miniconda3\envs\TF220\lib\site-packages\tensorflow\python\autograph\impl\conversion.py"", line 300, in _instantiate
    factory = converted_entity_info.get_factory()
  File ""C:\Users\coumansFAW\Miniconda3\envs\TF220\lib\site-packages\tensorflow\python\autograph\impl\conversion.py"", line 94, in get_factory
    assert self.module_name in sys.modules
**AssertionError
Error transforming entity <function Model.make_train_function.<locals>.train_function at 0x0000020244B00798>
WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000020244B00798> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: 
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000020244B00798> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: 
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert**
INFO:tensorflow:Converted call: <bound method Model.train_step of <tensorflow.python.keras.engine.sequential.Sequential object at 0x00000202440228C8>>
    args: ((<tf.Tensor 'IteratorGetNext:0' shape=(None, None, None, None) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=float32>),)
    kwargs: {}

Converted call: <bound method Model.train_step of <tensorflow.python.keras.engine.sequential.Sequential object at 0x00000202440228C8>>
    args: ((<tf.Tensor 'IteratorGetNext:0' shape=(None, None, None, None) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=float32>),)
    kwargs: {}

INFO:tensorflow:Whitelisted <bound method Model.train_step of <tensorflow.python.keras.engine.sequential.Sequential object at 0x00000202440228C8>>: from cache
Whitelisted <bound method Model.train_step of <tensorflow.python.keras.engine.sequential.Sequential object at 0x00000202440228C8>>: from cache
INFO:tensorflow:Converted call: <function OptimizerV2._aggregate_gradients.<locals>.all_reduce_fn at 0x0000020244B0F9D8>
    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x000002025A2D4388>, ((<tf.Tensor 'gradient_tape/sequential_8/conv2d_24/Conv2DBackpropFilter:0' shape=(3, 3, 3, 16) dtype=float32>, <tf.Variable 'conv2d_24/kernel:0' shape=(3, 3, 3, 16) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_24/BiasAdd/BiasAddGrad:0' shape=(16,) dtype=float32>, <tf.Variable 'conv2d_24/bias:0' shape=(16,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_25/Conv2DBackpropFilter:0' shape=(3, 3, 16, 32) dtype=float32>, <tf.Variable 'conv2d_25/kernel:0' shape=(3, 3, 16, 32) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_25/BiasAdd/BiasAddGrad:0' shape=(32,) dtype=float32>, <tf.Variable 'conv2d_25/bias:0' shape=(32,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_26/Conv2DBackpropFilter:0' shape=(3, 3, 32, 64) dtype=float32>, <tf.Variable 'conv2d_26/kernel:0' shape=(3, 3, 32, 64) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_26/BiasAdd/BiasAddGrad:0' shape=(64,) dtype=float32>, <tf.Variable 'conv2d_26/bias:0' shape=(64,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_16/MatMul_1:0' shape=(20736, 512) dtype=float32>, <tf.Variable 'dense_16/kernel:0' shape=(20736, 512) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_16/BiasAdd/BiasAddGrad:0' shape=(512,) dtype=float32>, <tf.Variable 'dense_16/bias:0' shape=(512,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_17/MatMul_1:0' shape=(512, 1) dtype=float32>, <tf.Variable 'dense_17/kernel:0' shape=(512, 1) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_17/BiasAdd/BiasAddGrad:0' shape=(1,) dtype=float32>, <tf.Variable 'dense_17/bias:0' shape=(1,) dtype=float32>)))
    kwargs: {}

Converted call: <function OptimizerV2._aggregate_gradients.<locals>.all_reduce_fn at 0x0000020244B0F9D8>
    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x000002025A2D4388>, ((<tf.Tensor 'gradient_tape/sequential_8/conv2d_24/Conv2DBackpropFilter:0' shape=(3, 3, 3, 16) dtype=float32>, <tf.Variable 'conv2d_24/kernel:0' shape=(3, 3, 3, 16) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_24/BiasAdd/BiasAddGrad:0' shape=(16,) dtype=float32>, <tf.Variable 'conv2d_24/bias:0' shape=(16,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_25/Conv2DBackpropFilter:0' shape=(3, 3, 16, 32) dtype=float32>, <tf.Variable 'conv2d_25/kernel:0' shape=(3, 3, 16, 32) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_25/BiasAdd/BiasAddGrad:0' shape=(32,) dtype=float32>, <tf.Variable 'conv2d_25/bias:0' shape=(32,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_26/Conv2DBackpropFilter:0' shape=(3, 3, 32, 64) dtype=float32>, <tf.Variable 'conv2d_26/kernel:0' shape=(3, 3, 32, 64) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_26/BiasAdd/BiasAddGrad:0' shape=(64,) dtype=float32>, <tf.Variable 'conv2d_26/bias:0' shape=(64,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_16/MatMul_1:0' shape=(20736, 512) dtype=float32>, <tf.Variable 'dense_16/kernel:0' shape=(20736, 512) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_16/BiasAdd/BiasAddGrad:0' shape=(512,) dtype=float32>, <tf.Variable 'dense_16/bias:0' shape=(512,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_17/MatMul_1:0' shape=(512, 1) dtype=float32>, <tf.Variable 'dense_17/kernel:0' shape=(512, 1) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_17/BiasAdd/BiasAddGrad:0' shape=(1,) dtype=float32>, <tf.Variable 'dense_17/bias:0' shape=(1,) dtype=float32>)))
    kwargs: {}

INFO:tensorflow:Whitelisted: <function OptimizerV2._aggregate_gradients.<locals>.all_reduce_fn at 0x0000020244B0F9D8>: DoNotConvert rule for tensorflow
Whitelisted: <function OptimizerV2._aggregate_gradients.<locals>.all_reduce_fn at 0x0000020244B0F9D8>: DoNotConvert rule for tensorflow
INFO:tensorflow:Converted call: functools.partial(<bound method OptimizerV2._distributed_apply of <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x0000020394F55B08>>, apply_state={('/job:localhost/replica:0/task:0/device:GPU:0', tf.float32): {'lr_t': <tf.Tensor 'Adam/Identity:0' shape=() dtype=float32>, 'lr': <tf.Tensor 'Adam/mul:0' shape=() dtype=float32>, 'epsilon': <tf.Tensor 'Adam/Const:0' shape=() dtype=float32>, 'beta_1_t': <tf.Tensor 'Adam/Identity_1:0' shape=() dtype=float32>, 'beta_1_power': <tf.Tensor 'Adam/Pow:0' shape=() dtype=float32>, 'one_minus_beta_1_t': <tf.Tensor 'Adam/sub_2:0' shape=() dtype=float32>, 'beta_2_t': <tf.Tensor 'Adam/Identity_2:0' shape=() dtype=float32>, 'beta_2_power': <tf.Tensor 'Adam/Pow_1:0' shape=() dtype=float32>, 'one_minus_beta_2_t': <tf.Tensor 'Adam/sub_3:0' shape=() dtype=float32>}})
    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x000002025A2D4388>, ((<tf.Tensor 'gradient_tape/sequential_8/conv2d_24/Conv2DBackpropFilter:0' shape=(3, 3, 3, 16) dtype=float32>, <tf.Variable 'conv2d_24/kernel:0' shape=(3, 3, 3, 16) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_24/BiasAdd/BiasAddGrad:0' shape=(16,) dtype=float32>, <tf.Variable 'conv2d_24/bias:0' shape=(16,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_25/Conv2DBackpropFilter:0' shape=(3, 3, 16, 32) dtype=float32>, <tf.Variable 'conv2d_25/kernel:0' shape=(3, 3, 16, 32) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_25/BiasAdd/BiasAddGrad:0' shape=(32,) dtype=float32>, <tf.Variable 'conv2d_25/bias:0' shape=(32,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_26/Conv2DBackpropFilter:0' shape=(3, 3, 32, 64) dtype=float32>, <tf.Variable 'conv2d_26/kernel:0' shape=(3, 3, 32, 64) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_26/BiasAdd/BiasAddGrad:0' shape=(64,) dtype=float32>, <tf.Variable 'conv2d_26/bias:0' shape=(64,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_16/MatMul_1:0' shape=(20736, 512) dtype=float32>, <tf.Variable 'dense_16/kernel:0' shape=(20736, 512) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_16/BiasAdd/BiasAddGrad:0' shape=(512,) dtype=float32>, <tf.Variable 'dense_16/bias:0' shape=(512,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_17/MatMul_1:0' shape=(512, 1) dtype=float32>, <tf.Variable 'dense_17/kernel:0' shape=(512, 1) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_17/BiasAdd/BiasAddGrad:0' shape=(1,) dtype=float32>, <tf.Variable 'dense_17/bias:0' shape=(1,) dtype=float32>)))
    kwargs: {'name': None}

Converted call: functools.partial(<bound method OptimizerV2._distributed_apply of <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x0000020394F55B08>>, apply_state={('/job:localhost/replica:0/task:0/device:GPU:0', tf.float32): {'lr_t': <tf.Tensor 'Adam/Identity:0' shape=() dtype=float32>, 'lr': <tf.Tensor 'Adam/mul:0' shape=() dtype=float32>, 'epsilon': <tf.Tensor 'Adam/Const:0' shape=() dtype=float32>, 'beta_1_t': <tf.Tensor 'Adam/Identity_1:0' shape=() dtype=float32>, 'beta_1_power': <tf.Tensor 'Adam/Pow:0' shape=() dtype=float32>, 'one_minus_beta_1_t': <tf.Tensor 'Adam/sub_2:0' shape=() dtype=float32>, 'beta_2_t': <tf.Tensor 'Adam/Identity_2:0' shape=() dtype=float32>, 'beta_2_power': <tf.Tensor 'Adam/Pow_1:0' shape=() dtype=float32>, 'one_minus_beta_2_t': <tf.Tensor 'Adam/sub_3:0' shape=() dtype=float32>}})
    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x000002025A2D4388>, ((<tf.Tensor 'gradient_tape/sequential_8/conv2d_24/Conv2DBackpropFilter:0' shape=(3, 3, 3, 16) dtype=float32>, <tf.Variable 'conv2d_24/kernel:0' shape=(3, 3, 3, 16) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_24/BiasAdd/BiasAddGrad:0' shape=(16,) dtype=float32>, <tf.Variable 'conv2d_24/bias:0' shape=(16,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_25/Conv2DBackpropFilter:0' shape=(3, 3, 16, 32) dtype=float32>, <tf.Variable 'conv2d_25/kernel:0' shape=(3, 3, 16, 32) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_25/BiasAdd/BiasAddGrad:0' shape=(32,) dtype=float32>, <tf.Variable 'conv2d_25/bias:0' shape=(32,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_26/Conv2DBackpropFilter:0' shape=(3, 3, 32, 64) dtype=float32>, <tf.Variable 'conv2d_26/kernel:0' shape=(3, 3, 32, 64) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_26/BiasAdd/BiasAddGrad:0' shape=(64,) dtype=float32>, <tf.Variable 'conv2d_26/bias:0' shape=(64,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_16/MatMul_1:0' shape=(20736, 512) dtype=float32>, <tf.Variable 'dense_16/kernel:0' shape=(20736, 512) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_16/BiasAdd/BiasAddGrad:0' shape=(512,) dtype=float32>, <tf.Variable 'dense_16/bias:0' shape=(512,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_17/MatMul_1:0' shape=(512, 1) dtype=float32>, <tf.Variable 'dense_17/kernel:0' shape=(512, 1) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_17/BiasAdd/BiasAddGrad:0' shape=(1,) dtype=float32>, <tf.Variable 'dense_17/bias:0' shape=(1,) dtype=float32>)))
    kwargs: {'name': None}

INFO:tensorflow:Forwarding call of partial functools.partial(<bound method OptimizerV2._distributed_apply of <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x0000020394F55B08>>, apply_state={('/job:localhost/replica:0/task:0/device:GPU:0', tf.float32): {'lr_t': <tf.Tensor 'Adam/Identity:0' shape=() dtype=float32>, 'lr': <tf.Tensor 'Adam/mul:0' shape=() dtype=float32>, 'epsilon': <tf.Tensor 'Adam/Const:0' shape=() dtype=float32>, 'beta_1_t': <tf.Tensor 'Adam/Identity_1:0' shape=() dtype=float32>, 'beta_1_power': <tf.Tensor 'Adam/Pow:0' shape=() dtype=float32>, 'one_minus_beta_1_t': <tf.Tensor 'Adam/sub_2:0' shape=() dtype=float32>, 'beta_2_t': <tf.Tensor 'Adam/Identity_2:0' shape=() dtype=float32>, 'beta_2_power': <tf.Tensor 'Adam/Pow_1:0' shape=() dtype=float32>, 'one_minus_beta_2_t': <tf.Tensor 'Adam/sub_3:0' shape=() dtype=float32>}}) with
(<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x000002025A2D4388>, ((<tf.Tensor 'gradient_tape/sequential_8/conv2d_24/Conv2DBackpropFilter:0' shape=(3, 3, 3, 16) dtype=float32>, <tf.Variable 'conv2d_24/kernel:0' shape=(3, 3, 3, 16) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_24/BiasAdd/BiasAddGrad:0' shape=(16,) dtype=float32>, <tf.Variable 'conv2d_24/bias:0' shape=(16,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_25/Conv2DBackpropFilter:0' shape=(3, 3, 16, 32) dtype=float32>, <tf.Variable 'conv2d_25/kernel:0' shape=(3, 3, 16, 32) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_25/BiasAdd/BiasAddGrad:0' shape=(32,) dtype=float32>, <tf.Variable 'conv2d_25/bias:0' shape=(32,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_26/Conv2DBackpropFilter:0' shape=(3, 3, 32, 64) dtype=float32>, <tf.Variable 'conv2d_26/kernel:0' shape=(3, 3, 32, 64) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_26/BiasAdd/BiasAddGrad:0' shape=(64,) dtype=float32>, <tf.Variable 'conv2d_26/bias:0' shape=(64,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_16/MatMul_1:0' shape=(20736, 512) dtype=float32>, <tf.Variable 'dense_16/kernel:0' shape=(20736, 512) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_16/BiasAdd/BiasAddGrad:0' shape=(512,) dtype=float32>, <tf.Variable 'dense_16/bias:0' shape=(512,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_17/MatMul_1:0' shape=(512, 1) dtype=float32>, <tf.Variable 'dense_17/kernel:0' shape=(512, 1) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_17/BiasAdd/BiasAddGrad:0' shape=(1,) dtype=float32>, <tf.Variable 'dense_17/bias:0' shape=(1,) dtype=float32>)))
{'apply_state': {('/job:localhost/replica:0/task:0/device:GPU:0', tf.float32): {'lr_t': <tf.Tensor 'Adam/Identity:0' shape=() dtype=float32>, 'lr': <tf.Tensor 'Adam/mul:0' shape=() dtype=float32>, 'epsilon': <tf.Tensor 'Adam/Const:0' shape=() dtype=float32>, 'beta_1_t': <tf.Tensor 'Adam/Identity_1:0' shape=() dtype=float32>, 'beta_1_power': <tf.Tensor 'Adam/Pow:0' shape=() dtype=float32>, 'one_minus_beta_1_t': <tf.Tensor 'Adam/sub_2:0' shape=() dtype=float32>, 'beta_2_t': <tf.Tensor 'Adam/Identity_2:0' shape=() dtype=float32>, 'beta_2_power': <tf.Tensor 'Adam/Pow_1:0' shape=() dtype=float32>, 'one_minus_beta_2_t': <tf.Tensor 'Adam/sub_3:0' shape=() dtype=float32>}}, 'name': None}

Forwarding call of partial functools.partial(<bound method OptimizerV2._distributed_apply of <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x0000020394F55B08>>, apply_state={('/job:localhost/replica:0/task:0/device:GPU:0', tf.float32): {'lr_t': <tf.Tensor 'Adam/Identity:0' shape=() dtype=float32>, 'lr': <tf.Tensor 'Adam/mul:0' shape=() dtype=float32>, 'epsilon': <tf.Tensor 'Adam/Const:0' shape=() dtype=float32>, 'beta_1_t': <tf.Tensor 'Adam/Identity_1:0' shape=() dtype=float32>, 'beta_1_power': <tf.Tensor 'Adam/Pow:0' shape=() dtype=float32>, 'one_minus_beta_1_t': <tf.Tensor 'Adam/sub_2:0' shape=() dtype=float32>, 'beta_2_t': <tf.Tensor 'Adam/Identity_2:0' shape=() dtype=float32>, 'beta_2_power': <tf.Tensor 'Adam/Pow_1:0' shape=() dtype=float32>, 'one_minus_beta_2_t': <tf.Tensor 'Adam/sub_3:0' shape=() dtype=float32>}}) with
(<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x000002025A2D4388>, ((<tf.Tensor 'gradient_tape/sequential_8/conv2d_24/Conv2DBackpropFilter:0' shape=(3, 3, 3, 16) dtype=float32>, <tf.Variable 'conv2d_24/kernel:0' shape=(3, 3, 3, 16) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_24/BiasAdd/BiasAddGrad:0' shape=(16,) dtype=float32>, <tf.Variable 'conv2d_24/bias:0' shape=(16,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_25/Conv2DBackpropFilter:0' shape=(3, 3, 16, 32) dtype=float32>, <tf.Variable 'conv2d_25/kernel:0' shape=(3, 3, 16, 32) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_25/BiasAdd/BiasAddGrad:0' shape=(32,) dtype=float32>, <tf.Variable 'conv2d_25/bias:0' shape=(32,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_26/Conv2DBackpropFilter:0' shape=(3, 3, 32, 64) dtype=float32>, <tf.Variable 'conv2d_26/kernel:0' shape=(3, 3, 32, 64) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_26/BiasAdd/BiasAddGrad:0' shape=(64,) dtype=float32>, <tf.Variable 'conv2d_26/bias:0' shape=(64,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_16/MatMul_1:0' shape=(20736, 512) dtype=float32>, <tf.Variable 'dense_16/kernel:0' shape=(20736, 512) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_16/BiasAdd/BiasAddGrad:0' shape=(512,) dtype=float32>, <tf.Variable 'dense_16/bias:0' shape=(512,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_17/MatMul_1:0' shape=(512, 1) dtype=float32>, <tf.Variable 'dense_17/kernel:0' shape=(512, 1) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_17/BiasAdd/BiasAddGrad:0' shape=(1,) dtype=float32>, <tf.Variable 'dense_17/bias:0' shape=(1,) dtype=float32>)))
{'apply_state': {('/job:localhost/replica:0/task:0/device:GPU:0', tf.float32): {'lr_t': <tf.Tensor 'Adam/Identity:0' shape=() dtype=float32>, 'lr': <tf.Tensor 'Adam/mul:0' shape=() dtype=float32>, 'epsilon': <tf.Tensor 'Adam/Const:0' shape=() dtype=float32>, 'beta_1_t': <tf.Tensor 'Adam/Identity_1:0' shape=() dtype=float32>, 'beta_1_power': <tf.Tensor 'Adam/Pow:0' shape=() dtype=float32>, 'one_minus_beta_1_t': <tf.Tensor 'Adam/sub_2:0' shape=() dtype=float32>, 'beta_2_t': <tf.Tensor 'Adam/Identity_2:0' shape=() dtype=float32>, 'beta_2_power': <tf.Tensor 'Adam/Pow_1:0' shape=() dtype=float32>, 'one_minus_beta_2_t': <tf.Tensor 'Adam/sub_3:0' shape=() dtype=float32>}}, 'name': None}

INFO:tensorflow:Converted call: <bound method OptimizerV2._distributed_apply of <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x0000020394F55B08>>
    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x000002025A2D4388>, ((<tf.Tensor 'gradient_tape/sequential_8/conv2d_24/Conv2DBackpropFilter:0' shape=(3, 3, 3, 16) dtype=float32>, <tf.Variable 'conv2d_24/kernel:0' shape=(3, 3, 3, 16) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_24/BiasAdd/BiasAddGrad:0' shape=(16,) dtype=float32>, <tf.Variable 'conv2d_24/bias:0' shape=(16,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_25/Conv2DBackpropFilter:0' shape=(3, 3, 16, 32) dtype=float32>, <tf.Variable 'conv2d_25/kernel:0' shape=(3, 3, 16, 32) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_25/BiasAdd/BiasAddGrad:0' shape=(32,) dtype=float32>, <tf.Variable 'conv2d_25/bias:0' shape=(32,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_26/Conv2DBackpropFilter:0' shape=(3, 3, 32, 64) dtype=float32>, <tf.Variable 'conv2d_26/kernel:0' shape=(3, 3, 32, 64) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_26/BiasAdd/BiasAddGrad:0' shape=(64,) dtype=float32>, <tf.Variable 'conv2d_26/bias:0' shape=(64,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_16/MatMul_1:0' shape=(20736, 512) dtype=float32>, <tf.Variable 'dense_16/kernel:0' shape=(20736, 512) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_16/BiasAdd/BiasAddGrad:0' shape=(512,) dtype=float32>, <tf.Variable 'dense_16/bias:0' shape=(512,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_17/MatMul_1:0' shape=(512, 1) dtype=float32>, <tf.Variable 'dense_17/kernel:0' shape=(512, 1) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_17/BiasAdd/BiasAddGrad:0' shape=(1,) dtype=float32>, <tf.Variable 'dense_17/bias:0' shape=(1,) dtype=float32>)))
    kwargs: {'apply_state': {('/job:localhost/replica:0/task:0/device:GPU:0', tf.float32): {'lr_t': <tf.Tensor 'Adam/Identity:0' shape=() dtype=float32>, 'lr': <tf.Tensor 'Adam/mul:0' shape=() dtype=float32>, 'epsilon': <tf.Tensor 'Adam/Const:0' shape=() dtype=float32>, 'beta_1_t': <tf.Tensor 'Adam/Identity_1:0' shape=() dtype=float32>, 'beta_1_power': <tf.Tensor 'Adam/Pow:0' shape=() dtype=float32>, 'one_minus_beta_1_t': <tf.Tensor 'Adam/sub_2:0' shape=() dtype=float32>, 'beta_2_t': <tf.Tensor 'Adam/Identity_2:0' shape=() dtype=float32>, 'beta_2_power': <tf.Tensor 'Adam/Pow_1:0' shape=() dtype=float32>, 'one_minus_beta_2_t': <tf.Tensor 'Adam/sub_3:0' shape=() dtype=float32>}}, 'name': None}

Converted call: <bound method OptimizerV2._distributed_apply of <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x0000020394F55B08>>
    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x000002025A2D4388>, ((<tf.Tensor 'gradient_tape/sequential_8/conv2d_24/Conv2DBackpropFilter:0' shape=(3, 3, 3, 16) dtype=float32>, <tf.Variable 'conv2d_24/kernel:0' shape=(3, 3, 3, 16) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_24/BiasAdd/BiasAddGrad:0' shape=(16,) dtype=float32>, <tf.Variable 'conv2d_24/bias:0' shape=(16,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_25/Conv2DBackpropFilter:0' shape=(3, 3, 16, 32) dtype=float32>, <tf.Variable 'conv2d_25/kernel:0' shape=(3, 3, 16, 32) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_25/BiasAdd/BiasAddGrad:0' shape=(32,) dtype=float32>, <tf.Variable 'conv2d_25/bias:0' shape=(32,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_26/Conv2DBackpropFilter:0' shape=(3, 3, 32, 64) dtype=float32>, <tf.Variable 'conv2d_26/kernel:0' shape=(3, 3, 32, 64) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_26/BiasAdd/BiasAddGrad:0' shape=(64,) dtype=float32>, <tf.Variable 'conv2d_26/bias:0' shape=(64,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_16/MatMul_1:0' shape=(20736, 512) dtype=float32>, <tf.Variable 'dense_16/kernel:0' shape=(20736, 512) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_16/BiasAdd/BiasAddGrad:0' shape=(512,) dtype=float32>, <tf.Variable 'dense_16/bias:0' shape=(512,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_17/MatMul_1:0' shape=(512, 1) dtype=float32>, <tf.Variable 'dense_17/kernel:0' shape=(512, 1) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_17/BiasAdd/BiasAddGrad:0' shape=(1,) dtype=float32>, <tf.Variable 'dense_17/bias:0' shape=(1,) dtype=float32>)))
    kwargs: {'apply_state': {('/job:localhost/replica:0/task:0/device:GPU:0', tf.float32): {'lr_t': <tf.Tensor 'Adam/Identity:0' shape=() dtype=float32>, 'lr': <tf.Tensor 'Adam/mul:0' shape=() dtype=float32>, 'epsilon': <tf.Tensor 'Adam/Const:0' shape=() dtype=float32>, 'beta_1_t': <tf.Tensor 'Adam/Identity_1:0' shape=() dtype=float32>, 'beta_1_power': <tf.Tensor 'Adam/Pow:0' shape=() dtype=float32>, 'one_minus_beta_1_t': <tf.Tensor 'Adam/sub_2:0' shape=() dtype=float32>, 'beta_2_t': <tf.Tensor 'Adam/Identity_2:0' shape=() dtype=float32>, 'beta_2_power': <tf.Tensor 'Adam/Pow_1:0' shape=() dtype=float32>, 'one_minus_beta_2_t': <tf.Tensor 'Adam/sub_3:0' shape=() dtype=float32>}}, 'name': None}

INFO:tensorflow:Whitelisted <bound method OptimizerV2._distributed_apply of <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x0000020394F55B08>>: from cache
Whitelisted <bound method OptimizerV2._distributed_apply of <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x0000020394F55B08>>: from cache
INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B103A8>
    args: (<tf.Variable 'conv2d_24/kernel:0' shape=(3, 3, 3, 16) dtype=float32>, <tf.Tensor 'gradient_tape/sequential_8/conv2d_24/Conv2DBackpropFilter:0' shape=(3, 3, 3, 16) dtype=float32>)
    kwargs: {}

Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B103A8>
    args: (<tf.Variable 'conv2d_24/kernel:0' shape=(3, 3, 3, 16) dtype=float32>, <tf.Tensor 'gradient_tape/sequential_8/conv2d_24/Conv2DBackpropFilter:0' shape=(3, 3, 3, 16) dtype=float32>)
    kwargs: {}

INFO:tensorflow:Whitelisted: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B103A8>: DoNotConvert rule for tensorflow
Whitelisted: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B103A8>: DoNotConvert rule for tensorflow
INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B103A8>
    args: (<tf.Variable 'conv2d_24/bias:0' shape=(16,) dtype=float32>, <tf.Tensor 'gradient_tape/sequential_8/conv2d_24/BiasAdd/BiasAddGrad:0' shape=(16,) dtype=float32>)
    kwargs: {}

Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B103A8>
    args: (<tf.Variable 'conv2d_24/bias:0' shape=(16,) dtype=float32>, <tf.Tensor 'gradient_tape/sequential_8/conv2d_24/BiasAdd/BiasAddGrad:0' shape=(16,) dtype=float32>)
    kwargs: {}

INFO:tensorflow:Whitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B103A8>: from cache
Whitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B103A8>: from cache
INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B103A8>
    args: (<tf.Variable 'conv2d_25/kernel:0' shape=(3, 3, 16, 32) dtype=float32>, <tf.Tensor 'gradient_tape/sequential_8/conv2d_25/Conv2DBackpropFilter:0' shape=(3, 3, 16, 32) dtype=float32>)
    kwargs: {}

Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B103A8>
    args: (<tf.Variable 'conv2d_25/kernel:0' shape=(3, 3, 16, 32) dtype=float32>, <tf.Tensor 'gradient_tape/sequential_8/conv2d_25/Conv2DBackpropFilter:0' shape=(3, 3, 16, 32) dtype=float32>)
    kwargs: {}

INFO:tensorflow:Whitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B103A8>: from cache
Whitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B103A8>: from cache
INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B103A8>
    args: (<tf.Variable 'conv2d_25/bias:0' shape=(32,) dtype=float32>, <tf.Tensor 'gradient_tape/sequential_8/conv2d_25/BiasAdd/BiasAddGrad:0' shape=(32,) dtype=float32>)
    kwargs: {}

Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B103A8>
    args: (<tf.Variable 'conv2d_25/bias:0' shape=(32,) dtype=float32>, <tf.Tensor 'gradient_tape/sequential_8/conv2d_25/BiasAdd/BiasAddGrad:0' shape=(32,) dtype=float32>)
    kwargs: {}

INFO:tensorflow:Whitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B103A8>: from cache
Whitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B103A8>: from cache
INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B103A8>
    args: (<tf.Variable 'conv2d_26/kernel:0' shape=(3, 3, 32, 64) dtype=float32>, <tf.Tensor 'gradient_tape/sequential_8/conv2d_26/Conv2DBackpropFilter:0' shape=(3, 3, 32, 64) dtype=float32>)
    kwargs: {}

Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B103A8>
    args: (<tf.Variable 'conv2d_26/kernel:0' shape=(3, 3, 32, 64) dtype=float32>, <tf.Tensor 'gradient_tape/sequential_8/conv2d_26/Conv2DBackpropFilter:0' shape=(3, 3, 32, 64) dtype=float32>)
    kwargs: {}

INFO:tensorflow:Whitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B103A8>: from cache
Whitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B103A8>: from cache
INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B103A8>
    args: (<tf.Variable 'conv2d_26/bias:0' shape=(64,) dtype=float32>, <tf.Tensor 'gradient_tape/sequential_8/conv2d_26/BiasAdd/BiasAddGrad:0' shape=(64,) dtype=float32>)
    kwargs: {}

Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B103A8>
    args: (<tf.Variable 'conv2d_26/bias:0' shape=(64,) dtype=float32>, <tf.Tensor 'gradient_tape/sequential_8/conv2d_26/BiasAdd/BiasAddGrad:0' shape=(64,) dtype=float32>)
    kwargs: {}

INFO:tensorflow:Whitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B103A8>: from cache
Whitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B103A8>: from cache
INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B103A8>
    args: (<tf.Variable 'dense_16/kernel:0' shape=(20736, 512) dtype=float32>, <tf.Tensor 'gradient_tape/sequential_8/dense_16/MatMul_1:0' shape=(20736, 512) dtype=float32>)
    kwargs: {}

Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B103A8>
    args: (<tf.Variable 'dense_16/kernel:0' shape=(20736, 512) dtype=float32>, <tf.Tensor 'gradient_tape/sequential_8/dense_16/MatMul_1:0' shape=(20736, 512) dtype=float32>)
    kwargs: {}

INFO:tensorflow:Whitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B103A8>: from cache
Whitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B103A8>: from cache
Traceback (most recent call last):
  File ""C:\Users\coumansFAW\Miniconda3\envs\TF220\lib\site-packages\tensorflow\python\autograph\impl\api.py"", line 538, in converted_call
    converted_f = conversion.convert(target_entity, program_ctx)
  File ""C:\Users\coumansFAW\Miniconda3\envs\TF220\lib\site-packages\tensorflow\python\autograph\impl\conversion.py"", line 362, in convert
    return _instantiate(entity, converted_entity_info, free_nonglobal_var_names)
  File ""C:\Users\coumansFAW\Miniconda3\envs\TF220\lib\site-packages\tensorflow\python\autograph\impl\conversion.py"", line 300, in _instantiate
    factory = converted_entity_info.get_factory()
  File ""C:\Users\coumansFAW\Miniconda3\envs\TF220\lib\site-packages\tensorflow\python\autograph\impl\conversion.py"", line 94, in get_factory
    assert self.module_name in sys.modules
AssertionError
INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B103A8>
    args: (<tf.Variable 'dense_16/bias:0' shape=(512,) dtype=float32>, <tf.Tensor 'gradient_tape/sequential_8/dense_16/BiasAdd/BiasAddGrad:0' shape=(512,) dtype=float32>)
    kwargs: {}

Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B103A8>
    args: (<tf.Variable 'dense_16/bias:0' shape=(512,) dtype=float32>, <tf.Tensor 'gradient_tape/sequential_8/dense_16/BiasAdd/BiasAddGrad:0' shape=(512,) dtype=float32>)
    kwargs: {}

INFO:tensorflow:Whitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B103A8>: from cache
Whitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B103A8>: from cache
INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B103A8>
    args: (<tf.Variable 'dense_17/kernel:0' shape=(512, 1) dtype=float32>, <tf.Tensor 'gradient_tape/sequential_8/dense_17/MatMul_1:0' shape=(512, 1) dtype=float32>)
    kwargs: {}

Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B103A8>
    args: (<tf.Variable 'dense_17/kernel:0' shape=(512, 1) dtype=float32>, <tf.Tensor 'gradient_tape/sequential_8/dense_17/MatMul_1:0' shape=(512, 1) dtype=float32>)
    kwargs: {}

INFO:tensorflow:Whitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B103A8>: from cache
Whitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B103A8>: from cache
INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B103A8>
    args: (<tf.Variable 'dense_17/bias:0' shape=(1,) dtype=float32>, <tf.Tensor 'gradient_tape/sequential_8/dense_17/BiasAdd/BiasAddGrad:0' shape=(1,) dtype=float32>)
    kwargs: {}

Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B103A8>
    args: (<tf.Variable 'dense_17/bias:0' shape=(1,) dtype=float32>, <tf.Tensor 'gradient_tape/sequential_8/dense_17/BiasAdd/BiasAddGrad:0' shape=(1,) dtype=float32>)
    kwargs: {}

INFO:tensorflow:Whitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B103A8>: from cache
Whitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B103A8>: from cache
INFO:tensorflow:Converted call: <function Model.make_train_function.<locals>.train_function at 0x0000020244B00798>
    args: (<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x000002038C6E5F08>,)
    kwargs: {}

Converted call: <function Model.make_train_function.<locals>.train_function at 0x0000020244B00798>
    args: (<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x000002038C6E5F08>,)
    kwargs: {}

INFO:tensorflow:Whitelisted <function Model.make_train_function.<locals>.train_function at 0x0000020244B00798>: from cache
Whitelisted <function Model.make_train_function.<locals>.train_function at 0x0000020244B00798>: from cache
INFO:tensorflow:Converted call: <bound method Model.train_step of <tensorflow.python.keras.engine.sequential.Sequential object at 0x00000202440228C8>>
    args: ((<tf.Tensor 'IteratorGetNext:0' shape=(None, None, None, None) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=float32>),)
    kwargs: {}

Converted call: <bound method Model.train_step of <tensorflow.python.keras.engine.sequential.Sequential object at 0x00000202440228C8>>
    args: ((<tf.Tensor 'IteratorGetNext:0' shape=(None, None, None, None) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=float32>),)
    kwargs: {}

INFO:tensorflow:Whitelisted <bound method Model.train_step of <tensorflow.python.keras.engine.sequential.Sequential object at 0x00000202440228C8>>: from cache
Whitelisted <bound method Model.train_step of <tensorflow.python.keras.engine.sequential.Sequential object at 0x00000202440228C8>>: from cache
INFO:tensorflow:Converted call: <function OptimizerV2._aggregate_gradients.<locals>.all_reduce_fn at 0x0000020244B10CA8>
    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x000002025A2D4388>, ((<tf.Tensor 'gradient_tape/sequential_8/conv2d_24/Conv2DBackpropFilter:0' shape=(3, 3, 3, 16) dtype=float32>, <tf.Variable 'conv2d_24/kernel:0' shape=(3, 3, 3, 16) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_24/BiasAdd/BiasAddGrad:0' shape=(16,) dtype=float32>, <tf.Variable 'conv2d_24/bias:0' shape=(16,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_25/Conv2DBackpropFilter:0' shape=(3, 3, 16, 32) dtype=float32>, <tf.Variable 'conv2d_25/kernel:0' shape=(3, 3, 16, 32) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_25/BiasAdd/BiasAddGrad:0' shape=(32,) dtype=float32>, <tf.Variable 'conv2d_25/bias:0' shape=(32,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_26/Conv2DBackpropFilter:0' shape=(3, 3, 32, 64) dtype=float32>, <tf.Variable 'conv2d_26/kernel:0' shape=(3, 3, 32, 64) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_26/BiasAdd/BiasAddGrad:0' shape=(64,) dtype=float32>, <tf.Variable 'conv2d_26/bias:0' shape=(64,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_16/MatMul_1:0' shape=(20736, 512) dtype=float32>, <tf.Variable 'dense_16/kernel:0' shape=(20736, 512) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_16/BiasAdd/BiasAddGrad:0' shape=(512,) dtype=float32>, <tf.Variable 'dense_16/bias:0' shape=(512,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_17/MatMul_1:0' shape=(512, 1) dtype=float32>, <tf.Variable 'dense_17/kernel:0' shape=(512, 1) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_17/BiasAdd/BiasAddGrad:0' shape=(1,) dtype=float32>, <tf.Variable 'dense_17/bias:0' shape=(1,) dtype=float32>)))
    kwargs: {}

Converted call: <function OptimizerV2._aggregate_gradients.<locals>.all_reduce_fn at 0x0000020244B10CA8>
    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x000002025A2D4388>, ((<tf.Tensor 'gradient_tape/sequential_8/conv2d_24/Conv2DBackpropFilter:0' shape=(3, 3, 3, 16) dtype=float32>, <tf.Variable 'conv2d_24/kernel:0' shape=(3, 3, 3, 16) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_24/BiasAdd/BiasAddGrad:0' shape=(16,) dtype=float32>, <tf.Variable 'conv2d_24/bias:0' shape=(16,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_25/Conv2DBackpropFilter:0' shape=(3, 3, 16, 32) dtype=float32>, <tf.Variable 'conv2d_25/kernel:0' shape=(3, 3, 16, 32) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_25/BiasAdd/BiasAddGrad:0' shape=(32,) dtype=float32>, <tf.Variable 'conv2d_25/bias:0' shape=(32,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_26/Conv2DBackpropFilter:0' shape=(3, 3, 32, 64) dtype=float32>, <tf.Variable 'conv2d_26/kernel:0' shape=(3, 3, 32, 64) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_26/BiasAdd/BiasAddGrad:0' shape=(64,) dtype=float32>, <tf.Variable 'conv2d_26/bias:0' shape=(64,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_16/MatMul_1:0' shape=(20736, 512) dtype=float32>, <tf.Variable 'dense_16/kernel:0' shape=(20736, 512) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_16/BiasAdd/BiasAddGrad:0' shape=(512,) dtype=float32>, <tf.Variable 'dense_16/bias:0' shape=(512,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_17/MatMul_1:0' shape=(512, 1) dtype=float32>, <tf.Variable 'dense_17/kernel:0' shape=(512, 1) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_17/BiasAdd/BiasAddGrad:0' shape=(1,) dtype=float32>, <tf.Variable 'dense_17/bias:0' shape=(1,) dtype=float32>)))
    kwargs: {}

INFO:tensorflow:Whitelisted: <function OptimizerV2._aggregate_gradients.<locals>.all_reduce_fn at 0x0000020244B10CA8>: DoNotConvert rule for tensorflow
Whitelisted: <function OptimizerV2._aggregate_gradients.<locals>.all_reduce_fn at 0x0000020244B10CA8>: DoNotConvert rule for tensorflow
INFO:tensorflow:Converted call: functools.partial(<bound method OptimizerV2._distributed_apply of <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x0000020394F55B08>>, apply_state={('/job:localhost/replica:0/task:0/device:GPU:0', tf.float32): {'lr_t': <tf.Tensor 'Adam/Identity:0' shape=() dtype=float32>, 'lr': <tf.Tensor 'Adam/mul:0' shape=() dtype=float32>, 'epsilon': <tf.Tensor 'Adam/Const:0' shape=() dtype=float32>, 'beta_1_t': <tf.Tensor 'Adam/Identity_1:0' shape=() dtype=float32>, 'beta_1_power': <tf.Tensor 'Adam/Pow:0' shape=() dtype=float32>, 'one_minus_beta_1_t': <tf.Tensor 'Adam/sub_2:0' shape=() dtype=float32>, 'beta_2_t': <tf.Tensor 'Adam/Identity_2:0' shape=() dtype=float32>, 'beta_2_power': <tf.Tensor 'Adam/Pow_1:0' shape=() dtype=float32>, 'one_minus_beta_2_t': <tf.Tensor 'Adam/sub_3:0' shape=() dtype=float32>}})
    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x000002025A2D4388>, ((<tf.Tensor 'gradient_tape/sequential_8/conv2d_24/Conv2DBackpropFilter:0' shape=(3, 3, 3, 16) dtype=float32>, <tf.Variable 'conv2d_24/kernel:0' shape=(3, 3, 3, 16) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_24/BiasAdd/BiasAddGrad:0' shape=(16,) dtype=float32>, <tf.Variable 'conv2d_24/bias:0' shape=(16,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_25/Conv2DBackpropFilter:0' shape=(3, 3, 16, 32) dtype=float32>, <tf.Variable 'conv2d_25/kernel:0' shape=(3, 3, 16, 32) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_25/BiasAdd/BiasAddGrad:0' shape=(32,) dtype=float32>, <tf.Variable 'conv2d_25/bias:0' shape=(32,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_26/Conv2DBackpropFilter:0' shape=(3, 3, 32, 64) dtype=float32>, <tf.Variable 'conv2d_26/kernel:0' shape=(3, 3, 32, 64) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_26/BiasAdd/BiasAddGrad:0' shape=(64,) dtype=float32>, <tf.Variable 'conv2d_26/bias:0' shape=(64,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_16/MatMul_1:0' shape=(20736, 512) dtype=float32>, <tf.Variable 'dense_16/kernel:0' shape=(20736, 512) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_16/BiasAdd/BiasAddGrad:0' shape=(512,) dtype=float32>, <tf.Variable 'dense_16/bias:0' shape=(512,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_17/MatMul_1:0' shape=(512, 1) dtype=float32>, <tf.Variable 'dense_17/kernel:0' shape=(512, 1) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_17/BiasAdd/BiasAddGrad:0' shape=(1,) dtype=float32>, <tf.Variable 'dense_17/bias:0' shape=(1,) dtype=float32>)))
    kwargs: {'name': None}

Converted call: functools.partial(<bound method OptimizerV2._distributed_apply of <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x0000020394F55B08>>, apply_state={('/job:localhost/replica:0/task:0/device:GPU:0', tf.float32): {'lr_t': <tf.Tensor 'Adam/Identity:0' shape=() dtype=float32>, 'lr': <tf.Tensor 'Adam/mul:0' shape=() dtype=float32>, 'epsilon': <tf.Tensor 'Adam/Const:0' shape=() dtype=float32>, 'beta_1_t': <tf.Tensor 'Adam/Identity_1:0' shape=() dtype=float32>, 'beta_1_power': <tf.Tensor 'Adam/Pow:0' shape=() dtype=float32>, 'one_minus_beta_1_t': <tf.Tensor 'Adam/sub_2:0' shape=() dtype=float32>, 'beta_2_t': <tf.Tensor 'Adam/Identity_2:0' shape=() dtype=float32>, 'beta_2_power': <tf.Tensor 'Adam/Pow_1:0' shape=() dtype=float32>, 'one_minus_beta_2_t': <tf.Tensor 'Adam/sub_3:0' shape=() dtype=float32>}})
    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x000002025A2D4388>, ((<tf.Tensor 'gradient_tape/sequential_8/conv2d_24/Conv2DBackpropFilter:0' shape=(3, 3, 3, 16) dtype=float32>, <tf.Variable 'conv2d_24/kernel:0' shape=(3, 3, 3, 16) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_24/BiasAdd/BiasAddGrad:0' shape=(16,) dtype=float32>, <tf.Variable 'conv2d_24/bias:0' shape=(16,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_25/Conv2DBackpropFilter:0' shape=(3, 3, 16, 32) dtype=float32>, <tf.Variable 'conv2d_25/kernel:0' shape=(3, 3, 16, 32) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_25/BiasAdd/BiasAddGrad:0' shape=(32,) dtype=float32>, <tf.Variable 'conv2d_25/bias:0' shape=(32,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_26/Conv2DBackpropFilter:0' shape=(3, 3, 32, 64) dtype=float32>, <tf.Variable 'conv2d_26/kernel:0' shape=(3, 3, 32, 64) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_26/BiasAdd/BiasAddGrad:0' shape=(64,) dtype=float32>, <tf.Variable 'conv2d_26/bias:0' shape=(64,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_16/MatMul_1:0' shape=(20736, 512) dtype=float32>, <tf.Variable 'dense_16/kernel:0' shape=(20736, 512) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_16/BiasAdd/BiasAddGrad:0' shape=(512,) dtype=float32>, <tf.Variable 'dense_16/bias:0' shape=(512,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_17/MatMul_1:0' shape=(512, 1) dtype=float32>, <tf.Variable 'dense_17/kernel:0' shape=(512, 1) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_17/BiasAdd/BiasAddGrad:0' shape=(1,) dtype=float32>, <tf.Variable 'dense_17/bias:0' shape=(1,) dtype=float32>)))
    kwargs: {'name': None}

INFO:tensorflow:Forwarding call of partial functools.partial(<bound method OptimizerV2._distributed_apply of <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x0000020394F55B08>>, apply_state={('/job:localhost/replica:0/task:0/device:GPU:0', tf.float32): {'lr_t': <tf.Tensor 'Adam/Identity:0' shape=() dtype=float32>, 'lr': <tf.Tensor 'Adam/mul:0' shape=() dtype=float32>, 'epsilon': <tf.Tensor 'Adam/Const:0' shape=() dtype=float32>, 'beta_1_t': <tf.Tensor 'Adam/Identity_1:0' shape=() dtype=float32>, 'beta_1_power': <tf.Tensor 'Adam/Pow:0' shape=() dtype=float32>, 'one_minus_beta_1_t': <tf.Tensor 'Adam/sub_2:0' shape=() dtype=float32>, 'beta_2_t': <tf.Tensor 'Adam/Identity_2:0' shape=() dtype=float32>, 'beta_2_power': <tf.Tensor 'Adam/Pow_1:0' shape=() dtype=float32>, 'one_minus_beta_2_t': <tf.Tensor 'Adam/sub_3:0' shape=() dtype=float32>}}) with
(<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x000002025A2D4388>, ((<tf.Tensor 'gradient_tape/sequential_8/conv2d_24/Conv2DBackpropFilter:0' shape=(3, 3, 3, 16) dtype=float32>, <tf.Variable 'conv2d_24/kernel:0' shape=(3, 3, 3, 16) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_24/BiasAdd/BiasAddGrad:0' shape=(16,) dtype=float32>, <tf.Variable 'conv2d_24/bias:0' shape=(16,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_25/Conv2DBackpropFilter:0' shape=(3, 3, 16, 32) dtype=float32>, <tf.Variable 'conv2d_25/kernel:0' shape=(3, 3, 16, 32) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_25/BiasAdd/BiasAddGrad:0' shape=(32,) dtype=float32>, <tf.Variable 'conv2d_25/bias:0' shape=(32,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_26/Conv2DBackpropFilter:0' shape=(3, 3, 32, 64) dtype=float32>, <tf.Variable 'conv2d_26/kernel:0' shape=(3, 3, 32, 64) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_26/BiasAdd/BiasAddGrad:0' shape=(64,) dtype=float32>, <tf.Variable 'conv2d_26/bias:0' shape=(64,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_16/MatMul_1:0' shape=(20736, 512) dtype=float32>, <tf.Variable 'dense_16/kernel:0' shape=(20736, 512) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_16/BiasAdd/BiasAddGrad:0' shape=(512,) dtype=float32>, <tf.Variable 'dense_16/bias:0' shape=(512,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_17/MatMul_1:0' shape=(512, 1) dtype=float32>, <tf.Variable 'dense_17/kernel:0' shape=(512, 1) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_17/BiasAdd/BiasAddGrad:0' shape=(1,) dtype=float32>, <tf.Variable 'dense_17/bias:0' shape=(1,) dtype=float32>)))
{'apply_state': {('/job:localhost/replica:0/task:0/device:GPU:0', tf.float32): {'lr_t': <tf.Tensor 'Adam/Identity:0' shape=() dtype=float32>, 'lr': <tf.Tensor 'Adam/mul:0' shape=() dtype=float32>, 'epsilon': <tf.Tensor 'Adam/Const:0' shape=() dtype=float32>, 'beta_1_t': <tf.Tensor 'Adam/Identity_1:0' shape=() dtype=float32>, 'beta_1_power': <tf.Tensor 'Adam/Pow:0' shape=() dtype=float32>, 'one_minus_beta_1_t': <tf.Tensor 'Adam/sub_2:0' shape=() dtype=float32>, 'beta_2_t': <tf.Tensor 'Adam/Identity_2:0' shape=() dtype=float32>, 'beta_2_power': <tf.Tensor 'Adam/Pow_1:0' shape=() dtype=float32>, 'one_minus_beta_2_t': <tf.Tensor 'Adam/sub_3:0' shape=() dtype=float32>}}, 'name': None}

Forwarding call of partial functools.partial(<bound method OptimizerV2._distributed_apply of <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x0000020394F55B08>>, apply_state={('/job:localhost/replica:0/task:0/device:GPU:0', tf.float32): {'lr_t': <tf.Tensor 'Adam/Identity:0' shape=() dtype=float32>, 'lr': <tf.Tensor 'Adam/mul:0' shape=() dtype=float32>, 'epsilon': <tf.Tensor 'Adam/Const:0' shape=() dtype=float32>, 'beta_1_t': <tf.Tensor 'Adam/Identity_1:0' shape=() dtype=float32>, 'beta_1_power': <tf.Tensor 'Adam/Pow:0' shape=() dtype=float32>, 'one_minus_beta_1_t': <tf.Tensor 'Adam/sub_2:0' shape=() dtype=float32>, 'beta_2_t': <tf.Tensor 'Adam/Identity_2:0' shape=() dtype=float32>, 'beta_2_power': <tf.Tensor 'Adam/Pow_1:0' shape=() dtype=float32>, 'one_minus_beta_2_t': <tf.Tensor 'Adam/sub_3:0' shape=() dtype=float32>}}) with
(<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x000002025A2D4388>, ((<tf.Tensor 'gradient_tape/sequential_8/conv2d_24/Conv2DBackpropFilter:0' shape=(3, 3, 3, 16) dtype=float32>, <tf.Variable 'conv2d_24/kernel:0' shape=(3, 3, 3, 16) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_24/BiasAdd/BiasAddGrad:0' shape=(16,) dtype=float32>, <tf.Variable 'conv2d_24/bias:0' shape=(16,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_25/Conv2DBackpropFilter:0' shape=(3, 3, 16, 32) dtype=float32>, <tf.Variable 'conv2d_25/kernel:0' shape=(3, 3, 16, 32) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_25/BiasAdd/BiasAddGrad:0' shape=(32,) dtype=float32>, <tf.Variable 'conv2d_25/bias:0' shape=(32,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_26/Conv2DBackpropFilter:0' shape=(3, 3, 32, 64) dtype=float32>, <tf.Variable 'conv2d_26/kernel:0' shape=(3, 3, 32, 64) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_26/BiasAdd/BiasAddGrad:0' shape=(64,) dtype=float32>, <tf.Variable 'conv2d_26/bias:0' shape=(64,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_16/MatMul_1:0' shape=(20736, 512) dtype=float32>, <tf.Variable 'dense_16/kernel:0' shape=(20736, 512) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_16/BiasAdd/BiasAddGrad:0' shape=(512,) dtype=float32>, <tf.Variable 'dense_16/bias:0' shape=(512,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_17/MatMul_1:0' shape=(512, 1) dtype=float32>, <tf.Variable 'dense_17/kernel:0' shape=(512, 1) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_17/BiasAdd/BiasAddGrad:0' shape=(1,) dtype=float32>, <tf.Variable 'dense_17/bias:0' shape=(1,) dtype=float32>)))
{'apply_state': {('/job:localhost/replica:0/task:0/device:GPU:0', tf.float32): {'lr_t': <tf.Tensor 'Adam/Identity:0' shape=() dtype=float32>, 'lr': <tf.Tensor 'Adam/mul:0' shape=() dtype=float32>, 'epsilon': <tf.Tensor 'Adam/Const:0' shape=() dtype=float32>, 'beta_1_t': <tf.Tensor 'Adam/Identity_1:0' shape=() dtype=float32>, 'beta_1_power': <tf.Tensor 'Adam/Pow:0' shape=() dtype=float32>, 'one_minus_beta_1_t': <tf.Tensor 'Adam/sub_2:0' shape=() dtype=float32>, 'beta_2_t': <tf.Tensor 'Adam/Identity_2:0' shape=() dtype=float32>, 'beta_2_power': <tf.Tensor 'Adam/Pow_1:0' shape=() dtype=float32>, 'one_minus_beta_2_t': <tf.Tensor 'Adam/sub_3:0' shape=() dtype=float32>}}, 'name': None}

INFO:tensorflow:Converted call: <bound method OptimizerV2._distributed_apply of <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x0000020394F55B08>>
    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x000002025A2D4388>, ((<tf.Tensor 'gradient_tape/sequential_8/conv2d_24/Conv2DBackpropFilter:0' shape=(3, 3, 3, 16) dtype=float32>, <tf.Variable 'conv2d_24/kernel:0' shape=(3, 3, 3, 16) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_24/BiasAdd/BiasAddGrad:0' shape=(16,) dtype=float32>, <tf.Variable 'conv2d_24/bias:0' shape=(16,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_25/Conv2DBackpropFilter:0' shape=(3, 3, 16, 32) dtype=float32>, <tf.Variable 'conv2d_25/kernel:0' shape=(3, 3, 16, 32) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_25/BiasAdd/BiasAddGrad:0' shape=(32,) dtype=float32>, <tf.Variable 'conv2d_25/bias:0' shape=(32,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_26/Conv2DBackpropFilter:0' shape=(3, 3, 32, 64) dtype=float32>, <tf.Variable 'conv2d_26/kernel:0' shape=(3, 3, 32, 64) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_26/BiasAdd/BiasAddGrad:0' shape=(64,) dtype=float32>, <tf.Variable 'conv2d_26/bias:0' shape=(64,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_16/MatMul_1:0' shape=(20736, 512) dtype=float32>, <tf.Variable 'dense_16/kernel:0' shape=(20736, 512) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_16/BiasAdd/BiasAddGrad:0' shape=(512,) dtype=float32>, <tf.Variable 'dense_16/bias:0' shape=(512,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_17/MatMul_1:0' shape=(512, 1) dtype=float32>, <tf.Variable 'dense_17/kernel:0' shape=(512, 1) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_17/BiasAdd/BiasAddGrad:0' shape=(1,) dtype=float32>, <tf.Variable 'dense_17/bias:0' shape=(1,) dtype=float32>)))
    kwargs: {'apply_state': {('/job:localhost/replica:0/task:0/device:GPU:0', tf.float32): {'lr_t': <tf.Tensor 'Adam/Identity:0' shape=() dtype=float32>, 'lr': <tf.Tensor 'Adam/mul:0' shape=() dtype=float32>, 'epsilon': <tf.Tensor 'Adam/Const:0' shape=() dtype=float32>, 'beta_1_t': <tf.Tensor 'Adam/Identity_1:0' shape=() dtype=float32>, 'beta_1_power': <tf.Tensor 'Adam/Pow:0' shape=() dtype=float32>, 'one_minus_beta_1_t': <tf.Tensor 'Adam/sub_2:0' shape=() dtype=float32>, 'beta_2_t': <tf.Tensor 'Adam/Identity_2:0' shape=() dtype=float32>, 'beta_2_power': <tf.Tensor 'Adam/Pow_1:0' shape=() dtype=float32>, 'one_minus_beta_2_t': <tf.Tensor 'Adam/sub_3:0' shape=() dtype=float32>}}, 'name': None}

Converted call: <bound method OptimizerV2._distributed_apply of <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x0000020394F55B08>>
    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x000002025A2D4388>, ((<tf.Tensor 'gradient_tape/sequential_8/conv2d_24/Conv2DBackpropFilter:0' shape=(3, 3, 3, 16) dtype=float32>, <tf.Variable 'conv2d_24/kernel:0' shape=(3, 3, 3, 16) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_24/BiasAdd/BiasAddGrad:0' shape=(16,) dtype=float32>, <tf.Variable 'conv2d_24/bias:0' shape=(16,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_25/Conv2DBackpropFilter:0' shape=(3, 3, 16, 32) dtype=float32>, <tf.Variable 'conv2d_25/kernel:0' shape=(3, 3, 16, 32) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_25/BiasAdd/BiasAddGrad:0' shape=(32,) dtype=float32>, <tf.Variable 'conv2d_25/bias:0' shape=(32,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_26/Conv2DBackpropFilter:0' shape=(3, 3, 32, 64) dtype=float32>, <tf.Variable 'conv2d_26/kernel:0' shape=(3, 3, 32, 64) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/conv2d_26/BiasAdd/BiasAddGrad:0' shape=(64,) dtype=float32>, <tf.Variable 'conv2d_26/bias:0' shape=(64,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_16/MatMul_1:0' shape=(20736, 512) dtype=float32>, <tf.Variable 'dense_16/kernel:0' shape=(20736, 512) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_16/BiasAdd/BiasAddGrad:0' shape=(512,) dtype=float32>, <tf.Variable 'dense_16/bias:0' shape=(512,) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_17/MatMul_1:0' shape=(512, 1) dtype=float32>, <tf.Variable 'dense_17/kernel:0' shape=(512, 1) dtype=float32>), (<tf.Tensor 'gradient_tape/sequential_8/dense_17/BiasAdd/BiasAddGrad:0' shape=(1,) dtype=float32>, <tf.Variable 'dense_17/bias:0' shape=(1,) dtype=float32>)))
    kwargs: {'apply_state': {('/job:localhost/replica:0/task:0/device:GPU:0', tf.float32): {'lr_t': <tf.Tensor 'Adam/Identity:0' shape=() dtype=float32>, 'lr': <tf.Tensor 'Adam/mul:0' shape=() dtype=float32>, 'epsilon': <tf.Tensor 'Adam/Const:0' shape=() dtype=float32>, 'beta_1_t': <tf.Tensor 'Adam/Identity_1:0' shape=() dtype=float32>, 'beta_1_power': <tf.Tensor 'Adam/Pow:0' shape=() dtype=float32>, 'one_minus_beta_1_t': <tf.Tensor 'Adam/sub_2:0' shape=() dtype=float32>, 'beta_2_t': <tf.Tensor 'Adam/Identity_2:0' shape=() dtype=float32>, 'beta_2_power': <tf.Tensor 'Adam/Pow_1:0' shape=() dtype=float32>, 'one_minus_beta_2_t': <tf.Tensor 'Adam/sub_3:0' shape=() dtype=float32>}}, 'name': None}

INFO:tensorflow:Whitelisted <bound method OptimizerV2._distributed_apply of <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x0000020394F55B08>>: from cache
Whitelisted <bound method OptimizerV2._distributed_apply of <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x0000020394F55B08>>: from cache
INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B10CA8>
    args: (<tf.Variable 'conv2d_24/kernel:0' shape=(3, 3, 3, 16) dtype=float32>, <tf.Tensor 'gradient_tape/sequential_8/conv2d_24/Conv2DBackpropFilter:0' shape=(3, 3, 3, 16) dtype=float32>)
    kwargs: {}

Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B10CA8>
    args: (<tf.Variable 'conv2d_24/kernel:0' shape=(3, 3, 3, 16) dtype=float32>, <tf.Tensor 'gradient_tape/sequential_8/conv2d_24/Conv2DBackpropFilter:0' shape=(3, 3, 3, 16) dtype=float32>)
    kwargs: {}

INFO:tensorflow:Whitelisted: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B10CA8>: DoNotConvert rule for tensorflow
Whitelisted: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B10CA8>: DoNotConvert rule for tensorflow
INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B10CA8>
    args: (<tf.Variable 'conv2d_24/bias:0' shape=(16,) dtype=float32>, <tf.Tensor 'gradient_tape/sequential_8/conv2d_24/BiasAdd/BiasAddGrad:0' shape=(16,) dtype=float32>)
    kwargs: {}

Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B10CA8>
    args: (<tf.Variable 'conv2d_24/bias:0' shape=(16,) dtype=float32>, <tf.Tensor 'gradient_tape/sequential_8/conv2d_24/BiasAdd/BiasAddGrad:0' shape=(16,) dtype=float32>)
    kwargs: {}

INFO:tensorflow:Whitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B10CA8>: from cache
Whitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B10CA8>: from cache
INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B10CA8>
    args: (<tf.Variable 'conv2d_25/kernel:0' shape=(3, 3, 16, 32) dtype=float32>, <tf.Tensor 'gradient_tape/sequential_8/conv2d_25/Conv2DBackpropFilter:0' shape=(3, 3, 16, 32) dtype=float32>)
    kwargs: {}

Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B10CA8>
    args: (<tf.Variable 'conv2d_25/kernel:0' shape=(3, 3, 16, 32) dtype=float32>, <tf.Tensor 'gradient_tape/sequential_8/conv2d_25/Conv2DBackpropFilter:0' shape=(3, 3, 16, 32) dtype=float32>)
    kwargs: {}

INFO:tensorflow:Whitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B10CA8>: from cache
Whitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B10CA8>: from cache
INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B10CA8>
    args: (<tf.Variable 'conv2d_25/bias:0' shape=(32,) dtype=float32>, <tf.Tensor 'gradient_tape/sequential_8/conv2d_25/BiasAdd/BiasAddGrad:0' shape=(32,) dtype=float32>)
    kwargs: {}

Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B10CA8>
    args: (<tf.Variable 'conv2d_25/bias:0' shape=(32,) dtype=float32>, <tf.Tensor 'gradient_tape/sequential_8/conv2d_25/BiasAdd/BiasAddGrad:0' shape=(32,) dtype=float32>)
    kwargs: {}

INFO:tensorflow:Whitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B10CA8>: from cache
Whitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B10CA8>: from cache
INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B10CA8>
    args: (<tf.Variable 'conv2d_26/kernel:0' shape=(3, 3, 32, 64) dtype=float32>, <tf.Tensor 'gradient_tape/sequential_8/conv2d_26/Conv2DBackpropFilter:0' shape=(3, 3, 32, 64) dtype=float32>)
    kwargs: {}

Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B10CA8>
    args: (<tf.Variable 'conv2d_26/kernel:0' shape=(3, 3, 32, 64) dtype=float32>, <tf.Tensor 'gradient_tape/sequential_8/conv2d_26/Conv2DBackpropFilter:0' shape=(3, 3, 32, 64) dtype=float32>)
    kwargs: {}

INFO:tensorflow:Whitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B10CA8>: from cache
Whitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B10CA8>: from cache
INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B10CA8>
    args: (<tf.Variable 'conv2d_26/bias:0' shape=(64,) dtype=float32>, <tf.Tensor 'gradient_tape/sequential_8/conv2d_26/BiasAdd/BiasAddGrad:0' shape=(64,) dtype=float32>)
    kwargs: {}

Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B10CA8>
    args: (<tf.Variable 'conv2d_26/bias:0' shape=(64,) dtype=float32>, <tf.Tensor 'gradient_tape/sequential_8/conv2d_26/BiasAdd/BiasAddGrad:0' shape=(64,) dtype=float32>)
    kwargs: {}

INFO:tensorflow:Whitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B10CA8>: from cache
Whitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B10CA8>: from cache
INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B10CA8>
    args: (<tf.Variable 'dense_16/kernel:0' shape=(20736, 512) dtype=float32>, <tf.Tensor 'gradient_tape/sequential_8/dense_16/MatMul_1:0' shape=(20736, 512) dtype=float32>)
    kwargs: {}

Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B10CA8>
    args: (<tf.Variable 'dense_16/kernel:0' shape=(20736, 512) dtype=float32>, <tf.Tensor 'gradient_tape/sequential_8/dense_16/MatMul_1:0' shape=(20736, 512) dtype=float32>)
    kwargs: {}

INFO:tensorflow:Whitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B10CA8>: from cache
Whitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B10CA8>: from cache
INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B10CA8>
    args: (<tf.Variable 'dense_16/bias:0' shape=(512,) dtype=float32>, <tf.Tensor 'gradient_tape/sequential_8/dense_16/BiasAdd/BiasAddGrad:0' shape=(512,) dtype=float32>)
    kwargs: {}

Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B10CA8>
    args: (<tf.Variable 'dense_16/bias:0' shape=(512,) dtype=float32>, <tf.Tensor 'gradient_tape/sequential_8/dense_16/BiasAdd/BiasAddGrad:0' shape=(512,) dtype=float32>)
    kwargs: {}

INFO:tensorflow:Whitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B10CA8>: from cache
Whitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B10CA8>: from cache
INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B10CA8>
    args: (<tf.Variable 'dense_17/kernel:0' shape=(512, 1) dtype=float32>, <tf.Tensor 'gradient_tape/sequential_8/dense_17/MatMul_1:0' shape=(512, 1) dtype=float32>)
    kwargs: {}

Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B10CA8>
    args: (<tf.Variable 'dense_17/kernel:0' shape=(512, 1) dtype=float32>, <tf.Tensor 'gradient_tape/sequential_8/dense_17/MatMul_1:0' shape=(512, 1) dtype=float32>)
    kwargs: {}

INFO:tensorflow:Whitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B10CA8>: from cache
Whitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B10CA8>: from cache
INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B10CA8>
    args: (<tf.Variable 'dense_17/bias:0' shape=(1,) dtype=float32>, <tf.Tensor 'gradient_tape/sequential_8/dense_17/BiasAdd/BiasAddGrad:0' shape=(1,) dtype=float32>)
    kwargs: {}

Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B10CA8>
    args: (<tf.Variable 'dense_17/bias:0' shape=(1,) dtype=float32>, <tf.Tensor 'gradient_tape/sequential_8/dense_17/BiasAdd/BiasAddGrad:0' shape=(1,) dtype=float32>)
    kwargs: {}

INFO:tensorflow:Whitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B10CA8>: from cache
Whitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000020244B10CA8>: from cache
15/15 [==============================] - ETA: 0s - loss: 0.9207 - accuracy: 0.5139INFO:tensorflow:Converted call: <function DatasetV2.from_generator.<locals>.flat_map_fn at 0x0000020244B10288>
    args: (<tf.Tensor 'args_0:0' shape=() dtype=int32>,)
    kwargs: {}

Converted call: <function DatasetV2.from_generator.<locals>.flat_map_fn at 0x0000020244B10288>
    args: (<tf.Tensor 'args_0:0' shape=() dtype=int32>,)
    kwargs: {}

INFO:tensorflow:Whitelisted: <function DatasetV2.from_generator.<locals>.flat_map_fn at 0x0000020244B10288>: DoNotConvert rule for tensorflow
Whitelisted: <function DatasetV2.from_generator.<locals>.flat_map_fn at 0x0000020244B10288>: DoNotConvert rule for tensorflow
INFO:tensorflow:Converted call: <function DatasetV2.from_generator.<locals>.get_iterator_id_fn at 0x0000020244B10F78>
    args: (<tf.Tensor 'args_0:0' shape=() dtype=int32>,)
    kwargs: {}

Converted call: <function DatasetV2.from_generator.<locals>.get_iterator_id_fn at 0x0000020244B10F78>
    args: (<tf.Tensor 'args_0:0' shape=() dtype=int32>,)
    kwargs: {}

INFO:tensorflow:Whitelisted: <function DatasetV2.from_generator.<locals>.get_iterator_id_fn at 0x0000020244B10F78>: DoNotConvert rule for tensorflow
Whitelisted: <function DatasetV2.from_generator.<locals>.get_iterator_id_fn at 0x0000020244B10F78>: DoNotConvert rule for tensorflow
INFO:tensorflow:Converted call: <function DatasetV2.from_generator.<locals>.generator_next_fn at 0x0000020244B10558>
    args: (<tf.Tensor 'args_0:0' shape=<unknown> dtype=int64>,)
    kwargs: {}

Converted call: <function DatasetV2.from_generator.<locals>.generator_next_fn at 0x0000020244B10558>
    args: (<tf.Tensor 'args_0:0' shape=<unknown> dtype=int64>,)
    kwargs: {}

INFO:tensorflow:Whitelisted: <function DatasetV2.from_generator.<locals>.generator_next_fn at 0x0000020244B10558>: DoNotConvert rule for tensorflow
Whitelisted: <function DatasetV2.from_generator.<locals>.generator_next_fn at 0x0000020244B10558>: DoNotConvert rule for tensorflow
INFO:tensorflow:Converted call: <function DatasetV2.from_generator.<locals>.finalize_fn at 0x0000020244B10D38>
    args: (<tf.Tensor 'args_0:0' shape=<unknown> dtype=int64>,)
    kwargs: {}

Converted call: <function DatasetV2.from_generator.<locals>.finalize_fn at 0x0000020244B10D38>
    args: (<tf.Tensor 'args_0:0' shape=<unknown> dtype=int64>,)
    kwargs: {}

INFO:tensorflow:Whitelisted: <function DatasetV2.from_generator.<locals>.finalize_fn at 0x0000020244B10D38>: DoNotConvert rule for tensorflow
Whitelisted: <function DatasetV2.from_generator.<locals>.finalize_fn at 0x0000020244B10D38>: DoNotConvert rule for tensorflow
INFO:tensorflow:Converted call: <function Model.make_test_function.<locals>.test_function at 0x0000020244B10CA8>
    args: (<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x0000020244E66388>,)
    kwargs: {}

Converted call: <function Model.make_test_function.<locals>.test_function at 0x0000020244B10CA8>
    args: (<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x0000020244E66388>,)
    kwargs: {}

INFO:tensorflow:Cache hit for entity <function Model.make_test_function.<locals>.test_function at 0x0000020244B10CA8> subkey (<tensorflow.python.autograph.core.converter.ConversionOptions object at 0x0000020244E667C8>, frozenset({'self'})): _ConvertedEntityFactoryInfo(tf__test_function in tmpqvpbtv4e)
Cache hit for entity <function Model.make_test_function.<locals>.test_function at 0x0000020244B10CA8> subkey (<tensorflow.python.autograph.core.converter.ConversionOptions object at 0x0000020244E667C8>, frozenset({'self'})): _ConvertedEntityFactoryInfo(tf__test_function in tmpqvpbtv4e)
INFO:tensorflow:Error transforming entity <function Model.make_test_function.<locals>.test_function at 0x0000020244B10CA8>
Traceback (most recent call last):
  File ""C:\Users\coumansFAW\Miniconda3\envs\TF220\lib\site-packages\tensorflow\python\autograph\impl\api.py"", line 538, in converted_call
    converted_f = conversion.convert(target_entity, program_ctx)
  File ""C:\Users\coumansFAW\Miniconda3\envs\TF220\lib\site-packages\tensorflow\python\autograph\impl\conversion.py"", line 362, in convert
    return _instantiate(entity, converted_entity_info, free_nonglobal_var_names)
  File ""C:\Users\coumansFAW\Miniconda3\envs\TF220\lib\site-packages\tensorflow\python\autograph\impl\conversion.py"", line 300, in _instantiate
    factory = converted_entity_info.get_factory()
  File ""C:\Users\coumansFAW\Miniconda3\envs\TF220\lib\site-packages\tensorflow\python\autograph\impl\conversion.py"", line 94, in get_factory
    assert self.module_name in sys.modules
AssertionError
Error transforming entity <function Model.make_test_function.<locals>.test_function at 0x0000020244B10CA8>
WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000020244B10CA8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: 
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000020244B10CA8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: 
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
INFO:tensorflow:Converted call: <bound method Model.test_step of <tensorflow.python.keras.engine.sequential.Sequential object at 0x00000202440228C8>>
    args: ((<tf.Tensor 'IteratorGetNext:0' shape=(None, None, None, None) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=float32>),)
    kwargs: {}

Converted call: <bound method Model.test_step of <tensorflow.python.keras.engine.sequential.Sequential object at 0x00000202440228C8>>
    args: ((<tf.Tensor 'IteratorGetNext:0' shape=(None, None, None, None) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=float32>),)
    kwargs: {}

INFO:tensorflow:Whitelisted <bound method Model.test_step of <tensorflow.python.keras.engine.sequential.Sequential object at 0x00000202440228C8>>: from cache
Whitelisted <bound method Model.test_step of <tensorflow.python.keras.engine.sequential.Sequential object at 0x00000202440228C8>>: from cache
Traceback (most recent call last):
  File ""C:\Users\coumansFAW\Miniconda3\envs\TF220\lib\site-packages\tensorflow\python\autograph\impl\api.py"", line 538, in converted_call
    converted_f = conversion.convert(target_entity, program_ctx)
  File ""C:\Users\coumansFAW\Miniconda3\envs\TF220\lib\site-packages\tensorflow\python\autograph\impl\conversion.py"", line 362, in convert
    return _instantiate(entity, converted_entity_info, free_nonglobal_var_names)
  File ""C:\Users\coumansFAW\Miniconda3\envs\TF220\lib\site-packages\tensorflow\python\autograph\impl\conversion.py"", line 300, in _instantiate
    factory = converted_entity_info.get_factory()
  File ""C:\Users\coumansFAW\Miniconda3\envs\TF220\lib\site-packages\tensorflow\python\autograph\impl\conversion.py"", line 94, in get_factory
    assert self.module_name in sys.modules
AssertionError
15/15 [==============================] - 16s 1s/step - loss: 0.9207 - accuracy: 0.5139 - val_loss: 0.6922 - val_accuracy: 0.5000"
41229,ImportError: impossible d'importer le nom 'export_saved_model' de 'tensorflow.python.keras.saving.saved_model' (F: \ New folder \ New folder \ lib \ site-packages \ tensorflow \ python \ keras \ Saving \ Saved_model \ __ init__.py) ,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
41228,Quantization instruction problem after training,"I have a problem with the instruction [integer quantization after training] in the official website.
this code:

mnist_train, _ = tf.keras.datasets.mnist.load_data()
images = tf.cast(mnist_train[0], tf.float32) / 255.0
mnist_ds = tf.data.Dataset.from_tensor_slices((images)).batch(1)
def representative_data_gen():
  for input_value in mnist_ds.take(100):
    # Model has only one input so each data point has one element.
    yield [input_value]
converter.representative_dataset = representative_data_gen

My question is the fifth itinerary [for input_value in mnist_ds.take(100): ], if I use my own data set, do I need to change this part of mnist_ds.take? There is also a question 100 what does it mean? 
"
41227,"Can anyone give a C_api example with multiple inputs and multiple outputs, like object detection?","Hello,
        Can anyone give a C_api example with multiple inputs and multiple outputs , like object detection,with one input and three outputs( detection boxes, detection classes and detection scores)?
        With one input and one output, I can run without error. But for mutilple outputs, I do not know how to set 
[const TF_Output* outputs, TF_Tensor** output_values] the two parameter in TF_SessionRun(***) function.
![image](https://user-images.githubusercontent.com/28335784/87021919-5c29ed80-c208-11ea-8756-6fd5f39f9163.png)


Thank you !"
41226,Error when running inference on CPU using C++ bindings: Not found: Container localhost does not exist,"**System information**
- Have I written custom code: **yes**
- OS/Distro: **Arch Linux**
- TensorFlow installed from: **virtualenv with pip, C/C++ bindings from source without CUDA support**
- TensorFlow version virtualenv: **v1.12.1-34938-g99fea8da0d 2.3.0-rc0**
- TensorFlow version C/C++ bindings: **v1.12.1-35361-ge89160d8d3 2.5.0-dev20200629**
- Python version: **3.7**
- Bazel version: **3.1.0- (@non-git)**
- GCC/Compiler version: **10.1.0**
- CUDA/cuDNN version: 10.2z (Error happens on CPU)
- GPU model and memory: Nvidia RTX 2080 Ti

**Describe the current behavior**
Predicting works fine in Python, however when I try to run inference on CPU by using the C++ bindings I get the following error:
```
Failed precondition: Error while reading resource variable dense/kernel from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/dense/kernel)
	 [[{{node dense/Tensordot/ReadVariableOp}}]]
```
**Describe the expected behavior**
Being able to predict in C++ without issues.

**Standalone code to reproduce the issue**
Code is on pastebin.com to increase readability. Links:
[Training the model in Python](https://pastebin.com/iFccjjnx)
[C++ Header](https://pastebin.com/6mkwK5bX)
[C++ Code](https://pastebin.com/eUrPt0YT)
[cli_saved_model --dir ... --all output](https://pastebin.com/QGgX1UXr)

**Other info / logs**
A similar issue was already [posted on Stackoverflow](https://stackoverflow.com/questions/60059177/failed-precondition-errors-when-using-keras-model-in-c), however with no answers.
Issue is persistent in TF2.2."
41225,Masked Pooling and Convolution,"In geospatial domain often masked images are used due to various reasons. In some specific cases is not sufficient to fill the masked areas as zero and work with that image. Therefore it would be vvery helpful if it would be possible to perform convolution and pooling operations (specifically 2D pooling and convolution), optionally with a mask. "
41224,migration problem from tensorflow 1.4.1 to tensorflow 2.0,"I'm trying to implement a Vae model using tensorflow 2 through Google Colab. In order to do that, I need to start from a code wrote with tensorflow 1.4.1 and convert it in order to make it works. I report the original code wrote with TF1.4.1. Right now it works but unfortunately during the fit process the loss function deverges all the time, leaving me with a useless model ( the problem is related with TF version and not with the model itself, casuse I tried with TF1.4.1 and it perfectly worked.


```

import numpy as np
import keras
from keras.layers import *
from keras.models import Sequential,Model
from keras import backend as K

def define_pre_encoder(data_dim,layers=2,units=512,dropout=0.0,BN=False): #define pre_encoder network
    model = Sequential(name='pre-encoder')
    model.add(InputLayer(input_shape=(data_dim,)))
    for i in range(1,layers+1):
        #model.add(Dense(int(units/i), activation='relu'))
        model.add(Dense(units,activation='relu'))
        if dropout != 0. and dropout != None:
            model.add(Dropout(dropout))
        if BN:
            model.add(BatchNormalization())
    return model

def define_generator(Nb,data_dim,layers=2,units=32,dropout=0.0,BN=False,exclusive=True):
    model = Sequential(name='generator/decoder')
    model.add(InputLayer(input_shape=(Nb,)))
    for i in np.arange(layers,0,-1):
        #model.add(Dense(int(units/i), activation='relu'))
        model.add(Dense(units,activation='relu'))
        if dropout != 0. and dropout != None:
            model.add(Dropout(dropout))
        if BN:
            model.add(BatchNormalization())
    if exclusive:
        model.add(Dense(data_dim, activation='softmax')) #softmax generator
    else:
        model.add(Dense(data_dim, activation='sigmoid'))
    return model

def traditional_VAE(data_dim,Nb,units,layers_e,layers_d,opt='adam',BN=True):
    pre_encoder = define_pre_encoder(data_dim, layers=layers_e,units=units,BN=BN)
    print(""pre-encoder network:"")
    pre_encoder.summary()
    generator = define_generator(Nb,data_dim,layers=layers_d,units=units,BN=BN)
    print(""generator network:"")
    generator.summary()

    ## Encoder
    x = Input(shape=(data_dim,))
    hidden = pre_encoder(x)
    z_mean = Dense(Nb,activation='linear', name='z-mean')(hidden)
    z_log_var = Dense(Nb,activation='linear',name = 'z-log_var')(hidden)
    encoder = Model(x, z_mean) # build a model to project inputs on the latent space

    def sampling(args):
        epsilon_std = 1.0
        z_mean, z_log_var = args
        epsilon = K.random_normal(shape=(K.shape(z_mean)[0], Nb),mean=0., stddev=epsilon_std)
        return z_mean + K.exp(0.5*z_log_var) * epsilon #+sigma (desvest)
    
    ## Decoder
    z_sampled = Lambda(sampling, output_shape=(Nb,), name='sampled')([z_mean, z_log_var])
    output = generator(z_sampled)

    def vae_loss(x, x_hat):
        reconstruction_loss = keras.losses.categorical_crossentropy(x, x_hat)*data_dim 
        #reconstruction_loss = keras.losses.binary_crossentropy(x, x_hat)*data_dim 

        kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1) #con varianza
        return K.mean(reconstruction_loss  + kl_loss)

    traditional_vae = Model(x, output)
    traditional_vae.compile(optimizer=opt,loss=vae_loss)
    return traditional_vae,encoder,generator
```"
41220,Uninstal TensorFlow with all it's packages *Windows 10,"Hello, 
I uninstalled TensorFlow with `pip uninstall tensorflow` but there are still some packages that came with it when i installed it and now i have to uninstall them manually.
Is there anyway to uninstall them all together not by one? 

Thanks for any help in advanced. 
"
41218,Differences in using tf.keras.Model.fit() API and writing training loop from scratch (simple regression model),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2
- Python version: 3.6.3
- CUDA/cuDNN version: 10.1
- GPU model and memory: nvidia gtx 1080 

**Describe the current behavior**
I'm trying to learn the new tensorflow 2 API using the following tutorials:
[1] https://www.tensorflow.org/tutorials/keras/regression
[2] https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch/

Currently, the regression model based on [1] works really well when I use the model.fit() API. 

Let's call this (A):
~~~
def fit_model_a(model, dataset, normalize_fn):
    model.compile(loss='mse',
                    optimizer=tf.keras.optimizers.RMSprop(0.001),
                    metrics=['mse'])

    x_train = dataset.copy()
    y_train = x_train.pop('MPG')
    history = model.fit(
        normalize_fn(x_train), y_train,
        epochs=100, validation_split = 0.2, verbose=1)
~~~
Using my own training loop however exhibits drastically different behavior.

Here's the training function using my own loop (B):
~~~
def fit_model_b(model, dataset, normalize_fn):
    trainset = dataset.sample(frac=0.8,random_state=0)
    devset = dataset.drop(trainset.index)

    optimizer = tf.keras.optimizers.RMSprop(0.001)

    bsize = 32
    for ep in range(100):
        trainset = trainset.sample(frac=1)

        for i in range(0,len(trainset.index),bsize):
            x_batch = trainset.iloc[i:i+bsize]
            y_batch = x_batch.pop('MPG')
            
            with tf.GradientTape() as tape:
                y_batch_pred = model(normalize_fn(x_batch), training=True)
                loss = tf.keras.losses.MeanSquaredError()(y_batch.values, y_batch_pred)

            gradients = tape.gradient(loss, model.trainable_weights)
            optimizer.apply_gradients(zip(gradients, model.trainable_weights))
        
        print('{}\tloss={:.2f}\tfit-mse={:.2f}\tdev-mse={:.2f}'.format(ep, loss.numpy(), 
                    eval_model(model, trainset, normalize_fn), eval_model(model, devset, normalize_fn)))
~~~

I've included a collab with the full source code at the bottom. Between the two, the models used are exactly the same (built using tf.keras.Sequential()), optimized with the same optimizer (RMSProp with LR=0.001), and trained on the same data from [1]. The training function (B) is meant to train for 100 epochs with a batch size of 32 to replicate the behavior of model.fit().  However, (B) is not fitting well on the data and by looking at some of the predictions, it looks like (B) is fixated on learning the average, while predictions by (A) is much more meaningful. How can this be?

Here's the output of the model using (A):

~~~
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_15 (Dense)             (None, 64)                640       
_________________________________________________________________
dense_16 (Dense)             (None, 64)                4160      
_________________________________________________________________
dense_17 (Dense)             (None, 1)                 65        
=================================================================
Total params: 4,865
Trainable params: 4,865
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
8/8 [==============================] - 0s 13ms/step - loss: 573.9155 - mse: 573.9155 - val_loss: 577.1165 - val_mse: 577.1165
Epoch 2/100
8/8 [==============================] - 0s 6ms/step - loss: 535.4519 - mse: 535.4519 - val_loss: 535.2042 - val_mse: 535.2042
Epoch 3/100
8/8 [==============================] - 0s 5ms/step - loss: 492.2293 - mse: 492.2293 - val_loss: 486.6287 - val_mse: 486.6287
Epoch 4/100
8/8 [==============================] - 0s 5ms/step - loss: 441.6238 - mse: 441.6238 - val_loss: 429.2172 - val_mse: 429.2172
Epoch 5/100
8/8 [==============================] - 0s 5ms/step - loss: 384.5562 - mse: 384.5562 - val_loss: 367.9710 - val_mse: 367.9710
Epoch 6/100
8/8 [==============================] - 0s 5ms/step - loss: 324.1839 - mse: 324.1839 - val_loss: 304.0136 - val_mse: 304.0136
Epoch 7/100
8/8 [==============================] - 0s 5ms/step - loss: 261.1829 - mse: 261.1829 - val_loss: 237.4727 - val_mse: 237.4727
Epoch 8/100
8/8 [==============================] - 0s 6ms/step - loss: 199.2595 - mse: 199.2595 - val_loss: 175.9138 - val_mse: 175.9138
Epoch 9/100
8/8 [==============================] - 0s 5ms/step - loss: 143.4326 - mse: 143.4326 - val_loss: 122.1979 - val_mse: 122.1979
Epoch 10/100
8/8 [==============================] - 0s 5ms/step - loss: 97.0918 - mse: 97.0918 - val_loss: 79.8414 - val_mse: 79.8414
Epoch 11/100
8/8 [==============================] - 0s 5ms/step - loss: 61.4929 - mse: 61.4929 - val_loss: 48.1678 - val_mse: 48.1678
Epoch 12/100
8/8 [==============================] - 0s 5ms/step - loss: 38.5452 - mse: 38.5452 - val_loss: 30.5858 - val_mse: 30.5858
Epoch 13/100
8/8 [==============================] - 0s 6ms/step - loss: 26.7722 - mse: 26.7722 - val_loss: 22.1917 - val_mse: 22.1917
Epoch 14/100
8/8 [==============================] - 0s 6ms/step - loss: 22.0821 - mse: 22.0821 - val_loss: 19.3342 - val_mse: 19.3342
Epoch 15/100
8/8 [==============================] - 0s 6ms/step - loss: 20.2049 - mse: 20.2049 - val_loss: 17.5354 - val_mse: 17.5354
Epoch 16/100
8/8 [==============================] - 0s 5ms/step - loss: 19.3463 - mse: 19.3463 - val_loss: 17.0012 - val_mse: 17.0012
Epoch 17/100
8/8 [==============================] - 0s 5ms/step - loss: 18.8869 - mse: 18.8869 - val_loss: 16.9589 - val_mse: 16.9589
Epoch 18/100
8/8 [==============================] - 0s 6ms/step - loss: 18.7929 - mse: 18.7929 - val_loss: 16.5268 - val_mse: 16.5268
Epoch 19/100
8/8 [==============================] - 0s 5ms/step - loss: 18.6861 - mse: 18.6861 - val_loss: 16.2446 - val_mse: 16.2446
Epoch 20/100
8/8 [==============================] - 0s 5ms/step - loss: 18.6174 - mse: 18.6174 - val_loss: 16.1443 - val_mse: 16.1443
Epoch 21/100
8/8 [==============================] - 0s 5ms/step - loss: 17.9203 - mse: 17.9203 - val_loss: 16.8566 - val_mse: 16.8566
Epoch 22/100
8/8 [==============================] - 0s 5ms/step - loss: 18.1555 - mse: 18.1555 - val_loss: 16.3021 - val_mse: 16.3021
Epoch 23/100
8/8 [==============================] - 0s 5ms/step - loss: 18.0476 - mse: 18.0476 - val_loss: 15.5666 - val_mse: 15.5666
Epoch 24/100
8/8 [==============================] - 0s 5ms/step - loss: 17.9181 - mse: 17.9181 - val_loss: 15.6645 - val_mse: 15.6645
Epoch 25/100
8/8 [==============================] - 0s 5ms/step - loss: 18.1296 - mse: 18.1296 - val_loss: 15.3343 - val_mse: 15.3343
Epoch 26/100
8/8 [==============================] - 0s 5ms/step - loss: 17.5297 - mse: 17.5297 - val_loss: 15.3144 - val_mse: 15.3144
Epoch 27/100
8/8 [==============================] - 0s 6ms/step - loss: 17.4787 - mse: 17.4787 - val_loss: 15.4818 - val_mse: 15.4818
Epoch 28/100
8/8 [==============================] - 0s 6ms/step - loss: 17.2343 - mse: 17.2343 - val_loss: 15.5582 - val_mse: 15.5582
Epoch 29/100
8/8 [==============================] - 0s 5ms/step - loss: 17.3628 - mse: 17.3628 - val_loss: 15.0562 - val_mse: 15.0562
Epoch 30/100
8/8 [==============================] - 0s 5ms/step - loss: 17.4319 - mse: 17.4319 - val_loss: 14.8126 - val_mse: 14.8126
Epoch 31/100
8/8 [==============================] - 0s 5ms/step - loss: 16.8817 - mse: 16.8817 - val_loss: 14.8000 - val_mse: 14.8000
Epoch 32/100
8/8 [==============================] - 0s 6ms/step - loss: 17.0150 - mse: 17.0150 - val_loss: 14.7323 - val_mse: 14.7323
Epoch 33/100
8/8 [==============================] - 0s 5ms/step - loss: 17.0183 - mse: 17.0183 - val_loss: 14.7309 - val_mse: 14.7309
Epoch 34/100
8/8 [==============================] - 0s 6ms/step - loss: 16.7561 - mse: 16.7561 - val_loss: 14.6972 - val_mse: 14.6972
Epoch 35/100
8/8 [==============================] - 0s 6ms/step - loss: 16.8762 - mse: 16.8762 - val_loss: 14.9364 - val_mse: 14.9364
Epoch 36/100
8/8 [==============================] - 0s 5ms/step - loss: 16.7464 - mse: 16.7464 - val_loss: 14.4770 - val_mse: 14.4770
Epoch 37/100
8/8 [==============================] - 0s 7ms/step - loss: 16.9630 - mse: 16.9630 - val_loss: 14.8516 - val_mse: 14.8516
Epoch 38/100
8/8 [==============================] - 0s 5ms/step - loss: 16.5634 - mse: 16.5634 - val_loss: 14.3185 - val_mse: 14.3185
Epoch 39/100
8/8 [==============================] - 0s 5ms/step - loss: 16.2602 - mse: 16.2602 - val_loss: 14.5408 - val_mse: 14.5408
Epoch 40/100
8/8 [==============================] - 0s 5ms/step - loss: 16.5903 - mse: 16.5903 - val_loss: 14.7360 - val_mse: 14.7360
Epoch 41/100
8/8 [==============================] - 0s 5ms/step - loss: 16.2692 - mse: 16.2692 - val_loss: 14.1311 - val_mse: 14.1311
Epoch 42/100
8/8 [==============================] - 0s 5ms/step - loss: 16.1788 - mse: 16.1788 - val_loss: 14.3953 - val_mse: 14.3953
Epoch 43/100
8/8 [==============================] - 0s 5ms/step - loss: 16.2510 - mse: 16.2510 - val_loss: 14.2353 - val_mse: 14.2353
Epoch 44/100
8/8 [==============================] - 0s 5ms/step - loss: 16.0959 - mse: 16.0959 - val_loss: 13.9634 - val_mse: 13.9634
Epoch 45/100
8/8 [==============================] - 0s 5ms/step - loss: 16.2482 - mse: 16.2482 - val_loss: 13.9016 - val_mse: 13.9016
Epoch 46/100
8/8 [==============================] - 0s 5ms/step - loss: 15.7698 - mse: 15.7698 - val_loss: 15.3176 - val_mse: 15.3176
Epoch 47/100
8/8 [==============================] - 0s 6ms/step - loss: 16.0930 - mse: 16.0930 - val_loss: 14.1587 - val_mse: 14.1587
Epoch 48/100
8/8 [==============================] - 0s 5ms/step - loss: 16.1624 - mse: 16.1624 - val_loss: 14.0680 - val_mse: 14.0680
Epoch 49/100
8/8 [==============================] - 0s 5ms/step - loss: 15.7494 - mse: 15.7494 - val_loss: 13.9287 - val_mse: 13.9287
Epoch 50/100
8/8 [==============================] - 0s 5ms/step - loss: 15.5207 - mse: 15.5207 - val_loss: 13.7302 - val_mse: 13.7302
Epoch 51/100
8/8 [==============================] - 0s 5ms/step - loss: 15.8989 - mse: 15.8989 - val_loss: 13.7902 - val_mse: 13.7902
Epoch 52/100
8/8 [==============================] - 0s 6ms/step - loss: 15.4386 - mse: 15.4386 - val_loss: 13.6323 - val_mse: 13.6323
Epoch 53/100
8/8 [==============================] - 0s 5ms/step - loss: 15.5308 - mse: 15.5308 - val_loss: 13.7107 - val_mse: 13.7107
Epoch 54/100
8/8 [==============================] - 0s 6ms/step - loss: 15.2940 - mse: 15.2940 - val_loss: 13.5425 - val_mse: 13.5425
Epoch 55/100
8/8 [==============================] - 0s 6ms/step - loss: 15.2990 - mse: 15.2990 - val_loss: 13.6214 - val_mse: 13.6214
Epoch 56/100
8/8 [==============================] - 0s 6ms/step - loss: 15.5306 - mse: 15.5306 - val_loss: 13.7859 - val_mse: 13.7859
Epoch 57/100
8/8 [==============================] - 0s 6ms/step - loss: 15.2955 - mse: 15.2955 - val_loss: 13.4464 - val_mse: 13.4464
Epoch 58/100
8/8 [==============================] - 0s 5ms/step - loss: 15.3422 - mse: 15.3422 - val_loss: 14.4806 - val_mse: 14.4806
Epoch 59/100
8/8 [==============================] - 0s 5ms/step - loss: 15.2947 - mse: 15.2947 - val_loss: 13.3782 - val_mse: 13.3782
Epoch 60/100
8/8 [==============================] - 0s 5ms/step - loss: 15.2226 - mse: 15.2226 - val_loss: 13.6523 - val_mse: 13.6523
Epoch 61/100
8/8 [==============================] - 0s 6ms/step - loss: 14.7474 - mse: 14.7474 - val_loss: 15.0304 - val_mse: 15.0304
Epoch 62/100
8/8 [==============================] - 0s 5ms/step - loss: 14.9154 - mse: 14.9154 - val_loss: 13.6243 - val_mse: 13.6243
Epoch 63/100
8/8 [==============================] - 0s 5ms/step - loss: 15.0523 - mse: 15.0523 - val_loss: 13.1219 - val_mse: 13.1219
Epoch 64/100
8/8 [==============================] - 0s 5ms/step - loss: 15.0700 - mse: 15.0700 - val_loss: 13.0726 - val_mse: 13.0726
Epoch 65/100
8/8 [==============================] - 0s 5ms/step - loss: 15.1012 - mse: 15.1012 - val_loss: 13.1141 - val_mse: 13.1141
Epoch 66/100
8/8 [==============================] - 0s 7ms/step - loss: 15.1817 - mse: 15.1817 - val_loss: 12.9964 - val_mse: 12.9964
Epoch 67/100
8/8 [==============================] - 0s 5ms/step - loss: 14.7159 - mse: 14.7159 - val_loss: 13.1849 - val_mse: 13.1849
Epoch 68/100
8/8 [==============================] - 0s 5ms/step - loss: 15.0554 - mse: 15.0554 - val_loss: 12.9389 - val_mse: 12.9389
Epoch 69/100
8/8 [==============================] - 0s 5ms/step - loss: 14.6790 - mse: 14.6790 - val_loss: 13.2163 - val_mse: 13.2163
Epoch 70/100
8/8 [==============================] - 0s 5ms/step - loss: 14.4835 - mse: 14.4835 - val_loss: 12.9312 - val_mse: 12.9312
Epoch 71/100
8/8 [==============================] - 0s 6ms/step - loss: 14.4713 - mse: 14.4713 - val_loss: 12.9020 - val_mse: 12.9020
Epoch 72/100
8/8 [==============================] - 0s 5ms/step - loss: 14.5055 - mse: 14.5055 - val_loss: 12.8103 - val_mse: 12.8103
Epoch 73/100
8/8 [==============================] - 0s 5ms/step - loss: 14.6565 - mse: 14.6565 - val_loss: 12.7457 - val_mse: 12.7457
Epoch 74/100
8/8 [==============================] - 0s 5ms/step - loss: 14.4647 - mse: 14.4647 - val_loss: 12.7685 - val_mse: 12.7685
Epoch 75/100
8/8 [==============================] - 0s 6ms/step - loss: 14.2886 - mse: 14.2886 - val_loss: 12.7231 - val_mse: 12.7231
Epoch 76/100
8/8 [==============================] - 0s 6ms/step - loss: 14.2142 - mse: 14.2142 - val_loss: 12.6104 - val_mse: 12.6104
Epoch 77/100
8/8 [==============================] - 0s 5ms/step - loss: 14.3990 - mse: 14.3990 - val_loss: 12.9287 - val_mse: 12.9287
Epoch 78/100
8/8 [==============================] - 0s 5ms/step - loss: 14.4393 - mse: 14.4393 - val_loss: 13.0988 - val_mse: 13.0988
Epoch 79/100
8/8 [==============================] - 0s 7ms/step - loss: 14.2155 - mse: 14.2155 - val_loss: 12.4750 - val_mse: 12.4750
Epoch 80/100
8/8 [==============================] - 0s 6ms/step - loss: 14.0817 - mse: 14.0817 - val_loss: 13.0547 - val_mse: 13.0547
Epoch 81/100
8/8 [==============================] - 0s 6ms/step - loss: 14.1034 - mse: 14.1034 - val_loss: 12.5163 - val_mse: 12.5163
Epoch 82/100
8/8 [==============================] - 0s 5ms/step - loss: 13.9492 - mse: 13.9492 - val_loss: 12.4928 - val_mse: 12.4928
Epoch 83/100
8/8 [==============================] - 0s 5ms/step - loss: 14.0635 - mse: 14.0635 - val_loss: 12.6222 - val_mse: 12.6222
Epoch 84/100
8/8 [==============================] - 0s 5ms/step - loss: 13.9243 - mse: 13.9243 - val_loss: 12.3200 - val_mse: 12.3200
Epoch 85/100
8/8 [==============================] - 0s 6ms/step - loss: 14.0242 - mse: 14.0242 - val_loss: 12.2624 - val_mse: 12.2624
Epoch 86/100
8/8 [==============================] - 0s 6ms/step - loss: 13.9066 - mse: 13.9066 - val_loss: 12.3266 - val_mse: 12.3266
Epoch 87/100
8/8 [==============================] - 0s 5ms/step - loss: 13.6502 - mse: 13.6502 - val_loss: 12.2916 - val_mse: 12.2916
Epoch 88/100
8/8 [==============================] - 0s 5ms/step - loss: 14.1173 - mse: 14.1173 - val_loss: 12.7280 - val_mse: 12.7280
Epoch 89/100
8/8 [==============================] - 0s 5ms/step - loss: 13.4772 - mse: 13.4772 - val_loss: 12.1312 - val_mse: 12.1312
Epoch 90/100
8/8 [==============================] - 0s 5ms/step - loss: 13.5305 - mse: 13.5305 - val_loss: 12.6813 - val_mse: 12.6813
Epoch 91/100
8/8 [==============================] - 0s 5ms/step - loss: 13.4792 - mse: 13.4792 - val_loss: 12.7643 - val_mse: 12.7643
Epoch 92/100
8/8 [==============================] - 0s 5ms/step - loss: 12.7118 - mse: 12.7118 - val_loss: 13.1304 - val_mse: 13.1304
Epoch 93/100
8/8 [==============================] - 0s 6ms/step - loss: 13.4823 - mse: 13.4823 - val_loss: 12.6384 - val_mse: 12.6384
Epoch 94/100
8/8 [==============================] - 0s 5ms/step - loss: 13.3819 - mse: 13.3819 - val_loss: 13.0808 - val_mse: 13.0808
Epoch 95/100
8/8 [==============================] - 0s 6ms/step - loss: 13.3348 - mse: 13.3348 - val_loss: 12.1661 - val_mse: 12.1661
Epoch 96/100
8/8 [==============================] - 0s 5ms/step - loss: 13.1636 - mse: 13.1636 - val_loss: 11.8638 - val_mse: 11.8638
Epoch 97/100
8/8 [==============================] - 0s 6ms/step - loss: 13.0808 - mse: 13.0808 - val_loss: 12.3383 - val_mse: 12.3383
Epoch 98/100
8/8 [==============================] - 0s 5ms/step - loss: 13.2087 - mse: 13.2087 - val_loss: 11.7358 - val_mse: 11.7358
Epoch 99/100
8/8 [==============================] - 0s 6ms/step - loss: 13.1870 - mse: 13.1870 - val_loss: 11.9876 - val_mse: 11.9876
Epoch 100/100
8/8 [==============================] - 0s 5ms/step - loss: 13.2171 - mse: 13.2171 - val_loss: 12.1973 - val_mse: 12.1973

MSE ON TRAINING:  12.762719337215518

PREDICTIONS
>> actual=15.0, predicted=13.902673721313477
>> actual=10.0, predicted=11.325315475463867
>> actual=9.0, predicted=11.17175579071045
>> actual=25.0, predicted=25.51102638244629
>> actual=19.0, predicted=20.770906448364258
>> actual=14.0, predicted=13.679293632507324
>> actual=14.0, predicted=14.169425964355469
>> actual=13.0, predicted=13.453120231628418
>> actual=18.0, predicted=20.06334114074707
>> actual=35.0, predicted=32.093849182128906
>> actual=25.0, predicted=28.19291877746582
>> actual=19.0, predicted=26.57512092590332
>> actual=13.0, predicted=15.338750839233398
>> actual=28.0, predicted=28.23664093017578
>> actual=13.0, predicted=13.387893676757812
>> actual=14.0, predicted=15.006385803222656
>> actual=15.0, predicted=14.883852005004883
>> actual=13.0, predicted=13.88344955444336
>> actual=18.0, predicted=20.642784118652344
>> actual=12.0, predicted=12.577205657958984
MSE ON TESTING: 8.60193713820663
~~~

Here's the output of the model with (B):

~~~
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_18 (Dense)             (None, 64)                640       
_________________________________________________________________
dense_19 (Dense)             (None, 64)                4160      
_________________________________________________________________
dense_20 (Dense)             (None, 1)                 65        
=================================================================
Total params: 4,865
Trainable params: 4,865
Non-trainable params: 0
_________________________________________________________________
ep=0	loss=451.15	fit-mse=562.54	dev-mse=605.75
ep=1	loss=634.75	fit-mse=528.33	dev-mse=570.50
ep=2	loss=578.77	fit-mse=489.81	dev-mse=530.83
ep=3	loss=383.15	fit-mse=445.30	dev-mse=485.17
ep=4	loss=330.32	fit-mse=394.18	dev-mse=433.01
ep=5	loss=350.10	fit-mse=338.05	dev-mse=375.80
ep=6	loss=275.47	fit-mse=280.20	dev-mse=316.84
ep=7	loss=216.04	fit-mse=224.03	dev-mse=259.55
ep=8	loss=219.84	fit-mse=171.60	dev-mse=205.86
ep=9	loss=169.93	fit-mse=128.87	dev-mse=161.59
ep=10	loss=86.35	fit-mse=98.62	dev-mse=129.46
ep=11	loss=67.55	fit-mse=81.66	dev-mse=110.51
ep=12	loss=63.47	fit-mse=70.28	dev-mse=96.27
ep=13	loss=40.30	fit-mse=64.55	dev-mse=88.48
ep=14	loss=44.24	fit-mse=60.42	dev-mse=82.77
ep=15	loss=55.45	fit-mse=58.35	dev-mse=79.49
ep=16	loss=66.44	fit-mse=57.02	dev-mse=77.70
ep=17	loss=49.05	fit-mse=55.90	dev-mse=75.52
ep=18	loss=96.35	fit-mse=54.94	dev-mse=74.29
ep=19	loss=46.29	fit-mse=54.72	dev-mse=74.27
ep=20	loss=76.34	fit-mse=55.96	dev-mse=76.21
ep=21	loss=54.97	fit-mse=56.31	dev-mse=76.22
ep=22	loss=54.00	fit-mse=54.82	dev-mse=74.10
ep=23	loss=65.32	fit-mse=55.19	dev-mse=74.64
ep=24	loss=53.25	fit-mse=56.81	dev-mse=77.45
ep=25	loss=40.72	fit-mse=56.05	dev-mse=75.60
ep=26	loss=46.65	fit-mse=55.25	dev-mse=74.81
ep=27	loss=57.01	fit-mse=55.16	dev-mse=74.98
ep=28	loss=64.03	fit-mse=55.09	dev-mse=74.56
ep=29	loss=68.08	fit-mse=55.22	dev-mse=75.21
ep=30	loss=69.12	fit-mse=53.94	dev-mse=73.48
ep=31	loss=42.17	fit-mse=55.08	dev-mse=75.19
ep=32	loss=39.22	fit-mse=56.67	dev-mse=77.53
ep=33	loss=76.51	fit-mse=56.36	dev-mse=76.03
ep=34	loss=70.53	fit-mse=55.41	dev-mse=75.23
ep=35	loss=36.58	fit-mse=54.01	dev-mse=73.56
ep=36	loss=65.90	fit-mse=53.55	dev-mse=72.95
ep=37	loss=59.30	fit-mse=53.91	dev-mse=73.55
ep=38	loss=61.93	fit-mse=54.09	dev-mse=73.31
ep=39	loss=66.33	fit-mse=54.25	dev-mse=73.54
ep=40	loss=72.43	fit-mse=55.24	dev-mse=75.45
ep=41	loss=37.08	fit-mse=55.69	dev-mse=76.03
ep=42	loss=47.11	fit-mse=55.91	dev-mse=75.43
ep=43	loss=42.63	fit-mse=54.87	dev-mse=75.01
ep=44	loss=54.36	fit-mse=55.95	dev-mse=76.42
ep=45	loss=26.99	fit-mse=55.46	dev-mse=75.88
ep=46	loss=76.64	fit-mse=55.95	dev-mse=75.45
ep=47	loss=48.25	fit-mse=56.55	dev-mse=76.82
ep=48	loss=54.42	fit-mse=55.08	dev-mse=74.42
ep=49	loss=73.48	fit-mse=55.05	dev-mse=74.62
ep=50	loss=43.22	fit-mse=55.49	dev-mse=75.26
ep=51	loss=54.03	fit-mse=54.87	dev-mse=74.38
ep=52	loss=66.74	fit-mse=54.54	dev-mse=73.11
ep=53	loss=65.90	fit-mse=55.99	dev-mse=75.91
ep=54	loss=58.84	fit-mse=56.37	dev-mse=75.97
ep=55	loss=49.63	fit-mse=55.29	dev-mse=74.15
ep=56	loss=48.78	fit-mse=55.60	dev-mse=74.83
ep=57	loss=54.61	fit-mse=54.00	dev-mse=72.63
ep=58	loss=47.21	fit-mse=55.10	dev-mse=74.48
ep=59	loss=80.70	fit-mse=54.28	dev-mse=72.64
ep=60	loss=66.65	fit-mse=55.22	dev-mse=74.33
ep=61	loss=82.05	fit-mse=53.16	dev-mse=70.96
ep=62	loss=51.54	fit-mse=54.64	dev-mse=73.51
ep=63	loss=48.69	fit-mse=56.23	dev-mse=76.05
ep=64	loss=92.59	fit-mse=55.95	dev-mse=74.36
ep=65	loss=62.16	fit-mse=55.92	dev-mse=75.03
ep=66	loss=46.01	fit-mse=55.84	dev-mse=75.34
ep=67	loss=50.22	fit-mse=56.25	dev-mse=76.24
ep=68	loss=92.09	fit-mse=55.56	dev-mse=73.48
ep=69	loss=53.25	fit-mse=55.99	dev-mse=75.80
ep=70	loss=71.81	fit-mse=55.77	dev-mse=75.19
ep=71	loss=66.39	fit-mse=55.28	dev-mse=74.20
ep=72	loss=54.23	fit-mse=54.80	dev-mse=74.16
ep=73	loss=54.83	fit-mse=55.40	dev-mse=74.64
ep=74	loss=40.68	fit-mse=55.03	dev-mse=73.95
ep=75	loss=81.32	fit-mse=54.96	dev-mse=73.06
ep=76	loss=50.36	fit-mse=55.94	dev-mse=75.51
ep=77	loss=64.00	fit-mse=56.08	dev-mse=75.65
ep=78	loss=70.95	fit-mse=54.84	dev-mse=74.51
ep=79	loss=64.96	fit-mse=53.87	dev-mse=72.72
ep=80	loss=56.27	fit-mse=54.93	dev-mse=74.10
ep=81	loss=61.17	fit-mse=55.11	dev-mse=74.21
ep=82	loss=60.23	fit-mse=57.36	dev-mse=77.07
ep=83	loss=41.64	fit-mse=55.80	dev-mse=75.71
ep=84	loss=38.13	fit-mse=55.96	dev-mse=75.40
ep=85	loss=58.91	fit-mse=57.40	dev-mse=77.60
ep=86	loss=47.32	fit-mse=56.12	dev-mse=75.42
ep=87	loss=77.51	fit-mse=55.13	dev-mse=74.06
ep=88	loss=75.37	fit-mse=53.90	dev-mse=72.67
ep=89	loss=49.26	fit-mse=55.25	dev-mse=74.08
ep=90	loss=91.33	fit-mse=52.48	dev-mse=70.72
ep=91	loss=38.65	fit-mse=52.77	dev-mse=71.15
ep=92	loss=43.26	fit-mse=54.84	dev-mse=73.87
ep=93	loss=59.81	fit-mse=55.12	dev-mse=73.99
ep=94	loss=60.65	fit-mse=54.58	dev-mse=73.89
ep=95	loss=49.32	fit-mse=54.81	dev-mse=73.37
ep=96	loss=37.07	fit-mse=55.97	dev-mse=75.03
ep=97	loss=51.49	fit-mse=55.03	dev-mse=73.78
ep=98	loss=32.92	fit-mse=55.53	dev-mse=75.13
ep=99	loss=79.23	fit-mse=54.59	dev-mse=72.64

MSE ON TRAINING:  58.21192624872478

PREDICTIONS
>> actual=15.0, predicted=22.958749771118164
>> actual=10.0, predicted=23.622652053833008
>> actual=9.0, predicted=23.67362403869629
>> actual=25.0, predicted=23.90199851989746
>> actual=19.0, predicted=23.206247329711914
>> actual=14.0, predicted=23.894834518432617
>> actual=14.0, predicted=23.627328872680664
>> actual=13.0, predicted=24.387636184692383
>> actual=18.0, predicted=23.90337562561035
>> actual=35.0, predicted=24.2421932220459
>> actual=25.0, predicted=24.03383445739746
>> actual=19.0, predicted=24.37618064880371
>> actual=13.0, predicted=23.93202781677246
>> actual=28.0, predicted=24.228940963745117
>> actual=13.0, predicted=23.033733367919922
>> actual=14.0, predicted=23.603410720825195
>> actual=15.0, predicted=23.061141967773438
>> actual=13.0, predicted=23.7595157623291
>> actual=18.0, predicted=23.451452255249023
>> actual=12.0, predicted=23.622989654541016
MSE ON TESTING: 63.039009816852854
~~~

**Describe the expected behavior**

The output of these models should be similar, or at least within the same ballpark.

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/1DZk1DHuN7MlgkS56-Zgsceg0OkWJ3WEI?usp=sharing
"
41216,Spectral Normalization,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): TF 2.2
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
Need to support Spectral Normalization as a tf.keras.layers.Wrapper to wrap dense and convolution layers.

**Will this change the current api? How?**
No

**Who will benefit with this feature?**
Spectral Normalization (https://arxiv.org/abs/1802.05957) is widely used for training generative adversarial networks. Many recent research use it and it is supported in pytorch ([torch.nn.utils.spectral_norm](https://pytorch.org/docs/master/generated/torch.nn.utils.spectral_norm.html)) for a long time now, but still no supported in tensorflow.

**Any Other info.**
The main difficulty in implementing Spectral Normalization is that it needs to update the layer kernel (e.g. the kernel of tf.keras.layers.Conv2D) using tf.Variable.assign() which is not permitted within a Replica context (e.g. building a convolution layer under tf.distribute.MirroredStrategy).
"
41213,MirroredStrategy slows down training,"My expectation was that the training time would be drastically reduced by using MirroredStrategy with multi-gpu (4 in my case). However I see the average step time gets increased significantly. Could you please advice on what is the problem?


**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):('v1.12.1-35610-gd8c49c2fde', '2.4.0-dev20200701')
  tf-estimator-nightly       2.4.0.dev2020063001
  tf-nightly                 2.4.0.dev20200701
- Python version: 3.7
- CUDA/cuDNN version: 


- GPU model and memory: 
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:00:1B.0 Off |                    0 |
| N/A   38C    P0    50W / 300W |  15450MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-SXM2...  On   | 00000000:00:1C.0 Off |                    0 |
| N/A   37C    P0    54W / 300W |  15450MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-SXM2...  On   | 00000000:00:1D.0 Off |                    0 |
| N/A   36C    P0    55W / 300W |  15522MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |
| N/A   37C    P0    55W / 300W |  15522MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+


== cuda libs  ===================================================
/usr/local/cuda-9.2/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.2/doc/man/man7/libcudart.7
/usr/local/cuda-9.2/lib64/libcudart.so.9.2.148
/usr/local/cuda-9.2/lib64/libcudart_static.a
/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.0/doc/man/man7/libcudart.7
/usr/local/cuda-9.0/lib64/libcudart.so.9.0.176
/usr/local/cuda-9.0/lib64/libcudart_static.a
/usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudart.so.10.1.243
/usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-10.1/doc/man/man7/libcudart.so.7
/usr/local/cuda-10.1/doc/man/man7/libcudart.7
/usr/local/cuda-10.2/targets/x86_64-linux/lib/libcudart.so.10.2.89
/usr/local/cuda-10.2/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-10.2/doc/man/man7/libcudart.so.7
/usr/local/cuda-10.2/doc/man/man7/libcudart.7
/usr/local/cuda-10.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-10.0/doc/man/man7/libcudart.7
/usr/local/cuda-10.0/lib64/libcudart.so.10.0.130
/usr/local/cuda-10.0/lib64/libcudart_static.a
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61
/usr/local/cuda-8.0/lib64/libcudart_static.a

== tensorflow installed from info ==================

== python version  ==============================================
(major, minor, micro, releaselevel, serial)
(3, 6, 10, 'final', 0)
**Describe the current behavior**

#average step for single-gpu
Epoch 2/50
8/8 [==============================] - 0s 5ms/step - loss: 0.7215 - accuracy: 0.5620 - val_loss: 0.6550 - val_accuracy: 0.7213

#average step for multi-gpu, is more than 10 times slower!!
Epoch 2/50
2/2 [==============================] - 0s 224ms/step - loss: 0.6756 - accuracy: 0.5702 - val_loss: 0.6435 - val_accuracy: 0.6885


**Describe the expected behavior**

I expect the average step time to be reduced to 1/4, given that there are 4 gpu's doing data splitting and processing in parallel

**Standalone code to reproduce the issue**

source code from: https://keras.io/examples/structured_data/structured_data_classification_from_scratch/

```
import tensorflow as tf
import numpy as np
import pandas as pd
from tensorflow import keras
from tensorflow.keras import layers

from tensorflow.keras.layers.experimental.preprocessing import Normalization
from tensorflow.keras.layers.experimental.preprocessing import CategoryEncoding
from tensorflow.keras.layers.experimental.preprocessing import StringLookup

#tf.debugging.set_log_device_placement(True)

file_url = ""https://storage.googleapis.com/applied-dl/heart.csv""
dataframe = pd.read_csv(file_url)

val_dataframe = dataframe.sample(frac=0.2, random_state=1337)
train_dataframe = dataframe.drop(val_dataframe.index)

print(
    ""Using %d samples for training and %d for validation""
    % (len(train_dataframe), len(val_dataframe))
)

def dataframe_to_dataset(dataframe):
    dataframe = dataframe.copy()
    labels = dataframe.pop(""target"")
    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))
    ds = ds.shuffle(buffer_size=len(dataframe))
    return ds


train_ds = dataframe_to_dataset(train_dataframe)
val_ds = dataframe_to_dataset(val_dataframe)

for x, y in train_ds.take(1):
    print(""Input:"", x)
    print(""Target:"", y)


def encode_numerical_feature(feature, name, dataset):
    normalizer = Normalization()

    feature_ds = dataset.map(lambda x, y: x[name])
    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))

    normalizer.adapt(feature_ds)

    encoded_feature = normalizer(feature)
    return encoded_feature


def encode_string_categorical_feature(feature, name, dataset):
    index = StringLookup()

    feature_ds = dataset.map(lambda x, y: x[name])
    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))

    index.adapt(feature_ds)

    encoded_feature = index(feature)

    encoder = CategoryEncoding(output_mode=""binary"")

    feature_ds = feature_ds.map(index)

    encoder.adapt(feature_ds)

    encoded_feature = encoder(encoded_feature)
    return encoded_feature


def encode_integer_categorical_feature(feature, name, dataset):
    encoder = CategoryEncoding(output_mode=""binary"")

    feature_ds = dataset.map(lambda x, y: x[name])
    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))

    encoder.adapt(feature_ds)

    encoded_feature = encoder(feature)
    return encoded_feature

def build_model():
    sex = keras.Input(shape=(1,), name=""sex"", dtype=""int64"")
    cp = keras.Input(shape=(1,), name=""cp"", dtype=""int64"")
    fbs = keras.Input(shape=(1,), name=""fbs"", dtype=""int64"")
    restecg = keras.Input(shape=(1,), name=""restecg"", dtype=""int64"")
    exang = keras.Input(shape=(1,), name=""exang"", dtype=""int64"")
    ca = keras.Input(shape=(1,), name=""ca"", dtype=""int64"")

    thal = keras.Input(shape=(1,), name=""thal"", dtype=""string"")

    age = keras.Input(shape=(1,), name=""age"")
    trestbps = keras.Input(shape=(1,), name=""trestbps"")
    chol = keras.Input(shape=(1,), name=""chol"")
    thalach = keras.Input(shape=(1,), name=""thalach"")
    oldpeak = keras.Input(shape=(1,), name=""oldpeak"")
    slope = keras.Input(shape=(1,), name=""slope"")

    all_inputs = [
        sex,
        cp,
        fbs,
        restecg,
        exang,
        ca,
        thal,
        age,
        trestbps,
        chol,
        thalach,
        oldpeak,
        slope,
    ]


    sex_encoded = encode_integer_categorical_feature(sex, ""sex"", train_ds)
    cp_encoded = encode_integer_categorical_feature(cp, ""cp"", train_ds)
    fbs_encoded = encode_integer_categorical_feature(fbs, ""fbs"", train_ds)
    restecg_encoded = encode_integer_categorical_feature(restecg, ""restecg"", train_ds)
    exang_encoded = encode_integer_categorical_feature(exang, ""exang"", train_ds)
    ca_encoded = encode_integer_categorical_feature(ca, ""ca"", train_ds)

    thal_encoded = encode_string_categorical_feature(thal, ""thal"", train_ds)

    age_encoded = encode_numerical_feature(age, ""age"", train_ds)
    trestbps_encoded = encode_numerical_feature(trestbps, ""trestbps"", train_ds)
    chol_encoded = encode_numerical_feature(chol, ""chol"", train_ds)
    thalach_encoded = encode_numerical_feature(thalach, ""thalach"", train_ds)
    oldpeak_encoded = encode_numerical_feature(oldpeak, ""oldpeak"", train_ds)
    slope_encoded = encode_numerical_feature(slope, ""slope"", train_ds)

    all_features = layers.concatenate(
        [
            sex_encoded,
            cp_encoded,
            fbs_encoded,
            restecg_encoded,
            exang_encoded,
            slope_encoded,
            ca_encoded,
            thal_encoded,
            age_encoded,
            trestbps_encoded,
            chol_encoded,
            thalach_encoded,
            oldpeak_encoded,
        ]
    )

    x = layers.Dense(32, activation=""relu"")(all_features)
    x = layers.Dropout(0.5)(x)
    output = layers.Dense(1, activation=""sigmoid"")(x)
    model = keras.Model(all_inputs, output)
    model.compile(""adam"", ""binary_crossentropy"", metrics=[""accuracy""])
    return model

model_single_gpu = build_model()
model_single_gpu.fit(train_ds.batch(32), epochs=50, validation_data=val_ds.batch(32))

mirrored_strategy = tf.distribute.MirroredStrategy()
with mirrored_strategy.scope():
    model_multi_gpu = build_model()
model_multi_gpu.fit(train_ds.batch(32*4), epochs=50, validation_data=val_ds.batch(32*4))

```"
41212,DataType error: DataType 6 is not recognized in Java (version 2.4.0),"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 9
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: custom
- TensorFlow installed from (source or binary):org.tensorflow:tensorflow-lite:0.0.0-nightly
- TensorFlow version (use command below):version 2.4.0
- Python version:
- Bazel version (if compiling from source):N/A
- GCC/Compiler version (if compiling from source): Android SDK Java API 28
- CUDA/cuDNN version: N/A - CPU
- GPU model and memory: N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
Loaded a TTS mel generator tflite model in java on the android target.
runForMultipleInputsOutputs function. The model takes 4 tensors as input with INT32 (1, 1), BOOL(1,1), INT32 and Float. 
Trying to print and feed input tensors with type boolean and dynamic size, however, it is crashing in printing and also later in 
It is printing first input tensor details, however crashes while printing the seconds tensor details
 
D/SnsTTS: input tensors count: 4
D/SnsTTS: tensor[0] Shape: [1, 1] data type: INT32
java.lang.IllegalArgumentException: DataType error: DataType 6 is not recognized in Java (version 2.4.0)
        at org.tensorflow.lite.DataType.fromC(DataType.java:79)
        at org.tensorflow.lite.Tensor.<init>(Tensor.java:484)
        at org.tensorflow.lite.Tensor.fromIndex(Tensor.java:44)
        at org.tensorflow.lite.NativeInterpreterWrapper.getInputTensor(NativeInterpreterWrapper.java:303)
        at org.tensorflow.lite.Interpreter.getInputTensor(Interpreter.java:423)

**Describe the expected behavior**
It should print the input tensor details, for boolean type and also accept it in the run function in java

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

private void printTensor(Tensor tensor, int index){
        int[] shape = tensor.shape();
        DataType dataType = tensor.dataType();
        Log.d(Tag, ""tensor["" + index + ""] Shape: "" + Arrays.toString(shape) + "" data type: "" + dataType.toString());
    }

    private void printInterpreterDetails(Interpreter interpreter){

        int inputTensorCount = interpreter.getInputTensorCount();
        Log.d(Tag, String.format(""input tensors count: %d"", inputTensorCount));
        for (int inputTensorIndex =0; inputTensorIndex<inputTensorCount; inputTensorIndex++) {
            printTensor(interpreter.getInputTensor(inputTensorIndex), inputTensorIndex);
        }
}

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Here is the link to python implementation
https://github.com/TensorSpeech/TensorflowTTS"
41208,Premature end of JPEG file,"Premature end of JPEG file
this is brain bug of tf team that can't imagine that if there is issue with image it should not be processed and should be skipped because after this error optimisation become some mess.

I get this error frequently then I try to train on some basic dataset from images downloaded from internet
Images are processed ad handled by OS and image software without problems and errors
I am using TF2.1 on conda dor windows but my problem is not that it get errors from time to time, but that TF team are not smart enough to:
1. print the path of file giving this error
2. check file health before to use it for training and if it is bad to skip it!!! or better optionally move it to some debug folder from where people could get it and inspect and analyze it, then fix it and return it back
As you understand for this issue, it is not important reason, but is important issue handling that is completely missing
error message Premature end of JPEG file is not handling of anything especially because in this case for normal people it is clear what must be done on this error
"
41207,model.predict_generator & model.fit [Keras] don't give the same prediction for the same image !!,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
N/A (google colab)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
n/a
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
1.15.2
- Python version:
3.6.9
- Keras version:
2.1.6


**Describe the current behavior**

Hi everyone,

I trained a model using Keras.model.fit_generator. However, when I use model.predict_generator(...) and model.predict(...), everything works well BUT I don't get the same predictions for the same image... I am aware of a similar problem [https://github.com/keras-team/keras/issues/3477](url) but I tried everything that was suggested (no augmentation used, shuffle=False, renamed my files,...) and still get different results... Anyone could help please?

**Describe the expected behavior**
I expect to have the same results...
"
41206,TF-Lite Ops supported (available),"Is there a way to check what operations (list of all operators) are supported and distinguish CPU vs. GPU or other platform specifics?
What is the best way to check their correction of operation and functionality?
"
41205,ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.,"hello i keep getting error 
(The max number of people in your video.
If no input and press Enter, the number of be set to default: 1 person.
The max number of people in your video: 2
--------------
If you want the detailed information of GIF, input yes.
If no input and press Enter, the generation setting of GIF will be set to default.
warn If you input warn, then no GIF will be generated.
the detailed information[yes/no/warn]: yes
Traceback (most recent call last):
  File ""C:\Users\laris\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\laris\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\laris\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\laris\anaconda3\envs\tensorflow\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\laris\anaconda3\envs\tensorflow\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""src/openpose_3dpose_sandbox_vmd.py"", line 9, in <module>
    import tensorflow as tf
  File ""C:\Users\laris\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\laris\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\__init__.py"", line 50, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\laris\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 69, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\laris\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\laris\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\laris\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\laris\anaconda3\envs\tensorflow\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\laris\anaconda3\envs\tensorflow\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
(tensorflow) C:\Users\laris\OneDrive\Bureaublad\OpenMMD_1.0\3d-pose-baseline-vmd>) 
 i trying to follow this video https://www.youtube.com/watch?v=hKx6jl9a5-I&t=293s
but i keep run problem after problem please someone help . i keep run into trials of errors 
"
41204,"""Tensor is unhashable"" and ""too many values to unpack"" with transformers","**System information:**

- OS: Ubuntu 20.04;
- Tensorflow: v2.2.0-rc4-8-g2b96f3662b 2.2.0. Installed used pip3;
- Tensorflow-gpu: 2.2.0. Installed used pip3;
- Python: 3.8.2;
- CUDA Version: 10.2;
- cuDNN: 7.6.5;
- GPU: GeForce GTX 1050 Ti;

**Description:**
I have successfully installed `tensorflow` and used it for a while. But now I want to use [transformers](https://github.com/huggingface/transformers) and I started getting problems, here is [my previous issues report](https://github.com/huggingface/transformers/issues/5555). But now I think that some of them are more related to the `tensorflow` then to the `transformers`. 
Here is my code:
```
df = pd.DataFrame({'text': ['SOME ANGRY TEXT!!!', 'Some friendly text :)'], 'label': [1, 0]})

def create_model():
    bert_model = transformers.TFBertModel.from_pretrained(""bert-base-cased"")
    
    input_ids = tf.keras.layers.Input(shape=(10,), dtype=tf.int32, name='input_ids')
    token_type_ids = tf.keras.layers.Input((10,), dtype=tf.int32, name='token_type_ids')
    attention_mask = tf.keras.layers.Input((10,), dtype=tf.int32, name='attention_mask')
    
    # Use pooled_output(hidden states of [CLS]) as sentence level embedding
    pooled_output = bert_model({'input_ids': input_ids, 'attention_mask': attention_mask, 'token_type_ids': token_type_ids})[1]
    x = tf.keras.layers.Dropout(rate=0.1)(pooled_output)
    x = tf.keras.layers.Dense(1, activation='sigmoid')(x)
    model = tf.keras.models.Model(inputs={'input_ids': input_ids, 'attention_mask': attention_mask, 'token_type_ids': token_type_ids}, outputs=x)
    return model

bert_model = create_model()
bert_tokenizer = transformers.BertTokenizer.from_pretrained(""bert-base-cased"")

x = bert_tokenizer.batch_encode_plus(
    df.text.values,
    max_length=10,
    pad_to_max_length=True, 
    return_tensors='tf'
)

bert_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['Accuracy'])

bert_history = bert_model.fit(
    x=x,
    y=df.label.values
)
```

Output:
```
~/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in __hash__(self)
    724     if (Tensor._USE_EQUALITY and executing_eagerly_outside_functions() and
    725         (g is None or g.building_function)):
--> 726       raise TypeError(""Tensor is unhashable. ""
    727                       ""Instead, use tensor.ref() as the key."")
    728     else:

TypeError: Tensor is unhashable. Instead, use tensor.ref() as the key.
```
After that I just tried to use [this working Kaggle notebook](https://www.kaggle.com/definedennis/pretrained-bert-with-huggingface-tensorflow-2-1/output)(it's working because it has output genarated on the Kaggle side, `train.csv` - [file from here](https://www.kaggle.com/c/nlp-getting-started/data)):
```
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from tqdm.notebook import tqdm

import tensorflow as tf
from tensorflow import keras
import tensorflow.keras.backend as K
from tensorflow.keras import layers
from tensorflow.keras.utils import plot_model
from transformers import (
    BertTokenizer,
    TFBertForSequenceClassification,
    TFBertModel,
    BertConfig,
)
tf.__version__

MAX_SEQUENCE_LENGTH = 255
PRETRAINED_MODEL_NAME = 'bert-base-uncased'
BATCH_SIZE = 32

df = pd.read_csv('train.csv')

df.head()

df['target'].value_counts()

df.isnull().sum()

data = df['text'].values
targets = df['target'].values

def create_model():
    bert_model = TFBertModel.from_pretrained(PRETRAINED_MODEL_NAME)
    
    input_ids = layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_ids')
    token_type_ids = layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='token_type_ids')
    attention_mask = layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='attention_mask')
    
    # Use pooled_output(hidden states of [CLS]) as sentence level embedding
    pooled_output = bert_model({'input_ids': input_ids, 'attention_mask': attention_mask, 'token_type_ids': token_type_ids})[1]
    x = layers.Dropout(rate=0.1)(pooled_output)
    x = layers.Dense(1, activation='sigmoid')(x)
    model = keras.models.Model(inputs={'input_ids': input_ids, 'attention_mask': attention_mask, 'token_type_ids': token_type_ids}, outputs=x)
    return model

tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)
model = create_model()

model.summary()

plot_model(model, to_file='model.png', expand_nested=True, show_shapes=True)

opt = tf.keras.optimizers.Adam(learning_rate=3e-5)
model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])

X_train, X_val, y_train, y_val = train_test_split(data, targets, test_size=0.33, random_state=42, stratify=targets)

X_train = tokenizer.batch_encode_plus(X_train, max_length=MAX_SEQUENCE_LENGTH, pad_to_max_length=True, return_tensors='tf')
X_val = tokenizer.batch_encode_plus(X_val, max_length=MAX_SEQUENCE_LENGTH, pad_to_max_length=True, return_tensors='tf')

history = model.fit(
    x=X_train,
    y=y_train,
    validation_data=(X_val, y_val),
    epochs=3,
    batch_size=BATCH_SIZE
)
```

Output:
```
/usr/lib/python3.8/_collections_abc.py in update(self, other, **kwds)
    835                 self[key] = other[key]
    836         else:
--> 837             for key, value in other:
    838                 self[key] = value
    839         for key, value in kwds.items():

ValueError: too many values to unpack (expected 2)
``` 

So, what is the problem and how can I fix it?"
41202,Dask and Tensorflow Compatibility,"There was a package called dask_tensorflow which is currently archived but they stated there are many way to get dask to work with tensorflow. 
So I thought I'd try it with the mnist model as follow:
```
%load_ext tensorboard
import tensorflow as tf
from tensorflow import keras
import datetime
import matplotlib.pyplot as plt x

EPOCHS = 50
BATCH_SIZE = 128
N_HIDDEN = 128
VALIDATION_SPLIT = 0.2
VERBOSE = 1
DROPOUT= 0.3
NB_CLASSES = 10
RESHAPED = 784

mnist = keras.datasets.mnist

(X_train, Y_train), (X_test, Y_test) = mnist.load_data()

Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)
Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)

import dask.array as da
import numpy as np
import joblib

X_train = X_train.reshape(60000, RESHAPED)
X_test = X_test.reshape(10000, RESHAPED)
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')

X_train /= 255
X_test /= 255
print(X_train.shape[0],'train samples')
print(X_test.shape[0], 'test samples')

X_train = da.from_array(np.asarray(X_train))
Y_train = da.from_array(np.asarray(Y_train))
X_test = da.from_array(np.asarray(X_test))
Y_test = da.from_array(np.asarray(Y_test))

model = tf.keras.models.Sequential()
model.add(keras.layers.Dense(NB_CLASSES,
                             input_shape = (RESHAPED,),
                             kernel_initializer = 'zeros',
                             name = 'dense_layer',
                             activation ='softmax'))

model.compile(optimizer ='SGD',
             loss= 'categorical_crossentropy',
             metrics = ['accuracy'])

with joblib.parallel_backend('dask'):
        model.fit(X_train,Y_train,
                  batch_size = BATCH_SIZE,
                  epochs = EPOCHS, 
                  verbose = VERBOSE, 
                  validation_split = VALIDATION_SPLIT)
```

But I get an error:

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-28-2b3bc3bdb162> in <module>
      4                   epochs = EPOCHS,
      5                   verbose = VERBOSE,
----> 6                   validation_split = VALIDATION_SPLIT)

~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
    106   def _method_wrapper(self, *args, **kwargs):
    107     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
--> 108       return method(self, *args, **kwargs)
    109 
    110     # Running inside `run_distribute_coordinator` already.

~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1034           data_adapter.train_validation_split((x, y, sample_weight),
   1035                                               validation_split=validation_split,
-> 1036                                               shuffle=False))
   1037 
   1038     with self.distribute_strategy.scope(), \

~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py in train_validation_split(arrays, validation_split, shuffle)
   1393     raise ValueError(
   1394         ""`validation_split` is only supported for Tensors or NumPy ""
-> 1395         ""arrays, found following types in the input: {}"".format(unsplitable))
   1396 
   1397   if all(t is None for t in flat_arrays):

ValueError: `validation_split` is only supported for Tensors or NumPy arrays, found following types in the input: [<class 'dask.array.core.Array'>, <class 'dask.array.core.Array'>]

```
Does this mean tensorflow 2.0 no longer supports working with Dask Arrays?"
41200,Usage model's intermediate layer output in custom loss causes OOM,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2 ('tf.version.GIT_VERSION, tf.version.VERSION' output: v2.2.0-rc4-8-g2b96f3662b 2.2.0)
- Python version: 3.7.5
- CUDA/cuDNN version: 10.1
- GPU model and memory: RTX 2080 Ti

**Describe the current behavior**
Usage model's intermediate layer output in custom loss function causes OOM.
I found the issue working on some custom loss function, but report the issue using much simplified toy example (the code is below). The code is based on this example https://www.tensorflow.org/guide/eager#variables_and_optimizers (conceptually I only added `+ x` in loss function, details are below)
It crashes with error (full script's output is attached):
`tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[2000,2000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:RealDiv]`

**Describe the expected behavior**
The below code runs ok if in the line...
`  error = model(inputs) - targets + x  # dummy operation just to use intermediate layer in loss  `
...you remove + x like this
`  error = model(inputs) - targets `
So it feels like the root cause is usage intermediate tensor in my loss function. I need to reference intermediate model layer output in custom loss function and being able to train model.

**Standalone code to reproduce the issue**
```
import tensorflow as tf

input = tf.keras.Input(shape=(1,))
x = tf.keras.layers.Dense(1)(input)

model = tf.keras.Model(inputs=input, outputs=x)

# A toy dataset of points around 3 * x + 2
NUM_EXAMPLES = 2000
training_inputs = tf.random.normal([NUM_EXAMPLES])
noise = tf.random.normal([NUM_EXAMPLES])
training_outputs = training_inputs * 3 + 2 + noise


def loss(model, inputs, targets):
  error = model(inputs) - targets + x  # dummy operation just to use intermediate layer in loss (but conceptually it's case from a real project as many useful losses need reference some intermediate model's layers outputs)
  return tf.reduce_mean(tf.square(error))


def grad(model, inputs, targets):
  with tf.GradientTape() as tape:
    loss_value = loss(model, inputs, targets)
  return tape.gradient(loss_value, model.trainable_variables)


optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)

steps = 300
for i in range(steps):
  grads = grad(model, training_inputs, training_outputs)
  optimizer.apply_gradients(zip(grads, model.trainable_variables))

```

**Other info / logs** 
[scriptoutput.txt](https://github.com/tensorflow/tensorflow/files/4891146/scriptoutput.txt)"
41196,Issue with CUDA and cuDNN,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): stable-baselines library
- TensorFlow version: 1.15.3 also TensorFlow-gpu 1.15.0
- Python version: 3.7.7
- Installed using virtualenv? pip? conda?: Created conda environment using PyCharm
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA release 10.0, V10.0.130 / cuDNN 7.4.2
- GPU model and memory: NVIDIA GeForce GTX 1050

I am trying to train models with images as observations. I am using A2C and PPO2 with the Pendulum-v1 environment from OpenAI Gym. I am also using stable-baselines, a reinforcement learning library. Here is the complete code that I am using:

```
import gym
import numpy as np
from gym.envs.classic_control import PendulumEnv
from stable_baselines.common.env_checker import check_env
from stable_baselines.sac.policies import CnnPolicy
from stable_baselines import PPO2

from skimage import data, color
from skimage.transform import rescale, resize, downscale_local_mean

from gym import Wrapper, spaces

import tensorflow as tf
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession


class RGBArrayAsObservationWrapper(Wrapper):
    """"""
    Use env.render(rgb_array) as observation
    rather than the observation environment provides
    """"""

    def __init__(self, env):
        # TODO this might not work before environment has been reset
        super(RGBArrayAsObservationWrapper, self).__init__(env)
        self.reset()
        dummy_obs = env.render('rgb_array')
        dummy_obs_resized = resize(dummy_obs, (dummy_obs.shape[0] // 10, dummy_obs.shape[1] // 10),
                                   anti_aliasing=True)
        # Update observation space
        # TODO assign correct low and high
        self.observation_space = spaces.Box(low=0, high=255, shape=dummy_obs_resized.shape,
                                            dtype=dummy_obs_resized.dtype)

    def reset(self, **kwargs):
        obs = self.env.reset(**kwargs)
        obs = self.env.render(""rgb_array"")
        obs = resize(obs, (obs.shape[0] // 10, obs.shape[1] // 10),
                     anti_aliasing=True)
        return obs

    def step(self, action):
        obs, reward, done, info = self.env.step(action)
        obs = self.env.render(""rgb_array"")
        obs = resize(obs, (obs.shape[0] // 10, obs.shape[1] // 10),
                     anti_aliasing=True)
        return obs, reward, done, info


# tensorboard --logdir=PPO2_IMG_PENDULUM:C:\Users\meric\OneDrive\Masaüstü\TUM\Thesis\Pycharm\pioneer\ppo2_pendulum_tensorboard --host localhost


config = ConfigProto()
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

TEST_COUNT = 100

pendulum_env = PendulumEnv()
pendulum_env = RGBArrayAsObservationWrapper(pendulum_env)
check_env(pendulum_env, warn=True)

model = PPO2(""CnnPolicy"", pendulum_env, verbose=1, tensorboard_log=""./ppo2_pendulum_tensorboard/"")
model.learn(total_timesteps=100_000, log_interval=10)
model.save(""ppo2_pendulum"")

sum_rewards = 0
done = False
obs = pendulum_env.reset()
for i in range(TEST_COUNT):
    while not done:
        action, _states = model.predict(obs)
        obs, rewards, done, info = pendulum_env.step(action)
        pendulum_env.render()
        sum_rewards += rewards

    pendulum_env.reset()
    done = False

print(sum_rewards / TEST_COUNT)

```
And here is the complete output in terminal.
```
2020-07-08 13:35:50.450192: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

2020-07-08 13:35:53.202978: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2020-07-08 13:35:53.206120: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2020-07-08 13:35:53.236974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce GTX 1050 major: 6 minor: 1 memoryClockRate(GHz): 1.493
pciBusID: 0000:01:00.0
2020-07-08 13:35:53.237468: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2020-07-08 13:35:53.242817: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
2020-07-08 13:35:53.246183: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_100.dll
2020-07-08 13:35:53.247490: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_100.dll
2020-07-08 13:35:53.252276: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_100.dll
2020-07-08 13:35:53.255771: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_100.dll
2020-07-08 13:35:53.268419: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-07-08 13:35:53.268904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2020-07-08 13:35:53.903957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-08 13:35:53.904131: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2020-07-08 13:35:53.904234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2020-07-08 13:35:53.904599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3001 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1)
C:\Users\meric\Anaconda3\envs\pioneer\lib\site-packages\stable_baselines\common\env_checker.py:25: UserWarning: It seems that your observation is an image but the `dtype` of your observation_space is not `np.uint8`. If your observation is not an image, we recommend you to flatten the observation to have only a 1D vector
  warnings.warn(""It seems that your observation is an image but the `dtype` ""
C:\Users\meric\Anaconda3\envs\pioneer\lib\site-packages\stable_baselines\common\env_checker.py:210: UserWarning: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) cf https://stable-baselines.readthedocs.io/en/master/guide/rl_tips.html
  warnings.warn(""We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) ""
Wrapping the env in a DummyVecEnv.
WARNING:tensorflow:From C:\Users\meric\Anaconda3\envs\pioneer\lib\site-packages\stable_baselines\common\tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From C:\Users\meric\Anaconda3\envs\pioneer\lib\site-packages\stable_baselines\common\tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2020-07-08 13:35:55.069303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce GTX 1050 major: 6 minor: 1 memoryClockRate(GHz): 1.493
pciBusID: 0000:01:00.0
2020-07-08 13:35:55.069545: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2020-07-08 13:35:55.069705: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
2020-07-08 13:35:55.069866: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_100.dll
2020-07-08 13:35:55.070064: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_100.dll
2020-07-08 13:35:55.070230: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_100.dll
2020-07-08 13:35:55.070391: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_100.dll
2020-07-08 13:35:55.070552: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-07-08 13:35:55.070737: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2020-07-08 13:35:55.070901: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-08 13:35:55.071070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2020-07-08 13:35:55.071172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2020-07-08 13:35:55.071344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3001 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1)
WARNING:tensorflow:From C:\Users\meric\Anaconda3\envs\pioneer\lib\site-packages\stable_baselines\common\policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From C:\Users\meric\Anaconda3\envs\pioneer\lib\site-packages\stable_baselines\common\input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From C:\Users\meric\Anaconda3\envs\pioneer\lib\site-packages\stable_baselines\common\tf_layers.py:103: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

WARNING:tensorflow:From C:\Users\meric\Anaconda3\envs\pioneer\lib\site-packages\stable_baselines\common\distributions.py:418: The name tf.random_normal is deprecated. Please use tf.random.normal instead.

WARNING:tensorflow:From C:\Users\meric\Anaconda3\envs\pioneer\lib\site-packages\stable_baselines\ppo2\ppo2.py:190: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From C:\Users\meric\Anaconda3\envs\pioneer\lib\site-packages\stable_baselines\ppo2\ppo2.py:198: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.

WARNING:tensorflow:From C:\Users\meric\Anaconda3\envs\pioneer\lib\site-packages\tensorflow_core\python\ops\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From C:\Users\meric\Anaconda3\envs\pioneer\lib\site-packages\stable_baselines\ppo2\ppo2.py:206: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From C:\Users\meric\Anaconda3\envs\pioneer\lib\site-packages\stable_baselines\ppo2\ppo2.py:240: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.

WARNING:tensorflow:From C:\Users\meric\Anaconda3\envs\pioneer\lib\site-packages\stable_baselines\ppo2\ppo2.py:242: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

WARNING:tensorflow:From C:\Users\meric\Anaconda3\envs\pioneer\lib\site-packages\stable_baselines\common\base_class.py:1169: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

2020-07-08 13:35:55.970675: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
2020-07-08 13:35:56.208807: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-07-08 13:35:57.076059: E tensorflow/stream_executor/cuda/cuda_dnn.cc:319] Loaded runtime CuDNN library: 7.4.2 but source was compiled with: 7.6.0.  CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.
2020-07-08 13:35:57.079198: E tensorflow/stream_executor/cuda/cuda_dnn.cc:319] Loaded runtime CuDNN library: 7.4.2 but source was compiled with: 7.6.0.  CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.
Traceback (most recent call last):
  File ""C:\Users\meric\Anaconda3\envs\pioneer\lib\site-packages\tensorflow_core\python\client\session.py"", line 1365, in _do_call
    return fn(*args)
  File ""C:\Users\meric\Anaconda3\envs\pioneer\lib\site-packages\tensorflow_core\python\client\session.py"", line 1350, in _run_fn
    target_list, run_metadata)
  File ""C:\Users\meric\Anaconda3\envs\pioneer\lib\site-packages\tensorflow_core\python\client\session.py"", line 1443, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.
  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[{{node model/c1/Conv2D}}]]
	 [[output/strided_slice_1/_9]]
  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[{{node model/c1/Conv2D}}]]
0 successful operations.
0 derived errors ignored.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:/Users/meric/OneDrive/Masaüstü/TUM/Thesis/Pycharm/pioneer/pendulum_image_PPO2.py"", line 65, in <module>
    model.learn(total_timesteps=100_000, log_interval=10)
  File ""C:\Users\meric\Anaconda3\envs\pioneer\lib\site-packages\stable_baselines\ppo2\ppo2.py"", line 336, in learn
    rollout = self.runner.run(callback)
  File ""C:\Users\meric\Anaconda3\envs\pioneer\lib\site-packages\stable_baselines\common\runners.py"", line 48, in run
    return self._run()
  File ""C:\Users\meric\Anaconda3\envs\pioneer\lib\site-packages\stable_baselines\ppo2\ppo2.py"", line 472, in _run
    actions, values, self.states, neglogpacs = self.model.step(self.obs, self.states, self.dones)
  File ""C:\Users\meric\Anaconda3\envs\pioneer\lib\site-packages\stable_baselines\common\policies.py"", line 576, in step
    {self.obs_ph: obs})
  File ""C:\Users\meric\Anaconda3\envs\pioneer\lib\site-packages\tensorflow_core\python\client\session.py"", line 956, in run
    run_metadata_ptr)
  File ""C:\Users\meric\Anaconda3\envs\pioneer\lib\site-packages\tensorflow_core\python\client\session.py"", line 1180, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\Users\meric\Anaconda3\envs\pioneer\lib\site-packages\tensorflow_core\python\client\session.py"", line 1359, in _do_run
    run_metadata)
  File ""C:\Users\meric\Anaconda3\envs\pioneer\lib\site-packages\tensorflow_core\python\client\session.py"", line 1384, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.
  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[node model/c1/Conv2D (defined at \Users\meric\Anaconda3\envs\pioneer\lib\site-packages\tensorflow_core\python\framework\ops.py:1748) ]]
	 [[output/strided_slice_1/_9]]
  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[node model/c1/Conv2D (defined at \Users\meric\Anaconda3\envs\pioneer\lib\site-packages\tensorflow_core\python\framework\ops.py:1748) ]]
0 successful operations.
0 derived errors ignored.

Original stack trace for 'model/c1/Conv2D':
  File ""/Users/meric/OneDrive/Masaüstü/TUM/Thesis/Pycharm/pioneer/pendulum_image_PPO2.py"", line 64, in <module>
    model = PPO2(""CnnPolicy"", pendulum_env, verbose=1, tensorboard_log=""./ppo2_pendulum_tensorboard/"")
  File ""\Users\meric\Anaconda3\envs\pioneer\lib\site-packages\stable_baselines\ppo2\ppo2.py"", line 97, in __init__
    self.setup_model()
  File ""\Users\meric\Anaconda3\envs\pioneer\lib\site-packages\stable_baselines\ppo2\ppo2.py"", line 131, in setup_model
    n_batch_step, reuse=False, **self.policy_kwargs)
  File ""\Users\meric\Anaconda3\envs\pioneer\lib\site-packages\stable_baselines\common\policies.py"", line 602, in __init__
    feature_extraction=""cnn"", **_kwargs)
  File ""\Users\meric\Anaconda3\envs\pioneer\lib\site-packages\stable_baselines\common\policies.py"", line 559, in __init__
    pi_latent = vf_latent = cnn_extractor(self.processed_obs, **kwargs)
  File ""\Users\meric\Anaconda3\envs\pioneer\lib\site-packages\stable_baselines\common\policies.py"", line 25, in nature_cnn
    layer_1 = activ(conv(scaled_images, 'c1', n_filters=32, filter_size=8, stride=4, init_scale=np.sqrt(2), **kwargs))
  File ""\Users\meric\Anaconda3\envs\pioneer\lib\site-packages\stable_baselines\common\tf_layers.py"", line 107, in conv
    return bias + tf.nn.conv2d(input_tensor, weight, strides=strides, padding=pad, data_format=data_format)
  File ""\Users\meric\Anaconda3\envs\pioneer\lib\site-packages\tensorflow_core\python\ops\nn_ops.py"", line 2010, in conv2d
    name=name)
  File ""\Users\meric\Anaconda3\envs\pioneer\lib\site-packages\tensorflow_core\python\ops\gen_nn_ops.py"", line 1071, in conv2d
    data_format=data_format, dilations=dilations, name=name)
  File ""\Users\meric\Anaconda3\envs\pioneer\lib\site-packages\tensorflow_core\python\framework\op_def_library.py"", line 794, in _apply_op_helper
    op_def=op_def)
  File ""\Users\meric\Anaconda3\envs\pioneer\lib\site-packages\tensorflow_core\python\util\deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""\Users\meric\Anaconda3\envs\pioneer\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 3357, in create_op
    attrs, op_def, compute_device)
  File ""\Users\meric\Anaconda3\envs\pioneer\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 3426, in _create_op_internal
    op_def=op_def)
  File ""\Users\meric\Anaconda3\envs\pioneer\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 1748, in __init__
    self._traceback = tf_stack.extract_stack()


Process finished with exit code 1

```



 My code was running smoothly before I tried putting cufft64_100.dll, curand64_100.dll, cusolver64_100.dll, cusparse64_100.dll, cudnn64_7.dll in my environment library in the anaconda3 folder.  My goal is to use my GPU during training so it is faster. Any help would be appreciated. I am also okay with deleting my anaconda environment and uninstalling all the CUDA and cuDNN files and starting from scratch if someone could help me on how to do this correctly. I need tensorflow 1.15 any other version does not work for me.
"
41195,module 'tensorflow.keras.layers' has no attribute 'Conv1DTranspose',"Hello,

I am trying to import tf.keras.layers.conv1DTranspose in colab and this creates an error message.
I have followed this fix: https://github.com/tensorflow/tensorflow/issues/40937

However, this makes tensorflow probability import to fail:
----> 3 import tensorflow_probability as tfp
      4 import numpy as np
      5 from sklearn import preprocessing

4 frames
/usr/local/lib/python3.6/dist-packages/tensorflow_probability/python/experimental/auto_batching/frontend.py in <module>()
     43 from tensorflow.python.autograph.converters import return_statements
     44 from tensorflow.python.autograph.core import converter
---> 45 from tensorflow.python.autograph.core import naming
     46 from tensorflow.python.autograph.pyct import anno
     47 from tensorflow.python.autograph.pyct import inspect_utils

ImportError: cannot import name 'naming'

Thanks for your help!

Marc"
41194,Building a custom delegate as shared library,"## Description of the system/problem
My research group is currently working on a custom delegate to accelerate neural networks using FPGAs.
We wanted to build a custom delegate for the inference of the neural networks using the tensorflow lite interpreter.
To achieve this we wanted to build a shared library which can be loaded by ""tflite.load_delegate"" - using the the tflite_runtime in python.

I created the delegate using c++, with help of the official tensorflow lite delegate [guide](https://www.tensorflow.org/lite/performance/delegates) and the source code of the [nnapi delegate](https://github.com/tensorflow/tensorflow/blob/v2.1.1/tensorflow/lite/delegates/nnapi/nnapi_delegate.cc).
For compiling and linking I used CMake and built a static library of tensorflow lite using the [build scripts](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/make). I made sure, to use the same git repository version of tensorflow, as the newest runtime wheel build did (tflite_runtime-2.1.0.post1) and which I am using.

Writing and compiling a small inference example in c++ using my delegate and the compiled static library works as expected and without errors. If I want to use my delegate as shared library in combination with the tflite_runtime, it comes to weird behaviors:  
The tensors seem not to be allocated directly. Data access / Dimension array access are leading to segmentation faults. Allocation Types and Data Types containing really large numbers instead of enum indices.

I suppose there is something wrong in the way I compile my shared library. I will append the source code I used.
Any help is appreciated!

## Source Code
Delegate source code, with minimal example. Requires compiled tensorflow lite static library:
~~[delegate.zip]~~

Python minimal example using tflite_runtime-2.1.0.post1 and numpy:
[python_example.zip](https://github.com/tensorflow/tensorflow/files/4890286/python_example.zip)

Used tensorflow git version: `0.6.0-76902-gd855adfc5a` (output of `git describe`)

Expected output from delegate:
```
OutputIndex: 10 
Using outputIndex: 10
Allocation type is: 2 kTfLiteArenaRw
tensor type: 9 INT8
dims pointer: 0x557c461b2d20 is null: 0
dims: [1 75 75 64  ]
```

Given output using delegate shared library:
```
OutputIndex: 10 
Using outputIndex: 10
Allocation type is: 33539632 UNKNOWN!
tensor type: 1016998161 Unknown type
dims pointer: 0x15f900 is null: 0
Segmentation fault (core dumped)
```


"
41193,DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,"Deprecation warnings as in the subject:
```
home/bjourne/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py:546
  /home/bjourne/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py:546: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.9 it will stop working
    class IteratorBase(collections.Iterator, trackable.Trackable,

/home/bjourne/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:106
  /home/bjourne/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:106: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.9 it will stop working
    class DatasetV2(collections.Iterable, tracking_base.Trackable,
```
These warnings should be very easy to fix using conditional imports. tf version 2.4.0-dev20200708"
41191,C++ Program crashed while running tensorflow 2.0.0 with cuda 10.0 on Tegra Tx2,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): jetpack 4.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below):2.0.0
- Python version: 2.7
- Bazel version (if compiling from source):0.26.1
- GCC/Compiler version (if compiling from source):7.5
- CUDA/cuDNN version:10.0/7.6.3
- GPU model and memory: tegra tx2


**Describe the current behavior**
when I try running sample application it is crashing. With error logs as ""CUDA runtime implicit initialization on GPU:0 failed. Status: unknown error""

**Describe the expected behavior**
it should start session

**Standalone code to reproduce the issue**
Sample Code  **example.cpp**
```
#include <tensorflow/core/platform/env.h>
#include <tensorflow/core/public/session.h>
#include <iostream>
using namespace std;
using namespace tensorflow;

int main()
{
    Session* session;
    Status status = NewSession(SessionOptions(), &session);
    if (!status.ok()) {
        cout << status.ToString() << ""\n"";
    }
    session->Close();
    cout << ""Session successfully created.\n"";
}

```

Compile with the following command
```
g++ -c -std=c++11 -Wall -Wno-variadic-macros -g -O3 -fPIC -pedantic -I/usr/local/bin example.cpp -o example.o
g++ -MM -std=c++11 -Wall -Wno-variadic-macros -g -O3 -fPIC -pedantic -I/usr/local/bin example.cpp > example.d
g++ -o example example.o -std=c++11 -Wall -Wno-variadic-macros -g -O3 -fPIC -pedantic -L/usr/local/lib -ltensorflow_cc -lprotobuf

```

**Other info / logs**

```
$ ./example 
2020-07-08 15:01:34.122381: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-07-08 15:01:34.127413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:973] ARM64 does not support NUMA - returning NUMA node zero
2020-07-08 15:01:34.127566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: NVIDIA Tegra X2 major: 6 minor: 2 memoryClockRate(GHz): 1.3
pciBusID: 0000:00:00.0
2020-07-08 15:01:34.127608: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2020-07-08 15:01:34.127693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:973] ARM64 does not support NUMA - returning NUMA node zero
2020-07-08 15:01:34.127815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:973] ARM64 does not support NUMA - returning NUMA node zero
2020-07-08 15:01:34.127879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: unknown error
Segmentation fault (core dumped)

```"
41190,_0_QueueInput/input_queue: Skipping cancelled enqueue attempt with queue not closed,"I am trying to start the training mask rcnn of tensorpack:
```
python train.py --config DATA.BASEDIR=/home/n/car/car MODE_FPN=True ""DATA.VAL=('balloon_val',)""  ""DATA.TRAIN=('balloon_train',)"" TRAIN.BASE_LR=1e-3 TRAIN.EVAL_PERIOD=0 ""TRAIN.LR_SCHEDULE=[1000]"" ""PREPROC.TRAIN_SHORT_EDGE_SIZE=[600,1200]"" TRAIN.CHECKPOINT_PERIOD=1 DATA.NUM_WORKERS=1 --load COCO-MaskRCNN-R50FPN2x.npz --logdir log_full
```
And exactly after epoch 20 Iam getting:
```
2020-07-08 10:21:42.621628: W tensorflow/core/kernels/queue_base.cc:277] _0_QueueInput/input_queue: Skipping cancelled enqueue attempt with queue not closed
```
Environment:
```
sys.platform          linux
Python                3.6.10 |Anaconda, Inc.| (default, May  8 2020, 02:54:21) [GCC 7.3.0]
Tensorpack            v0.10.1-0-g8f831349
Numpy                 1.19.0
TensorFlow            1.13.1/b'v1.13.1-0-g6612da8951'
TF Compiler Version   4.8.5
TF CUDA support       True
TF MKL support        False
Nvidia Driver         /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.440.33.01
CUDA                  /usr/local/cuda-10.0/lib64/libcudart.so.10.0.130
CUDNN                 /usr/local/cuda-10.0/lib64/libcudnn.so.7.5.1
NCCL
CUDA_VISIBLE_DEVICES  Unspecified
GPU 0,1,2,3,4,5,6,7   Tesla K80
Free RAM              475.14/480.07 GB
CPU Count             32
cv2                   4.2.0
msgpack               1.0.0
python-prctl          False
```
I have tried changing:
```
 MODE_FPN=True/False
```
However with ""False"" Im getting worse results.

Here is the [link](https://paste.ofcode.org/ZsreU9FeP4EbJKqsGmK3nm)

What is happening ? And how to fix it ?"
41189,Autograph applied to Keras Custom Loss during Eager Execution,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, x64
- TensorFlow installed from (source or binary): binary (pip)
- TensorFlow version (use command below): TF 2.4.0.dev20200707
- Python version: 3.7
- CUDA/cuDNN version: 10.1 / 7.6
- GPU model and memory: Bug appears on several computers with different GPU


**Describe the current behavior**
Tensorflow applies AutoGraph to keras custom loss even in eager execution, meaning that we can't debug the loss anymore (unless using tf.print). This did not happen in previous versions of Tensorflow.
Notice that it both happens when run_eagerly is set to True in model.compile() and when tf.config.run_functions_eagerly is set to True.

**Describe the expected behavior**
When run_eagerly=True is passed to the model during compilation, we should expect Tensorflow to run eagerly in the loss function.

**Standalone code to reproduce the issue**

```
import numpy as np
import tensorflow as tf
from tensorflow import keras

# Custom Model. Autograph is not applied in eager execution so debugging is possible.
class CustomModel(keras.models.Model):
	def __init__(self):
		super(CustomModel, self).__init__()
		self.layer = tf.keras.layers.Dense(3) # Can debug here

	def call(self, inputs, training=None, mask=None):
		x = self.layer(inputs) # Can debug here
		return x

# Custom Loss. AutoGraph is applied in eager execution so debugging is impossible.
class CustomLoss(keras.losses.Loss):
	def call(self, y_true, y_pred):
		x = tf.reduce_mean(tf.abs(y_pred-y_true)) # Cannot debug here
		return x

if __name__ == '__main__':
	data = np.random.random((1000, 3)).astype(np.float32)

	model = CustomModel()

	model.compile(loss=CustomLoss(), run_eagerly=True)
	model.fit(x=data, y=data, batch_size=32)
```


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
41188,DLL,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
41187, module 'tensorflow' has no attribute 'compat' in 2.2.0,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow version: 2.2.0
- Python version: 3.8.2
- Installed using virtualenv? pip? conda?: pip
- CUDA/cuDNN version: 10.1
- GPU model and memory: 

After installing tensorflow 2.2.0 and CUDA 10.1 by using pip but when I am importing tensorflow, I get this error - module 'tensorflow' has no attribute 'compat' . "
41186,ModuleNotFoundError: No module named 'tensorflow_core.keras',"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10, anaconda
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Nope
- TensorFlow installed from (source or binary): Anaconda Navigator
- TensorFlow version (use command below):  tensorflow 2.0
- Python version: 3.6.10
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

I wrote the code below, and I came up with an error.(error report is attached under the code)
------------------------------------------------------------------------------------------------------------------------------------------
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

import pandas as pd
import numpy as np
import tensorflow as tf

np.random.seed(3)
tf.random.set_seed(3)

'''
dataset = np.loadtxt(""C:\\Users\\tongt\\PycharmProjects\\kaggle\\titanic\\train_sample1.csv"", delimiter= ',')
Y = dataset[:, 0]
X = dataset[:, 1:6+1]
'''
df= pd.read_csv(""C:\\Users\\tongt\\PycharmProjects\\kaggle\\train_sample1.csv"")

dataset = df.values
X = dataset[:, 2:6+1]
Y = dataset[:, 1]

model = Sequential()
model.add(Dense(12, input_dim=6, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics = ['accracy'])

model.fit(X, Y, epochs = 200, batch_size = 10)
--------------------------------------------------------------------------------------------------------------------------------------

C:\Users\tongt\anaconda3\envs\ML\python.exe C:/Users/tongt/PycharmProjects/kaggle/code.py
Traceback (most recent call last):
  File ""C:/Users/tongt/PycharmProjects/kaggle/code.py"", line 1, in <module>
    from tensorflow.keras.models import Sequential
  File ""C:\Users\tongt\anaconda3\envs\ML\lib\site-packages\tensorflow\__init__.py"", line 101, in <module>
    from tensorflow_core import *
  File ""C:\Users\tongt\anaconda3\envs\ML\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\tongt\anaconda3\envs\ML\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\tongt\anaconda3\envs\ML\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\tongt\anaconda3\envs\ML\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\tongt\anaconda3\envs\ML\lib\site-packages\tensorflow_core\python\__init__.py"", line 75, in <module>
    from tensorflow.python.framework.framework_lib import *  # pylint: disable=redefined-builtin
  File ""C:\Users\tongt\anaconda3\envs\ML\lib\site-packages\tensorflow_core\python\framework\framework_lib.py"", line 25, in <module>
    from tensorflow.python.framework.ops import Graph
  File ""C:\Users\tongt\anaconda3\envs\ML\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 58, in <module>
    from tensorflow.python.platform import app
  File ""C:\Users\tongt\anaconda3\envs\ML\lib\site-packages\tensorflow_core\python\platform\app.py"", line 23, in <module>
    from absl.app import run as _run
  File ""C:\Users\tongt\anaconda3\envs\ML\lib\site-packages\absl\app.py"", line 35, in <module>
    import pdb
  File ""C:\Users\tongt\anaconda3\envs\ML\lib\pdb.py"", line 76, in <module>
    import code
  File ""C:\Users\tongt\PycharmProjects\kaggle\code.py"", line 1, in <module>
    from tensorflow.keras.models import Sequential
  File ""C:\Users\tongt\anaconda3\envs\ML\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\tongt\anaconda3\envs\ML\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\tongt\anaconda3\envs\ML\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named 'tensorflow_core.keras'

----------------------------------------------------------------------------------------------------------------------------------------------

I already searched google and tensorflow document to fix this problem, but I cannot handle this one... 
So I tried to reinstall anaconda and pycharm. But it didn't work..


**Describe the expected behavior**
dd
**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
41185,How to gather the last X indices embeddings before every grouping of False in a boolean mask for every row in a batch,"Urgent help needed in doing the below operation. Mask for input is given, its shape is [batch size, no of timesteps]. From this, I need to collect X number of embeddings of shape [batch size, timestep index, embedding size] such that they are grouped just before each grouping of False.

say mask of a batch size of 1 is T,T,T,F,F,F,F,|T,T,F,F,F,F,F and X=2 (by '|', I assumed a break which indicates a row split length=7), then should get list of concatenated embeddings given by indices (1,2) (8, 9). In the case when X is greater than number of Trues, should return the embeddings from index of first True till index which is X away from it.

Should be able to replicate the above when batch size is variable without having to do individually for each batch as my batch size is pretty high. Output should be [ [ (1,2) , (.,.),.. for other batches at first row split (0:7) ] , [ (8,9) , (.,.) for other batches at second row split (7:14)], ..]

Any help will be highly appreciated.
"
41184,C++ Program crashed while running tensorflow 2.0.0 with cuda 10.0 on NVIDIA Tegra Tx2,"I am trying to build tensorflow2.0.0 on tx2. below are the details.

**System information**
- OS Platform and Distribution: jetpack 4.3
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.0.0
- Python version: 2.7
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): 7.5
- CUDA/cuDNN version: 10.0/7.6.3
- GPU model and memory: Tegra TX2

**Describe the current behavior**
when I try running sample application it is crashing. With error logs as ""CUDA runtime implicit initialization on GPU:0 failed. Status: unknown error""

**Describe the expected behavior**
it should start a session.

**Standalone code to reproduce the issue**
Sample Code 
```
#include <tensorflow/core/platform/env.h>
#include <tensorflow/core/public/session.h>
#include <iostream>
using namespace std;
using namespace tensorflow;

int main()
{
    Session* session;
    Status status = NewSession(SessionOptions(), &session);
    if (!status.ok()) {
        cout << status.ToString() << ""\n"";
    }
    session->Close();
    cout << ""Session successfully created.\n"";
}

```

CMakeLists.txt
```
cmake_minimum_required(VERSION 3.10)

project(test_hello)

set(TENSORFLOW_LIBRARIES tensorflow_cc protobuf)
add_executable(example example.cpp)
set(CUDA_NVCC_FLAGS
        ${CUDA_NVCC_FLAGS};
        -O3 -gencode arch=compute_30,code=sm_30;
        --std=c++11
        )
set(CMAKE_CXX_STANDARD 11)
set(CMAKE_CXX_STANDARD_REQUIRED ON)

# Link the Tensorflow library.
# TensorFlow headers
include_directories(""/usr/local/include/"")
include_directories(""/usr/local/include/tensorflow/"")
include_directories(""/usr/local/include/third-party/"")
target_link_libraries(example ""/usr/local/lib/libtensorflow_cc.so"")

# You may also link cuda if it is available.
find_package(CUDA)
if(CUDA_FOUND)
  target_link_libraries(example ${CUDA_LIBRARIES})
endif()

```

**Other info / logs**

```
$ ./example 
2020-07-08 15:01:34.122381: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-07-08 15:01:34.127413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:973] ARM64 does not support NUMA - returning NUMA node zero
2020-07-08 15:01:34.127566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: NVIDIA Tegra X2 major: 6 minor: 2 memoryClockRate(GHz): 1.3
pciBusID: 0000:00:00.0
2020-07-08 15:01:34.127608: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2020-07-08 15:01:34.127693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:973] ARM64 does not support NUMA - returning NUMA node zero
2020-07-08 15:01:34.127815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:973] ARM64 does not support NUMA - returning NUMA node zero
2020-07-08 15:01:34.127879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: unknown error
Segmentation fault (core dumped)

```

I am very new to tensorflow ...not sure what is going on. Appreciate the help.

NOTE: I even tried by installing tensorflow-1.15.0 and running the same program. Same crash happens."
41183,model.fit hangs when using matplotlib in the same script.,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
MacOS Catalina 10.15.5
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No
- TensorFlow installed from (source or binary):
Anaconda 3.6
- TensorFlow version (use command below):
unknown 2.0.0
- Python version:
Python 3.6
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
Radeon Pro 560X / Intel UHD Graphics 630

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
When displaying a plot, then running model.fit, the following error is produced:
```bash
Train on 60000 samples
Epoch 1/5
OMP: Error #15: Initializing libiomp5.dylib, but found libiomp5.dylib already initialized.
OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://www.intel.com/software/products/support/.
```
I have discovered two workarounds:
-  One workaround is described in this github issue # https://github.com/dmlc/xgboost/issues/1715
```python
import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
```
- The other is to remove any references to matplot lib code. I have included an example in my sample code below.

**Describe the expected behavior**
I would expect that leveraging matplotlib code and tensorflow code in the same script to work without any work aorunds.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
```python
if __name__ == '__main__':
    import tensorflow as tf
    import matplotlib.pyplot as plt

    mnist = tf.keras.datasets.mnist
    (x_train, y_train), (x_test, y_test) = mnist.load_data()

    # This pyplot block of code contributes to the error. Without it, model.train functions fine.
    # Begin Block
    def display_image(position):
        image = x_train[position].squeeze()
        plt.title('Example %d. Label: %d' % (position, y_train[position]))
        plt.imshow(image, cmap='gray')
    display_image(0)
    # End block

    x_train = x_train.reshape(len(x_train), 28, 28, 1)
    x_test = x_test.reshape(len(x_test), 28, 28, 1)

    model = tf.keras.models.Sequential([
        tf.keras.layers.Conv2D(filters=6, kernel_size=(5, 5), activation='relu', input_shape=(28, 28, 1)),
        tf.keras.layers.AveragePooling2D(),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(units=10, activation='relu'),
        tf.keras.layers.Dense(10, activation=tf.nn.softmax)
    ])

    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    """"""
    # This will throw the error
    OMP: Error #15: Initializing libiomp5.dylib, but found libiomp5.dylib already initialized.
    OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://www.intel.com/software/products/support/.
    """"""
    model.fit(x_train, y_train, epochs=5, verbose=1)

    # test accuracy
    model.evaluate(x_test, y_test)
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

This error doesnt seem to be reproducible in google collab."
41182,Bad input tensor parameters in model,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):FreeRTOS
- TensorFlow installed from (source or binary): TensorFlow Lite
- Tensorflow version (commit SHA if source):
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):ESP32 with 2MB external flash
https://www.banggood.com/LILYGO-TTGO-T-Camera-Plus-ESP32-DOWDQ6-8MB-SPRAM-OV2640-Camera-Module-1_3-Inch-Display-With-WiFi-bluetooth-Board-p-1426498.html?ID=566073&cur_warehouse=CN

**Describe the problem**
Followed the steps in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/micro_speech#deploy-to-esp32

Got the following error:

Bad input tensor parameters in model
Guru Meditation Error: Core  1 panic'ed (LoadProhibited). Exception was unhandled.

Core  1 register dump:
PC      : 0x400d389b  PS      : 0x00060730  A0      : 0x800d35cf  A1      : 0x3ffc2080  

0x400d389b: FeatureProvider::PopulateFeatureData(tflite::ErrorReporter*, int, int, int*) at /Users/velsaran/project/sound-ai/micro_speech/esp-idf/build/../main/feature_provider.cc:37

A2      : 0x00000000  A3      : 0x3ffb1200  A4      : 0x00000000  A5      : 0x00000000  
A6      : 0x3ffc20c4  A7      : 0x00000004  A8      : 0x800f1610  A9      : 0x3ffc1f20  
A10     : 0x00000000  A11     : 0x3f402bac  A12     : 0x3ffc2080  A13     : 0x3ffc2060  
A14     : 0x00000008  A15     : 0x3ffb56a0  SAR     : 0x00000001  EXCCAUSE: 0x0000001c  
EXCVADDR: 0x00000000  LBEG    : 0x400014fd  LEND    : 0x4000150d  LCOUNT  : 0xffffffff  

Backtrace:0x400d3898:0x3ffc2080 0x400d35cc:0x3ffc20c0 0x400d2fe2:0x3ffc20f0
0x400d3898: FeatureProvider::PopulateFeatureData(tflite::ErrorReporter*, int, int, int*) at /Users/velsaran/project/sound-ai/micro_speech/esp-idf/build/../main/feature_provider.cc:36

0x400d35cc: loop at /Users/velsaran/project/sound-ai/micro_speech/esp-idf/build/../main/main_functions.cc:132

0x400d2fe2: tf_main(int, char**) at /Users/velsaran/project/sound-ai/micro_speech/esp-idf/build/../main/esp/main.cc:29 (discriminator 1)

**Please provide the exact sequence of commands/steps when you ran into the problem**

"
41180,index error found in tf.keras.backend.dot function ,"<em>Sorry for bothering. an error both found in tf1.15</em>

**System information**
- OS: Windows 10 1904
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): tensorflow 1.15 and tensorflow 2.1
- Python version: python 3.6 for tensorflow 1.15, python 3.6 for tensorflow 2.1
- CUDA/cuDNN version:10
- GPU model and memory: Nvidia rtx 2080ti 11g

**Describe the current behavior**
Here is the source code of tensorflow.keras.backend.dot:
`
@keras_export('keras.backend.dot')
def dot(x, y):
  """"""Multiplies 2 tensors (and/or variables) and returns a *tensor*.

  When attempting to multiply a nD tensor
  with a nD tensor, it reproduces the Theano behavior.
  (e.g. `(2, 3) * (4, 3, 5) -> (2, 4, 5)`)

  Arguments:
      x: Tensor or variable.
      y: Tensor or variable.

  Returns:
      A tensor, dot product of `x` and `y`.
  """"""
   if ndim(x) is not None and (ndim(x) > 2 or ndim(y) > 2):
        x_shape = []
        for i, s in zip(int_shape(x), tf.unstack(tf.shape(x))):
            if i is not None:
                x_shape.append(i)
            else:
                x_shape.append(s)
        x_shape = tuple(x_shape)
        y_shape = []
        for i, s in zip(int_shape(y), tf.unstack(tf.shape(y))):
            if i is not None:
                y_shape.append(i)
            else:
                y_shape.append(s)
        y_shape = tuple(y_shape)
        y_permute_dim = list(range(ndim(y)))
        y_permute_dim = [y_permute_dim.pop(-2)] + y_permute_dim
        xt = tf.reshape(x, [-1, x_shape[-1]])
        yt = tf.reshape(tf.transpose(y, perm=y_permute_dim), [y_shape[-2], -1])
        return tf.reshape(tf.matmul(xt, yt),
                          x_shape[:-1] + y_shape[:-2] + y_shape[-1:])
    if is_sparse(x):
        out = tf.sparse.sparse_dense_matmul(x, y)
    else:
        out = tf.matmul(x, y)
    return out

`
The issue is that I have a tensor x, which shape is (?,?,128) and a tensor y with shape (128,) . Thus , satisfy the 
if judgement clause, and variant **y_permute_dim will be [0]**.  Next, an index out of range error would be raised
from the code `y_permute_dim = [y_permute_dim.pop(-2)] + y_permute_dim`.
It is obvious, for there is only one element in y_permute_dim.
This error was caught both in tensorflow_backend.py(tensorflow1.15), and source code is just the same in backend.py(tensorflow 2.1)

Looking forward for your reply.

**Other info / logs** I
  File ""d:\Anaconda3\envs\tf1.15\lib\site-packages\keras\backend\tensorflow_backend.py"", line 1365, in dot
    y_permute_dim = [y_permute_dim.pop(-2)] + y_permute_dim
IndexError: pop index out of range"
41179,pypi packages built against different CUDA versions needed,"


**System information** Arch Linux
- TensorFlow version (you are using): 2.2.0
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**
Different Linux distributions ships with different CUDA & cudnn version, it would help researchers save a lot of time to build pip package if you can provide pypi package built against different CUDA version, just as what pytorch has done:
![pytorch](https://user-images.githubusercontent.com/36614441/86866464-8f616500-c0c0-11ea-8aa0-881ce6d65032.png)
Thank you!

**Will this change the current api? How?**
No.
**Who will benefit with this feature?**
People without plenty of resources.
**Any Other info.**
No."
41176,Keras timeseriesgenerator: predict multiple data points in one step?,"I have meteorological data that looks like this:

```
DateIdx               winddir   windspeed   hum         press       temp
2017-04-17 00:00:00   0.369397  0.155039    0.386792    0.196721    0.238889
2017-04-17 00:15:00   0.363214  0.147287    0.429245    0.196721    0.233333
2017-04-17 00:30:00   0.357032  0.139535    0.471698    0.196721    0.227778
2017-04-17 00:45:00   0.323029  0.127907    0.429245    0.204918    0.219444
2017-04-17 01:00:00   0.347759  0.116279    0.386792    0.213115    0.211111
2017-04-17 01:15:00   0.346213  0.127907    0.476415    0.204918    0.169444
2017-04-17 01:30:00   0.259660  0.139535    0.566038    0.196721    0.127778
2017-04-17 01:45:00   0.205564  0.073643    0.523585    0.172131    0.091667
2017-04-17 02:00:00   0.157650  0.007752    0.481132    0.147541    0.055556
2017-04-17 02:15:00   0.122101  0.003876    0.476415    0.122951    0.091667
```

My aim: to use the keras timeseriesgenerator (`from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator`) to predict multiple data points (multiple rows) at once, e.g. not to do

```
[input X]                  | [predictions y]
[dp1, dp2, dp3, dp4, dp5]  | [dp6]
[dp2, dp3, dp4, dp5, dp6]  | [dp7]
[dp3, dp4, dp5, dp6, dp7]  | [dp8]
                          ...
```
but to do

```
[input X]                  | [predictions y]
[dp1, dp2, dp3, dp4, dp5]  | [dp6, dp7, dp8]
[dp2, dp3, dp4, dp5, dp6]  | [dp7, dp8, dp9]
[dp3, dp4, dp5, dp6, dp7]  | [dp8, dp9, dp10]
                          ...
```
I can achieve the top kind of predictions with

```
generator = TimeseriesGenerator(
    X,
    X,
    length=5,
    sampling_rate=1,
    stride=1,
    start_index=0,
    end_index=None,
    shuffle=False,
    reverse=False,
    batch_size=1,
)
```
, but I haven't figured out how I can tweak the generator options for the second kind of predictions.

Is there an easy way to achieve the desired prediction window of 3 data points with the timeseriesgenerator? If not, can you suggest me some code to bin my predictions y to achieve the task? Tnx"
41174,setting model.stop_training=True in custom callbacks does not stop training in v2.2.0 (works in v2.1.0 though),"

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **YES**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  **Ubuntu 18.04** 
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0
- Python version: 3.6.9


**Describe the current behavior**
Setting `self.model.stop_training=True` in `on_train_batch_end` does **NOT** stop training in v2.2.0.  It **does** work correctly in v2.1.0, though.

**Describe the expected behavior**
Setting `self.model.stop_training=True` in `on_train_batch_end` should correctly stop training.


**Standalone code to reproduce the issue**
Please see [this Google Colab link](https://colab.research.google.com/drive/1cDOIJTiVR0wFTdsTi0RaE-MVY71dYpD-) to reproduce the problem.

The code to reproduce is also here:
```python
import tensorflow as tf
from tensorflow import keras

# Define the Keras model to add callbacks to
def get_model():
    model = keras.Sequential()
    model.add(keras.layers.Dense(1, input_dim=784))
    model.compile(
        optimizer=keras.optimizers.RMSprop(learning_rate=0.1),
        loss=""mean_squared_error"",
        metrics=[""mean_absolute_error""],
    )
    return model

# Load example MNIST data and pre-process it
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train = x_train.reshape(-1, 784).astype(""float32"") / 255.0
x_test = x_test.reshape(-1, 784).astype(""float32"") / 255.0

# Limit the data to 1000 samples
x_train = x_train[:1000]
y_train = y_train[:1000]
x_test = x_test[:1000]
y_test = y_test[:1000]

class CustomCallback(keras.callbacks.Callback):
    def on_train_batch_end(self, batch, logs=None):
        keys = list(logs.keys())
        print('value of model.stop_training: %s' % (self.model.stop_training))
        if batch == 1:
            print('stop training on batch %s' % (batch))
            self.model.stop_training = True
            return

model = get_model()
model.fit(
    x_train,
    y_train,
    batch_size=128,
    epochs=1,
    verbose=0,
    validation_split=0.5,
    callbacks=[CustomCallback()],
)

```

"
41171,"mlagents-learn is not responding: Please help me, am new on learn ml agents in unity","(base) D:\PROGRAMMING\UNITY\AI\ml-agents-release_3>mlagent-learn
'mlagent-learn' is not recognized as an internal or external command,
operable program or batch file.

(base) D:\PROGRAMMING\UNITY\AI\ml-agents-release_3>conda activate unity_rl

(unity_rl) D:\PROGRAMMING\UNITY\AI\ml-agents-release_3>mlagents-learn
Traceback (most recent call last):
  File ""C:\Users\Kumah\.conda\envs\unity_rl\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Kumah\.conda\envs\unity_rl\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Kumah\.conda\envs\unity_rl\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Kumah\.conda\envs\unity_rl\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Kumah\.conda\envs\unity_rl\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Kumah\.conda\envs\unity_rl\Scripts\mlagents-learn-script.py"", line 33, in <module>
    sys.exit(load_entry_point('mlagents', 'console_scripts', 'mlagents-learn')())
  File ""C:\Users\Kumah\.conda\envs\unity_rl\Scripts\mlagents-learn-script.py"", line 25, in importlib_load_entry_point
    return next(matches).load()
  File ""C:\Users\Kumah\.conda\envs\unity_rl\lib\site-packages\importlib_metadata\__init__.py"", line 105, in load
    module = import_module(match.group('module'))
  File ""C:\Users\Kumah\.conda\envs\unity_rl\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 728, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""d:\programming\unity\ai\ml-agents-release_3\ml-agents\mlagents\trainers\learn.py"", line 12, in <module>
    from mlagents import tf_utils
  File ""d:\programming\unity\ai\ml-agents-release_3\ml-agents\mlagents\tf_utils\__init__.py"", line 1, in <module>
    from mlagents.tf_utils.tf import tf as tf  # noqa
  File ""d:\programming\unity\ai\ml-agents-release_3\ml-agents\mlagents\tf_utils\tf.py"", line 3, in <module>
    import tensorflow as tf  # noqa I201
  File ""C:\Users\Kumah\.conda\envs\unity_rl\lib\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\Kumah\.conda\envs\unity_rl\lib\site-packages\tensorflow\python\__init__.py"", line 50, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Kumah\.conda\envs\unity_rl\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 69, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Kumah\.conda\envs\unity_rl\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Kumah\.conda\envs\unity_rl\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Kumah\.conda\envs\unity_rl\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Kumah\.conda\envs\unity_rl\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Kumah\.conda\envs\unity_rl\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

(unity_rl) D:\PROGRAMMING\UNITY\AI\ml-agents-release_3>mlagent-learn
'mlagent-learn' is not recognized as an internal or external command,
operable program or batch file.

(unity_rl) D:\PROGRAMMING\UNITY\AI\ml-agents-release_3>"
41170,Shape issue for `tensordot` with nested `vectorized_map` and graph exectued function,"I have some issues with shapes for nested vectorized function call, I'm including the simplest code here:
```python
import tensorflow as tf
x = tf.ones((100, 10, 5))
@tf.function()
def tdot(x):
    res = tf.tensordot(x, x, 1)
    return res
def func(x):
    def func_inner(x_):
        return tf.vectorized_map(tdot, x_)
    return tf.vectorized_map(func_inner, x)

@tf.function
def test():
    res = func(x)
    print(res.shape) # shape should be (100, 10) instead of (None, 10)

test()
```
Isn't it expected for the first dimension to be defined in the resulting tensor (see the comment). Or I am missing something in the docs?"
41169,failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure tensorflow/stream_executor/dnn.cc:613] CUDNN_STATUS_INTERNAL_ERROR,"
I am unable to train my model. I get the below error after around the 100th epoch or so (randomly). Sometimes it fails on the 500th or so epoch. There are a few times I won't get this error.

Python Version: 3.7
Tesorflow Version: 2.3
Tensorflow Installed: From binary.
OS: Windows 10
GPU Card: Nvidia Titan V
CUDA: 10.1
CUDNN: 7.6.5.32

: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure
2020-07-07 17:15:37.551112: E tensorflow/stream_executor/dnn.cc:613] CUDNN_STATUS_INTERNAL_ERROR
in tensorflow/stream_executor/cuda/cuda_dnn.cc(1867): 'cudnnRNNForwardTraining( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, input_desc.handles(), input_data.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), rnn_desc.params_handle(), params.opaque(), output_desc.handles(), output_data->opaque(), output_h_desc.handle(), output_h_data->opaque(), output_c_desc.handle(), output_c_data->opaque(), workspace.opaque(), workspace.size(), reserve_space.opaque(), reserve_space.size())'
2020-07-07 17:15:37.551176: F tensorflow/stream_executor/cuda/cuda_dnn.cc:189] Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream.



<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
41168,Add universal Tensor/EagerTensor to ndarray conversion function,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): tf-nightly==2.3.0dev20200622
- Are you willing to contribute it (Yes/No): Not skilled enough or knowledgeable enough of TF architecture



**Describe the feature and the current behavior/state.**
Currently there are many different ways to convert individual types of Tensor-like objects into corresponding ndarrays; however, each of these has a specific use condition.
<EagerTensor>.numpy() : only works with eager execution
<Tensor> : must be evaluated inside a tf.function or a Graph. Might not work in all environments either ( I couldn't get this to work)
There is a function that converts ProtoTensors, but it doesn't work on Tensors or EagerTensors.

The goal is to have a function that can convert at least Tensors and EagerTensors to ndarrays regardless of whether Eager Execution is enabled or not.

it should be callable in a manner like `X = tf.make_ndarray(model(X))`

**Will this change the current api? How?** To the best of my knowledge, it will not change the surface level API.

**Who will benefit with this feature?**
This will particularly benefit anyone who has to run multiple, separately-trained networks in the same python session where at least one of them is a port from TF 1.X that requires eager execution to be disabled. Because Eager Execution is a global setting (to the best of my knowledge), this causes the output to be as Tensors when they may normally be expected to be EagerTensors.
`X=model(X)` returns 'X' as a EagerTensor with EagerExecution but returns a Tensor with EagerExecution disabled. This could easily cause downstream data processing problems (i.e. numpy math operations no longer work because of non-compatibility).

The implementation of this would remove a significant barrier to making generalized code/networks. It also prevents unnecessary imports of tensorflow or tensorflow functions.

Supposedly, Tensors have to be evaluated in a graph because of something to do with unknown states (or something that sounds similar) and EagerTensors don't have that limitation. However, TF clearly has the tf.math module and knows what the numbers are and knows how to do math with them. I don't see why the tf.math module can't simply have a function that can return a ndarray.

**Any Other info.**
"
41166,Tensorflow importing issue,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: none
- TensorFlow installed from (source or binary): tensorflow.org
- TensorFlow version: 2.2.0
- Python version: 3.8.3
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



I can not import tensorflow, it shows an import issue that i tried to fix, but it is still not working. Any help is appreciated. Thanks a lot

Traceback (most recent call last):
  File ""C:\Users\david\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\david\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\david\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\david\AppData\Local\Programs\Python\Python38\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\david\AppData\Local\Programs\Python\Python38\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<pyshell#0>"", line 1, in <module>
    import tensorflow
  File ""C:\Users\david\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\david\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\__init__.py"", line 50, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\david\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 69, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\david\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\david\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\david\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\david\AppData\Local\Programs\Python\Python38\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\david\AppData\Local\Programs\Python\Python38\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
>>> 
"
41164,TypeError: __init__() got an unexpected keyword argument 'lambda',"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
N/A (google colab)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
n/a
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
1.15.2
- Python version:
3.6.9
- Bazel version (if compiling from source):
n/a

**Describe the current behavior**
I cannot load a saved model containing a subclass of keras.regularizer.Regularizer.
Error = TypeError: __init__() got an unexpected keyword argument 'lambda'

**Describe the expected behavior**
I expect it to load my model since the model works well and is saved correctly.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
class My_Regularizer(Regularizer):
    def __init__(self, lambd, matrix_x, matrix_y, matrix_z):
        self.lambd = lambd
        self.matrix_x = matrix_x
        self.matrix_y = matrix_y
        self.matrix_z = matrix_z

    def __call__(self, x):
        return tf.linalg.tensor_diag(tf.diag_part(self.lambd * K.dot(K.transpose(K.square(x)), K.variable(self.matrix_x, dtype='float32') + K.variable(self.matrix_y, dtype='float32') + K.variable(self.matrix_z, dtype='float32'))))

    def get_config(self):
        return {'lambd': float(self.lambd),
                'matrix_x': self.matrix_x,
                'matrix_y': self.matrix_y,
                'matrix_z': self.matrix_z,
               }
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-8-d7b61e297765> in <module>()
----> 1 test = load_model('/content/drive/My Drive/Memoire/Wolfsky models/final_regularized_model.h5')

13 frames
/usr/local/lib/python3.6/dist-packages/keras/regularizers.py in from_config(cls, config)
     24     @classmethod
     25     def from_config(cls, config):
---> 26         return cls(**config)
     27 
     28 

TypeError: __init__() got an unexpected keyword argument 'lambda'
```
"
41162,Two different styles of Tensorflow implementation for the same network architecture lead to two different results and behaviors?,"
- OS Platform: Linux Centos 7.6
- Distribution: Intel Xeon Gold 6152 (22x3.70 GHz); 
- GPU Model: NVIDIA Tesla V100 32 GB;
- Number of nodes/CPU/Cores/GPU: 26/52/1144/104;
- TensorFlow installed from (source or binary): official webpage
- TensorFlow version (use command below): 2.1.0
- Python version: 3.6.8

**Description of issue:**

While I was implementing my proposed method, using the second style of implementation (see below), I realized that the performance of the algorithm is indeed strange. To be more precise, the accuracy decreases and loss value increases while the number of epochs increases. 

So I narrow down the problem and finally, I decided to modify some codes from TensorFlow official page to check what is happening. As it is explained in TF v2 official webpage there are two styles of implementation which I have adopted as follows. 

- I have modified the code provided in ""getting started of TF v2"" the link below:
   
  [TensorFlow 2 quickstart for beginners](https://www.tensorflow.org/tutorials/quickstart/beginner) 
  
 as follows:

  import tensorflow as tf
  from sklearn.preprocessing import OneHotEncoder
  from sklearn.datasets import make_classification
  from sklearn.model_selection import train_test_split

  learning_rate = 1e-4
  batch_size = 100
  n_classes = 2
  n_units = 80


    # Generate synthetic data / load data sets
    x_in, y_in = make_classification(n_samples=1000, n_features=10, n_informative=4, n_redundant=2,
                                     n_repeated=2, n_classes=2, n_clusters_per_class=2, weights=[0.5, 0.5],
                                     flip_y=0.01, class_sep=1.0, hypercube=True,
                                     shift=0.0, scale=1.0, shuffle=True, random_state=42)

    x_in = x_in.astype('float32')
    y_in = y_in.astype('float32').reshape(-1, 1)

    one_hot_encoder = OneHotEncoder(sparse=False)
    y_in = one_hot_encoder.fit_transform(y_in)
    y_in = y_in.astype('float32')

    x_train, x_test, y_train, y_test = train_test_split(x_in, y_in, test_size=0.4, random_state=42, shuffle=True)
    x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state=42, shuffle=True)
    print(""shapes:"", x_train.shape, y_train.shape, x_test.shape, y_test.shape, x_val.shape, y_val.shape)

    V = x_train.shape[1]

    model = tf.keras.models.Sequential([
        tf.keras.layers.Dense(n_units, activation='relu', input_shape=(V,)),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(n_classes)
    ])

    loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)

    model.compile(optimizer='adam',
                  loss=loss_fn,
                  metrics=['accuracy'])

    model.fit(x_train, y_train, epochs=5)

    model.evaluate(x_test, y_test, verbose=2)


the output is as it is expected, as one can see below:

600/600 [==============================] - 0s 419us/sample - loss: 0.7114 - accuracy: 0.5350
Epoch 2/5
600/600 [==============================] - 0s 42us/sample - loss: 0.6149 - accuracy: 0.6050
Epoch 3/5
600/600 [==============================] - 0s 39us/sample - loss: 0.5450 - accuracy: 0.6925
Epoch 4/5
600/600 [==============================] - 0s 46us/sample - loss: 0.4895 - accuracy: 0.7425
Epoch 5/5
600/600 [==============================] - 0s 40us/sample - loss: 0.4579 - accuracy: 0.7825

test: 200/200 - 0s - loss: 0.4110 - accuracy: 0.8350

To be more precise, the training accuracy increases and the loss value decrease as the number epochs increases (which is expected and it is normal).

HOWEVER, the following chunk of code which is adapted from the link below:

  [TensorFlow 2 quickstart for experts](https://www.tensorflow.org/tutorials/quickstart/advanced)

as follows:

import tensorflow as tf
from sklearn.preprocessing import OneHotEncoder
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

learning_rate = 1e-4
batch_size = 100
n_classes = 2
n_units = 80

    # Generate synthetic data / load data sets
    x_in, y_in = make_classification(n_samples=1000, n_features=10, n_informative=4, n_redundant=2,
                                     n_repeated=2, n_classes=2, n_clusters_per_class=2, weights=[0.5, 0.5],
                                     flip_y=0.01, class_sep=1.0, hypercube=True,
                                     shift=0.0, scale=1.0, shuffle=True, random_state=42)

    x_in = x_in.astype('float32')
    y_in = y_in.astype('float32').reshape(-1, 1)

    one_hot_encoder = OneHotEncoder(sparse=False)
    y_in = one_hot_encoder.fit_transform(y_in)
    y_in = y_in.astype('float32')

    x_train, x_test, y_train, y_test = train_test_split(x_in, y_in, test_size=0.4, random_state=42, shuffle=True)
    x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state=42, shuffle=True)

    print(""shapes:"", x_train.shape, y_train.shape, x_test.shape, y_test.shape, x_val.shape, y_val.shape)

    training_dataset = tf.data.Dataset.from_tensor_slices(
        (x_train, y_train)).batch(batch_size)

    valid_dataset = tf.data.Dataset.from_tensor_slices(
        (x_val, y_val)).batch(batch_size)

    testing_dataset = tf.data.Dataset.from_tensor_slices(
        (x_test, y_test)).batch(batch_size)

    V = x_train.shape[1]


    class MyModel(tf.keras.models.Model):
        def __init__(self):
            super(MyModel, self).__init__()
            self.d1 = tf.keras.layers.Dense(n_units, activation='relu', input_shape=(V,))
            self.d2 = tf.keras.layers.Dropout(0.2)
            self.d3 = tf.keras.layers.Dense(n_classes,)

        def call(self, x):
            x = self.d1(x)
            x = self.d2(x)
            return self.d3(x)


    # Create an instance of the model
    model = MyModel()

    loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)

    optimizer = tf.keras.optimizers.Adam()

    train_loss = tf.keras.metrics.Mean(name='train_loss')
    train_accuracy = tf.keras.metrics.BinaryCrossentropy(name='train_accuracy')

    test_loss = tf.keras.metrics.Mean(name='test_loss')
    test_accuracy = tf.keras.metrics.BinaryCrossentropy(name='test_accuracy')


    @tf.function
    def train_step(images, labels):
        with tf.GradientTape() as tape:
            # training=True is only needed if there are layers with different
            # behavior during training versus inference (e.g. Dropout).
            predictions = model(images,)  # training=True
            loss = loss_object(labels, predictions)
        gradients = tape.gradient(loss, model.trainable_variables)
        optimizer.apply_gradients(zip(gradients, model.trainable_variables))

        train_loss(loss)
        train_accuracy(labels, predictions)


    @tf.function
    def test_step(images, labels):
        # training=False is only needed if there are layers with different
        # behavior during training versus inference (e.g. Dropout).
        predictions = model(images,)  # training=False
        t_loss = loss_object(labels, predictions)

        test_loss(t_loss)
        test_accuracy(labels, predictions)


    EPOCHS = 5

    for epoch in range(EPOCHS):
        # Reset the metrics at the start of the next epoch
        train_loss.reset_states()
        train_accuracy.reset_states()
        test_loss.reset_states()
        test_accuracy.reset_states()

        for images, labels in training_dataset:
            train_step(images, labels)

        for test_images, test_labels in testing_dataset:
            test_step(test_images, test_labels)

        template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'
        print(template.format(epoch + 1,
                              train_loss.result(),
                              train_accuracy.result(),
                              test_loss.result(),
                              test_accuracy.result()))


Behaves indeed strange. Here is the output of this piece of code:

Epoch 1, Loss: 0.7299721837043762, Accuracy: 3.8341376781463623, Test Loss: 0.7290592193603516, Test Accuracy: 3.6925911903381348
Epoch 2, Loss: 0.6725851893424988, Accuracy: 3.1141700744628906, Test Loss: 0.6695905923843384, Test Accuracy: 3.2315549850463867
Epoch 3, Loss: 0.6256862878799438, Accuracy: 2.75959849357605, Test Loss: 0.6216427087783813, Test Accuracy: 2.920461416244507
Epoch 4, Loss: 0.5873140096664429, Accuracy: 2.4249706268310547, Test Loss: 0.5828182101249695, Test Accuracy: 2.575272560119629
Epoch 5, Loss: 0.555053174495697, Accuracy: 2.2128372192382812, Test Loss: 0.5501811504364014, Test Accuracy: 2.264410972595215

As one can see, not only the values of accuracy are strange but also instead of increasing, once the number of epochs increase, they decrease? Please note the strange behaviour of loss value.

**May you please explain what is happening here?**

"
41161,Inconsistent regularization loss computation when used with pretrained models. ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Google Colab Environment**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**
- TensorFlow installed from (source or binary): **Binary (Colab Pre-installed)**
- TensorFlow version (use command below): **2.4.0-nightly**
- Python version: **3.6.9**
- Bazel version (if compiling from source): **N/A**
- GCC/Compiler version (if compiling from source): **N/A**
- CUDA/cuDNN version: **N/A (Colab CPU Environment used)**
- GPU model and memory: **N/A**

**Describe the current behavior**
I am trying to use MobileNetV2 pretrained model for a simple classification task. Here I want to add regularization loss for MobileNetV2 - base model along with added FC layers. I have conducted 3 experiments as per attached colab notebook, where I get different results for the computation of regularization loss. The MobileNetV2 regularizer loss is computed twice in the model. And I dont know why it happens.

**Describe the expected behavior**
The computation of the loss in all the experiments should be same.

**Standalone code to reproduce the issue**
Colab Link : https://colab.research.google.com/drive/1CeKMIAq_g0AOKdakupKIeUjEhkL_TtI1?usp=sharing

**Other info / logs** 
1. I have tried the same on TF2.2 but got the same behaviour.
2. I have got the reference of this: [issue-37511](https://github.com/tensorflow/tensorflow/issues/37511#issuecomment-599173876 ) for adding regularization loss into pretrained model."
41159,InternalError: Tensorflow type 21 not convertible to numpy dtype.,"`kl = tf.reduce_mean(compute_kld( p_logit , p_logit_r ))
grad_kl = tf.gradients( kl ,r_tensor)[0]
r_vadv = tf.stop_gradient(grad_kl)
r_vadv = make_unit_norm( r_vadv )/3.0
p_logit_no_gradient = tf.stop_gradient(p_logit)

vadv_tensor = tf.add(clean_emb, grad_kl)
print('Gradient: ',type(grad_kl), grad_kl.shape)
print('Adversarial embedding: ',type(vadv_tensor), vadv_tensor.shape)`

Gradient:  <class 'tensorflow.python.framework.ops.Tensor'> (None, 500, 50)
Adversarial embedding:  <class 'tensorflow.python.framework.ops.Tensor'> (None, 500, 50)

`hidden = LSTM(units=128)(vadv_tensor)
output = Dense(units=32, activation='relu')(vadv_tensor)
q_logit = Dense(units=1, activation='relu')(output)

print('Q_Logit: ',type(q_logit), q_logit.shape)`

# Error

> /usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  run_metadata_ptr)
InternalError                             Traceback (most recent call last)
<ipython-input-8-9bcdd73aa054> in <module>()
----> 1 hidden = LSTM(units=128)(vadv_tensor)
      2 output = Dense(units=32, activation='relu')(vadv_tensor)
      3 q_logit = Dense(units=1, activation='relu')(output)
      4 
      5 print('Q_Logit: ',type(q_logit), q_logit.shape)

10 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in __call__(self, *args, **kwargs)
   1470         ret = tf_session.TF_SessionRunCallable(self._session._session,
   1471                                                self._handle, args,
-> 1472                                                run_metadata_ptr)
   1473         if run_metadata:
   1474           proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

InternalError: Tensorflow type 21 not convertible to numpy dtype.

---------------------------------------------------------------------------

I am using Tensorflow 1.x. Any suggestions on how I can solve this error. 
Thanks for your consideration.

The error can be replicated from this collab: https://colab.research.google.com/drive/1RlZTQaqM8Mb2MnyviUO0K88QXztFlY8Z?usp=sharing

The dataset used for the above code:
[npy.zip](https://github.com/tensorflow/tensorflow/files/4886083/npy.zip)

"
41157,TimeDistributed does not infer output batch size when timesteps=None,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): N/A
- TensorFlow version (use command below): v2.2.0-0-g2b96f3662b 2.2.0
- Python version: 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
I am trying to create a convolutional LSTM model which makes use of `ConvLSTM2D` as well as other layers such as `Conv2D` and `MaxPool2D`, wrapped in `TimeDistributed` layers. I'm using the Functional API.

This model should be stateful. Therefore, I pass the batch size to the `Input` layer, but I would like to keep the `timesteps` dimension flexible, so I set that to `None`.

The problem arises when using `TimeDistributed` layers, because their output batch size is always set to `None`, even if their input batch size is fixed (see following summary).

```
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_18 (InputLayer)        [(4, None, 64, 64, 1)]    0         
_________________________________________________________________
time_distributed_27 (TimeDis (None, None, 64, 64, 1)   257       # no LSTMs after this point because batch_size=None
=================================================================
```

As a result, stateful LSTM layers cannot be used after `TimeDistributed`, as they will fail with ""ValueError: If a RNN is stateful, it needs to know its batch size"".

**Describe the expected behavior**

I certainly could be missing something, but I don't see any reason why the TimeDistributed layer shouldn't be able to infer its output batch size correctly (is it not always the same as the input batch size?). Even if that's not true, shouldn't it be possible to have a mechanism to help the layer figure it out?

**Standalone code to reproduce the issue**

Link to a simple example in Colab:
https://colab.research.google.com/drive/11efUrQpkahgk-jgb0-4Flwii7Fd1y5Ej?usp=sharing

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
41156,Persistent GradientTape not working with LSTM,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes. Adapted code from [this tutorial](https://keras.io/guides/customizing_what_happens_in_fit/)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Build 1903
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2.0
- Python version: 3.7.7

**Describe the current behavior**

Training a model with `tf.keras.layers.LSTM`  and  custom `train_step` that uses a `tf.GradientTape(persistent=True)` is 
stuck at the first batch. The python process slowly uses all available RAM.

**Describe the expected behavior**

The model should train the same way as when using `persistent=False`.

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/16tBJzlHqPY5oL5im20noFSN8luVzPnjt?usp=sharing
"
41153,tensorflow/tensorflow:2.1.1-gpu has python3 instead of python2,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  N/A
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): N/A
- TensorFlow version (use command below): v2.1.0-33-g3ffdb91 2.1.1
- Python version: See details
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A


**Describe the current behavior**
Docker image tensorflow/tensorflow:2.1.1-gpu contains python3.6.9

**Describe the expected behavior**
Docker image tensorflow/tensorflow:2.1.1-gpu contains python2.7.17 as is expected for docker images <= 2.1 https://hub.docker.com/r/tensorflow/tensorflow/

**Standalone code to reproduce the issue**
$docker run tensorflow/tensorflow:2.1.1-gpu python --version
Python 3.6.9            

**Other info / logs** Include any logs or source code that would be helpful to
Note:  tensorflow/tensorflow:2.1.0-gpu contains python2 
$docker run tensorflow/tensorflow:2.1.0-gpu python --version
Python 2.7.17      
"
41152,The order of weights is changed for array of layers inside sub classed model,"
The order of moving variance and moving mean weights(BatchNormalization layer) changes when array of layers is created inside sub classed model

````python
import tensorflow as tf
from tensorflow.keras.layers import *
print(tf.__version__)

class A(tf.keras.layers.Layer):
    def  __init__(self,  name):
      super(A, self).__init__(name=name)
      self.conv = Conv2D(32, 3)
      self.bn = BatchNormalization()  
    def call(self,  x):
      x = self.conv(x)
      x = self.bn(x)
      return x

class B(tf.keras.Model):
    def __init__(self):
        super(B, self).__init__()
        self.blocks = []
        for i in range(3):
          self.blocks.append(A(f'a{i}'))    
    
    def call(self, x):
        for block in self.blocks:
          x = block(x)
        return x

b = B()
b.build((1, 128, 128, 3))
print([weight.name for weight in b.weights])

#The output is as below
'a0/conv2d/kernel:0',
 'a0/conv2d/bias:0',
 'a0/batch_normalization/gamma:0',
 'a0/batch_normalization/beta:0',
 'a1/conv2d_1/kernel:0',
 'a1/conv2d_1/bias:0',
 'a1/batch_normalization_1/gamma:0',
 'a1/batch_normalization_1/beta:0',
 'a2/conv2d_2/kernel:0',
 'a2/conv2d_2/bias:0',
 'a2/batch_normalization_2/gamma:0',
 'a2/batch_normalization_2/beta:0',
 'a0/batch_normalization/moving_mean:0',
 'a0/batch_normalization/moving_variance:0',
 'a1/batch_normalization_1/moving_mean:0',
 'a1/batch_normalization_1/moving_variance:0',
 'a2/batch_normalization_2/moving_mean:0',
 'a2/batch_normalization_2/moving_variance:0'"
41151,uint8 model runtime input(s) num is 2.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 2-1

**Command used to run the converter or code if you’re using the Python API**

```
import tensorflow as tf
from tensorflow.keras.models import load_model

keras_file = ""./keras-0127127.h5""

model = load_model(keras_file)
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.inference_input_type = tf.uint8
converter.inference_type = tf.uint8    #tf.lite.constants.QUANTIZED_UINT8
converter.inference_output_type = tf.uint8
converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]
tflite_uint8_model = converter.convert()
open(""uint8.tflite"", ""wb"").write(tflite_uint8_model)
```

**The output from the converter invocation**
I use netron software to visualiz the ""uint8.tflite"" model, it shows that:
![image](https://user-images.githubusercontent.com/20535427/86765182-68cd0b00-c07b-11ea-83c2-026fbc5b5c50.png)
We can see some  DEPTHWISE_CONV_2D operations have 2 inputs.

**Also, please include a link to the keras model I used above**

```
https://github.com/chenpengf0223/semantic_segmentation_distillation/blob/master/keras-0127127.h5
```

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
- Producing model that c++ tflite gpu delegate library can not process.


When I use c++ tflite gpu delegate library to run the model, I get such log:

INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite delegate for GPU.
ERROR: Following operations are not supported by GPU delegate:
DEPTHWISE_CONV_2D: Expected 1 runtime input tensor(s), but node has 2 runtime input(s).
DEQUANTIZE: Expected 1 runtime input tensor(s), but node has 0 runtime input(s).
20 operations will run on the GPU, and the remaining 41 operations will run on the CPU.
INFO: Initialized OpenCL-based API.
INFO: Created 1 GPU delegate kernels."
41150,The order of weights is changed for array of layers inside sub classed model,"The order of  moving variance and moving mean weights(BatchNormalization layer) changes when array of layers is created inside sub classed model

````python
import tensorflow as tf
from tensorflow.keras.layers import *
print(tf.__version__)

class A(tf.keras.layers.Layer):
    def  __init__(self,  name):
      super(A, self).__init__(name=name)
      self.conv = Conv2D(32, 3)
      self.bn = BatchNormalization()  
    def call(self,  x):
      x = self.conv(x)
      x = self.bn(x)
      return x

class B(tf.keras.Model):
    def __init__(self):
        super(B, self).__init__()
        self.blocks = []
        for i in range(3):
          self.blocks.append(A(f'a{i}'))    
    
    def call(self, x):
        for block in self.blocks:
          x = block(x)
        return x

b = B()
b.build((1, 128, 128, 3))
print([weight.name for weight in b.weights])

#The output is as below
'a0/conv2d/kernel:0',
 'a0/conv2d/bias:0',
 'a0/batch_normalization/gamma:0',
 'a0/batch_normalization/beta:0',
 'a1/conv2d_1/kernel:0',
 'a1/conv2d_1/bias:0',
 'a1/batch_normalization_1/gamma:0',
 'a1/batch_normalization_1/beta:0',
 'a2/conv2d_2/kernel:0',
 'a2/conv2d_2/bias:0',
 'a2/batch_normalization_2/gamma:0',
 'a2/batch_normalization_2/beta:0',
 'a0/batch_normalization/moving_mean:0',
 'a0/batch_normalization/moving_variance:0',
 'a1/batch_normalization_1/moving_mean:0',
 'a1/batch_normalization_1/moving_variance:0',
 'a2/batch_normalization_2/moving_mean:0',
 'a2/batch_normalization_2/moving_variance:0'




"
41149,bug in create an op of tensorflow 1.15 ,"- tensorflow
  - tensorflow core
    - create an op
       - GPU kernels
There is an example of GPU kernel implementation.
There are three files:
kernel_example.h  kernel_example.cc  kernel_example.cu.cc
When I compile the kernel_example.cu.cc with the commond:
```
nvcc -std=c++11 -c -o kernel_example.cu.o kernel_example.cu.cc \
  ${TF_CFLAGS[@]} -D GOOGLE_CUDA=1 -x cu -Xcompiler -fPIC
```
there are some errors:
```
kernel_example.h: error: a class or namespace qualified name is required
kernel_example.h: warning: nonstandard qualified name in global scope declaration
kernel_example.h: error: class template ""ExampleFunctor "" has already been defined
```
How to deal with that?"
41148,Tensorflow GPU needs to wait a few minutes each time before computing,"**Tensorflow GPU needs to wait a few minutes each time before computing,GPU runs slower than CPU because of waiting**

**System information**
- OS Platform: ubuntu18.04
- TensorFlow:  2.2
- Python version:3.7
- CUDA/cuDNN version: cuda10.2/cudnn7.6.5
- GPU model and memory: GTX 950m, memory is 2048MB

The code  is official's helloworld demo .
[https://www.tensorflow.org/tutorials/quickstart/beginner](url)

Here is the information of output.
 `2020-07-07 13:59:25.106703: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 950M, Compute Capability 5.0``


I've tried the official apt installation, but it's still the same. Does anyone have the same problem? How did you solve it.
"
41147,Creating an instance of ResNet50() uses high GPU Memory,"Creating a model using the below method results in 14GB of GPU usage

import tensorflow as tf
from tensorflow.keras.applications import MobileNet, MobileNetV2, resnet
base_model = resnet.ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

![image](https://user-images.githubusercontent.com/52433565/86726230-f5b49c00-c05c-11ea-8676-f7cf1b8a26fb.png)

Any reason for such high GPU usage ?"
41146,"UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.","https://www.tensorflow.org/tutorials/generative/style_transfer

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
no. downloaded the notebook and ran verbatim. 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 20.04

- TensorFlow installed from (source or binary):
pip
- TensorFlow version (use command below):
v2.2.0-rc4-8-g2b96f3662b 2.2.0
- Python version:
3.8.3
- CUDA/cuDNN version:
```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 440.100      Driver Version: 440.100      CUDA Version: 10.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce RTX 206...  Off  | 00000000:0A:00.0  On |                  N/A |
| 29%   48C    P2    73W / 175W |   7950MiB /  7979MiB |     41%      Default |
+-------------------------------+----------------------+----------------------+
```
- GPU model and memory:
Nvidia RTX 2060 Super 8GB

**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
https://www.tensorflow.org/tutorials/generative/style_transfer jupyter notebook from this article, unmodified

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```
---------------------------------------------------------------------------
UnknownError                              Traceback (most recent call last)
<ipython-input-21-5d399fb9441b> in <module>
      1 import tensorflow_hub as hub
      2 hub_module = hub.load('https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/1')
----> 3 stylized_image = hub_module(tf.constant(content_image), tf.constant(style_image))[0]
      4 tensor_to_image(stylized_image)

~/code/github.com/quinn/ml/venv/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py in _call_attribute(instance, *args, **kwargs)
    484 
    485 def _call_attribute(instance, *args, **kwargs):
--> 486   return instance.__call__(*args, **kwargs)
    487 
    488 

~/code/github.com/quinn/ml/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)
   1603       TypeError: For invalid positional/keyword argument combinations.
   1604     """"""
-> 1605     return self._call_impl(args, kwargs)
   1606 
   1607   def _call_impl(self, args, kwargs, cancellation_manager=None):

~/code/github.com/quinn/ml/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _call_impl(self, args, kwargs, cancellation_manager)
   1643       raise TypeError(""Keyword arguments {} unknown. Expected {}."".format(
   1644           list(kwargs.keys()), list(self._arg_keywords)))
-> 1645     return self._call_flat(args, self.captured_inputs, cancellation_manager)
   1646 
   1647   def _filtered_call(self, args, kwargs):

~/code/github.com/quinn/ml/venv/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py in _call_flat(self, args, captured_inputs, cancellation_manager)
     98       captured_inputs = list(
     99           map(get_cross_replica_handle, captured_inputs))
--> 100     return super(_WrapperFunction, self)._call_flat(args, captured_inputs,
    101                                                     cancellation_manager)
    102 

~/code/github.com/quinn/ml/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1743         and executing_eagerly):
   1744       # No tape is watching; skip to running the function.
-> 1745       return self._build_call_outputs(self._inference_function.call(
   1746           ctx, args, cancellation_manager=cancellation_manager))
   1747     forward_backward = self._select_forward_and_backward_functions(

~/code/github.com/quinn/ml/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)
    591       with _InterpolateFunctionError(self):
    592         if cancellation_manager is None:
--> 593           outputs = execute.execute(
    594               str(self.signature.name),
    595               num_outputs=self._num_outputs,

~/code/github.com/quinn/ml/venv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     57   try:
     58     ctx.ensure_initialized()
---> 59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     60                                         inputs, attrs, num_outputs)
     61   except core._NotOkStatusException as e:

UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[node InceptionV3/Conv2d_1a_3x3/Conv2D (defined at /home/quinn/code/github.com/quinn/ml/venv/lib/python3.8/site-packages/tensorflow_hub/module_v2.py:102) ]] [Op:__inference_pruned_18205]

Function call stack:
pruned
```
"
41141,Predict is slow on first call when using variable batch_size,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary (via pip)
- TensorFlow version (use command below): v2.2.0-0-g2b96f3662b 2.2.0 (and tf-nightly: v1.12.1-35161-gd659eb9c0d 2.3.0-dev20200625)
- Python version: 3.6.9
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10.1 
- GPU model and memory: Tesla P100 / 16GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
- When I use a variable batch_size (<=32) the first prediction for a new size is slow. The following predictions for the same shape are fast. The interesting thing is for batch sizes > 32 there is no slowdown:
![tf-nightly](https://user-images.githubusercontent.com/864213/86664759-ea268e00-bfe6-11ea-93fc-d646bce6fb64.png)



**Describe the expected behavior**
- I expect that calls will take the same time for execution

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/1er9s3e4GgyMevU7MC6gOkpscsJ3j0Nr-?usp=sharing

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

There is a similar issue with the variable input shape: https://github.com/tensorflow/tensorflow/issues/39458"
41139,ERROR: No matching distribution found for tensorflow,"I'm trying to install Tensorflow with python 3.8. After running the command `python3.8 -m pip install tensorflow` I got this error:
```
ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)
ERROR: No matching distribution found for tensorflow
```

`pip debug --verbose` returns:`

```
pip version: pip 20.1.1 from /home/****/ENV/lib/python3.8/site-packages/pip (python 3.8)
sys.version: 3.8.0 (default, Nov 12 2019, 19:43:25)
[GCC 5.4.0]
sys.executable: /home/****/ENV/bin/python
sys.getdefaultencoding: utf-8
sys.getfilesystemencoding: utf-8
locale.getpreferredencoding: UTF-8
sys.platform: linux
sys.implementation:
  name: cpython
'cert' config value: :env:, install, wheel
REQUESTS_CA_BUNDLE: None
CURL_CA_BUNDLE: /etc/pki/tls/certs/ca-bundle.crt
pip._vendor.certifi.where(): /home/****/ENV/lib/python3.8/site-packages/pip/_vendor/certifi/cacert.pem
pip._vendor.DEBUNDLED: False
vendored library versions:
  appdirs==1.4.3
  CacheControl==0.12.6
  colorama==0.4.3
  contextlib2==0.6.0.post1 (Unable to locate actual module version, using vendor.txt specified version)
  distlib==0.3.0
  distro==1.5.0 (Unable to locate actual module version, using vendor.txt specified version)
  html5lib==1.0.1
  ipaddress==1.0.23
  msgpack==1.0.0 (Unable to locate actual module version, using vendor.txt specified version)
  packaging==20.3
  pep517==0.8.2
  progress==1.5
  pyparsing==2.4.7
  requests==2.23.0
  certifi==2020.04.05.1
  chardet==3.0.4
  idna==2.9
  urllib3==1.25.8
  resolvelib==0.3.0
  retrying==1.3.3 (Unable to locate actual module version, using vendor.txt specified version)
  setuptools==44.0.0 (Unable to locate actual module version, using vendor.txt specified version)
  six==1.14.0
  toml==0.10.0
  webencodings==0.5.1 (Unable to locate actual module version, using vendor.txt specified version)
Compatible tags: 30
  cp38-cp38-linux_x86_64
  cp38-abi3-linux_x86_64
  cp38-none-linux_x86_64
  cp37-abi3-linux_x86_64
  cp36-abi3-linux_x86_64
  cp35-abi3-linux_x86_64
  cp34-abi3-linux_x86_64
  cp33-abi3-linux_x86_64
  cp32-abi3-linux_x86_64
  py38-none-linux_x86_64
  py3-none-linux_x86_64
  py37-none-linux_x86_64
  py36-none-linux_x86_64
  py35-none-linux_x86_64
  py34-none-linux_x86_64
  py33-none-linux_x86_64
  py32-none-linux_x86_64
  py31-none-linux_x86_64
  py30-none-linux_x86_64
  cp38-none-any
  py38-none-any
  py3-none-any
  py37-none-any
  py36-none-any
  py35-none-any
  py34-none-any
  py33-none-any
  py32-none-any
  py31-none-any
  py30-none-any
```

**System information**
- OS Platform and Distribution: CentOS Linux release 7.7.1908 (Core)
- Python version: 3.8.0 / 64 bits
- Installed using virtualenv? pip? conda?: virtualenv
- GCC/Compiler version: 5.4.0
- pip version: 20.1.1




**EDIT:**
The command bellow worked for me:
`pip3 install --user tensorflow-gpu`"
41136,Add custom scalar loss to 'categorical_cross_entropy',"I wish to add a custom loss to regular cross-entropy used when we build/compile a tf model using model.compile()
However, I get an error as provided in the below snapshot. 

Code:
def createEmbedding(features):
  seq = Input(shape=(300,))
  emb = Embedding(input_dim=len(vocab)+1,
                  output_dim = 50,
                  weights = [embedding_matrix],
                  trainable=False)(seq)
  emb_model = Model(seq, emb)
  clean_emb = emb_model(features)
  return clean_emb

def createModel(embedding_features):
  emb_tensor = Input(shape=(300,50,))
  hidden = LSTM(units=128)(emb_tensor)
  output = Dense(units=32, activation='relu')(hidden)
  model = Model(inputs=emb_tensor, outputs=output)
  logits = model(embedding_features)
  return emb_tensor, output, logits

def compute_kld(p_logit, q_logit):
  p = tf.nn.softmax(p_logit)
  q = tf.nn.softmax(q_logit)
  kl_score = tf.reduce_sum( p * (tf.math.log(p+1e-16) - tf.math.log(q+1e-16)), axis = 1)
  return kl_score

def calculateGradient(clean_features, noised_features):
  with tf.GradientTape(watch_accessed_variables=False) as tape:
    tape.watch(noised_features)
    _, _, p_logit = createModel(clean_features)
    _, _, p_logit_r = createModel(noised_features)
    kl_score = compute_kld(p_logit, p_logit_r)
    print(kl_score)
  grads = tape.gradient(kl_score, noised_features)
  return grads

features, labels = next(iter(train_dataset))

clean_features = createEmbedding(features)
noised_features = tf.add(clean_features, 0.01)

print('Clean Embedding: ', type(clean_features), clean_features.shape) # <class 'tensorflow.python.framework.ops.EagerTensor'> (1024, 300, 50)
print('Noised Embedding: ', type(noised_features), noised_features.shape) # <class 'tensorflow.python.framework.ops.EagerTensor'> (1024, 300, 50)

clean_ip_tensor, clean_op_tensor, p_logit = createModel(clean_features)
noise_ip_tensor, noise_op_tensor, p_logit_r = createModel(noised_features)
print('P_Logit: ',type(p_logit), p_logit.shape) # <class 'tensorflow.python.framework.ops.EagerTensor'> (1024, 32)
print('P_Logit_R: ',type(p_logit_r), p_logit_r.shape) # <class 'tensorflow.python.framework.ops.EagerTensor'> (1024, 32)

grads = calculateGradient(clean_features, noised_features)
norm_ball = tf.math.l2_normalize(grads, axis=None, epsilon=1e-12, name=None)
type(norm_ball), norm_ball.shape
rvadv = (grads/norm_ball) * -1
vadv_features = tf.add(clean_features, rvadv)

vat_ip_tensor, vat_op_tensor, q_logit = createModel(vadv_features)
vat_loss = compute_kld(p_logit, q_logit) # <tf.Tensor: shape=(1024,), dtype=float32; these will be scalar value
# I wish to add this vat_loss to regular categorical cross entropy loss

p = Dense(units=1, activation='softmax')(clean_op_tensor)
model = Model(inputs=clean_ip_tensor, outputs=p)
model.add_loss(vat_loss) # Error
model.summary()

# Future implementation
model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(clean_features, labels)

![Screenshot 2020-07-06 at 22 16 46](https://user-images.githubusercontent.com/63437596/86637762-682e6900-bfd6-11ea-8e46-cfd329eed5e6.png)

Any suggestions on how can I work around this scenario. 
Thanks for your time and consideration
"
41135,"Possible bug(?): tensorflow.python.eager.core._SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor X>]","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS High Sierra
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): >= 2.0
- Python version: 3.6
- Running on: CPUs (But I guess it happens on GPUs as well)

The following issue is very similar to this [one](https://github.com/tensorflow/tensorflow/issues/41111) I posted before. The difference here is that I use eager execution in the code and it produces an error of Keras symbolic tensors. To be fair I am not sure this is a bug or the error is on purpose. Here is the code to produce the error:

```
import tensorflow as tf
from tensorflow.keras import backend as K
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np

class MyWordEmbedding(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super(MyWordEmbedding, self).__init__(**kwargs)

    def build(self, input_shape):
        self.kernel = self.add_weight(shape=(300, 512), dtype='float32')
        super(MyWordEmbedding, self).build(input_shape)  # Be sure to call this at the end
    
    def call(self, inputs):
        return tf.nn.embedding_lookup(params=self.kernel, ids=inputs[0])

class EncoderLayer(tf.keras.layers.Layer):
    def __init__(self, mask_para, **kwargs):
        self.mask_para = mask_para
        super(EncoderLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        self.Qdense = self.add_weight(name='Qdense', shape=(512, 512))
        super(EncoderLayer, self).build(input_shape)

    def call(self, x):
        Qoutput = tf.einsum('aij,jk->aik', x[0], self.Qdense)
        Koutput =  tf.einsum('aij,jk->aik', x[0], self.Qdense)
        Voutput =  tf.einsum('aij,jk->aik', x[0], self.Qdense)
        a = tf.einsum('ajk,afk->ajf', Qoutput, Koutput) * tf.tile(K.expand_dims(self.mask_para, axis=1), [1, 64, 1])
        a = tf.matmul(a, Voutput)
        print(a)
        return a

    def compute_mask(self, inputs, mask):
        return mask

    def compute_output_shape(self, input_shape):
        return input_shape[0]

def create_encoder_model():
    word_ids_fr = tf.keras.layers.Input(dtype='int32', shape=(None,))
    a = MyWordEmbedding()([word_ids_fr])
    a = EncoderLayer(K.cast(K.not_equal(0, word_ids_fr), dtype='float32'))([a])
    model = tf.keras.models.Model(inputs=[word_ids_fr], outputs=a)
    return model

def create_model():
    word_ids_en = tf.keras.layers.Input(dtype='int32', shape=(None,))
    a = tf.keras.layers.Input(shape=(None, 512,))
    b = MyWordEmbedding()([word_ids_en])
    b = b + a
    model = tf.keras.models.Model(inputs=[word_ids_en, a], outputs=b)
    return model
    
def evaluate():
    source_sequence_ids = pad_sequences(np.random.randint(5, size=(3, 64)), maxlen=64, padding='pre')
    output = decoder_model.predict([pad_sequences(np.random.randint(5, size=(3, 64)), maxlen=64, padding='post'), encoder_model(source_sequence_ids, training=False)], steps=1, verbose=1, batch_size=256)

decoder_model = create_model()
encoder_model = create_encoder_model()
evaluate()
```

Error:

`tensorflow.python.eager.core._SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor 'MatMul:0' shape=(3, 64, 512) dtype=float32>]`

Meanwhile, I also provide a way to fix this as follows (just a simple modification):

```
import tensorflow as tf
from tensorflow.keras import backend as K
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np

class MyWordEmbedding(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super(MyWordEmbedding, self).__init__(**kwargs)

    def build(self, input_shape):
        self.kernel = self.add_weight(shape=(300, 512), dtype='float32')
        super(MyWordEmbedding, self).build(input_shape)  # Be sure to call this at the end
    
    def call(self, inputs):
        return tf.nn.embedding_lookup(params=self.kernel, ids=inputs[0])

class EncoderLayer(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super(EncoderLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        self.Qdense = self.add_weight(name='Qdense', shape=(512, 512))
        super(EncoderLayer, self).build(input_shape)

    def call(self, x):
        mask_para = x[1]
        Qoutput = tf.einsum('aij,jk->aik', x[0], self.Qdense)
        Koutput =  tf.einsum('aij,jk->aik', x[0], self.Qdense)
        Voutput =  tf.einsum('aij,jk->aik', x[0], self.Qdense)
        a = tf.einsum('ajk,afk->ajf', Qoutput, Koutput) * tf.tile(K.expand_dims(mask_para, axis=1), [1, 64, 1])
        a = tf.matmul(a, Voutput)
        print(a)
        return a

    def compute_mask(self, inputs, mask):
        return mask

    def compute_output_shape(self, input_shape):
        return input_shape[0]

def create_encoder_model():
    word_ids_fr = tf.keras.layers.Input(dtype='int32', shape=(None,))
    a = MyWordEmbedding()([word_ids_fr])
    a = EncoderLayer()([a, K.cast(K.not_equal(0, word_ids_fr), dtype='float32')])
    model = tf.keras.models.Model(inputs=[word_ids_fr], outputs=a)
    return model

def create_model():
    word_ids_en = tf.keras.layers.Input(dtype='int32', shape=(None,))
    a = tf.keras.layers.Input(shape=(None, 512,))
    b = MyWordEmbedding()([word_ids_en])
    b = b + a
    model = tf.keras.models.Model(inputs=[word_ids_en, a], outputs=b)
    return model
    
def evaluate():
    source_sequence_ids = pad_sequences(np.random.randint(5, size=(3, 64)), maxlen=64, padding='pre')
    output = decoder_model.predict([pad_sequences(np.random.randint(5, size=(3, 64)), maxlen=64, padding='post'), encoder_model(source_sequence_ids, training=False)], steps=1, verbose=1, batch_size=256)

decoder_model = create_model()
encoder_model = create_encoder_model()
evaluate()
```
"
41134,Error with instalation of Tensorflow,"
**System information**
- OS Platform and Distribution : Ubuntu 20.04
- TensorFlow version: 1.15.0
- Python version: 3.7.0
- Installed using : pip
- CUDA/cuDNN version: None
- GPU model and memory: CPU



**Describe the problem**
   I am trying to install tensorflow via terminal in PyCharm, cant really use any other version of python or tensorflow cause my old  CPU. This is my last resort any newer version of tensorflow crashes immediately. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**

        pip3 install tensorflow==1.15.0


**Any other info / logs**
 ```
pip3 install tensorflow==1.15.0
Defaulting to user installation because normal site-packages is not writeable
Collecting tensorflow==1.15.0
  Downloading tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)
     |████████████████████████████████| 412.3 MB 21 kB/s 
Collecting six>=1.10.0
  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)
Collecting wrapt>=1.11.1
  Downloading wrapt-1.12.1.tar.gz (27 kB)
    ERROR: Command errored out with exit status 1:
     command: /usr/local/bin/python3.7 -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-v5_uvp33/wrapt/setup.py'""'""'; __file__='""'""'/tmp/pip-install-v5_uvp33/wrapt/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' egg_info --egg-base /tmp/pip-pip-egg-info-gahhrkjb
         cwd: /tmp/pip-install-v5_uvp33/wrapt/
    Complete output (11 lines):
    Traceback (most recent call last):
      File ""<string>"", line 1, in <module>
      File ""/usr/local/lib/python3.7/site-packages/setuptools/__init__.py"", line 23, in <module>
        from setuptools.dist import Distribution
      File ""/usr/local/lib/python3.7/site-packages/setuptools/dist.py"", line 34, in <module>
        from setuptools import windows_support
      File ""/usr/local/lib/python3.7/site-packages/setuptools/windows_support.py"", line 2, in <module>
        import ctypes
      File ""/usr/local/lib/python3.7/ctypes/__init__.py"", line 7, in <module>
        from _ctypes import Union, Structure, Array
    ModuleNotFoundError: No module named '_ctypes'
    ----------------------------------------
ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.

```"
41133,[TF/MLIR] Graph pruning / canonicalization pass doesn't eliminate some dead constant nodes,"This is with the tensorflow trunk at e13ff2da43ab00dba94dd00d87a876b2a03f48e7 (Jul 6). 

This is a reduced test case where both nodes/operations in the graph are supposed to be dead (results unused and not side-effecting), but neither the `-tf-executor-graph-pruning` nor the `-canonicalize` pass gets rid of them. However, if one of them is removed, `-canonicalize` is able to eliminate the other. Similar patterns are often DCE'd by `-tf-executor-graph-pruning` in the presence of other larger islands. 

```
module {
  func @main() attributes {tf.entry_function = {control_outputs = """", inputs = """", outputs = """"}} {
    tf_executor.graph {
      %outputs, %control = tf_executor.island wraps ""tf.Const""() {value = dense<1> : tensor<1xi32>} : () -> tensor<1xi32>
      %outputs_0, %control_1 = tf_executor.island wraps ""tf.Const""() {value = dense<100> : tensor<1xi32>} : () -> tensor<1xi32>
      tf_executor.fetch
    }
    return
  }
}
```
To reproduce, run:
```
$ tf-opt -tf-executor-graph-pruning -canonicalize test_case.mlir
```

OS: CentOS 8 x86-64.
TF built from sources with GCC 8.3.1 and with --linkopt='-fuse-ld=lld'. 

"
41132,Docker with GPU 2.3rc0 CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid,"It seem that the Docker image tensorflow/tensorflow:2.3.0rc0-gpu won't work with my GPU   **BUT** on the other hand the image tensorflow/tensorflow:2.2.0rc0-gpu works fine

Or in other words, the solution to the present issue was to ""downgrade"" to  tensorflow/tensorflow:2.2.0rc0-gpu
tensorflow/tensorflow:2.3.0rc0-gpu also works fine with CPU only.

 **System information**
- Ubuntu 20.4
- TensorFlow through Docker
- TensorFlow version (use command below):
- GPU model and memory: Geforce GTX 960M, coreClock: 1.176GHz coreCount: 5 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 74.65GiB/s
- GPU drivers: 440.100


**how to reproduce**
``` 
> docker run -it --rm --gpus all  --entrypoint bash tensorflow/tensorflow:2.3.0rc0-gpu
> python
>>> import tensorflow as tf
>>> inputs = tf.keras.layers.Input(shape=(None,), name=""input"")
>>> embedded = tf.keras.layers.Embedding(100, 16)(inputs)
```
**full stack trace:**
```
2020-07-06 18:46:55.604377: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-07-06 18:46:55.608404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-06 18:46:55.608911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 960M computeCapability: 5.0
coreClock: 1.176GHz coreCount: 5 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 74.65GiB/s
2020-07-06 18:46:55.608943: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-07-06 18:46:55.610544: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-07-06 18:46:55.611696: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-07-06 18:46:55.611988: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-07-06 18:46:55.613589: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-07-06 18:46:55.614478: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-07-06 18:46:55.618025: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-07-06 18:46:55.618159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-06 18:46:55.618734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-06 18:46:55.619206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-07-06 18:46:55.619480: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-07-06 18:46:55.643133: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2693910000 Hz
2020-07-06 18:46:55.643781: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x44161a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-07-06 18:46:55.643809: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-07-06 18:46:55.725002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-06 18:46:55.725324: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x44aa610 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-07-06 18:46:55.725349: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 960M, Compute Capability 5.0
2020-07-06 18:46:55.725532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-06 18:46:55.725767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 960M computeCapability: 5.0
coreClock: 1.176GHz coreCount: 5 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 74.65GiB/s
2020-07-06 18:46:55.725796: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-07-06 18:46:55.725828: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-07-06 18:46:55.725854: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-07-06 18:46:55.725882: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-07-06 18:46:55.725908: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-07-06 18:46:55.725938: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-07-06 18:46:55.725988: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-07-06 18:46:55.726091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-06 18:46:55.726485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-06 18:46:55.726724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-07-06 18:46:55.726756: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 926, in __call__
    input_list)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 1098, in _functional_construction_call
    self._maybe_build(inputs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 2643, in _maybe_build
    self.build(input_shapes)  # pylint:disable=not-callable
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py"", line 323, in wrapper
    output_shape = fn(instance, input_shape)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/embeddings.py"", line 135, in build
    if (context.executing_eagerly() and context.context().num_gpus() and
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py"", line 1082, in num_gpus
    self.ensure_initialized()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py"", line 539, in ensure_initialized
    context_handle = pywrap_tfe.TFE_NewContext(opts)
tensorflow.python.framework.errors_impl.InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid

```
"
41131,Support strings as a first-class input/output data type in Swift,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.2.0
- Are you willing to contribute it (Yes/No): Maybe if time/company policy permits.


**Describe the feature and the current behavior/state.**
Support strings as a first-class input/output data type in Swift. Currently only the several bool/int/float types are supported, per https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/swift/Sources/Tensor.swift

**Will this change the current api? How?**
Yes on the Swift language bindings

**Who will benefit with this feature?**
iOS/Swift developers

**Any Other info.**
Asked on the community group, and was redirected here to file this feature request. CC: @jdduke "
41130,ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none) ERROR: No matching distribution found for tensorflow,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- Windows 10 64bits
- pip install tensorflow
- Tensorflow 2.2
- python 3.8.3
- installation with pip
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.0 / 11.0
- NVIDIA GEFORCE 920M



i installed all the required packages and checked if my hardward is equiped with what i need, i have VT-X and GPU but can't install i have this kind of error:

ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)
ERROR: No matching distribution found for tensorflow

can anyone explain me why? and how to fix it?
**Provide the exact sequence of commands / steps that you executed before running into the problem**



"
41129,Grappler errors on LSTM jacobian when `experimental_use_pfor=False`,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): `Yes`
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Ubuntu 16.04`
- TensorFlow installed from (source or binary): `Binary`
- TensorFlow version (use command below): `v1.12.1-35711-g8202ae9d5c 2.4.0-dev20200705`
- Python version: `3.7`
- CUDA/cuDNN version:
```
$ conda list | grep cud
cudatoolkit               10.1.243             h6bb024c_0
cudnn                     7.6.5                cuda10.1_0
```
- GPU model and memory: `Nvidia GeForce GTX 1080 Ti`

**Describe the current behavior**
When running with gpu, grappler errors when computing `GradientTape.jacobian(..., experimental_use_pfor=False)` through LSTM. The code below runs, but outputs error messages to stdout. In some cases, similar code seems to be slower on gpu than cpu, even though the `GradientTape.gradient` is much faster on gpu than cpu.

**Describe the expected behavior**
Code runs without grappler errors.

**Standalone code to reproduce the issue**
```python
import tensorflow as tf


batch_size, sequence_length = 3, 5

x_input = tf.keras.layers.Input(
    shape=(sequence_length, 1),
    name='input',
    dtype=tf.float32)

mask_input = tf.keras.layers.Input(
    shape=(sequence_length, ),
    name='mask',
    dtype=tf.bool)


out = tf.keras.layers.LSTM(
    units=256,
    return_sequences=True,
    return_state=False,
)(x_input, mask=mask_input)
hidden_layer_sizes = (256,  256)
for hidden_layer_size in hidden_layer_sizes:
    out = tf.keras.layers.Dense(hidden_layer_size, activation='relu')(out)
out = tf.keras.layers.Dense(1, activation='linear')(out)
model = tf.keras.Model((x_input, mask_input), out)

x = tf.random.uniform(
    (batch_size, sequence_length, x_input.shape[-1]),
    dtype=x_input.dtype)

mask = tf.sequence_mask(
    tf.random.uniform(
        (batch_size, ), minval=0, maxval=sequence_length, dtype=tf.int32),
    maxlen=sequence_length,
)[..., ::-1]


@tf.function(experimental_relax_shapes=True)
def compute_jacobian(x):
    y_true = tf.zeros(batch_size)
    with tf.GradientTape() as tape:
        y = model((x, mask))
        y = tf.reduce_sum(y, axis=1)
        loss = tf.losses.MSE(y_pred=y, y_true=y_true)

    jacobian = tape.jacobian(
        loss,
        model.trainable_variables,
        parallel_iterations=10,
        experimental_use_pfor=False)

    return jacobian

_ = compute_jacobian(x)
```

**Other info / logs**
```
$ export CUDA_VISIBLE_DEVICES=1 && python ./test_lstm_jacobian.py
...
2020-07-06 18:19:52.907774: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:581] function_optimizer failed: Invalid argument: Input 0 of node while/enter/_25 was passed bool from functional_1/lstm/PartitionedCall:5 incompatible with expected int32.
2020-07-06 18:19:52.954497: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:581] layout failed: Out of range: src_output = 26, but num_outputs is only 26
2020-07-06 18:19:52.999724: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:581] function_optimizer failed: Invalid argument: Input 0 of node while/enter/_25 was passed bool from functional_1/lstm/PartitionedCall:5 incompatible with expected int32.
2020-07-06 18:19:53.074385: W tensorflow/core/common_runtime/process_function_library_runtime.cc:773] Ignoring multi-device function optimization failure: Invalid argument: Input 0 of node while/enter/_25 was passed bool from functional_1/lstm/PartitionedCall:5 incompatible with expected int32.
...
```"
41128,"Slow TFLite GPU inference on Android, not matched with the benchmark","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Pixel 2
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): nightly build
- Python version:
- Bazel version (if compiling from source): 3.3.1
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

We're running GPU delegate on an Android phone (NDKr18b). However, it's very slow (15ms) compared with the latency running benchmark tool (6ms).

This is the performance reported by benchmark tool (6ms):
![Screenshot from 2020-07-06 23-42-06](https://user-images.githubusercontent.com/1322198/86620587-c105fd80-bfe6-11ea-8c16-79c3b42a74f2.png)

While this is the actual latency when running within our app (15ms)
![Screenshot from 2020-07-06 23-43-36](https://user-images.githubusercontent.com/1322198/86620591-c2cfc100-bfe6-11ea-9f28-7566a57e983e.png)

We're using `libtensorflowlite_gpu_gl.so` generated by running the following:

```
bazel build -c opt --config android_arm64 tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_gl.so  # for dynamic library
```

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
41127,Error TensorArrayWrite/TensorArrayWriteV3,"
**System information**
- TensorFlow version (use command below): 1.14
- Python version: 3.7
- GPU model and memory: GOOGLE COLAB
I want to do the deep cosine metric training with the VeRI data set, for this, I followed some recommendations proposed by the author [here](https://github.com/nwojke/cosine_metric_learning/issues/5#issuecomment-381883310), my configuration files are the same used in this [repository ](https://github.com/duyao-art/cosine_metric_learning_customer)
**any help will be very useful**

`/content/gdrive/My Drive/Trainig_Cosenet/cosine_metric_learning-master
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
Train set size: 34291 images, 518 identites
WARNING:tensorflow:From /content/gdrive/My Drive/Trainig_Cosenet/cosine_metric_learning-master/train_app.py:243: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /content/gdrive/My Drive/Trainig_Cosenet/cosine_metric_learning-master/train_app.py:250: The name tf.read_file is deprecated. Please use tf.io.read_file instead.

WARNING:tensorflow:From /content/gdrive/My Drive/Trainig_Cosenet/cosine_metric_learning-master/train_app.py:252: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.

WARNING:tensorflow:From /content/gdrive/My Drive/Trainig_Cosenet/cosine_metric_learning-master/queued_trainer.py:316: The name tf.FIFOQueue is deprecated. Please use tf.queue.FIFOQueue instead.

WARNING:tensorflow:From /content/gdrive/My Drive/Trainig_Cosenet/cosine_metric_learning-master/train_app.py:266: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.

WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0c4ffd0c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0c4ffd0c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /content/gdrive/My Drive/Trainig_Cosenet/cosine_metric_learning-master/nets/deep_sort/network_definition.py:19: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.

WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0c4ff62438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0c4ff62438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /content/gdrive/My Drive/Trainig_Cosenet/cosine_metric_learning-master/nets/deep_sort/network_definition.py:28: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.

WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0c4ffd0fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0c4ffd0fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0c4ff4fba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0c4ff4fba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f0c4ffd0b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f0c4ffd0b00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0c4ff62550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0c4ff62550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0c4ff62e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0c4ff62e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f0c4ffd0748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f0c4ffd0748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0c4ff4fe10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0c4ff4fe10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0c4fe1b4a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0c4fe1b4a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0c4feff0b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0c4feff0b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0c4fdd0ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0c4fdd0ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f0c4ff4e0b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f0c4ff4e0b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0c4fdd0ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0c4fdd0ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0c4fdc0a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0c4fdc0a20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0c4fdc04e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0c4fdc04e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0c4f08e2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0c4f08e2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f0c4ff4e4e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f0c4ff4e4e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0c4ff2b748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0c4ff2b748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0c4ff2b748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0c4ff2b748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0c4f034f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0c4f034f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0c4ff4ee10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0c4ff4ee10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0c4fe4a358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0c4fe4a358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f0c4f034a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f0c4f034a58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0c4fdc0b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0c4fdc0b70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0c4ef2bfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0c4ef2bfd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0c4feff390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0c4feff390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0c4ee74828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0c4ee74828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f0c4f08ef28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f0c4f08ef28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0c4fe4a860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0c4fe4a860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0c4ef4b470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0c4ef4b470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0c4eebe710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0c4eebe710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0c4eebe240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0c4eebe240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0c4ee08710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0c4ee08710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f0c4eebec50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f0c4eebec50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0c4edebeb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0c4edebeb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
feature dimensionality:  128
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f0c4f055cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f0c4f055cc0>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f0c4eebe048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f0c4eebe048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f0c4eea2e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f0c4eea2e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /content/gdrive/My Drive/Trainig_Cosenet/cosine_metric_learning-master/nets/deep_sort/network_definition.py:84: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.
Instructions for updating:
dim is deprecated, use axis instead
WARNING:tensorflow:From /content/gdrive/My Drive/Trainig_Cosenet/cosine_metric_learning-master/nets/deep_sort/network_definition.py:94: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /content/gdrive/My Drive/Trainig_Cosenet/cosine_metric_learning-master/nets/deep_sort/network_definition.py:97: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /content/gdrive/My Drive/Trainig_Cosenet/cosine_metric_learning-master/train_app.py:615: sparse_softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.
Instructions for updating:
Use tf.losses.sparse_softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:409: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.
Instructions for updating:
Use tf.losses.compute_weighted_loss instead.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:152: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:154: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:121: add_arg_scope.<locals>.func_with_args (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.
Instructions for updating:
Use tf.losses.add_loss instead.
WARNING:tensorflow:From /content/gdrive/My Drive/Trainig_Cosenet/cosine_metric_learning-master/losses.py:142: The name tf.log is deprecated. Please use tf.math.log instead.

WARNING:tensorflow:From /content/gdrive/My Drive/Trainig_Cosenet/cosine_metric_learning-master/train_app.py:280: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.

WARNING:tensorflow:From /content/gdrive/My Drive/Trainig_Cosenet/cosine_metric_learning-master/train_app.py:282: The name tf.losses.get_total_loss is deprecated. Please use tf.compat.v1.losses.get_total_loss instead.

WARNING:tensorflow:From /content/gdrive/My Drive/Trainig_Cosenet/cosine_metric_learning-master/train_app.py:284: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From /content/gdrive/My Drive/Trainig_Cosenet/cosine_metric_learning-master/train_app.py:290: The name tf.losses.get_regularization_loss is deprecated. Please use tf.compat.v1.losses.get_regularization_loss instead.

---------------------------------------
Run ID:  cosine-softmax
Log directory:  ./output/veri/cosine-softmax
---------------------------------------
WARNING:tensorflow:From /content/gdrive/My Drive/Trainig_Cosenet/cosine_metric_learning-master/queued_trainer.py:404: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/slim/python/slim/learning.py:742: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.MonitoredTrainingSession
2020-07-06 14:06:25.932540: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-07-06 14:06:25.936373: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz
2020-07-06 14:06:25.936668: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2121800 executing computations on platform Host. Devices:
2020-07-06 14:06:25.936703: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
2020-07-06 14:06:26.086949: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1066: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file utilities to get mtimes.
2020-07-06 14:06:36.550855: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at tensor_array_ops.cc:447 : Invalid argument: TensorArray map/TensorArray_1_2: Could not write to TensorArray index 1 because the value shape is [220,322,3] which is incompatible with the TensorArray's inferred element shape: [159,184,3] (consider setting infer_shape=False).
EnqueueError: TensorArray map/TensorArray_1_2: Could not write to TensorArray index 1 because the value shape is [220,322,3] which is incompatible with the TensorArray's inferred element shape: [159,184,3] (consider setting infer_shape=False).
	 [[node map/while/TensorArrayWrite/TensorArrayWriteV3 (defined at /content/gdrive/My Drive/Trainig_Cosenet/cosine_metric_learning-master/train_app.py:251) ]]

Errors may have originated from an input operation.
Input Source operations connected to node map/while/TensorArrayWrite/TensorArrayWriteV3:
 map/while/DecodeJpeg (defined at /content/gdrive/My Drive/Trainig_Cosenet/cosine_metric_learning-master/train_app.py:250)

Original stack trace for 'map/while/TensorArrayWrite/TensorArrayWriteV3':
  File ""train_veri.py"", line 133, in <module>
    main()
  File ""train_veri.py"", line 97, in main
    **train_kwargs)
  File ""/content/gdrive/My Drive/Trainig_Cosenet/cosine_metric_learning-master/train_app.py"", line 188, in train_loop
    trainable_scopes=trainable_scopes)
  File ""/content/gdrive/My Drive/Trainig_Cosenet/cosine_metric_learning-master/train_app.py"", line 251, in create_trainer
    filename_var, back_prop=False, dtype=tf.uint8)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/map_fn.py"", line 268, in map_fn
    maximum_iterations=n)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 3501, in while_loop
    return_same_structure)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 3012, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 2937, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 3456, in <lambda>
    body = lambda i, lv: (i + 1, orig_body(*lv))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/map_fn.py"", line 260, in compute
    tas = [ta.write(i, value) for (ta, value) in zip(tas, flat_fn_values)]
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/map_fn.py"", line 260, in <listcomp>
    tas = [ta.write(i, value) for (ta, value) in zip(tas, flat_fn_values)]
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/tensor_array_ops.py"", line 1191, in write
    return self._implementation.write(index, value, name=name)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py"", line 193, in wrapped
    return _add_should_use_warning(fn(*args, **kwargs))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/tensor_array_ops.py"", line 293, in write
    name=name)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_data_flow_ops.py"", line 8288, in tensor_array_write_v3
    flow_in=flow_in, name=name)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3616, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 2005, in __init__
    self._traceback = tf_stack.extract_stack()

2020-07-06 14:06:37.324861: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at tensor_array_ops.cc:447 : Invalid argument: TensorArray map/TensorArray_1_0: Could not write to TensorArray index 1 because the value shape is [235,345,3] which is incompatible with the TensorArray's inferred element shape: [134,111,3] (consider setting infer_shape=False).
EnqueueError: TensorArray map/TensorArray_1_0: Could not write to TensorArray index 1 because the value shape is [235,345,3] which is incompatible with the TensorArray's inferred element shape: [134,111,3] (consider setting infer_shape=False).
	 [[node map/while/TensorArrayWrite/TensorArrayWriteV3 (defined at /content/gdrive/My Drive/Trainig_Cosenet/cosine_metric_learning-master/train_app.py:251) ]]

Errors may have originated from an input operation.
Input Source operations connected to node map/while/TensorArrayWrite/TensorArrayWriteV3:
 map/while/DecodeJpeg (defined at /content/gdrive/My Drive/Trainig_Cosenet/cosine_metric_learning-master/train_app.py:250)

Original stack trace for 'map/while/TensorArrayWrite/TensorArrayWriteV3':
  File ""train_veri.py"", line 133, in <module>
    main()
  File ""train_veri.py"", line 97, in main
    **train_kwargs)
  File ""/content/gdrive/My Drive/Trainig_Cosenet/cosine_metric_learning-master/train_app.py"", line 188, in train_loop
    trainable_scopes=trainable_scopes)
  File ""/content/gdrive/My Drive/Trainig_Cosenet/cosine_metric_learning-master/train_app.py"", line 251, in create_trainer
    filename_var, back_prop=False, dtype=tf.uint8)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/map_fn.py"", line 268, in map_fn
    maximum_iterations=n)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 3501, in while_loop
    return_same_structure)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 3012, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 2937, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 3456, in <lambda>
    body = lambda i, lv: (i + 1, orig_body(*lv))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/map_fn.py"", line 260, in compute
    tas = [ta.write(i, value) for (ta, value) in zip(tas, flat_fn_values)]
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/map_fn.py"", line 260, in <listcomp>
    tas = [ta.write(i, value) for (ta, value) in zip(tas, flat_fn_values)]
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/tensor_array_ops.py"", line 1191, in write
    return self._implementation.write(index, value, name=name)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py"", line 193, in wrapped
    return _add_should_use_warning(fn(*args, **kwargs))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/tensor_array_ops.py"", line 293, in write
    name=name)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_data_flow_ops.py"", line 8288, in tensor_array_write_v3
    flow_in=flow_in, name=name)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3616, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 2005, in __init__
    self._traceback = tf_stack.extract_stack()

2020-07-06 14:06:44.331223: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at tensor_array_ops.cc:447 : Invalid argument: TensorArray map/TensorArray_1_4: Could not write to TensorArray index 1 because the value shape is [230,308,3] which is incompatible with the TensorArray's inferred element shape: [433,568,3] (consider setting infer_shape=False).
EnqueueError: TensorArray map/TensorArray_1_4: Could not write to TensorArray index 1 because the value shape is [230,308,3] which is incompatible with the TensorArray's inferred element shape: [433,568,3] (consider setting infer_shape=False).
	 [[node map/while/TensorArrayWrite/TensorArrayWriteV3 (defined at /content/gdrive/My Drive/Trainig_Cosenet/cosine_metric_learning-master/train_app.py:251) ]]

Errors may have originated from an input operation.
Input Source operations connected to node map/while/TensorArrayWrite/TensorArrayWriteV3:
 map/while/DecodeJpeg (defined at /content/gdrive/My Drive/Trainig_Cosenet/cosine_metric_learning-master/train_app.py:250)

Original stack trace for 'map/while/TensorArrayWrite/TensorArrayWriteV3':
  File ""train_veri.py"", line 133, in <module>
    main()
  File ""train_veri.py"", line 97, in main
    **train_kwargs)
  File ""/content/gdrive/My Drive/Trainig_Cosenet/cosine_metric_learning-master/train_app.py"", line 188, in train_loop
    trainable_scopes=trainable_scopes)
  File ""/content/gdrive/My Drive/Trainig_Cosenet/cosine_metric_learning-master/train_app.py"", line 251, in create_trainer
    filename_var, back_prop=False, dtype=tf.uint8)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/map_fn.py"", line 268, in map_fn
    maximum_iterations=n)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 3501, in while_loop
    return_same_structure)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 3012, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 2937, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 3456, in <lambda>
    body = lambda i, lv: (i + 1, orig_body(*lv))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/map_fn.py"", line 260, in compute
    tas = [ta.write(i, value) for (ta, value) in zip(tas, flat_fn_values)]
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/map_fn.py"", line 260, in <listcomp>
    tas = [ta.write(i, value) for (ta, value) in zip(tas, flat_fn_values)]
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/tensor_array_ops.py"", line 1191, in write
    return self._implementation.write(index, value, name=name)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py"", line 193, in wrapped
    return _add_should_use_warning(fn(*args, **kwargs))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/tensor_array_ops.py"", line 293, in write
    name=name)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_data_flow_ops.py"", line 8288, in tensor_array_write_v3
    flow_in=flow_in, name=name)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3616, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 2005, in __init__
    self._traceback = tf_stack.extract_stack()

/usr/local/lib/python3.6/dist-packages/tensorflow/python/summary/writer/writer.py:386: UserWarning: Attempting to use a closed FileWriter. The operation will be a noop unless the FileWriter is explicitly reopened.
  warnings.warn(""Attempting to use a closed FileWriter. ""
2020-07-06 14:06:45.082525: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at tensor_array_ops.cc:447 : Invalid argument: TensorArray map/TensorArray_1_6: Could not write to TensorArray index 1 because the value shape is [165,213,3] which is incompatible with the TensorArray's inferred element shape: [176,242,3] (consider setting infer_shape=False).`


"
41126,tf.io.decode_image rotates the images,"When I read some image files tf.io.decode_image rotates the images by n*90 degrees in comparison with how the images are shown in image viewers programs such as IrfanView and OpenCV's imread. I'm attaching one of the images: https://imgur.com/vPMQPq7
You can check it using my code:

```
import os
import tensorflow as tf

path = r'path_to_folder'

filenames = os.listdir(path)
new_path = path + '_after_tf'
if not os.path.isdir(new_path):
    os.mkdir(new_path)

for filename in filenames:
    img_bytes = open(os.path.join(path, filename), 'rb').read()
    img = tf.io.decode_image(img_bytes)
    new_img_bytes = tf.io.encode_jpeg(img)
    tf.io.write_file(os.path.join(new_path, os.path.splitext(filename)[0] + '.jpg'), new_img_bytes)
```
It inputs image files in folder ""path"" and outputs images after tf.io.decode_image, tf.io.encode_jpeg, tf.io.write_file in the folder with the same name + the suffix ""_after_tf""

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): pip install
- TensorFlow version (use command below): 2.2
- Python version: 3.7
"
41125,nan bug running automatic differentiation,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10  and Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):  tf 2.2.0
- Python version:  3.6.5
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version:N/A
- GPU model and memory:N/A

**Describe the current behavior**

tf.GradientTape() gives wrong high order derivatives of the Morse potential function.

**Describe the expected behavior**

See minimon reproduction code below.

**Standalone code to reproduce the issue**



 ```python
import numpy as np
import tensorflow as tf

def main() :
    x=np.arange(0.9,1.1,0.01)
    print(gradientcalculation(x))
    print(d3(x))

def morse(x): # this is morse potential
    return (1.0-tf.math.exp(-(x-1.0)))**2

def d3(x):#this is analytical 3rd order derivative of morse potential
    return np.exp(1.0-2.0*x)*(2.0*np.exp(x)-8.0*np.exp(1.0))

#calculate gradient of morse potential:(1-e**(-1(r-1))**2
def gradientcalculation(x):
    x=tf.convert_to_tensor(x,dtype='float64')
    with tf.GradientTape() as t:
            x=tf.convert_to_tensor(x,dtype='float64')
    with tf.GradientTape() as t:
        with tf.GradientTape() as t2:
            with tf.GradientTape() as t3:
                t3.watch(x)
                t2.watch(x)
                t.watch(x)
                y=morse(x)
            dy_dx=t3.gradient(y,x)
        t.watch(dy_dx)
        d2y_dx2=t2.gradient(dy_dx,x)
    d3y_dx3=t.gradient(d2y_dx2,x)
    return d3y_dx3

main()
```
outpu is :
```
tf.Tensor(
[-7.56088023 -7.38939034 -7.22151283 -7.05717403 -6.89630172 -6.73882515
 -6.58467499 -6.4337833  -6.28608351 -6.14151039         nan -5.86148972
 -5.72591817 -5.5932252  -5.46335189 -5.3362405  -5.21183443 -5.09007824
 -4.97091762 -4.85429932 -4.74017119], shape=(21,), dtype=float64)
[-7.56088023 -7.38939034 -7.22151283 -7.05717403 -6.89630172 -6.73882515
 -6.58467499 -6.4337833  -6.28608351 -6.14151039 -6.         -5.86148972
 -5.72591817 -5.5932252  -5.46335189 -5.3362405  -5.21183443 -5.09007824
 -4.97091762 -4.85429932 -4.74017119]
```

It should output same values, but when x=1.0 , tf.GradientTape()  provide 'nan'.

**Other info / logs** 
N/A"
41124,tf.summary.image log spam when using file_writer in graph mode,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2.0
- Python version: 3.7.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: [Running on CPU]
- GPU model and memory:

**Describe the current behavior**

Running TF in 1.x graph mode, I create a trivial tf.summary.image. The error message below appears, although the code continues running, and the image summary is written.

```
ERROR:tensorflow:==================================
Object was never used (type <class 'tensorflow.python.framework.ops.Operation'>):
<tf.Operation 'fooey/write_summary/assert_non_negative/assert_less_equal/Assert/Assert' type=Assert>
If you want to mark it as used call its ""mark_used()"" method.
It was originally created here:
  File ""/home/rosea/conda/envs/tf2n/lib/python3.7/site-packages/tensorflow/python/ops/check_ops.py"", line 540, in assert_non_negative_v2
    name=name)  File ""/home/rosea/conda/envs/tf2n/lib/python3.7/site-packages/tensorflow/python/ops/check_ops.py"", line 560, in assert_non_negative
    return assert_less_equal(zero, x, data=data, summarize=summarize)  File ""/home/rosea/conda/envs/tf2n/lib/python3.7/site-packages/tensorflow/python/ops/check_ops.py"", line 924, in assert_less_equal
    np.less_equal, x, y, data, summarize, message, name)  File ""/home/rosea/conda/envs/tf2n/lib/python3.7/site-packages/tensorflow/python/ops/check_ops.py"", line 372, in _binary_assert
    return control_flow_ops.Assert(condition, data, summarize=summarize)  File ""/home/rosea/conda/envs/tf2n/lib/python3.7/site-packages/tensorflow/python/util/tf_should_use.py"", line 237, in wrapped
    error_in_function=error_in_function)
==================================
```

**Describe the expected behavior**

The error message should not appear.

**Standalone code to reproduce the issue**

```
import tensorflow as tf
tf.compat.v1.disable_eager_execution()

writer = tf.summary.create_file_writer('.')
with writer.as_default():
  y = tf.constant(0, shape=(2,2,2,2))  
  s = tf.summary.image(name=""foo"", data=y, step=0)

# Error message occurs before code below runs
with tf.compat.v1.Session() as sess:
  sess.run(writer.init())
  sess.run(s)
```


**Other info / logs**

See previous issue: https://github.com/tensorflow/tensorflow/issues/33223'
The difference here is the use of the summary file writer.

Any of the following changes make the error message go away:
- Removing `with writer.as_default():` (though of course then nothing gets written)
- using `tf.summary.scalar(""foo"", tf.reduce_mean(y), step=0)` in place of `tf.summary.image(...)`
- using eager execution instead of graph mode
"
41123,"tf.keras.layers.Permute documentation is misleading, error messages even more so","
## URL(s) with the issue:

https://www.tensorflow.org/api_docs/python/tf/keras/layers/Permute

## Description of issue (what needs changing):

### Clear description

Permute layer is quite picky about its dims argument despite docs clearly saying 

`dims: Tuple of integers. Permutation pattern, does not include the samples dimension. Indexing starts at 1. For instance, (2, 1) permutes the first and second dimensions of the input. `

 It does not actually follow from this documentation that the dims input MUST have all the dimensions clearly listed without misses -  as per check actually present in the code:
``` python
   if sorted(dims) != list(range(1, len(dims) + 1)):
      raise ValueError(
          'Invalid permutation `dims` for Permute Layer: %s. '
          'The set of indices in `dims` must be consecutive and start from 1.' %
          (dims,))
```
The next hurdle is that the error message thrown by runtime is kind of inconsistent:

``` python
keras.layers.Permute((3, 2), input_shape=[30, 6, 8], name=f""Permute_layer"")
>>> Invalid permutation `dims` for Permute Layer: (3, 2). The set of indices in `dims` must be consecutive and start from 1.
```
It does not say in the doc that the dims must start with 1, it just says indexing starts with 1! Ok now the user knows it must start with 1, but how does one actually get dimensions 2 and 3 swapped? At that point it's just a mess from there on.

### Suggest following changes:
to the docs: please make example use 3D tensor, rather than 2D. Then it would be clear that all dims must be listed, even if they are not to be permuted, e.g.
``` python
tmp = keras.layers.Permute((1, 3, 2), name=f""Permute_input"")(tmp)
```

Another note that docs do not make is that the len(ndim) must match the  input_shape dimensions, which is however checked by the __init__, again possibly conflicting with the doc that says that input_shape can be whatever (clearly not whatever, it is coupled with ndim argument.


"
41122,Huge memory usage on hosts with 1TB RAM,"**System information**

We use keras and TF for our models:

Centos7

Python 3.6.1

Keras==2.2.4
Keras-Applications==1.0.8
Keras-Preprocessing==1.1.0

tensorboard==1.12.2
tensorflow==1.12.0
tensorflow-estimator==1.13.0

Threre is no GPU on our servers.

**Describe the current behavior**

Our script loads trained model from files (json + h5) and predict some useful parameter.
When we run script on small server with 8GB RAM and measure memory after script load model we see it take 160Mb commited memory 1,5GB  virtual size.

When we run script on large production server with 1TB RAM and measure memory after script load exactly same model we see it take 220Mb commited memory 14GB  virtual size.

**Describe the expected behavior**

There are some limitation from our DEVOPS team and our models must take not more 1GB commited and not more 2GB virtual size. So we would like to have same memory fingerprint on any server. If there is any way setup TF to use fixed memory size on any server?

"
41120,WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000021DB69A21F8> and will run it as-is.,"```
import tensorflow as tf

model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(loss='binary_crossentropy',
              optimizer=""adam"",
              metrics=['accuracy'])

from tensorflow.keras.preprocessing.image import ImageDataGenerator
train_datagen = ImageDataGenerator(rescale=1/255)

train_generator = train_datagen.flow_from_directory(
        r'C:\Users\asus\Downloads\tf horse human\\horsehuman', 
        target_size=(300, 300),  
        batch_size=128,
        class_mode='binary')

model.fit(train_generator,
          steps_per_epoch=8,  
          epochs=15)
```

Then it shows the warning.
WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000021DB69A21F8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: Bad argument number for Name: 4, expecting 3
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

8/8 [==============================] - 45s 6s/step - loss: 0.5747 - accuracy: 0.6699
Epoch 2/15
8/8 [==============================] - 47s 6s/step - loss: 0.2592 - accuracy: 0.8821
Epoch 3/15
8/8 [==============================] - 41s 5s/step - loss: 0.2060 - accuracy: 0.9121
Epoch 4/15
8/8 [==============================] - 40s 5s/step - loss: 0.1459 - accuracy: 0.9466
Epoch 5/15
8/8 [==============================] - 41s 5s/step - loss: 0.0600 - accuracy: 0.9800
Epoch 6/15
8/8 [==============================] - 40s 5s/step - loss: 0.0341 - accuracy: 0.9855
Epoch 7/15
8/8 [==============================] - 41s 5s/step - loss: 0.0334 - accuracy: 0.9911
Epoch 8/15
8/8 [==============================] - 40s 5s/step - loss: 0.0198 - accuracy: 0.9911
Epoch 9/15
8/8 [==============================] - 39s 5s/step - loss: 0.0085 - accuracy: 0.9989
Epoch 10/15
8/8 [==============================] - 44s 6s/step - loss: 0.0043 - accuracy: 0.9990
Epoch 11/15
8/8 [==============================] - 46s 6s/step - loss: 0.0025 - accuracy: 1.0000
Epoch 12/15
8/8 [==============================] - 40s 5s/step - loss: 0.0014 - accuracy: 1.0000
Epoch 13/15
8/8 [==============================] - 40s 5s/step - loss: 0.0014 - accuracy: 1.0000
Epoch 14/15
8/8 [==============================] - 40s 5s/step - loss: 5.8768e-04 - accuracy: 1.0000
Epoch 15/15
8/8 [==============================] - 40s 5s/step - loss: 7.8857e-04 - accuracy: 1.0000


"
41119,Unable to convert ALBERT lite,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Catalina 10.15.3 (19D76), also tried using colab runtime
- TensorFlow installed from source
- TensorFlow version: have tried multiple - 2.3.0-dev20200608, 2.4.0-dev20200705, 2.2.0, 1.15.0


Please refer to the [colab notebook](https://colab.research.google.com/drive/16eEDreq7F2DZLSFrQJJrrwPQ2MubKrh3#scrollTo=WOZp9iGvp9yb), I am able to convert [bert_en_uncased](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1) but not [albert_lite](https://tfhub.dev/tensorflow/albert_lite_base/1) even though albert lite is meant to be tflite compatible. 

I have tried different versions of tensorflow, as well as different ways of converting - from keras model / saved model / concrete function but none works :(

related issue: https://github.com/tensorflow/tensorflow/issues/34396"
41118,Debug .dll build with CUDA support on Windows fails to link,"**System information**
- OS Platform and Distribution: Windows 10 Enterprise 1809
- TensorFlow installed from: source
- TensorFlow version: v2.2.0
- Python version: 3.5.6
- Bazel version (if compiling from source): 2.0.0
- Compiler version (if compiling from source): MSVC 14.26.28801
- CUDA/cuDNN version: CUDA 10.1 / cuDNN 7.6.5.32
- GPU model and memory: NVIDIA Titan Xp


After [some issues](https://github.com/tensorflow/tensorflow/issues/39905), a ""default"" build for the Tensorflow dll on my finally machine succeeded (i.e. with a command not unlike `bazel build --jobs=8 --define=no_tensorflow_py_deps=true -c opt //tensorflow:tensorflow.dll`).

Now I tried to build the same dll in debug mode: `bazel build --jobs=8 --define=no_tensorflow_py_deps=true` **-c opt -c dbg** `--config=c++17 //tensorflow:tensorflow.dll`.

This causes several issues, which are at least somewhat solvable, thought probably still not great:

1. Errors of the kind ""error C4716: 'xla::HloInstruction::unique_indices': must return a value"" -> solved by adding `--copt=/wd4716` to build:windows in .bazelrc
2. It cannot compile with multiple jobs (get multiple - as many as --jobs - error windows with messages like ""Debug errror! Program ...\x64_windows-dbg\bin\external\llvm-project\llvm-tblgen: abort() has been called"", and messages in the terminal like ""Assertion failed: !Ptr && !DeleterFn && !Next && ""Partially initialized ManagedStatic!?"", file external/llvm-project/llvm/lib/Support/ManagedStatic.cpp, line 51"". 

The latter actually does not go away once it shows up - i.e. a repeat of the build command (with 1 or multiple jobs, does not matter) does not go any further, but adding `--define=with_xla_support=false` to the build command helped (I used to add it before because of the issue 1 above, then found out that that problem is not just inside xla::* but also arises for some other code in the repo, and found another workaround there). ~~Not sure if xla is to blame here, maybe adding the flag just triggered some re-do in the build chain...~~  **UPDATE:** without the  `--define=with_xla_support=false` option, this appears on a clean one-job build as well.

3. Another issue of the similar kind: Getting errors like ""'C:\users\mikhail.startsev\_bazel_mikhail.startsev\3watukzj\execroot\org_tensorflow\vc140.pdb'; if multiple CL.EXE write to the same .PDB file, please use /FS"""" -> adding `--copt=/FS` **does not help**, no idea why, still had to run with `--jobs=1` to make it work.


And _after all that_, it fails to link with **a lot** of undefined externals and a ton warnings to boot. I attach a full subcommand log (with `-s --verbose_failures`): [link_log.txt](https://github.com/tensorflow/tensorflow/files/4877528/link_log.txt)

Examples of unresolved symbols:

- void __cdecl **tensorflow::ConcatGPU**<unsigned __int64>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<unsigned __int64 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<unsigned __int64 const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<unsigned __int64 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<unsigned __int64 const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<unsigned __int64,2,1,__int64>,16,struct Eigen::MakePointer> *)
- public: void __cdecl **tensorflow::functor::SpaceToDepthOpFunctor**<struct Eigen::GpuDevice,bool,1>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<bool const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<bool,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$SpaceToDepthOpFunctor@UGpuDevice@Eigen@@_N$00@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CB_N$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@_N$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,bool>::Compute(class tensorflow::OpKernelContext *)
- public: void __cdecl **tensorflow::functor::DenseUpdate**<struct Eigen::GpuDevice,class tensorflow::Variant,2>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant,1,1,__int64>,16,struct Eigen::MakePointer>,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant const ,1,1,__int64>,16,struct Eigen::MakePointer>)


Warnings seem to be of 2 kinds:

1. libdebug_ops_gpu.lo(debug_ops_gpu.cu.o) : **warning LNK4099: PDB 'vc140.pdb' was not found** with 'libdebug_ops_gpu.lo(debug_ops_gpu.cu.o)' or at 'C:\users\mikhail.startsev\_bazel_mikhail.startsev\3watukzj\execroot\org_tensorflow\bazel-out\x64_windows-dbg\bin\tensorflow\vc140.pdb'; linking object as if no debug info
2. LINK : warning LNK4286: symbol '??0Status@tensorflow@@QEAA@W4Code@error@1@Vstring_view@absl@@@Z (public: __cdecl tensorflow::Status::Status(enum tensorflow::error::Code,class absl::string_view))' defined in 'libstatus.a(status.o)' is imported by 'libenv.a(env.o)'

There are plenty of both to go around...

Will try to building from scratch with 1 job from the very beginning, thought that might take a while. 

Any help is appreciated :)

"
41117,Error Converting ssd_mobilenet_v2_oid_v4_2018_12_12 To Tflite,"**System information**
- Windows 10 build 2004
- pip install tf_night-cpu
- TensorFlow version 2.4.0-dev20200705

**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
from tensorflow.compat.v1.lite import TFLiteConverter
dir = 'models/ssd_mobilenet_v2_oid_v4_2018_12_12/saved_model'
converter = TFLiteConverter.from_saved_model(dir)
tflite_model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(tflite_model)
```

**The output from the converter invocation**

```
2020-07-06 01:15:38.959932: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-07-06 01:15:38.964732: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2b8abaa2fa0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-07-06 01:15:38.964778: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
WARNING:tensorflow:From tf\lib\site-packages\tensorflow\lite\python\convert_saved_model.py:60: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.
2020-07-06 01:16:12.245270: I tensorflow/core/grappler/devices.cc:78] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)
2020-07-06 01:16:12.245407: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-07-06 01:16:15.644668: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize
2020-07-06 01:16:15.644700: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.005ms.
2020-07-06 01:16:15.644707: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-07-06 01:17:05.110725: I tensorflow/core/grappler/devices.cc:78] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)
2020-07-06 01:17:05.110861: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-07-06 01:17:08.586833: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize
2020-07-06 01:17:08.586867: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2020-07-06 01:17:08.586874: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-07-06 01:17:29.988798: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:313] Ignored output_format.
2020-07-06 01:17:29.988832: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored drop_control_dependency.
loc(callsite(""Preprocessor/map/TensorArray""(""tf\lib\site-packages\tensorflow\python\saved_model\loader_impl.py"":299:0) at callsite(""tf\lib\site-packages\tensorflow\python\util\deprecation.py"":324:0 at callsite(""tf\lib\site-packages\tensorflow\lite\python\convert_saved_model.py"":198:0 at callsite(""tf\lib\site-packages\tensorflow\lite\python\lite.py"":1922:0 at ""convert.py"":4:0))))): error: 'tf.TensorArrayV3' op is neither a custom op nor a flex op
...
error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):
	tf.NonMaxSuppressionV3 {T = f32, T_threshold = f32, device = """"}
	tf.Size {device = """"}
	tf.TensorArrayGatherV3 {_class = [""loc:@Postprocessor/BatchMultiClassNonMaxSuppression/map/TensorArray_5""], device = """", element_shape = #tf.shape<100x4>}...
...
tf\lib\site-packages\tensorflow\python\saved_model\loader_impl.py:299:0: note: called from
tf\lib\site-packages\tensorflow\python\util\deprecation.py:324:0: note: called from
tf\lib\site-packages\tensorflow\lite\python\convert_saved_model.py:198:0: note: called from
tf\lib\site-packages\tensorflow\lite\python\lite.py:1922:0: note: called from
convert.py:4:0: note: called from
<unknown>:0: note: loc(callsite(""Preprocessor/map/while/TensorArrayReadV3@_functionalize_body_0"" at callsite(""Preprocessor/map/while/LoopCond""(""tf\lib\site-packages\tensorflow\python\saved_model\loader_impl.py"":299:0) at callsite(""tf\lib\site-packages\tensorflow\python\util\deprecation.py"":324:0 at callsite(""tf\lib\site-packages\tensorflow\lite\python\convert_saved_model.py"":198:0 at callsite(""tf\lib\site-packages\tensorflow\lite\python\lite.py"":1922:0 at ""convert.py"":4:0)))))): see current operation: %2 = ""tf.TensorArrayReadV3""(%arg5, %arg1, %arg6) {device = """"} : (tensor<2x!tf.resource<tensor<*xf32>>>, tensor<i32>, tensor<f32>) -> tensor<*xf32>
<unknown>:0: error: loc(callsite(""Preprocessor/map/while/TensorArrayWrite/TensorArrayWriteV3@_functionalize_body_0"" at callsite(""Preprocessor/map/while/LoopCond""(""tf\lib\site-packages\tensorflow\python\saved_model\loader_impl.py"":299:0) at callsite(""tf\lib\site-packages\tensorflow\python\util\deprecation.py"":324:0 at callsite(""tf\lib\site-packages\tensorflow\lite\python\convert_saved_model.py"":198:0 at callsite(""tf\lib\site-packages\tensorflow\lite\python\lite.py"":1922:0 at ""convert.py"":4:0)))))): 'tf.TensorArrayWriteV3' op is neither a custom op nor a flex op
...
>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):
	tf.NonMaxSuppressionV3 {T = f32, T_threshold = f32, device = """"}
	tf.Size {device = """"}
	tf.TensorArrayGatherV3 {_class = [""loc:@Postprocessor/BatchMultiClassNonMaxSuppression/map/TensorArray_5""], device = """", element_shape = #tf.shape<100x4>}
	tf.TensorArrayGatherV3 {_class = [""loc:@Postprocessor/BatchMultiClassNonMaxSuppression/map/TensorArray_6""], device = """", element_shape = #tf.shape<100>}
	tf.TensorArrayGatherV3 {_class = [""loc:@Postprocessor/BatchMultiClassNonMaxSuppression/map/TensorArray_7""], device = """", element_shape = #tf.shape<100>}
	tf.TensorArrayGatherV3 {_class = [""loc:@Postprocessor/BatchMultiClassNonMaxSuppression/map/TensorArray_9""], device = """", element_shape = #tf.shape<>}
	tf.TensorArrayGatherV3 {_class = [""loc:@Preprocessor/map/TensorArray_1""], device = """", element_shape = #tf.shape<300x300x3>}
...
  %18 = ""tfl.depthwise_conv_2d""(%17, %cst_118, %cst_34) {depth_multiplier = 1 : i32, dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<?x75x75x144xf32>, tensor<1x3x3x144xf32>, tensor<144xf32>) -> tensor<?x75x75x144xf32>
  %19 = ""tfl.conv_2d""(%18, %cst_64, %cst_119) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<?x75x75x144xf32>, tensor<24x1x1x144xf32>, tensor<24xf32>) -> tensor<?x75x75x24xf32>
  %20 = ""tfl.add""(%19, %16) {fused_activation_function = ""NONE""} : (tensor<?x75x75x24xf32>, tensor<?x75x75x24xf32>) -> tensor<?x75x75x24xf32>
  %21 = ""tfl.conv_2d""(%20, %cst_65, %cst_37) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<?x75x75x24xf32>, tensor<144x1x1x24xf32>, tensor<144xf32>) -> tensor<?x75x75x144xf32>
  %22 = ""tfl.depthwise_conv_2d""(%21, %cst_120, %cst_36) {depth_multiplier = 1 : i32, dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 2 : i32, stride_w = 2 : i32} : (tensor<?x75x75x144xf32>, tensor<1x3x3x144xf32>, tensor<144xf32>) -> tensor<?x38x38x144xf32>
...
```

**Also, please include a link to the saved model or GraphDef**

```
http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_oid_v4_2018_12_12.tar.gz
```

**Failure details**
It gives the error above."
41116,save_weights and load_weights do not work as expected when used to restore original initialized weights,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0
- Python version: 3.6.9



**Describe the current behavior**

The `save_weights` and `load_weights` functions do not work as expected when used to save and restore the original, initialized weights of a model. For instance, in the following example, we create a model, save the original weights, corrupt the weights, and restore the original, uncorrupted weights.  The training results in a very high loss when the `corrupt_weights_but_restore` function is invoked, but a low loss when the `corrupt_weights_but_restore` is commented out.

```python
import tempfile
from tensorflow.keras import backend as K
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.datasets import boston_housing
(x_train, y_train), (x_test, y_test) = boston_housing.load_data()
def corrupt_weights_but_restore(model, X, Y):

   # save original uncorrupted weights
    new_file, weightfile = tempfile.mkstemp()
    model.save_weights(weightfile)       

   # corrupt/damage weights with very high learning rate
    K.set_value(model.optimizer.lr, 100) 
    model.fit(X, Y, epochs=5)

   # load original uncorrupted weights
    model.load_weights(weightfile) 
    return model

# build model
model = Sequential()
model.add(Dense(1, input_shape=(x_train.shape[1],), activation='linear'))
model.compile(optimizer='adam', loss='mse', metrics=['mse', 'mae'])

# uncommenting the line below produces high loss despite restoring original weights
model = corrupt_weights_but_restore(model, x_train, y_train)

# train
K.set_value(model.optimizer.lr, 0.05)
model.fit(x_train, y_train,
          batch_size=32,
          epochs=100)
~                       
```

**Describe the expected behavior**
Loss should be the same (and be low) regardless of whether or not `corrupt_weights_but_restore` is run, since the original, uncorrupted weights are restored in `corrupt_weights_but_restore`.
"
41115,Doc of rmsprop optimizer seems not consistent with the code,https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/RMSprop
41114,multilabel accuracy is lower in 2.2.0 than 2.1.0 for same code,"The following simple multilabel classification example returns a **100% accuracy in TensorFlow 2.1.0** but only **60%  accuracy in TensorFlow 2.2.0**.  The loss appears to be the same, though.    **Why is this?** 

```python
X = [[1,0,0,0,0,0,0],
      [1,2,0,0,0,0,0],
      [3,0,0,0,0,0,0],
      [3,4,0,0,0,0,0],
      [2,0,0,0,0,0,0],
      [3,0,0,0,0,0,0],
      [4,0,0,0,0,0,0],
      [2,3,0,0,0,0,0],
      [1,2,3,0,0,0,0],
      [1,2,3,4,0,0,0],
      [0,0,0,0,0,0,0],
      [1,1,2,3,0,0,0],
      [2,3,3,4,0,0,0],
      [4,4,1,1,2,0,0],
      [1,2,3,3,3,3,3],
      [2,4,2,4,2,0,0],
      [1,3,3,3,0,0,0],
      [4,4,0,0,0,0,0],
      [3,3,0,0,0,0,0],
      [1,1,4,0,0,0,0]]

Y = [[1,0,0,0],
    [1,1,0,0],
    [0,0,1,0],
    [0,0,1,1],
    [0,1,0,0],
    [0,0,1,0],
    [0,0,0,1],
    [0,1,1,0],
    [1,1,1,0],
    [1,1,1,1],
    [0,0,0,0],
    [1,1,1,0],
    [0,1,1,1],
    [1,1,0,1],
    [1,1,1,0],
    [0,1,0,0],
    [1,0,1,0],
    [0,0,0,1],
    [0,0,1,0],
    [1,0,0,1]]
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Embedding
from tensorflow.keras.layers import GlobalAveragePooling1D
import numpy as np


X = np.array(X)
Y = np.array(Y)
MAXLEN = 7
MAXFEATURES = 4
NUM_CLASSES = 4
model = Sequential()
model.add(Embedding(MAXFEATURES+1,
                    50,
                    input_length=MAXLEN))
model.add(GlobalAveragePooling1D())
model.add(Dense(NUM_CLASSES, activation='sigmoid'))
model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
model.fit(X, Y,
          batch_size=1,
          epochs=200,
          validation_data=(X, Y))
```"
41113,SyncBatchNormalization layer segfaults on multi-worker with NCCL,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 14.04 (in a Docker container, on an 18.04 host)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 2.2.0
- Python version: 3.6.8
- Bazel version (if compiling from source): 0.24.1-3.0
- GCC/Compiler version (if compiling from source): 4.8.5
- CUDA/cuDNN version: 10.0
- GPU model and memory: Nvidia TITAN X 11GB

**Describe the current behavior**
When training models with the `tf.keras.layers.experimental.SyncBatchNormalization` layer, and using `tf.distribute.experimental.MultiWorkerMirroredStrategy` to train across multiple workers with `tf.distribute.experimental.CollectiveCommunication.NCCL` communication, the model trains for some amount of time (e.g. several thousand steps), then crashes with a segfault.

**Describe the expected behavior**
The model should train without segfaulting.

**Standalone code to reproduce the issue**
An example is below. **Please note that this code must run on multiple workers.** The TF_CONFIG environment variable must be set appropriately for your specific multi-worker configuration. 

```
import tensorflow as tf
from tensorflow import keras

def get_dataset():
    x = tf.zeros([10], dtype=tf.float32)
    x = tf.data.Dataset.from_tensors(x)

    y = tf.constant([5])
    y = tf.data.Dataset.from_tensor_slices(y)

    dataset = tf.data.Dataset.zip((x, y))
    dataset = dataset.batch(1)
    dataset = dataset.repeat()
    return dataset

def main():
    # NOTE: You must set os.environ[""TF_CONFIG""] as appropriate
    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(tf.distribute.experimental.CollectiveCommunication.NCCL)
    # assert strategy.num_replicas_in_sync == 2

    # Create dataset
    dataset = get_dataset()

    with strategy.scope():
        # Construct model
        model = keras.Sequential(
            layers=[
                tf.keras.layers.experimental.SyncBatchNormalization(),
                tf.keras.layers.Dense(1),
            ]
        )
        model.compile(
            optimizer=keras.optimizers.Adam(),
            loss=keras.losses.MeanSquaredError(),
        )

    model.fit(x=dataset, steps_per_epoch=10 ** 6, epochs=10 ** 3)


if __name__ == ""__main__"":
    main()
```

This is reproducible across a wide array of contexts, for example a Keras model, an Estimator model, different GPU types, etc.

**Other info / logs**
I used `gdb` to inspect a coredump from the crashed process. The backtrace is:
```
#0  0x00007f1d68cef711 in tensorflow::NcclReducer::Run(std::function<void (tensorflow::Status const&)>) (this=this@entry=0x7f18cc011af0, 
    done=...) at external/org_tensorflow/tensorflow/core/kernels/collective_nccl_reducer.cc:185
#1  0x00007f1d71797ca6 in tensorflow::BaseCollectiveExecutor::<lambda()>::operator()(void) const (__closure=<optimized out>)
    at external/org_tensorflow/tensorflow/core/common_runtime/base_collective_executor.cc:276
#2  0x00007f1d71797efe in std::_Function_handler<void(), tensorflow::BaseCollectiveExecutor::ExecuteAsync(tensorflow::OpKernelContext*, const tensorflow::CollectiveParams&, const string&, tensorflow::StatusCallback)::<lambda()> >::_M_invoke(const std::_Any_data &) (__functor=...)
    at external/gcc_7_4/usr/include/c++/7/bits/std_function.h:316
#3  0x00007f1d669a0e08 in std::function<void ()>::operator()() const (this=this@entry=0x7f185e7fbe60)
    at external/gcc_7_4/usr/include/c++/7/bits/std_function.h:706
#4  0x00007f1d71cc4f44 in tensorflow::UnboundedWorkQueue::PooledThreadFunc (this=0x20758660)
    at external/org_tensorflow/tensorflow/core/platform/default/unbounded_work_queue.cc:99
#5  0x00007f1d71cc5004 in tensorflow::UnboundedWorkQueue::<lambda()>::operator() (__closure=<optimized out>)
    at external/org_tensorflow/tensorflow/core/platform/default/unbounded_work_queue.cc:68
#6  std::_Function_handler<void(), tensorflow::UnboundedWorkQueue::Schedule(tensorflow::UnboundedWorkQueue::WorkFunction)::<lambda()> >::_M_invoke(const std::_Any_data &) (__functor=...) at external/gcc_7_4/usr/include/c++/7/bits/std_function.h:316
#7  0x00007f1d669a0e08 in std::function<void ()>::operator()() const (this=<optimized out>)
    at external/gcc_7_4/usr/include/c++/7/bits/std_function.h:706
#8  0x00007f1d71d136dd in std::__invoke_impl<void, std::function<void ()>>(std::__invoke_other, std::function<void ()>&&) (__f=...)
    at external/gcc_7_4/usr/include/c++/7/bits/invoke.h:60
#9  std::__invoke<std::function<void ()>>(std::function<void ()>&&) (__fn=...) at external/gcc_7_4/usr/include/c++/7/bits/invoke.h:95
#10 std::thread::_Invoker<std::tuple<std::function<void ()> > >::_M_invoke<0ul>(std::_Index_tuple<0ul>) (this=<optimized out>)
    at external/gcc_7_4/usr/include/c++/7/thread:234
#11 std::thread::_Invoker<std::tuple<std::function<void ()> > >::operator()() (this=<optimized out>)
    at external/gcc_7_4/usr/include/c++/7/thread:243
#12 std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::function<void ()> > > >::_M_run() (this=<optimized out>)
    at external/gcc_7_4/usr/include/c++/7/thread:186
#13 0x00007f1d2eb72ae0 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#14 0x00007f1da0c12184 in start_thread (arg=0x7f185e7fc700) at pthread_create.c:312
#15 0x00007f1d9fdef03d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111
```

Disassembling the function showed that this was the offending instruction:
```
   0x00007f1d68cef707 <+2651>:  jg     0x7f1d68cf0642 <tensorflow::NcclReducer::Run(std::function<void (tensorflow::Status const&)>)+6550>
   0x00007f1d68cef70d <+2657>:  mov    0x18(%rbx),%rax
=> 0x00007f1d68cef711 <+2661>:  mov    0x8(%rax),%rdi
   0x00007f1d68cef715 <+2665>:  mov    (%rdi),%rax
   0x00007f1d68cef718 <+2668>:  mov    0x20(%rbx),%rsi
```

And printing out the registers shows `rax            0x0                 0`, so some sort of pointer is set to 0. It therefore looks like there is some sort of null pointer exception in line 185 of `collective_nccl_reducer.cc`, which I believe is the line `col_ctx_->col_exec->UnblockDependencies(*col_params_);`. I don't have any idea why it would segfault there, however. The same line appears shortly above on line 176, so it's strange that it would segfault the second time.

Also, a log is attached [here](https://github.com/tensorflow/tensorflow/files/4875867/sbn_multinode_log.txt), however it is not very interesting as it just runs for a while and then segfaults."
41112,[RPI Zero] ModuleNotFoundError: No module named 'tflite_runtime',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspbian GNU/Linux 10 (buster)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Raspberry Pi Zero
- TensorFlow installed from (source or binary): Compile natively on Raspberry Pi using https://www.tensorflow.org/lite/guide/build_rpi
- TensorFlow version: Trying to install and Tensorflow lite as a standalone
- Python version: 3.7.3
- Installed using virtualenv? pip? conda?: 
- Bazel version (if compiling from source): None
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the problem**
Trying to install Tensorflow Lite on RPI Zero as a standalone. However once installation is complete as per the following steps,
sudo apt-get install build-essential
git clone https://github.com/tensorflow/tensorflow.git tensorflow_src
cd tensorflow_src && ./tensorflow/lite/tools/make/download_dependencies.sh
./tensorflow/lite/tools/make/build_rpi_lib.sh

I can not import,
import tflite_runtime.interpreter as tflite

Error I am getting is 
ModuleNotFoundError: No module named 'tflite_runtime'

In addition as per the instructions, I should see a static library. I do not see this either.
`tensorflow/lite/tools/make/gen/lib/rpi_armv6/libtensorflow-lite.a`

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached."
41111,Bug: tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor X with dtype Y and shape X ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS High Sierra
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): >= 2.0
- Python version: 3.6
- Running on: CPUs (But I guess it happens on GPUs as well)

From my experience, this type of bug appears very often (I have posted a similar bug before https://github.com/tensorflow/tensorflow/issues/40977). Here is a (dumb) example where I write a custom layer and pass a parameter via __init__() when I call that layer (Note that I turn off eager_execution). Running the following code results in the below error:

```
import tensorflow as tf
tf.compat.v1.disable_eager_execution()
from tensorflow.keras import backend as K
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np

class MyWordEmbedding(tf.keras.layers.Layer):
    def build(self, input_shape):
        self.kernel = self.add_weight(shape=(300, 512), dtype='float32')
        super(MyWordEmbedding, self).build(input_shape)  # Be sure to call this at the end
    
    def call(self, inputs):
        return tf.nn.embedding_lookup(params=self.kernel, ids=inputs[0])

class EncoderLayer(tf.keras.layers.Layer):
    def __init__(self, mask_para, **kwargs):
        self.mask_para = mask_para
        super(EncoderLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        self.Qdense = self.add_weight(name='Qdense', shape=(512, 512))
        super(EncoderLayer, self).build(input_shape)

    def call(self, x):
        Qoutput = tf.einsum('aij,jk->aik', x[0], self.Qdense)
        Koutput =  tf.einsum('aij,jk->aik', x[0], self.Qdense)
        Voutput =  tf.einsum('aij,jk->aik', x[0], self.Qdense)
        a = tf.einsum('ajk,afk->ajf', Qoutput, Koutput) * tf.tile(K.expand_dims(self.mask_para, axis=1), [1, 64, 1])
        a = tf.matmul(a, Voutput)
        return a

    def compute_mask(self, inputs, mask):
        return mask

    def compute_output_shape(self, input_shape):
        return input_shape[0]

def create_encoder_model():
    word_ids_fr = tf.keras.layers.Input(dtype='int32', shape=(None,))
    a = MyWordEmbedding()([word_ids_fr])
    a = EncoderLayer(K.cast(K.not_equal(0, word_ids_fr), dtype='float32'))([a])
    model = tf.keras.models.Model(inputs=[word_ids_fr], outputs=a)
    return model

def create_model():
    word_ids_en = tf.keras.layers.Input(dtype='int32', shape=(None,))
    a = tf.keras.Input(shape=(None, 512,))
    b = MyWordEmbedding()([word_ids_en])
    b = b + a
    model = tf.keras.models.Model(inputs=[word_ids_en, a], outputs=b)
    return model
    
def evaluate():
    source_sequence_ids = pad_sequences(np.random.randint(5, size=(3, 64)), maxlen=64, padding='pre')
    output = decoder_model.predict([pad_sequences(np.random.randint(5, size=(3, 64)), maxlen=64, padding='post'), encoder_model(source_sequence_ids, training=False)], steps=1, verbose=1, batch_size=256)

decoder_model = create_model()
encoder_model = create_encoder_model()
evaluate()
```
Error:
```
tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'input_3' with dtype int32 and shape [?,?]
	 [[{{node input_3}}]]
```

Fixing this error is easy for this case because I know exactly where (see the solution below). In detail I pass the para though call function and not the __init__ function. Nonetheless finding this error in a big project is very difficult because I need to throw many lines of code to know exactly where it causes the error.

```
import tensorflow as tf
tf.compat.v1.disable_eager_execution()
from tensorflow.keras import backend as K
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np

class MyWordEmbedding(tf.keras.layers.Layer):
    def build(self, input_shape):
        self.kernel = self.add_weight(shape=(300, 512), dtype='float32')
        super(MyWordEmbedding, self).build(input_shape)  # Be sure to call this at the end
    
    def call(self, inputs):
        return tf.nn.embedding_lookup(params=self.kernel, ids=inputs[0])

class EncoderLayer(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super(EncoderLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        self.Qdense = self.add_weight(name='Qdense', shape=(512, 512))
        super(EncoderLayer, self).build(input_shape)

    def call(self, x):
        Qoutput = tf.einsum('aij,jk->aik', x[0], self.Qdense)
        Koutput =  tf.einsum('aij,jk->aik', x[0], self.Qdense)
        Voutput =  tf.einsum('aij,jk->aik', x[0], self.Qdense)
        mask_para = x[1]
        a = tf.einsum('ajk,afk->ajf', Qoutput, Koutput) * tf.tile(K.expand_dims(mask_para, axis=1), [1, 64, 1])
        a = tf.matmul(a, Voutput)
        return a

    def compute_mask(self, inputs, mask):
        return mask

    def compute_output_shape(self, input_shape):
        return input_shape[0]

def create_encoder_model():
    word_ids_fr = tf.keras.layers.Input(dtype='int32', shape=(None,))
    a = MyWordEmbedding()([word_ids_fr])
    a = EncoderLayer()([a, K.cast(K.not_equal(0, word_ids_fr), dtype='float32')])
    model = tf.keras.models.Model(inputs=[word_ids_fr], outputs=a)
    return model

def create_model():
    word_ids_en = tf.keras.layers.Input(dtype='int32', shape=(None,))
    a = tf.keras.Input(shape=(None, 512,))
    b = MyWordEmbedding()([word_ids_en])
    b = b + a
    model = tf.keras.models.Model(inputs=[word_ids_en, a], outputs=b)
    return model
    
def evaluate():
    source_sequence_ids = pad_sequences(np.random.randint(5, size=(3, 64)), maxlen=64, padding='pre')
    output = decoder_model.predict([pad_sequences(np.random.randint(5, size=(3, 64)), maxlen=64, padding='post'), encoder_model(source_sequence_ids, training=False)], steps=1, verbose=1, batch_size=256)

decoder_model = create_model()
encoder_model = create_encoder_model()
evaluate()
```
"
41110,"Artistic Style Transfer Android Demo missing ""Style blending"" code","Hi,

I have two questions regarding ""Artistic Style Transfer with TensorFlow Lite""
https://www.tensorflow.org/lite/models/style_transfer/overview

**1) Style Blending Missing in Android**

In [Artistic Style Transfer with TensorFlow Lite](https://www.tensorflow.org/lite/models/style_transfer/overview) blog, there is complete section for ""[Style blending](https://www.tensorflow.org/lite/models/style_transfer/overview#style_blending)"" which says
> We can blend the style of content image into the stylized output, which in turn making the output look more like the content image. 
```
# Define content blending ratio between [0..1].
# 0.0: 0% style extracts from content image.
# 1.0: 100% style extracted from content image.
```

But in the [Android Demo](https://github.com/tensorflow/examples/tree/master/lite/examples/style_transfer/android) for the same, there is no option or code for ""Style Blending"".

Could you provide some code or guide me on how to achieve this in Android?

**2) Face Blending**

If I am doing artistic style transfer on any portrait images, the face looks too wrinkled. Is there any technique to make the face look better? Something like,
https://github.com/zfergus/face-preserving-style-transfer

"
