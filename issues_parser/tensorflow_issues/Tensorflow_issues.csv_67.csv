Issue Number,Issue Title,Issue Body
7402,Unable to build pip package from sources. Error: invalid command 'bdist_wheel' in pip_package/build_pip_package,"Hi, trying to build tensorflow for ppce64le got stuck with the issue. No CUDA (yet), trying to build binaries for CPU

```
$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
Fri Feb 10 01:47:35 CET 2017 : === Using tmpdir: /tmp/tmp.6Ayjop9NY1
~/dmitry/tf/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles ~/dmitry/tf/tensorflow
~/dmitry/tf/tensorflow
/tmp/tmp.6Ayjop9NY1 ~/dmitry/tf/tensorflow
Fri Feb 10 01:47:36 CET 2017 : === Building wheel
usage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]
   or: setup.py --help [cmd1 cmd2 ...]
   or: setup.py --help-commands
   or: setup.py cmd --help

error: invalid command 'bdist_wheel'
```

```
$ uname -a
Linux hostname.domainname 3.10.0-514.2.2.el7.ppc64le #1 SMP Wed Dec 7 17:03:53 GMT 2016 ppc64le ppc64le ppc64le GNU/Linux

$ git rev-parse HEAD
2180bb97de1fedfe249523bb8fcc2144d97fd00e

$ bazel version
Build label: 0.4.4- (@non-git)
Build target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Feb 9 23:48:24 2017 (1486684104)
Build timestamp: 1486684104
Build timestamp as int: 1486684104

```
"
7399,Tensorflow GPU on Windows Python 3.5 CUDA_ERROR_OUT_OF_MEMORY,"I am working on

    Notebook Lenova Z50-70
    GPU:Nvidia Geforce 840M
    Python 3.5
    Windows 7
    pip install tensorflow-gpu

I open windows and try to test how tensorflow use GPU.

    import tensorflow as tf
>>> I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA libr
ary cublas64_80.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA libr
ary cudnn64_5.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA libr
ary cufft64_80.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA libr
ary nvcuda.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA libr
ary curand64_80.dll locally

    from __future__ import print_function
    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)
    sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))

>>>I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:885] Found device 0 with p
roperties:
name: GeForce 840M
major: 5 minor: 0 memoryClockRate (GHz) 1.124
pciBusID 0000:03:00.0
Total memory: 4.00GiB
Free memory: 3.93GiB
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:906] DMA: 0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:916] 0: Y
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:975] Creating TensorFlow d
evice (/gpu:0) -> (device: 0, name: GeForce 840M, pci bus id: 0000:03:00.0)
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\cuda\cuda_driver.cc:1002] failed to allocate 1.3
3G (1430223872 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\cuda\cuda_driver.cc:1002] failed to allocate 1.2
0G (1287201536 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\cuda\cuda_driver.cc:1002] failed to allocate 1.0
8G (1158481408 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\cuda\cuda_driver.cc:1002] failed to allocate 994
.33M (1042633216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\cuda\cuda_driver.cc:1002] failed to allocate 894
.90M (938370048 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\cuda\cuda_driver.cc:1002] failed to allocate 805
.41M (844532992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\cuda\cuda_driver.cc:1002] failed to allocate 724
.87M (760079872 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\cuda\cuda_driver.cc:1002] failed to allocate 652
.38M (684071936 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\cuda\cuda_driver.cc:1002] failed to allocate 587
.14M (615664896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\cuda\cuda_driver.cc:1002] failed to allocate 528
.43M (554098432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\cuda\cuda_driver.cc:1002] failed to allocate 475
.59M (498688768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
>>> hello = tf.constant('Hello, TensorFlow!')
>>> print(sess.run(hello))

And got window error that it's not enought memory and OS closed python.
Yes also Gchrome was open and use a lot of system memory.
My question.
Why such error appear, why when i close GChrome system starts to works better, as i understand tensorflow gpu, use gpu memory and my Nvidia Geforce 840M has Dedicated Memory. I also have integrated video IntelHD.
Why tensorflow start to use system resources not gpu resources? and how to fix it? i don't have error in theano framework
"
7398,"Cuda 8.0 .deb file installation ""E: Unable to locate package cuda""","I am following the instructions from https://developer.nvidia.com/cuda-downloads for the Ubuntu 16.04 setup but the `cuda` package is not available after

```
sudo dpkg -i cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64.deb
sudo apt-get update
sudo apt-get install cuda
```

Here is my output:

```
stefan@stefan-pc:~/Downloads$ sudo dpkg -i cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64.deb
(Reading database ... 272242 files and directories currently installed.)
Preparing to unpack cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64.deb ...
Unpacking cuda-repo-ubuntu1604-8-0-local-ga2 (8.0.61-1) over (8.0.61-1) ...
Setting up cuda-repo-ubuntu1604-8-0-local-ga2 (8.0.61-1) ...
OK
stefan@stefan-pc:~/Downloads$ sudo apt-get update
Ign:1 http://dl.google.com/linux/chrome/deb stable InRelease
Hit:2 http://dl.google.com/linux/chrome/deb stable Release                 
Hit:3 https://deb.nodesource.com/node_6.x xenial InRelease                 
Get:4 http://security.ubuntu.com/ubuntu xenial-security InRelease [102 kB]
Hit:6 http://at.archive.ubuntu.com/ubuntu xenial InRelease                          
Hit:7 http://at.archive.ubuntu.com/ubuntu xenial-updates InRelease                  
Hit:8 http://at.archive.ubuntu.com/ubuntu xenial-backports InRelease               
Fetched 102 kB in 0s (115 kB/s)                                                    
Reading package lists... Done
stefan@stefan-pc:~/Downloads$ sudo apt-get install cuda
Reading package lists... Done
Building dependency tree       
Reading state information... Done
E: Unable to locate package cuda
```"
7397,`tf.dynamic_stitch` gradient is incorrect,"### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
``` python
import tensorflow as tf

x = tf.zeros((1, 3))
y = tf.dynamic_stitch([[0], [0]], [x, tf.ones((1, 3))])

with tf.Session() as sess:
    print(""y"")
    print(sess.run(y))

    analytic, numeric = tf.test.compute_gradient(x, (1, 3), y, (1, 3))
    print(""analytic"")
    print(analytic)
    print(""numeric"")
    print(numeric)
```

gives output
```
y
[[ 1.  1.  1.]]
analytic
[[ 1.  0.  0.]
 [ 0.  1.  0.]
 [ 0.  0.  1.]]
numeric
[[ 0.  0.  0.]
 [ 0.  0.  0.]
 [ 0.  0.  0.]]
```

The numeric gradient correctly shows that `x` has no impact on `y` (since the value of `x` is completely overwritten by a constant in the `dynamic_stitch`).  The analytic gradient is incorrect; it seems like the gradient calculation in `dynamic_stitch` does not handle the case where there are duplicate indices being merged.
"
7394,Segfault when calling TF_OperationGetAttrTensor on malformed tensor,"On 1.0.0-rc1 (and earlier versions), this segfaults:

```c
void dealloc(void* data, size_t len, void* arg) {
}

int main() {
  TF_Tensor* empty = TF_NewTensor(TF_FLOAT, NULL, 0, NULL, 0, dealloc, NULL);
  TF_Graph* graph = TF_NewGraph();
  TF_OperationDescription* desc = TF_NewOperation(graph, ""Const"", ""empty"");
  TF_Status* status = TF_NewStatus();
  TF_SetAttrTensor(desc, ""value"", empty, status);
  TF_SetAttrType(desc, ""dtype"", TF_FLOAT);
  TF_Operation* op = TF_FinishOperation(desc, status);
  TF_Tensor* value;
  TF_OperationGetAttrTensor(op, ""value"", &value, status); //Segfaults
  return 0;
}
```

Note that `TF_Message(status)` is `TF_OK` after the calls to `TF_SetAttrTensor` and `TF_FinishOperation`. "
7393,Have Docker Build not Install via Pip,"In the [Development Dockerfile](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile.devel), I see this note,

```
# TODO(craigcitro): Don't install the pip package, since it makes it
# more difficult to experiment with local changes. Instead, just add
# the built directory to the path.
```

Due to the [lack of important header files](https://github.com/tensorflow/tensorflow/issues/1419) in mainstream Tensorflow, I need to patch the TF build so I can get those headers. Unfortunately, the current Dockerfile clears the Bazel cache. I am requesting that the above TODO, be resolved soon, so I can patch Tensorflow's Bazel BUILD files in order to make local changes and build quicker. Rebuilding TF without a cache in the container takes a pretty long time."
7392,docs: Broken link/missing py file in Model Files guide,"On the page [""A Tool Developer's Guide to TensorFlow Model Files""](https://www.tensorflow.org/how_tos/tool_developers/)
([GitHub md link](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/tool_developers/index.md))

In the GraphDef section, there's a link to `graph_metrics.py`, pointing to the url https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/graph_metrics.py

This url does not exist. Perhaps it's intended to point at a different file in that folder? I don't see an obvious suitable replacement from scanning the filenames listed: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/tools


"
7391,Change `dim` to `axis` for neural network classification ops to match others for the TF 1.0.0 API?,"It looks like the neural network classification ops still use `dim` whereas the rest of the API is being moved to `axis` in v1.0.0. Is this purposeful, or was this bit just missed?"
7389,"Add unsorted segment ops for prod, min, mean, sqrt_n","As requested by @girving in #7362 I'll file a separate issue.

The sorted segment reduction ops feature  prod, min, mean, sqrt_n while the unsorted ops don't include those. So I suggest to add the following functions:
```
tf.unsorted_segment_prod(data, segment_ids, num_segments, name=None)
tf.unsorted_segment_min(data, segment_ids, num_segments, name=None)
tf.unsorted_segment_mean(data, segment_ids, num_segments, name=None)
tf.unsorted_segment_sqrt_n(data, segment_ids, num_segments, name=None
```

If you mark this as contributions welcome, I'd start working on this."
7388,error with bazel build in windows,"I'm trying to build Tensorflow android camera demo in Windows.
When I try to build the application with 

`bazel build -c opt //tensorflow/examples/android:tensorflow_demo`

I have these errors:

```
ERROR: missing input file '@androidsdk//:build-tools/25.0.1/aapt'.
ERROR: C:/Users/DAVIDE/AppData/Local/Temp/_bazel_davide.biraghi/55PhBU2g/external/androidsdk/BUILD:5:1: Executing genrule @androidsdk//:zipalign_runner failed: bazel failed: error executing command C:/ProgramData/chocolatey/lib/bazel -c ... (remaining 1 argument(s) skipped): java.io.IOException: CreateProcess(): Access is denied.
.
ERROR: C:/Users/DAVIDE/AppData/Local/Temp/_bazel_davidee/55PhBU2g/external/androidsdk/BUILD:5:1: @androidsdk//:aapt_binary: missing input file '@androidsdk//:build-tools/25.0.1/aapt'.
Target //tensorflow/examples/android:tensorflow_demo failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: C:/Users/DAVIDE/AppData/Local/Temp/_bazel_davide/55PhBU2g/external/androidsdk/BUILD:5:1 1 input file(s) do not exist.
```

I have android sdk installed with the 25.0.1 build-tools version and the aapt file. I have also modified my WORKSPACE with the correct path (and version. I tried also with version 24.0.1 with same error).  I'm working on Windows 10. Any suggestion? "
7385,[Tensorboard Request]Ignoring specific subdirectories,"I think this will be simple but very useful feature for Tensorboard.

What I want is the ignoring specific subdirectories.

Tensorboard load all of subdirectries and display them, but it become slower as more subfolders are added. So, I just want to ignore some directories without moving them.

There's several options to do this.
1. If there is special character(like #) in folder name, just ignore it.
2. Like git, .ignore file manage folders to ignore.
3. If .nolog file in the folder to ignore.(Like .nomedia file)

I think option 3 is best, because folder structure can be changed."
7384,freeze_graph.py script fails with FailedPreconditionError: 01,"I want to freeze a graph for the usage on a mobile device. The code I wrote was motivated by the freeze_graph_test.py script.

import modules
```
import os
import numpy as np
import tensorflow as tf

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from tensorflow.core.framework import graph_pb2
from tensorflow.core.protobuf import saver_pb2
from tensorflow.python.client import session

import imp
graph_io = imp.load_source('graph_io.py', 
'/export/home/oblum/bibs/tensorflow/tensorflow/python/framework/graph_io.py')   

from tensorflow.python.framework import ops
from tensorflow.python.framework import test_util
from tensorflow.python.ops import math_ops
from tensorflow.python.ops import variables
from tensorflow.python.platform import test
from tensorflow.python.tools import freeze_graph
from tensorflow.python.training import saver as saver_lib
```

set path and file names
```
checkpoint_prefix     = ""01/saved_checkpoint""
checkpoint_state_name = ""checkpoint_state""
input_graph_name      = ""input_graph.pb""
output_graph_name     = ""output_graph.pb""
checkpoint_path       = ""01""
```

define the graph, load weights and and save checkpoint
```
with ops.Graph().as_default():


    # load weights
    fname = ""alex_finetuned.npy""
    pretrained_net = np.load(fname).item()

    # save weights in dictionary
    weights = { 
            ""conv1"": tf.Variable(pretrained_net[""conv1""][0]),
            ""conv2"": tf.Variable(pretrained_net[""conv2""][0]),
            ""conv3"": tf.Variable(pretrained_net[""conv3""][0]),
            ""conv4"": tf.Variable(pretrained_net[""conv4""][0]),
            ""conv5"": tf.Variable(pretrained_net[""conv5""][0]),
            ""fc6"": tf.Variable(pretrained_net[""fc6""][0]),
            ""fc7"": tf.Variable(pretrained_net[""fc7""][0]),
            ""fc8"": tf.Variable(pretrained_net[""fc8""][0])
              }

    # save biases to dictionary
    biases = { 
            ""conv1"": tf.Variable(pretrained_net[""conv1""][1]),
            ""conv2"": tf.Variable(pretrained_net[""conv2""][1]),
            ""conv3"": tf.Variable(pretrained_net[""conv3""][1]),
            ""conv4"": tf.Variable(pretrained_net[""conv4""][1]),
            ""conv5"": tf.Variable(pretrained_net[""conv5""][1]),
            ""fc6"": tf.Variable(pretrained_net[""fc6""][1]),
            ""fc7"": tf.Variable(pretrained_net[""fc7""][1]),
            ""fc8"": tf.Variable(pretrained_net[""fc8""][1])
              }


    def conv(input, kernel, biases, k_h, k_w, c_o, s_h, s_w,  
             padding=""VALID"", group=1):
        '''
        From https://github.com/ethereon/caffe-tensorflow
        '''
        c_i = input.get_shape()[-1]
        assert c_i%group==0
        assert c_o%group==0
        convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], 
                                             padding=padding)


        if group==1:
            conv = convolve(input, kernel)
        else:
            input_groups = tf.split(3, group, input)
            kernel_groups = tf.split(3, group, kernel)
            output_groups = [convolve(i, k) for i,k in zip(input_groups, 
                                                           kernel_groups)]
            conv = tf.concat(3, output_groups)
        return  tf.reshape(tf.nn.bias_add(conv, biases), 
                           [-1]+conv.get_shape().as_list()[1:])


    # input
    #####################################################################
    x = tf.placeholder(tf.float32, (None, 227, 227, 3), name=""input"")
    #####################################################################

    #conv1
    k_h = 11; k_w = 11; c_o = 96; s_h = 4; s_w = 4
    conv1_in = conv(x, weights[""conv1""], biases[""conv1""], k_h, k_w, c_o, 
                    s_h, s_w, padding=""SAME"", group=1)
    conv1 = tf.nn.relu(conv1_in)

    #lrn1
    radius = 2; alpha = 2e-05; beta = 0.75; bias = 1.0
    lrn1 = tf.nn.local_response_normalization(conv1,
                                              depth_radius=radius,
                                              alpha=alpha,
                                              beta=beta,
                                              bias=bias)

    #maxpool1
    k_h = 3; k_w = 3; s_h = 2; s_w = 2; padding = 'VALID'
    maxpool1 = tf.nn.max_pool(lrn1, ksize=[1, k_h, k_w, 1], 
                              strides=[1, s_h, s_w, 1], padding=padding)


    #conv2
    k_h = 5; k_w = 5; c_o = 256; s_h = 1; s_w = 1; group = 2
    conv2_in = conv(maxpool1, weights[""conv2""], biases[""conv2""], k_h, k_w, 
                    c_o, s_h, s_w, padding=""SAME"", group=group)
    conv2 = tf.nn.relu(conv2_in)


    #lrn2
    radius = 2; alpha = 2e-05; beta = 0.75; bias = 1.0
    lrn2 = tf.nn.local_response_normalization(conv2,
                                              depth_radius=radius,
                                              alpha=alpha,
                                              beta=beta,
                                              bias=bias)

    #maxpool2
    k_h = 3; k_w = 3; s_h = 2; s_w = 2; padding = 'VALID'
    maxpool2 = tf.nn.max_pool(lrn2, ksize=[1, k_h, k_w, 1], 
                              strides=[1, s_h, s_w, 1], padding=padding)

    #conv3
    k_h = 3; k_w = 3; c_o = 384; s_h = 1; s_w = 1; group = 1
    conv3_in = conv(maxpool2, weights[""conv3""], biases[""conv3""], k_h, k_w, 
                    c_o, s_h, s_w, padding=""SAME"", group=group)
    conv3 = tf.nn.relu(conv3_in)

    #conv4
    k_h = 3; k_w = 3; c_o = 384; s_h = 1; s_w = 1; group = 2
    conv4_in = conv(conv3, weights[""conv4""], biases[""conv4""], k_h, k_w, 
                    c_o, s_h, s_w, padding=""SAME"", group=group)
    conv4 = tf.nn.relu(conv4_in)


    #conv5
    k_h = 3; k_w = 3; c_o = 256; s_h = 1; s_w = 1; group = 2
    conv5_in = conv(conv4, weights[""conv5""], biases[""conv5""], k_h, k_w, 
                    c_o, s_h, s_w, padding=""SAME"", group=group)
    conv5 = tf.nn.relu(conv5_in)

    #maxpool5
    k_h = 3; k_w = 3; s_h = 2; s_w = 2; padding = 'VALID'
    maxpool5 = tf.nn.max_pool(conv5, ksize=[1, k_h, k_w, 1], 
                              strides=[1, s_h, s_w, 1], padding=padding)

    #fc6
    reshape = tf.reshape(maxpool5,
                         [-1, int(np.prod(maxpool5.get_shape()[1:]))]) 
    fc6 = tf.nn.relu(tf.matmul(reshape,  weights[""fc6""]) + biases[""fc6""])

    #fc7
    fc7 = tf.nn.relu(tf.matmul(fc6, weights[""fc7""]) + biases[""fc7""])

    #fc8
    fc8 = tf.nn.xw_plus_b(fc7, weights[""fc8""], biases[""fc8""])

    # output
    #####################################################################
    prob = tf.nn.softmax(fc8, name = ""output"")
    #####################################################################
        
    sess = session.Session()
    
    init = variables.global_variables_initializer()
    sess.run(init)
    
#     output = sess.run(prob)
    
#     self.assertNear(2.0, output, 0.00001)
    
    saver = saver_lib.Saver()
    checkpoint_path = saver.save(
      sess,
      checkpoint_prefix,
      global_step = 0,
      latest_filename = checkpoint_state_name)
    
    graph_io.write_graph(sess.graph, ""01"", input_graph_name)
    
```

set further paths for freezing the graph
```
input_graph_path     = input_graph_name
input_saver_def_path = ""01""
input_binary         = False
output_node_names    = ""output""
restore_op_name      = ""save/restore_all""
filename_tensor_name = ""save/Const:0""
output_graph_path    = output_graph_name
clear_devices        = False
initializer_nodes    = ""input""
```

freeze the graph
```
freeze_graph.freeze_graph(input_graph_path, input_saver_def_path,
                          input_binary, checkpoint_path, output_node_names,
                          restore_op_name, filename_tensor_name,
                          output_graph_path, clear_devices, initializer_nodes)
```

when I run the code I get an **FailedPreconditionError: 01** error in the last function freeze_graph.freeze_graph():
```
FailedPreconditionError                   Traceback (most recent call last)
<ipython-input-6-5251505a5d45> in <module>()
      2                           input_binary, checkpoint_path, output_node_names,
      3                           restore_op_name, filename_tensor_name,
----> 4                           output_graph_path, clear_devices, initializer_nodes)

/net/hciserver03/storage/oblum/bibs/venv_new/local/lib/python2.7/site-packages/tensorflow/python/tools/freeze_graph.pyc in freeze_graph(input_graph, input_saver, input_binary, input_checkpoint, output_node_names, restore_op_name, filename_tensor_name, output_graph, clear_devices, initializer_nodes)
    111           saver_def.ParseFromString(f.read())
    112         else:
--> 113           text_format.Merge(f.read(), saver_def)
    114         saver = tf.train.Saver(saver_def=saver_def)
    115         saver.restore(sess, input_checkpoint)

/net/hciserver03/storage/oblum/bibs/venv_new/local/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.pyc in read(self, n)
    110       else:
    111         length = n
--> 112       return pywrap_tensorflow.ReadFromStream(self._read_buf, length, status)
    113 
    114   def seek(self, position):

/usr/lib/python2.7/contextlib.pyc in __exit__(self, type, value, traceback)
     22         if type is None:
     23             try:
---> 24                 self.gen.next()
     25             except StopIteration:
     26                 return

/net/hciserver03/storage/oblum/bibs/venv_new/local/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.pyc in raise_exception_on_not_ok_status()
    467           None, None,
    468           compat.as_text(pywrap_tensorflow.TF_Message(status)),
--> 469           pywrap_tensorflow.TF_GetCode(status))
    470   finally:
    471     pywrap_tensorflow.TF_DeleteStatus(status)

FailedPreconditionError: 01
```

does anyone has suggestions how to resolve this?"
7383,use cuda7.5 on windows,I installed tensorflow-gpu with pip on windows. But it does not work with cuda7.5. And there is no way to update cuda due to driver limit. Is there any way to use cuda7.5 on windows with tensorflow-gpu? Or is there any tutorial about how to compile tensorflow with cuda7.5 on windows?  Thanks.
7382,[TensorBoard] load data from relative path,"To embed tensorBoard the data needs to be loaded from a relative directory, not /data."
7381,Tensorflow gemmlowp preformance,"I ran the tensorflow android demo on android with the google inception model. I used the quantized model transformed from tools/quantize_graph but  was sad to found that the gemmlowp works worse than float Eigen convolution on Android. Any suggestions to this , thanks a lot."
7380,[Discussion] Limit the exported symbols from _pywrap_tensorflow.so,"TensorFlow statically links many fundamental libraries and exported all their symbols, this can cause version conflict for other modules outside of TensorFlow which links against a different version library, see [the symbol lookup rules](http://stackoverflow.com/questions/12666248/elf-dynamic-loader-symbol-lookup-ordering).

For example, I have a script which involves TensorFlow and matplotlib. Just start a Python CLI and execute:
```
import tensorflow af tf
import matplotlib.pyplot as plt
# ...
plt.show()
```
The matplotlib may links to libpng symbols from _pywrap_tensorflow.so instead of libpng even though the only installed libpng is the correct one. And it will complains and fails:
```
libpng warning: Application was compiled with png.h from libpng-1.6.21
libpng warning: Application  is  running with png.c from libpng-1.2.53
libpng error: Incompatible libpng version in application and library
```
Similar to #1927 and #1924

Fortunately, we have a clear interface, the SWIG APIs. So we may consider only exporting these symbols, [here is the guide](https://www.gnu.org/software/gnulib/manual/html_node/Exported-Symbols-of-Shared-Libraries.html).

This also applies to other shared libraries like libtensorflow.so which export C/C++ interfaces."
7379,Compile error: use of deleted function,"### Environment info
Operating System: Rocks OS (CentOS 6.5)

Installed version of CUDA and cuDNN:  CUDA 8.0 and cuDNN 5.1

1. The commit hash (`git rev-parse HEAD`)：v1.0.0-rc2 1536a84f32f1fe77efd3fee6e5933a1dfe4e10bb
2. The output of `bazel version`：
Build label: 0.4.4- (@non-git)
Build target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Feb 8 08:15:45 2017 (1486541745)
Build timestamp: 1486541745
Build timestamp as int: 1486541745

### Logs or other output that would be helpful
![image](https://cloud.githubusercontent.com/assets/1501158/22772919/544cc3ee-eeda-11e6-9130-88c52db9206a.png)

I can successfully compile v0.12.1. However, i don't know how to solve this problem after switching to v1.0.0-rc2. This problem appear on master, v1.0.0-rc1,v1.0.0-rc2 branches. And all of these branches can be built on another Ubuntu 16.04 server.

Someone please help me."
7378,Seg fault when using tf session with opencv 3,"Hi, 

We noticed that when we try to use tensorflow with opencv 3, it consistently seg faults and crashes. The commands are:

```
import cv2
import numpy as np
import tensorflow as tf

with tf.Session() as sess:
    img = cv2.imread('messi5.jpg', 0)
    rows, cols = img.shape
    M = np.float32([[1, 0, 100], [0, 1, 50]])
    dst = cv2.warpAffine(img, M, (cols, rows))
    cv2.imshow('img', dst)
    cv2.waitKey(0)
    cv2.destroyAllWindows()
```

Assuming you have a messi5.jpg in the folder (https://raw.githubusercontent.com/abidrahmank/OpenCV2-Python-Tutorials/master/data/messi5.jpg)

We see this issue when we use tensorflow 0.12 GPU enabled, opencv 3.2.0 and python 2.7.6, CUDA 8.0 and CuDNN 5.1.5 . We did not observe this issue with opencv version 2.4.13 or 2.4.9.

We will also filed a bug report on opencv (https://github.com/opencv/opencv/issues/8155)

"
7376,Distributed mnist training in synchronous mode raises a ValueError in worker node,"I am trying to run distributed mnist training using the file given here: [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dist_test/python/mnist_replica.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dist_test/python/mnist_replica.py)

The async mode doesn't have any issues but I cannot get the sync mode to work. As soon as I start a worker in the sync mode I get a ValueError exception.

The command used to start the worker is:
```
python mnist_replica.py --ps_hosts=localhost:2222 --worker_hosts=localhost:2223 --task_index=0 --job_name=worker --sync_replicas=True
```

For python2 the Traceback looks something like this:
```
Traceback (most recent call last):
  File ""mnist_replica.py"", line 279, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 43, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""mnist_replica.py"", line 184, in main
    train_step = opt.minimize(cross_entropy, global_step=global_step)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py"", line 279, in minimize
    name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/sync_replicas_optimizer.py"", line 751, in apply_gradients
    array_ops.reshape(self._replica_id, (1,)),
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py"", line 2448, in reshape
    name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 503, in apply_op
    as_ref=input_arg.is_ref).dtype.name
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 669, in convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py"", line 176, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py"", line 165, in constant
    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py"", line 360, in make_tensor_proto
    raise ValueError(""None values not supported."")
ValueError: None values not supported.
```

For python3 it looks something like this:
```
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py"", line 490, in apply_op
    preferred_dtype=default_dtype)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 669, in convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/constant_op.py"", line 176, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/constant_op.py"", line 165, in constant
    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_util.py"", line 360, in make_tensor_proto
    raise ValueError(""None values not supported."")
ValueError: None values not supported.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""mnist_replica.py"", line 279, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py"", line 43, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""mnist_replica.py"", line 184, in main
    train_step = opt.minimize(cross_entropy, global_step=global_step)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py"", line 279, in minimize
    name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/sync_replicas_optimizer.py"", line 751, in apply_gradients
    array_ops.reshape(self._replica_id, (1,)),
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py"", line 2448, in reshape
    name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py"", line 503, in apply_op
    as_ref=input_arg.is_ref).dtype.name
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 669, in convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/constant_op.py"", line 176, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/constant_op.py"", line 165, in constant
    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_util.py"", line 360, in make_tensor_proto
    raise ValueError(""None values not supported."")
ValueError: None values not supported.
```

I have tried running the example on Python2 and Python3 on Ubuntu 14.04 and Ubuntu 16.04 in cpu only mode. In all the 4 cases the version of tensorflow used was 0.12.0.

This issue is very similar to the one here: [https://github.com/tensorflow/tensorflow/issues/6687](https://github.com/tensorflow/tensorflow/issues/6687) except that I'm using v0.12.0 and running in CPU only mode. That issue mentions that the example was running in 0.12.0 but that is not the case for me.
"
7375,tensorboard show empty page (master),"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

[#5341](https://github.com/tensorflow/tensorflow/issues/5341)


### Environment info
Ubuntu 14.04 LTS. 
git clone https://github.com/tensorflow/tensorflow
./configure
bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
pip install /tmp/tensorflow_pkg/tensorflow-1.0.0rc1-cp35-cp35m-linux_x86_64.whl 

The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
1.0.0-rc1

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
bd4a1c58d28734c77ecb17d719e72c45c6c33077
2. The output of `bazel version`
Build label: 0.4.4

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
I use a tensorboard example that I think is quite easy to reproduce the issue. 
```shell
git clone https://github.com/normanheckscher/mnist-tensorboard-embeddings
cd mnist-tensorboard-embeddings/

# modify the path at line 97 and 99 in mnist_t-sne.py
python mnist_t-sne.py
# start tensorboard
tensorboard --logdir=/home/chih-yao/Documents/mnist-tensorboard-embeddings/logs/
```
### What other attempted solutions have you tried?
- [x] Make sure my `tensorboard --logdir=/home/chih-yao/Documents/mnist-tensorboard-embeddings/logs/` path is correct
- [x] Reinstall tensorflow from source
- [x] Make sure the metadata and event files exist in the `logdir`

### Logs or other output that would be helpful
```
tensorboard --logdir=/home/chih-yao/Documents/mnist-tensorboard-embeddings-master/logs
Starting TensorBoard b'46' on port 6006
(You can navigate to http://127.0.1.1:6006)
 * Running on http://0.0.0.0:6006/ (Press CTRL+C to quit)
127.0.0.1 - - [08/Feb/2017 21:06:07] ""GET /webcomponentsjs/webcomponents-lite.min.js HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /lib/css/global.css HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /plottable/plottable.css HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /dist/bazel-html-imports.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /dist/tf-tensorboard.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /polymer/polymer.html HTTP/1.1"" 404 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /numericjs_numeric_min_js/file/numeric.min.js HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /iron-icons/iron-icons.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-tabs/paper-tabs.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-checkbox/paper-checkbox.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-toolbar/paper-toolbar.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-dialog/paper-dialog.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-button/paper-button.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-icon-button/paper-icon-button.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-header-panel/paper-header-panel.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /iron-icon/iron-icon.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /iron-iconset-svg/iron-iconset-svg.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /three_js_three_min_js/file/three.min.js HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /iron-resizable-behavior/iron-resizable-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /iron-flex-layout/iron-flex-layout.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /iron-menu-behavior/iron-menubar-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-tabs/paper-tabs-icons.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-styles/color.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-styles/default-theme.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-tabs/paper-tab.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-behaviors/paper-checked-element-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-styles/typography.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /neon-animation/neon-animation-runner-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-dialog-behavior/paper-dialog-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-dialog-behavior/paper-dialog-shared-styles.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-material/paper-material.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-ripple/paper-ripple.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-behaviors/paper-button-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-behaviors/paper-inky-focus-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /iron-meta/iron-meta.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /iron-menu-behavior/iron-menu-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /iron-behaviors/iron-control-state.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /iron-behaviors/iron-button-state.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /three_js_orbitcontrols_js/file/OrbitControls.js HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-behaviors/paper-ripple-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /font-roboto/roboto.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /neon-animation/neon-animatable-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /iron-checked-element-behavior/iron-checked-element-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /iron-overlay-behavior/iron-overlay-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-styles/shadow.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-material/paper-material-shared-styles.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /iron-a11y-keys-behavior/iron-a11y-keys-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /iron-selector/iron-multi-selectable.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /weblas_weblas_js/file/weblas.js HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /neon-animation/animations/opaque-animation.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /iron-validatable-behavior/iron-validatable-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /iron-form-element-behavior/iron-form-element-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /iron-fit-behavior/iron-fit-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /iron-overlay-behavior/iron-overlay-manager.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /iron-overlay-behavior/iron-focusables-helper.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /iron-selector/iron-selectable.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /neon-animation/neon-animation-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /iron-overlay-behavior/iron-overlay-backdrop.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /neon-animation/web-animations.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /iron-selector/iron-selection.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /web-animations-js/web-animations-next-lite.min.js HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-input/paper-input.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-slider/paper-slider.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /lodash/lodash.min.js HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-input/paper-input-error.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /iron-input/iron-input.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-input/paper-input-char-counter.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-input/paper-input-container.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-input/paper-input-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /iron-range-behavior/iron-range-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-progress/paper-progress.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /iron-a11y-announcer/iron-a11y-announcer.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-input/paper-input-addon-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /d3/d3.js HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-styles/paper-styles.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /iron-flex-layout/classes/iron-flex-layout.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /iron-flex-layout/classes/iron-shadow-flex-layout.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-item/paper-item.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-menu/paper-menu.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-dropdown-menu/paper-dropdown-menu.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-item/paper-item-shared-styles.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-item/paper-item-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-menu-button/paper-menu-button.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-dropdown-menu/paper-dropdown-menu-icons.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-menu/paper-menu-shared-styles.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-dropdown-menu/paper-dropdown-menu-shared-styles.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /neon-animation/animations/fade-in-animation.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /neon-animation/animations/fade-out-animation.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /iron-dropdown/iron-dropdown.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-menu-button/paper-menu-button-animations.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /iron-dropdown/iron-dropdown-scroll-manager.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /iron-collapse/iron-collapse.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /plottable/plottable.js HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-toggle-button/paper-toggle-button.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /graphlib/dist/graphlib.core.js HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /dagre/dist/dagre.core.js HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-item/all-imports.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /iron-list/iron-list.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-item/paper-item-body.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-item/paper-icon-item.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /iron-scroll-target-behavior/iron-scroll-target-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-radio-group/paper-radio-group.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-tooltip/paper-tooltip.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-radio-button/paper-radio-button.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-listbox/paper-listbox.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-toast/paper-toast.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /iron-icons/image-icons.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-spinner/paper-spinner-lite.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-dialog-scrollable/paper-dialog-scrollable.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-input/paper-textarea.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-spinner/paper-spinner-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /paper-spinner/paper-spinner-styles.html HTTP/1.1"" 200 -
127.0.0.1 - - [08/Feb/2017 21:06:08] ""GET /iron-autogrow-textarea/iron-autogrow-textarea.html HTTP/1.1"" 200 -
```"
7374,Issues with Eigen when building tf_tutorials_example_trainer on Windows 10,"I'm attempting to build Tensorflow C++ on Windows 10 following the instructions of the cmake readme. However, I'm getting issues with compiling at the Eigen step.

```Compiling the Fortran compiler identification source file ""CMakeFortranCompilerId.F"" failed.
Compiler:  
Build flags: 
Id flags: 

The output was:
1

Microsoft Visual Studio 2015 Version 14.0.25420.1.
Copyright (C) Microsoft Corp. All rights reserved.

The license for Visual Studio has expired.

The evaluation period for this product has ended.

Performing C++ SOURCE FILE Test COMPILER_SUPPORT_FASTMATH failed with the following output:
Change Dir: C:/Users/Irene/Documents/tensorflow/tensorflow/contrib/cmake/build/eigen/src/eigen-build/CMakeFiles/CMakeTmp

Run Build Command:""C:/Program Files (x86)/MSBuild/14.0/bin/MSBuild.exe"" ""cmTC_70ffc.vcxproj"" ""/p:Configuration=Debug"" ""/p:VisualStudioVersion=14.0""
Microsoft (R) Build Engine version 14.0.25420.1

Copyright (C) Microsoft Corporation. All rights reserved.



Build started 9/02/2017 3:13:30 PM.

Project ""C:\Users\Irene\Documents\tensorflow\tensorflow\contrib\cmake\build\eigen\src\eigen-build\CMakeFiles\CMakeTmp\cmTC_70ffc.vcxproj"" on node 1 (default targets).

PrepareForBuild:

  Creating directory ""cmTC_70ffc.dir\Debug\"".

  Creating directory ""C:\Users\Irene\Documents\tensorflow\tensorflow\contrib\cmake\build\eigen\src\eigen-build\CMakeFiles\CMakeTmp\Debug\"".

  Creating directory ""cmTC_70ffc.dir\Debug\cmTC_70ffc.tlog\"".

InitializeBuildStatus:

  Creating ""cmTC_70ffc.dir\Debug\cmTC_70ffc.tlog\unsuccessfulbuild"" because ""AlwaysCreate"" was specified.

ClCompile:

  C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\bin\x86_amd64\CL.exe /c /Zi /W4 /WX- /Od /Ob0 /D WIN32 /D _WINDOWS /D _CRT_SECURE_NO_WARNINGS /D _SCL_SECURE_NO_WARNINGS /D COMPILER_SUPPORT_FASTMATH /D _DEBUG /D ""CMAKE_INTDIR=\""Debug\"""" /D _MBCS /Gm- /EHsc /RTC1 /MDd /GS /fp:precise /Zc:wchar_t /Zc:forScope /Zc:inline /GR /Fo""cmTC_70ffc.dir\Debug\\"" /Fd""cmTC_70ffc.dir\Debug\vc140.pdb"" /Gd /TP /wd4127 /wd4505 /wd4714 /errorReport:queue  -ffast-math ""C:\Users\Irene\Documents\tensorflow\tensorflow\contrib\cmake\build\eigen\src\eigen-build\CMakeFiles\CMakeTmp\src.cxx""

  Microsoft (R) C/C++ Optimizing Compiler Version 19.00.24215.1 for x64

  Copyright (C) Microsoft Corporation.  All rights reserved.

  

  cl /c /Zi /W4 /WX- /Od /Ob0 /D WIN32 /D _WINDOWS /D _CRT_SECURE_NO_WARNINGS /D _SCL_SECURE_NO_WARNINGS /D COMPILER_SUPPORT_FASTMATH /D _DEBUG /D ""CMAKE_INTDIR=\""Debug\"""" /D _MBCS /Gm- /EHsc /RTC1 /MDd /GS /fp:precise /Zc:wchar_t /Zc:forScope /Zc:inline /GR /Fo""cmTC_70ffc.dir\Debug\\"" /Fd""cmTC_70ffc.dir\Debug\vc140.pdb"" /Gd /TP /wd4127 /wd4505 /wd4714 /errorReport:queue  -ffast-math ""C:\Users\Irene\Documents\tensorflow\tensorflow\contrib\cmake\build\eigen\src\eigen-build\CMakeFiles\CMakeTmp\src.cxx""

  

cl : Command line warning D9002: ignoring unknown option '-ffast-math' [C:\Users\Irene\Documents\tensorflow\tensorflow\contrib\cmake\build\eigen\src\eigen-build\CMakeFiles\CMakeTmp\cmTC_70ffc.vcxproj]

  src.cxx

Link:

  C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\bin\x86_amd64\link.exe /ERRORREPORT:QUEUE /OUT:""C:\Users\Irene\Documents\tensorflow\tensorflow\contrib\cmake\build\eigen\src\eigen-build\CMakeFiles\CMakeTmp\Debug\cmTC_70ffc.exe"" /INCREMENTAL /NOLOGO kernel32.lib user32.lib gdi32.lib winspool.lib shell32.lib ole32.lib oleaut32.lib uuid.lib comdlg32.lib advapi32.lib /MANIFEST /MANIFESTUAC:""level='asInvoker' uiAccess='false'"" /manifest:embed /DEBUG /PDB:""C:/Users/8i/Documents/tensorflow/tensorflow/contrib/cmake/build/eigen/src/eigen-build/CMakeFiles/CMakeTmp/Debug/cmTC_70ffc.pdb"" /SUBSYSTEM:CONSOLE /TLBID:1 /DYNAMICBASE /NXCOMPAT /IMPLIB:""C:/Users/8i/Documents/tensorflow/tensorflow/contrib/cmake/build/eigen/src/eigen-build/CMakeFiles/CMakeTmp/Debug/cmTC_70ffc.lib"" /MACHINE:X64  /machine:x64 cmTC_70ffc.dir\Debug\src.obj

  cmTC_70ffc.vcxproj -> C:\Users\Irene\Documents\tensorflow\tensorflow\contrib\cmake\build\eigen\src\eigen-build\CMakeFiles\CMakeTmp\Debug\cmTC_70ffc.exe

FinalizeBuildStatus:

  Deleting file ""cmTC_70ffc.dir\Debug\cmTC_70ffc.tlog\unsuccessfulbuild"".

  Touching ""cmTC_70ffc.dir\Debug\cmTC_70ffc.tlog\cmTC_70ffc.lastbuildstate"".

Done Building Project ""C:\Users\Irene\Documents\tensorflow\tensorflow\contrib\cmake\build\eigen\src\eigen-build\CMakeFiles\CMakeTmp\cmTC_70ffc.vcxproj"" (default targets).



Build succeeded.



""C:\Users\Irene\Documents\tensorflow\tensorflow\contrib\cmake\build\eigen\src\eigen-build\CMakeFiles\CMakeTmp\cmTC_70ffc.vcxproj"" (default target) (1) ->

(ClCompile target) -> 

  cl : Command line warning D9002: ignoring unknown option '-ffast-math' [C:\Users\Irene\Documents\tensorflow\tensorflow\contrib\cmake\build\eigen\src\eigen-build\CMakeFiles\CMakeTmp\cmTC_70ffc.vcxproj]



    1 Warning(s)

    0 Error(s)



Time Elapsed 00:00:00.54


Source file was:
int main() { return 0; }
Determining if the include file pthread.h exists failed with the following output:
Change Dir: C:/Users/Irene/Documents/tensorflow/tensorflow/contrib/cmake/build/eigen/src/eigen-build/CMakeFiles/CMakeTmp

Run Build Command:""C:/Program Files (x86)/MSBuild/14.0/bin/MSBuild.exe"" ""cmTC_55681.vcxproj"" ""/p:Configuration=Debug"" ""/p:VisualStudioVersion=14.0""
Microsoft (R) Build Engine version 14.0.25420.1

Copyright (C) Microsoft Corporation. All rights reserved.



Build started 9/02/2017 3:13:37 PM.

Project ""C:\Users\Irene\Documents\tensorflow\tensorflow\contrib\cmake\build\eigen\src\eigen-build\CMakeFiles\CMakeTmp\cmTC_55681.vcxproj"" on node 1 (default targets).

PrepareForBuild:

  Creating directory ""cmTC_55681.dir\Debug\"".

  Creating directory ""C:\Users\Irene\Documents\tensorflow\tensorflow\contrib\cmake\build\eigen\src\eigen-build\CMakeFiles\CMakeTmp\Debug\"".

  Creating directory ""cmTC_55681.dir\Debug\cmTC_55681.tlog\"".

InitializeBuildStatus:

  Creating ""cmTC_55681.dir\Debug\cmTC_55681.tlog\unsuccessfulbuild"" because ""AlwaysCreate"" was specified.

ClCompile:

  C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\bin\x86_amd64\CL.exe /c /Zi /W3 /WX- /Od /Ob0 /D WIN32 /D _WINDOWS /D _DEBUG /D ""CMAKE_INTDIR=\""Debug\"""" /D _MBCS /Gm- /RTC1 /MDd /GS /fp:precise /Zc:wchar_t /Zc:forScope /Zc:inline /Fo""cmTC_55681.dir\Debug\\"" /Fd""cmTC_55681.dir\Debug\vc140.pdb"" /Gd /TC /errorReport:queue ""C:\Users\Irene\Documents\tensorflow\tensorflow\contrib\cmake\build\eigen\src\eigen-build\CMakeFiles\CMakeTmp\CheckIncludeFile.c""

  Microsoft (R) C/C++ Optimizing Compiler Version 19.00.24215.1 for x64

  Copyright (C) Microsoft Corporation.  All rights reserved.

  

  cl /c /Zi /W3 /WX- /Od /Ob0 /D WIN32 /D _WINDOWS /D _DEBUG /D ""CMAKE_INTDIR=\""Debug\"""" /D _MBCS /Gm- /RTC1 /MDd /GS /fp:precise /Zc:wchar_t /Zc:forScope /Zc:inline /Fo""cmTC_55681.dir\Debug\\"" /Fd""cmTC_55681.dir\Debug\vc140.pdb"" /Gd /TC /errorReport:queue ""C:\Users\Irene\Documents\tensorflow\tensorflow\contrib\cmake\build\eigen\src\eigen-build\CMakeFiles\CMakeTmp\CheckIncludeFile.c""

  

  CheckIncludeFile.c

C:\Users\Irene\Documents\tensorflow\tensorflow\contrib\cmake\build\eigen\src\eigen-build\CMakeFiles\CMakeTmp\CheckIncludeFile.c(1): fatal error C1083: Cannot open include file: 'pthread.h': No such file or directory [C:\Users\Irene\Documents\tensorflow\tensorflow\contrib\cmake\build\eigen\src\eigen-build\CMakeFiles\CMakeTmp\cmTC_55681.vcxproj]

Done Building Project ""C:\Users\Irene\Documents\tensorflow\tensorflow\contrib\cmake\build\eigen\src\eigen-build\CMakeFiles\CMakeTmp\cmTC_55681.vcxproj"" (default targets) -- FAILED.



Build FAILED.



""C:\Users\Irene\Documents\tensorflow\tensorflow\contrib\cmake\build\eigen\src\eigen-build\CMakeFiles\CMakeTmp\cmTC_55681.vcxproj"" (default target) (1) ->

(ClCompile target) -> 

  C:\Users\Irene\Documents\tensorflow\tensorflow\contrib\cmake\build\eigen\src\eigen-build\CMakeFiles\CMakeTmp\CheckIncludeFile.c(1): fatal error C1083: Cannot open include file: 'pthread.h': No such file or directory [C:\Users\Irene\Documents\tensorflow\tensorflow\contrib\cmake\build\eigen\src\eigen-build\CMakeFiles\CMakeTmp\cmTC_55681.vcxproj]



    0 Warning(s)

    1 Error(s)



Time Elapsed 00:00:00.24
```

"
7373,there is a error when i import tensorflow in python script,"after i finish 'Configure the installation' step without finishing 'create the pip package and install'step.
there is a error when i import tensorflow in python script.
i type 'import tensorflow' in my python script and run it.i get this error:
`Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 60, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
ImportError: libcudart.so.8.0: cannot open shared object file: No such file or directory


Error importing tensorflow.  Unless you are using bazel,
you should not try to import tensorflow from its source directory;
please exit the tensorflow source tree, and relaunch your python interpreter
from there.
`
i install the latest version tensorflow from sourse on ubuntu15.04 with cuda7.5 and cudnn5. in 'Configure the installation' step,i set cuda version 7.5."
7372,`Estimator.evaluate` froze,"### Environment info
Operating System:

`Linux 3.13.0-107-generic #154-Ubuntu SMP Tue Dec 20 09:57:27 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux`

Installed version of CUDA and cuDNN: 

`None`

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)

`a7338d74be3b07968dd2c2a94167db7b9ae1a9f8`

2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

I implement a customized function for an Estimator class and run fit and evaluate with the Estimator model. The training works fine but it will stops at `evaluate` with around 220% CPU usage.

The following is a script to reproduce the similar problem. If you comment the `evaluate` function, this example script works fine, otherwise it will freeze at that part. 

```python
import tensorflow as tf
from tensorflow.contrib.layers.python.layers.optimizers import optimize_loss
from tensorflow.contrib.learn.python.learn.estimators import model_fn
from tensorflow.contrib.learn.python.learn.estimators.estimator import Estimator
from tensorflow.python import debug as tf_debug
from tensorflow.python.framework import ops


def main(_):
    hooks = [tf_debug.LocalCLIDebugHook()]

    def func(features, targets, mode, params):
        idx = tf.concat([features['a'], features['b']], axis=1)

        embedding = tf.get_variable(""embed"", [10, 20], dtype=tf.float32)

        pred = tf.reduce_sum(tf.nn.embedding_lookup(embedding, idx))

        train_op = optimize_loss(loss=pred,
                                 global_step=tf.train.get_global_step(),
                                 learning_rate=0.001,
                                 optimizer='Adam',
                                 variables=tf.trainable_variables(),
                                 name=""training_loss_optimizer"")

        eval_metric_dict = dict()
        eval_metric_dict['metric'] = pred

        return model_fn.ModelFnOps(mode=mode,
                                   predictions=pred,
                                   loss=pred,
                                   train_op=train_op,
                                   eval_metric_ops=eval_metric_dict)

    model = Estimator(func, params={})

    model.fit(
        input_fn=lambda: (
            {'a': ops.convert_to_tensor([[1, 2, 3, 4, 5]]), 'b': ops.convert_to_tensor([[2, 3, 4, 3, 5]])},
            None), max_steps=10, monitors=hooks)
    model.evaluate(
         input_fn=lambda: (
             {'a': ops.convert_to_tensor([[1, 2, 3, 4, 5]]), 'b': ops.convert_to_tensor([[2, 3, 4, 3, 5]])},
             None))


if __name__ == ""__main__"":
    tf.app.run()

```

### Logs or other output that would be helpful

If I press `ctrl + c`, the stack info are:

```
WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp5jemyudd


1q^CTraceback (most recent call last):
  File ""estimator_test.py"", line 48, in <module>
    tf.app.run()
  File ""/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""estimator_test.py"", line 42, in main
    input_fn=lambda: (
  File ""/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/util/deprecation.py"", line 281, in new_func
    return func(*args, **kwargs)
  File ""/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 507, in evaluate
    log_progress=log_progress)
  File ""/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 825, in _evaluate_model
    config=config_pb2.ConfigProto(allow_soft_placement=True))
  File ""/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/contrib/training/python/training/evaluation.py"", line 442, in evaluate_once
    session.run(eval_ops, feed_dict)
  File ""/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/training/monitored_session.py"", line 469, in run
    run_metadata=run_metadata)
  File ""/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/training/monitored_session.py"", line 793, in run
    run_metadata=run_metadata)
  File ""/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/training/monitored_session.py"", line 751, in run
    return self._sess.run(*args, **kwargs)
  File ""/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/training/monitored_session.py"", line 898, in run
    run_metadata=run_metadata)
  File ""/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/training/monitored_session.py"", line 751, in run
    return self._sess.run(*args, **kwargs)
  File ""/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/client/session.py"", line 767, in run
    run_metadata_ptr)
  File ""/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/client/session.py"", line 965, in _run
    feed_dict_string, options, run_metadata)
  File ""/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/client/session.py"", line 1015, in _do_run
    target_list, options, run_metadata)
  File ""/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/client/session.py"", line 1022, in _do_call
    return fn(*args)
  File ""/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/client/session.py"", line 1004, in _run_fn
    status, run_metadata)
KeyboardInterrupt
^C⏎
```
"
7370,"number of class mismatch between .py file and .ckpt, which makes wrong prediction.","the total class which is assigned num_classes=1000 in the file below.

https://github.com/tensorflow/models/blob/master/slim/nets/inception_v2.py

however, the published checkpoint file on the page below  for inception v2 is 1001.

https://github.com/tensorflow/models/tree/master/slim

The prediction use this check point file is wrong, even if we update num_classes=1001 in the code. the wrong prediction is not shift 1 position, but completely mess.


"
7368,Feature Request: Official Docker base image with python 3.5 (and not only 3.4),"I was building my Dockerfile for my docker image and when I ran containers, I noticed that python 3.4 was the one that was used. I installed python 3.5 successfully on the image but I was still unable to use tensorflow. Is it possible to request a base image that is always with the latest version of TensorFlow and also uses other version of pyhton 3? Like 3.5.

I am aware I can just pip install it directly, but that doesn't keep it up to date with the latest. A base image with python 3.5 seems the most appropriate answer.

### What related GitHub issues or StackOverflow threads have you found by searching the web for 
Related Stack Over Flow: http://stackoverflow.com/questions/42122826/can-one-use-python-3-5-in-a-docker-container-based-out-of-the-tensorflow-docker

### Environment info
Ubuntu/Linux and Mac OS X. 

### What other attempted solutions have you tried?
You can install python 3.5 here:

http://askubuntu.com/questions/682869/how-do-i-install-newer-python-versions-using-apt-get

and then fix pip and numpy with:

http://stackoverflow.com/questions/42122639/how-does-one-install-fix-a-failed-numpy-installation-that-works-on-python-3-4-bu/42124828?noredirect=1#comment71418540_42124828

and then one can directly install python 3.5 with pip:

export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.1-cp35-cp35m-linux_x86_64.whl
python3.5 -m pip install TF_BINARY_URL

(note pip3 will **not** work if all this is done in the docker container).

This should work but doesn't start from an official base image using python 3.5 and doesn't get the latest version of TensorFlow automatically.
"
7367,TF Slim batch_norm does not expose beta_regularizer or gamma_regularizer,"[`batch_normalization`](https://github.com/tensorflow/tensorflow/blob/b00fc538638f87ac45be9105057b9865f0f9418b/tensorflow/python/layers/normalization.py#L255-L256) takes in a `beta_regularizer` and `gamma_regularizer` but the [TF slim `batch_norm` layer does not expose this](https://github.com/tensorflow/tensorflow/blob/bd4a1c58d28734c77ecb17d719e72c45c6c33077/tensorflow/contrib/layers/python/layers/layers.py#L362-L380).

Compare this to other TF slim layers, such as `convolution` which do expose the regularizers: https://github.com/tensorflow/tensorflow/blob/bd4a1c58d28734c77ecb17d719e72c45c6c33077/tensorflow/contrib/layers/python/layers/layers.py#L789-L791 "
7364,Can't build MacOS GPU pip because odf nccl_archive errors,"Looks like Google CI build is having the same issue:
https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-mac-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-mac/lastFailedBuild/console

```
90 errors detected in the compilation of ""/var/folders/9l/c8y8z62s0kjgnpgx6jwh0g9r0000gn/T//tmpxft_0000aebc_00000000-7_reduce_scatter.cu.cpp1.ii"".
ERROR: /private/var/tmp/_bazel_yaroslav/8430f3ac1504aea2a8d4e6b016af31c5/external/nccl_archive/BUILD.bazel:33:1: output 'external/nccl_archive/_objs/nccl/external/nccl_archive/src/all_reduce.cu.pic.o' was not created.
ERROR: /private/var/tmp/_bazel_yaroslav/8430f3ac1504aea2a8d4e6b016af31c5/external/nccl_archive/BUILD.bazel:33:1: output 'external/nccl_archive/_objs/nccl/external/nccl_archive/src/reduce.cu.pic.o' was not created.
ERROR: /private/var/tmp/_bazel_yaroslav/8430f3ac1504aea2a8d4e6b016af31c5/external/nccl_archive/BUILD.bazel:33:1: Couldn't build file external/nccl_archive/_objs/nccl/external/nccl_archive/src/all_reduce.cu.pic.o: not all outputs were created or valid.
ERROR: /private/var/tmp/_bazel_yaroslav/8430f3ac1504aea2a8d4e6b016af31c5/external/nccl_archive/BUILD.bazel:33:1: Couldn't build file external/nccl_archive/_objs/nccl/external/nccl_archive/src/reduce.cu.pic.o: not all outputs were created or valid.
ERROR: /private/var/tmp/_bazel_yaroslav/8430f3ac1504aea2a8d4e6b016af31c5/external/nccl_archive/BUILD.bazel:33:1: output 'external/nccl_archive/_objs/nccl/external/nccl_archive/src/reduce_scatter.cu.pic.o' was not created.
ERROR: /private/var/tmp/_bazel_yaroslav/8430f3ac1504aea2a8d4e6b016af31c5/external/nccl_archive/BUILD.bazel:33:1: Couldn't build file external/nccl_archive/_objs/nccl/external/nccl_archive/src/reduce_scatter.cu.pic.o: not all outputs were created or valid.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 2.220s, Critical Path: 1.47s
```"
7363,tf.decode_csv() seems to read the second field which actually doesn't exist.,"### Environment info
Operating System:
```
$ lsb_release -a
No LSB modules are available.
Distributor ID:	Ubuntu
Description:	Ubuntu 14.04.5 LTS
Release:	14.04
Codename:	trusty
```

Installed version of CUDA and cuDNN: 
```
$ ls -l /usr/local/cuda-8.0/lib64/libcud*
-rw-r--r-- 1 root root   558720 11월  4 05:18 /usr/local/cuda-8.0/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 11월  4 05:18 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19 11월  4 05:18 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44
-rwxr-xr-x 1 root root   415432 11월  4 05:18 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.44
-rw-r--r-- 1 root root   775162 11월  4 05:18 /usr/local/cuda-8.0/lib64/libcudart_static.a
-rwxr-xr-x 1 root root 79337624 11월  4 05:20 /usr/local/cuda-8.0/lib64/libcudnn.so
-rwxr-xr-x 1 root root 79337624 11월  4 05:20 /usr/local/cuda-8.0/lib64/libcudnn.so.5
-rwxr-xr-x 1 root root 79337624 11월  4 05:20 /usr/local/cuda-8.0/lib64/libcudnn.so.5.1.5
-rw-r--r-- 1 root root 69756172 11월  4 05:20 /usr/local/cuda-8.0/lib64/libcudnn_static.a
```

Tensorflow version:
```
$ python -c ""import tensorflow; print(tensorflow.__version__)""
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
0.12.1
```

I tried following ['A typical pipeline for reading records from' in 'Reading data'](https://www.tensorflow.org/how_tos/reading_data/).

I wanted to read just one csv file including file name of MS COCO dataset.
The content of csv file is the following : 
```
$ head -n 5 train_file_list.csv 
./train2014/COCO_train2014_000000322402.jpg,
./train2014/COCO_train2014_000000441507.jpg,
./train2014/COCO_train2014_000000555318.jpg,
./train2014/COCO_train2014_000000380820.jpg,
./train2014/COCO_train2014_000000496662.jpg,
$ tail -n 5 train_file_list.csv 
./val2014/COCO_val2014_000000283947.jpg,
./val2014/COCO_val2014_000000027620.jpg,
./val2014/COCO_val2014_000000067310.jpg,
./val2014/COCO_val2014_000000044520.jpg,
./val2014/COCO_val2014_000000027617.jpg,
```

I have repeatedly confirmed that the field in csv is the only one containing the filename. I can attach pictures for the provement, but I will not. Of course, the 'train2014' and 'val2014' directories contain corresponding pictures, but that is not the case here.

Trying to test for reproducing the above tutorial, I wrote code which is the following : 
```python
import tensorflow as tf
 
filename_queue = tf.train.string_input_producer(['/path/to/train_file_list.csv'])

reader = tf.TextLineReader()
key, value = reader.read(filename_queue)

record_defaults = [['aa']]
fname = tf.decode_csv(value, record_defaults=record_defaults)

with tf.Session() as sess:
    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(coord=coord)

    for i in range(5):
        example = sess.run([fname])
        print(example)
    coord.request_stop()
    coord.join(threads)
```

but it raise the error which is the following : 
```
(tensorflow)mikigom@mikigom-desktop:~/github/HSP2P/Training$ python input_test.py 
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: GeForce GTX 1070
major: 6 minor: 1 memoryClockRate (GHz) 1.797
pciBusID 0000:01:00.0
Total memory: 7.92GiB
Free memory: 6.41GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)
Traceback (most recent call last):
  File ""input_test.py"", line 20, in <module>
    example = sess.run([fname])
  File ""/path/to/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 766, in run
    run_metadata_ptr)
  File ""/path/to/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 964, in _run
    feed_dict_string, options, run_metadata)
  File ""/path/to/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1014, in _do_run
    target_list, options, run_metadata)
  File ""/path/to/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1034, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Expect 1 fields but have 2 in record 0
	 [[Node: DecodeCSV = DecodeCSV[OUT_TYPE=[DT_STRING], field_delim="","", _device=""/job:localhost/replica:0/task:0/cpu:0""](ReaderRead:1, DecodeCSV/record_defaults_0)]]

Caused by op u'DecodeCSV', defined at:
  File ""input_test.py"", line 11, in <module>
    fname = tf.decode_csv(value, record_defaults=record_defaults)
  File ""/path/to/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_parsing_ops.py"", line 45, in decode_csv
    field_delim=field_delim, name=name)
  File ""/path/to/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 759, in apply_op
    op_def=op_def)
  File ""/path/to/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2240, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/path/to/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1128, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): Expect 1 fields but have 2 in record 0
	 [[Node: DecodeCSV = DecodeCSV[OUT_TYPE=[DT_STRING], field_delim="","", _device=""/job:localhost/replica:0/task:0/cpu:0""](ReaderRead:1, DecodeCSV/record_defaults_0)]]
```

It seems that ```tf.decode_csv()``` reads the second field which actually doesn't exist.
To temporarily solve this problem, I changed ```record_defaults``` in the above code to:
```python
record_defaults = [['aa'], ['aa']]
```

And if you run the code again, it will return normally.
```
$ python input_test.py 
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: GeForce GTX 1070
major: 6 minor: 1 memoryClockRate (GHz) 1.797
pciBusID 0000:01:00.0
Total memory: 7.92GiB
Free memory: 6.40GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)
[['./train2014/COCO_train2014_000000322402.jpg', 'aa']]
[['./train2014/COCO_train2014_000000441507.jpg', 'aa']]
[['./train2014/COCO_train2014_000000555318.jpg', 'aa']]
[['./train2014/COCO_train2014_000000380820.jpg', 'aa']]
[['./train2014/COCO_train2014_000000496662.jpg', 'aa']]
```

I think this is a bug in ```tf.decode_csv()```."
7362,Clean up SegmentReduction Ops,"The segment reduction ops are currently inconsistent, they include different ops for sorted/unsorted/ and sparse/dense tensors.
I guess it would make sense to provide the same reduction ops  for these - I'd be happy to work on this and build on nikste's work.

For similar previous issues @andydavis1, @drpngx were responsible for reviewing, so I link you here.

Things to possibly consider:
- Replace the sorted segment options by the more general unsorted options. I did a quick, non-extensive benchmark for  unsorted_segment_max vs  segment_max, the processing time is about the same (with the unsorted op even being a bit faster sometimes).
The drawback would be, that num_segments needs to be specified or needs to be computed before the reduction.
- Include the feature request ""Extend tf.unsorted_segment_sum to allow 'rejecting' entries"" #478
- Address the ToDo in UnsortedSegmentMax: `// todo: Remove duplicate code in UnsortedSegmentSumFunctor and UnsortedSegmentMaxFunctor.`
To sum up, I'd suggest to replace and extend the currently provided functions
```
tf.segment_sum(data, segment_ids, name=None)
tf.segment_prod(data, segment_ids, name=None)
tf.segment_min(data, segment_ids, name=None)
tf.segment_max(data, segment_ids, name=None)
tf.segment_mean(data, segment_ids, name=None)
tf.unsorted_segment_sum(data, segment_ids, num_segments, name=None)
tf.sparse_segment_sum(data, indices, segment_ids, name=None)
tf.sparse_segment_mean(data, indices, segment_ids, name=None)
tf.sparse_segment_sqrt_n(data, indices, segment_ids, name=None)
```
with 
```
# all dense/sparse ops support unsorted segments
tf.segment_sum(data, segment_ids, num_segements=None, name=None) 
tf.segment_prod(data, segment_ids, num_segements=None, name=None)
tf.segment_min(data, segment_ids, num_segements=None, name=None)
tf.segment_max(data, segment_ids, num_segements=None, name=None) 
tf.segment_mean(data, segment_ids, num_segements=None, name=None)
tf.segment_sqrt_n(data, segment_ids, num_segements=None, name=None)  # new

tf.sparse_segment_sum(data, indices, segment_ids, name=None)
tf.sparse_segment_prod(data, indices, segment_ids, name=None)  # new
tf.sparse_segment_min(data, indices, segment_ids, name=None)  # new
tf.sparse_segment_max(data, indices, segment_ids, name=None)  # new
tf.sparse_segment_mean(data, indices, segment_ids, name=None)
tf.sparse_segment_sqrt_n(data, indices, segment_ids, name=None)
```

(Or `tf.unsorted_segment_reduce_op` instead of `tf.segement_reduceop` to not break backward compability)"
7359,resize_image_with_crop_or_pad should work with batch of images,"This is a repost of https://github.com/tensorflow/tensorflow/issues/2284 [per comment](https://github.com/tensorflow/tensorflow/issues/2284#issuecomment-277339057) from @mrry (CC @ziky90).

`resize_image_with_crop_or_pad` should be modified to take either a single image or a batch of images. Right now the signature of the function specifies:
```
    image: 3-D tensor of shape `[height, width, channels]`
```

For a reference of what the api should look like, see [`resize_images(images,....)`](https://github.com/tensorflow/tensorflow/blob/4c192f060cf9ff897911d240c140299d6db257b6/tensorflow/python/ops/image_ops_impl.py#L605-L608):
```
    images: 4-D Tensor of shape `[batch, height, width, channels]` or
            3-D Tensor of shape `[height, width, channels]`.
```

Some commentary:
* docs for `resize_images` say: "" Resized images will be distorted if their original aspect ratio is not the same as `size`.  To avoid distortions see [`resize_image_with_crop_or_pad`]."" However the method referenced _cannot_ be currently used in some cases for which `resize_images` works due to the above mentioned limitations.
* the entire operation can be implemented using slice operations on the tensor batch. This is what `resize_image_with_crop_or_pad` does but it has the following logic which would be nice not to copy and paste (there is some subtlety with round down vs up):
```python
  width_diff = target_width - width
  offset_crop_width = max_(-width_diff // 2, 0)
  offset_pad_width = max_(width_diff // 2, 0)

  height_diff = target_height - height
  offset_crop_height = max_(-height_diff // 2, 0)
  offset_pad_height = max_(height_diff // 2, 0)
```
* the fact this one image op method does not handle batches leads to confusion and various [SO](http://stackoverflow.com/questions/33944683/tensorflow-map-operation-for-tensor) posts. People even resort to solving the problem using all sorts of crazy solutions (e.g. `tf.while_loop`).
* this method is useful in combination with concatenation operations. For example see the[ Lasagne `ConcatLayer`](http://lasagne.readthedocs.io/en/latest/modules/layers/merge.html#lasagne.layers.ConcatLayer)."
7358,"name 'DATA_CFG' is not defined, Extension 'tensorflow/tensorflow.bzl' has errors","Hi!

I was following the [Nvidia instruction](http://www.nvidia.com/object/gpu-accelerated-applications-tensorflow-installation.html) on how to install tensorflow with CUDA/cuDNN when I faced with the next problem (step 5, ""call bazel to build the TensorFlow pip package""):
```
~/tensorflow$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package 
ERROR: /home/user/tensorflow/tensorflow/tensorflow.bzl:478:19: name 'DATA_CFG' is not defined.
ERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Extension 'tensorflow/tensorflow.bzl' has errors.
INFO: Elapsed time: 0.108s
```
Could you suggest something to overcome this issue?

Additional information:
```
~$ lsb_release -a
No LSB modules are available.
Distributor ID:	Ubuntu
Description:	Ubuntu 16.04.1 LTS
Release:	16.04
Codename:	xenial
~$ ls -l /usr/local/cuda/lib64/libcud*
-rw-r--r-- 1 root root    558720 сен 15 02:02 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root        16 сен 15 02:05 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root        19 сен 15 02:05 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44
-rw-r--r-- 1 root root    415432 сен 15 02:02 /usr/local/cuda/lib64/libcudart.so.8.0.44
-rw-r--r-- 1 root root    775162 сен 15 02:02 /usr/local/cuda/lib64/libcudart_static.a
lrwxrwxrwx 1 user users       13 ноя  7 10:00 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 user users       18 ноя  7 10:00 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.10
-rwxr-xr-x 1 user users 84163560 ноя  7 09:47 /usr/local/cuda/lib64/libcudnn.so.5.1.10
-rw-r--r-- 1 user users 70364814 ноя  7 09:47 /usr/local/cuda/lib64/libcudnn_static.a
~/tensorflow$ git rev-parse HEAD
70de76e696c21da617fd2e6435cf7fedab220db8
~$ bazel version
Build label: 0.4.4
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Feb 1 18:54:21 2017 (1485975261)
Build timestamp: 1485975261
Build timestamp as int: 1485975261
```"
7357,there is a error when i run 'bazel build --config opt //tensorflow/tools/pip_package:build_pip_package',"INFO: Found 1 target...
ERROR: /home/ben/.cache/bazel/_bazel_ben/a552231fe7b7da49217bcb6530531abd/external/protobuf/BUILD:73:1: C++ compilation of rule '@protobuf//:protobuf_lite' failed: gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 38 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
gcc: error: cuda: No such file or directory
gcc: error: cuda: No such file or directory
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 5.769s, Critical Path: 1.60s
"
7356,Potential UnboundLocalError in error-handling context manager,"I'm working on unrelated things while this came up, and it's possibly unlikely to happen in real development, but just to track it in case someone cares,

https://github.com/tensorflow/tensorflow/blob/5e5dc97dd3523509ce5f536d7be5122d016fc6b5/tensorflow/python/framework/errors_impl.py#L471

that is an `UnboundLocalError` if it was line 463 that raised there (i.e. if TF_NewStatus failed entirely).

Without any insider knowledge, possibly that line belongs outside the try block."
7355,Add QueueBase.is_closed(),"Can we add `bool QueueBase.is_closed()` or `bool QueueBase.is_opened()`?
It will be mainly for debugging purposes.
"
7354,dynamic_rnn_decoder doesn't have an initial_state argument,"The `rnn_decoder` function used to have a `initial_state` argument earlier, which was useful for custom cell state initializations.

However, in the new API, the `dynamic_rnn_decoder` method doesn't have this argument. Is there any other to initialize cell state in this case?"
7353,Session run timeout causes memory leak and saving stuck,"I am using distributed tensorflow, one of my machines was something wrong and caused almost all session run timeout, eventually memory reached 80G and been killed. This also caused worker0 process failed to save model, saver.save() stuck forever.

I am using 0.12.1 windows build. I searched and nobody ever reported issues like this. Any idea and advice to confirm and fix it?"
7352,Tensorboard ImportError: No module named 'tensorflow.tensorboard.plugins.debugger',"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

None.

### Environment info
Operating System:
Windows 10.

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
None.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
e946a6b63979a63f9e5a1d1603f6cc21d8aad1cf
2. The output of `bazel version`
(compiled by cmake)
cmake -G ""Visual Studio 14 2015 Win64"" .. -DCMAKE_BUILD_TYPE=RelWithDebInfo -DSWIG_EXECUTABLE=C:\prog\swigwin-3.0.12\swig.exe -DPYTHON_EXECUTABLE=C:\Python35\python.exe -DPYTHON_LIBRARIES=C:\Python35\libs\python35.lib -Dtensorflow_ENABLE_HDFS_SUPPORT=ON 


### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
```
> tensorboard --logdir=..\logs
Traceback (most recent call last):
  File ""c:\python35\lib\runpy.py"", line 184, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""c:\python35\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Python35\Scripts\tensorboard.exe\__main__.py"", line 5, in <module>
  File ""c:\python35\lib\site-packages\tensorflow\tensorboard\tensorboard.py"", line 35, in <module>
    from tensorflow.tensorboard.plugins.debugger import plugin as debugger_plugin
ImportError: No module named 'tensorflow.tensorboard.plugins.debugger'
```

```
>dir C:\Python35\Lib\site-packages\tensorflow\tensorboard\plugins
 Volume in drive C is OSDisk
 Volume Serial Number is 1066-463E

 Directory of C:\Python35\Lib\site-packages\tensorflow\tensorboard\plugins

02/08/2017  11:55 AM    <DIR>          .
02/08/2017  11:55 AM    <DIR>          ..
02/08/2017  11:55 AM             1,658 base_plugin.py
02/08/2017  11:55 AM    <DIR>          projector
02/08/2017  11:55 AM                 0 __init__.py
02/08/2017  11:55 AM    <DIR>          __pycache__
```"
7344,Documentation for tf.contrib.layers.stack replaced by documentation for tf.stack,"Documentation for tf.contrib.layers.stack appears to be replaced by documentation for tf.stack, see
[https://www.tensorflow.org/api_docs/python/contrib.layers/higher_level_ops_for_building_neural_network_layers_#stack](https://www.tensorflow.org/api_docs/python/contrib.layers/higher_level_ops_for_building_neural_network_layers_#stack)"
7341," pywrap_tensorflow.TF_GetCode(status)) tensorflow.python.framework.errors_impl.InvalidArgumentError: Shape in shape_and_slice spec does not match the shape in the save file: [10,5], save file shape: [10,3]","NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
7336,"Docs don't explain how to add the optimization flags for avx, sse instruction sets","After installing tensorflow from source, I saw the following warnings.

```
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
```

The current tensorflow docs mention adding `-march=native` to configure but don't explain how. I have found by inspecting the configure script you can add it with `--copt=-march=native`. 

Can someone explicitly state this in the docs?"
7329,404 error when installing Tensorflow on Python for Raspberry pi,"I want to install Tensorflow on Python for Raspberry Pi and followed the steps in this page:
https://github.com/samjabrahams/tensorflow-on-raspberry-pi

But when I apply the command:
wget https://github.com/samjabrahams/tensorflow-on-raspberry-pi/releases/download/v0.12.1/tensorflow-0.12.1-cp27-none-linux_armv7l.whl

I get Error 404 - not found ..

How to solve it?




NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
7328,Add network address check in clusterSpec,"Today, I got an error when starting a distributed TensorFlow computation. I debugged for hours and finally found the problem is that TensorFlow doesn't have network address check in `ClusterSpec` method. And it endlessly connect the illegal network address `100.xxx.xxx.232.:2223`, because there is a redundant `.` after `232`."
7327,CondaHTTPError: HTTP Error when  installing tensorflow on windows by anaconda,"(D:\Program Files (x86)\Anaconda2) C:\XX\XXXXX>conda install -c https://
api.anaconda.org/conda-forge tensorflow
Fetching package metadata ....Could not connect to https://repo.continuum.io/pkg
s/free/noarch/
Could not connect to https://repo.continuum.io/pkgs/pro/win-64/
Could not connect to https://repo.continuum.io/pkgs/free/win-64/
Could not connect to https://repo.continuum.io/pkgs/pro/noarch/
Could not connect to https://repo.continuum.io/pkgs/msys2/win-64/
Could not connect to https://repo.continuum.io/pkgs/msys2/noarch/
CondaHTTPError: HTTP Error: Could not find URL: https://api.anaconda.org/conda-f
orge/win-64/

why could not connect... ???? 
conda-forge/tensorflow | 0.12.1 | conda | linux-64, win-64, osx-64
"
7326,Can't find rt.jar when installing tensorflow on Mac OS,"Hi,

I downloaded and installed jdk 8 on my mac os. But there was no classes.jar or rt.jar shipped with the installation. When I tried to install tensorflow afterwards, I got a could not find rt.jar error. How should I fix this ?"
7324," Auto-Configuration Error: Invalid compute capability: ""6.0""","```
Extracting Bazel installation...
.
INFO: Starting clean.
INFO: Output base moved to /home/chaimb/.cache/bazel/_bazel_chaimb/acf2af86672e1552dfd6edd47d54a950_tmp_31124 for deletion
WARNING: Output base '/home/chaimb/.cache/bazel/_bazel_chaimb/acf2af86672e1552dfd6edd47d54a950' is on NFS. This may lead to surprising failures and undetermined behavior.
............
ERROR: package contains errors: tensorflow/stream_executor.
ERROR: error loading package 'tensorflow/stream_executor': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
        File ""/home/chaimb/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl"", line 824
                _create_cuda_repository(repository_ctx)
        File ""/home/chaimb/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl"", line 736, in _create_cuda_repository
                _get_cuda_config(repository_ctx)
        File ""/home/chaimb/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl"", line 590, in _get_cuda_config
                struct(cuda_toolkit_path = cuda_toolkit..., <5 more arguments>)
        File ""/home/chaimb/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl"", line 595, in struct
                _compute_capabilities(repository_ctx)
        File ""/home/chaimb/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl"", line 337, in _compute_capabilities
                auto_configure_fail(""Invalid compute capability: %s""...)
        File ""/home/chaimb/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl"", line 93, in auto_configure_fail
                fail(""
%sAuto-Configuration Error:%s ...))

Auto-Configuration Error: Invalid compute capability: ""6.0""
.
```
Works with default compute capability.
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
https://github.com/tensorflow/tensorflow/issues/4105
### Environment info
Operating System:
Ubuntu 16.04
Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
CUDA 8.0
```
-rw-r--r-- 1 root root    558720 Sep 15 02:02 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root        16 Sep 15 02:05 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root        19 Sep 15 02:05 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44
-rw-r--r-- 1 root root    415432 Sep 15 02:02 /usr/local/cuda/lib64/libcudart.so.8.0.44
-rw-r--r-- 1 root root    775162 Sep 15 02:02 /usr/local/cuda/lib64/libcudart_static.a
-rw-r--r-- 1 root root 105920110 Feb  7 10:47 /usr/local/cuda/lib64/libcudnn_static.a
```

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
`eb225d71b394c24f49a2e07f685be94d4ab7496f`
2. The output of `bazel version`
```
Build label: 0.4.4
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Feb 1 18:54:21 2017 (1485975261)
Build timestamp: 1485975261
Build timestamp as int: 1485975261

```
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
It crashes directly after I pick `configure` options

"
7323,convert_variables_to_constants doesn't preserve output shape,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System: Ubuntu 16.04

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

``` sh
$ ls -l /usr/local/cuda-8.0/lib64/libcud*
-rw-r--r-- 1 root root   558720 Sep 30 15:48 /usr/local/cuda-8.0/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 Sep 30 15:48 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19 Sep 30 15:48 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44
-rwxr-xr-x 1 root root   415432 Sep 30 15:48 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.44
-rw-r--r-- 1 root root   775162 Sep 30 15:48 /usr/local/cuda-8.0/lib64/libcudart_static.a
lrwxrwxrwx 1 root root       13 Sep 30 15:50 /usr/local/cuda-8.0/lib64/libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 root root       17 Sep 30 15:50 /usr/local/cuda-8.0/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5
-rwxr-xr-x 1 root root 79337624 Sep 30 15:50 /usr/local/cuda-8.0/lib64/libcudnn.so.5.1.5
-rw-r--r-- 1 root root 69756172 Sep 30 15:50 /usr/local/cuda-8.0/lib64/libcudnn_static.a
```

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`
Tensorflow Version: '0.12.1-3-g45ab528-dirty'
Bazel Version: 0.4.4

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

``` python
#!/usr/bin/python3
import tensorflow as tf
from tensorflow.python.framework import graph_util
import difflib

graph_1 = tf.Graph()
with graph_1.as_default():
    input_batch = tf.placeholder(shape=(None, 100), dtype=tf.float32, name=""Placeholder"")

graph_1_pb = graph_1.as_graph_def(add_shapes=True)
session = tf.Session(graph=graph_1)
graph_2_pb = graph_util.convert_variables_to_constants(session, session.graph.as_graph_def(), [""Placeholder""])

print(""\n"".join(difflib.unified_diff(str(graph_1_pb).split(""\n""), str(graph_2_pb).split(""\n""))))
```
I expected it will return no diff or have a little diff, but it has noticeable diffs.

```
--- 

+++ 

@@ -1,21 +1,6 @@

 node {
   name: ""Placeholder""
   op: ""Placeholder""
-  attr {
-    key: ""_output_shapes""
-    value {
-      list {
-        shape {
-          dim {
-            size: -1
-          }
-          dim {
-            size: 100
-          }
-        }
-      }
-    }
-  }
   attr {
     key: ""dtype""
     value {
@@ -30,7 +15,4 @@

     }
   }
 }
-versions {
-  producer: 17
-}
```
Preserving `_output_shapes attribute` is somewhat important for me, so we have `add_shapes=True` on `tf.Graph.as_graph_def` function.

seems like other [graph utils](https://github.com/tensorflow/tensorflow/blob/a0d784bdd31b27e013a7eac58a86ba62e86db299/tensorflow/python/tools/strip_unused_lib.py#L64)  copying `_output_shapes` attributes. 

### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
7322,How to uninstall tensorflow?,"Help I want to reinstall tensorflow, from GPU to CPU, because, just installed tensorflow not work correctly.
I test my installation, from documentation of tensorflow.org,
then it gives back that error:

Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> hello = tf.constant('Hello, TensorFlow!')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: module 'tensorflow' has no attribute 'constant'


SO SHOULD I REINSTALL TENSORFLOW?
before I installed by pip, but it did not work then I use virtualen, also did not work, then by using anaconda. something worked, but I stack in testing installation. Help me!"
7321,Tensorflow retrained model compression fails with error: terminate called after throwing an instance of 'std::bad_alloc'   what():  std::bad_alloc Aborted (core dumped) ,"I am doing a college project on tensorflow. I have successfully retrained using rerain.py file, to use that model in android program. I was trying to compress the model using commands in the following
link: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#shrinking-file-size

But it is getting terminated throwing the below error:
ERROR:   terminate called after throwing an instance of 'std::bad_alloc'   what():  std::bad_alloc Aborted (core dumped) 

I am working on  an Ubuntu 14.04 machine with 4GB RAM.

Please help.

Thanks,
Bruczzz"
7320,Tensorflow freezes for a small model in Windows 10,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
--> I have not been able to find any related information. I did post a SO thread (http://stackoverflow.com/questions/41889147/tensorflow-execution-freezes-for-a-small-cnn), and got the suggestion that I may have encountered a bug.

### Environment info
Operating System: 
--> Windows 10 home edition

Installed version of CUDA and cuDNN: 
--> None. Was using the CPU version.
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed: 
--> https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-0.12.1-cp35-cp35m-win_amd64.whl
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
--> 0.12.1

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
--> Attached as ""Issue_example_code.txt""
A brief explanation of the issue: 
Training freezes as I slightly increase the model size. For the attached code, using 50, 50 for the two fully-connected layers would work, but going slightly larger, e.g., to 100, 50, would result in execution freezing after model initialization or a few rounds of training. It does not seem to be constrained by memory, since when the freeze occurs the Python process takes around 50MB of memory (as observed from task manager), while when the training runs (for smaller sizes) it takes GB range of memory.


### What other attempted solutions have you tried?
--> None, as I was unable to find related info by searching and I myself does not understand the possible cause of this issue.

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).

[Issue_example_code.txt](https://github.com/tensorflow/tensorflow/files/756669/Issue_example_code.txt)

"
7319,"can any body write one simple seq2seq model with attention ,the example given by tensorflow.org is too hard to understand!","can any body write one simple seq2seq model with attention ,the example given by tensorflow.org is too hard to understand!
"
7317,Tensorboard fails to load with latest install from source.,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
https://github.com/tensorflow/tensorflow/issues/1421
https://github.com/tensorflow/tensorflow/issues/4596
https://github.com/tensorflow/tensorflow/issues/1076

### Environment info
Operating System: Ubuntu 16.04

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
```
-rw-r--r-- 1 root root   560184 Jul 23  2016 libcudadevrt.a
lrwxrwxrwx 1 root root       16 Jul 23  2016 libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19 Jul 23  2016 libcudart.so.8.0 -> libcudart.so.8.0.27
-rwxr-xr-x 1 root root   394472 Jul 23  2016 libcudart.so.8.0.27
-rw-r--r-- 1 root root   737516 Jul 23  2016 libcudart_static.a
lrwxrwxrwx 1 root root       13 Jul 24  2016 libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 root root       17 Jul 24  2016 libcudnn.so.5 -> libcudnn.so.5.0.5
-rwxr-xr-x 1 root root 78065952 Apr 22  2016 libcudnn.so.5.0.5
-rw-r--r-- 1 root root 68709594 Apr 22  2016 libcudnn_static.a
```

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`
commit hash: eb225d71b394c24f49a2e07f685be94d4ab7496f
bazel version
```
Build label: 0.4.3
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Dec 22 12:31:25 2016 (1482409885)
Build timestamp: 1482409885
Build timestamp as int: 1482409885
```

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
Chrome dev tools shows that tensorboard is failing to load because of missing polymer.html
```
GET http://0.0.0.0:6006/polymer/polymer.html 404 (NOT FOUND)
...Lots of other errors because Polymer is undefined...
```
"
7316,Explicit device does not work with saved GraphDef?,"### explicit device seems not work with a saved GragphDef
I try https://github.com/tensorflow/models/blob/master/tutorials/image/imagenet/classify_image.py
and want to use gpu to classify an image.

I change the create-graph code
```
  with tf.device(""/gpu:0""):
    with tf.gfile.FastGFile(os.path.join(
      FLAGS.model_dir, 'classify_image_graph_def.pb'), 'rb') as f:
      graph_def = tf.GraphDef()
      graph_def.ParseFromString(f.read())
      _ = tf.import_graph_def(graph_def, name='')

  with tf.Session(config=tf.ConfigProto(
      allow_soft_placement=True, log_device_placement=True)) as sess:
```
but from logs all operations are done on cpu not gpu.

Is there anyone to help me use gpu with classify_image.py
### Environment info
Operating System: Ubuntu 14.04 LTS, tensorflow 0.12.1
CUDA: 7.5.18 
cuDNN: v5.1
installed via `sudo pip install tensorflow --upgrade`

### Logs
```
conv_2/batchnorm/gamma: (Const): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:827] conv_2/batchnorm/gamma: (Const)/job:localhost/replica:0/task:0/cpu:0
conv_2/batchnorm/beta: (Const): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:827] conv_2/batchnorm/beta: (Const)/job:localhost/replica:0/task:0/cpu:0
conv_2/conv2d_params: (Const): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:827] conv_2/conv2d_params: (Const)/job:localhost/replica:0/task:0/cpu:0
conv_1/batchnorm/moving_variance: (Const): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:827] conv_1/batchnorm/moving_variance: (Const)/job:localhost/replica:0/task:0/cpu:0
conv_1/batchnorm/moving_mean: (Const): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:827] conv_1/batchnorm/moving_mean: (Const)/job:localhost/replica:0/task:0/cpu:0
conv_1/batchnorm/gamma: (Const): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:827] conv_1/batchnorm/gamma: (Const)/job:localhost/replica:0/task:0/cpu:0
conv_1/batchnorm/beta: (Const): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:827] conv_1/batchnorm/beta: (Const)/job:localhost/replica:0/task:0/cpu:0
conv_1/conv2d_params: (Const): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:827] conv_1/conv2d_params: (Const)/job:localhost/replica:0/task:0/cpu:0
conv/batchnorm/moving_variance: (Const): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:827] conv/batchnorm/moving_variance: (Const)/job:localhost/replica:0/task:0/cpu:0
conv/batchnorm/moving_mean: (Const): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:827] conv/batchnorm/moving_mean: (Const)/job:localhost/replica:0/task:0/cpu:0
conv/batchnorm/gamma: (Const): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:827] conv/batchnorm/gamma: (Const)/job:localhost/replica:0/task:0/cpu:0
conv/batchnorm/beta: (Const): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:827] conv/batchnorm/beta: (Const)/job:localhost/replica:0/task:0/cpu:0
conv/conv2d_params: (Const): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:827] conv/conv2d_params: (Const)/job:localhost/replica:0/task:0/cpu:0
Mul/y: (Const): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:827] Mul/y: (Const)/job:localhost/replica:0/task:0/cpu:0
Sub/y: (Const): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:827] Sub/y: (Const)/job:localhost/replica:0/task:0/cpu:0
ResizeBilinear/size: (Const): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:827] ResizeBilinear/size: (Const)/job:localhost/replica:0/task:0/cpu:0
ExpandDims/dim: (Const): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:827] ExpandDims/dim: (Const)/job:localhost/replica:0/task:0/cpu:0
DecodeJpeg/contents: (Const): /job:localhost/replica:0/task:0/cpu:0
```"
7314,slice_input_producer does not return a queue,"`tf.train.slice_input_producer` does not return a queue for each slice. This means the outputs cannot be used with the BaseReader api. Shouldn't this be consistent with the other `*_input_producer` methods like `string_input_producer`?

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
http://stackoverflow.com/questions/34340489/tensorflow-read-images-with-labels

### Environment info
Operating System:
Ubuntu 14.04

### Installed version of CUDA and cuDNN: 
Cuda 8.0.44
Cudnn 5.1.5

### Tensorflow Version:
0.12.1

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
```python
image_files = glob(...)
label_files = [s.replace('image.jpg', 'labels.jpg') for s in image_files]
# Create a queue where the elements are pairs (image, label)
image_queue, label_queue = tf.train.slice_input_producer([image_files, label_files])
images, labels = [], []
for _ in num_threads:
    # Create two readers for each thread - one for the image and one for the label
    image_reader, label_reader = tf.SomeReader(), tf.SomeReader()
    # Read the image and label pair
    _, image_op = image_reader.read(image_queue)
    _, label_op = label_reader.read(label_queue)
    images.append(image_op)
    labels.append(label_op)
```
## Error
```python
File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/io_ops.py"", line 265, in read
  return gen_io_ops._reader_read(self._reader_ref, queue_ref, name=name)
File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 213, in _reader_read
  queue_handle=queue_handle, name=name)
File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 613, in apply_op
  (input_name, op_type_name))
TypeError: Input 'queue_handle' of 'ReaderRead' Op requires l-value input
```

### What other attempted solutions have you tried?
Using two `string_input_producers` with the same random seed
"
7313,AttributeError: module 'tensorflow.contrib.learn' has no attribute 'SKCompat',"Hi,

I cannon import SKCompat. Running 0.12.head on osx. Any ideas?"
7312,incorrect usage of num_gpus & num_workers in mnist_replica.py,"[mnist_replica.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dist_test/python/mnist_replica.py#L126) has the following code. 
```
 if FLAGS.num_gpus < num_workers:
    raise ValueError(""number of gpus is less than number of workers"")
```
This will work fine in a single worker setup with one or more GPUs. However, it will throw error if the number of workers is greater than the number of GPUs available in each worker. For example, using 2 workers each with 1 GPU will trigger this error.

It looks like this if-block can be removed. If that is an acceptable change, I can send a PR for that."
7311,tfrecords giving parsing error when saved examples are too big,"
When I try to save a example to a tfrecord that is too large a parsing error occurs. I need to save a large sequence of float valued images (states of a fluid flow simulation) and when the states and sequences get too large a parsing error occurs.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

I think that this might be the same error and problem that other people are getting when saving long sequences to tfrecords like seen here https://github.com/tensorflow/tensorflow/issues/5234 and https://github.com/OliviaMG/xiaomeng/issues/1. While both of these threads found solutions the underlining problem does not seem to be found.

### Environment info
Operating System:

Ubuntu 16.04
CUDA 8.0
CUDNN 5.1
TensorFlow 0.12.1

### If possible, provide a minimal reproducible example

Running this minimal script will cause the error. Lowering the ```SIZE_RECORD``` value will cause the script to run with out error.

```
from tqdm import tqdm
import numpy as np
import tensorflow as tf

# this will kill it
SIZE_RECORD=20000000
# this will run just fine
# SIZE_RECORD=2000000

writer = tf.python_io.TFRecordWriter(""test.tfrecords"")
# iterate over 10 times
# wrap with tqdm for a progress bar
for example_idx in tqdm(xrange(2)):
    features = np.zeros((SIZE_RECORD))

    # construct the Example proto boject
    example = tf.train.Example(
        # Example contains a Features proto object
        features=tf.train.Features(
          # Features contains a map of string to Feature proto objects
          feature={
            # A Feature contains one of either a int64_list,
            # float_list, or bytes_list
            'image': tf.train.Feature(
                float_list=tf.train.FloatList(value=features.astype(""float""))),
    }))
    # use the proto object to serialize the example to a string
    serialized = example.SerializeToString()
    # write the serialized object to disk
    writer.write(serialized)
writer.close()

def read_and_decode_single_example(filename):
    # first construct a queue containing a list of filenames.
    # this lets a user split up there dataset in multiple files to keep
    # size down
    filename_queue = tf.train.string_input_producer([filename],
                                                    num_epochs=None)
    # Unlike the TFRecordWriter, the TFRecordReader is symbolic
    reader = tf.TFRecordReader()
    # One can read a single serialized example from a filename
    # serialized_example is a Tensor of type string.
    _, serialized_example = reader.read(filename_queue)
    # The serialized example is converted back to actual values.
    # One needs to describe the format of the objects to be returned
    features = tf.parse_single_example(
        serialized_example,
        features={
            # We know the length of both fields. If not the
            # tf.VarLenFeature could be used
            'image': tf.FixedLenFeature([SIZE_RECORD], tf.float32)
        })
    # now return the converted data
    image = features['image']
    return image

# get single examples
image = read_and_decode_single_example(""test.tfrecords"")
# groups examples into batches randomly
images_batch = tf.train.shuffle_batch(
    [image], batch_size=1,
    capacity=3,
    min_after_dequeue=2)

# simple model
w = tf.get_variable(""w1"", [SIZE_RECORD, 1])
y_pred = tf.matmul(images_batch, w)
loss = tf.nn.l2_loss(y_pred - 1.0)

# for monitoring
loss_mean = tf.reduce_mean(loss)

train_op = tf.train.AdamOptimizer().minimize(loss)

sess = tf.Session()
init = tf.initialize_all_variables()
sess.run(init)
tf.train.start_queue_runners(sess=sess)

_, loss_val = sess.run([train_op, loss_mean])
print loss_val
print(""worked!!!"")
````

### What other attempted solutions have you tried?

I have tried saving the example in a variety of different ways including converting it to a string and breaking up the vector into multiple features.

### Logs or other output that would be helpful

This is the output when the tfrecord is too big

W tensorflow/core/framework/op_kernel.cc:975] Invalid argument: Could not parse example input, value: '
���&
���&
image���&���&
���&
ERROR:tensorflow:Exception in QueueRunner: 'utf8' codec can't decode byte 0x9b in position 40: invalid start byte
Exception in thread Thread-3:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/threading.py"", line 801, in __bootstrap_inner
    self.run()
  File ""/usr/lib/python2.7/threading.py"", line 754, in run
    self.__target(*self.__args, **self.__kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/queue_runner_impl.py"", line 234, in _run
    sess.run(enqueue_op)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 766, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 964, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1014, in _do_run
    target_list, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1021, in _do_call
    return fn(*args)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1003, in _run_fn
    status, run_metadata)
  File ""/usr/lib/python2.7/contextlib.py"", line 24, in __exit__
    self.gen.next()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py"", line 468, in raise_exception_on_not_ok_status
    compat.as_text(pywrap_tensorflow.TF_Message(status)),
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/compat.py"", line 84, in as_text
    return bytes_or_text.decode(encoding)
  File ""/usr/lib/python2.7/encodings/utf_8.py"", line 16, in decode
    return codecs.utf_8_decode(input, errors, True)
UnicodeDecodeError: 'utf8' codec can't decode byte 0x9b in position 40: invalid start byte


Thank!!!"
7308,Small inaccuracy in Mandelbrot tutorial,"Just noticed a small inaccuracy in  [the Mandelbrot tutorial](https://www.tensorflow.org/tutorials/mandelbrot/):

There is a snippet like this:

```
# Use NumPy to create a 2D array of complex numbers on [-2,2]x[-2,2]
Y, X = np.mgrid[-1.3:1.3:0.005, -2:1:0.005]
```

In the comment there is an area [-2,2]x[-2,2], but in the code itself it looks more like [-2.1]x[-1.3,1,3].
Nothing important but still slightly confusing :)

"
7306,OutOfRangeError/Early EOF on file read in Windows Server 2012,"Hi folks,

I am finding that tensorflow code snippets which ran on Windows 10 are failing on Windows Server 2012/NTFS. In particular, anytime I try to load a file (with `tf.gfile.Open`, `tf.gfile.FastGFile`, or `tf.contrib.slim.assign_from_checkpoint_fn`), I encounter an ""out of range"" error.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

I found [Git issue #6791 (Contrib support for Windows) ](https://github.com/tensorflow/tensorflow/issues/6791) where it was suggested that the fault lies with contrib packages. However, the code sample I provide below displays the problem without using any contrib imports.

### Environment info
Operating System: Windows Server 2012 R2

Installed version of CUDA and cuDNN: N/A
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed: [tensorflow-0.12.1-cp35-cp35m-win_amd64.whl](https://pypi.python.org/packages/64/a3/0054a3329579de44d557f491adbcaf8127809a7992bc46af80f0a589e29b/tensorflow-0.12.1-cp35-cp35m-win_amd64.whl)
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`. **0.12.1**

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

```
with tf.Graph().as_default():
        with tf.Session('') as sess:
                image_data = tf.gfile.FastGFile(filepath_to_image, 'r').read()
---------------------------------------------------------------------------
OutOfRangeError                           Traceback (most recent call last)
<ipython-input-11-a7847d42df75> in <module>()
      1 with tf.Graph().as_default():
      2     with tf.Session('') as sess:
----> 3         image_data = tf.gfile.FastGFile(training_filenames[0], 'r').read()
C:\Anaconda\envs\py35\lib\site-packages\tensorflow\python\lib\io\file_io.py in read(self, n)
    110       else:
    111         length = n
--> 112       return pywrap_tensorflow.ReadFromStream(self._read_buf, length, status)
    113 
    114   def seek(self, position):
C:\Anaconda\envs\py35\lib\contextlib.py in __exit__(self, type, value, traceback)
     64         if type is None:
     65             try:
---> 66                 next(self.gen)
     67             except StopIteration:
     68                 return
C:\Anaconda\envs\py35\lib\site-packages\tensorflow\python\framework\errors_impl.py in raise_exception_on_not_ok_status()
    467           None, None,
    468           compat.as_text(pywrap_tensorflow.TF_Message(status)),
--> 469           pywrap_tensorflow.TF_GetCode(status))
    470   finally:
    471     pywrap_tensorflow.TF_DeleteStatus(status)
OutOfRangeError: reached end of file
```

A similar error is encountered when reading a text file, and a more complicated DataLossError occurs when trying to load a model from a checkpoint (but the trace includes references to ""out of range"" errors, so I assume the same underlying problem is responsible).

The same code snippet gives no error and returns the image byte data when run on the same version of Tensorflow on Windows 10 (also NTFS).

### What other attempted solutions have you tried?

I can workaround this problem for images and text files by using native Python to read the files. However, I really need to use tensorflow built-ins to e.g. read a trained model from a checkpoint.

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).

See above for output of MWE.

Thanks very much for your help!"
7304,Python Pandas in Docker Images,"### Feature

What are everyone's opinions on including Python Pandas in the [Docker images](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/docker)?

### Reason

TF.Lean is commonly used with Pandas for data ingestion, (And [in many](https://www.tensorflow.org/tutorials/wide/) of the [examples](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/text_classification.py)). The Docker implementation is a noob-friendly way to install TensorFlow, and given this, is likely to be used with TF.Learn.

### Pull Request
If we decide to implement this, here's a Pull Request: #7305 "
7303,Creating cyclic graphs with the C API,"I'm trying to use the C API to create a cyclic graph that corresponds to a while loop with `merge` and `switch` operations. 

It doesn't seem possible though, since `TF_AddInput` requires a `TF_Operation` pointer, which in turn can only come from `TF_FinishOperation`, which expects all the inputs of the `TF_OperationDescription` to be filled in.  How can you create cyclic graph by creating nodes one at a time, if to create each node, all its input must already be created?

Just wanted to check I'm not missing anything. Thanks!"
7302,Weird grouping of tf.layers ops in TensorBoard graph visualization,"This doesn't always happen, but sometimes when stacking multiple tf.layers, variables get placed in previous layers' graphical boxes. Setting variable_scope prevents this, but it's still a bit confusing as to why this is happening. It could have something to do with how tf.layers decide on a scope name automatically.

An example on TensorFlow 1.0.0-rc0 (but I think this also happens on the current PyPI release):
![image](https://cloud.githubusercontent.com/assets/1595907/22663580/686737ee-ecad-11e6-8044-5e09749378cc.png)

```python
x = tf.layers.conv2d(x, ...)
x = tf.layers.conv2d(x, ...)
x = tf.layers.conv2d(x, ...)
```

As it's hard to reproduce, I think it could be related to having multiple runs in the same TensorBoard, and some kind of namespace collision happening."
7300,"API documentation ""Core graph data structures""","A few things are wrong in the documentation around [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/framework.md#tfgraphadd_to_collectionname-value-graphadd_to_collection](url).

* Near the end of tf.Graph.name_scope, the line about ValueError is incomplete.
* The paragraph at the very end of tf.Graph.name_scope should be in tf.Graph.add_to_collection.
* Both tf.Graph.add_to_collection and tf.Graph.add_to_collections (plural) exist."
7297,Slow GPU performance,"Fresh installed on Azure Linux VM with Nvidia Tesla M60 which has 2x8 GB Ram and dual GPU but program prints only one GPU and 8 GB ram as available. Result is worse than GTX 1080.

I can't find specific information for Tesla M60. Is there any trick for dual core GPU's ? 

 


"
7296,Seperate gradients in tf.gradients,"based on [documentation](https://www.tensorflow.org/api_docs/python/train/gradient_computation#gradients) `tf.gradients` return the `sum(dy/dx)`. For our problem, **sum** doesn't make sense. Is it possible to get the list of gradients instead? "
7295,tf.get_collection to extract variables of one scope ,"Hi,

I have `n`(e.g: n=3) scopes and `x` (e.g: x=4) no of Variables defined in each scope. 
The scopes are:

        model/generator_0
        model/generator_1
        model/generator_2

Once I compute the loss, I want to extract and provide all the variables from only one of the scope based on a criteria during run-time. Hence the index of the scope `idx` that I select is an argmin tensor cast into `int32` 

        <tf.Tensor 'model/Cast:0' shape=() dtype=int32>

I have already tried: 

        train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'model/generator_'+tf.cast(idx, tf.string)) 

which obviously did not work. 
Is there any way to get all the `x` Variables belonging to that particular scope using `idx` to pass into the optimizer.
Forgive me if this question doesnt fit into tensorflow issues. 

Thanks in advance!
Vignesh Srinivasan"
7294,Any reason to not publish wheels for Mac OS < 10.11.x on PyPI?,"The tensorflow wheels for Mac OS on PyPI are tagged with a platform of 10.11 and thus aren't picked up by pip on Mac OS 10.10.x or earlier. Google hosts wheels that work on any version of Mac OS (e.g., https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.12.1-py2-none-any.whl).

Can these be uploaded to PyPI or can a wheel be published there that supports older versions of Mac OS?"
7293,How to do k-max pooling with proper dim ?,"Hi folks,

Does anybody have this same issue as me ? I want to apply a k-max pooling to this var, however it's is not in the proper dimension yet:

For ex:
h = tf.Variable(batch_size,1,18,512)     # This is the obligatory dim's result I got when I apply Deep CNN in NLP

I want to have the result of k-max pooling like this:
h_kmaxpooling with dim (batch_size,1,**8**,512) 

If I use tf.nn.top_k(h, 8), it will result in (batch_size,1,18,**8**) instead.
I've tried **tf.transpose(x, perm=[batch_size, 1, 512, 18])** but it always give error in batch_size since it doesn't know how many examples are. Or may be I've set wrong tf.transpose in this case ?
 "
7292,F c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\cuda\cuda_blas.cc:268] Check failed: s.ok() could not find cublasCreate_v2 in cuBLAS DSO; dlerror: cublasCreate_v2 not found,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System:win10

Installed version of CUDA and cuDNN: 
cuda8.0 cudnn 5

If installed from binary pip package, provide:
pip install --upgrade https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-0.12.1-cp35-cp35m-win_amd64.whl


If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
7290,"""SAME"" padding for ConvNet doesn't work as expected !","Hi folks,
When I want to reimplement the paper [VDCNN for Text Classification](https://arxiv.org/abs/1606.01781), I need to do padding=1 with filter_kernel=3 to keep the same input's dimension between ConvNet in the model.  When I use padding 'SAME', the result isn't well expected. There's a problem in the height_size_output. It seems doesn't do element-wise product of block convnet 72x3 between the input & the filter. 
By the way, there's no problem of height_size_output using the padding 'VALID'. It seems that it did. However, its width_size_output is reduced by 2. 
In my intuition, padding 'VALID' behave much more properly like in the paper, but how can I pad 1 to both the left & right of the 2D input before doing ConvNet ?

-----------------------------------------
import tensorflow as tf

data = tf.placeholder(""float"", shape=[1, 72, 1014, 1])
#
filter_shape = [72, 3, 1, 64]	
W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.05), name=""W"")
#
conv = tf.nn.conv2d(data, W, padding='SAME')
print(conv.get_shape())  # prints (1, **72**, 1014, 64), but should be (1, **1**, 1014, 64)

conv = tf.nn.conv2d(data, W, padding='VALID')
print(conv.get_shape())  # prints (1, **1**, **1012**, 64), but should be (1, **1**, **1014**, 64)"
7289,tensorflow/core/platform/setround.cc:27:4: error: 'fesetround' is not a member of 'std',"```
ERROR: /local/tmp/tensorflow/tensorflow/core/BUILD:1036:1: C++ compilation of rule '//tensorflow/core:lib_internal' failed: gcc failed: error executing command /local2/tmp/brew/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/local2/tmp/brew/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 ... (remaining 104 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
tensorflow/core/platform/setround.cc: In constructor 'tensorflow::port::ScopedSetRound::ScopedSetRound()':
tensorflow/core/platform/setround.cc:27:4: error: 'fesetround' is not a member of 'std'
    std::fesetround(FE_TONEAREST);
    ^
tensorflow/core/platform/setround.cc:27:4: note: suggested alternative:
In file included from tensorflow/core/platform/setround.cc:19:0:
/local2/tmp/brew/include/fenv.h:88:12: note:   'fesetround'
 extern int fesetround (int __rounding_direction) __THROW;
            ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
```

Looks like the issue is that this file is being compiled with gcc instead of g++"
7288,Are linkopts propagated from copts and/or deps ?,"The linker complains about not finding `-lpthread`, while I didn't add this flag to linkopts.

I've checked the executed command, and in fact there is extra flags on it `-lz  -lpthread ...`.

Where did they came from ? Is there a workaround for this ?


More details
====

BUILD file
---

```
cc_binary(
    name = ""libfoo.so"",
    srcs = glob([
         ""jni/**/*.cc"",
         ""jni/**/*.h"",
    ]),
    copts = [ ""-fexceptions"", ""-DEIGEN_AVOID_STL_ARRAY"",
              ""-mfpu=neon"", ""-std=c++11"",
              ""-DMIN_LOG_LEVEL=0"", ""-DTF_LEAN_BINARY"",
              ""-O2"", ],
    linkopts = [
        ""-llog"",
        ""-lm"",
    ],
    linkshared = 1,
    deps = [
        ""@org_tensorflow//tensorflow/core:android_tensorflow_lib"",
        ""@boringssl//:crypto"",
    ],
)
```

Command
---
`bazel build -c opt //:libfoo.so --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --verbose_failures --sandbox_debug --strategy=CppLink=standalone`


Full error
---

```
...
  external/bazel_tools/tools/cpp/link_dynamic_library.sh no ignored ignored ignored external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64/bin/arm-linux-androideabi-gcc -shared -o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/libfoo.so -Wl,-whole-archive bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/external/org_tensorflow/tensorflow/core/libandroid_tensorflow_lib.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/external/org_tensorflow/tensorflow/core/kernels/libandroid_tensorflow_kernels.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/external/org_tensorflow/tensorflow/core/libandroid_tensorflow_lib_lite.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/external/org_tensorflow/tensorflow/core/libprotos_all_cc.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/external/protobuf/libprotobuf.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/external/protobuf/libprotobuf_lite.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/external/boringssl/libcrypto.a -Wl,-no-whole-archive external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/libgnustl_static.a external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/libsupc++.a -llog -lm -lz -lpthread -static-libgcc -no-canonical-prefixes '-march=armv7-a' -Wl,--fix-cortex-a8 '--sysroot=external/androidndk/ndk/platforms/android-21/arch-arm'): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64/bin/../lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: error: cannot find -lpthread
collect2: error: ld returned 1 exit status
Target //:libfoo.so failed to build

```
"
7287,SKFLOW/TFLearn SKCompat does not properly implement SKLearn predict and predict_proba,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

None seem to apply, this is a new issue just introduced with 1.0.0-rc1

### Environment info
Operating System:
Mac
Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:

export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.0.0rc1-py3-none-any.whl

2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
1.0.0-rc1
If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

TFLearn has incorrect implementations of ```predict``` and ```predict_proba```, per the [sklearn developer's guide](http://scikit-learn.org/stable/developers/contributing.html#coding-guidelines).  The current (v1.0.0-rc1) version's ```predict``` actually performs as would be expected for ```predict_proba``` and ```SKCompat``` does not include a ```predict_proba```.

SKFLOW/TFLearn's compatibility with sklearn has been broken in various ways since when v0.8 (which worked) was refactored into SKCompat (see #6584).  

The following code shows how the accuracy function fails due to this issue.  This could be fixed with an argmax call, but the core problem is that SKCompat does not properly implement the sklearn functions.

Note, this code is based on the Contrib TF Learn example here:  [https://www.tensorflow.org/tutorials/tflearn/](https://www.tensorflow.org/tutorials/tflearn/)

```
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
from sklearn import cross_validation
from sklearn import metrics
import tensorflow as tf

def main(unused_argv):
  # Load dataset.
  iris = tf.contrib.learn.datasets.load_dataset('iris')
  x_train, x_test, y_train, y_test = cross_validation.train_test_split(
      iris.data, iris.target, test_size=0.2, random_state=42)

  # Build 3 layer DNN with 10, 20, 10 units respectively.
  feature_columns = tf.contrib.learn.infer_real_valued_columns_from_input(
      x_train)
  classifier = tf.contrib.learn.DNNClassifier(
      feature_columns=feature_columns, hidden_units=[10, 20, 10], n_classes=3)
  classifier = tf.contrib.learn.SKCompat(classifier)

  # Fit and predict.
  classifier.fit(x_train, y_train, steps=200)

  # This outputs something with shape [30,3] should output single class predictions
  # See http://scikit-learn.org/stable/developers/contributing.html#coding-guidelines
  # See predict vs predict_proba
  #predictions = classifier.predict(x_test)
  #score = metrics.accuracy_score(y_test, predictions)
  #print('Accuracy: {0:f}'.format(score))

  # predict_proba does not work either:
  classifier.predict_proba(x_test)
  # Generates a: AttributeError: 'SKCompat' object has no attribute 'predict_proba'

if __name__ == '__main__':
  tf.app.run()
```

### What other attempted solutions have you tried?

http://scikit-learn.org/stable/developers/contributing.html#coding-guidelines

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).

```
/Users/jeff/anaconda/envs/tf-latest/bin/python /Users/jeff/Dropbox/school/teaching/wustl/classes/T81_558_deep_learning/code/test3.py
/Users/jeff/anaconda/envs/tf-latest/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  ""This module will be removed in 0.20."", DeprecationWarning)
WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.
WARNING:tensorflow:Using temporary folder as model directory: /var/folders/bs/_w74fpx157v3vs82q0fwnflr0000gn/T/tmpl7bwdnzy
WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.
WARNING:tensorflow:From /Users/jeff/anaconda/envs/tf-latest/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.
Instructions for updating:
Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.
Traceback (most recent call last):
  File ""/Users/jeff/Dropbox/school/teaching/wustl/classes/T81_558_deep_learning/code/test3.py"", line 52, in <module>
    tf.app.run()
  File ""/Users/jeff/anaconda/envs/tf-latest/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""/Users/jeff/Dropbox/school/teaching/wustl/classes/T81_558_deep_learning/code/test3.py"", line 44, in main
    score = metrics.accuracy_score(y_test, predictions)
  File ""/Users/jeff/anaconda/envs/tf-latest/lib/python3.5/site-packages/sklearn/metrics/classification.py"", line 172, in accuracy_score
    y_type, y_true, y_pred = _check_targets(y_true, y_pred)
  File ""/Users/jeff/anaconda/envs/tf-latest/lib/python3.5/site-packages/sklearn/metrics/classification.py"", line 72, in _check_targets
    check_consistent_length(y_true, y_pred)
  File ""/Users/jeff/anaconda/envs/tf-latest/lib/python3.5/site-packages/sklearn/utils/validation.py"", line 181, in check_consistent_length
    "" samples: %r"" % [int(l) for l in lengths])
ValueError: Found input variables with inconsistent numbers of samples: [30, 3]

Process finished with exit code 1
```"
7286,Upload sdists to PyPI,"Hi.

It appears that PyPI only contains wheels for tensorflow:

https://pypi.org/project/tensorflow/#files

Thanks for uploading those! Wheels are great.

But sdists are good to have too -- specifically, I'm not sure whether tensorflow works on PyPy (I'm about to try it out -- EDIT: now I've seen #252, which I might try to help on) but not having sdists uploaded makes that harder to try without hunting down this repository."
7285,AttributeError: module 'tensorflow' has no attribute 'Variable',"I run project from here=> https://github.com/david-gpu/deep-makeover
I installed tensorflow by using Anaconda, then I run python3 dm_main.py --run train 

Then I have this problem: 
File ""dm_main.py"", line 18, in <module>
    import dm_model
  File ""/Users/oteldanagul/Downloads/deep-makeover-master/dm_model.py"", line 5, in <module>
    import dm_arch
  File ""/Users/oteldanagul/Downloads/deep-makeover-master/dm_arch.py"", line 10, in <module>
    _glbl_is_training = tf.Variable(initial_value=True, trainable=False, name='glbl_is_training')
AttributeError: module 'tensorflow' has no attribute 'Variable'
"
7284,Any clear way to restore saved network and parameters?,"I'm using tensorflow-gpu0.12.1 on a linux-minit18 machine.

I found it is very hard to restore the saved network and its parameters. The closest answer is 

http://stackoverflow.com/questions/33759623/tensorflow-how-to-restore-a-previously-saved-model-python

and 

https://www.tensorflow.org/api_docs/python/state_ops/exporting_and_importing_meta_graphs

I followed the example in the above link using tf.train.import_meta_graph, but it didn't work.

```
with tf.Session() as sess:
  new_saver = tf.train.import_meta_graph('my-save-dir/my-model-10000.meta')
  new_saver.restore(sess, 'my-save-dir/my-model-10000')
```



All I want is something like:

training ...
save_model(model_file)
save_parameters(par_file)
...

predicting...
model = load_model(model_file)
model.load_parameters(par_file)
...

I found similar implementations in Keras or Tensorlayer, etc. I like them because they split the training and predicting tasks and make them independant. In predicting task, I don't have to find and call the same def_network function in training task, which is boring and error prone. 

@yaroslavvb says it is on the roadmap, please make it faster. I'd like to test it ASAP!

Many thanks!
Ben

"
7283,“module has no attribute 'learn' ”,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
https://github.com/tensorflow/tensorflow/issues/6607

### Environment info
Operating System:
 Windows 7 Enterprise

### If possible, provide a minimal reproducible example 

1. I have download and installed the windows docker with:
docker run -p 8888:8888 -e ""PASSWORD=****"" -d --name tensorflow b.gcr.io/tensorflow-udacity/assignments

2. I run the docker with a jupyter notebook:
docker start tensorflow (and then navigate: 192.168.99.100:8888)

3. The code fails at the beggining:
import tensorflow as tf
import numpy as np
import os
import time
import datetime
**from tensorflow.contrib import learn**

---------------------------------------------------------------------------
**ImportError**                               Traceback (most recent call last)
<ipython-input-6-9bbc1f0d7c04> in <module>()
      8 import datetime
      9 # import tensorflow.contrib as tfc
---> 10 from tensorflow.contrib import learn
     11 
     12 # Parameters

**ImportError: cannot import name learn**

### What other attempted solutions have you tried?
Reinstalling everything and changing tensorflow.contrib by tf.contrib.

What can I do?
Help is appreciated.
"
7281,Feature Request : Latent Dirichlet Allocation impementation using tensorflow(skflow),"I am trying to Topic Modelling (LDA) using tensorflow. But I could find any existing LDA implementation in TF learn (skflow) package. 

Is there any other way to implement Topic Modelling using tensorflow?

It would be helpful if LDA support is given in TF Learn (skflow) similar to the scikit-learn Latent Dirichlet Implementation."
7278,Feature request: separable convolutions in 3D,"I would like a `tf.nn.separable_conv3d` identical to `tf.nn.separable_conv2d` except with separability between dimentions [1,2,3] and 4, for use with 3D CNNs.

Rationale: separable convolutions perform very well in 2D (see: Xception architecture https://arxiv.org/abs/1610.02357 ).  In 3D, the number of parameters grows even faster for non-separable convolutions, so the reduction in parameters from using separable convolutions would be relatively even bigger.  This is one of the reasons 3D networks tend to have a simpler architecture than 2D.  

Necessity: There doesn't seem to be any way to implement this other than in the TF core. (Suggestions?)"
7276,"Weights are nan, loss is computed fine and gradients are too small","I have been researching about this over the INTERNET for a very long time. But I still do not know where my problem lies. I am finetuning a siamese fashioned vgg. And the problem is that even after first iteration distance calcualted is fine, loss calculated is fine but the gradients are too small(which is a problem) and the weights that are updated are nan. I do not understand what the problem is. 
In the network, I have removed fully connected layers and the output from the last pooling layer is the input to the distance function. Here is my code. I have only attached important snippets of the code which I hope help understand the architecture. 
PS: stackoverflow was of no help that is why I am posting it here. 
PPS: I have attached snapshot of gradients as well
![screenshot from 2017-02-06 02-00-10](https://cloud.githubusercontent.com/assets/25512256/22630753/60a8760a-ec10-11e6-9a1f-d75ba573842b.png)


[code.txt](https://github.com/tensorflow/tensorflow/files/753375/code.txt)


"
7274,Is it possible to call TensorBoard smooth function manually?,"I have two arrays X and Y. 

Is there a function I can call in tensorboard to do the smooth? 

Right now I can do an alternative way in python like:
```python
    sav_smoooth = savgol_filter(Y, 51, 3)
    plt.plot(X, Y)
```
But I didn't get a similar results as shown in tensorboard by doing so. Is there a function I can call?  Could you provide an API that allow this kind of operation? 

Thanks. "
7272,CUDA_ERROR_OUT_OF_MEMORY,"i tried to excute chabot application (DeepQA). but i found out error.
this is the problem screen.....

E tensorflow/core/common_runtime/direct_session.cc:135] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 18446744073648275456
Traceback (most recent call last):
  File ""main.py"", line 29, in <module>
    chatbot.main()
  File ""/home/deepl/Desktop/chatbot/DeepQA-master/chatbot/chatbot.py"", line 179, in main
    log_device_placement=False)  # Too verbose ?
  File ""/home/deepl/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1186, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File ""/home/deepl/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 551, in __init__
    self._session = tf_session.TF_NewDeprecatedSession(opts, status)
  File ""/usr/lib/python3.5/contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""/home/deepl/.local/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 469, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.


i don't know how to solve this problem.. 
if you know to solve this problem, please comment for me. thank you for reading."
7270,"Failure in ""Linking CXX shared library libpywrap_tensorflow.so"" , with error info ""additional relocation overflows omitted from the output""","I am using Cmake 3.5.1 to compile sourcecode in Ubuntu 16.04 with a ppc64le machine ( Power64 little endian ).  Python 2.7.12.  The compile is failed in  ""Linking CXX shared library libpywrap_tensorflow.so"". The error info is as bellows.   I try compiling source code both 0.10.0 and 0.12.0,  and get the same error.
Could you give me a help please ? Thanks


[ 98%] Running SWIG to generate Python wrappers
[ 98%] Building CXX object CMakeFiles/pywrap_tensorflow.dir/pywrap_tensorflow.cc.o
[ 98%] Linking CXX shared library libpywrap_tensorflow.so
CMakeFiles/tf_core_kernels.dir/home/gaoshan/software/pkgbuild/tensorflow/tensorflow/tensorflow/core/kernels/tensor_array_ops.cc.o: In function `std::unique_ptr<Eigen::TensorMap<Eigen::Tensor<bool const, 2, 1, long>, 16, Eigen::MakePointer>, std::default_delete<Eigen::TensorMap<Eigen::Tensor<bool const, 2, 1, long>, 16, Eigen::MakePointer> > >::get_deleter()':
tensor_array_ops.cc:(.text._ZNSt10unique_ptrIN5Eigen9TensorMapINS0_6TensorIKbLi2ELi1ElEELi16ENS0_11MakePointerEEESt14default_deleteIS6_EE11get_deleterEv[_ZNSt10unique_ptrIN5Eigen9TensorMapINS0_6TensorIKbLi2ELi1ElEELi16ENS0_11MakePointerEEESt14default_deleteIS6_EE11get_deleterEv]+0x28): relocation truncated to fit: R_PPC64_REL24 (stub) against symbol `std::tuple_element<1ul, std::tuple<Eigen::TensorMap<Eigen::Tensor<bool const, 2, 1, long>, 16, Eigen::MakePointer>*, std::default_delete<Eigen::TensorMap<Eigen::Tensor<bool const, 2, 1, long>, 16, Eigen::MakePointer> > > >::type& std::get<1ul, Eigen::TensorMap<Eigen::Tensor<bool const, 2, 1, long>, 16, Eigen::MakePointer>*, std::default_delete<Eigen::TensorMap<Eigen::Tensor<bool const, 2, 1, long>, 16, Eigen::MakePointer> > >(std::tuple<Eigen::TensorMap<Eigen::Tensor<bool const, 2, 1, long>, 16, Eigen::MakePointer>*, std::default_delete<Eigen::TensorMap<Eigen::Tensor<bool const, 2, 1, long>, 16, Eigen::MakePointer> > >&)' defined in .text._ZSt3getILm1EJPN5Eigen9TensorMapINS0_6TensorIKbLi2ELi1ElEELi16ENS0_11MakePointerEEESt14default_deleteIS6_EEERNSt13tuple_elementIXT_ESt5tupleIJDpT0_EEE4typeERSE_[_ZSt3getILm1EJPN5Eigen9TensorMapINS0_6TensorIKbLi2ELi1ElEELi16ENS0_11MakePointerEEESt14default_deleteIS6_EEERNSt13tuple_elementIXT_ESt5tupleIJDpT0_EEE4typeERSE_] section in CMakeFiles/tf_core_kernels.dir/home/gaoshan/software/pkgbuild/tensorflow/tensorflow/tensorflow/core/kernels/tensor_array_ops.cc.o
CMakeFiles/tf_core_kernels.dir/home/gaoshan/software/pkgbuild/tensorflow/tensorflow/tensorflow/core/kernels/tensor_array_ops.cc.o: In function `std::default_delete<Eigen::TensorMap<Eigen::Tensor<bool const, 2, 1, long>, 16, Eigen::MakePointer> >::operator()(Eigen::TensorMap<Eigen::Tensor<bool const, 2, 1, long>, 16, Eigen::MakePointer>*) const':
tensor_array_ops.cc:(.text._ZNKSt14default_deleteIN5Eigen9TensorMapINS0_6TensorIKbLi2ELi1ElEELi16ENS0_11MakePointerEEEEclEPS6_[_ZNKSt14default_deleteIN5Eigen9TensorMapINS0_6TensorIKbLi2ELi1ElEELi16ENS0_11MakePointerEEEEclEPS6_]+0x2c): relocation truncated to fit: R_PPC64_REL24 (stub) against symbol `operator delete(void*, unsigned long)@@CXXABI_1.3.9' defined in .text section in /usr/lib/gcc/powerpc64le-linux-gnu/5/libstdc++.so
CMakeFiles/tf_core_kernels.dir/home/gaoshan/software/pkgbuild/tensorflow/tensorflow/tensorflow/core/kernels/tensor_array_ops.cc.o: In function `std::default_delete<Eigen::TensorMap<Eigen::Tensor<std::complex<double> const, 2, 1, long>, 16, Eigen::MakePointer> >::operator()(Eigen::TensorMap<Eigen::Tensor<std::complex<double> const, 2, 1, long>, 16, Eigen::MakePointer>*) const':
tensor_array_ops.cc:(.text._ZNKSt14default_deleteIN5Eigen9TensorMapINS0_6TensorIKSt7complexIdELi2ELi1ElEELi16ENS0_11MakePointerEEEEclEPS8_[_ZNKSt14default_deleteIN5Eigen9TensorMapINS0_6TensorIKSt7complexIdELi2ELi1ElEELi16ENS0_11MakePointerEEEEclEPS8_]+0x2c): relocation truncated to fit: R_PPC64_REL24 (stub) against symbol `operator delete(void*, unsigned long)@@CXXABI_1.3.9' defined in .text section in /usr/lib/gcc/powerpc64le-linux-gnu/5/libstdc++.so
CMakeFiles/tf_core_kernels.dir/home/gaoshan/software/pkgbuild/tensorflow/tensorflow/tensorflow/core/kernels/tensor_array_ops.cc.o: In function `std::default_delete<Eigen::TensorMap<Eigen::Tensor<std::complex<float> const, 2, 1, long>, 16, Eigen::MakePointer> >::operator()(Eigen::TensorMap<Eigen::Tensor<std::complex<float> const, 2, 1, long>, 16, Eigen::MakePointer>*) const':
tensor_array_ops.cc:(.text._ZNKSt14default_deleteIN5Eigen9TensorMapINS0_6TensorIKSt7complexIfELi2ELi1ElEELi16ENS0_11MakePointerEEEEclEPS8_[_ZNKSt14default_deleteIN5Eigen9TensorMapINS0_6TensorIKSt7complexIfELi2ELi1ElEELi16ENS0_11MakePointerEEEEclEPS8_]+0x2c): relocation truncated to fit: R_PPC64_REL24 (stub) against symbol `operator delete(void*, unsigned long)@@CXXABI_1.3.9' defined in .text section in /usr/lib/gcc/powerpc64le-linux-gnu/5/libstdc++.so
CMakeFiles/tf_core_kernels.dir/home/gaoshan/software/pkgbuild/tensorflow/tensorflow/tensorflow/core/kernels/tensor_array_ops.cc.o: In function `std::_Tuple_impl<1ul, std::default_delete<Eigen::TensorMap<Eigen::Tensor<double const, 2, 1, long>, 16, Eigen::MakePointer> > >::_M_head(std::_Tuple_impl<1ul, std::default_delete<Eigen::TensorMap<Eigen::Tensor<double const, 2, 1, long>, 16, Eigen::MakePointer> > >&)':
tensor_array_ops.cc:(.text._ZNSt11_Tuple_implILm1EJSt14default_deleteIN5Eigen9TensorMapINS1_6TensorIKdLi2ELi1ElEELi16ENS1_11MakePointerEEEEEE7_M_headERS9_[_ZNSt11_Tuple_implILm1EJSt14default_deleteIN5Eigen9TensorMapINS1_6TensorIKdLi2ELi1ElEELi16ENS1_11MakePointerEEEEEE7_M_headERS9_]+0x24): relocation truncated to fit: R_PPC64_REL24 (stub) against symbol `std::_Head_base<1ul, std::default_delete<Eigen::TensorMap<Eigen::Tensor<double const, 2, 1, long>, 16, Eigen::MakePointer> >, true>::_M_head(std::_Head_base<1ul, std::default_delete<Eigen::TensorMap<Eigen::Tensor<double const, 2, 1, long>, 16, Eigen::MakePointer> >, true>&)' defined in .text._ZNSt10_Head_baseILm1ESt14default_deleteIN5Eigen9TensorMapINS1_6TensorIKdLi2ELi1ElEELi16ENS1_11MakePointerEEEELb1EE7_M_headERS9_[_ZNSt10_Head_baseILm1ESt14default_deleteIN5Eigen9TensorMapINS1_6TensorIKdLi2ELi1ElEELi16ENS1_11MakePointerEEEELb1EE7_M_headERS9_] section in CMakeFiles/tf_core_kernels.dir/home/gaoshan/software/pkgbuild/tensorflow/tensorflow/tensorflow/core/kernels/tensor_array_ops.cc.o
CMakeFiles/tf_core_kernels.dir/home/gaoshan/software/pkgbuild/tensorflow/tensorflow/tensorflow/core/kernels/tensor_array_ops.cc.o: In function `std::default_delete<Eigen::TensorMap<Eigen::Tensor<double const, 2, 1, long>, 16, Eigen::MakePointer> >::operator()(Eigen::TensorMap<Eigen::Tensor<double const, 2, 1, long>, 16, Eigen::MakePointer>*) const':
tensor_array_ops.cc:(.text._ZNKSt14default_deleteIN5Eigen9TensorMapINS0_6TensorIKdLi2ELi1ElEELi16ENS0_11MakePointerEEEEclEPS6_[_ZNKSt14default_deleteIN5Eigen9TensorMapINS0_6TensorIKdLi2ELi1ElEELi16ENS0_11MakePointerEEEEclEPS6_]+0x2c): relocation truncated to fit: R_PPC64_REL24 (stub) against symbol `operator delete(void*, unsigned long)@@CXXABI_1.3.9' defined in .text section in /usr/lib/gcc/powerpc64le-linux-gnu/5/libstdc++.so
CMakeFiles/tf_core_kernels.dir/home/gaoshan/software/pkgbuild/tensorflow/tensorflow/tensorflow/core/kernels/tensor_array_ops.cc.o: In function `std::default_delete<Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> >::operator()(Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer>*) const':
tensor_array_ops.cc:(.text._ZNKSt14default_deleteIN5Eigen9TensorMapINS0_6TensorIKfLi2ELi1ElEELi16ENS0_11MakePointerEEEEclEPS6_[_ZNKSt14default_deleteIN5Eigen9TensorMapINS0_6TensorIKfLi2ELi1ElEELi16ENS0_11MakePointerEEEEclEPS6_]+0x2c): relocation truncated to fit: R_PPC64_REL24 (stub) against symbol `operator delete(void*, unsigned long)@@CXXABI_1.3.9' defined in .text section in /usr/lib/gcc/powerpc64le-linux-gnu/5/libstdc++.so
CMakeFiles/tf_core_kernels.dir/home/gaoshan/software/pkgbuild/tensorflow/tensorflow/tensorflow/core/kernels/tensor_array_ops.cc.o: In function `std::default_delete<Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer> >::operator()(Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer>*) const':
tensor_array_ops.cc:(.text._ZNKSt14default_deleteIN5Eigen9TensorMapINS0_6TensorIKNS0_4halfELi2ELi1ElEELi16ENS0_11MakePointerEEEEclEPS7_[_ZNKSt14default_deleteIN5Eigen9TensorMapINS0_6TensorIKNS0_4halfELi2ELi1ElEELi16ENS0_11MakePointerEEEEclEPS7_]+0x2c): relocation truncated to fit: R_PPC64_REL24 (stub) against symbol `operator delete(void*, unsigned long)@@CXXABI_1.3.9' defined in .text section in /usr/lib/gcc/powerpc64le-linux-gnu/5/libstdc++.so
CMakeFiles/tf_core_kernels.dir/home/gaoshan/software/pkgbuild/tensorflow/tensorflow/tensorflow/core/kernels/tensor_array_ops.cc.o: In function `std::default_delete<Eigen::TensorMap<Eigen::Tensor<signed char const, 2, 1, long>, 16, Eigen::MakePointer> >& std::__get_helper<1ul, std::default_delete<Eigen::TensorMap<Eigen::Tensor<signed char const, 2, 1, long>, 16, Eigen::MakePointer> >>(std::_Tuple_impl<1ul, std::default_delete<Eigen::TensorMap<Eigen::Tensor<signed char const, 2, 1, long>, 16, Eigen::MakePointer> >>&)':
tensor_array_ops.cc:(.text._ZSt12__get_helperILm1ESt14default_deleteIN5Eigen9TensorMapINS1_6TensorIKaLi2ELi1ElEELi16ENS1_11MakePointerEEEEJEERT0_RSt11_Tuple_implIXT_EJS9_DpT1_EE[_ZSt12__get_helperILm1ESt14default_deleteIN5Eigen9TensorMapINS1_6TensorIKaLi2ELi1ElEELi16ENS1_11MakePointerEEEEJEERT0_RSt11_Tuple_implIXT_EJS9_DpT1_EE]+0x24): relocation truncated to fit: R_PPC64_REL24 (stub) against symbol `std::_Tuple_impl<1ul, std::default_delete<Eigen::TensorMap<Eigen::Tensor<signed char const, 2, 1, long>, 16, Eigen::MakePointer> > >::_M_head(std::_Tuple_impl<1ul, std::default_delete<Eigen::TensorMap<Eigen::Tensor<signed char const, 2, 1, long>, 16, Eigen::MakePointer> > >&)' defined in .text._ZNSt11_Tuple_implILm1EJSt14default_deleteIN5Eigen9TensorMapINS1_6TensorIKaLi2ELi1ElEELi16ENS1_11MakePointerEEEEEE7_M_headERS9_[_ZNSt11_Tuple_implILm1EJSt14default_deleteIN5Eigen9TensorMapINS1_6TensorIKaLi2ELi1ElEELi16ENS1_11MakePointerEEEEEE7_M_headERS9_] section in CMakeFiles/tf_core_kernels.dir/home/gaoshan/software/pkgbuild/tensorflow/tensorflow/tensorflow/core/kernels/tensor_array_ops.cc.o
CMakeFiles/tf_core_kernels.dir/home/gaoshan/software/pkgbuild/tensorflow/tensorflow/tensorflow/core/kernels/tensor_array_ops.cc.o: In function `std::default_delete<Eigen::TensorMap<Eigen::Tensor<signed char const, 2, 1, long>, 16, Eigen::MakePointer> >::operator()(Eigen::TensorMap<Eigen::Tensor<signed char const, 2, 1, long>, 16, Eigen::MakePointer>*) const':
tensor_array_ops.cc:(.text._ZNKSt14default_deleteIN5Eigen9TensorMapINS0_6TensorIKaLi2ELi1ElEELi16ENS0_11MakePointerEEEEclEPS6_[_ZNKSt14default_deleteIN5Eigen9TensorMapINS0_6TensorIKaLi2ELi1ElEELi16ENS0_11MakePointerEEEEclEPS6_]+0x2c): relocation truncated to fit: R_PPC64_REL24 (stub) against symbol `operator delete(void*, unsigned long)@@CXXABI_1.3.9' defined in .text section in /usr/lib/gcc/powerpc64le-linux-gnu/5/libstdc++.so
CMakeFiles/tf_core_kernels.dir/home/gaoshan/software/pkgbuild/tensorflow/tensorflow/tensorflow/core/kernels/tensor_array_ops.cc.o: In function `void std::_Construct<std::unique_ptr<Eigen::TensorMap<Eigen::Tensor<unsigned char const, 2, 1, long>, 16, Eigen::MakePointer>, std::default_delete<Eigen::TensorMap<Eigen::Tensor<unsigned char const, 2, 1, long>, 16, Eigen::MakePointer> > >, std::unique_ptr<Eigen::TensorMap<Eigen::Tensor<unsigned char const, 2, 1, long>, 16, Eigen::MakePointer>, std::default_delete<Eigen::TensorMap<Eigen::Tensor<unsigned char const, 2, 1, long>, 16, Eigen::MakePointer> > > >(std::unique_ptr<Eigen::TensorMap<Eigen::Tensor<unsigned char const, 2, 1, long>, 16, Eigen::MakePointer>, std::default_delete<Eigen::TensorMap<Eigen::Tensor<unsigned char const, 2, 1, long>, 16, Eigen::MakePointer> > >*, std::unique_ptr<Eigen::TensorMap<Eigen::Tensor<unsigned char const, 2, 1, long>, 16, Eigen::MakePointer>, std::default_delete<Eigen::TensorMap<Eigen::Tensor<unsigned char const, 2, 1, long>, 16, Eigen::MakePointer> > >&&)':
tensor_array_ops.cc:(.text._ZSt10_ConstructISt10unique_ptrIN5Eigen9TensorMapINS1_6TensorIKhLi2ELi1ElEELi16ENS1_11MakePointerEEESt14default_deleteIS7_EEJSA_EEvPT_DpOT0_[_ZSt10_ConstructISt10unique_ptrIN5Eigen9TensorMapINS1_6TensorIKhLi2ELi1ElEELi16ENS1_11MakePointerEEESt14default_deleteIS7_EEJSA_EEvPT_DpOT0_]+0x60): **additional relocation overflows omitted from the output
collect2: error: ld returned 1 exit status
CMakeFiles/pywrap_tensorflow.dir/build.make:1711: recipe for target 'libpywrap_tensorflow.so' failed
make[2]: *** [libpywrap_tensorflow.so] Error 1
CMakeFiles/Makefile2:6759: recipe for target 'CMakeFiles/pywrap_tensorflow.dir/all' failed
make[1]: *** [CMakeFiles/pywrap_tensorflow.dir/all] Error 2
Makefile:83: recipe for target 'all' failed**"
7268,C++ compilation of rule '@jemalloc//:jemalloc' failed:,"Trying to install tensor flow on cluster from source. I have installed bazel
[bazel release 0.4.4- (@non-git)], and I am using python 2.7.13 with pyenv. Upon trying to build the tensorflow pip wheel I am getting a compilation error:

ERROR: ~/.cache/bazel/_bazel/e924d9c3ba75314415252c6f4f93bb86/external/jemalloc/BUILD:10:1: C++ compilation of rule '@jemalloc//:jemalloc' failed: gcc failed: error executing command /opt/apps/compilers/gcc/4.8.2/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/opt/apps/compilers/gcc/4.8.2/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object ... (remaining 38 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.

Has anyone experienced this? Is this a consequence of the warning I get from bazel for being on an NFS:
WARNING: Output base '~/.cache/bazel/_bazel/e924d9c3ba75314415252c6f4f93bb86' is on NFS. This may lead to surprising failures and undetermined behaviour.


My gcc version is:
gcc (GCC) 4.8.2
Copyright (C) 2013 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
"
7266,undefined reference to `google::protobuf::internal::fixed_address_empty_string',"![error_2_4](https://cloud.githubusercontent.com/assets/14129554/22622882/bd9c80a0-eb15-11e6-9ee1-3269efd7e0bd.png)

Hello,
I am tring to do install core of tensorflow on Raspberry PI 3, on Ubuntu 16.04 LTS using python 3.5.2, exactly following the instruction on https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile, but got error messages at last step ""make -f tensorflow/contrib/makefile/Makefile HOST_OS=PI TARGET=PI OPTFLAGS=""-Os"" CXX=g++-4.8"". 

Note that I tried the method shared on  https://github.com/samjabrahams/tensorflow-on-raspberry-pi, but there are also error occured. 

I am wondering is it due to some update of tensorflow? Looking forward to your reply. Thank you!"
7261,"""No Android SDK found"" error when building Android demo app","I'm only following [the README instructions](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/README.md) to build the Android demo app.

I believe I have configured my ./WORKSPACE file correctly.  This seems to be a different error to the obvious one you get if you haven't update the WORKSPACE file...

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

http://stackoverflow.com/q/41478820/112705
[No matching open or closed Issues in this Git project](https://github.com/tensorflow/tensorflow/issues?utf8=%E2%9C%93&q=is%3Aissue%20%22No%20Android%20SDK%20found%22%20is%3Aclosed%20
) (I tried various Android build searches too and they seemed to be different problems):

### Environment info
Operating System: Mac OS X 10.10.5

Installed version of CUDA and cuDNN: None

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
```
19d932f4cb49182c111d646267ac6c0bbc4f2f00
```
I cloned (with submodules) today, so this is the current master code.

2. The output of `bazel version`
```
Build label: 0.4.4-homebrew
Build target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Feb 2 01:06:38 2017 (1485997598)
Build timestamp: 1485997598
Build timestamp as int: 1485997598
```

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

Here's the error:

> Daniels-MacBook-Air:tensorflow Dan$ bazel build //tensorflow/examples/android:tensorflow_demo
> ...
> <snip>
> ...
> ERROR: /private/var/tmp/_bazel_Dan/e9cc2fe61f735d74f6a33af534dffc47/external/bazel_tools/src/tools/android/java/com/google/devtools/build/android/incrementaldeployment/BUILD:3:1: in android_library rule @bazel_tools//src/tools/android/java/com/google/devtools/build/android/incrementaldeployment:incremental_stub_application: **No Android SDK found. Use the --android_sdk command line option to specify one.**

Here's the relevant section of my WORKSPACE file (Bazel 0.4.4 seems to require Android build tools 24.0.3 or newer, which is why I updated that too):
```
android_sdk_repository(
    name = ""androidsdk"",
    api_level = 23,
    build_tools_version = ""24.0.3"",
    # Replace with path to Android SDK on your system
    path = ""/Developer/Android/sdk/"",
)

android_ndk_repository(
    name=""androidndk"",
    path=""/Developer/Android/android-ndk-r12b"",
    api_level=21)
```
### What other attempted solutions have you tried?

I tried the path both with and without the trailing slash.  

I investigated how to pass the `--android_sdk` option to bazel based on [this Bazel documentation](https://bazel.build/versions/master/docs/command-line-reference.html):
```
--android_sdk=<a build target label> default: ""@bazel_tools//tools/android:sdk""
Specifies Android SDK/platform that is used to build Android applications.
```
I couldn't figure out how to pass in that setting (plus I suspect it is supposed to come from the WORKSPACE file anyway).  Here's what I tried:
```
Daniels-MacBook-Air:tensorflow Dan$ bazel build --android_sdk=/Developer/Android/sdk //tensorflow/examples/android:tensorflow_demo
While parsing option --android_sdk=/Developer/Android/sdk: invalid label: /Developer/Android/sdk

Daniels-MacBook-Air:tensorflow Dan$ bazel build //tensorflow/examples/android:tensorflow_demo --android_sdk=/Developer/Android/sdk
While parsing option --android_sdk=/Developer/Android/sdk: invalid label: /Developer/Android/sdk
```

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).

Full logs are here:
```
Daniels-MacBook-Air:tensorflow Dan$ bazel build //tensorflow/examples/android:tensorflow_demo
WARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.build/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.
WARNING: /Users/Dan/Code/tensorflow/tensorflow/core/BUILD:826:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:avgpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/Dan/Code/tensorflow/tensorflow/core/BUILD:826:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:bounds_check.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/Dan/Code/tensorflow/tensorflow/core/BUILD:826:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/Dan/Code/tensorflow/tensorflow/core/BUILD:826:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/Dan/Code/tensorflow/tensorflow/core/BUILD:826:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_gradients.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/Dan/Code/tensorflow/tensorflow/core/BUILD:826:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_activations.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/Dan/Code/tensorflow/tensorflow/core/BUILD:826:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_attention.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/Dan/Code/tensorflow/tensorflow/core/BUILD:826:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_cuboid_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/Dan/Code/tensorflow/tensorflow/core/BUILD:826:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/Dan/Code/tensorflow/tensorflow/core/BUILD:826:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_cuboid_convolution.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/Dan/Code/tensorflow/tensorflow/core/BUILD:826:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_patch_3d.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/Dan/Code/tensorflow/tensorflow/core/BUILD:826:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_pooling.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/Dan/Code/tensorflow/tensorflow/core/BUILD:826:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_softmax.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/Dan/Code/tensorflow/tensorflow/core/BUILD:826:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/Dan/Code/tensorflow/tensorflow/core/BUILD:826:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/Dan/Code/tensorflow/tensorflow/core/BUILD:826:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:maxpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/Dan/Code/tensorflow/tensorflow/core/BUILD:826:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.cc' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/Dan/Code/tensorflow/tensorflow/core/BUILD:826:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/Dan/Code/tensorflow/tensorflow/core/BUILD:826:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:padding_fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/Dan/Code/tensorflow/tensorflow/core/BUILD:826:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.cc' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/Dan/Code/tensorflow/tensorflow/core/BUILD:826:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/Dan/Code/tensorflow/tensorflow/core/BUILD:826:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_base.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/Dan/Code/tensorflow/tensorflow/core/BUILD:826:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/Dan/Code/tensorflow/tensorflow/core/BUILD:826:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:typed_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/Dan/Code/tensorflow/tensorflow/core/BUILD:826:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_entry.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/Dan/Code/tensorflow/tensorflow/core/BUILD:826:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_scorer.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/Dan/Code/tensorflow/tensorflow/core/BUILD:826:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_search.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/Dan/Code/tensorflow/tensorflow/core/BUILD:826:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_decoder.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/Dan/Code/tensorflow/tensorflow/core/BUILD:826:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_loss_util.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/Dan/Code/tensorflow/tensorflow/core/BUILD:826:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.cc' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/Dan/Code/tensorflow/tensorflow/core/BUILD:826:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/Dan/Code/tensorflow/tensorflow/core/BUILD:826:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.cc' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/Dan/Code/tensorflow/tensorflow/core/BUILD:826:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.h' directly. You should either move the file to this package or depend on an appropriate rule there.
ERROR: /private/var/tmp/_bazel_Dan/e9cc2fe61f735d74f6a33af534dffc47/external/bazel_tools/src/tools/android/java/com/google/devtools/build/android/incrementaldeployment/BUILD:3:1: in android_library rule @bazel_tools//src/tools/android/java/com/google/devtools/build/android/incrementaldeployment:incremental_stub_application: No Android SDK found. Use the --android_sdk command line option to specify one.
ERROR: Analysis of target '//tensorflow/examples/android:tensorflow_demo' failed; build aborted.
INFO: Elapsed time: 7.929s
```
"
7260,load_op_library don't see Ops,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
http://stackoverflow.com/questions/42019818/tensorflow-load-op-library-load-incorrectly?noredirect=1#comment71258436_42019818
### Environment info
Operating System: Linux Gentoo

Installed version of CUDA and cuDNN: 
-rw-r--r-- 1 root root 558632 Dec 28 08:08 /opt/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root     16 Dec 28 08:08 /opt/cuda/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root     19 Dec 28 08:08 /opt/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44
-rwxr-xr-x 1 root root 415432 Sep 15 00:05 /opt/cuda/lib64/libcudart.so.8.0.44
-rw-r--r-- 1 root root 774168 Dec 28 08:08 /opt/cuda/lib64/libcudart_static.a

If installed from binary pip package, provide:
v 0.12.0

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
_actv_module = tf.load_op_library('<path>/actv.so')
print(_actv_module.OP_LIST)

When act.so situated in some directories I see the list of Ops in it (as it should be).
When I copy it to another directories list of Ops is empty. I can't find any reason why some directories are different from other. In both cases path is correct -- load_op_library is able to find file.

### What other attempted solutions have you tried?
I tried to copy source files to ""problem"" directories and compile it there (according to Adding new Op manual, compile with GPU support) -- with same results.
More strange -- I made a copy of all source files with another names, compiled it in ""normal"" directory, and load_op_library can't find Ops in it!

### Logs or other output that would be helpful
Attached: sources -- actv.cc, actv_kernel.cu.cc
Normal libs -- actv.so, actv_kernel.cu.o
""Corrupted"" libs (in which load_op_library can't see any ops) -- actv_corrupted.so, actv_kernel_corrupted.cu.o
[Archive.zip](https://github.com/tensorflow/tensorflow/files/752539/Archive.zip)





"
7259,Configure script optimization flags don't work,"The default optimization switches that are set when running the configure script do not change the compiler settings.  The values are put into the bazel.rc file.  They don't get into the gcc command line.  For example if I run ./configure with all of the default answers the optimization should be ""-march=native"".  I have tested this on multiple Ubuntu 16.04 systems with don't have CUDA.

If I then build the pip package with 'bazel build --c opt -s  ...' one example compile is this:

> /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/local-opt/bin/external/farmhash_archive/_objs/farmhash/external/farmhash_archive/src/farmhash.pic.d '-frandom-seed=bazel-out/local-opt/bin/external/farmhash_archive/_objs/farmhash/external/farmhash_archive/src/farmhash.pic.o' -fPIC -iquote external/farmhash_archive -iquote bazel-out/local-opt/genfiles/external/farmhash_archive -iquote external/bazel_tools -iquote bazel-out/local-opt/genfiles/external/bazel_tools -isystem external/farmhash_archive/src -isystem bazel-out/local-opt/genfiles/external/farmhash_archive/src -isystem external/bazel_tools/tools/cpp/gcc3 -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/farmhash_archive/src/farmhash.cc -o bazel-out/local-opt/bin/external/farmhash_archive/_objs/farmhash/external/farmhash_archive/src/farmhash.pic.o)

If I explicitly set the compile options with 'bazel build --c opt --copts=-march=native -s  ...' I get the correct results:

> /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections **'-march=native'** '-std=c++0x' -MD -MF bazel-out/local-opt/bin/external/farmhash_archive/_objs/farmhash/external/farmhash_archive/src/farmhash.pic.d '-frandom-seed=bazel-out/local-opt/bin/external/farmhash_archive/_objs/farmhash/external/farmhash_archive/src/farmhash.pic.o' -fPIC -iquote external/farmhash_archive -iquote bazel-out/local-opt/genfiles/external/farmhash_archive -iquote external/bazel_tools -iquote bazel-out/local-opt/genfiles/external/bazel_tools -isystem external/farmhash_archive/src -isystem bazel-out/local-opt/genfiles/external/farmhash_archive/src -isystem external/bazel_tools/tools/cpp/gcc3 -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/farmhash_archive/src/farmhash.cc -o bazel-out/local-opt/bin/external/farmhash_archive/_objs/farmhash/external/farmhash_archive/src/farmhash.pic.o)

The performance is slower for the first example.  Somehow the optimizations in bazel.rc are not getting through to the compiler command line."
7258,Building a .dll on Windows,"I have built libtensorflow.so with Bazel and used it successfully with the C Api in Ubuntu, however I was wondering if it is possible at all to use Bazel to build a .dll extension to use for Windows? As I want to integrate TensorFlow into an existing project. 

Thanks in advance"
7257,Feature request: Please provide AVX2/FMA capable builds,"I would go out on a limb and guess that the vast majority of Tensorflow users on Linux at least use fairly modern CPUs. It would therefore be beneficial for them to have the prebuilt TF binaries support AVX2/FMA. These two ISA extensions, and especially FMA, tend to speed up GEMM-like math pretty significantly.

It'd be great if TF team provided prebuilt Linux release *.whl that supports AVX2/FMA, perhaps as an alternative, non-default wheel. These should be compatible with Haswell and above. Haswell came out in 2013, lots of people have it by now.

To be clear, this is not a hugely pressing issue, `*.whl` can be easily rebuilt from source. It'd just make things faster and easier for people with modern CPUs.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

N/A

### Environment info
Operating System:
Linux Ubuntu 16.04

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`): NONE

If installed from binary pip package, provide:

1. A link to the pip package you installed: `https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.0rc0-cp35-cp35m-linux_x86_64.whl`
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`: `1.0.0-rc0`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

Code:
```
import tensorflow as tf
sess = tf.InteractiveSession()
```

Output:
```
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
```

### What other attempted solutions have you tried?

Compiled from source.

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
7256,"couldn't find ""libtensorflow_demo.so""","Hi All: 
  I'm a newbie on trying out tensorflow in Android,
by following the guildline to build the libtensorflow_demo.so,  I new a folder ""libs"" to put this so file in folder ""tensorflow/examples/android/libs""

    sourceSets {
        main {
            manifest.srcFile 'AndroidManifest.xml'
            java.srcDirs = ['src', '../../contrib/android/java']
            resources.srcDirs = ['src']
            aidl.srcDirs = ['src']
            renderscript.srcDirs = ['src']
            res.srcDirs = ['res']
            assets.srcDirs = ['assets']
            jniLibs.srcDirs = ['libs']
        }

however when app's running, it still can not find this lib.
any idea for resolving is appreciated

### Environment info
Operating System: Mac (CPU)

Process: org.tensorflow.demo, PID: 2401
                                                                   java.lang.UnsatisfiedLinkError: com.android.tools.fd.runtime.IncrementalClassLoader$DelegateClassLoader[DexPathList[[dex file ""/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_9-classes.dex"", dex file ""/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_8-classes.dex"", dex file ""/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_7-classes.dex"", dex file ""/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_6-classes.dex"", dex file ""/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_5-classes.dex"", dex file ""/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_4-classes.dex"", dex file ""/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_3-classes.dex"", dex file ""/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_2-classes.dex"", dex file ""/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_1-classes.dex"", dex file ""/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_0-classes.dex""],nativeLibraryDirectories=[/data/app/org.tensorflow.demo-1/lib/x86_64, /system/lib64, /vendor/lib64]]] couldn't find ""libtensorflow_demo.so""
                                                                       at java.lang.Runtime.loadLibrary0(Runtime.java:984)
                                                                       at java.lang.System.loadLibrary(System.java:1530)
"
7255,Multi-GPU training drastically slow after recent commit,"I recently upgraded to the latest master, in order to fix the issues caused by this bug: https://github.com/tensorflow/tensorflow/issues/7038
Though the above fix does resolve the issue of same batch being pulled from the GPU, now my model training has become drastically slow (> 4x), and most time is being spent in dequeues. A timeline snapshot is here:

![screenshot from 2017-02-03 18-43-55](https://cloud.githubusercontent.com/assets/1893429/22612749/be748f0c-ea40-11e6-9efb-989b9466301a.png)

Any ideas as to what might be going wrong?

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
https://github.com/tensorflow/tensorflow/issues/7038

### Environment info
Operating System: CentOS 6.5

Installed version of CUDA and cuDNN: 8.0.27 and 5.1
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

1. The commit hash (`git rev-parse HEAD`): 084b37a00f3cf2cc89d433528ca63ec1d3b5b313
2. The output of `bazel version`
```
Build label: 0.4.3- (@non-git)
Build target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Dec 23 16:35:28 2016 (1482510928)
Build timestamp: 1482510928
Build timestamp as int: 1482510928
```

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
My code is based on the tensorflow/models/slim interface"
7253,tf.reshape() of a tensor with an unknown dimension (None) does not seem to work,"
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
https://github.com/fchollet/keras/issues/4302

### Environment info
Operating System: Mac (CPU)

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
0.12.1

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
`input = tf.placeholder(dtype=tf.float32, shape=(None, 2, 3))
input_flattened = tf.reshape(input, shape=[input.get_shape()[0].value, -1])

with tf.Session() as sess:
    init = tf.global_variables_initializer()
    sess.run(init)
    input_val, input_flattened_val = \
        sess.run([input, input_flattened], feed_dict={input: np.random.random(24).reshape(4, 2, 3)})
    print input_val
    print input_flattened_val`

TypeErrorTraceback (most recent call last)<ipython-input-36-99d32472c9da> in <module>()
      1 input = tf.placeholder(dtype=tf.float32, shape=(None, 2, 3))
----> 2 input_flattened = tf.reshape(input, shape=[input.get_shape()[0].value, -1])
      3 
      4 with tf.Session() as sess:
      5     init = tf.global_variables_initializer()
/Users/adeoras/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.pyc in reshape(tensor, shape, name)
   2446   """"""
   2447   result = _op_def_lib.apply_op(""Reshape"", tensor=tensor, shape=shape,
-> 2448                                 name=name)
   2449   return result
   2450 
/Users/adeoras/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc in apply_op(self, op_type_name, name, **keywords)
    491           except TypeError as err:
    492             if dtype is None:
--> 493               raise err
    494             else:
    495               raise TypeError(
TypeError: Expected binary or unicode string, got None

### What other attempted solutions have you tried?
None

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).


"
7251,Native GPU version of `tf.dynamic_stitch`,"### Environment info
Operating System: Windows 10

Installed version of CUDA and cuDNN: 8.0, 5105
tensorflow release 0.12.1

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
``` python
import tensorflow as tf
from tensorflow.python.client.timeline import Timeline

with tf.device(""/gpu:0""):
    x = tf.ones(100)
    idxs = tf.range(100)

    for _ in range(10):
        y = tf.identity(x)
        x = tf.dynamic_stitch([idxs, idxs], [x, y])
        # x = tf.gather(y, idxs)

with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:
    metadata = tf.RunMetadata()
    sess.run(x, options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE),
             run_metadata=metadata)

timeline = Timeline(metadata.step_stats)
with open(""profile.json"", ""w"") as f:
    f.write(timeline.generate_chrome_trace_format())
```

The `log_device_placement` output shows that everything is assigned to the GPU, as expected.  However, inspecting the trace output shows that data is being copied on and off the GPU for each call to `dynamic_stitch`.  This is something specific to the `dynamic_stitch` implementation, because using `tf.gather` (a similar indexed read operation, and functionally equivalent in this case), doesn't show this behaviour.

Is this intended behaviour for `dynamic_stitch` (i.e., the copying to and from the GPU is necessary)?  Or is this a bug?  If it isn't a bug, is there some equivalent solution that doesn't require the data to be copied back and forth?"
7249,"ValueError: Only call `sigmoid_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...)","**OS:** 
macOS 10.12, CUDA 8.0 / cudnn 5, GeForce GTX 780

Tensorflow version:
0.12.head, build from source with CUDA 8 support. Is working!

---

**Problem:**
Trying to run the code of this project (written for tensorflow 0.10 – and i have 0.12):
https://github.com/david-gpu/srez

And am getting the following error:
```
Traceback (most recent call last):
  File ""srez_main.py"", line 190, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""srez_main.py"", line 187, in main
    _train()
  File ""srez_main.py"", line 168, in _train
    gene_loss = srez_model.create_generator_loss(disc_fake_output, gene_output, train_features)
  File ""/Users/david/github/image-manipulation/srez/srez_model.py"", line 452, in create_generator_loss
    cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(disc_output, tf.ones_like(disc_output))
  File ""/usr/local/lib/python3.5/site-packages/tensorflow/python/ops/nn_impl.py"", line 147, in sigmoid_cross_entropy_with_logits
    _sentinel, labels, logits)
  File ""/usr/local/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py"", line 1535, in _ensure_xent_args
    ""named arguments (labels=..., logits=..., ...)"" % name)
ValueError: Only call `sigmoid_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...)
```

The file which generates the **""ValueError: Only call `sigmoid_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...)""** error contains this code:

```python
def create_generator_loss(disc_output, gene_output, features):
    # I.e. did we fool the discriminator?
    cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(disc_output, tf.ones_like(disc_output))
    gene_ce_loss  = tf.reduce_mean(cross_entropy, name='gene_ce_loss')

    # I.e. does the result look like the feature?
    K = int(gene_output.get_shape()[1])//int(features.get_shape()[1])
    assert K == 2 or K == 4 or K == 8    
    downscaled = _downscale(gene_output, K)
    
    gene_l1_loss  = tf.reduce_mean(tf.abs(downscaled - features), name='gene_l1_loss')

    gene_loss     = tf.add((1.0 - FLAGS.gene_l1_factor) * gene_ce_loss,
                           FLAGS.gene_l1_factor * gene_l1_loss, name='gene_loss')
    
    return gene_loss

def create_discriminator_loss(disc_real_output, disc_fake_output):
    # I.e. did we correctly identify the input as real or not?
    cross_entropy_real = tf.nn.sigmoid_cross_entropy_with_logits(disc_real_output, tf.ones_like(disc_real_output))
    disc_real_loss     = tf.reduce_mean(cross_entropy_real, name='disc_real_loss')
    
    cross_entropy_fake = tf.nn.sigmoid_cross_entropy_with_logits(disc_fake_output, tf.zeros_like(disc_fake_output))
    disc_fake_loss     = tf.reduce_mean(cross_entropy_fake, name='disc_fake_loss')

    return disc_real_loss, disc_fake_loss
```

The project was written for tensorflow 0.10 and i already had to change a deprecated function name.

What do i need to change in 
```python
tf.nn.sigmoid_cross_entropy_with_logits(disc_output, tf.ones_like(disc_output))
```
to get rid of the error?

Thanks!"
7246,Pool Allocator Problem,"I was running LSTM having 2 layers and 64 nodes in each layer running in batch mode with small data size. I am unable to figure out the problem. I am getting warning like 

 tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 100 to 110
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 1323 get requests, put_count=2336 evicted_count=1000 eviction_rate=0.428082 and unsatisfied allocation rate=0
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 6326 get requests, put_count=5369 evicted_count=2000 eviction_rate=0.372509 and unsatisfied allocation rate=0.470123
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 193 to 212
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 1415 get requests, put_count=2440 evicted_count=1000 eviction_rate=0.409836 and unsatisfied allocation rate=0
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 1590 get requests, put_count=3623 evicted_count=2000 eviction_rate=0.552029 and unsatisfied allocation rate=0
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 1137 get requests, put_count=2186 evicted_count=1000 eviction_rate=0.457457 and unsatisfied allocation rate=0

Because of which code runs very slow.

**Machine 1**
Configuration as follows:
Environment Information (we've tried several different permutations):
OS: CentOS 7.2
CUDA: 8.0.44 and 7.5.17
CUDNN: 5.1 and 5.0
Tensorflow: 0.11.0rc0, 0.11.0, 0.12.1, 1.0.0rc0
Nvidia drivers: 352.39, 367.48
GPU:Tesla k80
servers with 2 K80 cards each (2 GPUs per card, for a total of 4 GPUs per machine).

If I run the same code on different machine the code runs fine without any warning:
**Machine 2**
OS:Ubuntu 16.04.1 LTS
CUDA: 8.0
CUDNN: 5.0
Tensorflow:0.12.1
Nvidia drivers: 367.48
GPU:GeForce GTX TITAN.
"
7245,OutOfRangeError on FIFOQueues of different experiments,"Hi,

I am running some experiments with Tensorflow on a 4-GPU machine using CIFAR10 with the FIFOQueue implementation for input data pipeline. 

I want to simultaneously run one different instance of my experiment on each of the four GPUs,
but because the data processing queues are always places on the CPU I get an OutOfRange error and
cannot run more than one instance per machine.

Is there a way to overcome this? (other than placing the data processing on GPUs)

Thanks"
7244,Saver unable to restore from checkpoint file although it had successfully restored previously.,"I am currently seeing a very strange behavior from using tf.train.Saver. Basically I had previously successfully run my code and restored all the variables from the TensorFlow-slim inception-resnet-v2 model and even obtained my losses and predictions. However, after I added some scopes to exclude in `variables_to_restore = slim.get_variables_to_restore(exclude = [...])` , which I successfully run,  when I returned back to using no excluded scope, my code actually fails. It gave me the following error:

`NotFoundError (see above for traceback): Tensor name ""InceptionResnetV2/Repeat_1/block17_13/Branch_1/Conv2d_0c_7x1/BatchNorm/beta/Adam"" not found in checkpoint files ./inception_resnet_v2_2016_08_30.ckpt`

Here is the code I run.
```
with slim.arg_scope(inception_resnet_v2.inception_resnet_v2_arg_scope()):
	logits, end_points = inception_resnet_v2.inception_resnet_v2(image_batch, is_training = False)
	predictions = end_points['Predictions']

#....then the typical building of operations till you get the train_op

variables_to_restore = slim.get_variables_to_restore()
saver = tf.train.Saver(variables_to_restore) #This is the line where the errors appear
def restore_fn(sess):
	return saver.restore(sess, checkpoint_file)
sv = tf.train.Supervisor(logdir = log_dir, summary_op = None, init_fn = restore_fn)
```

I have even replaced my checkpoint file again and this error persists. Why does this happen?

I strongly suspect it might have been because the original graph has been edited again as I did not encapsulate most of the definitions within a specified graph like `with tf.Graph.as_default() as g:`, but at the start of my code I have always done `tf.reset_default_graph()`."
7243,std::system_error after tensor flow install,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

Found no threads matching the exact output of the problem, and troubleshooting with the tensor flow website was unsuccessful.

### Environment info
Operating System:

CentOS Linux release 7.2.1511 (Core)

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

N/A, we're using only CPUs for the moment. The tensor flow documentation suggests this is only an optional requirement?

If installed from binary pip package, provide:

1. A link to the pip package you installed:

export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.0rc0-cp27-none-linux_x86_64.whl

2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

0.11.0

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

```
import tensorflow as tf
hello = tf.constant('Hello, TensorFlow!')
sess = tf.Session()
```

I get:

```
terminate called after throwing an instance of 'std::system_error'
what():  Resource temporarily unavailable
Aborted
```

I've looked around for the common troubleshooting with no success. Any ideas where to start?

### What other attempted solutions have you tried?

None.

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).

See above.

Any useful ideas of where to start debugging would be useful. "
7242,How to use Tensorflow in my android project?,"I am working on a project, where I change the View with a button (working with Fragments). I am changing between a FirstFragment and a CamFragment. Now I want to use Tensorflow (it works on my phone) instead of CamFragment.

When I ""import new Module"", I get Errors only. Do you have any Tips or solutions how I can do that?

Thank You
"
7236,Gradient is incorrect for log pdf of Normal distribution,"Am I missing something basic here?

Gradients of the log pdf of normal distributions are zero. I might be going crazy, it is late at night.

```
In [12]: mu = tf.get_variable('mu', [], 'float32')
In [13]: q = dist.Normal(mu, 1.)
In [14]: log_q = q.log_pdf(q.sample())
In [15]: grad = tf.gradients(log_q, mu)
In [16]: sess.run(tf.global_variables_initializer())
In [17]: sess.run(grad)
Out[17]: [0.0]
In [18]: tf.__version__
Out[18]: '0.12.1'
```"
7235,undefined symbol: clock_gettime on Ubuntu 12.04,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
Almost every possible workarounds and resolutions that are existing for all of the Tensorflow-related issues that have been discussed on GitHub and StackOverflow have been tried, but the issue still persists. Since, I could not find any discussions or fixes with respect to the particular issue that I am encountering, I thought reaching out to you folks would be the best option that is available, as it would help me learn from the developers themselves.


### Environment info
Operating System: Ubuntu 12.04 LTS 64-bit 
Python : 2.7.3 built on GCC 4.6.3
Tensorflow : 0.12.1, CPU-only version

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`): No such file or directory

Since installing from the binary pip package did not work, I tried installing Tensorflow from source, the details of which are provided below :

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`) - b5854872a49172e0502856b60cbaedf0df4df087
2. The output of `bazel version` - 0.4.3

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
~ $ cd /etc
/etc$ python
Python 2.7.3 (default, Oct 26 2016, 21:01:49) 
[GCC 4.6.3] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 72, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 61, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
ImportError: /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so: undefined symbol: clock_gettime


Failed to load the native TensorFlow runtime.

See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
>>> 


### What other attempted solutions have you tried?
The initial point of error was with respect to GLIBC_2.x not being found on my filesystem. With that I started troubleshooting and debugging Tensorflow configuration with the following list of attempted solutions -
1. Upgrade of GCC to v4.9
2. Upgrade of Bazel to v0.4.3
3. Upgrade of protobuf to v
3. Bazel installation via binary installer
4. Bazel installation via Git Repository clone (https://github.com/bazelbuild/bazel.git), followed by a hard reset to a specific HEAD
5. Tensorflow installation via the pip package directly using the .whl file
6. Tensorflow installation via Git Repository clone (https://github.com/tensorflow/tensorflow), followed by a hard reset to a specific HEAD
7. Bazel build with additional options such as --linkopt '-lrt' --copt '-DGPR_BACKWARDS_COMPATIBILITY_MODE' --conlyopt='-std=c99'


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
1. Execution of ./configure fetches all external dependencies successfully
2. During bazel build, massive number of warning messages regarding deprecation show up on the console. However, build completes successfully with building the pip package
3. No errors during bazel-bin execution
4. No errors during the .whl file execution either. Successfully installs Tensorflow 0.12.1 with most of the additional requirements like numpy, protobuf, wheel, werkzeug etc., being satisfied (the .whl file is named 'tensorflow-0.12.1-cp27-cp27mu-linux_x86_64.whl')


I may have missed out on mentioning slight details, but I have tried my best to cover the wide range of fixes that I have been trying on. If in case, you guys have any questions for me, please feel free to reach out as those details might help you better understand the issue being described above.

Just out of curiosity as it caught my eye, I upgraded my GCC to 4.9 as was mentioned in one of the StackOverflow discussion and updated the symlinks to point to the new version. After which, I performed all of the mentioned solutions that I tried. However, I see that my Python interpreter seems to have been compiled with GCC v4.6.3. Would this be causing any kind of issues with the native runtime not being loaded?

Thank you so much for helping us out! Looking forward to hear from your end with your inputs on this.



Adding a fellow colleague @nishithhaa to this discussion as we are facing issues during the configuration and setup of Tensorflow on our machines.
"
7227,ld: file not found: -lcudart.8.0 when building CUDA on MacOS and bazel 0.4.4,"In today's head https://github.com/tensorflow/tensorflow/commit/b47dc70e548e6958919f87864b83866919473a92, and bazel 0.4.4, bazel building with XLA + CUDA on MacOS fails because it can't find cuda library when linking. I was able to build with the same setup from head from Jan 24 .

```
bazel build --config=cuda --config=opt -k //tensorflow/tools/pip_package:build_pip_package
...
ld: file not found: -lcudart.8.0
clang: error: linker command failed with exit code 1 (use -v to see invocation)
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 0.513s, Critical Path: 0.11s

```

I see `libcudart_static.a` in my `/usr/local/cuda/lib` and similar issue [suggests](http://stackoverflow.com/questions/9633881/usr-bin-ld-cannot-find-lcudart) that linking needs to be done with -L$CUDA_HOME/lib flag, but I can't figure out how to massage that option into bazel config

I also tried building with flags ` --action_env PATH --action_env DYLD_LIBRARY_PATH --action_env LD_LIBRARY_PATH` and copying `libcudart_static.a` to `libcudart.8.0.a`

Note that libcudart dependency dependency exists in CPU-only build as well because of https://github.com/tensorflow/tensorflow/issues/7216"
7226,conv2d gives NaN gradients with float16 input,"### Environment info
Operating System: Ubuntu 16 LTS
breaks already on CPU

If installed from binary pip package, provide:

1. A link to the pip package you installed: recent nightly build
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
```
>tf.__version__
'0.12.head'
>tf.__git_version__
'0.12.1-2263-g4cc0d1e-dirty'
```

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

```python
import tensorflow as tf
import numpy as np

slim = tf.contrib.slim

dtype = tf.float16
shape = (4, 16, 16, 3)

inpt = tf.placeholder(dtype, shape, name='input')
net = slim.conv2d(inpt, 16, [3, 3], scope='conv')
loss = tf.reduce_mean(net)
opt = tf.train.AdamOptimizer(1e-3)
train_op = slim.learning.create_train_op(loss, opt)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for i in range(2):
        val = np.random.randn(*shape)
        print(sess.run(train_op, feed_dict={inpt: val}))
```

So basically it breaks on the second step of SGD because loss is NaN. If I change dtype in float32, it works. It should have nothing to do with CUDA, I tested it on CPU version as well as on GPU with CUDA8, CuDNN5.1.

### What other attempted solutions have you tried?
I have no idea what to try here. Now I continue with float32.

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
```
-0.00072765
Traceback (most recent call last):
  File ""/home/lear/kshmelko/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1022, in _do_call
    return fn(*args)
  File ""/home/lear/kshmelko/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1004, in _run_fn
    status, run_metadata)
  File ""/usr/lib/python3.5/contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""/home/lear/kshmelko/.local/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 469, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: LossTensor is inf or nan : Tensor had NaN values
         [[Node: train_op/CheckNumerics = CheckNumerics[T=DT_HALF, message=""LossTensor is inf or nan"", _device=""/job:localhost/replica:0/task:0/cpu:0""](control_dependency)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""test_bn.py"", line 22, in <module>
    print(sess.run(train_op, feed_dict={inpt: val}))
  File ""/home/lear/kshmelko/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 767, in run
    run_metadata_ptr)
  File ""/home/lear/kshmelko/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 965, in _run
    feed_dict_string, options, run_metadata)
  File ""/home/lear/kshmelko/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1015, in _do_run
    target_list, options, run_metadata)
  File ""/home/lear/kshmelko/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1035, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: LossTensor is inf or nan : Tensor had NaN values
         [[Node: train_op/CheckNumerics = CheckNumerics[T=DT_HALF, message=""LossTensor is inf or nan"", _device=""/job:localhost/replica:0/task:0/cpu:0""](control_dependency)]]

Caused by op 'train_op/CheckNumerics', defined at:
  File ""test_bn.py"", line 16, in <module>
    train_op = slim.learning.create_train_op(loss, opt)
  File ""/home/lear/kshmelko/.local/lib/python3.5/site-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 472, in create_train_op
    'LossTensor is inf or nan')
  File ""/home/lear/kshmelko/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 433, in check_numerics
    message=message, name=name)
  File ""/home/lear/kshmelko/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 768, in apply_op
    op_def=op_def)
  File ""/home/lear/kshmelko/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 2402, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/lear/kshmelko/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1264, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): LossTensor is inf or nan : Tensor had NaN values
         [[Node: train_op/CheckNumerics = CheckNumerics[T=DT_HALF, message=""LossTensor is inf or nan"", _device=""/job:localhost/replica:0/task:0/cpu:0""](control_dependency)]]
```"
7225,Feature request: equivalent of np.clip,"`tf.clip_by_value` is missing an important property of `np.clip` which lets upper/lower bounds to be tensors, whereas `tf.clip_by_value` only takes scalars

A work-around is `tf.minimum(upper, tf.maximum(lower, x))` but that uses 2x memory for gradients. Another potential work-around is to use `tf.map_fn` + `tf.clip_by_value`, but it is orders of magnitude slower. Some profiling: https://github.com/yaroslavvb/notebooks/blob/master/clipping-profile.ipynb"
7224,dropout does not take in dynamic shape,"I realize dropout `noise_shape` does not allow dynamic shape which becomes useless in batch processing whereby the batchsize can varies
```python
X = tf.placeholder('float', [None, 5, 10])
tf.dropout(X, keep_prob=0.5, noise_shape=[-1, 5, 1]) 
```
throws Exception. Whereby it requires dimension to be >= 0

however if we pass noise_shape as the shape of X which is dynamic, dropout can understand the shape.
```python
tf.dropout(X, keep_prob=0.5, noise_shape=tf.shape(X))
```
Therefore there is inconsistency in noise_shape inputs. How do we give dynamic shapes to `noise_shape` such as `noise_shape=[-1, 5, 1]` where the batchsize dimension can be variable just like how `tf.reshape` works

```python
tf.reshape(X, [-1, 5, 1])
```"
7223,OS/X doesn't compile Tensorflow,"Not sure of the exact circumstances of this, but I get an undefined symbol when trying to load the _pywrap_tensorflow.so.

The symbol is __cpu_model.  

There is a discussion here about it: https://github.com/numpy/numpy/issues/8530

It seems that the file crc32c_accelerate.cc calls the compiler function `__builtin_cpu_supports`.  While there is a test for the presence of this at the top of the file, there might be a bug in llvm/clang where this function references a symbol called __cpu_model, which isn't compiled into the final .so file.

As a test, I compiled:
```
#include <stdio.h>

main() {
  if (__builtin_cpu_supports(""sse4.2"")) {
    puts(""a"");
  }
}
```

and I received:
```
Undefined symbols for architecture x86_64:
  ""___cpu_model"", referenced from:
      _main in t-2e30e2.o
ld: symbol(s) not found for architecture x86_64
```

My version of clang is:
```
Apple LLVM version 7.3.0 (clang-703.0.31)
Target: x86_64-apple-darwin15.6.0
```

"
7221,Linking of rule '@protobuf//:internal/_api_implementation.so' failed:,"I am trying to build tensorflow v0.12.0 from source with bazel 0.4.3 and failed with this error:

```
ERROR: /ui/ncsa/qiyuelu1/.cache/bazel/_bazel_qiyuelu1/ce8d1fb9ff1b7a9dbbd225fe7f0c6f52/external/protobuf/BUILD:579:1: Linking of rule '@protobuf//:internal/_api_implementation.so' failed: link_dynamic_library.sh failed: error executing command 
  (cd /ui/ncsa/qiyuelu1/.cache/bazel/_bazel_qiyuelu1/ce8d1fb9ff1b7a9dbbd225fe7f0c6f52/execroot/tensorflow-0.12.0 && \
  exec env - \
    LD_LIBRARY_PATH= .......

```
I searched online and found github bazelbuild/bazel #1867 and tensorflow/tensorflow issue #2266, #5617 and #5616, but it seems the changes suggested in these issue have already been implemented in the source code of version 0.12.0. I am not sure why this problem still exists.  

Environment info
Operating System: RHEL 6.8
CUDA: 8.0
cuDNN: 5.1
Java: 1.8.0_112
gcc: 4.9.2
bazel: 0.4.3
tensorflow: v0.12.0 (Latest commit c62a66b on Dec 19, 2016)



### What other attempted solutions have you tried?
I tried tensorflow version 0.12.head and v1.0.1-rc0, both of them can be built successfully. However, we do need this specific version 0.12.0.

"
7220,"tf.pow(x,y) doesn't compute complex results","Slightly related to #7170 

**Operating System:** Debian 4.8.15-2
**Installed version of CUDA:** 8.0
**Installed version of cuDNN:** 5.1.5
**The output of `ls -l /path/to/cuda/lib/libcud*`:**
> myuser@mymachine:/mypath$ ls -l /usr/local/cuda-8.0/lib64/libcud*
> -rw-r--r-- 1 root root 558720 sept. 15 01:02 /usr/local/cuda-8.0/lib64/libcudadevrt.a
> lrwxrwxrwx 1 root root     16 sept. 15 01:05 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0
> lrwxrwxrwx 1 root root     19 sept. 15 01:05 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44
> -rw-r--r-- 1 root root 415432 sept. 15 01:02 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.44
> -rw-r--r-- 1 root root 775162 sept. 15 01:02 /usr/local/cuda-8.0/lib64/libcudart_static.a

**A link to the pip package you installed:** Lost in history. I don't think this is the problem, so I'm going to skip it.

**The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`:**
> myuser@mymachine:/mypath$ python -c ""import tensorflow; print(tensorflow.__version__)""
> I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
> I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
> I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
> I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
> I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
> 0.12.1

**Minimal reproducible example:** (using python3)
```
import tensorflow as tf
session = tf.InteractiveSession()
tf.pow(-83.56,2.0).eval() # this one works fine
tf.pow(-83.56,1.0).eval() # this one works fine
tf.pow(-83.56,1.5).eval() # nan
```

The docs aren't clear whether it should compute complex results or not — but it states the function accepts complex inputs, so I assume returning ""nan"" is not expected."
7217,Error while creating quantized graph using bazel tool,"Hi, 
I am trying to quantize the already stripped and retrained inception graph using the following bazel commands, also I am using docker environment in ubuntu 16.10:

bazel build tensorflow/contrib/quantization/tools:quantize_graph
bazel-bin/tensorflow/contrib/quantization/tools/quantize_graph \    
  --input=/face_tf_train/new_retrained_graph.pb \
  --output_node_names=final_result \
  --output=/face_tf_train/new_stripped_quantized_graph.pb \
  --mode=eightbit  
I am bit new in python and tensorflow, I am not sure what is happening. I want to use the quantized graph for ios mobile app which I am able to compile and run. 
Any help could be very much appriciated. 

I am getting folowing error in the command promt : 
root@f098a9662116:/tensorflow# bazel build tensorflow/contrib/quantization/tools:quantize_graph
INFO: Reading 'startup' options from /root/.bazelrc: --batch
WARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.
INFO: Found 1 target...
Target //tensorflow/contrib/quantization/tools:quantize_graph up-to-date:
  bazel-bin/tensorflow/contrib/quantization/tools/quantize_graph
INFO: Elapsed time: 37.731s, Critical Path: 24.75s
root@f098a9662116:/tensorflow# bazel-bin/tensorflow/contrib/quantization/tools/quantize_graph \    
Traceback (most recent call last):
  File ""/tensorflow/bazel-bin/tensorflow/contrib/quantization/tools/quantize_graph.runfiles/org_tensorflow/tensorflow/contrib/quantization/tools/quantize_graph.py"", line 1003, in <module>
    tf.app.run()
  File ""/tensorflow/bazel-bin/tensorflow/contrib/quantization/tools/quantize_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py"", line 30, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""/tensorflow/bazel-bin/tensorflow/contrib/quantization/tools/quantize_graph.runfiles/org_tensorflow/tensorflow/contrib/quantization/tools/quantize_graph.py"", line 985, in main
    data = f.read()
  File ""/tensorflow/bazel-bin/tensorflow/contrib/quantization/tools/quantize_graph.runfiles/org_tensorflow/tensorflow/python/lib/io/file_io.py"", line 101, in read
    compat.as_bytes(self.__name), status)
  File ""/usr/lib/python2.7/contextlib.py"", line 24, in __exit__
    self.gen.next()
  File ""/tensorflow/bazel-bin/tensorflow/contrib/quantization/tools/quantize_graph.runfiles/org_tensorflow/tensorflow/python/framework/errors.py"", line 463, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors.FailedPreconditionError: .


Regards,
Pankaj Wasnik
"
7216,"Cannot build Tensorflow with XLA, but without GPU support","I believe it is probably commit 191658d54f90ac03c15b339326129cd52d1f56a3

To be clear, this is when I build without configuring CUDA at the ./configure stage.   I have not tried to configure with CUDA but then build without the --config=cuda.

Here is the final few lines of the build output:
```
out/local-opt/bin/external/png_archive/libpng.pic.a bazel-out/local-opt/bin/external/zlib_archive/libzlib.pic.a -lcublas -lcuda -lcudnn -lcufft -lcurand '' -Wl,-exported_symbols_list tensorflow/tf_exported_symbols.lds -ldl -lm -Wl,-rpath,../local_config_cuda/cuda/lib -Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib -ldl -lpthread -pthread -lm -framework IOKit -lstdc++ -lm -undefined dynamic_lookup -headerpad_max_install_names): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
clang: warning: argument unused during compilation: '-pthread'
ld: file too small (length=0) file 'bazel-out/local-opt/bin/_solib_darwin/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Slib/libcudart.dylib' for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```

You can see that the cuda libraries are included in the build, and that the linker is complaining that the .so (.dylib) files are empty, which is true.

Maybe this is a problem with the OS/X linker  which might object to empty .so files where other operating systems' linkers do not.

For me,  this is is a high priority issue."
7215,Incorrect documentation on log_loss,"1) I noticed that [1] states that tf.contrib.losses.log_loss is deprecated and that tf.losses.log_loss should be used instead. However in v0.12 that function does not exist and the documentation in [2] does not talk about this. So which one is correct?
2) Furthermore it seems that tf.contrib.losses.log_loss only returns a scalar for the summed loss of all dimensions. Is there a function more like tf.nn.softmax_cross_entropy_with_logits (but without the softmax), that returns the loss for each element separately?

[1] https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard6/tf.contrib.losses.log_loss.md
[2] https://www.tensorflow.org/api_docs/python/contrib.losses/other_functions_and_classes#log_loss"
7214,"tf_upgrade do not update concat, tf.image.resize_images","There are 2 issues about `concat`:
- used `concat_dim` instead of `axis`.
- Incorrect position of the explicit signature if the parameter is `[...]`
e.g.
```
tf.concat(0, [tf.concat(0, tiles[y]) for y in range(4)])
```
becomes
```
tf.concat(concat_dim=0, [values=tf.concat(concat_dim=0, values=tiles[y]) for y in range(4)])
```

<br/><br/>

The `height` and `width` signature of `tf.image.resize_images` has been changed to `size`. But it is not updated the current converter.

"
7213,TensorFlow Linear Model Tutorial code,"Not sure this is right place for my question...

At any rate, I'm trying to follow the Linear model [tutorial ](https://www.tensorflow.org/tutorials/wide/), there is a code link there but leads to the wide and deep code , what am I missing here?"
7212,tf.import_graph_def prepends 'import' to tensor names but graph_util.convert_variables_to_constants can't find the tensors,"I am trying to merge two graph and encountering this issue.
```

# This is graph from session till now
g_current_def = sess.graph.as_graph_def()
with tf.Graph().as_default() as g_2:
    # Add another tensor which will be new input
    prepend_input = tf.placeholder(tf.string, name='prepend_input')
    # pass this new input tensor as input to imported graph
    output_tensor , = tf.import_graph_def(g_default_def, input_map={""input_tensor"": prepend_input}, return_elements=['output_tensor']) #This operation prepends 'import' to all tensor names in default graph
    g2_default_def = g_2.as_graph_def()
    # Convert merged graph into graph_def
    output_graph_def = graph_util.convert_variables_to_constants(sess, g2_default_def, [""import/"" + FLAGS.final_tensor_name])
```

This throws error

```
ValueError: Fetch argument u'import/final_training_ops/weights/final_weights:0' cannot be interpreted 
as a Tensor. (""The name 'import/final_training_ops/weights/final_weights:0' refers to a Tensor which d
oes not exist. The operation, 'import/final_training_ops/weights/final_weights', does not exist in the
 graph."")
```

whereas tensor 'final_training_ops/weights/final_weights:0' is declared in original graph."
7210,Tensorflow android does not recognize imported networks(.pb files) with python operations ,"tensorflow android does not recognize operations from an imported network(.pb files) defined/created with python operations  like  recogonize tf.python.ops.FIFOQueue . Works when the network is built/defined with tf.FIFOQueue. Similarly the tensorflow android implementation cribs about unrecognized operation RECIPROCAL when running inference for a network described [here](https://github.com/davidsandberg/facenet/blob/master/src/facenet_train.py)

"
7207,libcupti.so missing from Docker.gpu,"### Environment info:
The latest docker GPU image.
`gcr.io/tensorflow/tensorflow:latest-gpu`



### Issue:
While running [mnist_with_summaries.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py), you get 
`tensorflow/stream_executor/dso_loader.cc:119] Couldn't open CUDA library libcupti.so. LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:`

### Temporary Solution:
Adding the path to CUPTI to the LD_LIBRARY_PATH environment variable. 

`export LD_LIBRARY_PATH=""$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64""`

### Solution:
Add `ENV LD_LIBRARY_PATH /usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH` to [Dockerfile.gpu](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile.gpu), as in #7206 

"
7204,Should use math_ops.reduce_prod() in mixture.py instead of array_ops.reduce_prod(),"Hi,

I tried to use mixture density distribution with batch_size **None**, and triggered this error
```bash
/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distributions/python/ops/mixture.pyc in _sample_n(self, n, seed)
    260       else:
    261         batch_shape = self.batch_shape()
--> 262         batch_size = array_ops.reduce_prod(batch_shape)
    263       static_event_shape = self.get_event_shape()
    264       if static_event_shape.is_fully_defined():

AttributeError: 'module' object has no attribute 'reduce_prod'
```
Please see https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/contrib/distributions/python/ops/mixture.py#L262

I believe this is a bug since `reduce_prod` is in `math_ops.py` instead of `array_ops.py`.

I'm using version v0.12.1, but I checked it's the same for r1.0

Thanks"
7203,Noticable lag with different Inception model,Running the demo app on Android works absolutelly great. Then I tried retraining the inception5h model (as used by the app) with no success. I retrained Inception v3 model but then image recognition would lag. I tried quantizating the model but it lagged even more. So what I'm asking is for a direction or help on the issue or a retrainable Inception5h model with some instructions.
7201,Feeding to boolean placeholder -- works in TF 0.11 but not in 0.12.1,"@yaroslavvb The info is attached below. I've targeted the problem to the conflict of tf.cond on boolean placeholder with the ExponentialMovingAverage's apply function.

The same code works just fine in TF 0.11

Environment Info
============================
OS: Ubuntu 14.04
CUDA: 8.0 with cuDNN 5.1
TEnsorflow 0.12.1

Minimal Example:
============================
```python
import tensorflow as tf
import numpy as np

with tf.Graph().as_default():
    with tf.device('/gpu:0'):
        isTrain = tf.placeholder(tf.bool, shape=())
        user_input = tf.placeholder(tf.float32, shape=(2,4))

        with tf.variable_scope('batchnorm') as sc:
           batch_mean, batch_var = tf.nn.moments(user_input, [0,])
           ema = tf.train.ExponentialMovingAverage(decay=0.9)

           ema_apply_op = tf.cond(isTrain,
                           lambda: ema.apply([batch_mean, batch_var]),
                           lambda: tf.no_op())

    init = tf.initialize_all_variables()
    sess = tf.Session()
    sess.run(init)
    sess.run([batch_mean], feed_dict={user_input:[[1,2,3,4],[2,3,4,5]], isTrain: True})
```

Error Message:
============================
```
InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder' with dtype bool
	 [[Node: Placeholder = Placeholder[dtype=DT_BOOL, shape=[], _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]
```"
7199,Incorrect computation results on /gpu:1 on dual K80 servers,"We have a bunch of servers with 2 K80 cards each (2 GPUs per card, for a total of 4 GPUs per machine). We're having problems with even simple computations on `/gpu:1` returning wrong results. This happens across multiple machines. Interestingly, if we re-map the GPUs, using `CUDA_VISIBLE_DEVICES` (e.g. `CUDA_VISIBLE_DEVICES=""3,2,1,0""`), it's still always `/gpu:1` that has problems.

To reproduce, I wrote a short script that multiplies random matrices together on each GPU, comparing the results with Numpy:

```
import tensorflow as tf
import numpy as np

def test(sess, device):
    x = tf.placeholder(tf.float32, (500, 500))
    w_np = np.random.random((500, 20)).astype(np.float32)
    w = tf.Variable(w_np, dtype=tf.float32)

    with tf.device(""/"" + device):
        y = tf.matmul(x, w)

    x_np = np.random.random((500, 500)).astype(np.float32)
    y_np = np.dot(x_np, w_np)

    with sess.as_default():
        sess.run([tf.initialize_all_variables()])
        y_tf = sess.run([y], feed_dict={x: x_np})

    assert(np.all(np.abs(y_tf - y_np) < 1e-3))


def test_all():
    sess = tf.Session()
    for i in range(4):
        device = ""/gpu:{}"".format(i)
        try:
            for _ in range(10):
                test(sess, device)
        except AssertionError:
            print ""GPU {} FAILED!"".format(i)
            continue
        print ""GPU {} passed"".format(i)


if __name__ == ""__main__"":
    test_all()
```

After some initialization logging, this produces the output:

```
GPU 0 passed
GPU 1 FAILED!
GPU 2 passed
GPU 3 passed
```

Environment Information (we've tried several different permutations):
OS: CentOS 7.2
CUDA: 8.0.44 and 7.5.17
CUDNN: 5.1 and 5.0
Tensorflow: 0.11.0rc0, 0.11.0, 0.12.1, 1.0.0rc0
Nvidia drivers: 352.39, 367.48

We have another machine running Ubuntu 16.04 with 4x Titan Xs. All 4 GPUs pass this test script there.
"
7198,"""Failed to get the number of CUDA devices: CUDA driver version is insufficient for CUDA runtime version"" on Windows 10","Tensorflow was working on my new Windows 10 for about 24 hours then broke. 
Running a Keras lstm_text-generator with a small test txt file, all was well.
After running the same script with a larger 8mb txt file as input data, python encountered an error, system froze, rebooted. 

### Environment info

Windows 10
Tensorflow 0.12.1
Python 3.5.2

Path includes both:
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0\bin
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0\extras\CUPTI\libx64
(as recommended here: https://github.com/tensorflow/tensorflow/issues/6235 and https://github.com/tensorflow/tensorflow/issues/5968 )


### cmd

`testing>python saveModel_best_only-pima.py
Using TensorFlow backend.
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library cublas64_80.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library cudnn64_5.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library cufft64_80.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library nvcuda.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library curand64_80.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:885] Found device 0 with properties:
name: GeForce GTX TITAN X
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:01:00.0
Total memory: 12.00GiB
Free memory: 10.00GiB
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:906] DMA: 0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:916] 0:   Y
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:01:00.0)
Failed to get the number of CUDA devices: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #0: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #1: CUDA driver version is insufficient for CUDA runtime version
...`
""


###  Directory of C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0\lib

2017-01-31  03:37 PM    <DIR>          .
2017-01-31  03:37 PM    <DIR>          ..
2017-01-31  03:37 PM    <DIR>          Win32
2017-01-31  05:15 PM    <DIR>          x64


### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem? 
http://stackoverflow.com/questions/3253257/cuda-driver-version-is-insufficient-for-cuda-runtime-version



"
7196,./configure modifies build_config.bzl which is version-controlled,"Configure modifies `build_config.bzl` that is tracked by version control. This causes following annoyances:

1. When you do `git pull`, it'll overwrite `build_config.bzl` after which you need to do a ./configure and rebuild. I sometimes do testing of the same change on Mac and Unix host, and use `git pull` to sync them, which will overwrite my machine-specific config with default value.
2. You can't use `git commit -a` because that'll include `build_config.bzl`

Current work-around for 1. is to do following when pulling:
```
git stash
git pull
git stash pop
```
If you merge failed message, do
```
git checkout --theirs -- tensorflow/core/platform/default/build_config.bzl
```
For 2. it's possible to use `skip-worktree`

@aselle what do you thinking about adding `build_config.bzl` to `.gitignore`?"
7195,Running tensorflow-gpu on windows 10,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
I am trying to run to train a convolutional network on lenet dataset but whenever I open a session and start `sess.run(training_operation,` feed_dict={x: batch_x, y: batch_y})` I get the following error:

> name: GeForce GTX 980
> major: 5 minor: 2 memoryClockRate (GHz) 1.178
> pciBusID 0000:01:00.0
> Total memory: 8.00GiB
> Free memory: 6.70GiB
> I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:906] DMA: 0
> I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:916] 0:   Y
> I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0)
> E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\cuda\cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED
> F c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_event_mgr.cc:198] Unexpected Event status: 1
> E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\cuda\cuda_dnn.cc:385] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR

### Environment info
Operating System: Windows 10 Home

Installed version of CUDA and cuDNN:  
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
CUDA 8.0
cuDNN 5.1

If installed from binary pip package, provide:

1. A link to the pip package you installed:

> pip install --upgrade https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-0.12.1-cp35-cp35m-win_amd64.whl

2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

> I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library cublas64_80.dll locally
> I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library cudnn64_5.dll locally
> I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library cufft64_80.dll locally
> I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library nvcuda.dll locally
> I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library curand64_80.dll locally
> 0.12.1

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
7193,ImportError: No module named 'tensorflow.python' Mac GPU,"Running 2 Jupyter kernels reflecting conda envs:
- tensorflow CPU
- tensorflow GPU
Both are configured with 0.12.1 for Mac.

CPU VERSION (tensorflow 0.12.1) works well as expected: 

```
import tensorflow as tf
print(""OUT: tensorflow imported"")
```
OUT: tensorflow imported


```
from tensorflow.python.client import device_lib
def get_available_CPU_GPU():
    devices = device_lib.list_local_devices()
    #return [x.name for x in devices if x.device_type == 'CPU']
    return [x.name for x in devices ]
print(get_available_CPU_GPU())
```

['/cpu:0']  - as expected only CPU shows

SAME CODE GPU VERSION (tensorflow-gpu 0.12.1 ):

OUT: tensorflow imported

So **it sees the TensorFlow correctly, but then only a part of it**.
```
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
 in ()
----> 1 **from tensorflow.python.client import device_lib**
      2 
      3 def get_available_CPU_GPU():
      4     devices = device_lib.list_local_devices()
      5     #return [x.name for x in devices if x.device_type == 'CPU']

ImportError: No module named 'tensorflow.python'
```
```
features = tf.placeholder(tf.float32, (None, 32, 32, 3))
AttributeError: module 'tensorflow' has no attribute 'placeholder'
```



"
7188,Unexpected behavior after updating a placeholder,"The output of the following snippet
```
ph = tf.placeholder(tf.int32)
ph += 1000
a = ph + 0
init = tf.initialize_all_variables()
sess.run(init)
print(sess.run([a], {ph: 5}))
```
is `[5]`, and not `[1005]` as could be expected.
Perhaps it's because placeholders can't be evaluated, and therefore TF ignores the update op of `ph`?"
7187,hclhkbu dlbench shows Tensorflow is slower than other frameworks,"Based on https://github.com/tensorflow/tensorflow/issues/7065#issuecomment-276648478

Recent update of [Benchmarking State-of-the-Art Deep Learning Software Tools](https://arxiv.org/abs/1608.07249) (by @shyhuai @FreemanX @xiaowec , if I got it right) shows some performance issues. For example, (see table 7) `AlexNet-R` is significantly (~ 10 times) slower in TF than in other frameworks, an it's even slower at GTX 980 than at GTX 1080. Also, ResNet-50 is ~5.5 times faster in MXNet. Those are most significant differences. 

In addition, LSTM is around 3 times faster in CNTK, and ResNet-56 is twice faster in MXNet.

Version used was TensorFlow 0.11 (commit [47dd089](https://github.com/tensorflow/tensorflow/tree/47dd089db3cd16d76595791b2e8483e2fd0b0a25)) with CUDA 8.0 and cuDNN 5.1

 cc @yaroslavvb @annarev 

"
7186,missing fclose,"\contrib\pi_examples\label_image\label_image.cc line 105

missing fclose(infile);

"
7185,Float support for scatter_nd,"Hi,
It seems the updates input to scatter_nd is limited to int32 and int64 input: [https://www.tensorflow.org/api_docs/python/array_ops/slicing_and_joining#scatter_nd](https://www.tensorflow.org/api_docs/python/array_ops/slicing_and_joining#scatter_nd)

It'd be great to have float32 and float64 support. Is this possible?

Thanks!"
7184,1,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
7183,"If using multiple parameter servers, how to divide model parameters on these PSs? ","If using multiple parameter servers, how to divide model parameters on these PSs?  each PS stores an equal number of parameters and updates them?"
7181,can't open tensorboard while training ,"(You can navigate to http://127.0.1.1:6006)
 * Running on http://0.0.0.0:6006/ (Press CTRL+C to quit)
127.0.0.1 - - [31/Jan/2017 23:25:27] ""GET /neon-animation/neon-animation-behavior.html HTTP/1.1"" 200 -
127.0.0.1 - - [31/Jan/2017 23:25:27] ""GET /neon-animation/web-animations.html HTTP/1.1"" 200 -
127.0.0.1 - - [31/Jan/2017 23:25:27] ""GET /iron-overlay-behavior/iron-overlay-backdrop.html HTTP/1.1"" 200 -
127.0.0.1 - - [31/Jan/2017 23:25:27] ""GET /web-animations-js/web-animations-next-lite.min.js HTTP/1.1"" 200 -
127.0.0.1 - - [31/Jan/2017 23:25:27] ""GET /paper-input/paper-input.html HTTP/1.1"" 200 -
127.0.0.1 - - [31/Jan/2017 23:25:27] ""GET /paper-slider/paper-slider.html HTTP/1.1"" 200 -
*** Error in `/home/menglin/anaconda2/envs/tensorflow/bin/python': double free or corruption (!prev): 0x00000000009f6a80 ***
Aborted (core dumped)
"
7175,Go: Documentation of default values for attributes of generated ops can be confusing,"In the Go documentation of op.MatMulTransposeA it says 
""value: If true, ""a"" is transposed before multiplication. If not specified, defaults to b:false"" 
which should probably be 
""value: If true, ""a"" is transposed before multiplication. If not specified, defaults to a:false""

I haven't figured out yet where the documentation for the generated Go source code comes from, maybe it is used to generate documentation in other formats as well. "
7174,"MacOSX ImportError: dlopen(/usr/local/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so, 10): Library not loaded: @rpath/libcudart.8.0.dylib","Installed tensorflow for python3.5 on Mac, try the testing code as follows: 

```
import tensorflow as tf

hello = tf.constant(""test"")
sess = tf.Session()
print(sess.run(hello))
```

It is working when run it from the terminal. But if I run it from eclipse, will get this exception:
```
warning: Debugger speedups using cython not found. Run '""/usr/local/bin/python3.5"" ""/Users/leitian/.p2/pool/plugins/org.python.pydev_5.5.0.201701191708/pysrc/setup_cython.py"" build_ext --inplace' to build.
pydev debugger: starting (pid: 978)
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""/usr/local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: dlopen(/usr/local/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so, 10): Library not loaded: @rpath/libcudart.8.0.dylib
  Referenced from: /usr/local/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so
  Reason: image not found

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/leitian/.p2/pool/plugins/org.python.pydev_5.5.0.201701191708/pysrc/pydevd.py"", line 1537, in <module>
    globals = debugger.run(setup['file'], None, None, is_module)
  File ""/Users/leitian/.p2/pool/plugins/org.python.pydev_5.5.0.201701191708/pysrc/pydevd.py"", line 976, in run
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""/Users/leitian/.p2/pool/plugins/org.python.pydev_5.5.0.201701191708/pysrc/_pydev_imps/_pydev_execfile.py"", line 25, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""/Users/leitian/code/python/eclipse-ws/TestTF/Hello.py"", line 1, in <module>
    import tensorflow as tf
  File ""/usr/local/lib/python3.5/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/usr/local/lib/python3.5/site-packages/tensorflow/python/__init__.py"", line 60, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""/usr/local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: dlopen(/usr/local/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so, 10): Library not loaded: @rpath/libcudart.8.0.dylib
  Referenced from: /usr/local/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so
  Reason: image not found


Error importing tensorflow.  Unless you are using bazel,
you should not try to import tensorflow from its source directory;
please exit the tensorflow source tree, and relaunch your python interpreter
from there.
```"
7173,a bug in inception models?,"Hi,

I tried to use inception-vx model to extract features, but it keeps giving me error saying tensor name xxx not found in checkpoint files. I'm wondering whether it's my incorrect usage or there's some inconsistency.

The model files(ckpt) are downloaded from: https://github.com/tensorflow/models/blob/master/slim/README.md#pre-trained-models

My code is:

import tensorflow as tf
import nets.inception_v4 as net_v4

x = tf.placeholder(tf.float32, shape=[None, 299, 299,3])
inv4 = net_v4.inception_v4(x)
saver = tf.train.Saver()
sess = tf.Session()

saver.restore(sess, ""models/inception_v4.ckpt"")


The error output:

...
NotFoundError (see above for traceback): Tensor name ""InceptionV4/Mixed_6d/Branch_2/Conv2d_0e_1x7/biases"" not found in checkpoint files models/inception_v4.ckpt
	 [[Node: save/RestoreV2_150 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_recv_save/Const_0, save/RestoreV2_150/tensor_names, save/RestoreV2_150/shape_and_slices)]]
	 [[Node: save/RestoreV2_31/_567 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_1633_save/RestoreV2_31"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]



"
7172,freeze graph fail by using the published checkpoint file: Attempting to use uninitialized value,"I am using the latest Tensorflow code on Ubuntu 16.04

1. I download **Inception-ResNet-v2** model from the link https://github.com/tensorflow/models/tree/master/slim    
the check point file is:
http://download.tensorflow.org/models/inception_resnet_v2_2016_08_30.tar.gz

2. use the code below to get graph define, which output is **graph.pbtxt**

```
import tensorflow as tf
from nets import inception_resnet_v2
import numpy as np

x = tf.placeholder(shape=[1,299,299,3], dtype=tf.float32)
inception_resnet_v2 = inception_resnet_v2.inception_resnet_v2(x)

sess = tf.Session()
tf.train.write_graph(sess.graph_def, './', 'graph.pbtxt')
```
3, use the freeze graph tool to get **.pb** file. 

`bazel-bin/tensorflow/python/tools/freeze_graph
 --input_graph=/home/scopeserver/RaidDisk/DeepLearning/mwang/Model_backup2/slim/graph.pbtxt --input_checkpoint=/home/scopeserver/RaidDisk/DeepLearning/mwang/Model_backup2/slim/inception_resnet_v2_2016_08_30.ckpt  
--output_graph=/tmp/frozen_graph.pb --output_node_names=InceptionResnetV2/Logits/Predictions`

i get the error:

```
Traceback (most recent call last):
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 218, in <module>
    app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 150, in main
    FLAGS.variable_names_blacklist)
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 138, in freeze_graph
    variable_names_blacklist=variable_names_blacklist)
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/framework/graph_util_impl.py"", line 218, in convert_variables_to_constants
    returned_variables = sess.run(variable_names)
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 767, in run
    run_metadata_ptr)
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 965, in _run
    feed_dict_string, options, run_metadata)
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1015, in _do_run
    target_list, options, run_metadata)
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1035, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.FailedPreconditionError: **Attempting to use uninitialized value InceptionResnetV2/Repeat_1/block17_16/Branch_1/Conv2d_0c_7x1/biases**
	 [[Node: _send_InceptionResnetV2/Repeat_1/block17_16/Branch_1/Conv2d_0c_7x1/biases_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=4447330825950993621, tensor_name=""InceptionResnetV2/Repeat_1/block17_16/Branch_1/Conv2d_0c_7x1/biases:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionResnetV2/Repeat_1/block17_16/Branch_1/Conv2d_0c_7x1/biases)]]


```"
7171,DBpedia datasets not available. 404.,"Trying to run the code in /learn/text_classification returns a 404 when trying to download the DBpedia dataset.
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/text_classification.py

The constant ""DBPEDIA_URL"" in .../learn/python/learn/datasets/text_datasets.py seems to be wrong as it returns a 404 in Chrome as well.

This is the URL in text_datasets.py:
https://googledrive.com/host/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M/dbpedia_csv.tar.gz

Which returns a 404.

"
7170,"tf.pow(x,y) doesn't work for non-integer values of y","**Operating System:** Debian 4.8.15-2
**Installed version of CUDA:** 8.0
**Installed version of cuDNN:** 5.1.5
**The output of `ls -l /path/to/cuda/lib/libcud*`:**
> myuser@mymachine:/mypath$ ls -l /usr/local/cuda-8.0/lib64/libcud*
> -rw-r--r-- 1 root root 558720 sept. 15 01:02 /usr/local/cuda-8.0/lib64/libcudadevrt.a
> lrwxrwxrwx 1 root root     16 sept. 15 01:05 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0
> lrwxrwxrwx 1 root root     19 sept. 15 01:05 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44
> -rw-r--r-- 1 root root 415432 sept. 15 01:02 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.44
> -rw-r--r-- 1 root root 775162 sept. 15 01:02 /usr/local/cuda-8.0/lib64/libcudart_static.a

**A link to the pip package you installed:** Lost in history. I don't think this is the problem, so I'm going to skip it.

**The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`:**
> myuser@mymachine:/mypath$ python -c ""import tensorflow; print(tensorflow.__version__)""
> I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
> I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
> I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
> I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
> I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
> 0.12.1

**Minimal reproducible example:** (using python3)
```
import tensorflow as tf
session = tf.InteractiveSession()
tf.pow(2,1.5).eval()
```

**Logs or other output that would be helpful:**
> Traceback (most recent call last):
>   File ""/mypath/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 490, in apply_op
>     preferred_dtype=default_dtype)
>   File ""/mypath/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 669, in convert_to_tensor
>     ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
>   File ""/mypath/.local/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py"", line 176, in _constant_tensor_conversion_function
>     return constant(v, dtype=dtype, name=name)
>   File ""/mypath/.local/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py"", line 165, in constant
>     tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))
>   File ""/mypath/.local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py"", line 367, in make_tensor_proto
>     _AssertCompatible(values, dtype)
>   File ""/mypath/.local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py"", line 302, in _AssertCompatible
>     (dtype.name, repr(mismatch), type(mismatch).__name__))
> TypeError: Expected int32, got 1.5 of type 'float' instead.
> 
> During handling of the above exception, another exception occurred:
> 
> Traceback (most recent call last):
>   File ""<stdin>"", line 1, in <module>
>   File ""/mypath/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py"", line 507, in pow
>     return gen_math_ops._pow(x, y, name=name)
>   File ""/mypath/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 1705, in _pow
>     result = _op_def_lib.apply_op(""Pow"", x=x, y=y, name=name)
>   File ""/mypath/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 499, in apply_op
>     repr(values), type(values).__name__))
> TypeError: Expected int32 passed to parameter 'y' of op 'Pow', got 1.5 of type 'float' instead.
"
7169,Java: Update README to run the configure script,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
7166,How should Python packages depending on TensorFlow structure their requirements?,"Many packages build on TensorFlow. For example, our work in Edward uses `tensorflow>=1.0.0a0` as an install requirement. 

However, this conflicts with `tensorflow-gpu`, which can no longer be installed because of the requirement specifically on `tensorflow`. What do you suggest is the best way to handle this?

One option suggested by @gokceneraslan (https://github.com/blei-lab/edward/pull/428#issuecomment-276394115) is to hack in the dependency according to whether the user has a GPU. Another option, which PrettyTensor and Keras employ, is to not even require TensorFlow. (Both options sound not good.)

Also see https://github.com/blei-lab/edward/pull/428. also looping in GPflow devs (@jameshensman, @alexggmatthews) in case they have the same problem. (Note I'm raising this as an issue instead of asking on a mailing list, in case this is something that should be changed on TensorFlow's end and not our end.)"
7164,contrib.batch_norm fails on float16 input,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

I have found a bunch of old issues dated from the initial implementation of float16, but nothing relevant.

### Environment info
Operating System: FC21

Installed version of CUDA and cuDNN: CUDA8, CuDNN 5.1
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
```
/cuda8_cudnn5_1/lib64/libcudadevrt.a
/cuda8_cudnn5_1/lib64/libcudart.so -> libcudart.so.8.0
/cuda8_cudnn5_1/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44
/cuda8_cudnn5_1/lib64/libcudart.so.8.0.44
/cuda8_cudnn5_1/lib64/libcudart_static.a
/cuda8_cudnn5_1/lib64/libcudnn.so -> libcudnn.so.5
/cuda8_cudnn5_1/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5
/cuda8_cudnn5_1/lib64/libcudnn.so.5.1.5
/cuda8_cudnn5_1/lib64/libcudnn_static.a
```

If installed from binary pip package, provide:

1. A link to the pip package you installed: today nightly build
https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-0.12.1-cp34-cp34m-linux_x86_64.whl
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
```
>tf.__version__
'0.12.head'
>tf.__git_version__
'0.12.1-2263-g4cc0d1e-dirty'
```

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
```
p16 = tf.placeholder(tf.float16, (4, 16, 16, 3))
bn16 = tf.contrib.layers.batch_norm(p16)
```

It fails with float32 to float16 conversion error deep inside BN op. Traceback is below.

### What other attempted solutions have you tried?
For now working with float32. I tried to track where exactly it fails, but I couldn't. It looks like computed mean value is float32 by default although the variable itself should be `inputs.dtype.base_dtype` and `tf.nn.moments` returns `float16` with corresponding input.

### Logs or other output that would be helpful
```
ValueError                                Traceback (most recent call last)
<ipython-input-5-e659548f8d6c> in <module>()
----> 1 bn16 = tf.contrib.layers.batch_norm(p16)

/home/konstantin/.local/lib/python3.4/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py in func_with_args(*args, **kwargs)
    175       current_args = current_scope[key_func].copy()
    176       current_args.update(kwargs)
--> 177     return func(*args, **current_args)
    178   _add_op(func)
    179   setattr(func_with_args, '_key_op', _key_op(func))

/home/konstantin/.local/lib/python3.4/site-packages/tensorflow/contrib/layers/python/layers/layers.py in batch_norm(inputs, decay, center, scale, epsilon, activation_fn, param_initializers, updates_collections, is_training, reuse, variables_collections, outputs_collections, trainable, batch_weights, fused, data_format, zero_debias_moving_mean, scope)
    516           _scope=sc,
    517           _reuse=reuse)
--> 518       outputs = layer.apply(inputs, training=is_training)
    519
    520       # Add variables to collections.

/home/konstantin/.local/lib/python3.4/site-packages/tensorflow/python/layers/base.py in apply(self, inputs, **kwargs)
    301       Output tensor(s).
    302     """"""
--> 303     return self.__call__(inputs, **kwargs)
    304
    305

/home/konstantin/.local/lib/python3.4/site-packages/tensorflow/python/layers/base.py in __call__(self, inputs, **kwargs)
    271             self.build(input_shapes)
    272           self._built = True
--> 273         outputs = self.call(inputs, **kwargs)
    274
    275         # Apply activity regularization.

/home/konstantin/.local/lib/python3.4/site-packages/tensorflow/python/layers/normalization.py in call(self, inputs, training)
    191       if not self.updates:
    192         mean_update = moving_averages.assign_moving_average(
--> 193             self.moving_mean, mean, self.momentum, zero_debias=False)
    194         variance_update = moving_averages.assign_moving_average(
    195             self.moving_variance, variance, self.momentum, zero_debias=False)

/home/konstantin/.local/lib/python3.4/site-packages/tensorflow/python/training/moving_averages.py in assign_moving_average(variable, value, decay, zero_debias, name)
     70         update_delta = _zero_debias(variable, value, decay)
     71       else:
---> 72         update_delta = (variable - value) * decay
     73       return state_ops.assign_sub(variable, update_delta, name=scope)
     74

/home/konstantin/.local/lib/python3.4/site-packages/tensorflow/python/ops/variables.py in _run_op(a, *args)
    704     def _run_op(a, *args):
    705       # pylint: disable=protected-access
--> 706       return getattr(ops.Tensor, operator)(a._AsTensor(), *args)
    707     # Propagate __doc__ to wrapper
    708     try:

/home/konstantin/.local/lib/python3.4/site-packages/tensorflow/python/ops/math_ops.py in binary_op_wrapper(x, y)
    885     with ops.name_scope(None, op_name, [x, y]) as name:
    886       if not isinstance(y, sparse_tensor.SparseTensor):
--> 887         y = ops.convert_to_tensor(y, dtype=x.dtype.base_dtype, name=""y"")
    888       return func(x, y, name=name)
    889

/home/konstantin/.local/lib/python3.4/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype)
    649       name=name,
    650       preferred_dtype=preferred_dtype,
--> 651       as_ref=False)
    652
    653

/home/konstantin/.local/lib/python3.4/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype)
    714
    715         if ret is None:
--> 716           ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
    717
    718         if ret is NotImplemented:

/home/konstantin/.local/lib/python3.4/site-packages/tensorflow/python/framework/ops.py in _TensorTensorConversionFunction(t, dtype, name, as_ref)
    587     raise ValueError(
    588         ""Tensor conversion requested dtype %s for Tensor with dtype %s: %r""
--> 589         % (dtype.name, t.dtype.name, str(t)))
    590   return t
    591

ValueError: Tensor conversion requested dtype float32 for Tensor with dtype float16: 'Tensor(""BatchNorm/Reshape_1:0"", shape=(3,), dtype=float16)'
```"
7162,"Can't quantize nodes of an RNN (""The node has inputs from different frames."")","### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

I found a few similar, but not equivalent, problems with frozen/quantized graphs:

Cannot import graph_def for 8-bit Quantized cnn model #5470

tf.import_graph_def: graph_def is invalid at node #4044

Unable to import frozen graph with batchnorm #3628 

### Environment info
Operating System: Ubuntu 16.10

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

```
-rw-r--r-- 1 root root   558720 Sep 14 20:02 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 Sep 14 20:05 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19 Sep 14 20:05 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44
-rw-r--r-- 1 root root   415432 Sep 14 20:02 /usr/local/cuda/lib64/libcudart.so.8.0.44
-rw-r--r-- 1 root root   775162 Sep 14 20:02 /usr/local/cuda/lib64/libcudart_static.a
lrwxrwxrwx 1 root root       13 Jan 27 22:01 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 root root       17 Jan 27 22:01 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5
-rwxr-xr-x 1 root root 79337624 Jan 27 22:01 /usr/local/cuda/lib64/libcudnn.so.5.1.5
-rw-r--r-- 1 root root 69756172 Jan 27 22:01 /usr/local/cuda/lib64/libcudnn_static.a
```

If installed from binary pip package, provide:

1. A link to the pip package you installed: https://pypi.python.org/pypi/tensorflow-gpu
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`:

```
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
0.12.1
```

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

I reduced this to a simple RNN model available here on branch tf_issue_7162: https://github.com/reuben/tf-export-test/tree/tf_issue_7162

You don't have to train it again, the repository includes a checkpoint with trained weights. If you run the commands starting from the freeze_graph in the [README](https://github.com/reuben/tf-export-test/blob/master/README.md) there, you get this error when importing the quantized graph:

```
Traceback (most recent call last):
  File ""import.py"", line 14, in <module>
    imports = tf.import_graph_def(pb, name="""")
  File ""/home/reuben/.local/lib/python2.7/site-packages/tensorflow/python/framework/importer.py"", line 339, in import_graph_def
    % (input_name,)))
ValueError: graph_def is invalid at node u'RNN/cond/Greater/y': More inputs specified ('RNN/cond/Switch:0') than the op expects..
```"
7160,Decrease accuracy after upgrade from v0.11 to v0.12,"Hi! I've recently updated from v0.11 to v.012 using pip with CUDA 8.0, cuDNN 5.1in a TitanX under Ubuntu 16.04 LTS.

During the training of a small CNN triplet network with triplet margin loss I noticed that the performance in terms of accuracy went down by a factor of ~40%.  Getting around 9% and 14% FPR95 with v0.11 and v0.12 respectively, both at epoch 30th. Notice with v0.12 the accuracy gets stuck in early epoch 10th.

The optimizer that I'm using is Momentum with a LR of 1e-4 without decay policy.
I've also added the regularization term with a weight decay of 1e-4.

Did you introduce any change with last versions in the optimization API that could make this difference?

PD: I left here the link to the code.
https://github.com/vbalnt/tfeat/tree/master/tensorflow
Notice that we are trying to reproduce from the original LuaTorch version."
7159,Tensorflow restore doesn't work on AWS,"I am running a script that perfectly works on ubuntu 14.04, docker over ubuntu 14.04, docker over MAC OS - but doesn't work on ubuntu 16 on AWS.
the script is:

```
import tensorflow as tf
import os

def load_model_to_session(session, model_dir, meta_file, ckpt_file):
    model_dir_exp = os.path.expanduser(model_dir)
    saver = tf.train.import_meta_graph(os.path.join(model_dir_exp, meta_file))
    saver.restore(session, os.path.join(model_dir_exp, ckpt_file))

session = tf.Session()
print ('loading models')
model_dir = '/data/model'
meta_file, ckpt_file = 'a.meta', 'a.ckpt'
print ('ckpt file ' + ckpt_file)
load_model_to_session(session, model_dir, meta_file, ckpt_file)
print ('finish loading the model')
session.close()

`````

the errors i get:

> W tensorflow/core/framework/op_kernel.cc:975] Data loss: Unable to open table file /data/model/a.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?
> W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /data/model/a.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?
> W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /data/model/a.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?
> W tensorflow/core/framework/op_kernel.cc:975] Data loss: Unable to open table file /data/model/a.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?
> W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /data/model/a.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?
> W tensorflow/core/framework/op_kernel.cc:975] Data loss: Unable to open table file /data/model/a.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?
> Traceback (most recent call last):
>   File ""/tmp/runner.py"", line 35, in <module>
>     load_model_to_session(session, model_dir, meta_file, ckpt_file)
>   File ""/tmp/runner.py"", line 7, in load_model_to_session
>     saver.restore(session, os.path.join(model_dir_exp, ckpt_file))
>   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1388, in restore
>     {self.saver_def.filename_tensor_name: save_path})
>   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 766, in run
>     run_metadata_ptr)
>   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 964, in _run
>     feed_dict_string, options, run_metadata)
>   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1014, in _do_run
>     target_list, options, run_metadata)
>   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1034, in _do_call
>     raise type(e)(node_def, op, message)
> tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /data/model/a.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?
>          [[Node: save/restore_slice_318 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](_recv_save/Const_0, save/restore_slice_318/tensor_name, save/restore_slice_318/shape_and_slice)]]
> 
> Caused by op u'save/restore_slice_318', defined at:
>   File ""/tmp/runner.py"", line 35, in <module>
>     load_model_to_session(session, model_dir, meta_file, ckpt_file)
>   File ""/tmp/runner.py"", line 6, in load_model_to_session
>     saver = tf.train.import_meta_graph(os.path.join(model_dir_exp, meta_file))
>   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1526, in import_meta_graph
>     **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/meta_graph.py"", line 502, in import_scoped_meta_graph
>     producer_op_list=producer_op_list)
>   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py"", line 285, in import_graph_def
>     op_def=op_def)
>   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2240, in create_op
>     original_op=self._default_original_op, op_def=op_def)
>   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1128, in __init__
>     self._traceback = _extract_stack()
> 
> DataLossError (see above for traceback): Unable to open table file /data/model/a.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?
>          [[Node: save/restore_slice_318 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](_recv_save/Const_0, save/restore_slice_318/tensor_name, save/restore_slice_318/shape_and_slice)]]
> 

in order to reproduce this, you can use the pre-trained open source model here:
https://drive.google.com/file/d/0B5MzpY9kBtDVSTgxX25ZQzNTMGc/view

I'm using tensorflow (python) 0.12.1 .

I suspect that it has something to do with the virtualization (AWS).

BTW, the code works on the pretrained model http://download.tensorflow.org/models/inception_v1_2016_08_28.tar.gz"
7158,Documentation for Inference from saved model,"As explained at https://www.tensorflow.org/tutorials/mnist/pros/#build_a_multilayer_convolutional_network I created a CNN for MNIST image recognition. I don't want to test it in the same script so I trained the model, saved the checkpoint files and graph structure and then using freeze_graph.py script(https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py)  I froze it into single .pb file. Now using this .pb file I want to check the accuracy of the model but there is no clear documentation as to how to do it. There is one example written in examples/label_images directory but that too is in C++ and for inception network. I have already parsed the .pb file and made a tf.Graph object.  "
7157,AttributeError: 'module' object has no attribute 'constant',"I've just installed tensorflow (without the gpu) on my mac, through my virtual environment. I initially had tensorflow downloaded in my ~/ directory, but then found that I could not import it at all in python3.4. I found that I fixed this by making a copy of the tensorflow directory in my working directory fixed that. But now the problem is that once I import tensorflow with $ import tensorflow as tf, I get the message: 

`>>> a = tf.constant('Hello')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: 'module' object has no attribute 'constant'`"
7156,Spatial Cross Entropy Loss - Feature Request,"For Fully Convolutional Networks, the logits are reshaped into 2D tensors `[batch_size, n_classes]`. Can't there be a spatial loss function similar to [SpatialClassNLLCriterion](https://github.com/torch/cunn/blob/master/lib/THCUNN/SpatialClassNLLCriterion.cu') built by Torch?"
7152,Library not loaded: @rpath/AdobeCreativeSDKCore.framework/AdobeCreativeSDKCore,"I just completed migrating my project from swift 2 to swift 3. I had installed AdobeCreativeSDKCore and AdobeCreativeSDKImages as pod file. The code builds but I get a ""**_Thread 1: signal SIGABRT_**"" error in the ""**_0_abory_with_payload_**"" file and ""**_10_dyld_start_**"" 

```
dyld`__abort_with_payload:
    0x1063966f8 <+0>:  movl   $0x2000209, %eax          ; imm = 0x2000209 
    0x1063966fd <+5>:  movq   %rcx, %r10
    0x106396700 <+8>:  syscall 
->  0x106396702 <+10>: jae    0x10639670c               ; <+20>
    0x106396704 <+12>: movq   %rax, %rdi
    0x106396707 <+15>: jmp    0x106396014               ; cerror_nocancel
    0x10639670c <+20>: retq   
    0x10639670d <+21>: nop    
    0x10639670e <+22>: nop    
    0x10639670f <+23>: nop    
```

```
dyld`_dyld_start:
    0x10636e000 <+0>:   popq   %rdi
    0x10636e001 <+1>:   pushq  $0x0
    0x10636e003 <+3>:   movq   %rsp, %rbp
    0x10636e006 <+6>:   andq   $-0x10, %rsp
    0x10636e00a <+10>:  subq   $0x10, %rsp
    0x10636e00e <+14>:  movl   0x8(%rbp), %esi
    0x10636e011 <+17>:  leaq   0x10(%rbp), %rdx
    0x10636e015 <+21>:  movq   0x3ef04(%rip), %r8        ; _dyld_start_static
    0x10636e01c <+28>:  leaq   -0x23(%rip), %rcx         ; <+0>
    0x10636e023 <+35>:  subq   %r8, %rcx
    0x10636e026 <+38>:  leaq   -0x102d(%rip), %r8
    0x10636e02d <+45>:  leaq   -0x8(%rbp), %r9
    0x10636e031 <+49>:  callq  0x10636e073               ; dyldbootstrap::start(macho_header const*, int, char const**, long, macho_header const*, unsigned long*)
->  0x10636e036 <+54>:  movq   -0x8(%rbp), %rdi
    0x10636e03a <+58>:  cmpq   $0x0, %rdi
    0x10636e03e <+62>:  jne    0x10636e050               ; <+80>
    0x10636e040 <+64>:  movq   %rbp, %rsp
    0x10636e043 <+67>:  addq   $0x8, %rsp
    0x10636e047 <+71>:  movq   $0x0, %rbp
    0x10636e04e <+78>:  jmpq   *%rax
    0x10636e050 <+80>:  addq   $0x10, %rsp
    0x10636e054 <+84>:  pushq  %rdi
    0x10636e055 <+85>:  movq   0x8(%rbp), %rdi
    0x10636e059 <+89>:  leaq   0x10(%rbp), %rsi
    0x10636e05d <+93>:  leaq   0x8(%rsi,%rdi,8), %rdx
    0x10636e062 <+98>:  movq   %rdx, %rcx
    0x10636e065 <+101>: movq   (%rcx), %r8
    0x10636e068 <+104>: addq   $0x8, %rcx
    0x10636e06c <+108>: testq  %r8, %r8
    0x10636e06f <+111>: jne    0x10636e065               ; <+101>
    0x10636e071 <+113>: jmpq   *%rax
```

On the console I get
```
dyld: Library not loaded: @rpath/AdobeCreativeSDKCore.framework/AdobeCreativeSDKCore
  Referenced from: /Users/kelvinnjeri/Library/Developer/CoreSimulator/Devices/67DEC3A4-74B9-4203-97DA-004B8B4A254E/data/Containers/Bundle/Application/2A3FB423-BA0D-4DBD-9B51-748428D91D36/CloudiT.app/CloudiT
  Reason: image not found
(lldb) 
```
I've been checking the podfile, I seem to have the framework in the ""Pods"" droplist but not in the products drop list 

![screen shot 2017-01-30 at 4 23 24 pm](https://cloud.githubusercontent.com/assets/20662833/22442415/493d67ca-e709-11e6-81ef-de57964aa741.png)

![screen shot 2017-01-30 at 4 25 32 pm](https://cloud.githubusercontent.com/assets/20662833/22442284/cdca07ce-e708-11e6-895d-4cedfea7da17.png)

Im also not able to add it to the build phases cause Xcode is not giving me that option 

![screen shot 2017-01-30 at 4 28 21 pm](https://cloud.githubusercontent.com/assets/20662833/22442380/259b7f0a-e709-11e6-8d4a-fe54256d465c.png)

I'm sure it's a simple fix, probably somewhere in the settings but I'm don't know where. I am still new to iOS development. Any response is greatly appreciated "
7150,Quantization:quantize_graph produces corrupted graph,"Hi,
sorry to bother again. In order to save some space I would like to quantize my graph, but everytime I run quantize_graph, it becomes unusable in android app later on. I would like to generally ask, how is this facility meant to be used. What I have done:


1. retrained graph (works fine)
2. optimized it using optimize_for_inference (works fine)
3. bazel-bin/tensorflow/tools/quantization/quantize_graph \
--input=/tmp/optimized.pb \
--output=/tmp/eightbit.pb \
--output_node_names=""final_result_a,final_result_b"" \
--mode=eightbit

Then it threws on android following (excerpt):

`I/native: tensorflow_inference_jni.cc:85 Creating new session variables for 635cce3c13fa1ff4
I/native: tensorflow_inference_jni.cc:113 Loading Tensorflow.
I/native: tensorflow_inference_jni.cc:120 Session created.
I/native: tensorflow_inference_jni.cc:126 Acquired AssetManager.
I/native: tensorflow_inference_jni.cc:128 Reading file to proto: file:///android_asset/eightbit.pb
I/native: jni_utils.cc:111 Opening asset eightbit.pb from disk with zero-copy.
I/native: tensorflow_inference_jni.cc:132 GraphDef loaded from file:///android_asset/eightbit.pb with 1345 nodes.
I/native: stat_summarizer.cc:38 StatSummarizer found 1345 nodes
I/native: tensorflow_inference_jni.cc:139 Creating TensorFlow graph from GraphDef.
I/native: tensorflow_inference_jni.cc:151 Initialization done in 234.141ms
`
Which I suppose is correct

But then:

`
I/native: tensorflow_inference_jni.cc:228 End computing. Ran in 2665ms (2665ms avg over 1 runs)
A/libc: Fatal signal 11 (SIGSEGV), code 1, fault addr 0xb95eb000 in tid 31217 (inference)
Application terminated.
`
Would you please once again help? Thanks

"
7149,[Java][Feature] Generating operation methods used to build a graph,"As suggested in https://www.tensorflow.org/versions/r0.11/how_tos/language_bindings, the list of operation methods for building a graph should be generated dynamically from the list exposed by the core, using preferably protobuf. 

Is anyone already working on that feature? If not, I'm tempted to try it. I was thinking of generating a builder hierarchy that allows to optionally set an argument after adding an operation. For example
```java
GraphBuider
    .matMul(a, b)
        .withTransposeB(true)
    .softmax(logits)
    ...
```
and the GraphBuilder classes would be generated at build time by Bazel and would make use of the already existing OperationBuilder. Since I'm new to Tensorflow, please tell me if that doesn't make any sense to you

Thanks

(p.s. as suggested by @drpngx, I've started this discussion as a seperate issue to avoid continuing the #5 saga)"
7146,Nested `tf.while_loop`s hang when they are placed on CPU,"When I place all ops within nested `tf.while_loop` on CPU, evaluating the output of nested `tf.while_loop` makes the program hang.

I also [posted a question on StackOverflow](http://stackoverflow.com/questions/41929472/why-does-nested-tf-while-loop-freeze-in-test-session-of-tf-test-testcase).

### Environment info
Operating System: Ubuntu 16.04

Installed version of CUDA and cuDNN: 
```
/usr/local/cuda/lib64/libcudadevrt.a
/usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
/usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44
/usr/local/cuda/lib64/libcudart.so.8.0.44
/usr/local/cuda/lib64/libcudart_static.a
/usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5
/usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5
/usr/local/cuda/lib64/libcudnn.so.5.1.5
/usr/local/cuda/lib64/libcudnn_static.a
```

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
`28f5099d532ce59787ee58012b8ef04c498947ae`

2. The output of `bazel version`
```
Build label: 0.4.3
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Dec 22 12:31:25 2016 (1482409885)
Build timestamp: 1482409885
Build timestamp as int: 1482409885
```

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

The following code is run on python 3.5:

``` python
import tensorflow as tf

config = tf.ConfigProto(allow_soft_placement=False)
with tf.Session(config=config) as sess, tf.device('/cpu:0'):
    num_outer_iters = tf.constant(3)
    num_inner_iters = tf.constant(5)

    def outer_body(loop_outer_index, loop_outer_ta):

        def inner_body(loop_inner_index, loop_inner_ta):
            loop_inner_ta = loop_inner_ta.write(loop_inner_index,
                                                tf.constant(0))
            return (loop_inner_index + 1, loop_inner_ta)

        inner_index = tf.constant(0)
        inner_ta = tf.TensorArray(tf.int32, num_inner_iters)
        (_, inner_ta) = tf.while_loop(
            lambda index, *_: index < num_inner_iters,
            inner_body,
            (inner_index, inner_ta))

        loop_outer_ta = loop_outer_ta.write(loop_outer_index,
                                            inner_ta.stack())
        return loop_outer_index + 1, loop_outer_ta

    outer_index = tf.constant(0)
    outer_ta = tf.TensorArray(tf.int32, num_outer_iters)
    (_, outer_ta) = tf.while_loop(
        lambda index, *_: index < num_outer_iters,
        outer_body,
        (outer_index, outer_ta),
        parallel_iterations=1, back_prop=False)

    print(sess.run(outer_ta.concat()))

```

### What other attempted solutions have you tried?

If I replace `num_inner_iters = tf.constant(5)` with `num_inner_iters = 5`, the program will exit normally.

### Logs or other output that would be helpful
The program hangs without any error message.
"
7145,GPU not registrering after pip update to 0.12 from 0.10,"Since upgrading via pip, tensorflow does not use my GPU.

```
>>> from tensorflow.python.client import device_lib
>>> [x.name for x in device_lib.list_local_devices()]
['/cpu:0']
>>>
```

Device looks correctly installed
```
/usr/local/cuda-8.0/samples/1_Utilities/deviceQuery/deviceQuery Starting...

 CUDA Device Query (Runtime API) version (CUDART static linking)

Detected 1 CUDA Capable device(s)

Device 0: ""GeForce GTX 970""
  CUDA Driver Version / Runtime Version          8.0 / 8.0
  CUDA Capability Major/Minor version number:    5.2
  Total amount of global memory:                 4036 MBytes (4231528448 bytes)
  (13) Multiprocessors, (128) CUDA Cores/MP:     1664 CUDA Cores
  GPU Max Clock rate:                            1216 MHz (1.22 GHz)
  Memory Clock rate:                             3505 Mhz
  Memory Bus Width:                              256-bit
  L2 Cache Size:                                 1835008 bytes
  Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)
  Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers
  Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers
  Total amount of constant memory:               65536 bytes
  Total amount of shared memory per block:       49152 bytes
  Total number of registers available per block: 65536
  Warp size:                                     32
  Maximum number of threads per multiprocessor:  2048
  Maximum number of threads per block:           1024
  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)
  Maximum memory pitch:                          2147483647 bytes
  Texture alignment:                             512 bytes
  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)
  Run time limit on kernels:                     No
  Integrated GPU sharing Host Memory:            No
  Support host page-locked memory mapping:       Yes
  Alignment requirement for Surfaces:            Yes
  Device has ECC support:                        Disabled
  Device supports Unified Addressing (UVA):      Yes
  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0
  Compute Mode:
     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >

deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 8.0, CUDA Runtime Version = 8.0, NumDevs = 1, Device0 = GeForce GTX 970
Result = PASS
```

Theano works fine

```
> python -c ""import theano""
Using gpu device 0: GeForce GTX 970 (CNMeM is disabled, cuDNN 5105)
```

Added the following to ~/.bashrc
```
export CUDA_HOME=/usr/local/cuda-8.0
export CUDA_ROOT=/usr/local/cuda-8.0
export PATH=$PATH:$CUDA_ROOT/bin
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CUDA_ROOT/lib64:$CUDA_ROOT/extras/CUPTI/lib64
```

Anybody have an idea how to figure this out? I could off cause try to re-install everything, but I would rather figure out why this isn't working.

thanks in advance :)
 
NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

None

### Environment info
Operating System:

Ubuntu 16.04

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

Cuda 8.0, cuDNN 5.1

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

0.12.1

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)



### What other attempted solutions have you tried?

reinstalling cuDNN

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
7142,AttributeError: 'GFile' object has no attribute 'tell',"I'm using python 2.7.6, pip 9.0.1 and tensorflow 0.12.1 under Ubuntu 14.04. 

When I ran the script using this line,

import input_data
mnist_images = input_data.read_data_sets( ""MNIST_data/"", one_hot=True )

I got message like this:

Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.
Extracting MNIST_data/train-images-idx3-ubyte.gz
Traceback (most recent call last):
  File ""mnist.py"", line 5, in <module>
    mnist_images = input_data.read_data_sets( ""MNIST_data/"", one_hot=True )
  File ""~/machine_learn/input_data.py"", line 194, in read_data_sets
    train_images = extract_images(local_file)
  File ""~/machine_learn/input_data.py"", line 54, in extract_images
    magic = _read32(bytestream)
  File ""~/machine_learn/input_data.py"", line 47, in _read32
    return numpy.frombuffer(bytestream.read(4), dtype=dt)[0]
  File ""/usr/lib/python2.7/gzip.py"", line 261, in read
    self._read(readsize)
  File ""/usr/lib/python2.7/gzip.py"", line 288, in _read
    pos = self.fileobj.tell()   # Save current position
AttributeError: 'GFile' object has no attribute 'tell'

And the script ""input_data.py"" is from https://github.com/tensorflow/blob/r0.7/tensorflow/examples/tutorials/mnist/input_data.py 

"
7141,"iOS example missing #include ""tensorflow/core/framework/types.pb.h""","NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System: iOS 10.2

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`): not related it is iOS

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
7140,Invert bijector in tf.contrib.distributions fails to generate samples,"Sampling from the example provided in the docstring fails with recursion errors.
```python
ds = tf.contrib.distributions

exp_gamma = ds.TransformedDistribution(
  distribution=ds.Gamma(1.0, 2.0),
  bijector=ds.bijector.Invert(ds.bijector.Exp()))

sess = tf.Session()
sess.run(exp_gamma.sample())
  File ""/Users/dvt/Envs/venv/lib/python2.7/site-packages/tensorflow/contrib/distributions/python/ops/bijector.py"", line 556, in _get_inverse_event_shape
    return self._get_inverse_event_shape(tensor_shape.TensorShape(output_shape))
  ..
  File ""/Users/dvt/Envs/venv/lib/python2.7/site-packages/tensorflow/contrib/distributions/python/ops/bijector.py"", line 556, in _get_inverse_event_shape
    return self._get_inverse_event_shape(tensor_shape.TensorShape(output_shape))
  File ""/Users/dvt/Envs/venv/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py"", line 436, in __init__
    elif isinstance(dims, compat.bytes_or_text_types):
RuntimeError: maximum recursion depth exceeded while calling a Python object
```
I run into similar errors when using the Invert bijector for other distributions/bijectors.

I am using TensorFlow v1.0.1 and Python 2.7.

@jvdillon"
7138,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine.","Hello, 

My CPU is AMD Phenom II x6 1090t, I don't know if that's relevant.
I have an issue with my tensorflow, I have tried a few things and nothing is working. Here is the error I'm getting when I run my script. 

```
chad@chad-GA-990XA-UD3:~/tensorflow$ ./configure
Please specify the location of python. [Default is /usr/bin/python]:
Please specify optimization flags to use during compilation [Default is -march=native]:
Do you wish to use jemalloc as the malloc implementation? [Y/n]
jemalloc enabled
Do you wish to build TensorFlow with Google Cloud Platform support? [y/N]
No Google Cloud Platform support will be enabled for TensorFlow
Do you wish to build TensorFlow with Hadoop File System support? [y/N]
No Hadoop File System support will be enabled for TensorFlow
Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N]
No XLA support will be enabled for TensorFlow
Found possible Python library paths:
  /usr/local/lib/python2.7/dist-packages
  /usr/lib/python2.7/dist-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]

Using python library path: /usr/local/lib/python2.7/dist-packages
Do you wish to build TensorFlow with OpenCL support? [y/N]
No OpenCL support will be enabled for TensorFlow
Do you wish to build TensorFlow with CUDA support? [y/N] y
CUDA support will be enabled for TensorFlow
Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]:
Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0
Please specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:
Please specify the Cudnn version you want to use. [Leave empty to use system default]: 5
Please specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:
Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size.
[Default is: ""3.5,5.2""]: 6.1
```

Here is the error i get when i run my script.
```
2017-01-30 00:45:59: I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
2017-01-30 00:46:00: I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
2017-01-30 00:46:00: I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
2017-01-30 00:46:00: I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
2017-01-30 00:46:00: I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
2017-01-30 00:46:00: F tensorflow/core/platform/cpu_feature_guard.cc:35] The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine.
Aborted (core dumped)
```

I followed this tutorial generally https://alliseesolutions.wordpress.com/2016/09/08/install-gpu-tensorflow-from-sources-w-ubuntu-16-04-and-cuda-8-0-rc/

Can anyone help me?"
7136,"../genop/main.go:15: running ""sh"": exit status 1","I am unable to get the Go binding for TensorFlow working on OS X 10.12. I followed the installation instructions at https://github.com/tensorflow/tensorflow/tree/master/tensorflow/go.

Things go wrong when generating the wrapper functions for TensorFlow ops:
go generate github.com/tensorflow/tensorflow/tensorflow/go/op

The output from the above command is:
../genop/main.go:15: running ""sh"": exit status 1
op/generate.go:15: running ""go"": exit status 1

When browsing the sources i see an imported package
""github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/tensorflow/core/framework""
which isn't there. Maybe that is causing the issue?
"
7134,[Java] [Feature] Load from SavedModel,"Make it possible for the Java library to load exported TF models in SavedModel format.   

Outline of the proposed design:
1. Define a new class `SavedModelBundle` to represent the loaded saved model.  Properties include the graph, session, and metagraphdef.
2. Define a static method on `SavedModelBundle` to load from disk, given an export path and some tags.

```
class SavedModelBundle implements AutoCloseable { 
  public Graph graph() { ... }
  public Session session() { ... }
  public byte[] metaGraphDef() { ... }
  public void close() { /* close the session and graph */ }
  public static SavedModelBundle loadSavedModel(String exportDir, Set<String> tags) { ... }
}
```

3. Write a JNI method to support loading, based on `TF_LoadSessionFromSavedModel` from the C API."
7133,Loading Files with New Tensorflow,"Using newest version of Tensorflow on Linux.

Ok so I had a working Saver.restore going for literally months and everyone was fine:
saver = tf.train.Saver()
saver.restore(sess, ""deep_tweet_lstm_w-300000"")

deep_tweet_lstm_w-300000 was the name of the file with the data points.

Ever since updating my Tensorflow I get this error: 
NotFoundError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to find any matching files for deep_tweet_lstm_w-300000

I honestly find this change infuriating, how in the world do I load this file with the new Tensorflow?  "
7130,Possible bug in inception_v1 in slim networks library,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

This question is probably not relevant to this issue. I did a Google search and didn't find any. 

### Environment info
Operating System:

**Windows 7 - 64 bit**

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

**None.**

### What other attempted solutions have you tried?


`<                 branch_2, 32, [3, 3], scope='Conv2d_0b_3x3')`
`---`
`>                 branch_2, 32, [5, 5], scope='Conv2d_0b_5x5')`


The inception_v1.py in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/inception_v1.py, the inception layer is different from the paper. In the paper there is a 3x3 conv2d and a 5x5 conv2d module. I see that other frameworks have a 5x5 module and only here there are 2 3x3 modules instead.

Is this intentional or a bug ?





"
7129,Possible bug in ctc_beam_search_test.cc ?,"Hello,

I've found something very confusing in the above file, I can't see how [line number 54](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/ctc/ctc_beam_search_test.cc#L54) can work correctly.

```
 to_state->labels.push_back(to_label);
```

From what I can make out each child beam entry has a new state associated with it, so the `to_state` being passed to the `ExpandState` method should only ever be called with a single label, representing the current class at that time.

I'm attempting to integrate a full n-gram language model into a beam scorer and this certainly seems to be the case. In order to build a string of all characters in the beam I must make a reference back to `from_state` in my `HistoryBeamState` and traverse back up the tree.

This is not the case in the unit test though, and in fact if I log some output there I can see the `to_state` being reused with different `to_label` values. Hence the test does pass.

Have I greatly misunderstood something here or is this not what the expected behaviour should be? I notice the test has a beam width less that the number of classes in the input tensor, which I guess is not so realistic? Perhaps I'm seeing an artifact of this?

Any feedback would be great, I think understanding is correct but can't make sense of this unit test."
7128,QR decomposition is slow,"We are doing a bunch of of QR decompositions in numpy. I did preliminary investigation of moving them to TF, but TF version is slow compared to numpy.

Below is a benchmark script that runs QR decomposition of 4096x4096 matrix. It took 7.3 seconds in TF and 1.93 in numpy MKL. Numpy MKL is the default numpy that comes when installing Anaconda.

version: HEAD from last week, built with `--config=cuda --config=opt`
cpu: 32 core Intel(R) Xeon(R) CPU E5-2630 v3 @ 2.40GHz
https://github.com/yaroslavvb/stuff/blob/master/tiny_runs/qr_test.py

Note that `pip install --upgrade $TF_BINARY_URL` will overwrite MKL numpy with OpenBLAS numpy that is actually slower than TF version. The way to check is to look at `np.__config__.show()` and look for strings like `mkl_intel_lp64`. You can get MKL version back by uninstalling numpy and doing `conda install numpy`

@rmlarsen 
"
7127,graph nodes inside sess.run() brackets should be garbage-collected,"My input pipeline returns current epoch, and batch of images.

epoch,label_batch,image_batch  = my_input_pipeline(.....)
.....

Two examples:
`sess.run(epoch[0],train_step_run)`
gives 15 examples per second and RAM get filled over time very slowly.
`
sess.run(epoch,train_step_run)`
runs 100 examples per second and is stable. 

It would be great to make sess.run brackets ""special"" so that some TF function within it does not construct an infinite graph.
"
7126,No module named tensorflow,"System: macos sierra
Python 3.5.2 via anaconda, a special env `tensorflow` created and used (`source activate tensorflow`)

Here is how I installed the package:

```
  export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-0.12.1-py3-none-any.whl
  pip install --ignore-installed --upgrade $TF_BINARY_URL
```

pip says all packages have been successfully installed, but if I try to import tensorflow, python says there is no such module. 

It seems the module is named `tensorflow_gpu`:

```
(tensorflow) kaiyin@kaiyins-mbp 13:19:02 | ~ =>
pip show tensorflow
(tensorflow) kaiyin@kaiyins-mbp 13:19:09 | ~ =>
pip show tensorflow_gpu
Name: tensorflow-gpu
Version: 0.12.1
Summary: TensorFlow helps the tensors flow
Home-page: http://tensorflow.org/
Author: Google Inc.
Author-email: opensource@google.com
License: Apache 2.0
Location: /Users/kaiyin/anaconda3/envs/tensorflow/lib/python3.5/site-packages
Requires: wheel, numpy, protobuf, six
```

Although `tensorflow_gpu` couldn't be imported, either. 

Any idea what's going on here? 
"
7123,try ... catch like structure when loading tfrecords?,"When transferring tfrecords from place to place, some of them might be damaged or become incomplete. In this case, tensorflow will just return a queue error. 

Is it possible to put some try...catch structure into the data loading process to avoid the interuption?

Some of the core errors:
`W tensorflow/core/framework/op_kernel.cc:975] Invalid argument: Could not parse example inpu`

`tensorflow.python.framework.errors_impl.OutOfRangeError: FIFOQueue '_0_batch/fifo_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: batch = QueueDequeueMany[_class=[""loc:@batch/fifo_queue""], component_types=[DT_FLOAT, DT_INT64, DT_STRING], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](batch/fifo_queue, batch/n)]]

Caused by op u'batch', defined at:
  File ""check_tf_records.py"", line 34, in <module>`

`OutOfRangeError (see above for traceback): FIFOQueue '_0_batch/fifo_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: batch = QueueDequeueMany[_class=[""loc:@batch/fifo_queue""], component_types=[DT_FLOAT, DT_INT64, DT_STRING], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](batch/fifo_queue, batch/n)]]`
"
7118,Cannot run TensorFlow on GPU - RHEL 6,"Hello,
I have been trying for days to take advantage of the in one of the machines I have access to. 
Given that **I have no root**  access I had to compile everything from source. 
I tried both the last stable release and the current master branch but I had no luck at running TensorFlow on the GPU. 

My setup is the following : 
Red Hat EL 6.8 (no root access)
Python 2.7.8
virtualenv 13.1.0
devtoolset-4 (GCC 5.3.1)
Bazel 0.4.3 (built from source)
GeForce GTX680 (compute capability 3.0)
Cuda Toolkit 8.0
cuDNN 5.1

I had to modify few configuration files such that the configure script could complete successfully : 
```diff
diff --git a/configure b/configure
index a8e7bb773..002094aba 100755
--- a/configure
+++ b/configure
@@ -39,7 +39,7 @@ function bazel_clean_and_fetch() {
   # bazel clean --expunge currently doesn't work on Windows
   # TODO(pcloudy): Re-enable it after bazel clean --expunge is fixed.
   if ! is_windows; then
-    bazel clean --expunge
+    bazel clean --expunge_async
   fi
   bazel fetch ""//tensorflow/... -//tensorflow/examples/android/...""
 }


diff --git a/tensorflow/core/platform/default/build_config.bzl b/tensorflow/core/platform/default/build_config.bzl
index ebf835d11..824471640 100644
--- a/tensorflow/core/platform/default/build_config.bzl
+++ b/tensorflow/core/platform/default/build_config.bzl
@@ -8,7 +8,7 @@ load(""//tensorflow:tensorflow.bzl"", ""if_not_mobile"")
 WITH_GCP_SUPPORT = False
 WITH_HDFS_SUPPORT = False
 WITH_XLA_SUPPORT = False
-WITH_JEMALLOC = True
+WITH_JEMALLOC = False
 
 # Appends a suffix to a list of deps.
 def tf_deps(deps, suffix):


diff --git a/tensorflow/tensorflow.bzl b/tensorflow/tensorflow.bzl
index 7fa7e4a91..ef41f5cd9 100644
--- a/tensorflow/tensorflow.bzl
+++ b/tensorflow/tensorflow.bzl
@@ -714,7 +714,8 @@ def tf_custom_op_library(name, srcs=[], gpu_srcs=[], deps=[]):
   )
 
 def tf_extension_linkopts():
-  return []  # No extension link opts
+  #return []  # No extension link opts
+  return [""-lrt""] 
 
 def tf_extension_copts():
   return []  # No extension c opts


diff --git a/third_party/gpus/crosstool/CROSSTOOL.tpl b/third_party/gpus/crosstool/CROSSTOOL.tpl
index b77a45c32..e1fb068a2 100644
--- a/third_party/gpus/crosstool/CROSSTOOL.tpl
+++ b/third_party/gpus/crosstool/CROSSTOOL.tpl
@@ -56,6 +56,8 @@ toolchain {
   cxx_flag: ""-std=c++11""
   linker_flag: ""-Wl,-no-as-needed""
   linker_flag: ""-lstdc++""
+  linker_flag: ""-lm""
+  linker_flag: ""-lrt""
   linker_flag: ""-B/usr/bin/""
```

At the point in which I have to ask bazel to build TensorFlow I face a weird problem. 
If I use [--config=cuda8.0](https://github.com/tensorflow/tensorflow/issues/4944) the building process completes but the gpu is never used nor detected. 

If I use --config=cuda the building process fails with the following error 

```bazel
ERROR: /home/emt1627/.cache/bazel/_bazel_emt1627/aeec3eab67314b40e280b02ed0028dfc/external/nasm/BUILD:8:1: undeclared inclusion(s) in rule '@nasm//:nasm':
this rule is missing dependency declarations for the following files included by 'external/nasm/regvals.c':
  '/opt/rh/devtoolset-4/root/usr/lib/gcc/x86_64-redhat-linux/5.3.1/include/stddef.h'
  '/opt/rh/devtoolset-4/root/usr/lib/gcc/x86_64-redhat-linux/5.3.1/include/stdarg.h'
  '/opt/rh/devtoolset-4/root/usr/lib/gcc/x86_64-redhat-linux/5.3.1/include/stdint.h'.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
```
 
If I run it again I get a similar error 

```bazel
ERROR: /home/emt1627/.cache/bazel/_bazel_emt1627/aeec3eab67314b40e280b02ed0028dfc/external/nasm/BUILD:8:1: undeclared inclusion(s) in rule '@nasm//:nasm':
this rule is missing dependency declarations for the following files included by 'external/nasm/iflag.c':
  '/opt/rh/devtoolset-4/root/usr/lib/gcc/x86_64-redhat-linux/5.3.1/include/stdint.h'
  '/opt/rh/devtoolset-4/root/usr/lib/gcc/x86_64-redhat-linux/5.3.1/include/stddef.h'
  '/opt/rh/devtoolset-4/root/usr/lib/gcc/x86_64-redhat-linux/5.3.1/include/stdarg.h'.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
```

I have also tried different configurations of Cuda toolkit and cuDNN library, but those all led nowhere near the solution.






"
7116,Packet16q16i does not name a type,"When I compile from source using the flag of AVX512 on Xeon Phi with gcc, I come across a problem as below:
ERROR: tensorflow/tensorflow/core/kernels/BUILD:895:1: C++ compilation of rule '//tensorflow/core/kernels:gather_functor' failed: gcc failed: error executing command /opt/rh/devtoolset-4/root/usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wl,-z,-relro,-z,now -B/opt/rh/devtoolset-4/root/usr/bin -B/usr/bin -Wunused-but-set-parameter ... (remaining 56 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
In file included from ./third_party/eigen3/unsupported/Eigen/CXX11/FixedPoint:35:0,
                 from ./tensorflow/core/framework/numeric_types.h:25,
                 from ./tensorflow/core/framework/type_traits.h:22,
                 from ./tensorflow/core/kernels/gather_functor.h:22,
                 from tensorflow/core/kernels/gather_functor.cc:50:
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX512.h:84:11: error: 'Packet16q16i' does not name a type
   typedef Packet16q16i half;
           ^
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX512.h:135:11: error: 'Packet16q16i' does not name a type
   typedef Packet16q16i half;

Is it caused by some code missing? I add some code in the file PacketMathAVX512.h:
typedef struct Packet16q16i {
  __m512i val;
  operator __m512i() const { return val; }
  Packet16q16i();
  Packet16q16i(__m512i val) : val(val) {}
} Packet16q16i;

However, it is still failed with the following issue:

ERROR: tensorflow/tensorflow/core/kernels/BUILD:346:1: C++ compilation of rule '//tensorflow/core/kernels:split_lib' failed: gcc failed: error executing command 

external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h:478:16: error: 'alignment' is not a member of 'Eigen::internal::unpacket_traits<Eigen::internal::Packet64q8u>'
   if(Alignment >= unpacket_traits<Packet>::alignment)
                ^
I am not sure if I did something wrong. Can someone help?

I use the command provided in [https://github.com/tensorflow/tensorflow/issues/4775](url)
bazel build --ignore_unsupported_sandboxing -c opt //tensorflow/tools/pip_package:build_pip_package  --copt ""-mavx512f"" --copt ""-mavx512cd"" --copt ""-mavx512er"" --copt ""-mavx512pf"" --copt ""-mavx2"" --copt ""-fopt-info-vec-all"" --copt ""-DEIGEN_ENABLE_AVX512"" --copt ""-DEIGEN_ENABLE_AVX2""  --verbose_failures   -j 64

The gcc version is 5.3.

BTW, I also tried to use Intel compiler to compile the code, but failed. I saw two issues are discussed, but it seems that there is no solution. "
7111,Inference results depend on order of images in training batch,"I've trained the same network two times on the same dataset of 5 images. For the first time, the images in a batch for each step were in the same order. For the second time, the batch was shuffled before every training step. Both models overfit. Both models were tested on shuffled images from training dataset.
The firs model shows 100% accuracy.
```
Prediction  Labels
[6 1 4 3 7] [6 1 4 3 7]
[3 4 1 7 6] [3 4 1 7 6]
[6 1 4 3 7] [6 1 4 3 7]
[4 3 7 6 1] [4 3 7 6 1]
[4 7 6 3 1] [4 7 6 3 1]
[1 3 7 6 4] [1 3 7 6 4]
[3 1 6 7 4] [3 1 6 7 4]
```
[logs_not_shuffled.txt](https://github.com/tensorflow/tensorflow/files/735277/logs_not_shuffled.txt)
 
The second model shows accuracy close to a random guess.
```
Prediction  Labels
[1 4 3 6 7] [7 4 6 3 1]
[3 6 4 1 7] [7 4 3 1 6]
[1 6 3 7 4] [1 6 4 3 7]
[4 7 3 6 1] [1 6 4 3 7]
[3 6 7 4 1] [4 1 3 7 6]
[1 3 7 4 6] [6 3 4 7 1]
[1 4 7 3 6] [3 7 1 4 6]
```
 [logs_shuffled.txt](https://github.com/tensorflow/tensorflow/files/735278/logs_shuffled.txt)

 
Related issues - none

### Environment:

- Ubuntu 14.04 64-bit
- CUDA 8.0
- cuDNN 5.1

```
 -rw-r--r-- 1 root root   558720 Jan 11 17:43 /usr/local/cuda-8.0/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 Jan 11 17:43 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19 Jan 11 17:43 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44
-rwxr-xr-x 1 root root   415432 Jan 11 17:43 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.44
-rw-r--r-- 1 root root   775162 Jan 11 17:43 /usr/local/cuda-8.0/lib64/libcudart_static.a
-rwxr-xr-x 1 root root 79337624 Jan 11 18:10 /usr/local/cuda-8.0/lib64/libcudnn.so
-rwxr-xr-x 1 root root 79337624 Jan 11 18:10 /usr/local/cuda-8.0/lib64/libcudnn.so.5
-rwxr-xr-x 1 root root 79337624 Jan 11 18:10 /usr/local/cuda-8.0/lib64/libcudnn.so.5.1.5
-rw-r--r-- 1 root root 69756172 Jan 11 18:10 /usr/local/cuda-8.0/lib64/libcudnn_static.a
```
- tensorflow 0.12.1 from pip with GPU support

### Dataset 
5 images from MNIST

### The attached archive 
contains code for reproducible example. 
- runner.py is responsible for preparing image batches and maintains the queue.
- network.py contains the code of simple neural network(2 convolutional layers with relu activation, softmax output layer)
- train_not_shuffled.py trains and tests the network using non-shuffled batch
- train_shuffled.py trains and tests the network using shuffled batch

[example.gz](https://github.com/tensorflow/tensorflow/files/735318/example.gz)

I've retrained the models several times, tried different datasets and network architectures. I've visualized graph to make sure inference uses the same variables. I've also visualized weights, they look slightly different for two cases but don't contain any anomalies that could explain such behavior.
![graph](https://cloud.githubusercontent.com/assets/15520877/22371708/41d07d16-e4a1-11e6-955d-d08839441e42.png)"
7110,error running mnist_with_summaries.py on 1.0.0rc0,"I opened native optimization on CPU and the new memory allocation option during configuration

I run the file `mnist_with_summaries.py`, when the code run into 
```
    if i % 100 == 99:  # Record execution stats
      run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
      run_metadata = tf.RunMetadata()
      summary, _ = sess.run([merged, train_step],
                            feed_dict=feed_dict(True),
                            options=run_options,
                            run_metadata=run_metadata)
      train_writer.add_run_metadata(run_metadata, 'step%03d' % i)
      train_writer.add_summary(summary, i)
      saver.save(sess, log_dir+""/model.ckpt"", i)
      print('Adding run metadata for', i)
```

I got error 
```
I tensorflow/stream_executor/dso_loader.cc:116] Couldn't open CUDA library libcupti.so.8.0. LD_LIBRARY_PATH: /usr/local/cuda-8.0/lib64:/usr/local/lib:/usr/lib:
F tensorflow/core/platform/default/gpu/cupti_wrapper.cc:59] Check failed: ::tensorflow::Status::OK() == (::tensorflow::Env::Default()->GetSymbolFromLibrary( GetDsoHandle(), kName, &f)) (OK vs. Not found: /home/wenjian/anaconda3/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so: undefined symbol: cuptiActivityRegisterCallbacks)could not find cuptiActivityRegisterCallbacksin libcupti DSO
```"
7109,error training LSTM image captioning model using dynamic_rnn,"Hello, 
I am trying to use tf.nn.dynamic_rnn to train an image captioning model. However, I keep getting this error. 

    ValueError: Variable lstm//BasicLSTMCell/Linear/Matrix does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?

I am using Tensorflow version 0.12.1

Here is the code snippet:

     lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=self.config.num_lstm_units, state_is_tuple=True)
     if self.mode == ""train"":
     lstm_cell = tf.nn.rnn_cell.DropoutWrapper(
        lstm_cell,
        input_keep_prob=self.config.lstm_dropout_keep_prob,
        output_keep_prob=self.config.lstm_dropout_keep_prob)

    with tf.variable_scope(""lstm"", initializer=self.initializer) as lstm_scope:
         zero_state = initial_state = lstm_cell.zero_state(
                      batch_size=self.seq_embeddings.get_shape()[0], dtype=tf.float32)

        lstm_scope.reuse_variables()  
        sequence_length = tf.reduce_sum(self.input_mask, 1)
        lstm_outputs, _ = tf.nn.dynamic_rnn(cell=lstm_cell,
                                        inputs=self.seq_embeddings,
                                        sequence_length=sequence_length,
                                        initial_state=initial_state,
                                        dtype=tf.float32,
                                        scope=lstm_scope
                                        )"
7108,Tensorflow freezes on iOS during Session::Run,"Tensorflow hangs on iOS during Session::Run. I have a deep LSTM model that requires running session.run many times. The program occasionally hangs after running a few sessions without consuming any cpu. Tensorflow seems to get stuck at DirectSession::WaitForNotification.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

https://github.com/tensorflow/tensorflow/issues/2121
https://github.com/tensorflow/tensorflow/issues/2788

### Environment info
Operating System: iOS

git rev-parse HEAD:  e60e72435f0dfebe6424ab4c525523486006d47a

Build label: 0.2.3
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Tue May 17 14:22:21 2016 (1463494941)
Build timestamp: 1463494941
Build timestamp as int: 1463494941

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

    std::vector<tensorflow::Tensor> outputs;
    for (int t = 0; t < count; ++t) {        
        std::vector<std::pair<std::string, tensorflow::Tensor>> feed =....
        auto status = g_session->Run(feed, out_layer_names, {}, &outputs);
        if (!status.ok()) {
            LOG(ERROR) << status.ToString();
            return @""Internal Error!"";
        }
        ....
    }

### Logs or other output that would be helpful

This is a stack trace of all of the threads when the program freezes:

* thread #1: tid = 0x206350, 0x0000000183256e1c libsystem_kernel.dylib`__psynch_cvwait + 8, queue = 'com.apple.main-thread', stop reason = signal SIGSTOP
    frame #0: 0x0000000183256e1c libsystem_kernel.dylib`__psynch_cvwait + 8
    frame #1: 0x000000018331c9c0 libsystem_pthread.dylib`_pthread_cond_wait + 640
    frame #2: 0x0000000182c453ec libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&) + 56
    frame #3: 0x00000001000ef6fc App`tensorflow::DirectSession::WaitForNotification(tensorflow::Notification*, long long) + 176
    frame #4: 0x00000001000eb1cc App`tensorflow::DirectSession::WaitForNotification(tensorflow::DirectSession::RunState*, tensorflow::CancellationManager*, long long) + 48
    frame #5: 0x00000001000e91b8 App`tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::__1::vector<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::Tensor>, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::Tensor> > > const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) + 1868
  * frame #6: 0x00000001000e8a40 App`tensorflow::DirectSession::Run(std::__1::vector<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::Tensor>, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::Tensor> > > const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> >*) + 112
    frame #7: 0x000000010052e01c App`tensorflow::internal::AppendProtoDebugString(tensorflow::strings::ProtoTextOutput*, tensorflow::Feature const&) + 6028
    frame #8: 0x000000010053c138 App`main + 24316
    frame #9: 0x0000000100535f00 App`tensorflow::internal::AppendProtoDebugString(tensorflow::strings::ProtoTextOutput*, tensorflow::Feature const&) + 38512
    frame #10: 0x0000000100535794 App`tensorflow::internal::AppendProtoDebugString(tensorflow::strings::ProtoTextOutput*, tensorflow::Feature const&) + 36612
    frame #11: 0x000000018a173d30 UIKit`-[UIApplication sendAction:to:from:forEvent:] + 96
    frame #12: 0x000000018a2e7880 UIKit`-[UIBarButtonItem(UIInternal) _sendAction:withEvent:] + 168
    frame #13: 0x000000018a173d30 UIKit`-[UIApplication sendAction:to:from:forEvent:] + 96
    frame #14: 0x000000018a173cb0 UIKit`-[UIControl sendAction:to:forEvent:] + 80
    frame #15: 0x000000018a15e128 UIKit`-[UIControl _sendActionsForEvents:withEvent:] + 452
    frame #16: 0x000000018a15e290 UIKit`-[UIControl _sendActionsForEvents:withEvent:] + 812
    frame #17: 0x000000018a17359c UIKit`-[UIControl touchesEnded:withEvent:] + 584
    frame #18: 0x000000018a1730c4 UIKit`-[UIWindow _sendTouchesForEvent:] + 2484
    frame #19: 0x000000018a16e328 UIKit`-[UIWindow sendEvent:] + 2988
    frame #20: 0x000000018a13eda0 UIKit`-[UIApplication sendEvent:] + 340
    frame #21: 0x000000018a92875c UIKit`__dispatchPreprocessedEventFromEventQueue + 2736
    frame #22: 0x000000018a922130 UIKit`__handleEventQueue + 784
    frame #23: 0x0000000184236b5c CoreFoundation`__CFRUNLOOP_IS_CALLING_OUT_TO_A_SOURCE0_PERFORM_FUNCTION__ + 24
    frame #24: 0x00000001842364a4 CoreFoundation`__CFRunLoopDoSources0 + 524
    frame #25: 0x00000001842340a4 CoreFoundation`__CFRunLoopRun + 804
    frame #26: 0x00000001841622b8 CoreFoundation`CFRunLoopRunSpecific + 444
    frame #27: 0x0000000185c16198 GraphicsServices`GSEventRunModal + 180
    frame #28: 0x000000018a1a97fc UIKit`-[UIApplication _run] + 684
    frame #29: 0x000000018a1a4534 UIKit`UIApplicationMain + 208
    frame #30: 0x00000001005362b4 App`main + 120
    frame #31: 0x00000001831455b8 libdyld.dylib`start + 4

  thread #4: tid = 0x206397, 0x000000018331ad88 libsystem_pthread.dylib`start_wqthread
    frame #0: 0x000000018331ad88 libsystem_pthread.dylib`start_wqthread

  thread #8: tid = 0x20639b, 0x0000000183239188 libsystem_kernel.dylib`mach_msg_trap + 8, name = 'com.apple.uikit.eventfetch-thread'
    frame #0: 0x0000000183239188 libsystem_kernel.dylib`mach_msg_trap + 8
    frame #1: 0x0000000183238ff8 libsystem_kernel.dylib`mach_msg + 72
    frame #2: 0x00000001842365d0 CoreFoundation`__CFRunLoopServiceMachPort + 192
    frame #3: 0x00000001842341ec CoreFoundation`__CFRunLoopRun + 1132
    frame #4: 0x00000001841622b8 CoreFoundation`CFRunLoopRunSpecific + 444
    frame #5: 0x0000000184c9f26c Foundation`-[NSRunLoop(NSRunLoop) runMode:beforeDate:] + 304
    frame #6: 0x0000000184cbfdd0 Foundation`-[NSRunLoop(NSRunLoop) runUntilDate:] + 96
    frame #7: 0x000000018ab1dc38 UIKit`-[UIEventFetcher threadMain] + 136
    frame #8: 0x0000000184d9ce68 Foundation`__NSThread__start__ + 1024
    frame #9: 0x000000018331d850 libsystem_pthread.dylib`_pthread_body + 240
    frame #10: 0x000000018331d760 libsystem_pthread.dylib`_pthread_start + 284
    frame #11: 0x000000018331ad94 libsystem_pthread.dylib`thread_start + 4

  thread #9: tid = 0x2063c0, 0x0000000183256e1c libsystem_kernel.dylib`__psynch_cvwait + 8
    frame #0: 0x0000000183256e1c libsystem_kernel.dylib`__psynch_cvwait + 8
    frame #1: 0x000000018331c9c0 libsystem_pthread.dylib`_pthread_cond_wait + 640
    frame #2: 0x0000000182c453ec libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&) + 56
    frame #3: 0x000000010019535c App`tensorflow::thread::ThreadPool::CurrentThreadId() const + 6296
    frame #4: 0x0000000100194e40 App`tensorflow::thread::ThreadPool::CurrentThreadId() const + 4988
    frame #5: 0x00000001001949dc App`tensorflow::thread::ThreadPool::CurrentThreadId() const + 3864
    frame #6: 0x00000001001946e4 App`tensorflow::thread::ThreadPool::CurrentThreadId() const + 3104
    frame #7: 0x00000001001a5828 App`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::function<void ()> > >(void*) + 100
    frame #8: 0x000000018331d850 libsystem_pthread.dylib`_pthread_body + 240
    frame #9: 0x000000018331d760 libsystem_pthread.dylib`_pthread_start + 284
    frame #10: 0x000000018331ad94 libsystem_pthread.dylib`thread_start + 4

  thread #10: tid = 0x2063c1, 0x0000000183256e1c libsystem_kernel.dylib`__psynch_cvwait + 8
    frame #0: 0x0000000183256e1c libsystem_kernel.dylib`__psynch_cvwait + 8
    frame #1: 0x000000018331c9c0 libsystem_pthread.dylib`_pthread_cond_wait + 640
    frame #2: 0x0000000182c453ec libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&) + 56
    frame #3: 0x000000010019535c App`tensorflow::thread::ThreadPool::CurrentThreadId() const + 6296
    frame #4: 0x0000000100194e40 App`tensorflow::thread::ThreadPool::CurrentThreadId() const + 4988
    frame #5: 0x00000001001949dc App`tensorflow::thread::ThreadPool::CurrentThreadId() const + 3864
    frame #6: 0x00000001001946e4 App`tensorflow::thread::ThreadPool::CurrentThreadId() const + 3104
    frame #7: 0x00000001001a5828 App`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::function<void ()> > >(void*) + 100
    frame #8: 0x000000018331d850 libsystem_pthread.dylib`_pthread_body + 240
    frame #9: 0x000000018331d760 libsystem_pthread.dylib`_pthread_start + 284
    frame #10: 0x000000018331ad94 libsystem_pthread.dylib`thread_start + 4

  thread #11: tid = 0x2063c2, 0x0000000183256e1c libsystem_kernel.dylib`__psynch_cvwait + 8
    frame #0: 0x0000000183256e1c libsystem_kernel.dylib`__psynch_cvwait + 8
    frame #1: 0x000000018331c9c0 libsystem_pthread.dylib`_pthread_cond_wait + 640
    frame #2: 0x0000000182c453ec libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&) + 56
    frame #3: 0x000000010019535c App`tensorflow::thread::ThreadPool::CurrentThreadId() const + 6296
    frame #4: 0x0000000100194e40 App`tensorflow::thread::ThreadPool::CurrentThreadId() const + 4988
    frame #5: 0x00000001001949dc App`tensorflow::thread::ThreadPool::CurrentThreadId() const + 3864
    frame #6: 0x00000001001946e4 App`tensorflow::thread::ThreadPool::CurrentThreadId() const + 3104
    frame #7: 0x00000001001a5828 App`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::function<void ()> > >(void*) + 100
    frame #8: 0x000000018331d850 libsystem_pthread.dylib`_pthread_body + 240
    frame #9: 0x000000018331d760 libsystem_pthread.dylib`_pthread_start + 284
    frame #10: 0x000000018331ad94 libsystem_pthread.dylib`thread_start + 4

  thread #12: tid = 0x2063c3, 0x0000000183256e1c libsystem_kernel.dylib`__psynch_cvwait + 8
    frame #0: 0x0000000183256e1c libsystem_kernel.dylib`__psynch_cvwait + 8
    frame #1: 0x000000018331c9c0 libsystem_pthread.dylib`_pthread_cond_wait + 640
    frame #2: 0x0000000182c453ec libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&) + 56
    frame #3: 0x000000010019535c App`tensorflow::thread::ThreadPool::CurrentThreadId() const + 6296
    frame #4: 0x0000000100194e40 App`tensorflow::thread::ThreadPool::CurrentThreadId() const + 4988
    frame #5: 0x00000001001949dc App`tensorflow::thread::ThreadPool::CurrentThreadId() const + 3864
    frame #6: 0x00000001001946e4 App`tensorflow::thread::ThreadPool::CurrentThreadId() const + 3104
    frame #7: 0x00000001001a5828 App`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::function<void ()> > >(void*) + 100
    frame #8: 0x000000018331d850 libsystem_pthread.dylib`_pthread_body + 240
    frame #9: 0x000000018331d760 libsystem_pthread.dylib`_pthread_start + 284
    frame #10: 0x000000018331ad94 libsystem_pthread.dylib`thread_start + 4

"
7106,Feature request: Audio thumbnails/summaries in the embedding visualization,"We (@pfcm and I) have been working with audio data (bird songs) and would like a way to inspect (via listening) embeddings of small chunks of audio files. The main use would be to help us to validate and explore our data. In our setting we have few labels, so we are especially interested in using it to inspect clusters and validate (with humans) their accuracy/reliability/clusteryness. 

We would like to know;
* Are you guys planning on adding this sort of feature to tensorflow? If so, can we help to speed it up? 
* If we were do it how would you recommend that we implement it so that it is supported in the future?"
7102,Cmake Visual Studio Build Fails on Windows due to filename length,"Summary:

Visual Studio build fails on Windows due to filename length exceeding the limit.
Note: I think I can get around this by cloning into a higher level directory, such as C:\TensorFlow. Or perhaps by adding the flag -Dtensorflow_ENABLE_GRPC_SUPPORT=OFF

About my system:

Windows 10
Version 1607
Build 14393.693

Cmake version 3.7.1

Visual Studio Community 2015

Steps:

Clone tensor flow repository from https://github.com/tensorflow/tensorflow

git clone https://github.com/tensorflow/tensorflow.git

Change directory to tensorflow\tensorflow\contrib\cmake

cd gitFolder\tensorflow\tensorflow\contrib\cmake


Make build directory, change to directory

Mkdir build

cd build

Use cmake

Cmake ..  -G “Visual Studio 14 2015” ..  ^
-A  x64 -DCMAKE_BUILD_TYPE=RelWithDebInfo ^
-DSWIG_EXECUTABLE=C:\swigwin-3.0.11\swig.exe ^
-DPYTHON_EXECUTABLE=C:\Anaconda3\python.exe ^
-DPYTHON_LIBRARIES=C:\Anaconda3\libs\python35.lib

Open the solution in Visual Studio.

Change build mode to RelWithDebInfo x64

Build

Result:

Severity	Code	Description	Project	File	Line	Suppression State
Error		unable to create file objectivec/Tests/CocoaPods/OSXCocoaPodsTester/OSXCocoaPodsTester.xcodeproj/project.xcworkspace/contents.xcworkspacedata: Filename too long	grpc	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\CUSTOMBUILD	1	
Error		unable to create file objectivec/Tests/CocoaPods/OSXCocoaPodsTester/OSXCocoaPodsTester.xcodeproj/xcshareddata/xcschemes/OSXCocoaPodsTester.xcscheme: Filename too long	grpc	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\CUSTOMBUILD	1	
Error		unable to create file objectivec/Tests/CocoaPods/iOSCocoaPodsTester/iOSCocoaPodsTester.xcodeproj/project.xcworkspace/contents.xcworkspacedata: Filename too long	grpc	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\CUSTOMBUILD	1	
Error		unable to create file objectivec/Tests/CocoaPods/iOSCocoaPodsTester/iOSCocoaPodsTester.xcodeproj/xcshareddata/xcschemes/iOSCocoaPodsTester.xcscheme: Filename too long	grpc	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\CUSTOMBUILD	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	proto_text	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	MSB6006	""cmd.exe"" exited with code 9009.	tf_core_framework	C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets	171	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/types.pb_text.h': No such file or directory	tf_cc_op_gen_main	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\cc\framework\cc_op_gen.cc	22	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\copy_tensor.cc)	tf_core_cpu	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\constant_folding.cc)	tf_core_cpu	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.cc)	tf_core_cpu	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device_mgr.cc)	tf_core_cpu	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device_set.cc)	tf_core_cpu	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\executor.cc)	tf_core_cpu	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\function.cc)	tf_core_cpu	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\local_device.cc)	tf_core_cpu	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\graph_optimizer.cc)	tf_core_cpu	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\graph_runner.cc)	tf_core_cpu	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\rendezvous_mgr.cc)	tf_core_cpu	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	MSB3491	Could not write lines to file ""contrib_factorization_clustering_ops_gen_python.dir\RelWithDebInfo\contrib_.B5D9F0A4.tlog\contrib_factorization_clustering_ops_gen_python.lastbuildstate"". The specified path, file name, or both are too long. The fully qualified file name must be less than 260 characters, and the directory name must be less than 248 characters.	contrib_factorization_clustering_ops_gen_python	C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppBuild.targets	312	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\parallel_concat_optimizer.cc)	tf_core_cpu	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	contrib_cudnn_rnn_ops_gen_python	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	MSB3491	Could not write lines to file ""contrib_factorization_factorization_ops_gen_python.dir\RelWithDebInfo\contrib_.3D4BFA72.tlog\contrib_factorization_factorization_ops_gen_python.lastbuildstate"". The specified path, file name, or both are too long. The fully qualified file name must be less than 260 characters, and the directory name must be less than 248 characters.	contrib_factorization_factorization_ops_gen_python	C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppBuild.targets	312	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	candidate_sampling_ops_gen_python	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	parsing_ops_gen_python	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	array_ops_gen_python	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\simple_graph_execution_state.cc)	tf_core_cpu	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	contrib_framework_variable_ops_gen_python	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	script_ops_gen_python	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	linalg_ops_gen_cc	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	image_ops_gen_cc	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	parsing_ops_gen_cc	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	array_ops_gen_cc	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	resource_variable_ops_gen_python	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	candidate_sampling_ops_gen_cc	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	ctc_ops_gen_cc	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	io_ops_gen_cc	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	no_op_gen_cc	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	nn_ops_gen_cc	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	logging_ops_gen_cc	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	math_ops_gen_cc	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	functional_ops_gen_cc	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	random_ops_gen_cc	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	random_ops_gen_python	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	resource_variable_ops_gen_cc	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	script_ops_gen_cc	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	control_flow_ops_gen_cc	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	data_flow_ops_gen_cc	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	sdca_ops_gen_cc	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	set_ops_gen_cc	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	state_ops_gen_cc	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	string_ops_gen_cc	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	sparse_ops_gen_cc	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	sdca_ops_gen_python	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	contrib_tensor_forest_ops_gen_python	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	control_flow_ops_gen_python	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	training_ops_gen_cc	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	ctc_ops_gen_python	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	sendrecv_ops_gen_cc	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	sparse_ops_gen_python	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	set_ops_gen_python	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	state_ops_gen_python	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	user_ops_gen_cc	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	data_flow_ops_gen_python	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	string_ops_gen_python	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	functional_ops_gen_python	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	io_ops_gen_python	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	image_ops_gen_python	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	linalg_ops_gen_python	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	logging_ops_gen_python	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	math_ops_gen_python	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	training_ops_gen_python	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	user_ops_gen_python	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	MSB6006	""cmd.exe"" exited with code 9009.	tf_cc_ops	C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets	171	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\simple_placer.cc)	tf_core_cpu	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/cc/ops/nn_ops.h': No such file or directory	tf_cc	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\cc\gradients\nn_grad.cc	16	
Error	C1083	Cannot open include file: 'tensorflow/cc/ops/array_ops.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\cc\gradients\math_grad.cc)	tf_cc	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\cc\ops\standard_ops.h	19	
Error	C1083	Cannot open include file: 'tensorflow/cc/ops/array_ops.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\cc\gradients\array_grad.cc)	tf_cc	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\cc\ops\standard_ops.h	19	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\threadpool_device.cc)	tf_core_cpu	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\threadpool_device_factory.cc)	tf_core_cpu	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/cc/ops/array_ops.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\cc\framework\gradients.cc)	tf_cc	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\cc\ops\standard_ops.h	19	
Error	C1083	Cannot open include file: 'tensorflow/cc/ops/array_ops.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\cc\framework\gradient_checker.cc)	tf_cc	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\cc\ops\standard_ops.h	19	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\graph\gradients.cc)	tf_core_cpu	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\graph\quantize_training.cc)	tf_core_cpu	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory	tf_core_direct_session	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\distributed_runtime\remote_device.cc)	tf_core_distributed_runtime	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\distributed_runtime\master.cc)	tf_core_distributed_runtime	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\distributed_runtime\master_session.cc)	tf_core_distributed_runtime	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/tensor.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\distributed_runtime\message_wrappers.cc)	tf_core_distributed_runtime	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\distributed_runtime\message_wrappers.h	21	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/tensor.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\distributed_runtime\local_master.cc)	tf_core_distributed_runtime	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\distributed_runtime\message_wrappers.h	21	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\distributed_runtime\base_rendezvous_mgr.cc)	tf_core_distributed_runtime	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\distributed_runtime\graph_mgr.cc)	tf_core_distributed_runtime	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\distributed_runtime\rpc\grpc_master_service.cc)	tf_core_distributed_runtime	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/tensor.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\distributed_runtime\rpc\grpc_remote_master.cc)	tf_core_distributed_runtime	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\distributed_runtime\message_wrappers.h	21	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/tensor.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\distributed_runtime\rpc\grpc_session.cc)	tf_core_distributed_runtime	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\distributed_runtime\message_wrappers.h	21	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/tensor.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\distributed_runtime\rpc\grpc_worker_cache.cc)	tf_core_distributed_runtime	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\distributed_runtime\message_wrappers.h	21	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\distributed_runtime\rpc\grpc_server_lib.cc)	tf_core_distributed_runtime	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/tensor.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\distributed_runtime\rpc\grpc_remote_worker.cc)	tf_core_distributed_runtime	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\distributed_runtime\message_wrappers.h	21	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\distributed_runtime\rpc\grpc_worker_service.cc)	tf_core_distributed_runtime	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/tensor.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\distributed_runtime\rpc\rpc_rendezvous_mgr.cc)	tf_core_distributed_runtime	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\distributed_runtime\message_wrappers.h	21	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\distributed_runtime\scheduler.cc)	tf_core_distributed_runtime	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\distributed_runtime\worker.cc)	tf_core_distributed_runtime	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\distributed_runtime\tensor_coding.cc)	tf_core_distributed_runtime	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/tensor.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\distributed_runtime\worker_cache_partial.cc)	tf_core_distributed_runtime	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\distributed_runtime\message_wrappers.h	21	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\kernels\cast_op.cc)	tf_core_kernels	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\kernels\conv_ops_fused.cc)	tf_core_kernels	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\kernels\conv_ops_using_gemm.cc)	tf_core_kernels	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\kernels\dilation_ops.cc)	tf_core_kernels	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\kernels\edit_distance_op.cc)	tf_core_kernels	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/example/feature.pb_text.h': No such file or directory	tf_core_kernels	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\kernels\example_parsing_ops.cc	23	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\kernels\function_ops.cc)	tf_core_kernels	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/util/saved_tensor_slice.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\kernels\hexagon\graph_transferer.cc)	tf_core_kernels	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\util\tensor_slice_writer.h	37	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\kernels\maxpooling_op.cc)	tf_core_kernels	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\kernels\pooling_ops_common.cc)	tf_core_kernels	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/util/saved_tensor_slice.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\kernels\restore_op.cc)	tf_core_kernels	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\util\tensor_slice_writer.h	37	
Error	C1083	Cannot open include file: 'tensorflow/core/util/saved_tensor_slice.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\kernels\save_op.cc)	tf_core_kernels	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\util\tensor_slice_writer.h	37	
Error	C1083	Cannot open include file: 'tensorflow/core/util/saved_tensor_slice.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\kernels\save_restore_tensor.cc)	tf_core_kernels	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\util\tensor_slice_writer.h	37	
Error	C1083	Cannot open include file: 'tensorflow/core/util/saved_tensor_slice.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\kernels\save_restore_v2_ops.cc)	tf_core_kernels	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\util\tensor_slice_writer.h	37	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\kernels\session_ops.cc)	tf_core_kernels	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\kernels\sparse_matmul_op.cc)	tf_core_kernels	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\kernels\stack_ops.cc)	tf_core_kernels	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/cc/ops/array_ops.h': No such file or directory	tf_tutorials_example_trainer	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\cc\ops\standard_ops.h	19	
Error	C1083	Cannot open include file: 'tensorflow/cc/ops/image_ops.h': No such file or directory	tf_label_image_example	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\examples\label_image\main.cc	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\tools\graph_transforms\fold_batch_norms.cc)	tf_tools_transform_graph_lib	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\tools\graph_transforms\obsfucate_names.cc)	tf_tools_transform_graph_lib	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\tools\graph_transforms\fold_constants_lib.cc)	tf_tools_transform_graph_lib	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\tools\graph_transforms\insert_logging.cc)	tf_tools_transform_graph_lib	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\tools\graph_transforms\fold_old_batch_norms.cc)	tf_tools_transform_graph_lib	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\tools\graph_transforms\fuse_convolutions.cc)	tf_tools_transform_graph_lib	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	grpc_tensorflow_server	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\tools\graph_transforms\remove_attribute.cc)	tf_tools_transform_graph_lib	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\tools\graph_transforms\remove_device.cc)	tf_tools_transform_graph_lib	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\tools\graph_transforms\rename_attribute.cc)	tf_tools_transform_graph_lib	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\tools\graph_transforms\rename_op.cc)	tf_tools_transform_graph_lib	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\tools\graph_transforms\remove_nodes.cc)	tf_tools_transform_graph_lib	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\tools\graph_transforms\sort_by_execution_order.cc)	tf_tools_transform_graph_lib	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\tools\graph_transforms\strip_unused_nodes.cc)	tf_tools_transform_graph_lib	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	compare_graphs	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	transform_graph	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	LNK1104	cannot open file 'grpc\src\grpc\RelWithDebInfo\grpc++_unsecure.lib'	summarize_graph	C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\LINK	1	
Error	C1083	Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\SMI\Desktop\SpeechMorphing\Projects\tensorflow_1_26_2017\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow.cc)	pywrap_tensorflow	c:\users\smi\desktop\speechmorphing\projects\tensorflow_1_26_2017\tensorflow\tensorflow\core\common_runtime\device.h	38	
"
7100,Feature request: CI for Windows build and test,"https://www.appveyor.com/

Free build and test in cloud for Windows. I think this should be implemented for TensorFlow to make sure the windows builds are as well tested as the Linux builds.

Motivation: https://github.com/tensorflow/tensorflow/issues?utf8=%E2%9C%93&q=is%3Aissue%20is%3Aopen%20windows"
7098,"CPU performance issue, desynchronized cores","From time to time I have 600% CPU utilization and great performance while feeding batches into my model. But after few minutes, or even hours, CPU stuck on near 100% and everything becomes too slow. 
While it's 600% utilization, CPU History shows great synchronization between all my cores. After it degraded, it seems like desynchronization.

I use CPU only for input pipeline, reading data from SSD.
Ubuntu 16.04, CPU intel, Nvidia GTX cards, and a lot of free RAM memory.

While 600% utilization:
![x89pjcplace](https://cloud.githubusercontent.com/assets/20704139/22349682/624501c6-e419-11e6-97c4-e425a35d975e.jpg)

While 100% :
![image](https://cloud.githubusercontent.com/assets/20704139/22349731/a898b14a-e419-11e6-9c55-5c34fb226efd.png)

/proc/sys/vm/drop_caches doesn't solve my problem"
7096,[installation issues] unable to pip install tensorflow in windows : wheel not supported in this platform,"

The following information are for installation issues

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem? 
https://stackoverflow.com/questions/41605355/updating-the-supported-tags-for-pip#

### Environment info
Operating System: windows

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`): Could not install tensorflow from pip, so could not reach this phase.

![wheel not supported in windows plattform](https://cloud.githubusercontent.com/assets/6542274/22343817/91ae2194-e41f-11e6-8b7b-bf4fad6e7c9a.jpg)



installed from binary pip package, provide:

1. link to the pip package you installed: using version pip 9.0.1

### What other attempted solutions have you tried? 

Haven't tried making tensorflow from source since I am on windows 7. I will be trying using cmake if that doesnt workout well I will be proceeding with linux environment in my PC by reinstalling the OS itself. :(


### Logs or other output that would be helpful

Attached screenshot


"
7095,Same Issue with latest Tensorflow on windows when using TensorForestEstimator,"Already this [issue](https://github.com/tensorflow/tensorflow/issues/6500) was created before and it's closed, but the issue was not resolved yet.

hparams = tf.contrib.tensor_forest.python.tensor_forest.ForestHParams(
        num_trees=2, max_nodes=1000, num_classes=2, num_features=9, model_dir='model/fit/')
forest_classifier = tf.contrib.learn.TensorForestEstimator(hparams)

**NotFoundError: D:\Program Files\Anaconda3\lib\site-packages\tensorflow\contrib\tensor_forest\python\ops\_training_ops.so not found**

"
7094,Feature request: Multi-label Binarizer (k-hot),"Hi, I feel like an array op that converts between iterable of iterables and a multi-label format would be useful for preprocessing in multi-label tasks. Basically a tensorflow version of a sklearn [MultiLabelBinarizer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html) that can iterate through Tensors.
I would assume a `tf.one_hot()` modification can work.
Thanks."
7093,problems with Model saving and program slow to run,"[github_text_classification.zip](https://github.com/tensorflow/tensorflow/files/732694/github_text_classification.zip)
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
I tried to run the text_classification.py provided as one of the examples. But I basically ran into 2 problems:

1: The program took hours to finish (which just took minutes with version r0.8/0.9) and issued lots of warnings. Please run `text_classification.py` and the corresponding output is attached as `output.PNG`. 
2: Problems with saving models by using classifier = learn.DNNClassifier(..., model_dir). Please run `text_classification_with_model_saving.py` and the corresponding output is attached as `output_with_model_saving.PNG`. 


### Environment info (Operating System):
Running the command `lsb_release -a` gives the following:
Distributor ID: Ubuntu
Description: Ubuntu 16.04.1 LTS
Release: 16.04
Codename: xenial


### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
1. https://github.com/tensorflow/tensorflow/issues/6119
The person tried to solve a different problem. But the warnings he got were almost the same as I got. He seemed to fix his problem by updating the tensorflow to version r0.12.1, which is the one I'm using now.
2. http://stackoverflow.com/questions/40905736/rnn-lstm-time-series-tensorflow-0-12-error
The person also got almost the same warnings and errors. He got rid of all those by reverting back to version r0.10.0. But this is the last resort I would want.


Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
ls: cannot access '/path/to/cuda/lib/libcud*': No such file or directory


If installed from binary pip package, provide:
1. A link to the pip package you installed:
Running the command line `pip show tensorflow` gives the following:
Name: tensorflow
Version: 0.12.1
Location: /usr/local/lib/python2.7/dist-packages
Requires: mock, numpy, protobuf, wheel, six

2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
0.12.1
python version: 2.7.12
numpy version: 1.11.3

### What other attempted solutions have you tried?
I manipulated parameters for learn.DNNClassifier for a little bit, but still got the error.


### Logs or other output that would be helpful
1: See the attachment `output.PNG` for running `text_classification.py`
2: See the attachement `output_with_model_saving.PNG` for running `text_classification_with_model_saving.py`


### Other Notes
The original script appears to download corrupted dbpedia files. For convenience, I used my train and test csv files, and added my code for importing data files from Line 86 to Line 89. The two csv files are also attached. When running the scripts, please place the two data files in the same directory as the two scripts.

I ran both scripts in the CLI, e.g., python text_classification.py"
7091,Feature request: let tf.layers.batch_normalization normalize over multiple axes,"When using `tf.nn.batch_normalization` and `tf.nn.moments` it's possible to choose which axes are aggregated and thus which dimensions are normalized. For `tf.contrib.layers.batch_norm` and `tf.contrib.layers.layer_norm` the assumption is that only one dimension is normalized. There are use cases where it would be nice to independently normalize across both width and filters (for example audio spectrograms, where ""width"" could be seen as the time axis). Could the layers API be changed to be as flexible as `tf.nn.batch_normalization`?"
7089,Tensorboard multiple scalar summaries in one plot,"In 0.12 I could do this:
> testAccuracy = tf.Variable(0., name=""accuracy"")
trainAccuracy = tf.Variable(0., name=""accuracy"")
testAccSum = tf.scalar_summary(""Accuracy"", testAccuracy)
trainAccSum = tf.scalar_summary(""Accuracy"", trainAccuracy)
summaryOpsTest = tf.merge_summary([testAccSum])
summaryOpsTrain = tf.merge_summary([trainAccSum])
writerTest = tf.train.SummaryWriter(out_dir+'/test', sess.graph)
writerTrain = tf.train.SummaryWriter(out_dir+'/train', sess.graph)

to view both scalar summaries in a single plot.

In 0.12.1 I changed it to this:
> testAccuracy = tf.Variable(0., name=""accuracy"") 
trainAccuracy = tf.Variable(0., name=""accuracy"")
testAccSum = tf.summary.scalar(""Accuracy"", testAccuracy)
trainAccSum = tf.summary.scalar(""Accuracy"", trainAccuracy)
summaryOpsTest = tf.summary.merge([testAccSum])
summaryOpsTrain = tf.summary.merge([trainAccSum])
writerTest = tf.summary.FileWriter(out_dir+'/test', sess.graph)
writerTrain = tf.summary.FileWriter(out_dir+'/train', sess.graph)

due to the issued warnings.
But now each summary is drawn in its own plot (and the name is made unique (+""_1""))

Is this behavior intended? If so, is there a new way to achieve two summaries in one plot?

Thanks
"
7088,Running optimized graph with two output nodes on android gives “Session was not created with a graph before Run()” error.,"Hi, please would you please be so kind and help me with one issue that prevents me from moving forward. I have graph with two output layers (final_result_orig – which is basically coming form retraining example,final_result_added – my custom layer) and I am unable to strip/optimize_on_inference it in order to run on android device (on pc it runs fine)

When I run:

bazel-bin/tensorflow/python/tools/optimize_for_inference \
–input=/tmp/output.pb \
–output=/tmp/optimized.pb \
–input_names=Mul \
–output_names=”final_result_orig,final_result_added”
Then in my android application, I get “Session was not created with a graph before Run()” error, and both final_result_orig and final_result_added are not found.

When I run:

bazel-bin/tensorflow/python/tools/optimize_for_inference \
–input=/tmp/output.pb \
–output=/tmp/optimized.pb \
–input_names=Mul \
–output_names=”final_result_orig”

It works fine, final_result_orig is available and works correctly, however final_result_added is obviously not found and not available for my app to use.

And when I run:

bazel-bin/tensorflow/python/tools/optimize_for_inference \
–input=/tmp/output.pb \
–output=/tmp/optimized.pb \
–input_names=Mul \
–output_names=”final_result_added”
It does not work as well with “Session was not created with a graph before Run()” error, and both final_result_orig and final_result_added are not found.

I do not understand what I am doing wrong – what could be wrong with “final_result_added”, as it works fine on PC and not android? Thank you very much.

## What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
https://github.com/tensorflow/tensorflow/issues/5553


## Environment info

Ubuntu 16.04 + Android 6.0

"
7087,TensorBoard dones't work well on win10: ,"events.out.tfevents.1485424988 has been generated，its default been encoded with ANSI (check with notepad++)
but tensroboard decode it with some problems as bellow:

> **E:\>tensorboard --inspect --logdir=e:/**
> I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library cublas64_80.dll locally
> I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library cudnn64_5.dll locally
> I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library cufft64_80.dll locally
> I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library nvcuda.dll locally
> I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library curand64_80.dll locally
> 
> Processing event files... (this can take a few minutes)
> 
> 
> Traceback (most recent call last):
>   File ""d:\users\dc\anaconda3\lib\runpy.py"", line 184, in _run_module_as_main
>     ""__main__"", mod_spec)
>   File ""d:\users\dc\anaconda3\lib\runpy.py"", line 85, in _run_code
>     exec(code, run_globals)
>   File ""d:\Users\dc\Anaconda3\Scripts\tensorboard.exe\__main__.py"", line 9, in <module>
>   File ""d:\users\dc\anaconda3\lib\site-packages\tensorflow\tensorboard\tensorboard.py"", line 99, in main
>     efi.inspect(logdir, event_file, FLAGS.tag)
>   File ""d:\users\dc\anaconda3\lib\site-packages\tensorflow\python\summary\event_file_inspector.py"", line 409, in inspect
>     inspection_units = get_inspection_units(logdir, event_file, tag)
>   File ""d:\users\dc\anaconda3\lib\site-packages\tensorflow\python\summary\event_file_inspector.py"", line 360, in get_inspection_units
>     for subdir in subdirs:
>   File ""d:\users\dc\anaconda3\lib\site-packages\tensorflow\python\summary\event_multiplexer.py"", line 404, in <genexpr>
>     subdir
>   File ""d:\users\dc\anaconda3\lib\site-packages\tensorflow\python\summary\impl\io_wrapper.py"", line 50, in ListRecursively
>     for dir_path, _, filenames in gfile.Walk(top):
>   File ""d:\users\dc\anaconda3\lib\site-packages\tensorflow\python\lib\io\file_io.py"", line 446, in walk
>     for subitem in walk(os.path.join(top, subdir), in_order):
>   File ""d:\users\dc\anaconda3\lib\site-packages\tensorflow\python\lib\io\file_io.py"", line 427, in walk
>     listing = list_directory(top)
>   File ""d:\users\dc\anaconda3\lib\site-packages\tensorflow\python\lib\io\file_io.py"", line 406, in list_directory
>     compat.as_bytes(dirname), status)
>   File ""d:\users\dc\anaconda3\lib\site-packages\tensorflow\python\lib\io\file_io.py"", line 405, in <listcomp>
>     for filename in pywrap_tensorflow.GetChildren(
>   File ""d:\users\dc\anaconda3\lib\site-packages\tensorflow\python\util\compat.py"", line 106, in as_str_any
>     return as_str(value)
>   File ""d:\users\dc\anaconda3\lib\site-packages\tensorflow\python\util\compat.py"", line 84, in as_text
>     return bytes_or_text.decode(encoding)
> **UnicodeDecodeError: 'utf-8' codec can't decode byte 0xbb in position 2: invalid start byte**
> 
> E:\>"
7083,Feature request: simplify tf.train.write_graph() and tf.import_graph_def(),As can be seen in https://github.com/tensorflow/tensorflow/issues/616 the current mechanism is too painful. (Also #4044 etc )
7077,TensorBoard ImportError: No module named werkzeug,"using the current HEAD of tensorflow, I have bumped into an issue when I execute tensorboard.

Version is reported as: 0.12.head
git rev-parse HEAD: a12c7dc3d83049e10c1dca8903d73cc71d3cb7b2
Linux: Linux tensor 4.4.0-53-generic #74-Ubuntu SMP Fri Dec 2 15:59:10 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux

> 2017-01-25 17:02:36: I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcudnn.so.5 locally
2017-01-25 17:02:36: I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcufft.so.8.0 locally
2017-01-25 17:02:36: I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcuda.so.1 locally
2017-01-25 17:02:36: I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcurand.so.8.0 locally
Traceback (most recent call last):
  File ""/usr/local/bin/tensorboard"", line 9, in <module>
    load_entry_point('tensorflow', 'console_scripts', 'tensorboard')()
  File ""/home/greg/.local/lib/python2.7/site-packages/pkg_resources/__init__.py"", line 542, in load_entry_point
    return get_distribution(dist).load_entry_point(group, name)
  File ""/home/greg/.local/lib/python2.7/site-packages/pkg_resources/__init__.py"", line 2575, in load_entry_point
    return ep.load()
  File ""/home/greg/.local/lib/python2.7/site-packages/pkg_resources/__init__.py"", line 2235, in load
    return self.resolve()
  File ""/home/greg/.local/lib/python2.7/site-packages/pkg_resources/__init__.py"", line 2241, in resolve
    module = __import__(self.module_name, fromlist=['__name__'], level=0)
  File ""/home/greg/tensorflow/_python_build/tensorflow/tensorboard/tensorboard.py"", line 26, in <module>
    from werkzeug import serving
ImportError: No module named werkzeug


I see the werkzeug.BUILD file on my system so not sure why it cannot be found.
"
7072,[Windows] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
Only one, but it was not solved.
### Environment info
Operating System:
Windows 10 (anaconda 4.3.8)
`conda --version conda 4.3.8`
Installed version of CUDA and cuDNN: 
`nvcc --version nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2016 NVIDIA Corporation
Built on Sat_Sep__3_19:05:48_CDT_2016
Cuda compilation tools, release 8.0, V8.0.44`

If installed from binary pip package, provide:

1. A link to the pip package you installed:
`pip install --upgrade https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-0.12.0rc0-cp35-cp35m-win_amd64.whl`
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
`I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library cublas64_80.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library cudnn64_5.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library cufft64_80.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library nvcuda.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library curand64_80.dll locally`


### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
When I tried  [Single GPU computing example with tensorflow](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/5_MultiGPU/multigpu_basics.py) and get the following error:
`Placeholder_1: (Placeholder): /job:localhost/replica:0/task:0/gpu:0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:827] Placeholder_1: (Placeholder)/job:localhost/replica:0/task:0/gpu:0
MatMul_10: (MatMul): /job:localhost/replica:0/task:0/gpu:0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:827] MatMul_10: (MatMul)/job:localhost/replica:0/task:0/gpu:0
MatMul_11: (MatMul): /job:localhost/replica:0/task:0/gpu:0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:827] MatMul_11: (MatMul)/job:localhost/replica:0/task:0/gpu:0
MatMul_12: (MatMul): /job:localhost/replica:0/task:0/gpu:0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:827] MatMul_12: (MatMul)/job:localhost/replica:0/task:0/gpu:0
MatMul_13: (MatMul): /job:localhost/replica:0/task:0/gpu:0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:827] MatMul_13: (MatMul)/job:localhost/replica:0/task:0/gpu:0
MatMul_14: (MatMul): /job:localhost/replica:0/task:0/gpu:0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:827] MatMul_14: (MatMul)/job:localhost/replica:0/task:0/gpu:0
MatMul_15: (MatMul): /job:localhost/replica:0/task:0/gpu:0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:827] MatMul_15: (MatMul)/job:localhost/replica:0/task:0/gpu:0
MatMul_16: (MatMul): /job:localhost/replica:0/task:0/gpu:0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:827] MatMul_16: (MatMul)/job:localhost/replica:0/task:0/gpu:0
MatMul_17: (MatMul): /job:localhost/replica:0/task:0/gpu:0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:827] MatMul_17: (MatMul)/job:localhost/replica:0/task:0/gpu:0
MatMul_18: (MatMul): /job:localhost/replica:0/task:0/gpu:0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:827] MatMul_18: (MatMul)/job:localhost/replica:0/task:0/gpu:0
MatMul_19: (MatMul): /job:localhost/replica:0/task:0/gpu:0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:827] MatMul_19: (MatMul)/job:localhost/replica:0/task:0/gpu:0
Placeholder: (Placeholder): /job:localhost/replica:0/task:0/gpu:0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:827] Placeholder: (Placeholder)/job:localhost/replica:0/task:0/gpu:0
MatMul: (MatMul): /job:localhost/replica:0/task:0/gpu:0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:827] MatMul: (MatMul)/job:localhost/replica:0/task:0/gpu:0
MatMul_1: (MatMul): /job:localhost/replica:0/task:0/gpu:0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:827] MatMul_1: (MatMul)/job:localhost/replica:0/task:0/gpu:0
MatMul_2: (MatMul): /job:localhost/replica:0/task:0/gpu:0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:827] MatMul_2: (MatMul)/job:localhost/replica:0/task:0/gpu:0
MatMul_3: (MatMul): /job:localhost/replica:0/task:0/gpu:0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:827] MatMul_3: (MatMul)/job:localhost/replica:0/task:0/gpu:0
MatMul_4: (MatMul): /job:localhost/replica:0/task:0/gpu:0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:827] MatMul_4: (MatMul)/job:localhost/replica:0/task:0/gpu:0
MatMul_5: (MatMul): /job:localhost/replica:0/task:0/gpu:0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:827] MatMul_5: (MatMul)/job:localhost/replica:0/task:0/gpu:0
MatMul_6: (MatMul): /job:localhost/replica:0/task:0/gpu:0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:827] MatMul_6: (MatMul)/job:localhost/replica:0/task:0/gpu:0
MatMul_7: (MatMul): /job:localhost/replica:0/task:0/gpu:0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:827] MatMul_7: (MatMul)/job:localhost/replica:0/task:0/gpu:0
MatMul_8: (MatMul): /job:localhost/replica:0/task:0/gpu:0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:827] MatMul_8: (MatMul)/job:localhost/replica:0/task:0/gpu:0
MatMul_9: (MatMul): /job:localhost/replica:0/task:0/gpu:0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:827] MatMul_9: (MatMul)/job:localhost/replica:0/task:0/gpu:0
AddN: (AddN): /job:localhost/replica:0/task:0/cpu:0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:827] AddN: (AddN)/job:localhost/replica:0/task:0/cpu:0
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\cuda\cuda_blas.cc:372] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\stream.cc:1390] attempting to perform BLAS operation using StreamExecutor without BLAS support
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\cuda\cuda_blas.cc:372] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\stream.cc:1390] attempting to perform BLAS operation using StreamExecutor without BLAS support`

### What other attempted solutions have you tried?
Then I tried a sample matrix multiplication:
`sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:885] Found device 0 with properties:
name: Quadro M2000M
major: 5 minor: 0 memoryClockRate (GHz) 1.137
pciBusID 0000:01:00.0
Total memory: 4.00GiB
Free memory: 3.35GiB
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:906] DMA: 0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:916] 0:   Y
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0)
Device mapping:
/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\direct_session.cc:255] Device mapping:
/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0`

and the following error:

`a = tf.random_normal((100,100))`
`b = tf.random_normal((100,500))`
`c = tf.matmul(a,b)`
`sess.run(c)`
`random_normal_1/RandomStandardNormal: (RandomStandardNormal): /job:localhost/replica:0/task:0/gpu:0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:827] random_normal_1/RandomStandardNormal: (RandomStandardNormal)/job:localhost/replica:0/task:0/gpu:0
random_normal_1/mul: (Mul): /job:localhost/replica:0/task:0/gpu:0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:827] random_normal_1/mul: (Mul)/job:localhost/replica:0/task:0/gpu:0
random_normal_1: (Add): /job:localhost/replica:0/task:0/gpu:0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:827] random_normal_1: (Add)/job:localhost/replica:0/task:0/gpu:0
random_normal/RandomStandardNormal: (RandomStandardNormal): /job:localhost/replica:0/task:0/gpu:0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:827] random_normal/RandomStandardNormal: (RandomStandardNormal)/job:localhost/replica:0/task:0/gpu:0
random_normal/mul: (Mul): /job:localhost/replica:0/task:0/gpu:0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:827] random_normal/mul: (Mul)/job:localhost/replica:0/task:0/gpu:0
random_normal: (Add): /job:localhost/replica:0/task:0/gpu:0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:827] random_normal: (Add)/job:localhost/replica:0/task:0/gpu:0
MatMul: (MatMul): /job:localhost/replica:0/task:0/gpu:0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:827] MatMul: (MatMul)/job:localhost/replica:0/task:0/gpu:0
random_normal_1/stddev: (Const): /job:localhost/replica:0/task:0/gpu:0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:827] random_normal_1/stddev: (Const)/job:localhost/replica:0/task:0/gpu:0
random_normal_1/mean: (Const): /job:localhost/replica:0/task:0/gpu:0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:827] random_normal_1/mean: (Const)/job:localhost/replica:0/task:0/gpu:0
random_normal_1/shape: (Const): /job:localhost/replica:0/task:0/gpu:0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:827] random_normal_1/shape: (Const)/job:localhost/replica:0/task:0/gpu:0
random_normal/stddev: (Const): /job:localhost/replica:0/task:0/gpu:0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:827] random_normal/stddev: (Const)/job:localhost/replica:0/task:0/gpu:0
random_normal/mean: (Const): /job:localhost/replica:0/task:0/gpu:0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:827] random_normal/mean: (Const)/job:localhost/replica:0/task:0/gpu:0
random_normal/shape: (Const): /job:localhost/replica:0/task:0/gpu:0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:827] random_normal/shape: (Const)/job:localhost/replica:0/task:0/gpu:0
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\cuda\cuda_blas.cc:372] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\stream.cc:1390] attempting to perform BLAS operation using StreamExecutor without BLAS support`
`

Like @kingtaurus and @menggangmark,

I then copied the cudnn64_5.dll (cuda\bin\cudnn64_5.dll) from that zip archive into C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0\bin\;

cuda\include\cudnn.h to C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0\include\;

and

cuda\lib\x64\cudnn.lib to C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0\lib\x64\

cupti64_80.dll (C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0\extras\CUPTI\libx64) to C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0\bin;
and cupti.lib(C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0\extras\CUPTI\libx64) to C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0\lib\x64

WHERE C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0 is my install PATH for the CUDA toolkit.
I had already added C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0\bin\ to my PATH

"
7069,error in installing tf from source,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
7068,Feature Request - support distributed Tensorflow on Sun Grid Engine job scheduler,Sun Grid Engine is commonly used to schedule jobs on scientific computing clusters / supercomputers.  Please consider adding support for distributed Tensorflow on SGE.
7067,Recurrent Spatial Transformer Network,"i'm trying to replicate Recurrent Spatial Transformer Network implemented here (https://github.com/skaae/recurrent-spatial-transformer-code) , however the loss didn't decrease at all .


the configuration of the network is as follow:
1 -  relu activations .

2 - xavier weight initialization for weights , zero initialization for biases .

3 - cost function is softmax_cross_entropy_with_logits .

4 - optimizer is RMSProp (i tried 1e-6 ;1e-10 espilon) .

5 - gradient clipping by value .

so what should i try next ?"
7065,pytorch 2.5x faster on VGG16,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

Started on SO, and was told to post here ([SO post](http://stackoverflow.com/questions/41832779/tensorflow-2-5x-slower-than-pytorch-on-vgg16-architecture?noredirect=1#comment70901342_41832779))
### Environment info
Operating System:
Ubuntu 14.04 + Maxwell Titan X

Installed version of CUDA and cuDNN: 
CUDA 8.0, cuDNN 5.1
```python
:~$ ls -l /usr/local/cuda/lib64/libcud*
-rw-r--r-- 1 root root    558720 Jan 25 08:23 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root        16 Jan 25 08:23 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root        19 Jan 25 08:23 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44
-rwxr-xr-x 1 root root    415432 Jan 25 08:23 /usr/local/cuda/lib64/libcudart.so.8.0.44
-rw-r--r-- 1 root root    775162 Jan 25 08:23 /usr/local/cuda/lib64/libcudart_static.a
lrwxrwxrwx 1 1000 users       13 Jul 27 07:55 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 1000 users       17 Jul 27 07:55 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5
-rwxrwxr-x 1 1000 users 79337624 Jul 27 07:53 /usr/local/cuda/lib64/libcudnn.so.5.1.5
-rw-rw-r-- 1 1000 users 69756172 Jul 27 07:53 /usr/local/cuda/lib64/libcudnn_static.a
```

Installed from binary pip package :
1. https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp35-cp35m-linux_x86_64.whl with an Anaconda distribution
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`:
````
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
0.12.1
````


### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

Using the following code to do a forward pass on a pretrained VGG16 :
```python
import tensorflow as tf
from tensorflow.contrib import slim
from tensorflow.contrib.slim import nets

tf.reset_default_graph()
# Use RNG to avoid the feed_dict argument
input_images = tf.random_uniform((16, 224, 224, 3), maxval=255)  
preds = nets.vgg.vgg_16(input_images, is_training=False)[0]
saver = tf.train.Saver()

config = tf.ConfigProto(log_device_placement=True)
sess = tf.InteractiveSession(config=config)
saver.restore(sess, './vgg_16.ckpt')

# With jupyter notebook magic
%timeit sess.run(preds)
```

Compared to the pytorch version on the same machine :
```python
import numpy as np
import torch
import torchvision.models as models
from torch.autograd import Variable
torch.backends.cudnn.benchmark = True

net = models.vgg16()
net.cuda()

_in = Variable(torch.from_numpy(np.random.randn(16, 3, 224, 224).astype(np.float32)).cuda())

# With jupyter notebook magic
%timeit net(_in)
```

I get the following results by comparing the frameworks. Surprisingly, there is a small difference with the more complicated resnet-50 while I get a huge gap for the VGG16 architecture which (almost) just uses 3x3 convolutions.


Model | TF | pytorch
--- | --- | ---
VGG16 | 160ms | 65ms
resnet-50 | 58ms | 48ms"
7064,can't install tenserflow on ubuntu 16.04 via pip!!,"Exception:
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/pip/basecommand.py"", line 215, in main
    status = self.run(options, args)
  File ""/usr/local/lib/python2.7/dist-packages/pip/commands/install.py"", line 342, in run
    prefix=options.prefix_path,
  File ""/usr/local/lib/python2.7/dist-packages/pip/req/req_set.py"", line 784, in install
    **kwargs
  File ""/usr/local/lib/python2.7/dist-packages/pip/req/req_install.py"", line 851, in install
    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)
  File ""/usr/local/lib/python2.7/dist-packages/pip/req/req_install.py"", line 1064, in move_wheel_files
    isolated=self.isolated,
  File ""/usr/local/lib/python2.7/dist-packages/pip/wheel.py"", line 345, in move_wheel_files
    clobber(source, lib_dir, True)
  File ""/usr/local/lib/python2.7/dist-packages/pip/wheel.py"", line 316, in clobber
    ensure_dir(destdir)
  File ""/usr/local/lib/python2.7/dist-packages/pip/utils/__init__.py"", line 83, in ensure_dir
    os.makedirs(path)
  File ""/usr/lib/python2.7/os.py"", line 157, in makedirs
    mkdir(name, mode)
OSError: [Errno 13] Permission denied: '/usr/local/lib/python2.7/dist-packages/tensorflow_gpu-0.12.1.dist-info'

"
7063,Can't install Tenserflow on ubuntu 16.04 via pip?,"Exception:
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/pip/basecommand.py"", line 215, in main
    status = self.run(options, args)
  File ""/usr/local/lib/python2.7/dist-packages/pip/commands/install.py"", line 342, in run
    prefix=options.prefix_path,
  File ""/usr/local/lib/python2.7/dist-packages/pip/req/req_set.py"", line 784, in install
    **kwargs
  File ""/usr/local/lib/python2.7/dist-packages/pip/req/req_install.py"", line 851, in install
    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)
  File ""/usr/local/lib/python2.7/dist-packages/pip/req/req_install.py"", line 1064, in move_wheel_files
    isolated=self.isolated,
  File ""/usr/local/lib/python2.7/dist-packages/pip/wheel.py"", line 345, in move_wheel_files
    clobber(source, lib_dir, True)
  File ""/usr/local/lib/python2.7/dist-packages/pip/wheel.py"", line 316, in clobber
    ensure_dir(destdir)
  File ""/usr/local/lib/python2.7/dist-packages/pip/utils/__init__.py"", line 83, in ensure_dir
    os.makedirs(path)
  File ""/usr/lib/python2.7/os.py"", line 157, in makedirs
    mkdir(name, mode)
OSError: [Errno 13] Permission denied: '/usr/local/lib/python2.7/dist-packages/tensorflow_gpu-0.12.1.dist-info'
NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
7061,Is there any GPU implementation of Queues?,"what is the best way of pipelining the training samples in GPU to maximize its utilization?

The natural way of doing this seems to be using Queues, however it seems there is no GPU kernel for any type of Queue."
7060,Tensorboard: Please improve log-scale plots,"Hi!

Tensorboard currently (version 0.12.1) has an option to plot graphs with a logarithmic log scale, but this functionality doesn't work well whenever plotting very small values (e.g. the ones that converge to zero, like loss values). For example, compare this plot in Tensorboard:

![tensorboard_logplot](https://cloud.githubusercontent.com/assets/3627551/22293860/8abcdc40-e311-11e6-9e70-0796a503afac.png)

With the exact same data plotted using matplotlib/pandas using `logy=True`:

    runs = sorted(os.listdir('tensorboard/logs/'))[:2]
    d = [pd.read_csv('http://127.0.0.1:6033/data/scalars?run=%s&tag=loss&format=csv' % r)['Value'] for r in runs]
    res = pd.DataFrame(d, index=runs).T
    res.plot(ax=ax, logy=True)

![matplotlib_logplot](https://cloud.githubusercontent.com/assets/3627551/22293890/9af8e568-e311-11e6-95ff-a803a39f5d27.png)

Clearly, the Tensorboard plot is much less informative (the ""logarithmic plot"" option doesn't do anything to improve the plot). It would be nice if Tensorboard could produce logarithmic plots similar to the ones in matplotlib."
7059,Issue to compile on macOS with unrecognized command line option,"Hello,

I do have issues to compile Tensorflow on my mac in order to use the Java version. Here some useful information about my environment:

* macOS 10.12.3
* gcc 6.3.0 (from MacPorts)
* Bazel 4.3.0
* Python 2.7
* Tensorflow commit a12c7dc3d83049e10c1dca8903d73cc71d3cb7b2
* CPU only

Once I have done the ```./configure``` I'm trying to compile the Java version and here the log I get:

```
bazel build -c opt //tensorflow/java:tensorflow //tensorflow/java:libtensorflow_jni
INFO: Found 2 targets...
ERROR: /private/var/tmp/_bazel_jplu/00c84c0db16fb633b58c75ed37aef199/external/nanopb_git/BUILD:8:1: C++ compilation of rule '@nanopb_git//:nanopb' failed: cc_wrapper.sh failed: error executing command external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wthread-safety -Wself-assign -Wunused-but-set-parameter -Wno-free-nonheap-object ... (remaining 34 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
gcc: error: unrecognized command line option '-Wthread-safety'; did you mean '-fthread-jumps'?
gcc: error: unrecognized command line option '-Wself-assign'; did you mean '-Wparam-assign'?
INFO: Elapsed time: 6,013s, Critical Path: 0,05s
```

I also tried with a GCC 5.4 version and I get the exact same error.

Did anyone have an idea of what is going wrong? Am I using a wrong version of GCC?

Thanks in advance."
7058,what's the difference betwee tf.nn.seq2seq and tf.python.ops.seq2seq?,what's the difference betwee tf.nn.seq2seq and tf.python.ops.seq2seq?
7057,[PATCH] FreeBSD compatibility,"Hi,

I've managed to compile tensorflow in cpu mode so here's whats needed for FreeBSD compatibility.

As I've no clue how bazel really works I've mostly hacked the bazel stuff, so someone that knows what he's doing might want to make the BUILD changes in the proper place :)

**Summary**
1. -ldl needs to be removed from all linkopts for FreeBSD as libdl is integrated into libc  so FreeBSD does not have -ldl
2. No code needs to be changed becides adding a few ifstatements for FreeBSD right next to APPLE statements and adding a missing header file
3. external/protobuf/BUILD needs ""-lm"" in LINK_OPTS, I did not figure out how to tell bazel to do that during tar.gz extraction.
"
7056,Build is failing on Linux GPU,"Hi,

I'm trying with no luck to build TF for OpenCL from source. I configured a new installed machine as is described in your guide but the build is failing.

I'm launching the build through this command:
`bazel build -c opt --config=sycl //tensorflow/tools/pip_package:build_pip_package`

And below you can find all the requested informations from my OS. In attach you'll find the [Errors](https://github.com/tensorflow/tensorflow/files/729409/Errors.TF.txt) the compiler shows me.
I've tryed also to build with clang against GCC/G++ with no luck.
[Errors TF.txt](https://github.com/tensorflow/tensorflow/files/729409/Errors.TF.txt)

Thank you

------------------

Operating System: Ubuntu 16.04, Kernel 4.4

Building TF for OpenCL as descripted in ./configure below:
`$ ./configure 
Please specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3
Please specify optimization flags to use during compilation [Default is -march=native]: 
Do you wish to use jemalloc as the malloc implementation? [Y/n] 
jemalloc enabled
Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] 
No Google Cloud Platform support will be enabled for TensorFlow
Do you wish to build TensorFlow with Hadoop File System support? [y/N] 
No Hadoop File System support will be enabled for TensorFlow
Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] 
No XLA support will be enabled for TensorFlow
Found possible Python library paths:
  /usr/local/lib/python3.5/dist-packages
  /usr/lib/python3/dist-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python3.5/dist-packages]

Using python library path: /usr/local/lib/python3.5/dist-packages
Do you wish to build TensorFlow with OpenCL support? [y/N] y
OpenCL support will be enabled for TensorFlow
Do you wish to build TensorFlow with CUDA support? [y/N] n
No CUDA support will be enabled for TensorFlow
Please specify which C++ compiler should be used as the host C++ compiler. [Default is ]: /usr/bin/g++
Please specify which C compiler should be used as the host C compiler. [Default is ]: /usr/bin/gcc
Please specify the location where ComputeCpp for SYCL 1.2 is installed. [Default is /usr/local/computecpp]: `

`$ bazel version
Build label: 0.4.3
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Dec 22 12:31:25 2016 (1482409885)
Build timestamp: 1482409885
Build timestamp as int: 1482409885`

`$ git rev-parse HEAD
a12c7dc3d83049e10c1dca8903d73cc71d3cb7b2`"
7055,TensorFlow Bazel build is failing on Windows,"http://ci.tensorflow.org/job/tf-master-win-bzl/339/console

```
18:24:27 ERROR: C:/tf_jenkins/home/workspace/tf-master-win-bzl/tensorflow/core/kernels/BUILD:3802:1: C++ compilation of rule '//tensorflow/core/kernels:quantized_ops' failed: msvc_cl.bat failed: error executing command 
...
18:24:27 c:\tmp\_bazel_system\rrc05caq\execroot\tf-master-win-bzl\external\gemmlowp\internal\common.h(21): fatal error C1083: Cannot open include file: 'pthread.h': No such file or directory
```

Culprit: f736991fd3a7987665a6f9fcd26d464ea7f68e2b
This changes introduced the dependency on `//tensorflow/core/kernels:quantized_ops` through `//tensorflow/tools/graph_transforms:transform_graph_lib`, which doesn't build on Windows yet.
"
7054,Slicing,"I wish to implement the following line of code in tensorflow. Given below is the numpy version
correct_scores=scores[range(N), np.array(yTrain,dtype='int32')]

now in tensorflow, scores and yTrain are all variables, calculated on each minibatch. Is there a way to do this
"
7053,TensorFlow- Recurrent Neural networks.,"










NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
7052,Please create a new version tag.,"There's currently no version tag that has all the WORKSPACE links working correctly. Can we please create one that I can point to to have a stable TF version? If I point to HEAD using git submodule we often update the TF version by mistake.

Thanks!"
7051,Bug - tfdbg + multi-gpu gives ValueError: Duplicate node name: 'n/_0',"Hello tensorflow team,

I have been starting to use your tensorflow debugger but have run into the issue that when I try and use it on a multi-gpu model I get `ValueError: Duplicate node name: 'n/_0'`.

Inspecting things closer, I saw that the issue originated from the run_metadata, whose partition graphs have many _Send and _HostRecv ops with names like 'n/_0'.  These ops are replicated with identical names across my towers which is what is causing the issue.

Looking through the tensorflow code, I believe I tracked where this name is set down to [graph_partition.cc:195](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/graph/graph_partition.cc#L195) where the edge's source name is used as the prefix 'n'.  Unfortunately, I have not been able to figure out why the source's name is only 'n', but that seems to be the root of the issue here.

I should add that I never set any tensor name to 'n' anywhere in my own code.  Plus, I see certain tests in your codebase rely on names such as 'n/_0' which indicates to me the name is being set somewhere internally in the tensorflow code.

Any help you can provide would be much appreciated!

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

I didn't find any related issues.

### Environment info
Operating System: Ubuntu 14.04.5 LTS (running in a [singularity](http://singularity.lbl.gov) container on a CentOS 6.7 host). 

Installed version of CUDA and cuDNN: 
I am using CUDA 8.0 with NVIDIA driver 367.48, and cuDNN v5.1 . 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
```
libOpenCL.so
libOpenCL.so.1
libOpenCL.so.1.0
libOpenCL.so.1.0.0
libcublas.so
libcublas.so.8.0
libcublas.so.8.0.45
libcublas_device.a
libcublas_static.a
libcudadevrt.a
libcudart.so
libcudart.so.8.0
libcudart.so.8.0.44
libcudart_static.a
libcudnn.so
libcudnn.so.5
libcudnn.so.5.1.5
libcudnn_static.a
libcufft.so
libcufft.so.8.0
libcufft.so.8.0.44
libcufft_static.a
libcufftw.so
libcufftw.so.8.0
libcufftw.so.8.0.44
libcufftw_static.a
libcuinj64.so
libcuinj64.so.8.0
libcuinj64.so.8.0.44
libculibos.a
libcurand.so
libcurand.so.8.0
libcurand.so.8.0.44
libcurand_static.a
libcusolver.so
libcusolver.so.8.0
libcusolver.so.8.0.44
libcusolver_static.a
libcusparse.so
libcusparse.so.8.0
libcusparse.so.8.0.44
libcusparse_static.a
libnppc.so
libnppc.so.8.0
libnppc.so.8.0.44
libnppc_static.a
libnppi.so
libnppi.so.8.0
libnppi.so.8.0.44
libnppi_static.a
libnppial.so
libnppial.so.8.0
libnppial.so.8.0.44
libnppicc.so
libnppicc.so.8.0
libnppicc.so.8.0.44
libnppicom.so
libnppicom.so.8.0
libnppicom.so.8.0.44
libnppidei.so
libnppidei.so.8.0
libnppidei.so.8.0.44
libnppif.so
libnppif.so.8.0
libnppif.so.8.0.44
libnppig.so
libnppig.so.8.0
libnppig.so.8.0.44
libnppim.so
libnppim.so.8.0
libnppim.so.8.0.44
libnppist.so
libnppist.so.8.0
libnppist.so.8.0.44
libnppisu.so
libnppisu.so.8.0
libnppisu.so.8.0.44
libnppitc.so
libnppitc.so.8.0
libnppitc.so.8.0.44
libnpps.so
libnpps.so.8.0
libnpps.so.8.0.44
libnpps_static.a
libnvToolsExt.so
libnvToolsExt.so.1
libnvToolsExt.so.1.0.0
libnvblas.so
libnvblas.so.8.0
libnvblas.so.8.0.44
libnvgraph.so
libnvgraph.so.8.0
libnvgraph.so.8.0.44
libnvgraph_static.a
libnvrtc-builtins.so
libnvrtc-builtins.so.8.0
libnvrtc-builtins.so.8.0.44
libnvrtc.so
libnvrtc.so.8.0
libnvrtc.so.8.0.44
stubs
```



1. A link to the pip package you installed:
I installed tensorflow using `pip install tensorflow-gpu==0.12.1`
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
```
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally 
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally 
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
0.12.1 
```                                                                                     


### What other attempted solutions have you tried?

The single GPU case works fine.

### Logs or other output that would be helpful

Here is the dump of some of the problematic nodes.

```
node {
  name: ""n/_0""
  op: ""_Send""
  input: ""__copy_TOWER0/Const_0""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""client_terminated""
    value {
      b: false
    }
  }
  attr {
    key: ""recv_device""
    value {
      s: ""/job:localhost/replica:0/task:0/gpu:0""
    }
  }
  attr {
    key: ""send_device""
    value {
      s: ""/job:localhost/replica:0/task:0/gpu:0""
    }
  }
  attr {
    key: ""send_device_incarnation""
    value {
      i: 0
    }
  }
  attr {
    key: ""tensor_name""
    value {
      s: ""edge_545___copy_TOWER0/Const_0""
    }
  }
}
node {
  name: ""n/_1""
  op: ""_HostRecv""
  input: ""^n/_0""
  attr {
    key: ""client_terminated""
    value {
      b: false
    }
  }
  attr {
    key: ""recv_device""
    value {
      s: ""/job:localhost/replica:0/task:0/gpu:0""
    }
  }
  attr {
    key: ""send_device""
    value {
      s: ""/job:localhost/replica:0/task:0/gpu:0""
    }
  }
  attr {
    key: ""send_device_incarnation""
    value {
      i: 0
    }
  }
  attr {
    key: ""tensor_name""
    value {
      s: ""edge_545___copy_TOWER0/Const_0""
    }
  }
  attr {
    key: ""tensor_type""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""n/_2""
  op: ""_Send""
  input: ""__copy_TOWER0/Sub_0""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""client_terminated""
    value {
      b: false
    }
  }
  attr {
    key: ""recv_device""
    value {
      s: ""/job:localhost/replica:0/task:0/gpu:0""
    }
  }
  attr {
    key: ""send_device""
    value {
      s: ""/job:localhost/replica:0/task:0/gpu:0""
    }
  }
  attr {
    key: ""send_device_incarnation""
    value {
      i: 0
    }
  }
  attr {
    key: ""tensor_name""
    value {
      s: ""edge_551___copy_TOWER0/Sub_0""
    }
  }
}

```

End of backtrace at crash point
```
  /home/raphtown/.local/lib/python2.7/site-packages/tensorflow/python/debug/wrappers/framework.py(419)run()                 
-> run_end_resp = self.on_run_end(run_end_req)                                                                              
  /home/raphtown/.local/lib/python2.7/site-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py(262)on_run_end()  
-> self._dump_root, partition_graphs=partition_graphs)                                                                      
  /home/raphtown/.local/lib/python2.7/site-packages/tensorflow/python/debug/debug_data.py(407)__init__()                    
-> self._load_partition_graphs(partition_graphs)                                                                            
> /home/raphtown/.local/lib/python2.7/site-packages/tensorflow/python/debug/debug_data.py(493)_load_partition_graphs()      
-> raise ValueError(""Duplicate node name: '%s'"" % node.name)                                                                
```"
7049,NotFoundError: /tf_files/retrained_graph.pb,"When I tried to follow the Tensorflow Poets tutorial steps, I got this error when try to run the label_image.py file with classifier.

python /tf_files/label_image.py /tf_files/flower_photos/daisy/21652746_cc379e0eea_m.jpg 
Traceback (most recent call last):
  File ""/tf_files/label_image.py"", line 15, in <module>
    graph_def.ParseFromString(f.read())
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/lib/io/file_io.py"", line 106, in read
    self._preread_check()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/lib/io/file_io.py"", line 73, in _preread_check
    compat.as_bytes(self.__name), 1024 * 512, status)
  File ""/usr/lib/python2.7/contextlib.py"", line 24, in __exit__
    self.gen.next()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py"", line 469, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.NotFoundError: /tf_files/retrained_graph.pb
root@858f20e4c756:/tensorflow# ls /tf_files/retrained_graph.pd 


NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
git rev-parse HEAD              
45ab528211c962b19e12f6b77165848310271624
2. The output of `bazel version`
Build label: 0.3.2
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Oct 7 17:25:10 2016 (1475861110)
Build timestamp: 1475861110
Build timestamp as int: 1475861110
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
7048,XLA on MacOS: ./configure gives java.lang.RuntimeException: Unrecoverable error while evaluating node 'REPOSITORY_DIRECTORY:@llvm' (requested by nodes 'REPOSITORY:@llvm'),"Not sure if this is WAI since XLA is experimental, but this is the error on MacOS that happens when you try to run ./configure with XLA=true

Same version configures fine on Linux (master as of now)

```
bash-3.2$ ./configure
./configure
Please specify the location of python. [Default is /Users/yaroslav/anaconda/bin/python]: 

Please specify optimization flags to use during compilation [Default is -march=native]: 

Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] 

No Google Cloud Platform support will be enabled for TensorFlow
Do you wish to build TensorFlow with Hadoop File System support? [y/N] 

No Hadoop File System support will be enabled for TensorFlow
Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] y
y
XLA JIT support will be enabled for TensorFlow
Found possible Python library paths:
  /Users/yaroslav/anaconda/lib/python3.5/site-packages
Please input the desired Python library path to use.  Default is [/Users/yaroslav/anaconda/lib/python3.5/site-packages]


Using python library path: /Users/yaroslav/anaconda/lib/python3.5/site-packages
Do you wish to build TensorFlow with OpenCL support? [y/N] 

No OpenCL support will be enabled for TensorFlow
Do you wish to build TensorFlow with CUDA support? [y/N] y
y
CUDA support will be enabled for TensorFlow
Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 

Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 

Please specify the location where CUDA  toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 

Please specify the Cudnn version you want to use. [Leave empty to use system default]: 

Please specify the location where cuDNN  library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 

libcudnn.dylib resolves to libcudnn.dylib
Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size.
[Default is: ""3.5,5.2""]: 

____Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.
.........
____Loading package: tensorflow/contrib/image
____Loading package: tensorflow/core/debug
____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 65,536 bytes
____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 1,676,424 bytes
____Loading package: @bazel_tools//tools/jdk
____Loading package: tools/defaults
____Loading package: third_party/py/numpy
____Loading package: util/python
java.lang.RuntimeException: Unrecoverable error while evaluating node 'REPOSITORY_DIRECTORY:@llvm' (requested by nodes 'REPOSITORY:@llvm')
	at com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:429)
	at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:501)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalArgumentException: Invalid EvalException:
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
	at java.util.concurrent.Semaphore.acquire(Semaphore.java:312)
	at com.google.devtools.build.lib.bazel.repository.downloader.HttpDownloader.download(HttpDownloader.java:196)
	at com.google.devtools.build.lib.bazel.repository.skylark.SkylarkRepositoryContext.downloadAndExtract(SkylarkRepositoryContext.java:594)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.google.devtools.build.lib.syntax.FuncallExpression.callMethod(FuncallExpression.java:316)
	at com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:732)
	at com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:784)
	at com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:770)
	at com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:48)
	at com.google.devtools.build.lib.syntax.ExpressionStatement.doExec(ExpressionStatement.java:46)
	at com.google.devtools.build.lib.syntax.Statement.exec(Statement.java:37)
	at com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:136)
	at com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:439)
	at com.google.devtools.build.lib.bazel.repository.skylark.SkylarkRepositoryFunction.fetch(SkylarkRepositoryFunction.java:106)
	at com.google.devtools.build.lib.rules.repository.RepositoryDelegatorFunction.compute(RepositoryDelegatorFunction.java:155)
	at com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:370)
	at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:501)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

	at com.google.devtools.build.lib.syntax.EvalException.<init>(EvalException.java:112)
	at com.google.devtools.build.lib.syntax.EvalException$EvalExceptionWithJavaCause.<init>(EvalException.java:209)
	at com.google.devtools.build.lib.syntax.EvalException$EvalExceptionWithJavaCause.<init>(EvalException.java:217)
	at com.google.devtools.build.lib.syntax.FuncallExpression.callMethod(FuncallExpression.java:344)
	at com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:732)
	at com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:784)
	at com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:770)
	at com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:48)
	at com.google.devtools.build.lib.syntax.ExpressionStatement.doExec(ExpressionStatement.java:46)
	at com.google.devtools.build.lib.syntax.Statement.exec(Statement.java:37)
	at com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:136)
	at com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:439)
	at com.google.devtools.build.lib.bazel.repository.skylark.SkylarkRepositoryFunction.fetch(SkylarkRepositoryFunction.java:106)
	at com.google.devtools.build.lib.rules.repository.RepositoryDelegatorFunction.compute(RepositoryDelegatorFunction.java:155)
	at com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:370)
	... 4 more
java.lang.RuntimeException: Unrecoverable error while evaluating node 'REPOSITORY_DIRECTORY:@llvm' (requested by nodes 'REPOSITORY:@llvm')
	at com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:429)
	at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:501)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalArgumentException: Invalid EvalException:
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
	at java.util.concurrent.Semaphore.acquire(Semaphore.java:312)
	at com.google.devtools.build.lib.bazel.repository.downloader.HttpDownloader.download(HttpDownloader.java:196)
	at com.google.devtools.build.lib.bazel.repository.skylark.SkylarkRepositoryContext.downloadAndExtract(SkylarkRepositoryContext.java:594)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.google.devtools.build.lib.syntax.FuncallExpression.callMethod(FuncallExpression.java:316)
	at com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:732)
	at com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:784)
	at com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:770)
	at com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:48)
	at com.google.devtools.build.lib.syntax.ExpressionStatement.doExec(ExpressionStatement.java:46)
	at com.google.devtools.build.lib.syntax.Statement.exec(Statement.java:37)
	at com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:136)
	at com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:439)
	at com.google.devtools.build.lib.bazel.repository.skylark.SkylarkRepositoryFunction.fetch(SkylarkRepositoryFunction.java:106)
	at com.google.devtools.build.lib.rules.repository.RepositoryDelegatorFunction.compute(RepositoryDelegatorFunction.java:155)
	at com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:370)
	at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:501)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

	at com.google.devtools.build.lib.syntax.EvalException.<init>(EvalException.java:112)
	at com.google.devtools.build.lib.syntax.EvalException$EvalExceptionWithJavaCause.<init>(EvalException.java:209)
	at com.google.devtools.build.lib.syntax.EvalException$EvalExceptionWithJavaCause.<init>(EvalException.java:217)
	at com.google.devtools.build.lib.syntax.FuncallExpression.callMethod(FuncallExpression.java:344)
	at com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:732)
	at com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:784)
	at com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:770)
	at com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:48)
	at com.google.devtools.build.lib.syntax.ExpressionStatement.doExec(ExpressionStatement.java:46)
	at com.google.devtools.build.lib.syntax.Statement.exec(Statement.java:37)
	at com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:136)
	at com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:439)
	at com.google.devtools.build.lib.bazel.repository.skylark.SkylarkRepositoryFunction.fetch(SkylarkRepositoryFunction.java:106)
	at com.google.devtools.build.lib.rules.repository.RepositoryDelegatorFunction.compute(RepositoryDelegatorFunction.java:155)
	at com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:370)
	... 4 more

```"
7044,tf.gradients like functionality in the C and C++ API,"Currently gradients can only be computed manually in C++ or automatically in Python with tf.gradients. Work on registering C++ implemented gradients for all the OPs has started. However, there is currently no easy way to automatically use those gradients to generate a graph. The C-api also does not have gradient creation calls yet.

This supercedes the gradient part of #476, since the rest of #476's issue was addressed."
7042,replicated function tf.where()?,"Hi all,

I have a problem adding an Embedding layer to a `Sequential` model.

```
from keras.layers import Embedding
from keras.layers import Dense, Input, Flatten
from keras.layers import Conv1D, MaxPooling1D, Embedding
from keras.models import Model
from keras.models import Sequential
from keras.layers import LSTM,Bidirectional

embedding_layer = Embedding(len(word_index) + 1,
                            EMBEDDING_DIM,
                            weights=[embedding_matrix],
                            input_length=MAX_SEQUENCE_LENGTH*2,
                             dropout=0.2,
                            trainable=False)

model = Sequential()
model.add(embedding_layer)
model.add(Bidirectional(LSTM(100, dropout_W=0.2, dropout_U=0.2)))
model.add(Dense(200))
model.add(Dense(6, activation='softmax'))
```

I have the next error:
```
Traceback (most recent call last):
  File ""recurrent_ltsm.py"", line 201, in <module>
    model.add(embedding_layer)
  File ""/usr/local/lib/python2.7/dist-packages/keras/models.py"", line 299, in add
    layer.create_input_layer(batch_input_shape, input_dtype)
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 401, in create_input_layer
    self(x)
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 572, in __call__
    self.add_inbound_node(inbound_layers, node_indices, tensor_indices)
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 635, in add_inbound_node
    Node.create_node(self, inbound_layers, node_indices, tensor_indices)
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 166, in create_node
    output_tensors = to_list(outbound_layer.call(input_tensors[0], mask=input_masks[0]))
  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/embeddings.py"", line 123, in call
    B = K.random_binomial((self.input_dim,), p=retain_p) * (1. / retain_p)
  File ""/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py"", line 2895, in random_binomial
    tf.zeros(shape, dtype=dtype))
TypeError: where() takes at most 2 arguments (3 given)
```
I have seen two versions of `tf.where()` in the documentation:

[https://www.tensorflow.org/versions/r0.10/api_docs/python/control_flow_ops/comparison_operators#where](url)
and 
[https://www.tensorflow.org/api_docs/python/math_ops/sequence_comparison_and_indexing#where](url)

Any idea why the program confuses them?

```
python -c 'import tensorflow as tf; print tf.__version__'
0.10.0
```


Thank you very much."
7041,Tensorboard: Undefined import in serialize_tensorboard.py,"It looks like this change https://github.com/tensorflow/tensorflow/commit/e121667dc609de978a223c56ee906368d2c4ceef modified the module imports in `serialize_tensorboard.py` to remove the import of `tf` but did not update all the usages of `tf`. As such, `tf` is not defined error is thrown when trying to run the script."
7039,Tensorboard stopped showing runs names,"Hi,

Tensorboard stopped showing runs names.
I can still filter (blindly) and see the curves, but there are no names of the runs to filter.

Any idea on how to fix that?
TF is installed under anaconda.

It worked with no issues until 2 weeks ago. I got back to the project today and this issue occurred.
I tried reinstalling 0.11, or even installing the latest 0.12.1 (under conda-forge), but the issue hasn't solved.

I also tried (with no luck)
```
cd /home/gamir/yuval/anaconda2/envs/tf_011/lib/python2.7/site-packages/tensorflow/tensorboard/
python tensorboard.py --logdir=path/to/log
```

Attached is a screenshot, and below (the technical details) is the TB console output. Note that under TF12.1 there are no errors on the console output, but the issue still happens.

![screenshot from 2017-01-24 17 58 30](https://cloud.githubusercontent.com/assets/5911675/22257868/016e4ac2-e268-11e6-83a9-7e3b33ce5eae.png)


Thanks


NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
https://github.com/tensorflow/tensorflow/issues/1421
https://github.com/tensorflow/tensorflow/issues/5341

### Environment info
Operating System:
Ubuntu

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0rc0-cp27-none-linux_x86_64.whl

2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally                                    │······················
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally                                     │······················
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally                                     │······················
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally                                    │······················
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally                                    │······················
0.11.0rc0 

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).

```

$ tensorboard --port=8008 --logdir=""./""
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally
Starting TensorBoard 29 on port 8008
(You can navigate to http://XXXXXXXXXXXXXX:8008)
127.0.0.1 - - [24/Jan/2017 18:51:19] ""GET / HTTP/1.1"" 200 -
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/gamir/yuval/anaconda2/envs/tf_011/lib/python2.7/site-packages/tensorflow/tensorboard/webcomponentsjs/webcomponents-lite.min.js' on path /home/gamir/yuval/anaconda2/envs/tf_011/lib/python2.7/site-packages/tensorflow/tensorboard/webcomponentsjs/webcomponents-lite.min.js
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/gamir/yuval/anaconda2/envs/tf_011/lib/python2.7/site-packages/tensorflow/tensorboard/dist/bazel-html-imports.html' on path /home/gamir/yuval/anaconda2/envs/tf_011/lib/python2.7/site-packages/tensorflow/tensorboard/dist/bazel-html-imports.html
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/gamir/yuval/anaconda2/envs/tf_011/lib/python2.7/site-packages/external/dist/bazel-html-imports.html' on path /home/gamir/yuval/anaconda2/envs/tf_011/lib/python2.7/site-packages/external/dist/bazel-html-imports.html
127.0.0.1 - - [24/Jan/2017 18:51:19] code 404, message Not Found
127.0.0.1 - - [24/Jan/2017 18:51:19] ""GET /dist/bazel-html-imports.html HTTP/1.1"" 404 -
127.0.0.1 - - [24/Jan/2017 18:51:19] ""GET /dist/tf-tensorboard.html HTTP/1.1"" 200 -
127.0.0.1 - - [24/Jan/2017 18:51:19] ""GET /lib/css/global.css HTTP/1.1"" 200 -
127.0.0.1 - - [24/Jan/2017 18:51:19] ""GET /webcomponentsjs/webcomponents-lite.min.js HTTP/1.1"" 200 -
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/gamir/yuval/anaconda2/envs/tf_011/lib/python2.7/site-packages/tensorflow/tensorboard/polymer/polymer.html' on path /home/gamir/yuval/anaconda2/envs/tf_011/lib/python2.7/site-packages/tensorflow/tensorboard/polymer/polymer.html
127.0.0.1 - - [24/Jan/2017 18:51:19] ""GET /polymer/polymer.html HTTP/1.1"" 200 -
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/gamir/yuval/anaconda2/envs/tf_011/lib/python2.7/site-packages/tensorflow/tensorboard/d3/d3.js' on path /home/gamir/yuval/anaconda2/envs/tf_011/lib/python2.7/site-packages/tensorflow/tensorboard/d3/d3.js
127.0.0.1 - - [24/Jan/2017 18:51:19] ""GET /d3/d3.js HTTP/1.1"" 200 -
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/gamir/yuval/anaconda2/envs/tf_011/lib/python2.7/site-packages/tensorflow/tensorboard/plottable/plottable.js' on path /home/gamir/yuval/anaconda2/envs/tf_011/lib/python2.7/site-packages/tensorflow/tensorboard/plottable/plottable.js
127.0.0.1 - - [24/Jan/2017 18:51:20] ""GET /plottable/plottable.js HTTP/1.1"" 200 -
127.0.0.1 - - [24/Jan/2017 18:51:20] ""GET /data/runs HTTP/1.1"" 200 -
127.0.0.1 - - [24/Jan/2017 18:51:20] ""GET /data/runs HTTP/1.1"" 200 -
WARNING:tensorflow:Found more than one graph event per run. Overwriting the graph with the newest event.
WARNING:tensorflow:Detected out of order event.step likely caused by a TensorFlow restart. Purging expired events from Tensorboard display between the previous step: -1 (timestamp: -1) and current step: 20084 (timestamp: 1482950339.25). Removing 12 scalars, 23 histograms, 23 compressed histograms, 0 images, and 0 audio.

```
"
7038,multiple dequeue ops are optimized away in latest TF,"Multiple ops Dequeue ops from the same queue started getting optimized away in latest TensorFlow:

The following executes dequeue once in Jan17 head, but 3 times in 12.1 
`sess.run([q.dequeue(), q.dequeue(), q.dequeue()])
`

One gets expected behavior (3 dequeues) when graph optimization is turned off

```
tf.ConfigProto(graph_options=tf.GraphOptions(optimizer_options=tf.OptimizerOptions(opt_level=tf.OptimizerOptions.L0)))
sess = tf.Session(config = config)
``` 

Self-contained repro: https://github.com/yaroslavvb/stuff/blob/master/parallel_dequeue_test.py

Came up in http://stackoverflow.com/questions/41830206/how-to-share-a-queue-containing-variable-length-sequences-batches-between-multip"
7036,"tf slim, default initializers for relu","The defaults in layers such as conv and conv_transpose have the default activations as relu and the default initializer as xavier. From the paper https://arxiv.org/pdf/1502.01852v1.pdf, would this be the time to change the default initializer to use tf.contrib.layers.variance_scaling_initializer to account for the variance of the distributions being halved at each layer of activation?

Andrej Karpathy has a lecture which shows the effects of this https://youtu.be/gYpoJMlgyXA?t=47m6s

edit:forgot to add time in the video
"
7035,"Build with --config==cuda just runs fine, with --config== fails over and over","### Environment info
Operating System: Ubuntu 14.04 64 bit

Installed version of CUDA and cuDNN: Cuda7.5, CuDNN 7.5
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
```
spu@TOBCAT:~$ ls -l /usr/local/cuda-7.5/lib/libcud*
-rw-r--r-- 1 root root 189170 Aug  8 16:03 /usr/local/cuda-7.5/lib/libcudadevrt.a
lrwxrwxrwx 1 root root     16 Aug  8 16:03 /usr/local/cuda-7.5/lib/libcudart.so -> libcudart.so.7.5
lrwxrwxrwx 1 root root     19 Aug  8 16:03 /usr/local/cuda-7.5/lib/libcudart.so.7.5 -> libcudart.so.7.5.18
-rwxr-xr-x 1 root root 311596 Aug  8 16:03 /usr/local/cuda-7.5/lib/libcudart.so.7.5.18
-rw-r--r-- 1 root root 558020 Aug  8 16:03 /usr/local/cuda-7.5/lib/libcudart_static.a
```

I tried following the `build from sources` directives from `https://www.tensorflow.org/get_started/os_setup#installing_from_sources` but as soon as I get to the command `bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package` i get the following issues

```
spu@TOBCAT:~/Frameworks/deeplearning/tensorflow$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
INFO: Found 1 target...
ERROR: /home/spu/.cache/bazel/_bazel_spu/d4f375c9af64c0d3baa31a5949dd9b54/external/protobuf/BUILD:113:1: C++ compilation of rule '@protobuf//:protobuf' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 43 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
gcc: error trying to exec 'cc1plus': execvp: No such file or directory
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 3.875s, Critical Path: 1.50s
```

Went googling for possible related issues, but none of the suggestions seem to work out. Using Tensorflow without CUDA seems to kill the reason why I am willing to use Tensorflow in the first place ..."
7034,QueueRunner hanging,"I find the queuerunner is hanging in the code below unless I uncomment the line 

`#threads = tf.train.start_queue_runners(sess=sess, coord=coord)`

This isn't mentioned on the documentation 
[https://www.tensorflow.org/versions/r0.10/how_tos/threading_and_queues/]

```
import time
import tensorflow as tf
import numpy as np
import string

def run_training():
  # Tell TensorFlow that the model will be built into the default Graph.
  with tf.Graph().as_default():
    with tf.name_scope('input'):
      alph_initializer = tf.placeholder(
          dtype=tf.string,
          shape=[26,1])
      input_alph = tf.Variable(
          alph_initializer, trainable=False, collections=[])

      alph = tf.train.slice_input_producer(
          [input_alph], shuffle=False, capacity=1,num_epochs=1)
      alphs = tf.PaddingFIFOQueue(26,
                              tf.string,
                              [[1]])

      alphs_qr = tf.train.QueueRunner(queue=alphs,enqueue_ops=[alphs.enqueue(alph)] )

    my_list_val = np.array(list(string.ascii_lowercase)).reshape(26,1)


    # Create the op for initializing variables.
    init_op = tf.initialize_all_variables()

    # Create a session for running Ops on the Graph.
    sess = tf.Session()

    # the Op to initialize the variables.
    sess.run(init_op)

    sess.run(tf.local_variables_initializer())
    sess.run(tf.global_variables_initializer())
    # Start input enqueue threads.

    coord = tf.train.Coordinator()

    sess.run(input_alph.initializer,
             feed_dict={alph_initializer: my_list_val})
    collection = []
    #If I uncomment the below line then 
    #threads = tf.train.start_queue_runners(sess=sess, coord=coord)
    enqueue_threads = alphs_qr.create_threads(sess, coord=coord, start=True)
    try:
        while not coord.should_stop():
            char =  sess.run(alphs.dequeue())
            collection.append(char[0])
            print(""String val"", char)
    except tf.errors.OutOfRangeError:
        print('here')
    finally:
        coord.request_stop()
    coord.join(enqueue_threads)

    sess.close()


def main(_):
  run_training()


if __name__ == '__main__':
    tf.app.run()
```"
7032,Have tf.sub  and tf.mul been removed from latest TF build ? ,I am running TF 0.12.head and ran into this issue where tf.sub was not found. I am writing this since the [docs](https://www.tensorflow.org/api_docs/python/math_ops/) don't seem to indicate any such thing.
7031,"TypeError: Expected int32, got list containing Tensors of type '_Message' instead.","Hi,
I am using TF ver 0.12.head on Ubuntu 16.04 Python 2.7 and following [this](https://github.com/tensorflow/models/blob/master/slim/slim_walkthough.ipynb) for learning inception model in TF Slim
```

import numpy as np
import os
import tensorflow as tf
import urllib2

from datasets import imagenet
from nets import inception
from preprocessing import inception_preprocessing

slim = tf.contrib.slim

batch_size = 3
image_size = inception.inception_v1.default_image_size
checkpoints_dir = '/tmp/checkpoints/'
with tf.Graph().as_default():
    url = 'https://upload.wikimedia.org/wikipedia/commons/7/70/EnglishCockerSpaniel_simon.jpg'
    image_string = urllib2.urlopen(url).read()
    image = tf.image.decode_jpeg(image_string, channels=3)
    processed_image = inception_preprocessing.preprocess_image(image, image_size, image_size, is_training=False)
    processed_images  = tf.expand_dims(processed_image, 0)
    
    # Create the model, use the default arg scope to configure the batch norm parameters.
    with slim.arg_scope(inception.inception_v1_arg_scope()):
        logits, _ = inception.inception_v1(processed_images, num_classes=1001, is_training=False)
    probabilities = tf.nn.softmax(logits)
    
    init_fn = slim.assign_from_checkpoint_fn(
        os.path.join(checkpoints_dir, 'inception_v1.ckpt'),
        slim.get_model_variables('InceptionV1'))
    
    with tf.Session() as sess:
        init_fn(sess)
        np_image, probabilities = sess.run([image, probabilities])
        probabilities = probabilities[0, 0:]
        sorted_inds = [i[0] for i in sorted(enumerate(-probabilities), key=lambda x:x[1])]
        
    plt.figure()
    plt.imshow(np_image.astype(np.uint8))
    plt.axis('off')
    plt.show()

    names = imagenet.create_readable_names_for_imagenet_labels()
    for i in range(5):
        index = sorted_inds[i]
        print('Probability %0.2f%% => [%s]' % (probabilities[index], names[index]))
```
However I am getting the following issue when running this:
```
    logits, _ = inception.inception_v1(processed_images, num_classes=1001, is_training=False)
  File ""/home/deepankar1994/Desktop/MTP/TensorFlowEx/TFSlim/models/slim/nets/inception_v1.py"", line 290, in inception_v1
    net, end_points = inception_v1_base(inputs, scope=scope)
  File ""/home/deepankar1994/Desktop/MTP/TensorFlowEx/TFSlim/models/slim/nets/inception_v1.py"", line 96, in inception_v1_base
    net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py"", line 1053, in concat
    dtype=dtypes.int32).get_shape(
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 651, in convert_to_tensor
    as_ref=False)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 716, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py"", line 176, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py"", line 165, in constant
    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py"", line 367, in make_tensor_proto
    _AssertCompatible(values, dtype)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py"", line 302, in _AssertCompatible
    (dtype.name, repr(mismatch), type(mismatch).__name__))
TypeError: Expected int32, got list containing Tensors of type '_Message' instead.
```"
7030,Feature Request: an op that returns bytes_in_use for its device,"We are trying to optimize some models to fit into TitanX's 12GB of RAM, and it's hard because of lack of transparency in TF available memory.

What would help this situation is an op that returns amount of bytes on its device when executed. Something similar to what's done in [stack_ops](https://github.com/tensorflow/tensorflow/blob/64edd34ce69b4a8033af5d217cb8894105297d8a/tensorflow/core/kernels/stack_ops.cc#L222) for memory-aware heuristics

```
DeviceContext* device_ctxt = ctx->op_device_context();
auto device = static_cast<tensorflow::Device*>(ctx->device());
Allocator* allocator = device->GetAllocator(alloc_attrs);
AllocatorStats stats;
allocator->GetStats(&stats)
//  output stats.bytes_in_use

```

This op can be wedged between other ops using control dependencies and used for memory debugging. This is complementary to request in https://github.com/tensorflow/tensorflow/issues/6716 because it would account for memory from parallel run calls, variables, persistent tensors.

I can take this issue if this op fits into TF framework"
7029,tensorflow has no attribute 'parallel_stack',"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
7027,Is there any GPU implementation of tf.stack for int64? ,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
7026,It seems that tf.scatter_nd only can run on CPU?,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
7025,"Getting ""Dst tensor is not initialized."" when really the problem is out of GPU memory","This is the stack trace we sometimes get when trying to use TensorFlow on a GPU that's occupied by another process. It would help debugging if the error said something about memory.

@zheng-xq 

tf.version: '0.12.1-1934-g27fca7d-dirty'
(nightly from last week)

```
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:04:00.0
Total memory: 11.90GiB
Free memory: 381.44MiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:04:00.0)
Traceback (most recent call last):
  File ""/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1022, in _do_call
    return fn(*args)
  File ""/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1004, in _run_fn
    status, run_metadata)
  File ""/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 469, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.
	 [[Node: zeros_1266 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [160] values: 0 0 0...>, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""memory_test.py"", line 87, in <module>
    profile_densenet(False)
  File ""memory_test.py"", line 65, in profile_densenet
    sess.run(net.initializer, {net.x_init: trainx[:init_batch_size]})
  File ""/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 767, in run
    run_metadata_ptr)
  File ""/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 965, in _run
    feed_dict_string, options, run_metadata)
  File ""/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1015, in _do_run
    target_list, options, run_metadata)
  File ""/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1035, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.
	 [[Node: zeros_1266 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [160] values: 0 0 0...>, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]

Caused by op 'zeros_1266', defined at:
  File ""memory_test.py"", line 87, in <module>
    profile_densenet(False)
  File ""memory_test.py"", line 59, in profile_densenet
    net = densenet_lib.densenet(init_batch_size, batch_size, layers_per_block, filters_per_layer, save_memory=save_memory)
  File ""/home/yaroslav/openai.git/densenet/densenet.py"", line 183, in densenet
    optimizer = nn.adamax_updates(all_params, loss, lr=tf_lr)
  File ""/home/yaroslav/openai.git/densenet/nn.py"", line 41, in adamax_updates
    mg = tf.Variable(tf.zeros(int_shape(p)), p.name + '_adamax_mg')
  File ""/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py"", line 1376, in zeros
    output = constant(zero, shape=shape, dtype=dtype, name=name)
  File ""/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py"", line 169, in constant
    attrs={""value"": tensor_value, ""dtype"": dtype_value}, name=name).outputs[0]
  File ""/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 2395, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1264, in __init__
    self._traceback = _extract_stack()

InternalError (see above for traceback): Dst tensor is not initialized.
	 [[Node: zeros_1266 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [160] values: 0 0 0...>, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]

```"
7023,ImportError: No module named regularizers,"Any idea how to fix this for Keras on TensorFlow?

```
    from regularizers import EigenvalueRegularizer
ImportError: No module named regularizers
```
"
7020,ImportError: cannot import name control_flow_ops,"Please suggest fixes. I just recently installed tf for Python2.7 on Ubuntu 14.04 and CUDA8. 
```
mona@pascal:~/computer_vision/distracted-drivers-tf$ python main.py 
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so.5.0 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so.8.0 locally
/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  ""This module will be removed in 0.20."", DeprecationWarning)
Traceback (most recent call last):
  File ""main.py"", line 12, in <module>
    from model import Model
  File ""/home/mona/computer_vision/distracted-drivers-tf/model.py"", line 6, in <module>
    from layers import Input
  File ""/home/mona/computer_vision/distracted-drivers-tf/layers.py"", line 4, in <module>
    from tensorflow.python import control_flow_ops
ImportError: cannot import name control_flow_ops

```"
7019,Windows Cmake RelWithDebInfo not working,"Summary:
I'm having an issue getting TensorFlow to build with debug symbols in Windows using Cmake.

About my system:

Windows 10
Version 1607
Build 14393.693

Cmake version 3.7.1

Visual Studio Community 2015

Clone tensor flow repository from https://github.com/tensorflow/tensorflow

git clone https://github.com/tensorflow/tensorflow.git

Change directory to tensorflow\tensorflow\contrib\cmake

cd gitFolder\tensorflow\tensorflow\contrib\cmake


Make build directory, change to directory

Mkdir build

cd build

Use cmake

Cmake .. -A  x64 -DCMAKE_BUILD_TYPE=RelWithDebInfo ^
-DSWIG_EXECUTABLE=C:\tools\swigwin-3.0.10\swig.exe ^
-DPYTHON_EXECUTABLE=C:\Users\ian\Anaconda3\python.exe ^
-DPYTHON_LIBRARIES=C:\Users\ian\Anaconda3\python35.lib ^
-Dtensorflow_ENABLE_GRPC_SUPPORT=OFF

Note:
Swig, python executable directory and python library directory will change based on your system. I tried at with the GRPC support enabled (this is the default) but I got build errors related to GRPC. Because we will be handling remote web requests with our own server application, I didn’t think we needed the GRPC support.

Open the solution in Visual Studio.

Change build mode to RelWithDebInfo x64

Build

Result:

Most projects build, but sadly not the one that I’m interested in, tf_label_image_example. The errors I received are:
Link1120 1 Unresolved externals pywrap_tensorflow
Link1318  Unexpected PDB error; OK(0) tf_tutorials_example_trainer
Link1318  Unexpected PDB error; OK(0) tf_label_image_example
LNK2019	unresolved external symbol ""class tensorflow::Status __cdecl tensorflow::NewServer(class tensorflow::ServerDef const &,class std::unique_ptr<class tensorflow::ServerInterface,struct std::default_delete<class tensorflow::ServerInterface> > *)"" (?NewServer@tensorflow@@YA?AVStatus@1@AEBVServerDef@1@PEAV?$unique_ptr@VServerInterface@tensorflow@@U?$default_delete@VServerInterface@tensorflow@@@std@@@std@@@Z) referenced in function ""void __cdecl PyServer_New(class tensorflow::ServerDef const &,class std::unique_ptr<class tensorflow::ServerInterface,struct std::default_delete<class tensorflow::ServerInterface> > *,struct TF_Status *)"" (?PyServer_New@@YAXAEBVServerDef@tensorflow@@PEAV?$unique_ptr@VServerInterface@tensorflow@@U?$default_delete@VServerInterface@tensorflow@@@std@@@std@@PEAUTF_Status@@@Z)	pywrap_tensorflow	C:\tesorflow\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow.obj	



"
7015,"ptb_rnn_lm.py error: InvalidArgumentError: logits and labels must have the same first dimension, got logits shape [400,10000] and labels shape [420]","I am having the same issue that can be found here:
https://github.com/tensorflow/tensorflow/issues/6469

That issue was closed however. It seems like a bunch of people are still having the problem with the LSTM Penn Tree Bank dataset tutorial online [here](https://www.tensorflow.org/tutorials/recurrent/)

Has anyone found a fix? The exact error I am getting is: 
```
InvalidArgumentError (see above for traceback): logits and labels must have the same first dimension, got logits shape [400,10000] and labels shape [420]
	 [[Node: Train/Model/sequence_loss_by_example/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, _device=""/job:localhost/replica:0/task:0/cpu:0""](Train/Model/add, Train/Model/sequence_loss_by_example/Reshape)]]
```

I have made all of the changes to incorporate the model being a newer version. I have TensorFlow 12.1. Can anyone help?! Thank you all so much."
7014,clip_by_value clips NaN to clip_value_max,"Running 
`sess.run(tf.clip_by_value(float('nan'), 0.0, 100.0))`
returns `100.0`

I'm not sure if this is expected behavior or convenient for clipping gradients, but I believe it should return `nan` (as `np.clip()` does) or be documented.

### Environment info
Windows 7
Only CPU
Python 3.5.2 from Anaconda
Tensorflow 0.12.0-rc1 installed from binary pip package"
7013,ResourceExhaustedError while converting ImageNet dataset to TFRecord,"Determining list of input files and labels from /run/media/root/d9ecddfe-d069-4252-aade-b75fa707b3d2/imagenet-data/raw-data/train.
Traceback (most recent call last):
  File ""/cgpit/models/inception/bazel-bin/inception/build_imagenet_data.runfiles/inception/inception/data/build_imagenet_data.py"", line 704, in <module>
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 30, in run
  File ""/cgpit/models/inception/bazel-bin/inception/build_imagenet_data.runfiles/inception/inception/data/build_imagenet_data.py"", line 700, in main
  File ""/cgpit/models/inception/bazel-bin/inception/build_imagenet_data.runfiles/inception/inception/data/build_imagenet_data.py"", line 597, in _process_dataset
  File ""/cgpit/models/inception/bazel-bin/inception/build_imagenet_data.runfiles/inception/inception/data/build_imagenet_data.py"", line 513, in _find_image_files
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py"", line 251, in get_matching_files
  File ""/usr/lib64/python2.7/contextlib.py"", line 24, in __exit__
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/errors.py"", line 463, in raise_exception_on_not_ok_status
tensorflow.python.framework.errors.ResourceExhaustedError: /run/media/root/d9ecddfe-d069-4252-aade-b75fa707b3d2/imagenet-data/raw-data/train/n01440764

I have tried changing different number of threads,different shard size for both training and validation. But still getting error.  There is enough RAM is free while executing this command. 

Is it bug or it required some configuration changes?

Command used for converting:
bazel-bin/inception/download_and_preprocess_imagenet ""${DATA_DIR}""

where $DATA_DIR= /run/media/root/d9ecddfe-d069-4252-aade-b75fa707b3d2/imagenet-data/
Environment info:
Operating System:
CentOS 7 64bit
32GB RAM
Intel® Xeon(R) CPU E5-2630 v2 @ 2.60GHz × 12 

CPU only version of tensorflow
Installed from source
[root@cgpits inception]# bazel version
Build label: 0.3.1
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Jul 29 09:09:52 2016 (1469783392)
Build timestamp: 1469783392
Build timestamp as int: 1469783392

head for tenserflow

[root@cgpits tensorflow]# git rev-parse HEAD
787ab22d490e79ea8c06511d60d6cddf1b2dd2c2

head version for tesorflow/model
[root@cgpits inception]# git rev-parse HEAD
82c219f24d3a7c5c48a9550aef10bde3a031520d

[root@cgpits inception]# python -c ""import tensorflow; print(tensorflow.__version__)""
0.11.0rc0
"
7011,Is there any method to read HDFS file in a distributed way?,"I'm working on reading hdfs file in between graph mode. 
I used `tf.train.string_input_produce()` to read from hdfs and produce the input.

However, I found that all the workers are reading the whole hdfs file every time fetching a batch.

I'm wondering if it is a known issue or is there any way in TensorFlow that each worker could just read a part of the file like hadoop MapReduce?"
7008,Exported Tensorflow Model not Preserving Placeholder Shape,"I am using exporter from tensorflow.contrib.session_bundle to save out my model:

```
x = tf.placeholder(tf.float32, (None,) + (100, 200) + (1,))
....
saver = tf_saver.Saver(sharded=True)
model_exporter = exporter.Exporter(saver)
model_exporter.init(
    sess.graph.as_graph_def(),
    named_graph_signatures={
        'inputs': exporter.generic_signature({'images': x}),
        'outputs': exporter.generic_signature({'classes': y})})
```
and then I load the model back in (session_bundle from tensorflow.contrib.session_bundle):

```
sess, meta_graph_def = session_bundle.load_session_bundle_from_path(input)
```
However when I inspect the Placeholder tensor corresponding to the input x, I see no shape information:
```
> sess.graph.get_tensor_by_name(input_name)
<tf.Tensor 'Placeholder:0' shape=<unknown> dtype=float32>
```
Is this by design or is there some bug causing the shape to be lost?

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
Report from SO: http://stackoverflow.com/questions/40733752/exported-tensorflow-model-not-preserving-placeholder-shape

### Environment info
Operating System:
Using `gcr.io/tensorflow/tensorflow:latest-devel-gpu` docker iamge

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
```
# ls -al /usr/local/cuda/lib64/libcud*
-rw-r--r-- 1 root root    558720 Sep 14 23:02 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root        16 Sep 14 23:05 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root        19 Sep 14 23:05 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44
-rw-r--r-- 1 root root    415432 Sep 14 23:02 /usr/local/cuda/lib64/libcudart.so.8.0.44
-rw-r--r-- 1 root root    775162 Sep 14 23:02 /usr/local/cuda/lib64/libcudart_static.a
lrwxrwxrwx 1 1000 users       13 Jul 27 05:55 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 1000 users       17 Jul 27 05:55 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5
-rwxrwxr-x 1 1000 users 79337624 Jul 27 05:53 /usr/local/cuda/lib64/libcudnn.so.5.1.5
-rw-rw-r-- 1 1000 users 69756172 Jul 27 05:53 /usr/local/cuda/lib64/libcudnn_static.a
```

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
```
4d924e796368163eff11a8151e8505715345f58d
```
2. The output of `bazel version`
```
# bazel version
INFO: Reading 'startup' options from /root/.bazelrc: --batch
Extracting Bazel installation...
Build label: 0.3.2
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Oct 7 17:25:10 2016 (1475861110)
Build timestamp: 1475861110
Build timestamp as int: 1475861110
```
"
7007,"Native TF, Keras (Backend TF) and Lasagne(Theano) performance comparison","Hey everyone,

I created the exact same network with native tensorflow, keras(tensorflow) and lasagne(theano) but after many hours of testing using number of different parameters, still couldn't figure out why keras and lasagne outperforms the native tensorflow, do better convergences and produce better(slightly but better) results.

The score difference is always like
Keras with Tensorflow: ~0.9830 - 0.9885
Lasagne with Theano: ~0.9833 - 0.9885
Tensorflow Native ~0.9765 - 0.9830

If anyone feels like digging in, my environment is:

> Python 3.5.2 -Anaconda / Windows 10
> CUDA: 8.0 with cuDNN 5.1 
> Keras 1.2.1
> Tensorflow 0.12.1
> Nvidia Geforce GTX 860M

and **keras.json** file:

```
{
    ""image_dim_ordering"": ""tf"", 
    ""epsilon"": 1e-07, 
    ""floatx"": ""float32"", 
    ""backend"": ""tensorflow""
}
```

And you can copy and execute following files in my repo:
[Keras code](https://github.com/emrahyigit/deep/blob/master/keras_cnn_mnist.py)
[Native Tensorflow code](https://github.com/emrahyigit/deep/blob/master/tf_cnn_mnist.py)
[Mnist](https://github.com/emrahyigit/deep/blob/master/mnist.py)

"
7006,can not plot embedded data points using tensorboard embedding chart,"I tried to plot original input data points and embedded data points using tensorboard embedding function.
I did basically everything according to the official instruction.
There are three tensors in my graph, namely input, embedding, and embedded.
and embedded = tf.MatMul(input, embedding)
I failed to plot the embedded tensor, even thought I provide metadata file , tensorboard seems just ignore it.
However I did plot the input tensor, tensorboard has no problem locating metadata file, it shows all the points in 3D with labels I provided in the metadata file.
I also succeeded to plot the embedding tensor, So does anyone know how to solve this?"
7005,TypeError: inputs must be a sequence occure in Bidirectional RNN,"Sorry to bother.
I just use the bidirectional rnn now.
However, I just encounter some problem.
```terminal
Traceback (most recent call last):
  File ""test1.py"", line 19, in <module>
    sequence_length=length(sequence),
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py"", line 530, in bidirectional_rnn
    raise TypeError(""inputs must be a sequence"")
TypeError: inputs must be a sequence
```
I don't know what the problem it is?
The following show my code.
```python
import tensorflow as tf

max_length = 100
frame_size = 64
num_hidden = 200

def length(sequence):
    used = tf.sign(tf.reduce_max(tf.abs(sequence), reduction_indices=2))
    length = tf.reduce_sum(used, reduction_indices=1)
    length = tf.cast(length, tf.int32)
    return length

sequence = tf.placeholder(tf.float32, [None, max_length, frame_size])
output, state = tf.nn.bidirectional_rnn(
    tf.nn.rnn_cell.GRUCell(num_hidden),
    tf.nn.rnn_cell.GRUCell(num_hidden),
    sequence,
    dtype=tf.float32,
    sequence_length=length(sequence),
)
```

The code I refer to the [blog](https://danijar.com/variable-sequence-lengths-in-tensorflow/) that other issue demonstrate and recommond.

---
I try to figure out why the exception was raise.
```python
import tensorflow as tf
import collections

self.input_sentences = tf.placeholder(tf.float32, [None, sequence_length, embedding_size])
print ""six: "", isinstance(self.length, collections.Sequence)
```
This is the part of my code.
However, the result is shown below:
```
six:  False
```
Is somebody encounter such this problem?
or how can I solve this?

I use CPU only, the OS is ubuntu 16.04
the version of tensorflow is 0.12.1"
7004,Build with --define tensorflow_xsmm=1 fails on MacOS Sierra,"Building current master after the recent commits to enable libxsmm.

    bazel build --copt=-march=native -c opt --define tensorflow_xsmm=1 --verbose_failures //tensorflow/tools/pip_package:build_pip_package

The important error lines (I think) are:

> tensorflow/core/kernels/sparse_matmul_op.cc:1408:3: error: unknown type name 'cpu_set_t'
  cpu_set_t old_cpu_set;
  
> tensorflow/core/kernels/sparse_matmul_op.cc:1413:39: error: use of undeclared identifier 'cpu_set_t'
    ret = sched_getaffinity(0, sizeof(cpu_set_t), &old_cpu_set);

The full result of --verbose_failures is attached.
[tf_libxsmm_error.txt](https://github.com/tensorflow/tensorflow/files/721760/tf_libxsmm_error.txt)

Searching for 'cpu_set_t' on OSX turned up [this code example under heading 2.2](https://github.com/Cibiv/NextGenMap/issues/6) and claims that 'cpu_set_t' is not defined on OSX and Apple's own multithreading code needs to be substituted. So maybe the code using 'cpu_set_t' is not meant to be reached on Macs and the libxsmm code is not ready to be used on my machine.





"
7001,All links to summary operations in Convolution NN tutorial are broken,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
http://stackoverflow.com/questions/41782812/how-do-i-find-alternative-methods-in-tensorflow-latest-release-to-deprecated-one/41783019#41783019

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
For example, by following the link at summary_image found under the [Model Inputs](https://www.tensorflow.org/versions/master/tutorials/deep_cnn/#model_inputs) section. You will effectively land at https://www.tensorflow.org/versions/master/api_docs/python/train/ instead of https://www.tensorflow.org/versions/master/api_docs/python/train/#image_summary since image_summary under tf.train module is now deprecated according to [Release 0.12.0 release notes](https://github.com/tensorflow/tensorflow/releases/tag/0.12.0-rc0)

### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
7000,"hello there, i am trying to instal tensorflow on mac. Even though the installation is successful when I import tensor flow gives me this error:","ImportError: Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
ImportError: dlopen(/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so, 10): Symbol not found: ___sincos_stret
  Referenced from: /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
  Expected in: /usr/lib/libSystem.B.dylib
 in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so

Error importing tensorflow.  Unless you are using bazel,
you should not try to import tensorflow from its source directory;
please exit the tensorflow source tree, and relaunch your python interpreter
from there.

any idea??
"
6999,Please update Windows TF to support python 3.6,"The current .whl does not support python 3.6.  Please update to support the latest version of python.  Thanks!

C:\WINDOWS\system32>python --version
Python 3.6.0

C:\WINDOWS\system32>pip install --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-0.12.1-cp35-cp35m-win_amd64.whl
tensorflow-0.12.1-cp35-cp35m-win_amd64.whl is not a supported wheel on this platform.

"
6998,Back-propagating gradients through a sparse tensor?,"I have a normal feed-forward network that produces a vector v. The elements of v are then used as the non-zero entries of a sparse matrix M (assume the coordinates are predefined). The sparse matrix is then multiplied by a dense vector and a loss is defined on the resulting scalar. I want to back-propagate the loss w.r.t. the weights of the network, which entails going through the sparse matrix. 

This seems like a perfectly reasonable use-case for a sparse matrix, but it appears that such functionality is not supported. Indeed, even calling tf.gradients(M,[v]) produces an error:

> AttributeError: 'SparseTensor' object has no attribute 'value_index'

Am I doing something wrong or am I correct in presuming that this functionality doesn't (yet?) exist? If the latter, then is there a work-around for this particular use-case short of rewriting all of the sparse tensor operations with gradients defined? "
6996,Documentation for seq2seq,"Hello

I want to use the Tensorflow seq2seq library (I'm doing the Udacity Deep Learning course, Assignment 6).  I can't find anything that documents what functions are available, and the details of all the arguments that can be used for each function (ie what is documented for the Tensorflow API).

The library is referenced in the Sequence to Sequence Models tutorial - a tantalising amount of detail is given, but not enough to work with.

I've googled the subject and searched in stack-overflow - a few posts reference the library but they don't give documentation.

I found this:
https://github.com/tensorflow/tensorflow/blob/63409bd23facad471973b110df998782c0e19c06/tensorflow/models/rnn/translate/translate.py#L132

 - which, I think, gives the code but this is a lower level than I was hoping for!

I also found the link below on Tensorflow site - maybe the most promising - but none of the links work:

https://www.tensorflow.org/versions/r1.0/api_docs/python/contrib.legacy_seq2seq/#sequence_loss

Can anyone point me in the right direction?  Apologies if I've come to the wrong place (I've read the Guidelines) - the Support options on the Tensorflow site linked to Github or Stack overflow.
Thanks
Kathryn

"
6994,A tutorial for new dynamic seq2seq,"New `contrib.seq2seq` made it into master - exciting development! However, the only examples are in the tests. For my project, I've implemented a small but complete example against a toy copy task. Here it is: [notebook](https://github.com/ematvey/tensorflow-seq2seq-tutorials/blob/master/3-seq2seq-native-new.ipynb), [code](https://github.com/ematvey/tensorflow-seq2seq-tutorials/blob/master/model_new.py).

Can I contribute it? I do realise that API is likely not yet complete. I’ll be happy to keep it updated and improve it if needed."
6993,tensorflow install-gpu error on window 10 ,"I installed the tensorflow on window. 
and 
pip install tensorflow is successfully imported 
but pip install tensorflow-gpu brought some issues when they imported 
here is the issues.
=================================================
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:119] Couldn't open CUDA library cublas64_80.dll
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\cuda\cuda_blas.cc:2294] Unable to load cuBLAS DSO.
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:119] Couldn't open CUDA library cudnn64_5.dll
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\cuda\cuda_dnn.cc:3459] Unable to load cuDNN DSO
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:119] Couldn't open CUDA library cufft64_80.dll
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\cuda\cuda_fft.cc:344] Unable to load cuFFT DSO.
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:119] Couldn't open CUDA library nvcuda.dll
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\cuda\cuda_diagnostics.cc:165] hostname: ???
Traceback (most recent call last):
  File ""C:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Program Files\Anaconda3\envs\tensorflow\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: DLL 초기화 루틴을 실행할 수 없습니다.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\__init__.py"", line 54, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 21, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow')
  File ""C:\Program Files\Anaconda3\envs\tensorflow\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\__init__.py"", line 60, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Program Files\Anaconda3\envs\tensorflow\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: DLL 초기화 루틴을 실행할 수 없습니다.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\__init__.py"", line 54, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 21, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow')
  File ""C:\Program Files\Anaconda3\envs\tensorflow\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow'


Error importing tensorflow.  Unless you are using bazel,
you should not try to import tensorflow from its source directory;
please exit the tensorflow source tree, and relaunch your python interpreter
from there.
==================================================
any Idea?"
6992,Feature request: LU Decomposition,"I suggest adding LU decomposition with partial pivoting to Tensorflow. The implementation must support rectangular matrices as well, and not only square matrices. Tensorflow already uses LU decomposition for solving linear systems of equations.

LU decomposition can be used as an efficient algorithm for finding the range of a matrix and its low rank approximation. The advantages over QR and SVD are that LU is more computationally efficient and is very suited for GPUs giving a significant speed boost to many computational tasks.

See the following papers:

1) Li, H., Linderman, G. C., Szlam, A., Stanton, K. P., Kluger, Y., & Tygert, M. (2017). Algorithm 971: An Implementation of a Randomized Algorithm for Principal Component Analysis. ACM Transactions on Mathematical Software (TOMS), 43(3), 28
2) Shabat, G., Shmueli, Y., Aizenbud, Y., & Averbuch, A. (2016). Randomized LU decomposition. Applied and Computational Harmonic Analysis.
3) Li, H., Kluger, Y., & Tygert, M. (2016). Randomized algorithms for distributed computation of principal component analysis and singular value decomposition. arXiv preprint arXiv:1612.08709.

"
6985,API changes: Missing RNN Module (continued),"(This is a continuation of issue #6984, which was closed before I could respond.)
Thank you.  But the question is still unanswered.  
Given that `tf.contrib.rnn` no longer exists.  What is the replacement for it?  I've read the docs, but haven't found an answer.  
In other words, what method is used to fuse together multiple RNN cells?
I've tried the following:
```
lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(n_hidden)
outputs, states = tf.contrib.rnn.MultiRNNCell(lstm_cell_1, Xnew)
```
But get the following error message:
```
TypeError: 'MultiRNNCell' object is not iterable
```
So, What method is used to fuse together multiple RNN cells?"
6984,API changes: Missing RNN module,"This is a continuation #6973.  Given the ongoing API changes with r0.12 and the shortage of documentation/samples, it seems that API questions must be posed here.  
We had previously working code as follows:
```
lstm_cell_1 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)
outputs, states = tf.nn.rnn(lstm_cell_1, Xnew, dtype=tf.float32)
```
The new code should be something like:
```
lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(n_hidden)
outputs, states = tf.contrib.rnn(lstm_cell_1, Xnew, dtype=tf.float32)
```
But `tf.contrib.rnn` no longer exists.  I've tried `tf.contrib.rnn.MultiRNNCell` and `tf.contrib.rnn.fused_rnn_cell` but am told that `TypeError: 'module' object is not callable`.
What is the new method to fuse together Basic LSTM cells?"
6983,graph_metric.py removed?,"Hello, 

Is graph_metric.py removed from TF? This is a useful feature for me. Can we add it back?

https://github.com/tensorflow/tensorflow/blob/fe454464681b036ff7fed3e42c6bb541fa52dd7c/tensorflow/python/tools/graph_metrics.py

"
6982,tensorflow MNIST getting error IOError: Not a gzipped file,"When I try to run the basic tutorial example I get this as a return:

Traceback (most recent call last):
  File ""TensorFlowBasicTutorial"", line 76, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 43, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""TensorFlowBasicTutorial"", line 23, in main
    mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py"", line 213, in read_data_sets
    train_images = extract_images(f)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py"", line 53, in extract_images
    magic = _read32(bytestream)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py"", line 35, in _read32
    return numpy.frombuffer(bytestream.read(4), dtype=dt)[0]
  File ""/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/gzip.py"", line 268, in read
    self._read(readsize)
  File ""/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/gzip.py"", line 303, in _read
    self._read_gzip_header()
  File ""/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/gzip.py"", line 197, in _read_gzip_header
    raise IOError, 'Not a gzipped file'
IOError: Not a gzipped file


I uninstalled tensorflow and reinstalled tensor flow with no results. 
I have seen some information about installing the file locally, but then how do import the data? I am rather new to python so I do not know how to do this.

Thanks!"
6981,AttributeError: 'module' object has no attribute 'inv',"Hi there,

I have an issue when I am trying to inverse a tensor as shown below

>>> import tensorflow as tf
>>> a = tf.placeholder(tf.float32)
>>> tf.inv(a)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: 'module' object has no attribute 'inv'

Operation System: mac os x 10.11.6
Python 2.7.13
I installed tensor flow using pip install tensorflow 
>>> tf.__version__
'0.12.1'

If someone could help out, that would be cool! Thanks!"
6980,Images are missing (unreachable) from the documentation page,"In the documentation for tfdbg: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/debugger/index.md

there are missing images
"
6979,Feature Request: extend platform.flags (~ python-gflags),"Hello everyone,

I would like to suggest to incorporate more flags into the platform.flags package.

In particular:
DEFINE_multi
DEFINE_multi_float
DEFINE_multi_int

Thank you,
Philip"
6978,OpenCL Support for TensorFlow -- When is official support coming for macOS?,When will an official build of TensorFlow for the latest release be available for use with OpenCL-enabled graphics cards?  Should I fork a version and then hope to commit?  Is this something the core team is interested in because I have a NVIDIA-enabled iMac that is >3 years old and I cannot accelerate my computation for different projects.  What is possible within the next year?  
6977,iPython Tensorflow import error,"I am trying to install Tensorflow GPU on a different environment than my original anaconda root. So I followed the instruction on the website, installed CUDA 8, cuDNN 5.1, and tensorflow on a new Tensorflow environment. 

When I execute the import tensorflow on Spyder - iPython I get his error:

![tsfl4](https://cloud.githubusercontent.com/assets/21122937/22152365/7e7ebde0-df1a-11e6-8392-4a9c974e3f4b.jpeg)

But when I try to import it from Python then it works fine:

![tsfl 5](https://cloud.githubusercontent.com/assets/21122937/22152403/b7f02208-df1a-11e6-950e-0538306440de.jpeg)

PS: The same happens when I tried to install the simple tensorflow on my original root environment, works on python but not on ipython. 

Any suggestions please? Thank you."
6976,Unexpected behavior in tensorflow's distributed training,"Hi Tensorflowers,

I was doing some distributed training experiments on tensorflow v0.11.0.
I modified the ResNet code from the official model zoos [here](https://github.com/tensorflow/models/tree/master/resnet) to be the one can do distributed training.
In the experiments, ResNet56 is used for training on CIFAR10 data set and the settings follow the original [paper](https://arxiv.org/abs/1512.03385).

When I set the number of parameter server = 1, worker = 1, I expected the behavior would be the same as the single gpu one (which is the original code in the [repo](https://github.com/tensorflow/models/tree/master/resnet)).
It turns out that there is a huge performance gap. Please see the following figure.
![resnet_cifar10_1ps1worker_tf](https://cloud.githubusercontent.com/assets/1206058/22150624/fd6332d2-df54-11e6-8091-7aeeb4770c5b.png)
**x-axis**: time in second, **y-axis**: testing error
**tf-0**: single gpu version, **tf-1**: distributed version with # ps=1, # worker=1
Both code were run by 160 epochs and with the same parameter/learning rate schedule.

The single gpu version can achieve 7% error rate (which is consistent with the original paper), but the distributed one is stalled at 12% error.
I think there might be something wrong as the performance should be similar for both cases.
Could you check if that is the case? (or maybe I used the wrong way to do distributed training)
Please feel free to ask if you have any problem with the setting. Thanks.

The single gpu version code can be found [here](https://github.com/tensorflow/models/blob/d93ffd0b69253ee3c3a40a19a4c86ac6975bd570/resnet/resnet_main.py) (I used the earlier 0.11 compatible version)
The code for distributed version can be found [here](https://gist.github.com/infwinston/8a7c86a75d3177bac4737903cc4c4fe3).

Some detailed settings:
```
batch size=128, num_residual_units=9, relu_leakiness=0
```

The command I launched:
```
ps0
> export CUDA_VISIBLE_DEVICES=""""; python resnet_dist.py --ps_hosts=""localhost:50000"" --worker_hosts=""localhost:50001"" --job_name=""ps"" --task_id=""0"" --batch_size=128 --dataset='cifar10' --train_data_path=cifar10/data_batch* --log_root=./tmp-log-root/ --num_gpus=1 --mode train
worker0
> export CUDA_VISIBLE_DEVICES=""0""; python resnet_dist.py --ps_hosts=""localhost:50000"" --worker_hosts=""localhost:50001"" --job_name=""worker"" --task_id=""0"" --batch_size=128 --dataset='cifar10' --train_data_path=cifar10/data_batch* --log_root=./tmp-log-root/ --num_gpus=1 --mode train
```

### Environment info
Operating System:
Ubuntu 14.04

Installed version of CUDA and cuDNN:
CUDA 7.5, cuDNN 5.1
```
> ls -l /usr/local/cuda/lib/libcud*
-rw-r--r-- 1 root root 189170 Oct 25 22:51 /usr/local/cuda/lib/libcudadevrt.a
lrwxrwxrwx 1 root root     16 Oct 25 22:51 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5
lrwxrwxrwx 1 root root     19 Oct 25 22:51 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18
-rwxr-xr-x 1 root root 311596 Oct 25 22:51 /usr/local/cuda/lib/libcudart.so.7.5.18
-rw-r--r-- 1 root root 558020 Oct 25 22:51 /usr/local/cuda/lib/libcudart_static.a
```

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
282823b877f173e6a33bbc9d4b9ad7dd8413ada6
2. The output of `bazel version`
```
Build label: 0.4.2
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Dec 7 18:47:11 2016 (1481136431)
Build timestamp: 1481136431
Build timestamp as int: 1481136431
```
I built tensorflow v0.11.0 by myself.
"
6974,Race condition in EventFileWriter,"There is a race condition here in `EventFileWriter`: [here](https://github.com/tensorflow/tensorflow/blob/287db3a9b0701021f302e7bb58af5cf89fdcd424/tensorflow/python/summary/writer/event_file_writer.py#L68)

```python
    if not gfile.IsDirectory(self._logdir):
      gfile.MakeDirs(self._logdir)
```

It is possible for multiple concurrent threads and/or processes to get to this code simultaneously, both check `IsDirectory`, return `False`, then both try creating the directory, and have one succeed and one error.

This can be fixed by by catching the `AlreadyExistsError` from `MakeDirs` and handling it gracefully.

(this is a real issue that we have encountered in daily usage and causes us issues, not just a hypothetical)"
6973,ImportError: No module named nn.rnn,"I'm having trouble accessing the rnn models.  The following commands:
```
import tensorflow as tf
from tensorflow.nn.rnn import rnn, rnn_cell
```
yield the following error:
```
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-4-f6712a17b199> in <module>()
----> 1 from tensorflow.nn.rnn import rnn, rnn_cell

ImportError: No module named nn.rnn
```
And the line:
```
lstm_cell_1 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)
```
yields:
```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-5-f0f106c78703> in <module>()
----> 1 lstm_cell_1 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)

AttributeError: 'module' object has no attribute 'rnn_cell'

```
The version is  `'0.12.head'` and I'm working on MacOS 10.12.2.  I'm working off git hash #7c16e2e23.
I have been following the commentary in [#2685](https://github.com/tensorflow/tensorflow/issues/2685) and others.
First, I installed setuptools:
```
pip install --upgrade -I setuptools
```
and then the latest python:
```
pip install --ignore-installed --upgrade tensorflow-0.12.1-py2-none-any.whl 
```
Is this a real issue, or a problem with my setup?"
6971,Invalid argument: Cannot parse tensor from proto:  dtype: DT_FLOAT,"@lukaszkaiser I am facing the same problem. I have same specification as the previous one. My code is
`Final_dataset.csv` contains 456 X 147457

    `train = pd.read_csv('Final_dataset.csv')
     x = tf.placeholder(tf.float32, shape=[None, 128*128*3])
     y_ = tf.placeholder(tf.float32, shape=[None, 128*256*3])
     W = tf.Variable(tf.zeros([128*128*3,128*256*3]))
     b = tf.Variable(tf.zeros([128*256*3]))`

Whenever I run below command
     `sess.run(tf.global_variables_initializer())`

I am getting this error

    `W tensorflow/core/framework/op_kernel.cc:965] Invalid argument: Cannot parse tensor from proto:    dtype: DT_FLOAT
      tensor_shape {
          dim {
             size: 49152
          }
          dim {
             size: 98304
          }
      }
      float_val: 0

      E tensorflow/core/common_runtime/executor.cc:390] Executor failed to create kernel. Invalid argument: Cannot parse tensor from proto: dtype: DT_FLOAT
      tensor_shape {
         dim {
            size: 49152
         }
        dim {
            size: 98304
        }
     }
    float_val: 0

	 [[Node: zeros_2 = Const[dtype=DT_FLOAT, value=<Invalid TensorProto: dtype: DT_FLOAT tensor_shape { dim { size: 49152 } dim { size: 98304 } } float_val: 0>, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
       Traceback (most recent call last):
        File ""<stdin>"", line 1, in <module>
        File ""/home/lokesh/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 766, in run
    run_metadata_ptr)
         File ""/home/lokesh/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 964, in _run
    feed_dict_string, options, run_metadata)
        File ""/home/lokesh/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1014, in _do_run
    target_list, options, run_metadata)
       File ""/home/lokesh/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1034, in _do_call
    raise type(e)(node_def, op, message)
     tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot parse tensor from proto: dtype: DT_FLOAT
     tensor_shape {
       dim {
          size: 49152
        }
        dim {
            size: 98304
        }
     }
      float_val: 0

	 [[Node: zeros_2 = Const[dtype=DT_FLOAT, value=<Invalid TensorProto: dtype: DT_FLOAT tensor_shape { dim { size: 49152 } dim { size: 98304 } } float_val: 0>, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

    Caused by op u'zeros_2', defined at:
    File ""<stdin>"", line 1, in <module>
    File ""/home/lokesh/.local/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py"", line 1437, in zeros
    output = constant(zero, shape=shape, dtype=dtype, name=name)
     File ""/home/lokesh/.local/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.py"", line 169, in constant
    attrs={""value"": tensor_value, ""dtype"": dtype_value}, name=name).outputs[0]
     File ""/home/lokesh/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2240, in create_op
    original_op=self._default_original_op, op_def=op_def)
     File ""/home/lokesh/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1128, in __init__
    self._traceback = _extract_stack()

    InvalidArgumentError (see above for traceback): Cannot parse tensor from proto: dtype: DT_FLOAT
    tensor_shape {
     dim {
        size: 49152
     }
     dim {
       size: 98304
     }
     }
     float_val: 0

	 [[Node: zeros_2 = Const[dtype=DT_FLOAT, value=<Invalid TensorProto: dtype: DT_FLOAT tensor_shape { dim { size: 49152 } dim { size: 98304 } } float_val: 0>, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
`"
6970,RNN weights aren't being restored on saver.restore calls,"### Environment info
##Operating System:##
Ubuntu 16.04
##Installed version of CUDA and cuDNN: ##
Cuda 8.0,  CuDNN 5.1 
##tensorflow version##
0.12

### What other attempted solutions have you tried?
Not create the graph before using the model. 

### Logs or other output that would be helpful
[SO question](http://stackoverflow.com/questions/41737918/word-embeddings-change-between-sessions-even-if-saved)"
6969,Go: Unable to import bool tensors in Go,"Hi Tensorflow/Go team,

Following python code writes a graph with a bool tensor ""z"", which fails to import from a Go code.
```

import tensorflow as tf
z = tf.equal(0, 0, name=""z"")
sess = tf.Session()
print sess.run(z)
tf.train.write_graph(sess.graph_def, ""/tmp/load"", ""boolTest.pb"", False)
```

And here is the Go code
```

package bugs

import (
	""fmt""
	tf ""github.com/tensorflow/tensorflow/tensorflow/go""
	""io/ioutil""
	""testing""
)

func TestBoolTest(t *testing.T) {
	model, err := ioutil.ReadFile(""/tmp/load/boolTest.pb"")
	if err != nil {
		t.Fatal(err)
	}
	graph := tf.NewGraph()
	if err := graph.Import(model, """"); err != nil {
		t.Fatal(err)
	}

	z := graph.Operation(""z"").Output(0)
	sess, err := tf.NewSession(graph, nil)
	if err != nil {
		t.Fatal(err)
	}
	defer sess.Close()

	zOut, err := sess.Run(nil, []tf.Output{z}, nil)
	if err != nil {
		t.Fatal(err)
	}

	fmt.Println(zOut[0].Value().(bool))
}

```
Returns error:

=== RUN   TestBoolTest
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
--- FAIL: TestBoolTest (0.04s)
panic: BUG: Please report at https://github.com/tensorflow/tensorflow/issues with the note: Go TensorFlow 0.12.head: unable to decode Tensor of type 10 and shape [] - unsupported type bool [recovered]
        panic: BUG: Please report at https://github.com/tensorflow/tensorflow/issues with the note: Go TensorFlow 0.12.head: unable to decode Tensor of type 10 and shape [] - unsupported type bool

goroutine 5 [running]:
panic(0x5087c0, 0xc420012720)
        /usr/lib/golang/src/runtime/panic.go:500 +0x1a1
testing.tRunner.func1(0xc4200b8180)
        /usr/lib/golang/src/testing/testing.go:579 +0x25d
panic(0x5087c0, 0xc420012720)
        /usr/lib/golang/src/runtime/panic.go:458 +0x243
github.com/tensorflow/tensorflow/tensorflow/go.(*Tensor).Value(0xc420010300, 0x0, 0xc420065f38)
        /home/sdeoras/go/src/github.com/tensorflow/tensorflow/tensorflow/go/tensor.go:176 +0x411
bitbucket.hgst.com/x/tensorflow.git/bugs.TestBoolTest(0xc4200b8180)
        /home/sdeoras/go/src/bitbucket.hgst.com/x/tensorflow.git/bugs/boolTest_test.go:32 +0x25c
testing.tRunner(0xc4200b8180, 0x53b728)
        /usr/lib/golang/src/testing/testing.go:610 +0x81
created by testing.(*T).Run
        /usr/lib/golang/src/testing/testing.go:646 +0x2ec
exit status 2
FAIL    bitbucket.hgst.com/x/tensorflow.git/bugs        0.143s
"
6968,Error in `python': double free or corruption (!prev),"I'm consistently getting this error when stopping training (CTRL+C) on version built from head on Jan17. On other hand, running on version from Jan5 head does not exhibit this behavior

tf.__git__version = '0.12.1-1934-g27fca7d-dirty'

```
session.run completed in 0.01 sec with .0.500000 acc
session.run completed in 0.02 sec with .0.000000 acc
^CTraceback (most recent call last):
  File ""train.py"", line 247, in <module>
    a,_ = sess.run([train_acc,optimizer], feed_dict)
  File ""/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 767, in run
    run_metadata_ptr)
  File ""/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 965, in _run
    feed_dict_string, options, run_metadata)
  File ""/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1015, in _do_run
    target_list, options, run_metadata)
  File ""/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1022, in _do_call
    return fn(*args)
  File ""/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1004, in _run_fn
    status, run_metadata)
KeyboardInterrupt
*** Error in `python': double free or corruption (!prev): 0x00000000016c55d0 ***
Aborted (core dumped)
```

Looking at core, it looks like dictionary deletion.

```
#0  0x00007fe9cbf8a01f in _int_free (av=0x7fe9cc2c9760 <main_arena>, p=<optimized out>, have_lock=0) at malloc.c:3996
#1  0x00007fe9cceb500a in dict_dealloc (mp=0x7fe9558073c8) at Objects/dictobject.c:1596
#2  0x00007fe9cced121f in subtype_dealloc (self=0x7fe95580a080) at Objects/typeobject.c:1193
#3  0x00007fe9cceb023f in free_keys_object (keys=0x24f9620) at Objects/dictobject.c:354
#4  0x00007fe9cced3936 in type_clear (type=0x24f9c68) at Objects/typeobject.c:3270
#5  0x00007fe9ccf8a97c in delete_garbage (old=<optimized out>, collectable=<optimized out>) at Modules/gcmodule.c:866
#6  collect (generation=2, n_collected=0x0, n_uncollectable=0x0, nofail=1) at Modules/gcmodule.c:1014
#7  0x00007fe9ccf8aedd in _PyGC_CollectNoFail () at Modules/gcmodule.c:1605
#8  0x00007fe9ccf5e6d5 in PyImport_Cleanup () at Python/import.c:428
#9  0x00007fe9ccf6a90e in Py_Finalize () at Python/pylifecycle.c:576
#10 0x00007fe9ccf891b9 in Py_Main (argc=<optimized out>, argv=<optimized out>) at Modules/main.c:789
#11 0x0000000000400add in main (argc=2, argv=0x7ffde1cf3f98) at ./Programs/python.c:65
```
"
6967,"Build from sources, build issues","Hi, everyone.
I'm trying to build tensorflow from sources as https://www.tensorflow.org/get_started/os_setup#installing_from_sources illustrated.
however, when I came to the step bazel build -c opt //tensorflow/tools/pip_package:build_pip_package

it followed with ""[2,302 / 3,143] Still waiting for 200 jobs to complete: "" with more than 20 hours.
how long does it need to finish the job.
thanks.

Operating System:
CentOS6
no GPU
The commit hash: 49bab39b2a1be64879ba95515ab6130b7b951be0
bazel version: Build label: 0.4.3

Logs
```
[2,302 / 3,143] Still waiting for 200 jobs to complete:
      Running (standalone):
        Compiling tensorflow/core/kernels/svd_op_complex64.cc, 77639 s
        Compiling tensorflow/core/kernels/svd_op_complex128.cc, 77060 s
        Compiling tensorflow/core/kernels/svd_op_double.cc, 25386 s
      Scheduling:
        Linking tensorflow/python/libpython_op_gen_main.a [for host], 80391 s
        Linking tensorflow/contrib/cudnn_rnn/libcudnn_rnn_ops_op_lib.lo [for host], 80308 s
        Linking tensorflow/core/libnn_ops_op_lib.lo [for host], 80308 s
        Linking tensorflow/contrib/layers/libbucketization_op_op_lib.lo [for host], 80308 s
        Linking tensorflow/contrib/tensor_forest/libtensor_forest_ops_op_lib.lo [for host], 80308 s
        Linking tensorflow/contrib/layers/libsparse_feature_cross_op_op_lib.lo [for host], 80307 s
        Linking tensorflow/core/libresource_variable_ops_op_lib.lo [for host], 80306 s
        Linking tensorflow/core/libcontrol_flow_ops_op_lib.lo [for host], 80287 s
        Linking tensorflow/core/libuser_ops_op_lib.lo [for host], 80287 s
        ... 188 more jobs


Information with ""top""
top - 09:34:57 up 1 day,  4:49,  4 users,  load average: 5.17, 6.59, 6.21
Tasks: 219 total,   1 running, 217 sleeping,   0 stopped,   1 zombie
Cpu(s):  0.2%us,  0.4%sy,  0.0%ni, 50.1%id, 49.3%wa,  0.0%hi,  0.0%si,  0.0%st
Mem:   1004140k total,   939776k used,    64364k free,      576k buffers
Swap:  4046844k total,  2836996k used,  1209848k free,    27784k cached

   PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND                                                                                                                                                                      
 29135 root      20   0  694m  11m 1884 D  1.0  1.2   0:04.05 yumBackend.py                                                                                                                                                                 
 26945 root      20   0  898m 149m 1636 D  0.7 15.3   4:19.76 cc1plus                                                                                                                                                                       
 28266 root      20   0  768m 127m 1540 D  0.7 13.0   1:38.66 cc1plus                                                                                                                                                                       
    20 root      20   0     0    0    0 S  0.3  0.0   0:37.56 events/1                                                                                                                                                                      
    48 root      20   0     0    0    0 S  0.3  0.0   1:53.14 kblockd/2                                                                                                                                                                     
    57 root      20   0     0    0    0 S  0.3  0.0   0:05.53 ata_sff/3                                                                                                                                                                     
    73 root      20   0     0    0    0 S  0.3  0.0   3:46.86 kswapd0                                                                                                                                                                       
 26939 root      20   0  922m 150m 1940 D  0.3 15.4   4:26.14 cc1plus                                                                                                                                                                       
 29203 root      20   0 15148 1348  964 R  0.3  0.1   0:00.02 top          
```


English poor, thanks."
6962,inception_v4 checkpoint has no convolution biases,"I've been trying to export the trained model weights and biases from inception_v4 to be used in a Keras implementation of the model. However, it does not look like any of the convolution layers have biases when they (I'm assuming) should. Is there some other way to get the biases other than from the ckpt file?

**Other model checkpoints DO have biases for the convolution layers though such as vgg_16:**

```
I tensorflow/stream_executor/dso_loader.cc:120] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:120] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:120] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:120] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:120] successfully opened CUDA library libcurand.so.8.0 locally
('tensor_name: ', 'global_step')
('tensor_name: ', 'vgg_16/conv1/conv1_1/biases')
('tensor_name: ', 'vgg_16/conv1/conv1_1/weights')
('tensor_name: ', 'vgg_16/conv1/conv1_2/biases')
('tensor_name: ', 'vgg_16/conv1/conv1_2/weights')
('tensor_name: ', 'vgg_16/conv2/conv2_1/biases')
('tensor_name: ', 'vgg_16/conv2/conv2_1/weights')
('tensor_name: ', 'vgg_16/conv2/conv2_2/biases')
('tensor_name: ', 'vgg_16/conv2/conv2_2/weights')
('tensor_name: ', 'vgg_16/conv3/conv3_1/biases')
('tensor_name: ', 'vgg_16/conv3/conv3_1/weights')
('tensor_name: ', 'vgg_16/conv3/conv3_2/biases')
('tensor_name: ', 'vgg_16/conv3/conv3_2/weights')
('tensor_name: ', 'vgg_16/conv3/conv3_3/biases')
('tensor_name: ', 'vgg_16/conv3/conv3_3/weights')
('tensor_name: ', 'vgg_16/conv4/conv4_1/biases')
('tensor_name: ', 'vgg_16/conv4/conv4_1/weights')
('tensor_name: ', 'vgg_16/conv4/conv4_2/biases')
('tensor_name: ', 'vgg_16/conv4/conv4_2/weights')
('tensor_name: ', 'vgg_16/conv4/conv4_3/biases')
('tensor_name: ', 'vgg_16/conv4/conv4_3/weights')
('tensor_name: ', 'vgg_16/conv5/conv5_1/biases')
('tensor_name: ', 'vgg_16/conv5/conv5_1/weights')
('tensor_name: ', 'vgg_16/conv5/conv5_2/biases')
('tensor_name: ', 'vgg_16/conv5/conv5_2/weights')
('tensor_name: ', 'vgg_16/conv5/conv5_3/biases')
('tensor_name: ', 'vgg_16/conv5/conv5_3/weights')
('tensor_name: ', 'vgg_16/fc6/biases')
('tensor_name: ', 'vgg_16/fc6/weights')
('tensor_name: ', 'vgg_16/fc7/biases')
('tensor_name: ', 'vgg_16/fc7/weights')
('tensor_name: ', 'vgg_16/fc8/biases')
('tensor_name: ', 'vgg_16/fc8/weights')
('tensor_name: ', 'vgg_16/mean_rgb')
```

# Using a minimal version of the inspect checkpoint tool:

```
import tensorflow as tf

def print_layers(file_name):
	try:
		reader = tf.train.NewCheckpointReader(file_name)
		var_to_shape_map = reader.get_variable_to_shape_map()
		var_to_shape_map = sorted(var_to_shape_map)
		for key in var_to_shape_map:
			print(""tensor_name: "", key)
	except Exception as e:
		print(str(e))
		if ""corrupted compressed block contents"" in str(e):
			print(""It's likely that your checkpoint file has been compressed ""
					""with SNAPPY."")

if __name__ == ""__main__"":
	print_layers(""./inception_v4.ckpt"")
```

**The output is as follows (notice that only Logits and AuxLogits have biases):**

```
I tensorflow/stream_executor/dso_loader.cc:120] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:120] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:120] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:120] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:120] successfully opened CUDA library libcurand.so.8.0 locally
('tensor_name: ', 'InceptionV4/AuxLogits/Aux_logits/biases')
('tensor_name: ', 'InceptionV4/AuxLogits/Aux_logits/weights')
('tensor_name: ', 'InceptionV4/AuxLogits/Conv2d_1b_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/AuxLogits/Conv2d_1b_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/AuxLogits/Conv2d_1b_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/AuxLogits/Conv2d_1b_1x1/weights')
('tensor_name: ', 'InceptionV4/AuxLogits/Conv2d_2a/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/AuxLogits/Conv2d_2a/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/AuxLogits/Conv2d_2a/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/AuxLogits/Conv2d_2a/weights')
('tensor_name: ', 'InceptionV4/Conv2d_1a_3x3/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Conv2d_1a_3x3/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Conv2d_1a_3x3/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Conv2d_1a_3x3/weights')
('tensor_name: ', 'InceptionV4/Conv2d_2a_3x3/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Conv2d_2a_3x3/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Conv2d_2a_3x3/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Conv2d_2a_3x3/weights')
('tensor_name: ', 'InceptionV4/Conv2d_2b_3x3/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Conv2d_2b_3x3/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Conv2d_2b_3x3/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Conv2d_2b_3x3/weights')
('tensor_name: ', 'InceptionV4/Logits/Logits/biases')
('tensor_name: ', 'InceptionV4/Logits/Logits/weights')
('tensor_name: ', 'InceptionV4/Mixed_3a/Branch_1/Conv2d_0a_3x3/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_3a/Branch_1/Conv2d_0a_3x3/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_3a/Branch_1/Conv2d_0a_3x3/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_3a/Branch_1/Conv2d_0a_3x3/weights')
('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_0/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_0/Conv2d_1a_3x3/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_0/Conv2d_1a_3x3/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_0/Conv2d_1a_3x3/weights')
('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_1/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_1/Conv2d_0b_1x7/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_1/Conv2d_0b_1x7/weights')
('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_1/Conv2d_0c_7x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_1/Conv2d_0c_7x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_1/Conv2d_1a_3x3/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_1/Conv2d_1a_3x3/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_1/Conv2d_1a_3x3/weights')
('tensor_name: ', 'InceptionV4/Mixed_5a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_5a/Branch_0/Conv2d_1a_3x3/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_5a/Branch_0/Conv2d_1a_3x3/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_5a/Branch_0/Conv2d_1a_3x3/weights')
('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_0/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_1/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_1/Conv2d_0b_3x3/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_1/Conv2d_0b_3x3/weights')
('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_2/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_2/Conv2d_0b_3x3/weights')
('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_2/Conv2d_0c_3x3/weights')
('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_3/Conv2d_0b_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_1/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_1/Conv2d_0b_3x3/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_1/Conv2d_0b_3x3/weights')
('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_2/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_2/Conv2d_0b_3x3/weights')
('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights')
('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_3/Conv2d_0b_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_0/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_1/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_1/Conv2d_0b_3x3/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_1/Conv2d_0b_3x3/weights')
('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_2/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_2/Conv2d_0b_3x3/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_2/Conv2d_0b_3x3/weights')
('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_2/Conv2d_0c_3x3/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_2/Conv2d_0c_3x3/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_2/Conv2d_0c_3x3/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_2/Conv2d_0c_3x3/weights')
('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_3/Conv2d_0b_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_0/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_0/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_1/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_1/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_1/Conv2d_0b_3x3/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_1/Conv2d_0b_3x3/weights')
('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_2/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_2/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_2/Conv2d_0b_3x3/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_2/Conv2d_0b_3x3/weights')
('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_2/Conv2d_0c_3x3/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_2/Conv2d_0c_3x3/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_2/Conv2d_0c_3x3/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_2/Conv2d_0c_3x3/weights')
('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_3/Conv2d_0b_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_3/Conv2d_0b_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6a/Branch_0/Conv2d_1a_3x3/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6a/Branch_0/Conv2d_1a_3x3/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6a/Branch_0/Conv2d_1a_3x3/weights')
('tensor_name: ', 'InceptionV4/Mixed_6a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6a/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6a/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6a/Branch_1/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6a/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6a/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6a/Branch_1/Conv2d_0b_3x3/weights')
('tensor_name: ', 'InceptionV4/Mixed_6a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6a/Branch_1/Conv2d_1a_3x3/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6a/Branch_1/Conv2d_1a_3x3/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6a/Branch_1/Conv2d_1a_3x3/weights')
('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_0/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_1/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_1/Conv2d_0b_1x7/weights')
('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_1/Conv2d_0c_7x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_1/Conv2d_0c_7x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_2/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_2/Conv2d_0b_7x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_2/Conv2d_0c_1x7/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_2/Conv2d_0c_1x7/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_2/Conv2d_0c_1x7/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_2/Conv2d_0c_1x7/weights')
('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_2/Conv2d_0d_7x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_2/Conv2d_0d_7x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_2/Conv2d_0d_7x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_2/Conv2d_0d_7x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_2/Conv2d_0e_1x7/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_2/Conv2d_0e_1x7/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_2/Conv2d_0e_1x7/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_2/Conv2d_0e_1x7/weights')
('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_3/Conv2d_0b_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_0/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_1/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_1/Conv2d_0b_1x7/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_1/Conv2d_0b_1x7/weights')
('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_1/Conv2d_0c_7x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_1/Conv2d_0c_7x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_2/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_2/Conv2d_0b_7x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_2/Conv2d_0b_7x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_2/Conv2d_0c_1x7/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_2/Conv2d_0c_1x7/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_2/Conv2d_0c_1x7/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_2/Conv2d_0c_1x7/weights')
('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_2/Conv2d_0d_7x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_2/Conv2d_0d_7x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_2/Conv2d_0d_7x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_2/Conv2d_0d_7x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_2/Conv2d_0e_1x7/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_2/Conv2d_0e_1x7/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_2/Conv2d_0e_1x7/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_2/Conv2d_0e_1x7/weights')
('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_3/Conv2d_0b_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_0/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_1/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_1/Conv2d_0b_1x7/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_1/Conv2d_0b_1x7/weights')
('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_1/Conv2d_0c_7x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_1/Conv2d_0c_7x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_2/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_2/Conv2d_0b_7x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_2/Conv2d_0b_7x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_2/Conv2d_0c_1x7/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_2/Conv2d_0c_1x7/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_2/Conv2d_0c_1x7/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_2/Conv2d_0c_1x7/weights')
('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_2/Conv2d_0d_7x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_2/Conv2d_0d_7x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_2/Conv2d_0d_7x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_2/Conv2d_0d_7x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_2/Conv2d_0e_1x7/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_2/Conv2d_0e_1x7/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_2/Conv2d_0e_1x7/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_2/Conv2d_0e_1x7/weights')
('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_3/Conv2d_0b_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_0/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_0/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_1/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_1/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_1/Conv2d_0b_1x7/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_1/Conv2d_0b_1x7/weights')
('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_1/Conv2d_0c_7x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_1/Conv2d_0c_7x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_2/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_2/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_2/Conv2d_0b_7x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_2/Conv2d_0b_7x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_2/Conv2d_0c_1x7/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_2/Conv2d_0c_1x7/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_2/Conv2d_0c_1x7/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_2/Conv2d_0c_1x7/weights')
('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_2/Conv2d_0d_7x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_2/Conv2d_0d_7x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_2/Conv2d_0d_7x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_2/Conv2d_0d_7x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_2/Conv2d_0e_1x7/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_2/Conv2d_0e_1x7/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_2/Conv2d_0e_1x7/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_2/Conv2d_0e_1x7/weights')
('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_3/Conv2d_0b_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_3/Conv2d_0b_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_0/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_0/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_1/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_1/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_1/Conv2d_0b_1x7/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_1/Conv2d_0b_1x7/weights')
('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_1/Conv2d_0c_7x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_1/Conv2d_0c_7x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_2/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_2/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_2/Conv2d_0b_7x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_2/Conv2d_0b_7x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_2/Conv2d_0c_1x7/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_2/Conv2d_0c_1x7/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_2/Conv2d_0c_1x7/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_2/Conv2d_0c_1x7/weights')
('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_2/Conv2d_0d_7x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_2/Conv2d_0d_7x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_2/Conv2d_0d_7x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_2/Conv2d_0d_7x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_2/Conv2d_0e_1x7/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_2/Conv2d_0e_1x7/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_2/Conv2d_0e_1x7/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_2/Conv2d_0e_1x7/weights')
('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_3/Conv2d_0b_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_3/Conv2d_0b_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_0/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_0/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_1/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_1/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_1/Conv2d_0b_1x7/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_1/Conv2d_0b_1x7/weights')
('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_1/Conv2d_0c_7x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_1/Conv2d_0c_7x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_2/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_2/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_2/Conv2d_0b_7x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_2/Conv2d_0b_7x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_2/Conv2d_0c_1x7/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_2/Conv2d_0c_1x7/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_2/Conv2d_0c_1x7/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_2/Conv2d_0c_1x7/weights')
('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_2/Conv2d_0d_7x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_2/Conv2d_0d_7x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_2/Conv2d_0d_7x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_2/Conv2d_0d_7x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_2/Conv2d_0e_1x7/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_2/Conv2d_0e_1x7/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_2/Conv2d_0e_1x7/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_2/Conv2d_0e_1x7/weights')
('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_3/Conv2d_0b_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_3/Conv2d_0b_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_0/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_0/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_1/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_1/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_1/Conv2d_0b_1x7/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_1/Conv2d_0b_1x7/weights')
('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_1/Conv2d_0c_7x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_1/Conv2d_0c_7x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_2/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_2/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_2/Conv2d_0b_7x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_2/Conv2d_0b_7x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_2/Conv2d_0c_1x7/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_2/Conv2d_0c_1x7/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_2/Conv2d_0c_1x7/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_2/Conv2d_0c_1x7/weights')
('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_2/Conv2d_0d_7x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_2/Conv2d_0d_7x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_2/Conv2d_0d_7x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_2/Conv2d_0d_7x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_2/Conv2d_0e_1x7/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_2/Conv2d_0e_1x7/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_2/Conv2d_0e_1x7/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_2/Conv2d_0e_1x7/weights')
('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_3/Conv2d_0b_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_3/Conv2d_0b_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_0/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_0/Conv2d_1a_3x3/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_0/Conv2d_1a_3x3/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_0/Conv2d_1a_3x3/weights')
('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_1/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_1/Conv2d_0b_1x7/weights')
('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_1/Conv2d_0c_7x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_1/Conv2d_0c_7x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_1/Conv2d_1a_3x3/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_1/Conv2d_1a_3x3/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_1/Conv2d_1a_3x3/weights')
('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_0/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_1/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_1/Conv2d_0b_1x3/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_1/Conv2d_0b_1x3/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_1/Conv2d_0b_1x3/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_1/Conv2d_0b_1x3/weights')
('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_1/Conv2d_0c_3x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_1/Conv2d_0c_3x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_2/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_2/Conv2d_0b_3x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_2/Conv2d_0b_3x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_2/Conv2d_0b_3x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_2/Conv2d_0b_3x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_2/Conv2d_0c_1x3/weights')
('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_2/Conv2d_0d_1x3/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_2/Conv2d_0d_1x3/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_2/Conv2d_0d_1x3/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_2/Conv2d_0d_1x3/weights')
('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_2/Conv2d_0e_3x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_2/Conv2d_0e_3x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_2/Conv2d_0e_3x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_2/Conv2d_0e_3x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_3/Conv2d_0b_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_0/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_1/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_1/Conv2d_0b_1x3/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_1/Conv2d_0b_1x3/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_1/Conv2d_0b_1x3/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_1/Conv2d_0b_1x3/weights')
('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_1/Conv2d_0c_3x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_2/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_2/Conv2d_0b_3x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_2/Conv2d_0b_3x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_2/Conv2d_0b_3x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_2/Conv2d_0b_3x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_2/Conv2d_0c_1x3/weights')
('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_2/Conv2d_0d_1x3/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_2/Conv2d_0d_1x3/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_2/Conv2d_0d_1x3/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_2/Conv2d_0d_1x3/weights')
('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_2/Conv2d_0e_3x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_2/Conv2d_0e_3x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_2/Conv2d_0e_3x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_2/Conv2d_0e_3x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_3/Conv2d_0b_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_0/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_1/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_1/Conv2d_0b_1x3/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_1/Conv2d_0b_1x3/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_1/Conv2d_0b_1x3/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_1/Conv2d_0b_1x3/weights')
('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_1/Conv2d_0c_3x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_1/Conv2d_0c_3x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_2/Conv2d_0a_1x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_2/Conv2d_0b_3x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_2/Conv2d_0b_3x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_2/Conv2d_0b_3x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_2/Conv2d_0b_3x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_2/Conv2d_0c_1x3/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_2/Conv2d_0c_1x3/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_2/Conv2d_0c_1x3/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_2/Conv2d_0c_1x3/weights')
('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_2/Conv2d_0d_1x3/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_2/Conv2d_0d_1x3/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_2/Conv2d_0d_1x3/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_2/Conv2d_0d_1x3/weights')
('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_2/Conv2d_0e_3x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_2/Conv2d_0e_3x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_2/Conv2d_0e_3x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_2/Conv2d_0e_3x1/weights')
('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta')
('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean')
('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance')
('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_3/Conv2d_0b_1x1/weights')
('tensor_name: ', 'global_step')
```"
6961,Prevent/warn user setting up infinite loops with Defun/symbolic_gradient,"After session.run, the node `dx` seems to be evaluated in an infinite loop. Tensorflow version 12.1 and also in HEAD.

```
import tensorflow as tf

from tensorflow.python.framework import function
from tensorflow.python.ops import functional_ops


@function.Defun(tf.float32, tf.float32)
def XSquarePlusOneGrad(x, dy):
    message = tf.Print(1, [1], ""back prop"")
    with tf.control_dependencies([message]):
        dx = functional_ops._symbolic_gradient(input=[x, dy], Tout=[tf.float32], f=""XSquarePlusOneFn"", name=""dx"")
    return dx

@function.Defun(tf.float32, func_name=""XSquarePlusOneFn"", grad_func=XSquarePlusOneGrad)
def XSquarePlusOne(x):
    message = tf.Print(1, [1], ""forward prop"")
    with tf.control_dependencies([message]):
        y = tf.square(x, name=""fprop_square"")
    return y


x = tf.placeholder(tf.float32, shape=(), name=""x_input"")
fprop = XSquarePlusOne(x)
grad = tf.gradients(fprop, [x])[0]
sess = tf.Session()
sess.run([fprop, grad], feed_dict={x: 4})
```

You see

```
I tensorflow/core/kernels/logging_ops.cc:79] back prop[1]
I tensorflow/core/kernels/logging_ops.cc:79] back prop[1]
I tensorflow/core/kernels/logging_ops.cc:79] back prop[1]
I tensorflow/core/kernels/logging_ops.cc:79] back prop[1]
I tensorflow/core/kernels/logging_ops.cc:79] back prop[1]
I tensorflow/core/kernels/logging_ops.cc:79] back prop[1]
I tensorflow/core/kernels/logging_ops.cc:79] back prop[1]
...
```"
6957,Inception3 retraining - attach text/labels to individual images,"Hi

I am using the inception v3 model to retrain my own dataset. I have few folder which represent the classes which contain images for each class. What i would like to do is to 'attach' some text ids to these images so when they are retrained and used to run classification/similarity-detection those ids are retrieved too. (basically its image similarity detection)

For instance, Image X is of class 'Teachers' and it belongs to John. When i retrain the model, and run a classification on the new model, i would like to get the Teachers class, but in addition to this i would like to know who is teacher (John).

Any ideas how to go for it?

Regards"
6956,Error downloading nasm,"As noted in issue #6950 nasm-2.12.02.tar.bz2 is currently unavailable. (www.nasm.us does not accept connections.) @trsaunders observed this for head of r0.12 and I can confirm this for v0.12.0.

Workaround: Google the file to get it from another source and update URL in tensorflow/workspace.bzl (if you download a .gz file re-compress to bz2 and put it on a webserver) - sha256 fingerprinting should check that the file contents are correct; if you don't trust ./configure to check this, check the sha256sum yourself against the sha256 given in the .bzl file.
"
6955,Better way to transfer data from memory to tensor in C++ API,"This is more of a general feature request for a better way to load data into a tensor in the C++ API, but I'll take our specific case as a reference.

We currently train a model in Python, freeze it, and load it in a C++ production pipeline. This works fine, but it seems the only way of loading data into a tensor is by looping through every single element in the data and copying it to a tensor. In our case, we're dealing with a 1920x1080 ~30 fps video input signal (each frame coming as an OpenCV matrix) making it infeasible to do if we want to process a video within a reasonable amount of time.

It seems there is a way of creating a tensor from a pointer in the C API (see http://stackoverflow.com/questions/39379747/import-opencv-mat-into-c-tensorflow-without-copying), which we will test next, but it would be nice to also have this functionality in the C++ API.

For reference, it takes approximately 900 ms to copy from an 1920x1080x3 OpenCV matrix to a tensor while it takes 315 ms to do session.run (which I assume includes transferring between CPU and GPU memory). 

An issue related to this is that we will probably already have the data on the GPU from some earlier preprocessing, so we would also be very interested in not having to transfer between CPU and GPU unnecessarily.

So I guess it boils down to:

1. Are there any plans for making it easier to load data already in memory to a tensor?
2. How can we contribute?"
6954,only publish pre-trained model's checkpoint files are useless.,"@nathansilberman @sguada
I can see there are some pre-trained models in the link below for download.

https://github.com/tensorflow/models/tree/master/slim

However, these are only checkpoint files. We can't use them at all. Because without .pbtxt file, which is the graph define, we can't restore the any model only base on .ckpt file. There are no example about how to use these checkpoint file, such as for **a single image prediction** or feature extraction. I can only see one example about how to use V3 .pb file for prediction, but that one is .pb file, not .ckpt file. Without .pbtxt file, we can't freeze the graph at all.

I want to use Inception V2 model, which is smaller than V3 and better than V1. However, I can do nothing...

even if you fine tune the model, you can only evaluate it base on a batch file, but not to predict single image. in the section of **Evaluating performance of a model** of the page, we can only get accuracy. However, we need to test single image to see the result, but not a accuracy number for a batch file."
6953,Error when generating Python tools,"I attempted to generate the python tools with the following code from commit b03beb0:
```
bazel build tensorflow/python/tools:freeze_graph && \
bazel-bin/tensorflow/python/tools/freeze_graph \
--input_graph=some_graph_def.pb \
--input_checkpoint=model.ckpt-8361242 \
--output_graph=/tmp/frozen_graph.pb --output_node_names=softmax
```
The result was the following:
```
MacBook-Air-2:tensorflow kevin$ bazel build tensorflow/python/tools:freeze_graph && bazel-bin/tensorflow/python/tools/freeze_graph --input_graph=some_graph_def.pb --input_checkpoint=model.ckpt-8361242 --output_graph=/tmp/frozen_graph.pb --output_node_names=softmax
WARNING: Bazel Android NDK crosstools are based on Android NDK revision 12. The revision of the Android NDK given in android_ndk_repository rule 'androidndk' is '13.1.3345770'.
ERROR: /Users/kevin/tensorflow/tensorflow/core/BUILD:1207:1: no such target '//tensorflow/tools/git:gen/spec.json': target 'gen/spec.json' not declared in package 'tensorflow/tools/git' defined by /Users/kevin/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.
ERROR: /Users/kevin/tensorflow/tensorflow/core/BUILD:1207:1: no such target '//tensorflow/tools/git:gen/head': target 'gen/head' not declared in package 'tensorflow/tools/git' defined by /Users/kevin/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.
ERROR: /Users/kevin/tensorflow/tensorflow/core/BUILD:1207:1: no such target '//tensorflow/tools/git:gen/branch_ref': target 'gen/branch_ref' not declared in package 'tensorflow/tools/git' defined by /Users/kevin/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.
ERROR: Analysis of target '//tensorflow/python/tools:freeze_graph' failed; build aborted.
INFO: Elapsed time: 31.740s
```
Running on MacOS 10.12.2."
6950,Error downloading zlib 1.2.8 in TensorFlow r0.10,"While building TensorFlow r0.10, we faced below error:
`zlib_archive: Error downloading http://zlib.net/zlib-1.2.8.tar.gz`

It looks like the issue has been fixed in r0.12 through [commit](https://github.com/tensorflow/tensorflow/commit/1e317b1f7dc5ccf04fc51ac96d97f5bdaefa9af9).

Will it be possible to include same change for r0.10?



"
6949,Bazel building error : proxy address 127.0.0.1:8118 is not a valid URL,"I try to use bazel to build an Android APK by following https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android

but I meet download error like this after use this commend **bazel build -c opt //tensorflow/examples/android:tensorflow_demo**:

> _/home/gehen/tensorflow/tensorflow/examples/android/BUILD:58:1: no such package '@inception5h//': Error downloading [https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip] to /home/gehen/.cache/bazel/_bazel_gehen/9dc623b792acc42ff703ada723a44292/external/inception5h/inception5h.zip: Proxy address 127.0.0.1:8118 is not a valid URL and referenced by '//tensorflow/examples/android:tensorflow_demo'._

I use these attempt to fix this error ,but all failed.
1.**unset http_proxy**
     this report the same error
2.corret the http_proxy.
    Actually I use the http://localhost:1080 as my proxy , but I don't known why it change into 127.0.0.1:8118, so I use export **http_proxy=http://localhost:1080** and **https_proxy=http://localhost:1080**, it can not download

 _Timeout connecting to https://storage.googleapis.com/download.tensorflow\
.org/models/mobile_multibox_v1.zip_

3.I find I can wget the *.zip seperately , so why can't I get the sources from bazel , what might the resons .

Thank you for your reply

I'm almost crazy about it

"
6946,How do I use mkl's matmul in MatMulOp::Compute()?,"I've been trying to modify core/kernels/matmul_op.cc to use mkl's matmul api. Although I can link mkl without any problem to compile my simple standalone c++ file, things don't work after adding the linking option to matmul_op's rule.
copts = [""-Wl,--start-group /opt/intel/compilers_and_libraries_2017.0.098/linux/mkl/lib/intel64/libmkl_intel_lp64.a /opt/intel/compilers_and_libraries_2017.     0.098/linux/mkl/lib/intel64/libmkl_gnu_thread.a /opt/intel/compilers_and_libraries_2017.0.098/linux/mkl/lib/intel64/libmkl_core.a -Wl,--end-group -lgomp -lpthread -lm -ldl""]

I also used the mkl link option when compiling a test that uses the matmul op.

Like this:

bazel build -c opt tensorflow/tools/benchmark:2mklmatmul_test --linkopt=""-L${MKLROOT}/lib/intel64/libmkl_intel_lp64 -lmkl_gnu_thread -lmkl_core -lgomp -lpthread -lm -ldl""

However, tensorflow can't find the definition of mkl's matmul function during linking.

Please note that I understand that I should add a new op. I did this just to quickly see whether I can link MKL in tensorflow."
6945,Gradle builds for Tensorflow Android does not automatically fetch the models,"Builds work fine using bazel.
However when the project is imported into android studio using gradle, the assets directory remains empty. The app crashes on startup. When the models are manually downloaded and placed into the asset folder, the gradle builds work. Not sure if this was intended. "
6943,Adding new vocab words to seq2seq machine translation model during training?,"I am using the seq2seq tutorial to play with machine translation. Say I have trained the model for some time and determine that I want to supplement the original vocab with new words to enhance the quality of the model. Is there a way to pause training, add words to the vocabulary, and then resume training from the most recent checkpoint? I attempted to do so but when I began training again I got this error:

Traceback (most recent call last):
File ""execute.py"", line 405, in <module>
train()
File ""execute.py"", line 127, in train
model = create_model(sess, False)
File ""execute.py"", line 108, in create_model
model.saver.restore(session, ckpt.model_checkpoint_path)
File ""/home/jrthom18/.local/lib/python2.7/site-    packages/tensorflow/python/training/saver.py"", line 1388, in restore
{self.saver_def.filename_tensor_name: save_path})
File ""/home/jrthom18/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 766, in run
run_metadata_ptr)
File ""/home/jrthom18/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 964, in _run
feed_dict_string, options, run_metadata)
File ""/home/jrthom18/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1014, in _do_run
target_list, options, run_metadata)
File ""/home/jrthom18/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1034, in _do_call
raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Assign   requires shapes of both tensors to match. lhs shape= [384633] rhs shape=   [384617]
 [[Node: save/Assign_82 = Assign[T=DT_FLOAT, _class=[""loc:@proj_b""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](proj_b, save/RestoreV2_82)]]

Caused by op u'save/Assign_82', defined at:
File ""execute.py"", line 405, in <module>
train()
File ""execute.py"", line 127, in train
model = create_model(sess, False)
File ""execute.py"", line 99, in create_model
model = seq2seq_model.Seq2SeqModel( gConfig['enc_vocab_size'],  gConfig['dec_vocab_size'], _buckets, gConfig['layer_size'], gConfig['num_layers'], gConfig['max_gradient_norm'], gConfig['batch_size'], gConfig['learning_rate'], gConfig['learning_rate_decay_factor'], forward_only=forward_only)
File ""/home/jrthom18/data/3x256_bs32/easy_seq2seq/seq2seq_model.py"", line 166, in __init__
self.saver = tf.train.Saver(tf.global_variables(), keep_checkpoint_every_n_hours=2.0)
File ""/home/jrthom18/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1000, in __init__
self.build()
File ""/home/jrthom18/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1030, in build
restore_sequentially=self._restore_sequentially)
File ""/home/jrthom18/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 624, in build
restore_sequentially, reshape)
File ""/home/jrthom18/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 373, in _AddRestoreOps
assign_ops.append(saveable.restore(tensors, shapes))
File ""/home/jrthom18/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 130, in restore
self.op.get_shape().is_fully_defined())
File ""/home/jrthom18/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.py"", line 47, in assign
use_locking=use_locking, name=name)
File ""/home/jrthom18/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 759, in apply_op
op_def=op_def)
File ""/home/jrthom18/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2240, in create_op
original_op=self._default_original_op, op_def=op_def)
File ""/home/jrthom18/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1128, in __init__
self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [384633] rhs shape= [384617]
 [[Node: save/Assign_82 = Assign[T=DT_FLOAT, _class=[""loc:@proj_b""],   use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](proj_b, save/RestoreV2_82)]]

Obviously the new vocab is larger and so the tensor sizes do not match. Is there some way around this?"
6942,TensorFlow for Python3.4::The specified key does not exist,"I am trying to install TensorFlow for GPU and I have CUDA8 and CUDNN in Ubuntu 14.04.
Here's the errors I received. Please suggest solution:
```

$ sudo pip3 install --upgrade $TF_BINARY_URL
Downloading/unpacking https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp34
  HTTP error 404 while getting https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp34
Cleaning up...
Exception:
Traceback (most recent call last):
  File ""/usr/lib/python3/dist-packages/pip/basecommand.py"", line 122, in main
    status = self.run(options, args)
  File ""/usr/lib/python3/dist-packages/pip/commands/install.py"", line 278, in run
    requirement_set.prepare_files(finder, force_root_egg_info=self.bundle, bundle=self.bundle)
  File ""/usr/lib/python3/dist-packages/pip/req.py"", line 1198, in prepare_files
    do_download,
  File ""/usr/lib/python3/dist-packages/pip/req.py"", line 1376, in unpack_url
    self.session,
  File ""/usr/lib/python3/dist-packages/pip/download.py"", line 547, in unpack_http_url
    resp.raise_for_status()
  File ""/usr/share/python-wheels/requests-2.2.1-py2.py3-none-any.whl/requests/models.py"", line 773, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found

Storing debug log for failure in /home/mona/.pip/pip.log
```

Here's the log:
```
  1 ------------------------------------------------------------
  2 /usr/bin/pip3 run on Wed Jan 18 16:30:31 2017
  3 Downloading/unpacking https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp34
  4   HTTP error 404 while getting https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp34
  5 Cleaning up...
  6   Removing temporary dir /tmp/pip_build_root...
  7 Exception:
  8 Traceback (most recent call last):
  9   File ""/usr/lib/python3/dist-packages/pip/basecommand.py"", line 122, in main
 10     status = self.run(options, args)
 11   File ""/usr/lib/python3/dist-packages/pip/commands/install.py"", line 278, in run
 12     requirement_set.prepare_files(finder, force_root_egg_info=self.bundle, bundle=self.bundle)
 13   File ""/usr/lib/python3/dist-packages/pip/req.py"", line 1198, in prepare_files
 14     do_download,
 15   File ""/usr/lib/python3/dist-packages/pip/req.py"", line 1376, in unpack_url
 16     self.session,
 17   File ""/usr/lib/python3/dist-packages/pip/download.py"", line 547, in unpack_http_url
 18     resp.raise_for_status()
 19   File ""/usr/share/python-wheels/requests-2.2.1-py2.py3-none-any.whl/requests/models.py"", line 773, in raise_for_status
 20     raise HTTPError(http_error_msg, response=self)
 21 requests.exceptions.HTTPError: 404 Client Error: Not Found

```

![screenshot from 2017-01-18 16-29-24](https://cloud.githubusercontent.com/assets/1892917/22085884/aafbcb72-dd9b-11e6-99e9-6acea8f25526.png)
"
6939,Leave old binaries up?,"I've noticed that the binaries for older versions have been taken down. In particular, I need to install 0.11.rc0 and both 

`https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.11.rc0-cp27-none-linux_x86_64.whl`

and

`https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.11.0-cp27-none-linux_x86_64.whl`

are 404s. Pretty sure they used to work when 0.11.rc0 was the latest though. 

Were they intentionally taken down to prevent people from unwittingly installing older versions? That would be understandable, but it would be really nice if there was an easy way to access older binaries. Greatly looking forward to the release of 1.0, but for the time being there's a nest of dependency conflicts to manage and that's difficult when it's not clear how to install old versions!
"
6937,Confusing arg name for sequence_loss,"Sorry if this is noise, but it's just a minor suggestion to make the API better (before the release of TensorFlow 1.0 where it will be much more difficult to change)

For the function [sequence_loss](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/loss.py) on the master branch, the name of the parameter `softmax_loss_function` seems to imply that only softmax based loss are compatible which is not true (for instance `tf.nn.sigmoid_cross_entropy_with_logits` works too). Maybe the name was that just to indicate the default behavior but I think it is misleading.

Why not simply call the parameter `loss_function` ? It's more representative of what the parameter really does."
6934,"Download and setup instructions issue, incorrect version of CuDNN","[Instructions](https://www.tensorflow.org/get_started/os_setup) indicate to use CuDNN 5 with Tensorflow for GPU with CUDA Tookit 8.0 and Python 3.5 under Ubuntu/Linux. After doing that, when using the Tensorflow Python API I get a run-time error:

`E tensorflow/stream_executor/cuda/cuda_dnn.cc:378] Loaded runtime CuDNN library: 5005 (compatibility version 5000) but source was compiled with 5105 (compatibility version 5100).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.`

and the computation stops.

If instead I use CuDNN 5.1, the computation goes through correctly.

I have installed Tensorflow 0.12.1 for Python 3.5 with GPU with:

`sudo -H pip3 install tensorflow-gpu`

under Ubuntu 16.04 64-bit.

```
> python3 -c ""import tensorflow; print(tensorflow.__version__)""
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
0.12.1
```

[Here](https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/02_Convolutional_Neural_Network.ipynb) a program that can be used to reproduce the issue.

I believe installation instructions should be updated to reference CuDNN 5.1."
6933,cuDevicePrimaryCtxSetFlags not found issue on Windows 7,"I have successfully loaded tensorflow in python 3.5. But when I start a session, it gave me the following error and python will crash:

>>> import tensorflow as tf
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stre
am_executor\dso_loader.cc:128] successfully opened CUDA library cublas64_80.dll
locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stre
am_executor\dso_loader.cc:128] successfully opened CUDA library cudnn64_5.dll lo
cally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stre
am_executor\dso_loader.cc:128] successfully opened CUDA library cufft64_80.dll l
ocally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stre
am_executor\dso_loader.cc:128] successfully opened CUDA library nvcuda.dll local
ly
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stre
am_executor\dso_loader.cc:128] successfully opened CUDA library curand64_80.dll
locally
>>> sess = tf.Session()
F c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stre
am_executor\cuda\cuda_driver.cc:94] Check failed: s.ok() could not find cuDevice
PrimaryCtxSetFlags in libcuda DSO; dlerror: cuDevicePrimaryCtxSetFlags not found

My graphic card is NVDIA  quadro K1100M with cuda compute capability 3.0. I have cuda toolkit 8.0 and cudnnv5.1 installed. I tried different nvdia drivers but none is working. If the driver is too old, it will give me insufficient version number to be used with cuda. If the drive is new, my computer just freeze and saying the graphic card is not removable or cannot ejected. Could someone help me with this? Thanks."
6932,layers.core.Dense doesn't make use of argument activity_regularizer,"As it says on the tin, the ``Dense`` class of ``tensorflow.python.layers.core`` accepts an ``activity_regularizer`` argument, but never makes use of this argument. Not sure if this is intentional or an oversight, so pointing it out."
6931,Tensorflow builds on Windows are broken,The corresponding build projects are red on both Tensorflow CI (http://ci.tensorflow.org/job/tf-master-win-bzl/) and Bazel CI (http://ci.bazel.io/job/TensorFlow/).
6930,Is Python 3.3 still supported?,"Docs say Python 3.3 is supported.

https://www.tensorflow.org/versions/r0.10/get_started/os_setup says:

> The TensorFlow Python API supports Python 2.7 and Python 3.3+.

I'm not too bothered about Python 3.3, but if it's no longer supported, please update the docs.


### Environment info
Travis CI
Operating System: Ubuntu 14.04.5 LTS
Python 3.3.6
pip 8.1.2

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

```
$ pip install tensorflow
Collecting tensorflow
  Could not find a version that satisfies the requirement tensorflow (from versions: )
No matching distribution found for tensorflow
```

"
6929,Documentation of ctc_beam_search_decoder is inconsistent,"The documentation of ctc_beam_search_decoder describes the `input` argument as:

    inputs: 3-D `float` `Tensor`, size
        `[max_time x batch_size x num_classes]`.  The logits.

Implying that they should be the linear projections. However, by inspecting the [unit tests](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/ctc_decoder_ops_test.py#L96) and the [implementation](https://github.com/tensorflow/tensorflow/blob/679f95e9d8d538c3c02c0da45606bab22a71420e/tensorflow/core/util/ctc/ctc_beam_search.h#L290), it seems these inputs should actually be log-probabilities. In this case, the documentation should read:

    inputs: 3-D `float` `Tensor`, size
      `[max_time x batch_size x num_classes]`.
    The output symbol log-probabilities."
6928, how to resolve warning: Unable to load cuDNN DSO ,"Hello, 
I recently installed tensorflow GPU version 0.12.1 and python 3.5.2 for windows. 
There is CUDA GTX660 driver already installed on my PC.  

I am trying some sample programs and when ever i am using import tensorflow  I get a warning and then the result. How to resolve the issue ? What else should I be installing ? 

I tried to download cuDNN, select all in cuDNN Dowload Survey will make any difference? I will be working on RNN implementations and have many FFT calculations.
At the moment, I choose deeplearning framework -> tensorflow library and I have downloaded cuDNN v5.1 Library for Windows 10 which gave me a ""cudnn-8.0-windows10-x64-v5.1.zip"" file , but there is no executable file or something to install.  should I be copying those in to any folder or did i downloaded the wrong one ?

`C:\Users\raady\AppData\Local\Programs\Python\Python35\python.exe ""D:/Lab Project Files/TF/Practice Files/test.py""
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library cublas64_80.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:119] Couldn't open CUDA library cudnn64_5.dll
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\cuda\cuda_dnn.cc:3459] Unable to load cuDNN DSO
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library cufft64_80.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library nvcuda.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library curand64_80.dll locally
WARNING:tensorflow:From D:/Lab Project Files/TF/Practice Files/test.py:4 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:885] Found device 0 with properties: 
name: GeForce GTX 660
major: 3 minor: 0 memoryClockRate (GHz) 1.0325
pciBusID 0000:01:00.0
Total memory: 2.00GiB
Free memory: 1.65GiB
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:906] DMA: 0 
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:916] 0:   Y 
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 660, pci bus id: 0000:01:00.0)`"
6927,External optimizer with apply_gradients,"Hi all,

I'm trying to define some new optimization algorithm. I found an interface about external optimizer [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/opt/python/training/external_optimizer.py), it says that i need to subclass it and implement _minimize. However I need to call apply_gradients when using the external optimizer. Does anyone know how can i implement it?

Thanks."
6926,Add Maven artifacts to central mvnrepository,"For Java/Scala users, specifically - those using the Spark/Hadoop environment - it would help to have two artifacts:
example/feature compiled protos

TFRecordFileFormat for Hadoop/Spark io
"
6925,support for depth pooling in maxpool3d?,"Is depth pooling in the works for MaxPool3D? Any pointers on how I should/could get this going myself? 
```
UnimplementedError (see above for traceback): Pooling is not yet supported on the depth dimension.
[[Node: max_pool_1 = MaxPool3D[T=DT_FLOAT, ksize=[1, 2, 2, 2, 64], padding=""VALID"", strides=[1,
 1, 1, 1, 1], _device=""/job:localhost/replica:0/task:0/gpu:0""](Relu_1)]]
```
Here's an example of how I'm using it:
```Python
def maxpool3d(input_, kd, kh, kw, stride=1,scope=None):
    input_cdim = input_.get_shape().as_list()[-1]
    batch_size = input_.get_shape().as_list()[0]
    kernel = [batch_size, kd, kh, kw, input_cdim]
    return tf.nn.max_pool3d(input_, kernel,
        strides=[1,stride,stride,stride,1], padding=""VALID"", name=scope)
# ...
# Perform 2 3D convolutions without max pooling.
x = tf.nn.relu(conv3d(x, 4, 4, 4, 64, scope=""conv_1""))
x = tf.nn.relu(conv3d(x, 4, 4, 4, 64, scope=""conv_2""))

# Downsample with max pooling.
x = maxpool3d(x, 2, 2, 2, scope=""max_pool_1"")
```"
6924,iOS returns different results in simple and camera examples using retrained inception model,"I have retrained the inception model to recognize custom set of images. I have tried the simple and camera examples and replaced the .pb and .txt files with my own files. I am able to get a reliable result of probability of 0.99 when using the simple example (replaced the grace_hopper with my image). However, I got different results when using the camera example. The config I am using is as follows, is there any config I need to make? Thank you.

```
const int wanted_input_width = 299;
const int wanted_input_height = 299;
const int wanted_input_channels = 3;
const float input_mean = 128.0f;
const float input_std = 128.0f;
const std::string input_layer_name = ""Mul"";
const std::string output_layer_name = ""final_result"";
```"
6923,cmake vs bazel,"Hello I am writing with regard compiling tensorflow on linux clusters. 
As far as I understood bazel recompiles everything it needs. On out cluster we use spack (https://github.com/LLNL/spack) to compile new software and provide modules to the users. With spack it's easy to track dependencies and provenance of a given installation.
By recompiling everything bazel's need the dependencies are messed up and we don't understand how tensorflow has been compiled from a spack point of view. 
Why is it stated"" N.B. We provide Linux build instructions primarily for the purpose of testing the build. We recommend using the standard Bazel-based build on Linux.""?
Actually, how can we tell to bazel to use system-provided libraries like boost?

Thanks very much,

Nicola

"
6922,Getting sparse tensors from graph by name,"Standard (dense) tensors can be easily retrieved by name from a graph using:

`graph.get_tensor_by_name(""tensor_name:tensor_index"")`

Comparable functionality is provided for operations, but it's not clear to me how to retrieve a sparse tensor in a similar way.
Is such functionality provided at the moment? Having to pass the object around makes the code more involved than it should be."
6920,can tensorflow CPU version be updated to GPU version without uninstalling ?,"python 3.5 
tensorflow 0.11.0
Hello,
I wanted to do FFT operations using tensorflow. does this CPU version support working of that ? 

In some post I have seen we cannot perform FFT on CPU version of tensorflow. If so can I change the tensorflow version of CPU to GPU without installing it ? 

I need to work on the LSTM algorithm as well further. 
"
6919,tensorflow:Drop an example - too long,"warining :tensorflow:Drop an example - too long
I want to know what is this warning's meaning or where i can find the doc about it  ,thk u tell  me 
"
6917, How to init the graph once and reuse the graph to serve all the requests,"I want to provide a restful api or rpc for client to input some pictures and output their features, but every request will start a new graph and load all train variables from the disk, and this is very slow, how can I init the graph once and reuse the graph to serve all the requests, Thank You for your help"
6914,"Binaries should target CUDA architectures 6.0, 6.1 to avoid long delays on startup","The public docker images and pip packages are not compiled against CUDA architecture 6.0 or 6.1, which can cause a delay of more than a minute during initialization when executed on corresponding hardware (P100, GTX 10 series etc.) as the CUDA driver runs JIT compilation on all of the internal kernels.

Given that the public binaries are now built against CUDA 8.0, it would make sense to have them target archs 6.0 and 6.1.

Minimal reproducer (run with CUDA_CACHE_DISABLE=1):

```
import tensorflow as tf
sess = tf.Session()
rand_init = tf.random_uniform([32, 8], -1.0, 1.0)
rand_var  = tf.Variable(rand_init)
init = tf.global_variables_initializer()
sess.run(init)
```

### Related issues
https://github.com/tensorflow/tensorflow/issues/3651

### Environment info
Operating System: Linux
"
6912,Pip installation failing,"Fresh python3 install using `brew install python3`.

When I run `pip3 install tensorflow`, I get a 

`Could not find a version that satisfies the requirement tensorflow (from versions: )
No matching distribution found for tensorflow`

I've tried reinstalling python but I get the same error message."
6911,Switching branches forces ./configure and bazel rebuild,"Right now switching branches requires running ./configure which then makes next `bazel test` rebuild everything.

In particular on 32 core Xeon, switching branches/`./configure` makes `bazel test learning/python/...` run 17 minutes longer because it's building stuff from scratch. This makes it hard for open-source contributors to work on more than one Pull Request at a time.

In particular, the error from `bazel test` when switching git branches is as follows:

```
ERROR: /home/yaroslav/tensorflow.git/tensorflow/tensorflow/core/BUILD:1194:1: Executing genrule //tensorflow/core:version_info_gen failed: bash failed:\
 error executing command /bin/bash -c ... (remaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited\
 with status 1.
Traceback (most recent call last):
  File ""tensorflow/tools/git/gen_git_source.py"", line 260, in <module>
    generate(args.generate)
  File ""tensorflow/tools/git/gen_git_source.py"", line 211, in generate
    (old_branch, new_branch))
RuntimeError: Run ./configure again, branch was 'refs/heads/match-once-fix' but is now 'refs/heads/symbolic-gradient-fix'
_
```"
6910,Feature Request: Allow disabling the use of host pinned memory,"Right now, TensorFlow will always use pinned memory (allocated by the stream executor with `cuMemHostAlloc`) whenever it knows that a tensor will need to be transferred to the GPU (or something like that).

When a TensorFlow process crashes (does not exit gracefully) due to segfaults or other unforeseen circumstances, the PoolAllocator destructor is not called, and the pinned memory is not freed. When pinned memory is not freed, the CUDA driver does not actually release it, and so memory usage grows over time if you have processes that are not exiting gracefully. After this happens for a while, the machine needs to swap and/or runs out of pinned memory, and if there is no swap space enabled, the node dies (and the only way to recover it is a hard-reboot, not accessible via the network).

As far as we can tell, there is no workaround to this, and we simply need to disable the pinned memory allocator if we want to be absolutely certain that this cannot happen. Right now, TensorFlow does not allow you to do so.

Proposal: There is currently an environment variable called `TF_CUDA_HOST_MEM_LIMIT_IN_MB` which used to set a maximum size for the `BFCAllocator` that does CUDA host memory allocation. In order to implement this feature, we would check whether the value of that environment variable is zero, and, if it is zero, use a non-pinned memory allocator as the sub-allocator for the BFCAllocator.

Is that acceptable?

[This](https://github.com/tensorflow/tensorflow/issues/3701) issue was probably related."
6906,Relaxing inferenced shape,"Inferenced shape will put restriction on feeding, so sometimes it is desirable to relax inferenced shape. Currently this can be done by `lambda x: tf.cond(tf.constant(True), lambda: x, lambda: tf.placeholder(x.dtype))`, or `tf.placeholder_with_default` if gradient is not required (`placeholder_with_default` has no gradient registered).

I also read a comment in `identity_op.cc`: 

>PlaceholderWithDefault does the same thing as Identity, but has a
>different shape function (and constant value function) registered.

Possibly register a gradient for `placeholder_with_default`?"
6904,error in computing fft of a float variable,"Python 3.5
tensorflow GPU

I am unable to compute the 1 D fft. My input values will be positive and negative float values not complex numbers.
`x = tf.Variable([1.2,2,3,4.1,5.9],dtype=tf.float32)
sess.run(tf.fft(x))`
the error that i get

`TypeError:Input 'input' of 'FFT' Op has type int32 that does not match expected type of complex64`

I have tried 
`sess.run(tf.batch_fft(x))`

but it gives an error as tensorflow has no module batch_fft. 
did I miss any thing in the installation of tensorflow ??

I tried declaring these variables dtype=tf.complex64

it gave another error
`InvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'FFT' with  these attrs.  Registered kernels:
  <no registered kernels>
	 [[Node: FFT = FFT[](Variable/read)]]`
"
6903,Feeding tensors doesn't stop gradients flowing through,"Consider this code snippet:

```
a = tf.constant(0)
b = a + 1
c = b * 2
grad, = tf.gradients(c, a)
grad.eval(feed_dict={b: 0})
```

The result is `2`, instead of `0`. To obtain the true gradient, one has to wrap the tensor with an `tf.cond` and feed an additional indicator, which doesn't seem too elegant.

Apologize if I missed something."
6902,Sliding Window Layer,"Hi,

Is there implementation the sliding window layer like in here: https://github.com/voidrank/caffe-fm/blob/2863b2a41e3f2114d24451f41eeed9ab618262ad/src/caffe/layers/sliding_window_layer.cu ?.

Thanks"
6900,ImportError: libcudart.so.8.0: cannot open shared object file: No such file or directory,"Here is what happens when I try to import tensorflow. I installed using the compile from source method for Linux GPU. This is on Ubuntu 14.04. The file that it says there is no such file or directory is in the path file that I have listed. I've tried searching for help to no avail. 


(I apologize if this doesn't make a lot of sense. Inexperience, sleep-deprivation, and frustration are the main contributors to my inability to communicate clearly at the moment.)

>>> import tensorflow
Traceback (most recent call last):
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/__init__.py"", line 61, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
  File ""/usr/lib/python3.4/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
ImportError: libcudart.so.8.0: cannot open shared object file: No such file or directory
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""<pyshell#1>"", line 1, in <module>
    import tensorflow
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/__init__.py"", line 72, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/__init__.py"", line 61, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
  File ""/usr/lib/python3.4/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
ImportError: libcudart.so.8.0: cannot open shared object file: No such file or directory
Failed to load the native TensorFlow runtime.
See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error
for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
"
6899,tf.select missing in v1.0?,"Seems no tf.select in v1.0 and it exists in v0.12(also not marked as depreciated).
If no tf.select do we have corresponding ops to achieve select ?"
6897,compile failed at grpc_remote_worker.cc error: no matching function,"Hi, I am trying to build tensorflow from source. Configure is OK, but bazel build is failed.
Enviroment: Centos-6.5  gcc-4.8.0 bazel-0.42 tensorflow-0.12.1 cuda-8.0

Here is the error log:
ERROR: /root/Downloads/tensorflow/tensorflow/core/distributed_runtime/rpc/BUILD:70:1: C++ compilation of rule '//tensorflow/core/distributed_runtime/rpc:grpc_remote_worker' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 134 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
tensorflow/core/distributed_runtime/rpc/grpc_remote_worker.cc: In lambda function:
tensorflow/core/distributed_runtime/rpc/grpc_remote_worker.cc:215:68: error: no matching function for call to 'grpc::ClientContext::TryCancel() const'
         call_opts->SetCancelCallback([this]() { context_.TryCancel(); });
                                                                    ^
tensorflow/core/distributed_runtime/rpc/grpc_remote_worker.cc:215:68: note: candidate is:
In file included from external/grpc/include/grpc++/impl/codegen/call.h:43:0,
                 from external/grpc/include/grpc++/impl/call.h:37,
                 from external/grpc/include/grpc++/channel.h:39,
                 from external/grpc/include/grpc++/grpc++.h:56,
                 from ./tensorflow/core/distributed_runtime/rpc/grpc_util.h:21,
                 from ./tensorflow/core/distributed_runtime/rpc/grpc_remote_worker.h:21,
                 from tensorflow/core/distributed_runtime/rpc/grpc_remote_worker.cc:16:
external/grpc/include/grpc++/impl/codegen/client_context.h:296:8: note: void grpc::ClientContext::TryCancel() <near match>
   void TryCancel();
        ^
external/grpc/include/grpc++/impl/codegen/client_context.h:296:8: note:   no known conversion for implicit 'this' parameter from 'const grpc::ClientContext*' to 'grpc::ClientContext*'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 13.842s, Critical Path: 11.80s"
6896," tensorflow/core/platform/cpu_feature_guard.cc:35] The TensorFlow library was compiled to use AVX2 instructions, but these aren't available on your machine. ","# export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.1-cp27-none-linux_x86_64.whl
# pip install --upgrade $TF_BINARY_URL

then got errors!"
6893,get_matching_files issue,"In [saver.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/saver.py), if the argument of `get_matching_files` is a relative path with no ./ in the beginning, `get_matching_files` will return an empty list. e.g., `get_matching_files(""filename"")` returns `[]`, while `get_matching_files(""./filename"")` returns `['filename']`

Thus when using [freeze_graph](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py) with input checkpoint in the same directory, I found the issue below:

```
python freeze_graph.py \
--input_graph=input_graph.pb \
--input_checkpoint=input.ckpt \
--output_graph=output_graph.pb \
--output_node_names=Softmax

Input checkpoint 'input.ckpt' doesn't exist!
```

"
6891,log_cdf of Gaussian distribution returns -inf,"
Hi all! It seems that the log_cdf for normal distribution (tf.contrib.distributions.Normal.log_cdf) returns -inf for some extreme values: 

```
from tensorflow.contrib.distributions import Normal
sess = tf.Session()
print sess.run(Normal(0., 1.).log_cdf(-6.0))
-inf 
```
However, with the same mu and sigma for scipy.stats.norm.log_cdf, you get the correct value of -20.7367. Is there something wrong with the tensorflow implementation of log_cdf?

Thanks!
"
6890,"Use tf.get_variable() instead of tf.Variable, parameter did not update in every epoch?","I use the program in the page 
https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/tutorials/mnist
, just make tf.get_variable() instead of tf.Variable(), other parts are all the same, the result is very different, I doubt whether the parameter update in every epoch? 
# The result of using tf.get_variable():
Step 0: loss = 449.45 (0.013 sec)
Step 100: loss = 10.57 (0.004 sec)
Step 200: loss = 3.80 (0.005 sec)
Step 300: loss = 2.06 (0.004 sec)
Step 400: loss = 1.99 (0.004 sec)
Step 500: loss = 2.29 (0.004 sec)
Step 600: loss = 1.77 (0.005 sec)
Step 700: loss = 2.01 (0.004 sec)
Step 800: loss = 2.39 (0.007 sec)
Step 900: loss = 1.71 (0.004 sec)
Training Data Eval:
  Num examples: 55000  Num correct: 21297  Precision @ 1: 0.3872
Validation Data Eval:
  Num examples: 5000  Num correct: 1886  Precision @ 1: 0.3772
Test Data Eval:
  Num examples: 10000  Num correct: 3780  Precision @ 1: 0.3780
Step 1000: loss = 2.27 (0.009 sec)
Step 1100: loss = 2.45 (0.073 sec)
Step 1200: loss = 1.57 (0.004 sec)
Step 1300: loss = 1.80 (0.004 sec)
Step 1400: loss = 1.49 (0.004 sec)
Step 1500: loss = 1.84 (0.004 sec)
Step 1600: loss = 1.34 (0.004 sec)
Step 1700: loss = 1.44 (0.004 sec)
Step 1800: loss = 1.56 (0.004 sec)
Step 1900: loss = 1.48 (0.004 sec)
Training Data Eval:
  Num examples: 55000  Num correct: 26545  Precision @ 1: 0.4826
Validation Data Eval:
  Num examples: 5000  Num correct: 2336  Precision @ 1: 0.4672
Test Data Eval:
  Num examples: 10000  Num correct: 4785  Precision @ 1: 0.4785
Step 2000: loss = 1.58 (0.011 sec)
Step 2100: loss = 1.37 (0.004 sec)
Step 2200: loss = 1.50 (0.070 sec)
Step 2300: loss = 1.38 (0.004 sec)
Step 2400: loss = 1.72 (0.004 sec)
Step 2500: loss = 1.58 (0.004 sec)
Step 2600: loss = 1.20 (0.004 sec)
Step 2700: loss = 1.34 (0.004 sec)
Step 2800: loss = 1.49 (0.004 sec)
Step 2900: loss = 1.15 (0.004 sec)
Training Data Eval:
  Num examples: 55000  Num correct: 30216  Precision @ 1: 0.5494
Validation Data Eval:
  Num examples: 5000  Num correct: 2685  Precision @ 1: 0.5370
Test Data Eval:
  Num examples: 10000  Num correct: 5481  Precision @ 1: 0.5481
Step 3000: loss = 1.19 (0.010 sec)
Step 3100: loss = 1.69 (0.004 sec)
Step 3200: loss = 1.25 (0.004 sec)
Step 3300: loss = 1.25 (0.071 sec)
Step 3400: loss = 1.14 (0.004 sec)
Step 3500: loss = 1.27 (0.004 sec)
Step 3600: loss = 1.50 (0.004 sec)
Step 3700: loss = 1.07 (0.004 sec)
Step 3800: loss = 1.15 (0.004 sec)
Step 3900: loss = 1.12 (0.004 sec)
Training Data Eval:
  Num examples: 55000  Num correct: 29304  Precision @ 1: 0.5328
Validation Data Eval:
  Num examples: 5000  Num correct: 2684  Precision @ 1: 0.5368
Test Data Eval:
  Num examples: 10000  Num correct: 5234  Precision @ 1: 0.5234
Step 4000: loss = 1.13 (0.008 sec)
Step 4100: loss = 1.23 (0.004 sec)
Step 4200: loss = 1.24 (0.004 sec)
Step 4300: loss = 1.20 (0.004 sec)
Step 4400: loss = 1.03 (0.069 sec)
Step 4500: loss = 0.89 (0.004 sec)
Step 4600: loss = 1.37 (0.004 sec)
Step 4700: loss = 1.13 (0.004 sec)
Step 4800: loss = 1.17 (0.004 sec)
Step 4900: loss = 1.29 (0.004 sec)
Training Data Eval:
  Num examples: 55000  Num correct: 32965  Precision @ 1: 0.5994
Validation Data Eval:
  Num examples: 5000  Num correct: 2963  Precision @ 1: 0.5926
Test Data Eval:
  Num examples: 10000  Num correct: 5944  Precision @ 1: 0.5944
Step 5000: loss = 1.20 (0.008 sec)
Step 5100: loss = 1.28 (0.004 sec)
Step 5200: loss = 1.09 (0.004 sec)
Step 5300: loss = 0.94 (0.004 sec)
Step 5400: loss = 1.09 (0.004 sec)
Step 5500: loss = 1.23 (0.069 sec)
Step 5600: loss = 1.03 (0.004 sec)
Step 5700: loss = 1.14 (0.004 sec)
Step 5800: loss = 1.25 (0.004 sec)
Step 5900: loss = 1.28 (0.004 sec)
Training Data Eval:
  Num examples: 55000  Num correct: 33732  Precision @ 1: 0.6133
Validation Data Eval:
  Num examples: 5000  Num correct: 3071  Precision @ 1: 0.6142
Test Data Eval:
  Num examples: 10000  Num correct: 6170  Precision @ 1: 0.6170
Step 6000: loss = 0.99 (0.007 sec)
Step 6100: loss = 1.09 (0.004 sec)
Step 6200: loss = 1.00 (0.004 sec)
Step 6300: loss = 1.00 (0.004 sec)
Step 6400: loss = 1.28 (0.004 sec)
Step 6500: loss = 0.90 (0.004 sec)
Step 6600: loss = 0.73 (0.069 sec)
Step 6700: loss = 1.08 (0.004 sec)
Step 6800: loss = 0.99 (0.004 sec)
Step 6900: loss = 0.89 (0.004 sec)
Training Data Eval:
  Num examples: 55000  Num correct: 32685  Precision @ 1: 0.5943
Validation Data Eval:
  Num examples: 5000  Num correct: 2972  Precision @ 1: 0.5944
Test Data Eval:
  Num examples: 10000  Num correct: 5963  Precision @ 1: 0.5963
Step 7000: loss = 1.10 (0.009 sec)
Step 7100: loss = 1.32 (0.004 sec)
Step 7200: loss = 0.71 (0.004 sec)
Step 7300: loss = 0.88 (0.004 sec)
Step 7400: loss = 1.04 (0.004 sec)
Step 7500: loss = 1.69 (0.004 sec)
Step 7600: loss = 1.03 (0.004 sec)
Step 7700: loss = 1.32 (0.069 sec)
Step 7800: loss = 0.98 (0.004 sec)
Step 7900: loss = 1.31 (0.004 sec)
Training Data Eval:
  Num examples: 55000  Num correct: 36517  Precision @ 1: 0.6639
Validation Data Eval:
  Num examples: 5000  Num correct: 3351  Precision @ 1: 0.6702
Test Data Eval:
  Num examples: 10000  Num correct: 6659  Precision @ 1: 0.6659
Step 8000: loss = 0.91 (0.009 sec)
Step 8100: loss = 0.91 (0.004 sec)
Step 8200: loss = 0.95 (0.004 sec)
Step 8300: loss = 1.14 (0.004 sec)
Step 8400: loss = 1.00 (0.004 sec)
Step 8500: loss = 0.97 (0.004 sec)
Step 8600: loss = 0.73 (0.004 sec)
Step 8700: loss = 1.02 (0.004 sec)
Step 8800: loss = 0.81 (0.069 sec)
Step 8900: loss = 0.85 (0.004 sec)
Training Data Eval:
  Num examples: 55000  Num correct: 37801  Precision @ 1: 0.6873
Validation Data Eval:
  Num examples: 5000  Num correct: 3456  Precision @ 1: 0.6912
Test Data Eval:
  Num examples: 10000  Num correct: 6916  Precision @ 1: 0.6916
Step 9000: loss = 0.59 (0.010 sec)
Step 9100: loss = 1.02 (0.004 sec)
Step 9200: loss = 0.84 (0.004 sec)
Step 9300: loss = 0.79 (0.004 sec)
Step 9400: loss = 0.88 (0.004 sec)
Step 9500: loss = 0.94 (0.004 sec)
Step 9600: loss = 1.05 (0.004 sec)
Step 9700: loss = 0.78 (0.004 sec)
Step 9800: loss = 1.08 (0.004 sec)
Step 9900: loss = 1.12 (0.070 sec)
Training Data Eval:
  Num examples: 55000  Num correct: 38346  Precision @ 1: 0.6972
Validation Data Eval:
  Num examples: 5000  Num correct: 3526  Precision @ 1: 0.7052
Test Data Eval:
  Num examples: 10000  Num correct: 6997  Precision @ 1: 0.6997
[Finished in 60.3s]

# The result of using tf.Variable():
Extracting Mnist_data/train-images-idx3-ubyte.gz
Extracting Mnist_data/train-labels-idx1-ubyte.gz
Extracting Mnist_data/t10k-images-idx3-ubyte.gz
Extracting Mnist_data/t10k-labels-idx1-ubyte.gz
WARNING:tensorflow:Passing a `GraphDef` to the SummaryWriter is deprecated. Pass a `Graph` object instead, such as `sess.graph`.
Step 0: loss = 2.33 (0.013 sec)
Step 100: loss = 2.16 (0.004 sec)
Step 200: loss = 1.90 (0.004 sec)
Step 300: loss = 1.56 (0.004 sec)
Step 400: loss = 1.27 (0.004 sec)
Step 500: loss = 0.84 (0.006 sec)
Step 600: loss = 0.83 (0.004 sec)
Step 700: loss = 0.82 (0.004 sec)
Step 800: loss = 0.56 (0.004 sec)
Step 900: loss = 0.50 (0.005 sec)
Training Data Eval:
  Num examples: 55000  Num correct: 47541  Precision @ 1: 0.8644
Validation Data Eval:
  Num examples: 5000  Num correct: 4351  Precision @ 1: 0.8702
Test Data Eval:
  Num examples: 10000  Num correct: 8708  Precision @ 1: 0.8708
Step 1000: loss = 0.56 (0.009 sec)
Step 1100: loss = 0.49 (0.090 sec)
Step 1200: loss = 0.63 (0.004 sec)
Step 1300: loss = 0.38 (0.004 sec)
Step 1400: loss = 0.35 (0.004 sec)
Step 1500: loss = 0.46 (0.004 sec)
Step 1600: loss = 0.40 (0.006 sec)
Step 1700: loss = 0.47 (0.004 sec)
Step 1800: loss = 0.33 (0.004 sec)
Step 1900: loss = 0.30 (0.004 sec)
Training Data Eval:
  Num examples: 55000  Num correct: 49230  Precision @ 1: 0.8951
Validation Data Eval:
  Num examples: 5000  Num correct: 4511  Precision @ 1: 0.9022
Test Data Eval:
  Num examples: 10000  Num correct: 8998  Precision @ 1: 0.8998
Step 2000: loss = 0.33 (0.009 sec)
Step 2100: loss = 0.33 (0.004 sec)
Step 2200: loss = 0.34 (0.069 sec)
Step 2300: loss = 0.45 (0.004 sec)
Step 2400: loss = 0.31 (0.004 sec)
Step 2500: loss = 0.29 (0.005 sec)
Step 2600: loss = 0.17 (0.004 sec)
Step 2700: loss = 0.23 (0.004 sec)
Step 2800: loss = 0.34 (0.006 sec)
Step 2900: loss = 0.39 (0.004 sec)
Training Data Eval:
  Num examples: 55000  Num correct: 49923  Precision @ 1: 0.9077
Validation Data Eval:
  Num examples: 5000  Num correct: 4585  Precision @ 1: 0.9170
Test Data Eval:
  Num examples: 10000  Num correct: 9114  Precision @ 1: 0.9114
Step 3000: loss = 0.18 (0.007 sec)
Step 3100: loss = 0.25 (0.004 sec)
Step 3200: loss = 0.41 (0.004 sec)
Step 3300: loss = 0.67 (0.071 sec)
Step 3400: loss = 0.28 (0.004 sec)
Step 3500: loss = 0.27 (0.004 sec)
Step 3600: loss = 0.34 (0.004 sec)
Step 3700: loss = 0.50 (0.004 sec)
Step 3800: loss = 0.27 (0.005 sec)
Step 3900: loss = 0.54 (0.004 sec)
Training Data Eval:
  Num examples: 55000  Num correct: 50419  Precision @ 1: 0.9167
Validation Data Eval:
  Num examples: 5000  Num correct: 4627  Precision @ 1: 0.9254
Test Data Eval:
  Num examples: 10000  Num correct: 9205  Precision @ 1: 0.9205
Step 4000: loss = 0.43 (0.009 sec)
Step 4100: loss = 0.31 (0.004 sec)
Step 4200: loss = 0.31 (0.005 sec)
Step 4300: loss = 0.20 (0.004 sec)
Step 4400: loss = 0.24 (0.070 sec)
Step 4500: loss = 0.40 (0.005 sec)
Step 4600: loss = 0.27 (0.004 sec)
Step 4700: loss = 0.32 (0.005 sec)
Step 4800: loss = 0.30 (0.004 sec)
Step 4900: loss = 0.24 (0.004 sec)
Training Data Eval:
  Num examples: 55000  Num correct: 50727  Precision @ 1: 0.9223
Validation Data Eval:
  Num examples: 5000  Num correct: 4656  Precision @ 1: 0.9312
Test Data Eval:
  Num examples: 10000  Num correct: 9284  Precision @ 1: 0.9284
Step 5000: loss = 0.40 (0.010 sec)
Step 5100: loss = 0.22 (0.004 sec)
Step 5200: loss = 0.29 (0.004 sec)
Step 5300: loss = 0.21 (0.004 sec)
Step 5400: loss = 0.29 (0.004 sec)
Step 5500: loss = 0.27 (0.072 sec)
Step 5600: loss = 0.17 (0.004 sec)
Step 5700: loss = 0.14 (0.004 sec)
Step 5800: loss = 0.21 (0.004 sec)
Step 5900: loss = 0.32 (0.004 sec)
Training Data Eval:
  Num examples: 55000  Num correct: 51058  Precision @ 1: 0.9283
Validation Data Eval:
  Num examples: 5000  Num correct: 4677  Precision @ 1: 0.9354
Test Data Eval:
  Num examples: 10000  Num correct: 9329  Precision @ 1: 0.9329
Step 6000: loss = 0.17 (0.010 sec)
Step 6100: loss = 0.35 (0.004 sec)
Step 6200: loss = 0.21 (0.004 sec)
Step 6300: loss = 0.23 (0.004 sec)
Step 6400: loss = 0.15 (0.004 sec)
Step 6500: loss = 0.34 (0.004 sec)
Step 6600: loss = 0.17 (0.074 sec)
Step 6700: loss = 0.29 (0.004 sec)
Step 6800: loss = 0.20 (0.004 sec)
Step 6900: loss = 0.21 (0.004 sec)
Training Data Eval:
  Num examples: 55000  Num correct: 51370  Precision @ 1: 0.9340
Validation Data Eval:
  Num examples: 5000  Num correct: 4697  Precision @ 1: 0.9394
Test Data Eval:
  Num examples: 10000  Num correct: 9387  Precision @ 1: 0.9387
Step 7000: loss = 0.23 (0.007 sec)
Step 7100: loss = 0.25 (0.004 sec)
Step 7200: loss = 0.16 (0.004 sec)
Step 7300: loss = 0.22 (0.004 sec)
Step 7400: loss = 0.12 (0.004 sec)
Step 7500: loss = 0.31 (0.004 sec)
Step 7600: loss = 0.21 (0.004 sec)
Step 7700: loss = 0.16 (0.071 sec)
Step 7800: loss = 0.18 (0.004 sec)
Step 7900: loss = 0.21 (0.004 sec)
Training Data Eval:
  Num examples: 55000  Num correct: 51623  Precision @ 1: 0.9386
Validation Data Eval:
  Num examples: 5000  Num correct: 4725  Precision @ 1: 0.9450
Test Data Eval:
  Num examples: 10000  Num correct: 9409  Precision @ 1: 0.9409
Step 8000: loss = 0.26 (0.008 sec)
Step 8100: loss = 0.23 (0.004 sec)
Step 8200: loss = 0.32 (0.004 sec)
Step 8300: loss = 0.16 (0.004 sec)
Step 8400: loss = 0.18 (0.004 sec)
Step 8500: loss = 0.21 (0.004 sec)
Step 8600: loss = 0.26 (0.004 sec)
Step 8700: loss = 0.17 (0.004 sec)
Step 8800: loss = 0.24 (0.070 sec)
Step 8900: loss = 0.12 (0.004 sec)
Training Data Eval:
  Num examples: 55000  Num correct: 51853  Precision @ 1: 0.9428
Validation Data Eval:
  Num examples: 5000  Num correct: 4739  Precision @ 1: 0.9478
Test Data Eval:
  Num examples: 10000  Num correct: 9444  Precision @ 1: 0.9444
Step 9000: loss = 0.26 (0.009 sec)
Step 9100: loss = 0.13 (0.004 sec)
Step 9200: loss = 0.18 (0.004 sec)
Step 9300: loss = 0.21 (0.004 sec)
Step 9400: loss = 0.13 (0.004 sec)
Step 9500: loss = 0.17 (0.004 sec)
Step 9600: loss = 0.22 (0.004 sec)
Step 9700: loss = 0.10 (0.004 sec)
Step 9800: loss = 0.23 (0.004 sec)
Step 9900: loss = 0.14 (0.073 sec)
Training Data Eval:
  Num examples: 55000  Num correct: 52047  Precision @ 1: 0.9463
Validation Data Eval:
  Num examples: 5000  Num correct: 4757  Precision @ 1: 0.9514
Test Data Eval:
  Num examples: 10000  Num correct: 9481  Precision @ 1: 0.9481
[Finished in 61.5s]
"
6889,How to load a graph with tensorflow.so and c_api.h in c++ language?,"I can not find any example about how to load graph with tensorflow.so and c_api.h in c++ language.  I read the c_api.h , however, the ReadBinaryProto function was not in it. how can I load a graph without ReadBinaryProto function?"
6885,Rename parameter in Exponential distribution,"Current

```python
distributions.Exponential(lam=2.0)
```
I had to think a bit before figuring out that `lam` stands for `lambda`. Following the MLP book I think a better name would be `rate` or `average_rate`. 

> This distribution describes the times between events in a process in which events occur continuously and independently at a constant average rate _lambda_ 

```python
distributions.Exponential(rate=2.0)
```

Numpy uses another parameterization: https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.exponential.html

"
6884,Dead link to Graph.control_dependencies(),"""tf.control_dependencies(control_inputs) {#control_dependencies}

Wrapper for Graph.control_dependencies() using the default graph.
See **Graph.control_dependencies()** for more details.""

The link to **Graph.control_dependencies()** is dead, but I could really need the details. : 3

(Hope this is the right place the post this. X: )
"
6883,ERROR: Assign requires shapes of both tensors to match. lhs shape= [1000] rhs shape= [1001],"Searched for possible solutions, this page references using --Label-offsets=1 to use vgg or ResNet, here however I'm using InceptionV3 with imagenet TFRecord format, and the failure message occurs during initialization of the session.

Operating System: Windows 10/64 or LINUX

Installed version of CUDA and cuDNN: 
cublas64_80.dll locally
cudnn64_5.dll
cufft64_80.dll
nvcuda.dll locally
curand64_80.dll locally

If installed from binary pip package, provide:
0.12.1


CODE:
    init_fn = slim.assign_from_checkpoint_fn(model_path,slim.get_model_variables('InceptionV3'))

    with tf.Session() as sess:
        # Load weights
        init_fn(sess)


ERROR MESSAGE
InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [1000] rhs shape= [1001]
         [[Node: save/Assign_8 = Assign[T=DT_FLOAT, _class=[""loc:@InceptionV3/AuxLogits/Conv2d_2b_1x1/biases""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](InceptionV3/AuxLogits/Conv2d_2b_1x1/biases, save/RestoreV2_8)]]
"
6882,CPU version  AttributeError: 'module' object has no attribute ‘pack’,"when I was trying training a fcn with the latest cpu version tensoflow,I got an error:

> setting up vgg initialized conv layers ...
Traceback (most recent call last):
  File ""FCN.py"", line 285, in <module>
    tf.app.run()
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""FCN.py"", line 154, in main
    pred_annotation, logits = inference(image, keep_probability)
  File ""FCN.py"", line 129, in inference
    deconv_shape3 = tf.pack([shape[0], shape[1], shape[2], NUM_OF_CLASSESS])
AttributeError: 'module' object has no attribute 'pack'


My tensorflow is installed from tensorflow-0.12.1-cp27-none-linux_x86_64.whl.  Seemingly there's no pack function of tf or in other module? "
6880,tf.gfile.GFile issue on windows,"When I use tf.gfile.GFile and set mode='rb' to open a file on windows, it will read as 'str' instead of 'byte'. For example,

f = tf.gfile.GFile(""d2f42068.ini"", mode='rb')
f.readline()
The result shows
'<KVList>\n'.
It should be 
b'<KVList>\n'

I find the gfile.py replace the 'r' to empty.
def __init__(self, name, mode='r'):
    mode = mode.replace('b', '')
    super(FastGFile, self).__init__(name=name, mode=mode)

This issue will cause some error of data reading on windows.

My OS is win10 x64. The version of tensorflow is r0.12. "
6879,TensorBoard variable name formatting,"Variable names in TensorBoard going from 0.12rc0 to the latest 0.12.1 do not support spaces and special characters anymore. 

I would like to have spaces and percentage signs (for example) supported (again).

Below should say: `cpu->gpus FIFO capacity %`

![image](https://cloud.githubusercontent.com/assets/7721540/21986760/5cbc0866-dc09-11e6-9f81-c4761af3cb90.png)
"
6878,Symlink loop FATAL error,"Hello all,
I started teaching myself some tensor flow, and i faced a problem when trying to retrain an existing image classification model with a new class following that tutorial:
https://www.tensorflow.org/how_tos/image_retraining/

Since i installed tensorflow through pip and after reading a few issues, i had to download and install tensorflow from the source. when i got to configuring the install (calling ./configure) it gave me the following error:

```
____Loading package: tensorflow/contrib/lookup
____Loading package: tensorflow/java/src/main/native
____Loading package: tensorflow/tools/quantization
____Loading package: tensorflow/compiler/aot
____Loading package: tensorflow/core
____Loading package: tensorflow/contrib/quantization
____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 1,508,324 bytes
____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 3,342,484 bytes
ERROR: infinite symlink expansion detected
[start of symlink chain]
/usr/local/Cellar/python/2.7.12_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages
/usr/local/lib/python2.7/site-packages
/usr/local/lib/python2.7/site-packages/tensorflow/util/python/python_lib
[end of symlink chain]
```

Is there any particular way i can resolve this? I would really appreciate any input on the matter.

Thanks!


"
6877,TF-slim: slim.evaluation.evaluation_loop results in code run that is stuck without progress.,"Hi all, I am unsure why the evaluation loop would result in the code not running. My GPU memory is not consumed or whatsoever, and my CPU memory usage is at a minimum. This is what I get:

```
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
Starting Evaluation of Validation Dataset
predictions type: Tensor(""Cast_2:0"", shape=(?,), dtype=float32)
Running evaluation Loop...
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/evaluation.py:426 in evaluation_loop.: __init__ (from tensorflow.python.training.summary_io) is deprecated and will be removed after 2016-11-30.
Instructions for updating:
Please switch to tf.summary.FileWriter. The interface and behavior is the same; this is just a rename.
INFO:tensorflow:Waiting for new checkpoint at ../preprocessedData/model.ckpt-50
```

All I did was to run this:

```
print 'Starting Evaluation of Validation Dataset'
#Load the validation dataset for evaluation while training
test_dataset = get_split('validation', train_dir)
test_images, _, test_labels = load_batch(test_dataset, height = image_size, batch_size = batch_size, width = image_size, is_training = False)

with slim.arg_scope(inception_resnet_v2_arg_scope()):
	logits, _ = inception_resnet_v2(test_images, num_classes = test_dataset.num_classes, is_training = False)

predictions = tf.argmax(logits, 1)
predictions = tf.cast(predictions, tf.float32)
print 'predictions type:', predictions
# print tf.Print(predictions)

# Define the metrics:
names_to_values, names_to_updates = slim.metrics.aggregate_metric_map({
	'streaming_auc': slim.metrics.streaming_auc(predictions, test_labels),
    # 'eval/Accuracy': slim.metrics.streaming_accuracy(predictions, test_labels),
    # 'eval/Recall@5': slim.metrics.streaming_recall_at_k(logits, test_labels, 5),
})

print'Running evaluation Loop...'
checkpoint_path = tf.train.latest_checkpoint(log_dir)
metric_values = slim.evaluation.evaluation_loop(
    master='',
    checkpoint_dir=checkpoint_path,
    logdir=log_dir,
    eval_op=names_to_updates.values(),
    final_op=names_to_values.values())
```
It's been a really long time since there is no progress. Is the function working?"
6876,Python 3.6 support,"Run pip install tensorflow under python 3.6 get following:
 pip install tensorflow
Collecting tensorflow
  Could not find a version that satisfies the requirement tensorflow (from versions: )
No matching distribution found for tensorflow"
6875,Tensorflow require Protobuf 3.1 while attempting to use incompatible 3.0 function calls,"**Environment:**
Arch Linux with CUDA 8.0 and cuDNN 5.1

```
/opt/cuda/lib64/libcudadevrt.a
/opt/cuda/lib64/libcudart.so -> libcudart.so.8.0
/opt/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44
/opt/cuda/lib64/libcudart.so.8.0.44
/opt/cuda/lib64/libcudart_static.a
/opt/cuda/lib64/libcudnn.so -> libcudnn.so.5
/opt/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5
/opt/cuda/lib64/libcudnn.so.5.0.5
/opt/cuda/lib64/libcudnn.so.5.1.5
/opt/cuda/lib64/libcudnn_static.a
```

Tensorflow was succesfully compiled from source for a C++ pipeline using
**Bazel** 0.4.3
**Protobuf** 3.1.x (also tried 3.2.x)
**Tensorflow** r1.0 (as well as master <9830ed87d46245a72a9d0c2dce854e6732c5542a>)

with the command: 
`bazel build -c opt --config=cuda --copt=-march=native --verbose_failures //tensorflow/core:tensorflow //tensorflow/cc:cc_ops tensorflow:libtensorflow_cc.so`

In _my_ CPP pipeline, the following include breaks the compile process
`#include <tensorflow/core/public/session.h>`

```
/srv/builds/tensorflow/bazel-genfiles/tensorflow/core/protobuf/config.pb.h:1490:95: error: no matching function for call to ‘google::protobuf::internal::ArenaStringPtr::Get(const string*) const’
 return visible_device_list_.Get(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
                                                                                             ^
In file included from /srv/builds/tensorflow/bazel-genfiles/tensorflow/core/framework/graph.pb.h:23:0,
                 from /srv/builds/tensorflow/tensorflow/core/public/session.h:22,
                 from src/fragments/tensorflow.h:48,
                 from src/blocks/detector.h:27,
                 from src/json_deserializer.h:22,
                 from src/blocks/assign_camera.cpp:2:
/usr/include/google/protobuf/arenastring.h:66:31: note: candidate: const string& google::protobuf::internal::ArenaStringPtr::Get() const
   inline const ::std::string& Get() const { return *ptr_; }
                               ^~~
/usr/include/google/protobuf/arenastring.h:66:31: note:   candidate expects 0 arguments, 1 provided
```

----------------------------------------------

Digging in the documentation, I see for the python part that Protobuf 3.1.x is required, however 3.1.x had an API change from 3.0.X for the ArenaStringPtr::Get(), where the former takes a string as argument while the newer doesn't. I could downgrade to Protobuf 3.0.X, but macro's in e.g. error_codes.pb.h specifies a requirement for 3.1.X `#if 3001000 < GOOGLE_PROTOBUF_MIN_PROTOC_VERSION`"
6873,dynamic_rnn issue:,"when I run a copy of code based on tensorflow, it gives following error, what should I do next ?

Caused by op u'RNN/Assert/AssertGuard/Assert', defined at:
  File ""lstm_and_ctc_ocr_train.py"", line 188, in <module>
    train()
  File ""lstm_and_ctc_ocr_train.py"", line 78, in train
    logits, inputs, targets, seq_len, W, b = model.get_train_model()
  File ""/home/sanjie/projects/tensorflow_lstm_ctc_ocr_ywh/model.py"", line 111, in get_train_model
    outputs, _ = tf.nn.dynamic_rnn(cell, inputs, seq_len, dtype=tf.float32)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py"", line 823, in dynamic_rnn
    [_assert_has_shape(sequence_length, [batch_size])]):
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py"", line 818, in _assert_has_shape
    packed_shape, "" but saw shape: "", x_shape])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 140, in Assert
    condition, no_op, true_assert, name=""AssertGuard"")
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 1717, in cond
    _, res_f = context_f.BuildCondBranch(fn2)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 1613, in BuildCondBranch
    r = fn()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 137, in true_assert
    condition, data, summarize, name=""Assert"")
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_logging_ops.py"", line 37, in _assert
    summarize=summarize, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 756, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2380, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1298, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): assertion failed: [Expected shape for Tensor sequence_length:0 is ] [64] [ but saw shape: ] [85]
"
6871,Windows `uname -s` might return different values,"[`configure#L14`](https://github.com/tensorflow/tensorflow/blob/73bc42890/configure#L14) checks if the system is Windows, but doesn't check `uname -s` values correctly according to the [wiki page](https://en.wikipedia.org/wiki/Uname#Examples).

Unless not supported, I think other Windows SDE's should be included in the check."
6870,POSIX compliance,"This is a non-urgent issue that might require a lot of changes

I was looking into modifying some TF files to make it work on BSD, and noticed that a lot of files ignore the POSIX compliance. Quick example, `configure` is written in Bourne shell, which is not POSIX.

I think it is important to write everything with POSIX in mind, because later on if TF would need to add support for other OS's, it might be harder.

For example if you run `checkbashisms` on `configure`, that what you get:

```
possible bashism in ./configure line 7 ((push|pop)d):
pushd `dirname $0` > /dev/null
possible bashism in ./configure line 9 ((push|pop)d):
popd > /dev/null
possible bashism in ./configure line 12 ('function' is useless):
function is_windows() {
possible bashism in ./configure line 14 (alternative test command ([[ foo ]] should be [ foo ])):
  if [[ ""${PLATFORM}"" =~ msys_nt* ]]; then
possible bashism in ./configure line 21 ('function' is useless):
function bazel_clean_and_fetch() {
possible bashism in ./configure line 40 (read with option other than -r):
    read -p ""Please specify the location of python. [Default is $default_python_bin_path]: "" PYTHON_BIN_PATH
possible bashism in ./configure line 64 (should be 'b = a'):
while [ ""$TF_NEED_JEMALLOC"" == """" ]; do
possible bashism in ./configure line 66 (read with option other than -r):
  read -p ""Do you wish to use jemalloc as the malloc implementation? ""\
""(Linux only) [Y/n] "" INPUT
possible bashism in ./configure line 75 (should be 'b = a'):
if [ ""$TF_NEED_JEMALLOC"" == ""1"" ]; then
possible bashism in ./configure line 81 (should be 'b = a'):
while [ ""$TF_NEED_GCP"" == """" ]; do
possible bashism in ./configure line 83 (read with option other than -r):
  read -p ""Do you wish to build TensorFlow with ""\
""Google Cloud Platform support? [y/N] "" INPUT
possible bashism in ./configure line 95 (should be 'b = a'):
if [ ""$TF_NEED_GCP"" == ""1"" ]; then
possible bashism in ./configure line 98 (alternative test command ([[ foo ]] should be [ foo ])):
  if [[ $(uname -a) =~ Linux ]] && [[ ! -f ""/usr/include/curl/curl.h"" ]]; then
possible bashism in ./configure line 111 (should be 'b = a'):
while [ ""$TF_NEED_HDFS"" == """" ]; do
possible bashism in ./configure line 113 (read with option other than -r):
  read -p ""Do you wish to build TensorFlow with ""\
""Hadoop File System support? [y/N] "" INPUT
possible bashism in ./configure line 125 (should be 'b = a'):
if [ ""$TF_NEED_HDFS"" == ""1"" ]; then
possible bashism in ./configure line 134 (should be 'b = a'):
while [ ""$TF_ENABLE_XLA"" == """" ]; do
possible bashism in ./configure line 135 (read with option other than -r):
  read -p ""Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] "" INPUT
possible bashism in ./configure line 144 (should be 'b = a'):
if [ ""$TF_ENABLE_XLA"" == ""1"" ]; then
possible bashism in ./configure line 163 (should be 'b = a'):
while [ ""$TF_NEED_OPENCL"" == """" ]; do
possible bashism in ./configure line 164 (read with option other than -r):
  read -p ""Do you wish to build TensorFlow with OpenCL support? [y/N] "" INPUT
possible bashism in ./configure line 175 (should be 'b = a'):
while [ ""$TF_NEED_CUDA"" == """" ]; do
possible bashism in ./configure line 176 (read with option other than -r):
  read -p ""Do you wish to build TensorFlow with CUDA support? [y/N] "" INPUT
possible bashism in ./configure line 187 (alternative test command ([[ foo ]] should be [ foo ])):
if [[ ""$TF_NEED_CUDA"" == ""0"" ]] && [[ ""$TF_NEED_OPENCL"" == ""0"" ]]; then
possible bashism in ./configure line 187 (should be 'b = a'):
if [[ ""$TF_NEED_CUDA"" == ""0"" ]] && [[ ""$TF_NEED_OPENCL"" == ""0"" ]]; then
possible bashism in ./configure line 193 (should be 'b = a'):
if [ ""$TF_NEED_CUDA"" == ""1"" ]; then
possible bashism in ./configure line 200 (read with option other than -r):
    read -p ""Please specify which gcc should be used by nvcc as the host compiler. [Default is $default_gcc_host_compiler_path]: "" GCC_HOST_COMPILER_PATH
possible bashism in ./configure line 224 (read with option other than -r):
    read -p ""Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: "" TF_CUDA_VERSION
possible bashism in ./configure line 237 (read with option other than -r):
    read -p ""Please specify the location where CUDA $TF_CUDA_VERSION toolkit is installed. Refer to README.md for more details. [Default is $default_cuda_path]: "" CUDA_TOOLKIT_PATH
possible bashism in ./configure line 244 (alternative test command ([[ foo ]] should be [ foo ])):
  if [[ -z ""$TF_CUDA_VERSION"" ]]; then
possible bashism in ./configure line 252 (should be 'b = a'):
  elif [ ""$OSNAME"" == ""Linux"" ]; then
possible bashism in ./configure line 254 (should be 'b = a'):
  elif [ ""$OSNAME"" == ""Darwin"" ]; then
possible bashism in ./configure line 277 (read with option other than -r):
    read -p ""Please specify the Cudnn version you want to use. [Leave empty to use system default]: "" TF_CUDNN_VERSION
possible bashism in ./configure line 283 (read with option other than -r):
    read -p ""Please specify the location where cuDNN $TF_CUDNN_VERSION library is installed. Refer to README.md for more details. [Default is $default_cudnn_path]: "" CUDNN_INSTALL_PATH
possible bashism in ./configure line 293 (alternative test command ([[ foo ]] should be [ foo ])):
  if [[ -z ""$TF_CUDNN_VERSION"" ]]; then
possible bashism in ./configure line 300 (should be 'b = a'):
    elif [ ""$OSNAME"" == ""Linux"" ]; then
possible bashism in ./configure line 303 (should be 'b = a'):
    elif [ ""$OSNAME"" == ""Darwin"" ]; then
possible bashism in ./configure line 320 (alternative test command ([[ foo ]] should be [ foo ])):
    if [[ ""$REALVAL"" =~ .so[.]+([0-9]*) ]]; then
possible bashism in ./configure line 321 ($BASH_SOMETHING):
      TF_CUDNN_EXT="".""${BASH_REMATCH[1]}
possible bashism in ./configure line 321 (bash arrays, ${name[0|*|@]}):
      TF_CUDNN_EXT="".""${BASH_REMATCH[1]}
possible bashism in ./configure line 322 ($BASH_SOMETHING):
      TF_CUDNN_VERSION=${BASH_REMATCH[1]}
possible bashism in ./configure line 322 (bash arrays, ${name[0|*|@]}):
      TF_CUDNN_VERSION=${BASH_REMATCH[1]}
possible bashism in ./configure line 324 (alternative test command ([[ foo ]] should be [ foo ])):
    elif [[ ""$REALVAL"" =~ ([0-9]*).dylib ]]; then
possible bashism in ./configure line 325 ($BASH_SOMETHING):
      TF_CUDNN_EXT=${BASH_REMATCH[1]}"".dylib""
possible bashism in ./configure line 325 (bash arrays, ${name[0|*|@]}):
      TF_CUDNN_EXT=${BASH_REMATCH[1]}"".dylib""
possible bashism in ./configure line 326 ($BASH_SOMETHING):
      TF_CUDNN_VERSION=${BASH_REMATCH[1]}
possible bashism in ./configure line 326 (bash arrays, ${name[0|*|@]}):
      TF_CUDNN_VERSION=${BASH_REMATCH[1]}
possible bashism in ./configure line 330 (should be 'b = a'):
    if [ ""$OSNAME"" == ""Darwin"" ]; then
possible bashism in ./configure line 340 (should be 'b = a'):
  elif [ ""$OSNAME"" == ""Linux"" ]; then
possible bashism in ./configure line 343 (should be 'b = a'):
  elif [ ""$OSNAME"" == ""Darwin"" ]; then
possible bashism in ./configure line 354 (should be 'b = a'):
  if [ ""$OSNAME"" == ""Linux"" ]; then
possible bashism in ./configure line 365 (should be 'b = a'):
  if [ ""$OSNAME"" == ""Linux"" ]; then
possible bashism in ./configure line 388 (read with option other than -r):
    read -p ""[Default is: \""3.5,5.2\""]: "" TF_CUDA_COMPUTE_CAPABILITIES
possible bashism in ./configure line 395 (${parm/?/pat[/str]}):
  COMPUTE_CAPABILITIES=${TF_CUDA_COMPUTE_CAPABILITIES//,/ }
possible bashism in ./configure line 398 (alternative test command ([[ foo ]] should be [ foo ])):
    if [[ ! ""$CAPABILITY"" =~ [0-9]+.[0-9]+ ]]; then
possible bashism in ./configure line 404 (should be 'b = a'):
  if [ ""$ALL_VALID"" == ""0"" ]; then
possible bashism in ./configure line 430 (should be 'b = a'):
if [ ""$TF_NEED_OPENCL"" == ""1"" ]; then
possible bashism in ./configure line 437 (read with option other than -r):
    read -p ""Please specify which C++ compiler should be used as the host C++ compiler. [Default is $default_cxx_host_compiler]: "" HOST_CXX_COMPILER
possible bashism in ./configure line 460 (read with option other than -r):
    read -p ""Please specify which C compiler should be used as the host C compiler. [Default is $default_c_host_compiler]: "" HOST_C_COMPILER
possible bashism in ./configure line 485 (read with option other than -r):
    read -p ""Please specify the location where ComputeCpp for SYCL $TF_OPENCL_VERSION is installed. [Default is $default_computecpp_toolkit_path]: "" COMPUTECPP_TOOLKIT_PATH
possible bashism in ./configure line 492 (should be 'b = a'):
  if [ ""$OSNAME"" == ""Linux"" ]; then
```
"
6869,Confusion regarding `jemalloc`,"@jhseu In one of the recent commits (83c6e0c), `jemalloc` was introduced. The [blame comment](https://github.com/tensorflow/tensorflow/commit/83c6e0c63acdcab2c58c4ed7220bfa58879b1d57) says that it is ""Only enabled on Linux for now"". However, it defaults to `enabled` for any OS. If it is intended to work for Linux only, I think it should default to `disabled`, or should ask only if detected Linux-based system (i.e. `$(uname -a | tr 'A-Z' 'a-z') =~ linux`).

The main confusion is that if I run the `configure` from Unix-based (e.g. OS X) or Windows, and just zoom through the config requests, it will try enabling `jemalloc`, and doesn't have a check for OS in the [`configure`](https://github.com/tensorflow/tensorflow/blob/73bc42890/configure)."
6865,zlib removed from repository url mentioned in build,"I was building docker container for tensorflow r0.11 branch and it throws error because it repository url for package zlib mentioned in build file is no longer available on that url. Here is snippet for error.

```ERROR: /tensorflow/tensorflow/core/BUILD:903:1: no such package '@zlib_archive//': Error downloading 
from http://zlib.net/zlib-1.2.8.tar.gz to /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4
cff9/external/zlib_archive: Error downloading http://zlib.net/zlib-1.2.8.tar.gz to /root/.cache/bazel
/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/zlib_archive/zlib-1.2.8.tar.gz: 404 Not Found:
 <!DOCTYPE HTML PUBLIC ""-//IETF//DTD HTML 2.0//EN"">                                                  
<html><head>                                                                                         
<title>404 Not Found</title>                                                                         
</head><body>                                                                                        
<h1>Not Found</h1>                                                                                   
<p>The requested URL /zlib-1.2.8.tar.gz was not found on this server.</p>                            
<p>Additionally, a 404 Not Found                                                                     
error was encountered while trying to use an ErrorDocument to handle the request.</p>                
</body></html>                                                                                       
 and referenced by '//tensorflow/core:lib_internal'.                                                 
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.  
____Elapsed time: 3.516s                                                                             
```"
6863,Protobuf import issue,"I am trying to run tensorboard but I am getting the following import error on mac osx;

`AttributeError: module 'pkg_resources' has no attribute 'declare_namespace'`

I have tried reinstalling setuptools and distribute. This is coming up as an error in python 3.5 even though my tensorflow is installed in 2.7; I.e. 

```
 File ""/Library/Frameworks/Python.framework/Versions/3.5/bin/tensorboard"", line 7, in <module>
    from tensorflow.tensorboard.tensorboard import main
```"
6862,Error when tensorflow installed with conda virtual environment in windows 10,"### Environment info
Operating System:
    windows 10

I was trying to install tensorflow in `windows 10` with the guidance of the `readme.md`.
Because the version of `python` in my pc is `2.7.X` which is not compatiable with the guidance as it say  `python version=3.5`. Therefore, I use `conda create --name tensorflow python=3.5` to create a virtual environmnt and install tensorflow.

However, an error occurs:
Traceback (most recent call last):
  File ""D:\Program Files\anaconda\envs\tensorflow\Scripts\pip-script.py"", line 5, in <module>
    sys.exit(pip.main())
  File ""D:\Program Files\anaconda\envs\tensorflow\lib\site-packages\pip\__init__.py"", line 249, in main
    return command.main(cmd_args)
  File ""D:\Program Files\anaconda\envs\tensorflow\lib\site-packages\pip\basecommand.py"", line 252, in main
    pip_version_check(session)
  File ""D:\Program Files\anaconda\envs\tensorflow\lib\site-packages\pip\utils\outdated.py"", line 102, in pip_version_check
    installed_version = get_installed_version(""pip"")
  File ""D:\Program Files\anaconda\envs\tensorflow\lib\site-packages\pip\utils\__init__.py"", line 838, in get_installed_version
    working_set = pkg_resources.WorkingSet()
  File ""D:\Program Files\anaconda\envs\tensorflow\lib\site-packages\pip\_vendor\pkg_resources\__init__.py"", line 644, in __init__
    self.add_entry(entry)
  File ""D:\Program Files\anaconda\envs\tensorflow\lib\site-packages\pip\_vendor\pkg_resources\__init__.py"", line 700, in add_entry
    for dist in find_distributions(entry, True):
  File ""D:\Program Files\anaconda\envs\tensorflow\lib\site-packages\pip\_vendor\pkg_resources\__init__.py"", line 1949, in find_eggs_in_zip
    if metadata.has_metadata('PKG-INFO'):
  File ""D:\Program Files\anaconda\envs\tensorflow\lib\site-packages\pip\_vendor\pkg_resources\__init__.py"", line 1463, in has_metadata
    return self.egg_info and self._has(self._fn(self.egg_info, name))
  File ""D:\Program Files\anaconda\envs\tensorflow\lib\site-packages\pip\_vendor\pkg_resources\__init__.py"", line 1823, in _has
    return zip_path in self.zipinfo or zip_path in self._index()
  File ""D:\Program Files\anaconda\envs\tensorflow\lib\site-packages\pip\_vendor\pkg_resources\__init__.py"", line 1703, in zipinfo
    return self._zip_manifests.load(self.loader.archive)
  File ""D:\Program Files\anaconda\envs\tensorflow\lib\site-packages\pip\_vendor\pkg_resources\__init__.py"", line 1643, in load
    mtime = os.stat(path).st_mtime
FileNotFoundError: [WinError 2] 系统找不到指定的文件。: 'D:\\Program Files\\anaconda\\envs\\tensorflow\\lib\\site-packages\\setuptools-27.2.0-py3.5.egg'

Besides, the ananconda is installed in `'D:\\Program Files\\anaconda`, while there is no file folder `D:\\Program Files\\anaconda\\envs\\tensorflow\\lib`.
After searching in stackoverflow, I didn't find the same problem, since most guys just install tf in other systems. 

Is it that conda virtual environment of `python=3.5` not  compatiable with tf in windows?? And why tf in windows doesn't support python 2.7?? 

Thanks!"
6861,Strange bug with tf.nn.nce_loss,"So for tf.nn.nce_loss for word2vec_basic, if I explicitly use keyword arguments, such as:
      tf.nn.nce_loss(weights=nce_weights,
                     biases=nce_biases,
                     labels=train_labels,
                     inputs=embed,
                     num_sampled=num_sampled,
                     num_classes=vocabulary_size))

everything is fine. But if I get rid of the keywords:
      tf.nn.nce_loss(nce_weights, nce_biases, train_labels, embed, num_sampled, vocabulary_size))

I run into the bug: 
`  File "".../tensorflow/python/ops/nn.py"", line 1336, in nce_loss
    name=name)
  File "".../tensorflow/python/ops/nn.py"", line 1198, in _compute_sampled_logits
    array_ops.reshape(true_w, new_true_w_shape))
  File "".../tensorflow/python/ops/gen_math_ops.py"", line 1613, in mul
    result = _op_def_lib.apply_op(""Mul"", x=x, y=y, name=name)
  File "".../tensorflow/python/framework/op_def_library.py"", line 521, in apply_op
    inferred_from[input_arg.type_attr]))
TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'.`

I looked into the source code for nce_loss but couldn't seem to find the cause of this bug. Can anyone help me with this? Thanks a lot!"
6860,Problems with generate_batch() in word2vec_basic.py,"Does the generate_batch() function miss some possible (word, context) pairs? The sliding window always skips the end of a batch.

For example, if I change the demonstration codes followed by this function into
```
batch, labels = generate_batch(batch_size=4, num_skips=2, skip_window=1)
for i in range(4):
  print(batch[i], reverse_dictionary[batch[i]],
        '->', labels[i, 0], reverse_dictionary[labels[i, 0]])

print('-'*40)
batch, labels = generate_batch(batch_size=4, num_skips=2, skip_window=1)
for i in range(4):
  print(batch[i], reverse_dictionary[batch[i]],
        '->', labels[i, 0], reverse_dictionary[labels[i, 0]])
```
, the sampled pairs would be:
```
3083 originated -> 5242 anarchism
3083 originated -> 12 as
12 as -> 3083 originated
12 as -> 6 a
----------------------------------------
3134 abuse -> 2 of
3134 abuse -> 46 first
46 first -> 3134 abuse
46 first -> 59 used
```

Note that the sampled sentence is:
`Sample data [5242, 3083, 12, 6, 195, 2, 3134, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']`
, pairs such like `a -> term`, `term -> of`, `of -> abuse` are skipped."
6858,Support for mixed precision gradients,"To save memory while keeping precision it's useful to be allow functions that store activations as fp16, but compute gradients in fp32. We are working on custom ops that compute gradients in mixed mode, but it requires hacks on Python gradient type checks side to working. The feature request is to relax these checks.

Here's an example computation graph with mixed precision gradient. Forward propagation is done in fp16, and backprop is in fp32 as in the graph below 
![mixed-grads](https://cloud.githubusercontent.com/assets/23068/21962339/52c99a62-dad8-11e6-99a8-d034abd6c1af.png)


Here's a toy example that creates such a g

Graph:

```
import tensorflow as tf
from tensorflow.python.framework import function

@function.Defun(tf.float16, tf.float32)
def custom_grad(x, grad):
    return 2*tf.cast(x, tf.float32)*grad

@function.Defun(tf.float16, grad_func=custom_grad)
def custom(x):
    return tf.square(x)

x = tf.Variable(1., dtype=tf.float16)

# override cast to keep first backprop in fp32 
with tf.get_default_graph().gradient_override_map({""Cast"": ""Identity""}):
    loss = tf.cast(custom(x, name=""mycustom_apply""), tf.float32)
    
gradient = tf.gradients(loss, x)

sess = tf.Session()
sess.run(tf.global_variables_initializer())
sess.run(gradient)
```

Currently the example above crashes because various places in code assumes that grad function output matches type of incoming activation.

in particular:

- [tensorflow/python/ops/gradients_impl.py](https://github.com/tensorflow/tensorflow/blob/cb17d1b0e2b581b5da1d9597b7929ba764749d38/tensorflow/python/ops/gradients_impl.py#L260), line 264, in _VerifyGeneratedGradients: enforces that gradient output type is the same as activation input type"
6855,Installation error from source code with cuda7.5,"For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### Environment info
Operating System:
     Ubuntu 14.04

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
~$ ls -l /usr/local/cuda/lib64/libcud*
-rw-r--r-- 1 root root 322936  8月 16  2015 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root     16  8月 15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5
lrwxrwxrwx 1 root root     19  8月 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18
-rwxr-xr-x 1 root root 383336  8月 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18
-rw-r--r-- 1 root root 720192  8月 16  2015 /usr/local/cuda/lib64/libcudart_static.a

【cudnn】:
/usr/local/include/cudnn.h
/usr/local/lib/libcudnn.so.5
/usr/local/lib/libcudnn.so.5.0.5

/home/guorui/soft/cudnn_v5/cuda/include/cudnn.h
/home/guorui/soft/cudnn_v5/cuda/lib64/libcudnn.so
/home/guorui/soft/cudnn_v5/cuda/lib64/libcudnn.so.5
/home/guorui/soft/cudnn_v5/cuda/lib64/libcudnn.so.5.0.5

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
tensorflow version: download 2017.1.12
2. The output of `bazel version`
bazel-0.4.3-installer-linux-x86_64.sh


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
$ ./configure
Please specify the location of python. [Default is /usr/bin/python]: 
Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] N
No Google Cloud Platform support will be enabled for TensorFlow
Do you wish to build TensorFlow with Hadoop File System support? [y/N] N
No Hadoop File System support will be enabled for TensorFlow
Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] N
No XLA JIT support will be enabled for TensorFlow
Found possible Python library paths:
  /usr/local/lib/python2.7/dist-packages
  /usr/lib/python2.7/dist-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]

Using python library path: /usr/local/lib/python2.7/dist-packages
Do you wish to build TensorFlow with OpenCL support? [y/N] N
No OpenCL support will be enabled for TensorFlow
Do you wish to build TensorFlow with CUDA support? [y/N] y
CUDA support will be enabled for TensorFlow
Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 
Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 
Please specify the location where CUDA  toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 
Please specify the Cudnn version you want to use. [Leave empty to use system default]: 
Please specify the location where cuDNN  library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /home/guorui/soft/cudnn_v5/cuda
libcudnn.so resolves to libcudnn.5
Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size.
[Default is: ""3.5,5.2""]: 5.2

#Create the pip package and install
bazel build -c opt --config=cuda --local_resources 2048,.5,1.0 //tensorflow/tools/pip_package:build_pip_package

【error message】:
INFO: From Compiling tensorflow/core/kernels/cwise_op_gpu_mul.cu.cc:
Killed
ERROR: /home/guorui/soft/code/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2081:1: output 'tensorflow/core/kernels/_objs/cwise_op_gpu/tensorflow/core/kernels/cwise_op_gpu_mul.cu.pic.o' was not created.
ERROR: /home/guorui/soft/code/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2081:1: not all outputs were created or valid.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1555.053s, Critical Path: 1466.64s


Thank you very much.
"
6854,XLA: random numbers are the same across `session.run` calls,"Not sure if that's intended, but that changes behavior of training pipelines:

```
from tensorflow.contrib.compiler import jit
jit_scope = jit.experimental_jit_scope
with jit_scope(compile_ops=True):
    x = tf.random_uniform(())
    
sess = tf.Session()
print(sess.run(x))
print(sess.run(x))
```

Output:
```
0.768917
0.768917
```"
6850,cudnn.h header not found on Ubuntu when using NVidia's CUDNN .debs,"The official libcudnn5-dev package from NVidia (developers.nvidia.com), version 5.1.5-1+cuda-8.0, places the cudnn.h header at
`/usr/include/x86_64-linux-gnu/cudnn_v5.h`

but Tensorflow's config script looks for it at:
`/usr/lib/x86_64-linux-gnu/include/cudnn.h`

which means that running Tensorflow's `configure` script fails.

A workaround is to add a symlink:
```
sudo mkdir /usr/lib/x86_64-linux-gnu/include
sudo ln -s /usr/include/x86_64-linux-gnu/cudnn_v5.h /usr/lib/x86_64-linux-gnu/include/cudnn.h
```
after which Tensorflow's `configure` script completes successfully."
6849,Regression error (worked in 0.8): DNNRegressor no longer supports multiple output neurons ,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

http://stackoverflow.com/questions/34224826/skflow-regression-predict-multiple-values

### Environment info
Operating System:
Mac
Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
Tried in both:
0.12.1
1.0.0-rc1
If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

I had implemented code based in TF 0.8 that used multiple output neurons.  I based it on this stack overflow question:

 http://stackoverflow.com/questions/34224826/skflow-regression-predict-multiple-values

It looks like this has been broken since at least 0.10 based on comments above.  I accounted for all of the TF breaking changes and upgraded the code provided by @ilblackdragon and came up with:
```
import numpy as np
import tensorflow.contrib.learn as skflow
import tensorflow as tf
from sklearn.metrics import mean_squared_error

# Create random dataset.
rng = np.random.RandomState(1)
X = np.sort(200 * rng.rand(100, 1) - 100, axis=0)
y = np.array([np.pi * np.sin(X).ravel(), np.pi * np.cos(X).ravel()]).T

# Fit regression DNN model.
feature_columns = [tf.contrib.layers.real_valued_column("""", dimension=X.shape[0])]
regressor = skflow.DNNRegressor(hidden_units=[5, 5],feature_columns=feature_columns)
regressor.fit(X, y)
score = mean_squared_error(regressor.predict(X), y)
print(""Mean Squared Error: {0:f}"".format(score))
```

But this results in:

ValueError: Shapes (?, 1) and (?, 2) are incompatible

Has the important feature of multiple outputs been removed mistakenly somehow?

Also it looks like the example that @ilblackdragon was removed, the link in GitHub 404's.

### What other attempted solutions have you tried?

Upgraded code to account for breaking changes in TF, but still get an error.

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
6847,Better documentation for tf.extract_image_patches,"In this [issue](https://github.com/tensorflow/tensorflow/issues/6743#issuecomment-272525783) someone requested a reverse operation to `tf.extract_image_patches`. The comments suggest that there exists a [gradient operation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_grad.py#L575) for this purpose which was added with [this PR](https://github.com/tensorflow/tensorflow/pull/3672).

Unfortunately it is not obvious how to apply this gradient operation and there are no information on this in the [api docs](https://www.tensorflow.org/versions/master/api_docs/python/array_ops/slicing_and_joining#extract_image_patches) therefor I request to add an example to [tf.extract_image_patches](https://github.com/tensorflow/tensorflow/blob/a4c8df209d7413068f4ed3e71c43eb798fbd5580/tensorflow/g3doc/api_docs/python/functions_and_classes/shard4/tf.extract_image_patches.md) where one transforms an `image` tensor of shape `(image_height, image_width, channels)` to `(patch_num, patch_height, patch_width, channels)` and vice versa.

Thank you in advance!"
6846,tf.contrib.losses.sparse_softmax_cross_entropy not normal,"When I tried to use tf.contrib.losses.sparse_softmax_cross_entropy with weights for losses, I got the error:

ValueError: weight.get_shape().ndims cannot be None

Later, I used tf.contrib.losses.softmax_cross_entropy and onehot labels,  everything was ok.  Is there anything wrong with tf.contrib.losses.sparse_softmax_cross_entropy?

Thanks a lot!

"
6845,input pipeline spends all time in QueueDequeueMany,"My training process use tfrecord format for train&eval dataset .
I test the benchmark of reader , only 8000records/second.  and io speed(see from iotop command) just 400KB-500KB/s. 
I'm using the cpp version of protobuf here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#protobuf-library-related-issues

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
https://github.com/tensorflow/tensorflow/issues/4467
https://github.com/tensorflow/tensorflow/issues/4732
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
```
def read_and_decode(filename_queue):
     reader = tf.TFRecordReader()
    _, serialized_example = reader.read(filename_queue)
    return serialized_example
```

```
  serialized_example = read_and_decode(filename_queue)
  batch_serialized_example = tf.train.shuffle_batch(
      [serialized_example],
      batch_size=batch_size,
      num_threads=thread_number,
      capacity=capacity,
      min_after_dequeue=min_after_dequeue)
  features = tf.parse_example(
      batch_serialized_example,
      features={
          ""label"": tf.FixedLenFeature([], tf.float32),
          ""ids"": tf.VarLenFeature(tf.int64),
          ""values"": tf.VarLenFeature(tf.float32),
      })
```

### What other attempted solutions have you tried?

I try to set num_threads in  tf.train.shuffle_batch but not working. It seems that when set to 2 threads, it work at 8000records/s, when enlarge the thread number, it get slower. (I remove all ops that cost cpus. Just read data.) 
My sever are 24 core cpus.  
"
6844,changing number of outputs/classes,"my StackOverflow thread: http://stackoverflow.com/questions/41645571/changing-number-of-outputs-classes-in-tensorflow

### Environment info
Operating System: OSX
1. python3 installed with miniconda3, TF installation link: https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.11.0rc1-py3-none-any.whl
2. Version 0.11.0rc1

Minimal example:
```
import tensorflow as tf
import numpy as np

tf.reset_default_graph()

x = tf.placeholder(tf.float32, (None, 2))
y = tf.placeholder(tf.int32, (None,))

w = tf.Variable(tf.truncated_normal(shape=(2, 2)))
b = tf.Variable(tf.zeros(shape=(1, 2)))

n_more = tf.placeholder(tf.int32, ())

w_more = tf.truncated_normal((2, n_more))
new_w = tf.concat(concat_dim=1, values=[w, w_more])
change_w_op = tf.assign(w, new_w, validate_shape=False)

b_more = tf.zeros(shape=(1, n_more))
new_b = tf.concat(concat_dim=1, values=[b, b_more])
change_b_op = tf.assign(b, new_b, validate_shape=False)


p = tf.matmul(x, w) + b
p1 = tf.matmul(x, w)

loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(p, y))
loss1 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(p1, y))
opt = tf.train.GradientDescentOptimizer(0.1)
grads_and_vars = opt.compute_gradients(loss, var_list=[w, b])
train_op = opt.apply_gradients(grads_and_vars)

grads_and_vars1 = opt.compute_gradients(loss1, var_list=[w])
train_op1 = opt.apply_gradients(grads_and_vars1)

init = tf.initialize_all_variables()

sess = tf.Session()
sess.run(init)

# add two more classes
res = sess.run([change_w_op, change_b_op], feed_dict={n_more: 2})
sess.run([w, b])
#>[array([[-0.49880403,  0.1797405 , -0.66030115, -0.2065938 ],
#>        [-1.19939673,  1.05717182,  0.03949607,  1.66543031]], dtype=float32),
#> array([[-0.02882343,  0.02882343,  0.        ,  0.        ]], dtype=float32)]
# Works as expected
res = sess.run([p, p1], 
             feed_dict={
                 x:np.random.randn(3, 2), 
                 y:np.random.randint(0, 4, 3)
             })
# both elements in the res are (3, 4) arrays  -- Works as expected

res = sess.run([loss, loss1], 
             feed_dict={
                 x:np.random.randn(3, 2), 
                 y:np.random.randint(0, 4, 3)
             })
# res = [2.4644723, 2.4450662]  --  Works as expected

res = sess.run(grads_and_vars1, 
             feed_dict={
                 x:np.random.randn(3, 2), 
                 y:np.random.randint(0, 2, 3)
             })
# res is a list of two (2, 4) arrays -- Works as expected

res = sess.run(grads_and_vars, 
             feed_dict={
                 x:np.random.randn(3, 2), 
                 y:np.random.randint(0, 2, 3)
             })

```
The last one produces the error below.
My main questions are:
What am I doing wrong here? 
Why changing w does not cause problems but changing b does? 
Is this a bug?
```
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
    971     try:
--> 972       return fn(*args)
    973     except errors.OpError as e:

/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tensorflow/python/client/session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)
    953                                  feed_dict, fetch_list, target_list,
--> 954                                  status, run_metadata)
    955 

/Users/merekhinsky/.conda/envs/e1/lib/python3.5/contextlib.py in __exit__(self, type, value, traceback)
     65             try:
---> 66                 next(self.gen)
     67             except StopIteration:

/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tensorflow/python/framework/errors.py in raise_exception_on_not_ok_status()
    462           compat.as_text(pywrap_tensorflow.TF_Message(status)),
--> 463           pywrap_tensorflow.TF_GetCode(status))
    464   finally:

InvalidArgumentError: Incompatible shapes: [3,4] vs. [1,2]
	 [[Node: gradients/add_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/cpu:0""](gradients/add_grad/Shape, gradients/add_grad/Shape_1)]]

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-206-b8a2a3bd8471> in <module>()
      2              feed_dict={
      3                  x:np.random.randn(3, 2),
----> 4                  y:np.random.randint(0, 2, 3)
      5              })
      6 res

/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    715     try:
    716       result = self._run(None, fetches, feed_dict, options_ptr,
--> 717                          run_metadata_ptr)
    718       if run_metadata:
    719         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
    913     if final_fetches or final_targets:
    914       results = self._do_run(handle, final_targets, final_fetches,
--> 915                              feed_dict_string, options, run_metadata)
    916     else:
    917       results = []

/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
    963     if handle is None:
    964       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,
--> 965                            target_list, options, run_metadata)
    966     else:
    967       return self._do_call(_prun_fn, self._session, handle, feed_dict,

/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
    983         except KeyError:
    984           pass
--> 985       raise type(e)(node_def, op, message)
    986 
    987   def _extend_graph(self):

InvalidArgumentError: Incompatible shapes: [3,4] vs. [1,2]
	 [[Node: gradients/add_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/cpu:0""](gradients/add_grad/Shape, gradients/add_grad/Shape_1)]]

Caused by op 'gradients/add_grad/BroadcastGradientArgs', defined at:
  File ""/Users/merekhinsky/.conda/envs/e1/lib/python3.5/runpy.py"", line 184, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/Users/merekhinsky/.conda/envs/e1/lib/python3.5/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/ipykernel/__main__.py"", line 3, in <module>
    app.launch_new_instance()
  File ""/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/ipykernel/kernelapp.py"", line 474, in start
    ioloop.IOLoop.instance().start()
  File ""/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/zmq/eventloop/ioloop.py"", line 177, in start
    super(ZMQIOLoop, self).start()
  File ""/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tornado/ioloop.py"", line 887, in start
    handler_func(fd_obj, events)
  File ""/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tornado/stack_context.py"", line 275, in null_wrapper
    return fn(*args, **kwargs)
  File ""/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py"", line 440, in _handle_events
    self._handle_recv()
  File ""/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py"", line 472, in _handle_recv
    self._run_callback(callback, msg)
  File ""/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py"", line 414, in _run_callback
    callback(*args, **kwargs)
  File ""/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tornado/stack_context.py"", line 275, in null_wrapper
    return fn(*args, **kwargs)
  File ""/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/ipykernel/kernelbase.py"", line 276, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/ipykernel/kernelbase.py"", line 228, in dispatch_shell
    handler(stream, idents, msg)
  File ""/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/ipykernel/kernelbase.py"", line 390, in execute_request
    user_expressions, allow_stdin)
  File ""/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/ipykernel/ipkernel.py"", line 196, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/ipykernel/zmqshell.py"", line 501, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2717, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2821, in run_ast_nodes
    if self.run_code(code, result):
  File ""/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2881, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-196-ab717a15c74f>"", line 31, in <module>
    grads_and_vars = opt.compute_gradients(loss, var_list=[w, b])
  File ""/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py"", line 253, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tensorflow/python/ops/gradients.py"", line 469, in gradients
    in_grads = _AsList(grad_fn(op, *out_grads))
  File ""/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tensorflow/python/ops/math_grad.py"", line 555, in _AddGrad
    rx, ry = gen_array_ops._broadcast_gradient_args(sx, sy)
  File ""/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 388, in _broadcast_gradient_args
    name=name)
  File ""/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 749, in apply_op
    op_def=op_def)
  File ""/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 2380, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1298, in __init__
    self._traceback = _extract_stack()

...which was originally created as op 'add', defined at:
  File ""/Users/merekhinsky/.conda/envs/e1/lib/python3.5/runpy.py"", line 184, in _run_module_as_main
    ""__main__"", mod_spec)
[elided 18 identical lines from previous traceback]
  File ""/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2881, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-196-ab717a15c74f>"", line 25, in <module>
    p = tf.matmul(x, w) + b
  File ""/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py"", line 751, in binary_op_wrapper
    return func(x, y, name=name)
  File ""/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 71, in add
    result = _op_def_lib.apply_op(""Add"", x=x, y=y, name=name)
  File ""/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 749, in apply_op
    op_def=op_def)
  File ""/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 2380, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1298, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): Incompatible shapes: [3,4] vs. [1,2]
	 [[Node: gradients/add_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/cpu:0""](gradients/add_grad/Shape, gradients/add_grad/Shape_1)]]
```"
6841,A zip and/or tar for pypi,"As of 0.12.1, https://pypi.python.org/pypi/tensorflow/ only has os specific wheels.  It would be very nice for packages (e.g. me with fink on the mac) to have a point release tar or zip file too."
6838,Saver fails to restore on GCloud,"You can demonstrate this with the distributed MNIST example code: https://cloud.google.com/ml/docs/quickstarts/training#train_on_the_cloud_distributed

Running it on gcloud produces:

```ERROR	2017-01-13 10:56:52 -0500	master-replica-0		Traceback (most recent call last):
ERROR	2017-01-13 10:56:52 -0500	master-replica-0		  File ""/usr/lib/python2.7/runpy.py"", line 162, in _run_module_as_main
ERROR	2017-01-13 10:56:52 -0500	master-replica-0		    ""__main__"", fname, loader, pkg_name)
ERROR	2017-01-13 10:56:52 -0500	master-replica-0		  File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code
ERROR	2017-01-13 10:56:52 -0500	master-replica-0		    exec code in run_globals
ERROR	2017-01-13 10:56:52 -0500	master-replica-0		  File ""/root/.local/lib/python2.7/site-packages/trainer/task.py"", line 537, in <module>
ERROR	2017-01-13 10:56:52 -0500	master-replica-0		    tf.app.run()
ERROR	2017-01-13 10:56:52 -0500	master-replica-0		  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 43, in run
ERROR	2017-01-13 10:56:52 -0500	master-replica-0		    sys.exit(main(sys.argv[:1] + flags_passthrough))
ERROR	2017-01-13 10:56:52 -0500	master-replica-0		  File ""/root/.local/lib/python2.7/site-packages/trainer/task.py"", line 307, in main
ERROR	2017-01-13 10:56:52 -0500	master-replica-0		    run(model, argv)
ERROR	2017-01-13 10:56:52 -0500	master-replica-0		  File ""/root/.local/lib/python2.7/site-packages/trainer/task.py"", line 431, in run
ERROR	2017-01-13 10:56:52 -0500	master-replica-0		    dispatch(args, model, cluster, task)
ERROR	2017-01-13 10:56:52 -0500	master-replica-0		  File ""/root/.local/lib/python2.7/site-packages/trainer/task.py"", line 472, in dispatch
ERROR	2017-01-13 10:56:52 -0500	master-replica-0		    Trainer(args, model, cluster, task).run_training()
ERROR	2017-01-13 10:56:52 -0500	master-replica-0		  File ""/root/.local/lib/python2.7/site-packages/trainer/task.py"", line 243, in run_training
ERROR	2017-01-13 10:56:52 -0500	master-replica-0		    self.eval(session)
ERROR	2017-01-13 10:56:52 -0500	master-replica-0		  File ""/root/.local/lib/python2.7/site-packages/trainer/task.py"", line 284, in eval
ERROR	2017-01-13 10:56:52 -0500	master-replica-0		    self.model.format_metric_values(self.train_evaluator.evaluate()),
ERROR	2017-01-13 10:56:52 -0500	master-replica-0		  File ""/root/.local/lib/python2.7/site-packages/trainer/task.py"", line 75, in evaluate
ERROR	2017-01-13 10:56:52 -0500	master-replica-0		    self.sv.saver.restore(session, last_checkpoint)
ERROR	2017-01-13 10:56:52 -0500	master-replica-0		  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1388, in restore
ERROR	2017-01-13 10:56:52 -0500	master-replica-0		    {self.saver_def.filename_tensor_name: save_path})
ERROR	2017-01-13 10:56:52 -0500	master-replica-0		  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 766, in run
ERROR	2017-01-13 10:56:52 -0500	master-replica-0		    run_metadata_ptr)
ERROR	2017-01-13 10:56:52 -0500	master-replica-0		  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 964, in _run
ERROR	2017-01-13 10:56:52 -0500	master-replica-0		    feed_dict_string, options, run_metadata)
ERROR	2017-01-13 10:56:52 -0500	master-replica-0		  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1014, in _do_run
ERROR	2017-01-13 10:56:52 -0500	master-replica-0		    target_list, options, run_metadata)
ERROR	2017-01-13 10:56:52 -0500	master-replica-0		  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1034, in _do_call
ERROR	2017-01-13 10:56:52 -0500	master-replica-0		    raise type(e)(node_def, op, message)
ERROR	2017-01-13 10:56:52 -0500	master-replica-0		InternalError: Unable to get element from the feed as bytes.
INFO	2017-01-13 10:56:52 -0500	worker-replica-0		Train [worker/0], step 3195 (6.213 sec) 485.6 global steps/s, 170.5 local steps/s
ERROR	2017-01-13 10:56:52 -0500	master-replica-0		Module raised an exception Command '['python', '-m', u'trainer.task', u'--train_data_paths=gs://cloud-ml-data/mnist/train.tfr.gz', u'--eval_data_paths=gs://cloud-ml-data/mnist/eval.tfr/gz', u'--output_path=<output path>']' returned non-zero exit status 1.```

A fairly minimal test case follows:

```#!/usr/bin/python
import argparse
import json
import os
import tensorflow as tf

parser = argparse.ArgumentParser()
parser.add_argument('--output_path', type=str, default='/tmp/matrix_multiply')
flags = parser.parse_args()

matrix1 = tf.placeholder_with_default([[3., 3.]], shape=(1, 2))
matrix2 = tf.placeholder_with_default([[2.], [2.]], shape=(2, 1))
product = tf.Variable(tf.matmul(matrix1, matrix2))

inputs = {'matrix1': matrix1.name, 'matrix2': matrix2.name}
tf.add_to_collection('inputs', json.dumps(inputs))

outputs = {'product': product.name}
tf.add_to_collection('outputs', json.dumps(outputs))

saver = tf.train.Saver()

with tf.Session() as sess:
	sess.run(tf.global_variables_initializer())
	result = sess.run(product)
	if not os.path.isdir(flags.output_path):
		os.makedirs(flags.output_path)
	saver.save(sess, os.path.join(flags.output_path, ""export""))
	print result

with tf.Session() as sess:
	checkpoint = tf.train.latest_checkpoint(flags.output_path)
	print checkpoint
	saver.restore(sess, checkpoint)

	for v in tf.global_variables():
		print(v.name, v.eval())```"
6837,occasional GraphHandle not found error,"We occasionally get this error like below on universe-starter-agent. This is on a dedicated box and the code doesn't handle task restarts, so the common scenario doesn't apply . @mrry any idea if there's any way to get this error without a task restart?

https://github.com/openai/universe/issues/112

```
Exception in thread Thread-10:
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1021, in _do_call
    return fn(*args)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1003, in _run_fn
    status, run_metadata)
  File ""/usr/lib/python3.5/contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py"", line 469, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.AbortedError: Graph handle is not found: 000000000000000d

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
    self.run()
  File ""/experiment/universe-starter-agent/a3c.py"", line 92, in run
    self._run()
  File ""/experiment/universe-starter-agent/a3c.py"", line 101, in _run
    self.queue.put(next(rollout_provider), timeout=600.0)
  File ""/experiment/universe-starter-agent/a3c.py"", line 139, in env_runner
    summary_writer.add_summary(summary, policy.global_step.eval())
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py"", line 515, in eval
    return self._variable.eval(session=session)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 575, in eval
    return _eval_using_default_session(self, feed_dict, self.graph, session)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 3633, in _eval_using_default_session
    return session.run(tensors, feed_dict)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 766, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 964, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1014, in _do_run
    target_list, options, run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1034, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.AbortedError: Graph handle is not found: 000000000000000d

[2017-01-10 06:27:56,508] Received signal 15: exiting
I tensorflow/core/distributed_runtime/master_session.cc:891] DeregisterGraph error: Aborted: Graph handle is not found: 000000000000002c. Possibly, this worker just restarted.
[2017-01-10 06:27:56,601] Killing and removing container: id=e8bf782d91fa3d6e3bb685e37ed4f9622f054bfb281f1e1e61021f1ed6798a06
```"
6834,tf.strided_slice outputs tensor of incorrect shape,"so for tensor x of shape (?, 41, 41, 32), using

```
tf.strided_slice(x, [0, 1, 1, 0], [-1, -1, -1, -1], [1, 2, 2, 1])
```

will produce tensor <tf.Tensor 'StridedSlice_9:0' shape=(?, 20, 20, 31) dtype=float32>. This result is unexpected, since for last dimension, begin = 0, last = -1, and stride = 1. It should extract the entire dimension."
6833,Go: Unable to set shape attributes on operations,"Hi TensorFlow/Go team, I am attaching a code snippet that is failing due to the way shape argument is expected.

cc: @asimshankar and referencing [this](https://github.com/tensorflow/tensorflow/issues/10#issuecomment-272461355) discussion

```

package bugs

import (
	tf ""github.com/tensorflow/tensorflow/tensorflow/go""
	""github.com/tensorflow/tensorflow/tensorflow/go/op""
	""testing""
)

func TestVarHandleOpShape(t *testing.T) {
	scope := op.NewScope()
	_ = op.VarHandleOp(scope, tf.Int32, []int64{2, 3})
	_, err := scope.Finalize()
	if err != nil {
		t.Fatal(err)
	}
}

/*
=== RUN   TestVarHandleOpShape
--- FAIL: TestVarHandleOpShape (0.00s)
        bugs_test.go:14: failed to add operation ""VarHandleOp"": AttrValue had value with type 'list(int)' when 'shape' expected
                         for attr 'shape'
                        ; NodeDef: VarHandleOp = VarHandleOp[_class=[], container="""", dtype=DT_INT32, shape=[2, 3], shared_name=""""](); Op<name=VarHandleOp; signature= -> resource:resource; attr=container:string,default=""""; attr=shared_name:string,default=""""; attr=dtype:type; attr=shape:shape; is_stateful=true> (Stacktrace: goroutine 6 [running]:
                runtime/debug.Stack(0x0, 0x0, 0x0)
                        /usr/lib/golang/src/runtime/debug/stack.go:24 +0x79
                github.com/tensorflow/tensorflow/tensorflow/go/op.(*Scope).UpdateErr(0xc420012570, 0x51d427, 0xb, 0x79fba0, 0xc420054048)
                        /home/sdeoras/go/src/github.com/tensorflow/tensorflow/tensorflow/go/op/scope.go:113 +0x72
                github.com/tensorflow/tensorflow/tensorflow/go/op.(*Scope).AddOperation(0xc420012570, 0x51d427, 0xb, 0x51d427, 0xb, 0x0, 0x0, 0x0, 0xc4200125a0, 0xc420012540)
                        /home/sdeoras/go/src/github.com/tensorflow/tensorflow/tensorflow/go/op/scope.go:78 +0xf9
                github.com/tensorflow/tensorflow/tensorflow/go/op.VarHandleOp(0xc420012570, 0xc400000003, 0xc4200106e0, 0x2, 0x2, 0x0, 0x0, 0x0, 0x1123b1d4, 0xc420051f78)
                        /home/sdeoras/go/src/github.com/tensorflow/tensorflow/tensorflow/go/op/wrappers.go:14045 +0x27d
                bitbucket.hgst.com/x/tensorflow.git/bugs.TestVarHandleOpShape(0xc4200b8240)
                        /home/sdeoras/go/src/bitbucket.hgst.com/x/tensorflow.git/bugs/bugs_test.go:11 +0x96
                testing.tRunner(0xc4200b8240, 0x52bb48)
                        /usr/lib/golang/src/testing/testing.go:610 +0x81
                created by testing.(*T).Run
                        /usr/lib/golang/src/testing/testing.go:646 +0x2ec
                )
FAIL
exit status 1
FAIL    bitbucket.hgst.com/x/tensorflow.git/bugs        0.134s
*/

```"
6831,running session multiple times with tf.random returns different values for conv2d,"```
import tensorflow as tf 
import numpy as np 

x_tf = tf.placeholder('float',[None, 2, 5, 1])
x_np = np.random.noraml(0,1,[1,2,5,1])

# ======== filter option1 and option2 ===========
f_np = np.random.normal(0,1,[1,3,1,1])
f_tf = tf.constant(f_np,'float') # option 1
f_tf = tf.random_normal([1,3,1,1]) # option 2
# ===============================================

x_conv = tf.nn.conv2d(x_tf,f_tf,[1,1,1,1],'SAME')

with tf.Session() as sess:
     tf.gloval_variables_initializer().run()
     x_conv_np  = sess.run(x_conv, feed_dict={x_tf, x_np})
     x_conv_np2 = sess.run(x_conv, feed_dict={x_tf, x_np})
```
If I run the code above with option1, I get the same values for `x_conv_np` and `x_conv_np2`
However, when I run the above with option2, I get different values for `x_conv_np` and `x_conv_np2`.

I am guessing the `tf.random_normal` gets initialized every time the session is ran.
Is this meant to happen or is it a bug?
I found this out while I was trying to debug my code, and it took me a while to figure out the issue. 

Doesn't this mean that when option2 is chosen, the result of the convolution changes for each loop that iterates over mini batches?
"
6828,Tracking moving persons with ID,"I want to track person with IDs. I am able to run android APK for tracking, but  [C++ build](https://github.com/tensorflow/tensorflow/blob/v1.0.0-alpha/tensorflow/examples/android/jni/object_tracking/object_tracker.h) is taking a lot of time to build. Is there any way to run  python code for tracking?? "
6827,HOW can I USE tf.exp(x) to back propagation Gradient?,"@tf.RegisterGradient(""FGGrad"")
def grad_fg(op, x):
    return (2*tf.exp(x))/((1+tf.exp(x))**2)
def fg(x):
    with G.gradient_override_map({""Identity"": ""FGGrad""}):
         return tf.identity(x)

The code is above .
But i made a mistake.
HOW can I USE  (2*tf.exp(x))/((1+tf.exp(x))**2) to back propagation Gradient?
(I can use tf.pow(x,2))to back propagation Gradient)"
6826,Why is validation_metrics and validation_monitor code twice in tutorial code iris_monitors.py?,"Why is validation_metrics and validation_monitor code twice in tutorial code iris_monitors.py?
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/monitors/iris_monitors.py"
6824,tf.einsum fails on evaluation in some cases,"Hello,
I was a user of 'reshape-matmul-reshape' approach, however it felt me a bit messy, so I am trying some alternatives that do this automatically.
I had tried `tf.tensordot` before, however it did not work well when shapes are partially known (#6682).
As an alternative, I tried to use `tf.einsum`.
However, similar to `tf.tensordot`, it cannot handle some variables with partially-known-shape, for instance, the below codes fails on evaluation.

```
a = tf.placeholder('float32', shape=[None, None, 100])
b = tf.placeholder('float32', shape=[100, 300])
result_einsum = tf.einsum('ijk,kl->ijl', a, b)
a_value = np.random.randn(10, 20, 100)
b_value = np.random.randn(100, 300)
sess.run(result_einsum, {a: a_value, b: b_value})  # failure
```


This is the error message:
```
InvalidArgumentError (see above for traceback): only one input size may be -1, not both 0 and 1
         [[Node: Reshape_5 = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/cpu:0""](MatMul_3, Reshape_5/shape)]]
```"
6823,Upgrade HighwayHash,"The HighwayHash module which is downloaded as an external dependency in TensorFlow produces different hash results on big endian and little endian architectures. This causes the test `testStringToHashBucketsStrong` from `//tensorflow/python/kernel_tests:string_to_hash_bucket_op_test` to fail on big endian. After raising an [issue](https://github.com/google/highwayhash/issues/35) with HighwayHash community, they have added a change to make hash values consistent across architectures through [commit](https://github.com/google/highwayhash/commit/cdde139127319cb5eb3917b635c4d2b182533cb4).

Will it be possible to pick this or higher commit of HighwayHash in TensorFlow?"
6817,Add CUDA implementation of fused hue adjustment,"Currently, the fused (rgb2hsv->hue_adjust->hsv2rgb) hue adjustment kernel exists in TF [for the CPU](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/adjust_hue_op.cc), but not for the GPU. This is also why the CPU path can only be enabled ""secretly"" (via [setting the TF_ADJUST_HUE_FUSED env var](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/image_ops_impl.py#L1113) to true). Adding the CUDA kernel would make this op available for both CPUs and GPUs."
6816,Intermittent CUDA_ERROR_ILLEGAL_ADDRESS errors during back propagation for some networks when using XLA.,"### Summary
In playing around with the XLA JIT compiler I've experienced intermittent CUDA_ERROR_ILLEGAL_ADDRESS errors during back propagation for some network structures. While other structures may also fail, I can confirm that this happens with simple fully-connected networks when dropout is applied to the hidden units. I have not, however, observed errors when not using XLA or when using XLA, but no hidden dropout.

### Environment info
- Operating System: Ubuntu 14.04 LTS
- GPU: Titan X
- NVIDIA driver: 375.26
- CUDA: 8.0
- cuDNN: 5.1
- see also: [cuda_libs.txt](https://github.com/tensorflow/tensorflow/files/703021/cuda_libs.txt)
-  bazel: 4.3
- TensorFlow: f8fcd85b8151d287ad09a9323fabf1c5025775b7
- XLA: Yes
- TensorFlow compile command:

    `bazel build --copt=-march=native -c opt  --config=cuda //tensorflow/tools/pip_package:build_pip_package`
    

### Minimal reproducible example 
See https://gist.github.com/nryant/8cea9bb79d7bdb965167e917d8d5aa8b for the script `test_dropout.py`, which repeatedly builds a small network consisting of a single hidden layer of 512 units and performs 100 fprop/bprop steps. It accepts three command line arguments:

- `--bprop` enables backpropagation (by default, only forward propagation is performed)
- `--xla` enables XLA at the session level
- `--dropout` specifies the dropout proportion

Also see log files produced from running the script with various arguments:

- No XLA, fprop + bprop, and dropout: https://gist.github.com/nryant/83829ada37a9b0c221a76184c5e232fa#file-noxla_fprop_bprop_dropout-log
- XLA and fprop: https://gist.github.com/nryant/83829ada37a9b0c221a76184c5e232fa#file-xla_fprop-log
- XLA, fprop, and dropout: https://gist.github.com/nryant/83829ada37a9b0c221a76184c5e232fa#file-xla_fprop_dropout-log
- XLA, fprop + bprop, and dropout: https://gist.github.com/nryant/83829ada37a9b0c221a76184c5e232fa#file-xla_fprop_bprop_dropout-log

The only combination for which we ever observe an error is XLA, fprop + bprop, and dropout, which pretty reliable blows up in the first few iterations."
6815,keep_dims vs keepdims,"There are 15 results for [keepdims](https://github.com/tensorflow/tensorflow/search?utf8=%E2%9C%93&q=keepdims) and 77 results for [keep_dims](https://github.com/tensorflow/tensorflow/search?utf8=%E2%9C%93&q=keep_dims&type=Code) in TF codebase, any chance this can be made consistent in 1.0? Ideally TF would just match numpy API and use `keepdims` @martinwicke @karpathy

"
6814,mnist preload data reader example fails ,"I am using tensorflow version 12, following the documentation at 

https://www.tensorflow.org/how_tos/reading_data/

which points to the example:

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/fully_connected_preloaded_var.py

however, when I run that example, I get errors about un-initialized variables. Here is the output:

```
python tf_mnist_preload.py 
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.
Extracting /tmp/data/train-images-idx3-ubyte.gz
Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.
Extracting /tmp/data/train-labels-idx1-ubyte.gz
Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.
Extracting /tmp/data/t10k-images-idx3-ubyte.gz
Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.
Extracting /tmp/data/t10k-labels-idx1-ubyte.gz
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:87:00.0
Total memory: 11.17GiB
Free memory: 11.11GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:87:00.0)
W tensorflow/core/framework/op_kernel.cc:975] Failed precondition: Attempting to use uninitialized value input/input_producer/input_producer/fraction_of_32_full/limit_epochs/epochs
	 [[Node: input/input_producer/input_producer/fraction_of_32_full/limit_epochs/CountUpTo = CountUpTo[T=DT_INT64, _class=[""loc:@input/input_producer/input_producer/fraction_of_32_full/limit_epochs/epochs""], limit=2, _device=""/job:localhost/replica:0/task:0/cpu:0""](input/input_producer/input_producer/fraction_of_32_full/limit_epochs/epochs)]]
W tensorflow/core/framework/op_kernel.cc:975] Out of range: FIFOQueue '_1_input/batch/fifo_queue' is closed and has insufficient elements (requested 100, current size 0)
	 [[Node: input/batch = QueueDequeueMany[_class=[""loc:@input/batch/fifo_queue""], component_types=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](input/batch/fifo_queue, input/batch/n)]]
Saving
Done training for 2 epochs, 0 steps.
Traceback (most recent call last):
  File ""tf_mnist_preload.py"", line 195, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/home/davidsch/miniconda2/envs/mlearn/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 43, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""tf_mnist_preload.py"", line 147, in main
    run_training()
  File ""tf_mnist_preload.py"", line 142, in run_training
    coord.join(threads)
  File ""/home/davidsch/miniconda2/envs/mlearn/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py"", line 386, in join
    six.reraise(*self._exc_info_to_raise)
  File ""/home/davidsch/miniconda2/envs/mlearn/lib/python2.7/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 234, in _run
    sess.run(enqueue_op)
  File ""/home/davidsch/miniconda2/envs/mlearn/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 766, in run
    run_metadata_ptr)
  File ""/home/davidsch/miniconda2/envs/mlearn/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 964, in _run
    feed_dict_string, options, run_metadata)
  File ""/home/davidsch/miniconda2/envs/mlearn/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1014, in _do_run
    target_list, options, run_metadata)
  File ""/home/davidsch/miniconda2/envs/mlearn/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1034, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value input/input_producer/input_producer/fraction_of_32_full/limit_epochs/epochs
	 [[Node: input/input_producer/input_producer/fraction_of_32_full/limit_epochs/CountUpTo = CountUpTo[T=DT_INT64, _class=[""loc:@input/input_producer/input_producer/fraction_of_32_full/limit_epochs/epochs""], limit=2, _device=""/job:localhost/replica:0/task:0/cpu:0""](input/input_producer/input_producer/fraction_of_32_full/limit_epochs/epochs)]]
	 [[Node: input/input_producer/input_producer/fraction_of_32_full/limit_epochs/CountUpTo/_12 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_13_input/input_producer/input_producer/fraction_of_32_full/limit_epochs/CountUpTo"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]

Caused by op u'input/input_producer/input_producer/fraction_of_32_full/limit_epochs/CountUpTo', defined at:
  File ""tf_mnist_preload.py"", line 195, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/home/davidsch/miniconda2/envs/mlearn/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 43, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""tf_mnist_preload.py"", line 147, in main
    run_training()
  File ""tf_mnist_preload.py"", line 63, in run_training
    [input_images, input_labels], num_epochs=FLAGS.num_epochs)
  File ""/home/davidsch/miniconda2/envs/mlearn/lib/python2.7/site-packages/tensorflow/python/training/input.py"", line 305, in slice_input_producer
    shared_name=shared_name)
  File ""/home/davidsch/miniconda2/envs/mlearn/lib/python2.7/site-packages/tensorflow/python/training/input.py"", line 262, in range_input_producer
    shared_name, name, ""fraction_of_%d_full"" % capacity)
  File ""/home/davidsch/miniconda2/envs/mlearn/lib/python2.7/site-packages/tensorflow/python/training/input.py"", line 156, in input_producer
    input_tensor = limit_epochs(input_tensor, num_epochs)
  File ""/home/davidsch/miniconda2/envs/mlearn/lib/python2.7/site-packages/tensorflow/python/training/input.py"", line 96, in limit_epochs
    counter = epochs.count_up_to(num_epochs)
  File ""/home/davidsch/miniconda2/envs/mlearn/lib/python2.7/site-packages/tensorflow/python/ops/variables.py"", line 652, in count_up_to
    return state_ops.count_up_to(self._variable, limit=limit)
  File ""/home/davidsch/miniconda2/envs/mlearn/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.py"", line 126, in count_up_to
    result = _op_def_lib.apply_op(""CountUpTo"", ref=ref, limit=limit, name=name)
  File ""/home/davidsch/miniconda2/envs/mlearn/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 759, in apply_op
    op_def=op_def)
  File ""/home/davidsch/miniconda2/envs/mlearn/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2240, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/davidsch/miniconda2/envs/mlearn/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1128, in __init__
    self._traceback = _extract_stack()

FailedPreconditionError (see above for traceback): Attempting to use uninitialized value input/input_producer/input_producer/fraction_of_32_full/limit_epochs/epochs
	 [[Node: input/input_producer/input_producer/fraction_of_32_full/limit_epochs/CountUpTo = CountUpTo[T=DT_INT64, _class=[""loc:@input/input_producer/input_producer/fraction_of_32_full/limit_epochs/epochs""], limit=2, _device=""/job:localhost/replica:0/task:0/cpu:0""](input/input_producer/input_producer/fraction_of_32_full/limit_epochs/epochs)]]
	 [[Node: input/input_producer/input_producer/fraction_of_32_full/limit_epochs/CountUpTo/_12 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_13_input/input_producer/input_producer/fraction_of_32_full/limit_epochs/CountUpTo"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]

```


"
6812,python documentation for tensorflow.abs out of sync,"I checked out the master branch tonight:
d4b5c606fc9fbd1a20b5b113b4bc831f31d889a3

I installed the code with pip into a conda environment

It seems the tensorflow.complex_abs() function has been merged into the overloaded tensorflow.abs() function.

The docstring, along with the rest of the python documentation don't seem to reflect this change:

> In [53]: tf.abs?
> Signature: tf.abs(x, name=None)
> Docstring:
> Computes the absolute value of a tensor.
> 
> Given a tensor of real numbers `x`, this operation returns a tensor
> containing the absolute value of each element in `x`. For example, if x is
> an input element and y is an output element, this operation computes
> \\(y = |x|\\).
> 
> Args:
>   x: A `Tensor` or `SparseTensor` of type `float32`, `float64`, `int32`, or
>     `int64`.
>   name: A name for the operation (optional).
> 
> Returns:
>   A `Tensor` or `SparseTensor` the same size and type as `x` with absolute
>     values.
> File:      ~/src/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py
> Type:      function
> "
6811,tf.unstack:AttributeError: 'module' object has no attribute 'unstack',"I am trying to use tf.unstack but is getting the error:

AttributeError: 'module' object has no attribute 'unstack'

I am trying the alternative code version of the two lines in ptb_word_lm.py"
6809,TensorFlow 1.0.0 alpha binaries require SSE4.1,"The TensorFlow 1.0.0 alpha binaries require SSE4.1, which was not necessary to run the binaries of earlier versions (e.g. 0.12.1). SSE4.1 is only supported by a few AMD architectures. In particular, it is not supported by the  ""Barcelona"" and the ""Bobcat"" architecture. They support SSE4a, which is not supported by Intel processors.

Therefore, running the 1.0.0 binaries on our workstations with ""AMD Opteron(tm) Processor 6174"" fails with the following error:
`The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine.`

Is it possible to make the official TensorFlow 1.0.0 binaries not dependent on vendor specific SSE4.1 instructions? I am aware that these AMD processors are not the newest ones and that it's possible to compile TensorFlow without the SSE4.1 requirement (at least, I assume so), but I wanted to raise this issue to see if you think it might be possible to make the official 1.0.0 binaries work without SSE4.1. Alternatively, would you consider offering alternative binaries without SSE4.1 similar to how you offer different binaries for different python versions?"
6808,Wrong argument order for input_producer,"The signature of `input_producer` is:
```python
def input_producer(input_tensor,
                   element_shape=None,
                   num_epochs=None,
                   shuffle=True,
                   seed=None,
                   capacity=32,
                   shared_name=None,
                   summary_name=None,
                   name=None,
                   cancel_op=None):
""""""
    summary_name: (Optional.) If set, a scalar summary for the current queue
      size will be generated, using this name as part of the tag.
""""""
```
and this function adds scalar summary of the fraction of the queue that is full (not actually queue size), with name `summary_name`.

But `range_input_producer` is calling `input_producer` like this:
```python
def range_input_producer(limit, num_epochs=None, shuffle=True, seed=None,
                         capacity=32, shared_name=None, name=None):
  with ops.name_scope(name, ""input_producer"", [limit]) as name:
    range_tensor = math_ops.range(limit)
    return input_producer(
        range_tensor, [], num_epochs, shuffle, seed, capacity,
        shared_name, name, ""fraction_of_%d_full"" % capacity)
```
It looks like a bug to me: name and summary_name might need to swap."
6807,[TensorBoard] To small dropdown menu,"This is not a big issue, but it is a tad annoying.

When using TensorBoard to visualize the graph the 'run' drop-down menu is very small.

![tb_menu](https://cloud.githubusercontent.com/assets/8115763/21893472/783d3b5e-d8db-11e6-81b0-08bc6fadf982.png)

I am using Chrome `Version 55.0.2883.87 m`"
6806,convert_graphdef_memmapped_format produces corrupt graphs,"I tried memory mapping an exported/frozen graph. Let's say the graph is named `graph.pb`. After converting the graph like this

    ./path/to/convert_graphdef_memmapped_format --in_graph=graph.pb --out_graph=graph_mmap.pb

I get

- `Converted 16 nodes` for an unquantized graph
- `Converted 0 nodes` for the 8bit quantized version of the graph

If I try to load any of these output files using the C++ API `ReadBinaryProto` and then `Run` the graph the following errors occur

- Unquantized: `Session was not created with a graph before Run()`
- Quantized: `Out of range: Read less bytes than requested`

so I guess the output file is a corrupt protobuf file (or a different version).
"
6804,Defun broken: errors_impl.NotFoundError: Op type not registered,"This code fails:

```
import tensorflow as tf
from tensorflow.python.framework import function

@function.Defun(tf.float32)
def custom_op(x):
    return x

x = tf.Variable(1, dtype=tf.float32)

sess = tf.Session()
sess.run(tf.global_variables_initializer())
print(sess.run(custom_op(x)))

sess.close()
```

Exception:

    tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'custom_op_da39a3ee'
"
6803,dynamic seq2seq add example of using context_state,"tf v1.0, in contrib/seq2seq/seq2seq_test.py , the code do not show how to use context_state, so we can get best path as output instead of rnn outputs.
Can we add sample code?"
6802,build with CPU optimization and GPU support are conflict?,"```
bazel build --copt=-march=native -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
```
Can I use both --copt=-march=native and --config=cuda , for now it gives out an error
```
ERROR: /home/wenjian/tensorflow-1.0.0-alpha/tensorflow/core/BUILD:1200:1: C++ compilation of rule '//tensorflow/core:framework_internal' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 123 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
In file included from ./tensorflow/core/framework/numeric_types.h:25:0,
                 from ./tensorflow/core/framework/allocator.h:23,
                 from ./tensorflow/core/framework/tensor.h:21,
                 from ./tensorflow/core/util/sparse/group_iterator.h:21,
                 from tensorflow/core/util/sparse/group_iterator.cc:16:
./third_party/eigen3/unsupported/Eigen/CXX11/FixedPoint:42:52: fatal error: src/Tensor/TensorContractionThreadPool.h: No such file or directory
compilation terminated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 78.339s, Critical Path: 66.15s

```
"
6801,1.0alpha configure problem,"```
ERROR: /home/wenjian/pkgs/tensorflow-1.0.0-alpha/tensorflow/workspace.bzl:345:3: no such package '@junit_jar//jar': Error downloading [https://github.com/junit-team/junit4/releases/download/r4.12/junit-4.12.jar] to /home/wenjian/.cache/bazel/_bazel_wenjian/72810017c7bd644d4bee7673555b820d/external/junit_jar/junit-4.12.jar: Tried to reconnect at offset 121,286 but server didn't support it and referenced by '//external:junit'.
ERROR: /home/wenjian/pkgs/tensorflow-1.0.0-alpha/tensorflow/workspace.bzl:345:3: no such package '@junit_jar//jar': Error downloading [https://github.com/junit-team/junit4/releases/download/r4.12/junit-4.12.jar] to /home/wenjian/.cache/bazel/_bazel_wenjian/72810017c7bd644d4bee7673555b820d/external/junit_jar/junit-4.12.jar: Tried to reconnect at offset 121,286 but server didn't support it and referenced by '//external:junit'.
```"
6800,Getting AbortionError when running modified tensorflow serving client,"(I initially posted this on stackoverflow but have gotten no response.) I modified the mnist_export.py and mnist_client.py to run an LSTM on some excel data. No issue with the training and exporting, but I run into this error below when running the client code. 

```
grpc.framework.interfaces.face.face.AbortionError: 
    AbortionError(code=StatusCode.INTERNAL, details=""Output 0 of type 
    double does not match declared output type float for node _recv_x_0 = 
    _Recv[client_terminated=true, 
    recv_device=""/job:localhost/replica:0/task:0/cpu:0"", 
    send_device=""/job:localhost/replica:0/task:0/cpu:0"", 
    send_device_incarnation=-9032417372349471954, tensor_name=""x:0"", 
    tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""()"")
```
My input data is in the shape of [None, 1, 20] where 1 is the time_step and 20 is the features. 

Below are the relevant parts of my training and export code: 

   ```
 def RNN(x, weights, biases):
        # Prepare data shape to match `rnn` function requirements
        # Current data input shape: (batch_size, n_steps, n_input)
        # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)
        # Permuting batch_size and n_steps
        x = tf.transpose(x, [1, 0, 2])
        # Reshaping to (n_steps*batch_size, n_input)
        x = tf.reshape(x, [-1, n_input])
        # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)
        x = tf.split(0, n_steps, x)
        # Define a lstm cell with tensorflow
        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)
        # Add dropout
        #lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, input_keep_prob=keep_prob)
        #lstm_cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * n_layers, state_is_tuple=True)
        # Get lstm cell output
        outputs, states = tf.nn.rnn(lstm_cell, x, dtype=tf.float32)
        # Linear activation, using rnn inner loop last output
        return tf.matmul(outputs[-1], weights['out']) + biases['out']
    
    # get data
    train_data_set, test_data_set = read_data_sets('AUDJPY Data.csv')
    
    # tf Graph input
    #x = tf.placeholder(""float"", [None, n_steps, n_input])
    y_ = tf.placeholder(""float"", [None, n_classes])
    keep_prob = tf.placeholder(tf.float32)
    
    # Exporter signatures
    serialized_tf_example = tf.placeholder(tf.string, name='tf_example')
    feature_configs = {
        'x': tf.FixedLenFeature(shape=[n_steps,n_input], dtype=tf.float32),
    }
    tf_example = tf.parse_example(serialized_tf_example, feature_configs)
    x = tf.identity(tf_example['x'], name='x')  # use tf.identity() to assign name
    
    # Define weights
    weights = {
        'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))
    }
    biases = {
        'out': tf.Variable(tf.random_normal([n_classes]))
    }
    
    pred = RNN(x, weights, biases)
    
    # Define loss and optimizer
    y = tf.nn.softmax(pred, name='y')
    values, indices = tf.nn.top_k(y, k=4)
    classes = tf.contrib.lookup.index_to_string(tf.to_int64(indices), mapping=tf.constant([str(i) for i in range(n_classes)]))
    cost = -tf.reduce_sum(y_ * tf.log(y))
    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)
    
    # Evaluate model
    correct_pred = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))
    
    # Initializing the variables
    init = tf.global_variables_initializer()
    
    # Launch the graph
    with tf.Session() as sess:
        sess.run(init)
        step = 1
        # Keep training until reach max iterations
        while step * batch_size < training_iters:
            batch_x, batch_y = train_data_set.next_batch(batch_size)
            # Reshape data to get 28 seq of 28 elements
            batch_x = batch_x.reshape((batch_size, n_steps, n_input))
            # Run optimization op (backprop)
            sess.run(optimizer, feed_dict={x: batch_x, y_: batch_y})
            if step % display_step == 0:
                # Calculate batch accuracy
                acc = sess.run(accuracy, feed_dict={x: batch_x, y_: batch_y})
                # Calculate batch loss
                loss = sess.run(cost, feed_dict={x: batch_x, y_: batch_y})
                print(""Iter "" + str(step*batch_size) + "", Minibatch Loss= "" + \
                      ""{:.6f}"".format(loss) + "", Training Accuracy= "" + \
                      ""{:.5f}"".format(acc))
            step += 1
        print(""Optimization Finished!"")
        print(""Testing Accuracy:"", \
                sess.run(accuracy, feed_dict={x: test_data_set.features, y_: test_data_set.labels}))
        # Export inference model.
        export_path = '/tmp/'
        print('Exporting trained model to %s' % export_path)
        init_op = tf.group(tf.initialize_all_tables(), name='init_op')
        saver = tf.train.Saver(sharded=True)
        classification_signature = exporter.classification_signature(
            input_tensor=serialized_tf_example,
            classes_tensor=classes,
            scores_tensor=values)
        named_graph_signature = {
            'inputs': exporter.generic_signature({'images': x}),
            'outputs': exporter.generic_signature({'scores': y})}
        model_exporter = exporter.Exporter(saver)
        model_exporter.init(
            init_op=init_op,
            default_graph_signature=classification_signature,
            named_graph_signatures=named_graph_signature)
        model_exporter.export(export_path, tf.constant(FLAGS.export_version), sess)
        print('Done exporting!')
```

And below are the relevant part of the client code:

```
def do_inference(hostport, work_dir, concurrency, num_tests):
      """"""Tests PredictionService with concurrent requests.
      Args:
        hostport: Host:port address of the PredictionService.
        work_dir: The full path of working directory for test data set.
        concurrency: Maximum number of concurrent requests.
        num_tests: Number of test images to use.
      Returns:
        The classification error rate.
      Raises:
        IOError: An error occurred processing test data set.
      """"""
      train_data_set, test_data_set = read_data_sets(work_dir)
      print('read test data')
      host, port = hostport.split(':')
      channel = implementations.insecure_channel(host, int(port))
      stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)
      request = predict_pb2.PredictRequest()
      request.model_spec.name = 'mnist'
      image, label = test_data_set.next_batch(1)
      print(image.shape)
      request.inputs['images'].CopyFrom(
          tf.contrib.util.make_tensor_proto(image[0], shape=[1,1,20]))
      result = stub.Predict(request, 10.0)
```

Also below is the traceback if that helps:

```
Traceback (most recent call last):
      File ""/home/joel/Projects/serving/bazel-bin/tensorflow_serving/example/venatus_client.runfiles/tf_serving/tensorflow_serving/example/venatus_client.py"", line 225, in <module>
        tf.app.run()
      File ""/home/joel/Projects/serving/bazel-bin/tensorflow_serving/example/venatus_client.runfiles/org_tensorflow/tensorflow/python/platform/app.py"", line 44, in run
        _sys.exit(main(_sys.argv[:1] + flags_passthrough))
      File ""/home/joel/Projects/serving/bazel-bin/tensorflow_serving/example/venatus_client.runfiles/tf_serving/tensorflow_serving/example/venatus_client.py"", line 220, in main
        FLAGS.concurrency, FLAGS.num_tests)
      File ""/home/joel/Projects/serving/bazel-bin/tensorflow_serving/example/venatus_client.runfiles/tf_serving/tensorflow_serving/example/venatus_client.py"", line 208, in do_inference
        result = stub.Predict(request, 10.0)
      File ""/usr/local/lib/python2.7/dist-packages/grpc/beta/_client_adaptations.py"", line 305, in __call__
        self._request_serializer, self._response_deserializer)
      File ""/usr/local/lib/python2.7/dist-packages/grpc/beta/_client_adaptations.py"", line 203, in _blocking_unary_unary
        raise _abortion_error(rpc_error_call)
    grpc.framework.interfaces.face.face.AbortionError: AbortionError(code=StatusCode.INTERNAL, details=""Output 0 of type double does not match declared output type float for node _recv_x_0 = _Recv[client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=-9032417372349471954, tensor_name=""x:0"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()"")
```

Appreciate any help on how to fix this!

"
6799,Go: Unable to create two ops of the same type using generated op wrapper functions,"As reported in https://github.com/tensorflow/tensorflow/issues/10#issuecomment-272045853 , the following test (at head: d4b5c606fc9fbd1a20b5b113b4bc831f31d889a3):

```go
package bug

import (
        ""testing""

        tf ""github.com/tensorflow/tensorflow/tensorflow/go""
        ""github.com/tensorflow/tensorflow/tensorflow/go/op""
)

func TestBug(t *testing.T) {
        scope := op.NewScope()
        op.Placeholder(scope.Subscope(""x""), tf.Float)
        op.Placeholder(scope.Subscope(""y""), tf.Float)
        if _, err := scope.Finalize(); err != nil {
                t.Fatal(err)
        }
}
```

fails with:

```sh
--- FAIL: TestBug (0.01s)
	bug_test.go:15: failed to add operation ""Placeholder"": Duplicate node name in graph: 'Placeholder' (Stacktrace: goroutine 19 [running]:
		runtime/debug.Stack(0x0, 0x0, 0x0)
			/usr/local/go/src/runtime/debug/stack.go:24 +0x79
		github.com/tensorflow/tensorflow/tensorflow/go/op.(*Scope).UpdateErr(0xc420072420, 0x411b8ac, 0xb, 0x4196640, 0xc42007c030)
			/home/go/src/github.com/tensorflow/tensorflow/tensorflow/go/op/scope.go:106 +0x72
		github.com/tensorflow/tensorflow/tensorflow/go/op.(*Scope).AddOperation(0xc420072420, 0x411b8ac, 0xb, 0x0, 0x0, 0x0, 0x0, 0x0, 0xc420072450, 0x4108b00)
			/home/go/src/github.com/tensorflow/tensorflow/tensorflow/go/op/scope.go:70 +0xbf
		github.com/tensorflow/tensorflow/tensorflow/go/op.Placeholder(0xc420072420, 0x1, 0x0, 0x0, 0x0, 0xc42006e4f0, 0x0)
			/home/go/src/github.com/tensorflow/tensorflow/tensorflow/go/op/wrappers.go:4734 +0x1c7
		github.com/tensorflow/tensorflow/tensorflow/go/b10.TestBug(0xc420098180)
			/home/go/src/github.com/tensorflow/tensorflow/tensorflow/go/b10/bug_test.go:13 +0xd2
		testing.tRunner(0xc420098180, 0x4129d88)
			/usr/local/go/src/testing/testing.go:610 +0x81
		created by testing.(*T).Run
			/usr/local/go/src/testing/testing.go:646 +0x2ec
		)
```

(Thanks for pointing this out @sdeoras)."
6798,Poisson Number Generator,"Hi,

I was wondering if it is possible to implement the Poisson Random Number generator in the random_ops in the future? 

Thanks."
6797,eiddccfiutbfedncengrfggkburbbbfvkufejukucetr,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
6795,Unable to compile HEAD on Windows with Bazel,"I'm unable to successfully build the current HEAD (or versions 1.0 or 0.12) with Windows and Bazel.

## Environment info

* Operating System: Windows 10
* Installed version of CUDA and cuDNN: None
* Bazel version: 0.4.3 (Build time: Thu Dec 22 12:31:31 2016 (1482409891))
I have done the following

* install prerequisites for Bazel on Windows (https://bazel.build/versions/master/docs/windows.html)
* checked out TF head (I also tried this with release 1.0, and release 0.12)
* run exe C:\tools\msys64\msys2.exe
* set these env variables
** export JAVA_HOME=""$(ls -d C:/Program\ Files/Java/jdk* | sort | tail -n 1)""
** export TMPDIR=""C:/tmp""
** export BAZEL_SH=c:/tools/msys64/usr/bin/bash.exe
** export BAZEL_VS=""C:/Program Files (x86)/Microsoft Visual Studio 14.0""
** export BAZEL_PYTHON=""C:/Users/.../Anaconda3/python.exe""
** export PYTHON_BIN_PATH=""C:/Users/.../Anaconda3/python.exe""
** export PATH=/c/Users/.../Anaconda3/:$PATH
** export PATH=/c/tools/bazel/:$PATH
** export PATH=/c/Program\ Files/CMake/bin/:$PATH
* ./configure
** no GPU, etc, just default options and path to Anaconda
* bazel build -c opt --cpu=x64_windows_msvc //tensorflow/tools/pip_package:build_pip_package

However I get errors after a few minutes of building.
This is reproducible and happens every time. I am following all the default options for the basic build. I would be interested to know if anyone is currently managing to compile on Windows?
I can find a few tutorials using Docker or Cmake, however I'm trying to use the newest method described on TF's website.

```
INFO: From Linking external/protobuf/pyext/_message.so:
   Creating library bazel-out/vc_14_0_x64-py3-opt/bin/external/protobuf/pyext/_m  essage.lib and object bazel-out/vc_14_0_x64-py3-opt/bin/external/protobuf/pyext/  _message.exp
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:1013:  1: null failed: protoc.exe failed: error executing command bazel-out/host/bin/ex  ternal/protobuf/protoc.exe --cpp_out=bazel-out/vc_14_0_x64-py3-opt/genfiles/ -I.   -I. -Iexternal/protobuf/src -Ibazel-out/vc_14_0_x64-py3-opt/genfiles/external/p  rotobuf/src ... (remaining 3 argument(s) skipped): com.google.devtools.build.lib  .shell.BadExitStatusException: Process exited with status -1073741515.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/python/BUILD:208  8:1: output 'tensorflow/python/framework/cpp_shape_inference_pb2.py' was not cre  ated.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/example/example.pb.cc' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/example/feature.pb.cc' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/allocation_description.pb.cc' was not create  d.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/attr_value.pb.cc' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/cost_graph.pb.cc' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/device_attributes.pb.cc' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/function.pb.cc' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/graph.pb.cc' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/kernel_def.pb.cc' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/log_memory.pb.cc' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/node_def.pb.cc' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/op_def.pb.cc' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/resource_handle.pb.cc' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/step_stats.pb.cc' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/summary.pb.cc' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/tensor.pb.cc' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/tensor_description.pb.cc' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/tensor_shape.pb.cc' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/tensor_slice.pb.cc' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/types.pb.cc' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/versions.pb.cc' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/lib/core/error_codes.pb.cc' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/protobuf/config.pb.cc' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/protobuf/debug.pb.cc' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/protobuf/tensor_bundle.pb.cc' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/protobuf/saver.pb.cc' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/util/memmapped_file_system.pb.cc' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/util/saved_tensor_slice.pb.cc' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/example/example_parser_configuration.pb.cc' was not cr  eated.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/variable.pb.cc' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/protobuf/control_flow.pb.cc' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/protobuf/meta_graph.pb.cc' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/protobuf/named_tensor.pb.cc' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/protobuf/queue_runner.pb.cc' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/protobuf/saved_model.pb.cc' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/protobuf/tensorflow_server.pb.cc' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/util/event.pb.cc' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/util/test_log.pb.cc' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/example/example.pb.h' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/example/feature.pb.h' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/allocation_description.pb.h' was not created  .
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/attr_value.pb.h' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/cost_graph.pb.h' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/device_attributes.pb.h' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/function.pb.h' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/graph.pb.h' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/kernel_def.pb.h' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/log_memory.pb.h' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/node_def.pb.h' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/op_def.pb.h' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/resource_handle.pb.h' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/step_stats.pb.h' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/summary.pb.h' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/tensor.pb.h' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/tensor_description.pb.h' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/tensor_shape.pb.h' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/tensor_slice.pb.h' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/types.pb.h' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/versions.pb.h' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/lib/core/error_codes.pb.h' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/protobuf/config.pb.h' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/protobuf/debug.pb.h' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/protobuf/tensor_bundle.pb.h' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/protobuf/saver.pb.h' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/util/memmapped_file_system.pb.h' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/util/saved_tensor_slice.pb.h' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/example/example_parser_configuration.pb.h' was not cre  ated.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/variable.pb.h' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/protobuf/control_flow.pb.h' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/protobuf/meta_graph.pb.h' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/protobuf/named_tensor.pb.h' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/protobuf/queue_runner.pb.h' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/protobuf/saved_model.pb.h' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/protobuf/tensorflow_server.pb.h' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/util/event.pb.h' was not created.
ERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/util/test_log.pb.h' was not created.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 187.905s, Critical Path: 171.65s
```"
6793,One hot encoding of words?,"How can I create one hot encoding of words with  each word represented by a sparse vector of vocab size and the index of that particular word equated to 1 ?
something like `oneHotEncoding(words = ['a','b','c','d']) ->  [[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]]`"
6791,Contrib support on Windows,"# step to reproduce:
attached file (https://www.dropbox.com/s/7yglwzjx4tyoguz/save_restore_model.zip?dl=0)
 is my source code
I am testing saver.save and saver.restore function under windows.
working envirment : Windows R2 Server 2012 x64, 
python 3.5 x64, TF 1.0 and master daily build, working with virtualenv
I also tested TF 0.12

1. download source code save into C:\work\tensorflow001 , 
2. create folder mnist
3. download files
(t10k-images-idx3-ubyte.gz, 
t10k-labels-idx1-ubyte.gz, 
train-images-idx3-ubyte.gz, 
train-labels-idx1-ubyte.gz) from http://yann.lecun.com/exdb/mnist/
and save into mist folder(why not using input_data from examples? because that is another bug...I think.)
4. create folder ""model"" for saving values
6. python save_restore_model.py
7. output as fallow
```
(venv643) C:\work\tensorflow001>python save_restore_model.py
Extracting mnist\train-images-idx3-ubyte.gz
Extracting mnist\train-labels-idx1-ubyte.gz
Extracting mnist\t10k-images-idx3-ubyte.gz
Extracting mnist\t10k-labels-idx1-ubyte.gz
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""BestSplits"" device_type: ""CPU""') for unknown op: BestSplits
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""CountExtremelyRandomStats"" device_type: ""CPU""') for unknown op: CountExtrem
elyRandomStats
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""FinishedNodes"" device_type: ""CPU""') for unknown op: FinishedNodes
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""GrowTree"" device_type: ""CPU""') for unknown op: GrowTree
E c:\tf_jenkins\home\workspace\release-


win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""SampleInputs"" device_type: ""CPU""') for unknown op: SampleInputs
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ScatterAddNdim"" device_type: ""CPU""') for unknown op: ScatterAddNdim
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNInsert"" device_type: ""CPU""') for unknown op: TopNInsert
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNRemove"" device_type: ""CPU""') for unknown op: TopNRemove
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TreePredictions"" device_type: ""CPU""') for unknown op: TreePredictions
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""UpdateFertileSlots"" device_type: ""CPU""') for unknown op: UpdateFertileSlots

E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ReinterpretStringToFloat"" device_type: ""CPU""') for unknown op: ReinterpretS
tringToFloat
WARNING:tensorflow:From save_restore_model.py:66: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
Starting 1st session...
Epoch: 0001 cost= 171.464307973
Epoch: 0002 cost= 43.585798355
Epoch: 0003 cost= 27.564399517
First Optimization Finished!
Accuracy: 0.9124
Model saved in file: C:\work\tensorflow001\model\result
Starting 2nd session...
W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:993] Out of range: Read fewer bytes than requested
W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:993] Out of range: Read fewer bytes than requested
W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:993] Out of range: Read fewer bytes than requested
W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:993] Out of range: Read fewer bytes than requested
W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:993] Out of range: Read fewer bytes than requested
W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:993] Out of range: Read fewer bytes than requested
W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:993] Out of range: Read fewer bytes than requested
W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:993] Out of range: Read fewer bytes than requested
W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:993] Out of range: Read fewer bytes than requested
W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:993] Out of range: Read fewer bytes than requested
W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:993] Out of range: Read fewer bytes than requested
W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:993] Out of range: Read fewer bytes than requested


W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:993] Out of range: Read fewer bytes than requested
W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:993] Out of range: Read fewer bytes than requested
W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:993] Out of range: Read fewer bytes than requested
W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:993] Out of range: Read fewer bytes than requested
W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:993] Out of range: Read fewer bytes than requested
W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:993] Out of range: Read fewer bytes than requested
W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:993] Out of range: Read fewer bytes than requested
W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:993] Out of range: Read fewer bytes than requested
Traceback (most recent call last):  File ""C:\work\tensorflow001\venv643\lib\site-packages\tensorflow\python\client\session.py"", line 1
022, in _do_call
    return fn(*args)
  File ""C:\work\tensorflow001\venv643\lib\site-packages\tensorflow\python\client\session.py"", line 1
004, in _run_fn
    status, run_metadata)
  File ""c:\python35\Lib\contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""C:\work\tensorflow001\venv643\lib\site-packages\tensorflow\python\framework\errors_impl.py"",
 line 469, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.OutOfRangeError: Read fewer bytes than requested
         [[Node: save/RestoreV2_3 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/t
ask:0/cpu:0""](_recv_save/Const_0, save/RestoreV2_3/tensor_names, save/RestoreV2_3/shape_and_slices)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""save_restore_model.py"", line 113, in <module>
    saver.restore(sess, model_path)
  File ""C:\work\tensorflow001\venv643\lib\site-packages\tensorflow\python\training\saver.py"", line 1
439, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""C:\work\tensorflow001\venv643\lib\site-packages\tensorflow\python\client\session.py"", line 7
67, in run
    run_metadata_ptr)
  File ""C:\work\tensorflow001\venv643\lib\site-packages\tensorflow\python\client\session.py"", line 9
65, in _run
    feed_dict_string, options, run_metadata)
  File ""C:\work\tensorflow001\venv643\lib\site-packages\tensorflow\python\client\session.py"", line 1
015, in _do_run
    target_list, options, run_metadata)
  File ""C:\work\tensorflow001\venv643\lib\site-packages\tensorflow\python\client\session.py"", line 1
035, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeError: Read fewer bytes than requested
         [[Node: save/RestoreV2_3 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/t
ask:0/cpu:0""](_recv_save/Const_0, save/RestoreV2_3/tensor_names, save/RestoreV2_3/shape_and_slices)]
]

Caused by op 'save/RestoreV2_3', defined at:
  File ""save_restore_model.py"", line 69, in <module>
    saver = tf.train.Saver()
  File ""C:\work\tensorflow001\venv643\lib\site-packages\tensorflow\python\training\saver.py"", line 1
051, in __init__
    self.build()
  File ""C:\work\tensorflow001\venv643\lib\site-packages\tensorflow\python\training\saver.py"", line 1
081, in build
    restore_sequentially=self._restore_sequentially)
  File ""C:\work\tensorflow001\venv643\lib\site-packages\tensorflow\python\training\saver.py"", line 6
75, in build
    restore_sequentially, reshape)
  File ""C:\work\tensorflow001\venv643\lib\site-packages\tensorflow\python\training\saver.py"", line 4
02, in _AddRestoreOps
    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)
  File ""C:\work\tensorflow001\venv643\lib\site-packages\tensorflow\python\training\saver.py"", line 2
42, in restore_op
    [spec.tensor.dtype])[0])
  File ""C:\work\tensorflow001\venv643\lib\site-packages\tensorflow\python\ops\gen_io_ops.py"", line 6
68, in restore_v2
    dtypes=dtypes, name=name)
  File ""C:\work\tensorflow001\venv643\lib\site-packages\tensorflow\python\framework\op_def_library.p
y"", line 763, in apply_op
    op_def=op_def)
  File ""C:\work\tensorflow001\venv643\lib\site-packages\tensorflow\python\framework\ops.py"", line 23
92, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""C:\work\tensorflow001\venv643\lib\site-packages\tensorflow\python\framework\ops.py"", line 12
64, in __init__
    self._traceback = _extract_stack()

OutOfRangeError (see above for traceback): Read fewer bytes than requested
         [[Node: save/RestoreV2_3 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/t
ask:0/cpu:0""](_recv_save/Const_0, save/RestoreV2_3/tensor_names, save/RestoreV2_3/shape_and_slices)]

```

# expect result
## why fallow message shows up ?
`E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""BestSplits"" device_type: ""CPU""') for unknown op: `

## most import is when I using saver.restore, it  always show fallowing error message
```
OutOfRangeError (see above for traceback): Read fewer bytes than requested
         [[Node: save/RestoreV2_3 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/t
ask:0/cpu:0""](_recv_save/Const_0, save/RestoreV2_3/tensor_names, save/RestoreV2_3/shape_and_slices)]

```
## why I cannot download example automatic by using 
```
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(""mnist"", one_hot=True)
```

"
6790,Only Master worker do predict when using estimator.Estimator.predict,"I run distributed training via a estimator.Estimator.The model is save at hdfs with multiple files.
When I use this model to predict, only the master worker do prediction while other workers do nothing,
just stop at the beginning of the for loop.

classifier = tf.contrib.learn.estimator.Estimator(
        model_fn=l2m_model_fn,
        params=model_params,
        config=run_config,
        model_dir=FLAGS.ckp_dir
    )

predictions = classifier.predict(
        input_fn=lambda: l2m_input_fn2(args),
        as_iterable=True
)

for i, p in enumerate(predictions):
        #do sth with p"
6789,TensorBoard: Improve handling of tall images,"**[Example of how it looks today](https://cloud.githubusercontent.com/assets/1595907/21849403/e4a3a9e4-d805-11e6-9210-705cff983b4a.png), and [an example of how it would look without stretching](https://cloud.githubusercontent.com/assets/1595907/21849399/da03e30a-d805-11e6-80f4-02f2a44417b9.png).**

TensorBoard's CSS for image summaries stretches the image to 100% which makes it hard to interpret images, especially with pooling operations like in [VGG-like neural networks](https://www.cs.toronto.edu/~frossard/post/vgg16/).

Also, considering datasets as [CIFAR](https://www.cs.toronto.edu/~kriz/cifar.html) have images of 32x32, the large amount of stretching leads to very blurry images. Could we either have control of the number of columns in a pane, or disable image stretching altogether?"
6788,Tf-slim: Unable to read dataset using slim.data_set_provider.DataSetProvider when images are not in JPEG.,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
Couldn't find relevant threads as there aren't many tf-slim questions.

### Environment info
Operating System:
**Ubuntu 16.04**

Installed version of CUDA and cuDNN: 
**8.00**

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
**0.11**

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
Basically I have directory of subdirectories, with each subdirectory containing png images of a certain class. Editing the tf-slim download_and_convert_flowers.py to suit my images, I created a set of tfrecord files (train and validation both included) and stored it in a directory.

Following which, I used the 'get_split' function from dataset_utils in https://github.com/tensorflow/models/blob/master/slim/datasets/dataset_utils.py
to create a DataSet class from reading the tfrecord files of a certain type (either train or validation. I indicated 'train'). So now I have a DataSet class to read.

The problem comes when I try to use a batch loading function to actually start reading the DataSet object for extracting images and creating a batch:

```
def load_batch(dataset, batch_size=32, height=299, width=299, is_training=False):
    """"""Loads a single batch of data.
    
    Args:
      dataset: The dataset to load.
      batch_size: The number of images in the batch.
      height: The size of each image after preprocessing.
      width: The size of each image after preprocessing.
      is_training: Whether or not we're currently training or evaluating.
    
    Returns:
      images: A Tensor of size [batch_size, height, width, 3], image samples that have been preprocessed.
      images_raw: A Tensor of size [batch_size, height, width, 3], image samples that can be used for visualization.
      labels: A Tensor of size [batch_size], whose values range between 0 and dataset.num_classes.
    """"""
    data_provider = slim.dataset_data_provider.DatasetDataProvider(
        dataset)
        # common_queue_capacity=32)
        # common_queue_min=8)
    image_raw, label = data_provider.get(['image', 'label'])
    
    # Preprocess image for usage by Inception.
    image = inception_preprocessing.preprocess_image(image_raw, height, width, is_training=is_training)
    
    # Preprocess the image for display purposes.
    image_raw = tf.expand_dims(image_raw, 0)
    image_raw = tf.image.resize_images(image_raw, [height, width])
    image_raw = tf.squeeze(image_raw)

    # Batch it up.
    images, images_raw, labels = tf.train.batch(
          [image, image_raw, label],
          batch_size=batch_size,
          num_threads=1,
          capacity=2 * batch_size)
    
    return images, images_raw, labels
```
The problem comes from `slim.dataset_data_provider.DatasetDataProvider` as I couldn't read the dataset at all.

Here is my error traceback:

```
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: 
name: GeForce GTX 860M
major: 5 minor: 0 memoryClockRate (GHz) 1.0195
pciBusID 0000:01:00.0
Total memory: 3.95GiB
Free memory: 3.60GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 860M, pci bus id: 0000:01:00.0)
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 1898 get requests, put_count=1100 evicted_count=1000 eviction_rate=0.909091 and unsatisfied allocation rate=1
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 100 to 110
INFO:tensorflow:Starting Session.
INFO:tensorflow:Starting Queues.
INFO:tensorflow:global_step/sec: 0
Not a JPEG file: starts with 0x89 0x50
Not a JPEG file: starts with 0x89 0x50
INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors.InvalidArgumentError'>, Invalid JPEG data, size 26498
	 [[Node: case/If_0/DecodeJpeg = DecodeJpeg[acceptable_fraction=1, channels=3, fancy_upscaling=true, ratio=1, try_recover_truncated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](case/If_0/DecodeJpeg/Switch:1, ^case/Assert/AssertGuard/Merge/_7825)]]

Caused by op u'case/If_0/DecodeJpeg', defined at:
  File ""code.py"", line 148, in <module>
    images, images_raw, labels = load_batch(dataset, height = image_size, batch_size = batch_size, width = image_size, is_training = True)
  File ""code.py"", line 27, in load_batch
    dataset)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/data/dataset_data_provider.py"", line 78, in __init__
    tensors = dataset.decoder.decode(data, items)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/data/tfexample_decoder.py"", line 398, in decode
    outputs.append(handler.tensors_to_item(keys_to_tensors))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/data/tfexample_decoder.py"", line 297, in tensors_to_item
    image = self._decode(image_buffer, image_format)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/data/tfexample_decoder.py"", line 324, in _decode
    }, default=decode_jpg, exclusive=True)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 2886, in case
    case_seq = _build_case()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 2868, in _build_case
    name=""If_%d"" % i)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 1710, in cond
    orig_res, res_t = context_t.BuildCondBranch(fn1)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 1613, in BuildCondBranch
    r = fn()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/data/tfexample_decoder.py"", line 317, in decode_jpg
    return image_ops.decode_jpeg(image_buffer, self._channels)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_image_ops.py"", line 283, in decode_jpeg
    name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 749, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2380, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1298, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): Invalid JPEG data, size 26498
	 [[Node: case/If_0/DecodeJpeg = DecodeJpeg[acceptable_fraction=1, channels=3, fancy_upscaling=true, ratio=1, try_recover_truncated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](case/If_0/DecodeJpeg/Switch:1, ^case/Assert/AssertGuard/Merge/_7825)]]

W tensorflow/core/framework/op_kernel.cc:968] Out of range: FIFOQueue '_0_batch/fifo_queue' is closed and has insufficient elements (requested 3, current size 0)
	 [[Node: batch = QueueDequeueMany[_class=[""loc:@batch/fifo_queue""], component_types=[DT_FLOAT, DT_FLOAT, DT_INT64], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](batch/fifo_queue, batch/n)]]
W tensorflow/core/framework/op_kernel.cc:968] Out of range: FIFOQueue '_0_batch/fifo_queue' is closed and has insufficient elements (requested 3, current size 0)
	 [[Node: batch = QueueDequeueMany[_class=[""loc:@batch/fifo_queue""], component_types=[DT_FLOAT, DT_FLOAT, DT_INT64], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](batch/fifo_queue, batch/n)]]
Traceback (most recent call last):
  File ""code.py"", line 171, in <module>
    number_of_steps = 10)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 780, in train
    raise
  File ""/usr/lib/python2.7/contextlib.py"", line 35, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 969, in managed_session
    self.stop(close_summary_writer=close_summary_writer)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 797, in stop
    stop_grace_period_secs=self._stop_grace_secs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py"", line 386, in join
    six.reraise(*self._exc_info_to_raise)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/queue_runner.py"", line 225, in _run
    sess.run(enqueue_op)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 717, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 915, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 965, in _do_run
    target_list, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 985, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors.InvalidArgumentError: Invalid JPEG data, size 26498
	 [[Node: case/If_0/DecodeJpeg = DecodeJpeg[acceptable_fraction=1, channels=3, fancy_upscaling=true, ratio=1, try_recover_truncated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](case/If_0/DecodeJpeg/Switch:1, ^case/Assert/AssertGuard/Merge/_7825)]]

Caused by op u'case/If_0/DecodeJpeg', defined at:
  File ""code.py"", line 148, in <module>
    images, images_raw, labels = load_batch(dataset, height = image_size, batch_size = batch_size, width = image_size, is_training = True)
  File ""code.py"", line 27, in load_batch
    dataset)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/data/dataset_data_provider.py"", line 78, in __init__
    tensors = dataset.decoder.decode(data, items)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/data/tfexample_decoder.py"", line 398, in decode
    outputs.append(handler.tensors_to_item(keys_to_tensors))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/data/tfexample_decoder.py"", line 297, in tensors_to_item
    image = self._decode(image_buffer, image_format)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/data/tfexample_decoder.py"", line 324, in _decode
    }, default=decode_jpg, exclusive=True)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 2886, in case
    case_seq = _build_case()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 2868, in _build_case
    name=""If_%d"" % i)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 1710, in cond
    orig_res, res_t = context_t.BuildCondBranch(fn1)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 1613, in BuildCondBranch
    r = fn()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/data/tfexample_decoder.py"", line 317, in decode_jpg
    return image_ops.decode_jpeg(image_buffer, self._channels)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_image_ops.py"", line 283, in decode_jpeg
    name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 749, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2380, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1298, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): Invalid JPEG data, size 26498
	 [[Node: case/If_0/DecodeJpeg = DecodeJpeg[acceptable_fraction=1, channels=3, fancy_upscaling=true, ratio=1, try_recover_truncated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](case/If_0/DecodeJpeg/Switch:1, ^case/Assert/AssertGuard/Merge/_7825)]]
```
Upon inspection, I have checked that a likely error comes from the line ```  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/data/tfexample_decoder.py"", line 297, in tensors_to_item
    image = self._decode(image_buffer, image_format)```

in tfexample_decoder.py, the image_format, if not indicated, will be a JPEG image by default unless there are 4 channels (RGBA) in the images. But since my images are grayscale, a JPEG decoder is used by default. Yet, I have no way to specify the image format as 'png' specifically unless the source code is changed. How should I go about this, or am I mistaken about the error I have arrived at?

I have previously successfully run the code (many of which are referenced from the tf-slim walkthrough ipynb file), but the images I used were in jpeg. 

Any help is very much appreciated. Thank you for your time."
6787,could not find cuDevicePrimaryCtxSetFlags in libcuda DSO; dlerror: /usr/lib/libcuda.so.1: undefined symbol: cuDevicePrimaryCtxSetFlags,"Hi, when trying to get tensorflow on my Nvidia GPU, the output started with the following:

    I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
    I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
    I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
    I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
    I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
    F tensorflow/stream_executor/cuda/cuda_driver.cc:94] Check failed: s.ok() could not find cuDevicePrimaryCtxSetFlags in libcuda DSO; dlerror: /usr/lib/libcuda.so.1: undefined symbol: cuDevicePrimaryCtxSetFlags
After which, it exits.

I'm running with the following:

`uname -r`:
    
    4.8.13-1-ARCH


`lspci`:
    
    VGA compatible controller: NVIDIA Corporation GM107 [GeForce GTX 750] (rev a2)
    Subsystem: Gigabyte Technology Co., Ltd Device 362e
    Kernel driver in use: nvidia

`cat /proc/driver/nvidia/version`:

    NVRM version: NVIDIA UNIX x86_64 Kernel Module  340.101  Thu Dec  1 15:52:31 PST 2016
    GCC version:  gcc version 6.2.1 20160830 (GCC)

`ls -l /usr/lib/libcuda*`:

    lrwxrwxrwx 1 root root       12 Dec 16 01:18 /usr/lib/libcuda.so -> libcuda.so.1*
    lrwxrwxrwx 1 root root       18 Dec 16 01:18 /usr/lib/libcuda.so.1 -> libcuda.so.340.101*
    -rwxr-xr-x 1 root root 14011752 Dec 16 01:18 /usr/lib/libcuda.so.340.101*

Tensorflow version:
    
    0.12.1

**Note:** I _am_ running tensorflow on Python 3.6 having changed 'cp35' to 'cp36' in the file name of the binary, but I presume that this does not matter considering that this is a cuda issue, and that the CPU version worked fine.

The line that the error references is this one:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_driver.cc#L94

Whats going on here, and what can I do to fix this?"
6786,"Variable ""matching_filenames"" @  tf.train.match_filenames_once() should be local?","This var holds the matched file names for the input stream pipeline.
Since it is added to the graph, when using the contractor of tf.train.Saver() it is saved
as part of the checkpoint state.
Hence loading saved models from checkpoints for inference, the training file names are loaded instead of the val / inference files.

Shouldn't this Var become a local variable? will it not be saved by default treated as local?
"
6785,Bug with TensorFlow Serving in TensorFlow 0.12,"I have found the following code works fine when using TensorFlow 0.11, installed using:
export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0rc2-cp27-none-linux_x86_64.whl

However when using TensorFlow 0.12 installed using:
export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp27-none-linux_x86_64.whl

I get the following issue when loading the model:
```
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: GeForce GTX 960M
major: 5 minor: 0 memoryClockRate (GHz) 1.176
pciBusID 0000:02:00.0
Total memory: 3.95GiB
Free memory: 3.92GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 960M, pci bus id: 0000:02:00.0)
Traceback (most recent call last):
  File ""./demo2.py"", line 101, in <module>
    outp = sess.run(out, feed_dict={inp: [v]})[0]
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 766, in run
    run_metadata_ptr)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 964, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1014, in _do_run
    target_list, options, run_metadata)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1034, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value convolution2d_1_W_1
	 [[Node: convolution2d_1_W_1/read = Identity[T=DT_FLOAT, _class=[""loc:@convolution2d_1_W_1""], _device=""/job:localhost/replica:0/task:0/gpu:0""](convolution2d_1_W_1)]]
	 [[Node: Reshape_9/_7 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_2_Reshape_9"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

Caused by op u'convolution2d_1_W_1/read', defined at:
  File ""./demo2.py"", line 32, in <module>
    sess,inp,out,classes = load_graph(""/tmp/cnn/00000001/"")
  File ""./demo2.py"", line 18, in load_graph
    sess, meta_graph_def = session_bundle.load_session_bundle_from_path(output_graph_path)
  File ""/usr/lib/python2.7/site-packages/tensorflow/contrib/session_bundle/session_bundle.py"", line 95, in load_session_bundle_from_path
    saver = tf.train.import_meta_graph(meta_graph_def)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1526, in import_meta_graph
    **kwargs)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/meta_graph.py"", line 502, in import_scoped_meta_graph
    producer_op_list=producer_op_list)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/importer.py"", line 285, in import_graph_def
    op_def=op_def)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2240, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1128, in __init__
    self._traceback = _extract_stack()

FailedPreconditionError (see above for traceback): Attempting to use uninitialized value convolution2d_1_W_1
	 [[Node: convolution2d_1_W_1/read = Identity[T=DT_FLOAT, _class=[""loc:@convolution2d_1_W_1""], _device=""/job:localhost/replica:0/task:0/gpu:0""](convolution2d_1_W_1)]]
	 [[Node: Reshape_9/_7 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_2_Reshape_9"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
```

Create model:

```
#!/usr/bin/env python2

from __future__ import division, print_function, absolute_import
import tensorflow as tf
import numpy as np
import keras
from tensorflow.contrib.session_bundle import exporter

from keras.optimizers  import Adam
from keras.constraints import MaxNorm

import keras.models as models
from keras.layers.core import Reshape,Dense,Dropout,Activation,Flatten
from keras.layers.noise import GaussianNoise
from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D
from keras.regularizers import *
from keras.optimizers import adam


from keras.layers.core import Dense, Dropout, Activation, Flatten, Reshape, Merge
from keras import backend as K
from keras.callbacks import TensorBoard
from keras.preprocessing.image import ImageDataGenerator
from keras.regularizers import l2


mods = [""psk"",""fm""]

sess = tf.Session()

init_op = tf.group(tf.initialize_all_variables(), tf.initialize_local_variables())

K.set_session(sess)
K.set_learning_phase(1)

classes = mods

X_train = np.array([[[  1.23725709e-04,   3.21811251e-03,   8.11603665e-03,  -3.23975401e-04,
    1.07787224e-02,   2.94970046e-03,   4.02887911e-03,  -1.88605103e-03,
    1.07172830e-03,  -4.49557183e-03,   4.48013563e-03,  -4.36049374e-03,
    2.03782506e-03,   3.55514325e-03,  -6.66785333e-03,   1.42298348e-03,
   -1.50406240e-02,  -2.57269153e-03,   5.56691317e-03,   1.13693373e-02,
    2.71817949e-03,   5.25468131e-05,  -3.36736324e-03,  -9.75306786e-04,
    3.72400926e-03,   1.18668824e-02,  -1.42240804e-03,   1.44388294e-02,
    8.27464042e-04,   6.72260066e-03,  -5.76557242e-04,   1.67079712e-03,
   -1.02783879e-02,  -6.51435228e-04,  -1.59728453e-02,  -2.92223319e-03,
   -9.65267327e-03,   8.49308504e-04,  -4.86527057e-03,  -2.24598357e-03,
   -6.85441250e-04,  -7.86077511e-03,   8.25393759e-03,  -8.98091588e-03,
   -4.13634349e-03,   5.30316820e-03,  -2.96568638e-03,   7.47767324e-03,
    7.87805114e-03,   3.33608547e-03,   6.81637728e-04,  -3.26122390e-03,
   -5.83499251e-03,   6.05301978e-03,   3.19009693e-03,  -4.02274629e-04,
   -1.09375454e-02,   1.74824963e-03,   1.75286271e-03,  -1.26824277e-02,
    7.39318645e-03,   3.90595663e-03,   1.98931666e-03,   6.12980360e-03,
   -1.79607305e-03,   1.40197761e-02,  -9.81968828e-03,   5.02704596e-03,
    3.01699433e-03,  -6.52436400e-03,  -1.62929588e-03,   7.39145232e-03,
   -9.01302416e-03,   3.07239546e-03,  -1.82828668e-03,  -5.16610499e-03,
    2.61074631e-03,  -1.33442272e-05,   5.14040841e-03,  -6.64286781e-03,
    1.71982939e-03,  -4.85043926e-03,  -3.97882238e-03,  -1.37700920e-03,
   -1.10943802e-02,   2.20915396e-03,  -3.39583290e-04,   4.58237901e-03,
    4.64649638e-03,  -2.94209481e-03,  -1.55386878e-02,   5.68915205e-03,
   -5.80187945e-04,  -5.83021576e-03,  -3.40874423e-04,  -1.83014176e-03,
    3.61575768e-03,  -8.90286360e-03,  -4.51745838e-03,   1.93125161e-03,
   -8.45910795e-03,  -5.77241089e-03,   8.37801304e-03,  -1.38715087e-02,
   -3.31607228e-03,   5.12827048e-03,   9.08580422e-03,   1.84341776e-03,
   -8.04373343e-03,  -9.29000136e-03,  -9.47526656e-04,   4.20172885e-03,
   -5.26063796e-03,  -6.72675669e-03,  -3.59727233e-03,   4.08909051e-03,
    7.33140949e-03,  -7.76879140e-04,  -2.54971418e-03,  -8.70507117e-03,
    7.90149346e-03,   7.96920154e-03,   2.96101277e-03,  -5.38653834e-03,
   -4.88629332e-04,   1.00093251e-02,  -4.25783452e-03,  -5.71854087e-03],
 [ -6.87929394e-04,   4.60408907e-03,   7.26573868e-04,   4.26992076e-03,
   -7.02272868e-03,   2.22673942e-03,   1.22035667e-02,  -8.09120014e-03,
    1.39266049e-04,  -1.15088280e-02,   5.04087773e-04,  -2.15286622e-03,
   -7.33058015e-03,  -9.15534515e-03,  -5.08288946e-03,  -1.30671002e-02,
    1.47830602e-03,   1.16440572e-03,   4.02440550e-03,   8.58596340e-03,
    3.03325080e-03,  -2.05237372e-03,   1.05325170e-02,  -1.80078077e-03,
    4.63060196e-03,   1.23807620e-02,  -6.47541787e-03,  -4.13759379e-03,
   -2.83148466e-03,   7.43190618e-03,  -1.15842454e-03,   6.59148069e-03,
    5.59045048e-03,   3.75851267e-03,  -3.95106524e-03,  -2.56526005e-03,
    6.27654884e-03,  -1.24440319e-03,   3.46388144e-04,  -1.55166397e-03,
    1.04056811e-02,   1.30844014e-02,  -6.36276463e-03,  -6.97820855e-04,
   -3.15165240e-03,  -1.41060480e-03,   1.38492498e-03,   8.64384789e-03,
   -7.11268140e-03,  -1.76842324e-03,  -1.25329485e-02,  -4.83873859e-03,
   -5.18619781e-03,   1.30472714e-02,  -5.54988487e-03,   8.61867797e-03,
   -3.99610912e-03,   6.70848880e-04,  -9.35312640e-03,   1.23843951e-02,
   -3.27547453e-03,   4.86938097e-03,  -2.92926189e-03,   2.20531784e-03,
    4.75586858e-03,   3.00767994e-03,   7.01231230e-03,   1.93257479e-03,
    5.04882913e-03,  -1.04642799e-02,   7.11998856e-03,  -2.53466447e-03,
    1.29708648e-03,   1.07713938e-02,  -2.99122441e-03,   5.51079051e-04,
    5.26238093e-03,  -8.22351780e-04,   5.74991386e-03,   8.99204868e-04,
    1.13037638e-02,  -1.46015978e-03,   6.79054251e-03,  -2.91314325e-03,
    6.34925021e-03,   4.45276871e-03,   7.88977742e-03,  -5.24963858e-03,
   -2.57161981e-03,   5.67252794e-03,  -2.77268351e-03,   2.27351789e-03,
    3.13360780e-03,   9.63459164e-03,   3.79459164e-03,   2.40193959e-03,
    3.09617817e-03,   5.46766398e-03,  -1.21412217e-03,  -7.90829584e-03,
   -1.29530125e-03,  -7.43942289e-03,   3.87186417e-03,  -8.09667457e-04,
    1.91524532e-03,  -3.64716118e-03,   8.53588711e-03,  -5.08366944e-03,
    1.74835534e-03,   7.45685189e-04,  -4.69580526e-03,  -1.06729409e-02,
   -4.90031298e-03,  -2.10527773e-03,  -1.65928528e-02,  -9.58569441e-03,
    4.94536944e-03,   6.38392800e-03,  -1.75752665e-03,  -2.40226928e-03,
    4.23104968e-03,   4.33860486e-03,   4.77843359e-03,   4.94898483e-03,
    8.75659316e-05,   6.20994205e-03,   1.03254039e-02,   1.75160269e-04]]])

Y_train = np.array([[1.,0.]])
#X_test = test_i
#Y_test = test_o
in_shp = list(X_train.shape[1:])

dr = 0.5 # dropout rate (%)
model = models.Sequential()
model.add(Reshape([1]+in_shp, input_shape=in_shp))
model.add(ZeroPadding2D((0, 2)))
model.add(Convolution2D(256, 1, 3, border_mode='valid', activation=""relu"", init='glorot_uniform'))
model.add(Dropout(dr))
model.add(ZeroPadding2D((0, 2)))
model.add(Convolution2D(80, 2, 3, border_mode=""valid"", activation=""relu"",  init='glorot_uniform'))
model.add(Dropout(dr))
model.add(Flatten())
model.add(Dense(256, activation='relu', init='he_normal'))
model.add(Dropout(dr))
model.add(Dense( len(classes), init='he_normal' ))
model.add(Activation('softmax',name=""out""))
model.add(Reshape([len(classes)]))
model.compile(loss='categorical_crossentropy', optimizer='adam')
model.summary()

batch_size=1
nb_epoch = 1

sess.run([init_op])


history = model.fit(X_train,Y_train,batch_size=batch_size,nb_epoch=nb_epoch,show_accuracy=False,verbose=2,callbacks = [])

K.set_learning_phase(0)

config = model.get_config()
weights = model.get_weights()

new_model = models.Sequential.from_config(config)
new_model.set_weights(weights)

export_path = ""/tmp/cnn""
export_version = 1

labels_tensor = tf.constant(mods)

saver = tf.train.Saver(sharded=True)
model_exporter = exporter.Exporter(saver)
signature = exporter.classification_signature(
input_tensor=new_model.input,classes_tensor=labels_tensor, scores_tensor=new_model.output)
model_exporter.init(sess.graph.as_graph_def(),
                    default_graph_signature=signature)
model_exporter.export(export_path, tf.constant(export_version), sess)
```

Load model 
```
#!/usr/bin/python2

from __future__ import division, print_function, absolute_import
import tensorflow as tf    
from tensorflow.contrib.session_bundle import manifest_pb2
from tensorflow.contrib.session_bundle import constants
from tensorflow.contrib.session_bundle import session_bundle
 
import pmt
import numpy as np
from gnuradio import gr
import tensorflow as tf
from numpy import zeros, newaxis
import collections

def load_graph(output_graph_path):

    sess, meta_graph_def = session_bundle.load_session_bundle_from_path(output_graph_path)
    with sess.as_default():
        collection_def = meta_graph_def.collection_def
        signatures_any = collection_def[
        constants.SIGNATURES_KEY].any_list.value
        signatures = manifest_pb2.Signatures()
        signatures_any[0].Unpack(signatures)
        default_signature = signatures.default_signature
        input_name = default_signature.classification_signature.input.tensor_name
        output_name = default_signature.classification_signature.scores.tensor_name
        classes = default_signature.classification_signature.classes.tensor_name
        classes = sess.run(sess.graph.get_tensor_by_name(classes))
        return (sess, input_name, output_name,classes)

sess,inp,out,classes = load_graph(""/tmp/cnn/00000001/"")


v = np.array([[  1.23725709e-04,   3.21811251e-03,   8.11603665e-03,  -3.23975401e-04,
    1.07787224e-02,   2.94970046e-03,   4.02887911e-03,  -1.88605103e-03,
    1.07172830e-03,  -4.49557183e-03,   4.48013563e-03,  -4.36049374e-03,
    2.03782506e-03,   3.55514325e-03,  -6.66785333e-03,   1.42298348e-03,
   -1.50406240e-02,  -2.57269153e-03,   5.56691317e-03,   1.13693373e-02,
    2.71817949e-03,   5.25468131e-05,  -3.36736324e-03,  -9.75306786e-04,
    3.72400926e-03,   1.18668824e-02,  -1.42240804e-03,   1.44388294e-02,
    8.27464042e-04,   6.72260066e-03,  -5.76557242e-04,   1.67079712e-03,
   -1.02783879e-02,  -6.51435228e-04,  -1.59728453e-02,  -2.92223319e-03,
   -9.65267327e-03,   8.49308504e-04,  -4.86527057e-03,  -2.24598357e-03,
   -6.85441250e-04,  -7.86077511e-03,   8.25393759e-03,  -8.98091588e-03,
   -4.13634349e-03,   5.30316820e-03,  -2.96568638e-03,   7.47767324e-03,
    7.87805114e-03,   3.33608547e-03,   6.81637728e-04,  -3.26122390e-03,
   -5.83499251e-03,   6.05301978e-03,   3.19009693e-03,  -4.02274629e-04,
   -1.09375454e-02,   1.74824963e-03,   1.75286271e-03,  -1.26824277e-02,
    7.39318645e-03,   3.90595663e-03,   1.98931666e-03,   6.12980360e-03,
   -1.79607305e-03,   1.40197761e-02,  -9.81968828e-03,   5.02704596e-03,
    3.01699433e-03,  -6.52436400e-03,  -1.62929588e-03,   7.39145232e-03,
   -9.01302416e-03,   3.07239546e-03,  -1.82828668e-03,  -5.16610499e-03,
    2.61074631e-03,  -1.33442272e-05,   5.14040841e-03,  -6.64286781e-03,
    1.71982939e-03,  -4.85043926e-03,  -3.97882238e-03,  -1.37700920e-03,
   -1.10943802e-02,   2.20915396e-03,  -3.39583290e-04,   4.58237901e-03,
    4.64649638e-03,  -2.94209481e-03,  -1.55386878e-02,   5.68915205e-03,
   -5.80187945e-04,  -5.83021576e-03,  -3.40874423e-04,  -1.83014176e-03,
    3.61575768e-03,  -8.90286360e-03,  -4.51745838e-03,   1.93125161e-03,
   -8.45910795e-03,  -5.77241089e-03,   8.37801304e-03,  -1.38715087e-02,
   -3.31607228e-03,   5.12827048e-03,   9.08580422e-03,   1.84341776e-03,
   -8.04373343e-03,  -9.29000136e-03,  -9.47526656e-04,   4.20172885e-03,
   -5.26063796e-03,  -6.72675669e-03,  -3.59727233e-03,   4.08909051e-03,
    7.33140949e-03,  -7.76879140e-04,  -2.54971418e-03,  -8.70507117e-03,
    7.90149346e-03,   7.96920154e-03,   2.96101277e-03,  -5.38653834e-03,
   -4.88629332e-04,   1.00093251e-02,  -4.25783452e-03,  -5.71854087e-03],
 [ -6.87929394e-04,   4.60408907e-03,   7.26573868e-04,   4.26992076e-03,
   -7.02272868e-03,   2.22673942e-03,   1.22035667e-02,  -8.09120014e-03,
    1.39266049e-04,  -1.15088280e-02,   5.04087773e-04,  -2.15286622e-03,
   -7.33058015e-03,  -9.15534515e-03,  -5.08288946e-03,  -1.30671002e-02,
    1.47830602e-03,   1.16440572e-03,   4.02440550e-03,   8.58596340e-03,
    3.03325080e-03,  -2.05237372e-03,   1.05325170e-02,  -1.80078077e-03,
    4.63060196e-03,   1.23807620e-02,  -6.47541787e-03,  -4.13759379e-03,
   -2.83148466e-03,   7.43190618e-03,  -1.15842454e-03,   6.59148069e-03,
    5.59045048e-03,   3.75851267e-03,  -3.95106524e-03,  -2.56526005e-03,
    6.27654884e-03,  -1.24440319e-03,   3.46388144e-04,  -1.55166397e-03,
    1.04056811e-02,   1.30844014e-02,  -6.36276463e-03,  -6.97820855e-04,
   -3.15165240e-03,  -1.41060480e-03,   1.38492498e-03,   8.64384789e-03,
   -7.11268140e-03,  -1.76842324e-03,  -1.25329485e-02,  -4.83873859e-03,
   -5.18619781e-03,   1.30472714e-02,  -5.54988487e-03,   8.61867797e-03,
   -3.99610912e-03,   6.70848880e-04,  -9.35312640e-03,   1.23843951e-02,
   -3.27547453e-03,   4.86938097e-03,  -2.92926189e-03,   2.20531784e-03,
    4.75586858e-03,   3.00767994e-03,   7.01231230e-03,   1.93257479e-03,
    5.04882913e-03,  -1.04642799e-02,   7.11998856e-03,  -2.53466447e-03,
    1.29708648e-03,   1.07713938e-02,  -2.99122441e-03,   5.51079051e-04,
    5.26238093e-03,  -8.22351780e-04,   5.74991386e-03,   8.99204868e-04,
    1.13037638e-02,  -1.46015978e-03,   6.79054251e-03,  -2.91314325e-03,
    6.34925021e-03,   4.45276871e-03,   7.88977742e-03,  -5.24963858e-03,
   -2.57161981e-03,   5.67252794e-03,  -2.77268351e-03,   2.27351789e-03,
    3.13360780e-03,   9.63459164e-03,   3.79459164e-03,   2.40193959e-03,
    3.09617817e-03,   5.46766398e-03,  -1.21412217e-03,  -7.90829584e-03,
   -1.29530125e-03,  -7.43942289e-03,   3.87186417e-03,  -8.09667457e-04,
    1.91524532e-03,  -3.64716118e-03,   8.53588711e-03,  -5.08366944e-03,
    1.74835534e-03,   7.45685189e-04,  -4.69580526e-03,  -1.06729409e-02,
   -4.90031298e-03,  -2.10527773e-03,  -1.65928528e-02,  -9.58569441e-03,
    4.94536944e-03,   6.38392800e-03,  -1.75752665e-03,  -2.40226928e-03,
    4.23104968e-03,   4.33860486e-03,   4.77843359e-03,   4.94898483e-03,
    8.75659316e-05,   6.20994205e-03,   1.03254039e-02,   1.75160269e-04]])


outp = sess.run(out, feed_dict={inp: [v]})[0]

print(outp)
```

Thanks in advance!
"
6783,"TensorFlow GPU, CUDA_ERROR_LAUNCH_FAILED on tf.one_hot()","Use tf.one_hot() on Windows 10, GPU, Nvidia 970.  Error CUDA_ERROR_LAUNCH_FAILED happens.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

I have post an issue in http://stackoverflow.com/questions/41115476/tensorflow-gpu-cuda-error-launch-failed-on-tf-one-hot .  No solution found.  More people report the same issue.

### Environment info

- TensorFlow 0.12.0-rc1
- Python 3.5
- CUDA 8.0
- cuDNN 5.1
- OS: Windows 10
- GPU: GeForce GTX 970

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

```
import tensorflow as tf
idx_0 = tf.placeholder(tf.int64, [None])
mask = tf.one_hot(idx_0, 3, axis=-1)
sess = tf.Session()
sess.run(tf.global_variables_initializer())
a = sess.run([mask],feed_dict={idx_0:[0,1,2]})
print(a)
```

### What other attempted solutions have you tried?

1. Run the code in same machine, Linux, GPU.  No error.
2. Run the code in same machine, Windows, CPU.  No error.

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).

```
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\cuda\cuda_driver.cc:1177] could not synchronize on CUDA context: CUDA_ERROR_LAUNCH_FAILED :: No stack trace available
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\cuda\cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED
F c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_util.cc:370] GPU sync failed
```"
6782,./configure has issues after copying the tensorflow codebase to a different machine,"I copied my modified tensorflow codebase to a different machine. After running ./configure, I got the following errors. How can I fix it?

ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:17:3: //external:eigen_archive: no such attribute 'urls' in 'new_http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:17:3: //external:eigen_archive: missing value for mandatory attribute 'url' in 'new_http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:28:3: //external:libxsmm_archive: no such attribute 'urls' in 'new_http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:28:3: //external:libxsmm_archive: missing value for mandatory attribute 'url' in 'new_http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:44:3: //external:com_googlesource_code_re2: no such attribute 'urls' in 'http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:44:3: //external:com_googlesource_code_re2: missing value for mandatory attribute 'url' in 'http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:54:3: //external:gemmlowp: no such attribute 'urls' in 'http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:54:3: //external:gemmlowp: missing value for mandatory attribute 'url' in 'http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:64:3: //external:farmhash_archive: no such attribute 'urls' in 'new_http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:64:3: //external:farmhash_archive: missing value for mandatory attribute 'url' in 'new_http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:80:3: //external:highwayhash: no such attribute 'urls' in 'http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:80:3: //external:highwayhash: missing value for mandatory attribute 'url' in 'http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:90:3: //external:nasm: no such attribute 'urls' in 'new_http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:90:3: //external:nasm: missing value for mandatory attribute 'url' in 'new_http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:101:3: //external:jpeg: no such attribute 'urls' in 'new_http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:101:3: //external:jpeg: missing value for mandatory attribute 'url' in 'new_http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:112:3: //external:png_archive: no such attribute 'urls' in 'new_http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:112:3: //external:png_archive: missing value for mandatory attribute 'url' in 'new_http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:123:3: //external:gif_archive: no such attribute 'urls' in 'new_http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:123:3: //external:gif_archive: missing value for mandatory attribute 'url' in 'new_http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:135:3: //external:six_archive: no such attribute 'urls' in 'new_http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:135:3: //external:six_archive: missing value for mandatory attribute 'url' in 'new_http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:151:3: //external:protobuf: no such attribute 'urls' in 'http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:151:3: //external:protobuf: missing value for mandatory attribute 'url' in 'http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:161:3: //external:gmock_archive: no such attribute 'urls' in 'new_http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:161:3: //external:gmock_archive: missing value for mandatory attribute 'url' in 'new_http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:187:3: //external:pcre: no such attribute 'urls' in 'new_http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:187:3: //external:pcre: missing value for mandatory attribute 'url' in 'new_http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:198:3: //external:swig: no such attribute 'urls' in 'new_http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:198:3: //external:swig: missing value for mandatory attribute 'url' in 'new_http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:210:3: //external:curl: no such attribute 'urls' in 'new_http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:210:3: //external:curl: missing value for mandatory attribute 'url' in 'new_http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:233:3: //external:grpc: no such attribute 'urls' in 'new_http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:233:3: //external:grpc: missing value for mandatory attribute 'url' in 'new_http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:256:3: //external:linenoise: no such attribute 'urls' in 'new_http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:256:3: //external:linenoise: missing value for mandatory attribute 'url' in 'new_http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:269:3: //external:llvm: no such attribute 'urls' in 'new_http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:269:3: //external:llvm: missing value for mandatory attribute 'url' in 'new_http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:280:3: //external:jsoncpp_git: no such attribute 'urls' in 'new_http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:280:3: //external:jsoncpp_git: missing value for mandatory attribute 'url' in 'new_http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:296:3: //external:boringssl: no such attribute 'urls' in 'http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:296:3: //external:boringssl: missing value for mandatory attribute 'url' in 'http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:306:3: //external:nanopb_git: no such attribute 'urls' in 'new_http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:306:3: //external:nanopb_git: missing value for mandatory attribute 'url' in 'new_http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:322:3: //external:zlib_archive: no such attribute 'urls' in 'new_http_archive' rule.
ERROR: /home/bwu/ResearchProjects/tensorflow_org/tensorflow/workspace.bzl:322:3: //external:zlib_archive: missing value for mandatory attribute 'url' in 'new_http_archive' rule.
ERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': error loading package 'external': Could not load //external package.
ERROR: missing fetch expression. Type 'bazel help fetch' for syntax and help."
6781,Java API: How to implement a PlaceHolder,"I've been loving experimenting with @asimshankar 's Java API, and finally got it working tonight with a model that i've trained myself based of of inception. However, I noticed his comment in his tutorial/demo java class that says:

> Since the graph is being constructed once per execution here, we can use a constant for the input image. If the graph were to be re-used for multiple input images, a placeholder would have been more appropriate.

I'd be very interested in seeing how to do this with the java API. I want to be able to optimize using the same graph for a bunch of images which would be fed into the graph in real-time, so optimization is important. Would a modification to the graph be necessary in order to use a placeholder, or is it just a different manipulation of the API? I couldn't find any references to a placeholder class or method. Any help is much appreciated. "
6780,crash when run distributed training,"When I run distributed training on tensorflow 0.12, everything ok at first, loss and global step was printed.
But after thoudsands of steps, following errors appear

tensorflow.python.framework.errors_impl.UnavailableError: {""created"":""@1484093728.844289839"",""description"":""EOF"",""file"":""external/grpc/src/core/lib/iomgr/tcp_posix.c"",""file_line"":235,"" grpc_status"":14}

and

tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device to node 'save/RestoreV2_26': Could not satisfy explicit device specification

and

E tensorflow/core/distributed_runtime/master_session.cc:1372] Cleanup partition error: Unavailable"
6779,Training Slows Down -- But Speeds up If Entire Net is Reloaded,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

There are a few issues but none that simulate the same problem.

### Environment info
Operating System: Ubuntu 14.04, tf 0.12

Installed version of CUDA and cuDNN: 8.0
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide: tensorflow pip gpu 0.12

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

### Problem

I am training a network, but after about 4000 backward passes, the training of the network starts to slow down. I have tried reloading the network after saving it. And again, 4000 steps later it starts to slow down. This means that the slow down is independent of what global step the training is on. The network size never changes, which is usually the problem with these slow downs. Average Step times are like this:

`2.4, 2.4, 2.4, 4.6, 3.9, 12.2, 4.2, 2.4, 6.7`

I am feeding the model with placeholders, so there is no FIFO Queue problems, and feeding it with text data so its pretty light. I have tested this on three separate machines and the same behavior is replicated throughout them. It is the actual tensorflow `session.run` command that slows it down tremendously.

When I look at the gpu usage, both gpu's are at 0 percent the entire time (even during the `session.run` command). They spike up only when the actual forward and backward passes occur but remain dormant the rest of the time. I'm using allocator type 2 in the `tf.gradient` operation. This perhaps is the most revealing part.

My `htop` indicates there's no swap memory problems at all. 

From what I have concluded the problem must be with tensorflow. The fact that You can reload the model and its completely fine for a little bit is really weird. I've also found that running the following command does not help:

`sync; echo 3 > /proc/sys/vm/drop_caches`

```
INFO:tensorflow:global step 2001 learning rate 0.0003882 step-time 2.94 perplexity 28.97 loss 3.3662
INFO:tensorflow:get_batch_step_time 0.016 actual_step_time 2.929
INFO:tensorflow:16:01:09 01/10/17 EST
INFO:tensorflow:global step 2251 learning rate 0.0003867 step-time 2.94 perplexity 27.17 loss 3.3022
INFO:tensorflow:get_batch_step_time 0.016 actual_step_time 2.920
INFO:tensorflow:16:13:25 01/10/17 EST
INFO:tensorflow:global step 2501 learning rate 0.0003853 step-time 2.94 perplexity 26.03 loss 3.2594
INFO:tensorflow:get_batch_step_time 0.016 actual_step_time 2.929
INFO:tensorflow:16:25:48 01/10/17 EST
INFO:tensorflow:global step 2751 learning rate 0.0003838 step-time 2.97 perplexity 24.80 loss 3.2109
INFO:tensorflow:get_batch_step_time 0.016 actual_step_time 2.956
INFO:tensorflow:16:44:37 01/10/17 EST
INFO:tensorflow:global step 3001 learning rate 0.0003824 step-time 4.52 perplexity 23.77 loss 3.1686
INFO:tensorflow:get_batch_step_time 0.016 actual_step_time 4.500
INFO:tensorflow:16:58:05 01/10/17 EST
INFO:tensorflow:global step 3251 learning rate 0.0003809 step-time 3.23 perplexity 23.10 loss 3.1396
INFO:tensorflow:get_batch_step_time 0.016 actual_step_time 3.216
INFO:tensorflow:17:18:34 01/10/17 EST
INFO:tensorflow:global step 3501 learning rate 0.0003795 step-time 4.92 perplexity 21.82 loss 3.0828
INFO:tensorflow:get_batch_step_time 0.015 actual_step_time 4.902
```

Any help would be greatly appreciated!
"
6778,Parameter to MergeFrom() must be instance of same class: expected TensorProto got Variable. for field Value.tensor,"I am trying to save variables through checkpoints to introduce fault tolerance to my program. The following is my configuration:-

```
ChiefSessionCreator = tf.train.ChiefSessionCreator(scaffold=None, master='grpc://localhost:2222', config=None, checkpoint_dir='/home/chaitanya/tensorflow/codes/checkpoints')
summary_hook = tf.train.SummarySaverHook(save_steps=None, save_secs=10, output_dir='/home/chaitanya/tensorflow/codes/savepoints', summary_writer=None, scaffold=None, summary_op=tf.Summary(tf.Summary.Value(tensor=y)))
saver = tf.train.Saver([tf.Variable(y)])
saver_hook = tf.train.CheckpointSaverHook(checkpoint_dir='/home/chaitanya/tensorflow/codes/checkpoints', save_secs=10, save_steps=None, saver=saver, checkpoint_basename='model.ckpt', scaffold=None)
```

In this, the [CheckpointSaveHook](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard2/tf.train.CheckpointSaverHook.md#tftraincheckpointsaverhook__init__checkpoint_dir-save_secsnone-save_stepsnone-savernone-checkpoint_basenamemodelckpt-scaffoldnone-listenersnone-checkpointsaverhookinit) saves tf.Variable in saver and [SummarySaverHook](https://www.tensorflow.org/versions/master/api_docs/python/train/training_utilities?authuser=2#SummarySaverHook) takes a tensor input in summary_op. The error I get is:-

```
Traceback (most recent call last):
  File ""add_1.py"", line 35, in <module>
    summary_hook = tf.train.SummarySaverHook(save_steps=None, save_secs=10, output_dir='/tensorflow/savepoints', summary_writer=None, scaffold=None, summary_op=tf.Summary(tf.Summary.Value(tensor=y)))
TypeError: Parameter to MergeFrom() must be instance of same class: expected TensorProto got Tensor. for field Value.tensor
```

How do I rectify this issue?"
6777,Compiled fails on Ubuntu 16.04 server,"Hi, there,

I compiled the latest v1.0.0-alpha version on a Ubuntu 16.04 server with GPU support. Here is the error I got:
```
/home/xxu/Extern/tensorflow/tensorflow/core/kernels/BUILD:863:1: error while parsing .d file: /home/xxu/.cache/bazel/_bazel_xxu/09a841ffbaae1d24115959abeaae27c9/execroot/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/gather_functor_gpu/tensorflow/core/kernels/gather_functor_gpu.cu.pic.d (No such file or directory).
In file included from ./tensorflow/core/framework/register_types.h:21:0,
                 from ./tensorflow/core/kernels/gather_functor_gpu.cu.h:23,
                 from tensorflow/core/kernels/gather_functor_gpu.cu.cc:20:
bazel-out/local_linux-opt/genfiles/tensorflow/core/framework/resource_handle.pb.h:12:2: error: #error This file was generated by a newer version of protoc which is
 #error This file was generated by a newer version of protoc which is
  ^
bazel-out/local_linux-opt/genfiles/tensorflow/core/framework/resource_handle.pb.h:13:2: error: #error incompatible with your Protocol Buffer headers. Please update
 #error incompatible with your Protocol Buffer headers.  Please update
  ^
bazel-out/local_linux-opt/genfiles/tensorflow/core/framework/resource_handle.pb.h:14:2: error: #error your headers.
 #error your headers.
  ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```

Versions of packages: 
1. The protoc --version is 3.1.0. I installed both C++ and python version;
2. CUDA 7.5, cudnn 5

PS: I compile it without cuda and everything goes well.
"
6776,Siamese Inception model,"Already tried on StackOverflow - no success.
Please, help.

How to implement such network in TensorFlow?
```
    (input_1)    (input_2)
          \         /
(Inception(GoogLeNet) without softmax layer)
          /         \
    (output_1)    (output_2)
             \   / 
        (Contrastive loss) 
```

I need to reuse weights in Inception for transfer learning.
"
6775,Issue when using binary tensorflow shared library,"I generated a shared library of tensorflow using the following command:

> bazel build -c opt --config=cuda //tensorflow/cc:libtensorflow.so

it was generated without any error and it's located in /usr/local/lib/libtensorflow.so.

BUT, when I add its dependency to my CMake file: 

> set(XXXX_DEPENDENCIES ${XXXX_DEPENDENCIES} ${Tensorflow_LIBRARIES} PARENT_SCOPE)

and I try to build my project I got the following:
```
:-1: warning: libcudart.so.7.5, needed by /usr/local/lib/libtensorflow.so, not found (try using -rpath or -rpath-link)
/usr/local/lib/libtensorflow.so:-1: error: undefined reference to `cudaMemcpyAsync'
/usr/local/lib/libtensorflow.so:-1: error: undefined reference to `cudaConfigureCall'
/usr/local/lib/libtensorflow.so:-1: error: undefined reference to `cudaGetDeviceCount'
/usr/local/lib/libtensorflow.so:-1: error: undefined reference to `__cudaUnregisterFatBinary'
/usr/local/lib/libtensorflow.so:-1: error: undefined reference to `cudaStreamQuery'
/usr/local/lib/libtensorflow.so:-1: error: undefined reference to `cudaStreamAddCallback'
/usr/local/lib/libtensorflow.so:-1: error: undefined reference to `cudaSetupArgument'
/usr/local/lib/libtensorflow.so:-1: error: undefined reference to `cudaGetDeviceProperties'
/usr/local/lib/libtensorflow.so:-1: error: undefined reference to `cudaGetErrorString'
/usr/local/lib/libtensorflow.so:-1: error: undefined reference to `__cudaRegisterFunction'
/usr/local/lib/libtensorflow.so:-1: error: undefined reference to `__cudaRegisterFatBinary'
/usr/local/lib/libtensorflow.so:-1: error: undefined reference to `cudaLaunch'
```"
6774,`import_meta_graph` appends `_1` to node in GraphDef but doesn't add `_1` to Variable name in Collection,"In an example below, second `import_meta_graph` will create variable nodes `[a, a_1]`, but corresponding global variables collection has variables `[a, a]`. So now `report_uninitialized_variables` is empty, even though there's an uninitialized variable `a_1` in the graph. Example below crashes with `uninitialized` error.

```
import tensorflow as tf

tf.reset_default_graph()
sess = tf.Session()
tf.Variable(tf.ones(()), name='a')
sess.run(tf.global_variables_initializer())
saver = tf.train.Saver()
saver.save(sess, 'dummy')
        
tf.reset_default_graph()
sess = tf.Session()
saver = tf.train.import_meta_graph('dummy.meta')
saver = tf.train.import_meta_graph('dummy.meta')
saver.restore(sess, './dummy')
sess.run(tf.initialize_all_variables())
sess.run(tf.report_uninitialized_variables())  # => prints empty
sess.run(""a_1:0"")   # => crashes with a_1 not initialized
```"
6773,svhn digit recongnition,is posible add svhn digit recongnition to Scikit flow integrated in the framework now? 
6769,"TF Learn TypeError: DataType float64 for attr 'T' not in list of allowed values: float32, float16","NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
tf.cast()

### Environment info
Operating System: 
masOSSierra
jupyter notebook  
tensforflow v0.12.1

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
No

If installed from binary pip package, provide:

1. A link to the pip package you installed: pip install tensorflow
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.tensforflow v0.12.1

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
I try to run the examples of MNIST from this repository with tensorflow learn, but to read in data with Pandas and use sklearn StrandardScale to scale the data beforehand.
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/mnist.py

### What other attempted solutions have you tried?
try to cast DataFrame into float32 with
X_train = X_train.astype(np.float32)
try to cast each column with
tf.cast(col, tf.float32)
but after use feature_columns = learn.infer_real_valued_columns_from_input(X_train)
feature_columns dtype just turn to tf.float64
(tried and didn't find attribute from source code that I can change dtype here)

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).


---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-102-1fdec574ca0f> in <module>()
----> 1 classifier.fit(X_train, y_train, batch_size=100, steps=20000) #, monitors=[validation_monitor])

/usr/local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.pyc in new_func(*args, **kwargs)
    189             _call_location(), decorator_utils.get_qualified_name(func),
    190             func.__module__, arg_name, date, instructions)
--> 191       return func(*args, **kwargs)
    192     new_func.__doc__ = _add_deprecated_arg_notice_to_docstring(
    193         func.__doc__, date, instructions)

/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in fit(self, x, y, input_fn, steps, batch_size, monitors, max_steps)
    353                              steps=steps,
    354                              monitors=monitors,
--> 355                              max_steps=max_steps)
    356     logging.info('Loss for final step: %s.', loss)
    357     return self

/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _train_model(self, input_fn, steps, feed_fn, init_op, init_feed_fn, init_fn, device_fn, monitors, log_every_steps, fail_on_nan_loss, max_steps)
    697       # cases, but will soon be deleted after the subclasses are updated.
    698       # TODO(b/32664904): Update subclasses and delete the else-statement.
--> 699       train_ops = self._get_train_ops(features, labels)
    700       if isinstance(train_ops, model_fn_lib.ModelFnOps):  # Default signature
    701         train_op = train_ops.train_op

/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _get_train_ops(self, features, labels)
   1050       `ModelFnOps` object.
   1051     """"""
-> 1052     return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)
   1053 
   1054   def _get_eval_ops(self, features, labels, metrics):

/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _call_model_fn(self, features, labels, mode)
   1019                                           params=self.params)
   1020       else:
-> 1021         model_fn_results = self._model_fn(features, labels, mode=mode)
   1022     else:
   1023       model_fn_results = self._model_fn(features, labels)

<ipython-input-96-18f3ebce1f4a> in conv_model(feature, target, mode)
     10                                     activation_fn=tf.nn.relu)
     11 
---> 12         h_pool1 = max_pool_2x2(h_conv1)
     13 
     14     with tf.variable_scope('conv_layer2'):

<ipython-input-95-7b3697815c2b> in max_pool_2x2(tensor_in)
      1 def max_pool_2x2(tensor_in):
----> 2     return tf.nn.max_pool(tensor_in, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')

/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.pyc in max_pool(value, ksize, strides, padding, data_format, name)
   1615                                 padding=padding,
   1616                                 data_format=data_format,
-> 1617                                 name=name)
   1618 
   1619 

/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.pyc in _max_pool(input, ksize, strides, padding, data_format, name)
   1596   result = _op_def_lib.apply_op(""MaxPool"", input=input, ksize=ksize,
   1597                                 strides=strides, padding=padding,
-> 1598                                 data_format=data_format, name=name)
   1599   return result
   1600 

/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc in apply_op(self, op_type_name, name, **keywords)
    580             for base_type in base_types:
    581               _SatisfiesTypeConstraint(base_type,
--> 582                                        _Attr(op_def, input_arg.type_attr))
    583             attrs[input_arg.type_attr] = attr_value
    584             inferred_from[input_arg.type_attr] = input_name

/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc in _SatisfiesTypeConstraint(dtype, attr_def)
     58           ""DataType %s for attr '%s' not in list of allowed values: %s"" %
     59           (dtypes.as_dtype(dtype).name, attr_def.name,
---> 60            "", "".join(dtypes.as_dtype(x).name for x in allowed_list)))
     61 
     62 

TypeError: DataType float64 for attr 'T' not in list of allowed values: float32, float16


Many thanks."
6768,"Add ""Hide empty panes"" option to TensorBoard","When filtering runs, empty panes show up if a hidden run has a summary in a scope that the visible runs don't have. This makes TensorBoard very messy when comparing different machine learning models with different variable scopes, and so on. Could an option to hide empty panes be added?

Example (note the empty LSTM scope):
![image](https://cloud.githubusercontent.com/assets/1595907/21809055/56b42e02-d746-11e6-9356-37ccd16cee4c.png)
"
6767,dynamic_rnn slower using master code then 0.12.1 release code,"For master code, git rev-parse HEAD 
ec7929b878926c39255254e9aea992f0bc65aa68

same code running for 0.12.1
 batch_size:[256] batches/s:[4.76] insts/s:[1217.74] 
 batch_size:[256] batches/s:[5.39] insts/s:[1379.17] 
 batch_size:[256] batches/s:[5.11] insts/s:[1306.94] 
 batch_size:[256] batches/s:[5.05] insts/s:[1292.61] 

for master code:
batch_size:[256] batches/s:[4.18] insts/s:[1069.37] 
 batch_size:[256] batches/s:[4.77] insts/s:[1220.00] 
 batch_size:[256] batches/s:[4.81] insts/s:[1231.52] 
batch_size:[256] batches/s:[4.55] insts/s:[1164.99] 


"
6766,"softmax_cross_entropy_with_logits aborts the process, if a tensor with zero first dimension is passed as an argument","### Environment info

Operating System: Ubuntu 16.04
Installed version of CUDA and cuDNN: CUDA-8.0, CUDNN 5.1.5 

Tensorflow version: 0.12.1 installed from
https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp35-cp35m-linux_x86_64.whl

Reproduced also using tf-0.11.0, CUDA-7.5, CUDNN-5.1.3

### Minimal reproducible example 

```
import tensorflow as tf
y = tf.placeholder(""int64"", [None], ""y"")
one_hot_y=tf.one_hot(y,10)
ce = tf.nn.softmax_cross_entropy_with_logits(one_hot_y, one_hot_y)
sess = tf.Session()
sess.run(ce, {y: []})
```
Result on GPU: 
```
E tensorflow/core/common_runtime/bfc_allocator.cc:244] tried to allocate 0 bytes
W tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes
F tensorflow/core/common_runtime/gpu/gpu_device.cc:104] EigenAllocator for GPU ran out of memory when allocating 0. See error logs for more detailed info.
Aborted (core dumped)
```
Result on CPU: 
```
array([], dtype=float32)
```"
6765,Tensorflow v0.12 tf.nn has no module rnn_cell,"`Import tensorflow as tf`

`a = tf.nn.rnn_cell.LSTMCell(100)`

results in:

> AttributeError: 'module' object has no attribute 'rnn_cell'

Tensorflow 0.12

ls -l 'cuda':
-rw-r--r-- 1 root root 189170 Jan  9 10:49 /usr/local/cuda/lib/libcudadevrt.a
lrwxrwxrwx 1 root root     16 Jan  9 10:49 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5
lrwxrwxrwx 1 root root     19 Jan  9 10:49 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18
-rwxr-xr-x 1 root root 311596 Jan  9 10:49 /usr/local/cuda/lib/libcudart.so.7.5.18
-rw-r--r-- 1 root root 558020 Jan  9 10:49 /usr/local/cuda/lib/libcudart_static.a

Cuda 7.5
Cudnn 5

git rev-parse HEAD:
ec7929b878926c39255254e9aea992f0bc65aa68

Bazel Version:
Build label: 0.4.3
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Dec 22 12:31:25 2016 (1482409885)
Build timestamp: 1482409885
Build timestamp as int: 1482409885


"
6763,"Android: build shared library using Makefile with r0.12, migrating from r0.11","I've been successfully using r0.11 Makefile with couple of additions (see below) to build shared library for Android. 
Now trying to migrate to r0.12 with same Makefile modifications but getting a SIGSEGVs at model initialization.

I followed [contrib/makefile/README.md](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/makefile/README.md). I've checked https://github.com/tensorflow/tensorflow/issues/6166 as well.

Comparing r0.12 changes with r0.11 I've noticed some things:
- both libtensorflow-core.a and lib.so decreased in size from ~200MB and ~100MB in r0.11 to ~40MB and ~40MB in r0.12
- trying to revert optimization from -O2 back to -O0 causing compilation error in downloads/gemmlowp/
```shell
/Users/oleg/opt/ndk-bundle-r12e/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64/bin/arm-linux-androideabi-g++ --std=c++11 -DIS_SLIM_BUILD -fno-exceptions -DNDEBUG -DNOTFDBG -O0 --sysroot /Users/oleg/opt/ndk-bundle-r12e/platforms/android-14/arch-arm -Wno-narrowing -march=armv7-a -mfloat-abi=softfp -mfpu=neon -fPIE -fPIC -MT /Users/oleg/tensorflow/tensorflow/contrib/makefile/gen/obj/tensorflow/core/kernels/meta_support.o -MMD -MP -MF /Users/oleg/tensorflow/tensorflow/contrib/makefile/gen/dep//tensorflow/core/kernels/meta_support.Td -I/Users/oleg/opt/ndk-bundle-r12e/sources/android/support/include -I/Users/oleg/opt/ndk-bundle-r12e/sources/cxx-stl/gnu-libstdc++/4.9/include -I/Users/oleg/opt/ndk-bundle-r12e/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi/include -I. -I/Users/oleg/tensorflow/tensorflow/contrib/makefile/downloads/ -I/Users/oleg/tensorflow/tensorflow/contrib/makefile/downloads/eigen -I/Users/oleg/tensorflow/tensorflow/contrib/makefile/downloads/gemmlowp -I/Users/oleg/tensorflow/tensorflow/contrib/makefile/gen/protobuf/include -I/Users/oleg/tensorflow/tensorflow/contrib/makefile/gen/proto/ -I/Users/oleg/tensorflow/tensorflow/contrib/makefile/gen/proto_text/ -c tensorflow/core/kernels/meta_support.cc -o /Users/oleg/tensorflow/tensorflow/contrib/makefile/gen/obj/tensorflow/core/kernels/meta_support.o
In file included from /Users/oleg/tensorflow/tensorflow/contrib/makefile/downloads/gemmlowp/meta/transform_kernels.h:239:0,
                 from ./tensorflow/core/kernels/meta_support.h:23,
                 from tensorflow/core/kernels/meta_support.cc:18:
/Users/oleg/tensorflow/tensorflow/contrib/makefile/downloads/gemmlowp/meta/transform_kernels_arm_32.h: In static member function 'static void gemmlowp::meta::Transform1DKernel<InType, OutType, gemmlowp::meta::BiasAdd<Type>, kernel_size, leftovers>::Transform(const InType*, const gemmlowp::meta::BiasAdd<Type>&, OutType*) [with InType = unsigned char; OutType = int; int kernel_size = 16; int leftovers = 0; Type = unsigned char]':
/Users/oleg/tensorflow/tensorflow/contrib/makefile/downloads/gemmlowp/meta/transform_kernels_arm_32.h:5605:24: error: 'asm' operand has impossible constraints
         ""cc"", ""memory"");
                        ^
make: *** [/Users/oleg/tensorflow/tensorflow/contrib/makefile/gen/obj/tensorflow/core/kernels/meta_support.o] Error 1
```
- gemmlowp was downloaded but never used for Android in headers search path (-I) in r0.11
- reverting new flags -mfloat-abi=softfp  and -mfpu=neon by commenting them out allows to build libtensorflow-core.a and libtensorflow.so artifacts with ~100MB in size using -O0

After applying  all changes from above I'm still getting SIGSEGVs. What else could I try? Looks like I'm missing something. I might be wrong thinking Makefile is a good way to go, but it flexible and allows to add required kernels/*_op.cc straight to tf_op_files.txt. What is a right way to build lib.so for Android?

### Makefile additions 
```Makefile
ifeq ($(TARGET),ANDROID)
...
CXXFLAGS +=\
...
-fPIC

SO_NAME  := libtensorflow.so
SO_PATH  := $(LIBDIR)$(SO_NAME)

$(SO_PATH): $(LIB_OBJS)
	@echo ""------->>>>>>>>>  SO PATH started ------>>>>>>>>>""
	@mkdir -p $(dir $@)
	$(CXX) $(CXXFLAGS) $(INCLUDES) \
	-shared -rdynamic -o $(SO_PATH)  $(LIB_OBJS) \
	$(LIBFLAGS) $(LDFLAGS) $(LIBS)
```

### Environment:
OS X: 10.12
NDK: r12e
CUDA: not installed
SHA-1
r0.12 `4d924e796368163eff11a8151e8505715345f58d`
r0.11 `e39376b6e9c9541e1bd8f15333b6994046a84d16`
................
Build label: 0.4.1-homebrew
Build target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Nov 30 13:26:35 2016 (1480512395)
Build timestamp: 1480512395
Build timestamp as int: 1480512395

"
6762,configure assumes ldconfig can be called and that CuDNN is installed in a system library location,"The configure script calls ""ldconfig"" to locate the CuDNN library but this makes 2 assumptions that can be wrong: 
1. It assumes that ""ldconfig"" can be called. However, on some GNU/Linux systems such as openSUSE Leap 42.1, only root has it in their path. (On Leap 42.1, normal users get the desired output from ""/sbin/ldconfig -p"".)
2. It assumes that the CuDNN library is installed in a system library location that is searched by ldconfig. However, users may install it in their home directory and use LD_LIBRARY_PATH to load it for various reasons. I, for example, was not fully satisfied from reading the licence terms that I am allowed to share the library with other users of the system and therefore did not install it system-wide as root.

Severity is low as normally the CuDNN library location should be auto-detected from the CuDNN library path before ""ldconfig"" is tried.

How to reproduce:
1. git clone this repo on a system with ldconfig not in your PATH
2. cd tensorflow and run ./configure
3. when asked for the CuDNN path, enter a valid and readable path (to avoid other errors), for example /usr/bin, that does not contain CuDNN
4. Error message: ./configure: line 337: ldconfig: command not found

Workaround options:
- First double check the CuDNN library path (the default path suggests that the last component ""lib64"" or ""lib"" should be omitted) and verify that standard folder names are used under this location for library and header files, for example ""lib64"" and ""include"".
- If you can call /sbin/ldconfig and if CuDNN is in a standard system library location the user can temporarily add /sbin to their PATH before running ""./configure""
- Edit the ""configure"" script to provide the location manually, for example CUDNN_PATH_FROM_LDCONFIG=""$HOME/local/cuda/lib64/libcudnn.so""

A fix should test whether ""ldconfig"" can be called before trying to call it. Also try /sbin/ldocnfig. Given that the library should normally be found already earlier in the code using the user-provided CuDNN path, a simple solution may be to set CUDNN_PATH_FROM_LDCONFIG to the empty string if ldconfig cannot be called.

If you like to automate the locating of the library as much as possible, you could furthermore check all locations in LD_LIBRARY_PATH."
6761,variable_scope behave differently for master code and release 0.12.1,"For master code, git rev-parse HEAD
ec7929b878926c39255254e9aea992f0bc65aa68

The problem is one of my code used to work for release 0.12.1 fail for master code, which use adagrad as optimizer.
ValueError: Variable OptimizeLoss/w_h/Adagrad/ does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?

I traced this and find below two simple codes can reproduce, the second one works for both 0.12.1 release and master, but the first one only works for 0.12.1 and fail master code. 
I wonder what's the diff here ?  code 2 style is suggested ? Why code 1 will fail ? Also for code 1 if I use gradient desc instead of adgrad will not fail. Similar error can occur when using lstm cell.

code 1:


      loss, accuracy = model.build_graph(X, y)  

      tf.get_variable_scope().reuse_variables()    

      eval_loss, eval_accuracy = model.build_graph(eval_X, eval_y)    

code 2:


    with tf.variable_scope(""mlp"") as scope:    

        loss, accuracy = model.build_graph(X, y)    

        scope.reuse_variables()    

        eval_loss, eval_accuracy = model.build_graph(eval_X, eval_y)    


"
6760,distributed example fail on GPU,"When I run `https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dist_test/python/mnist_replica.py`, the distributed example
I can run it with CPU(1 ps and 2 workers all on CPU in same machine )
but when I run it with GPU, I set the GPU
number to 2(1 ps on CPU and 2 workers on GPU in same machine), the first worker runs normally, but when I start the second one, it shows the following errors

```
Traceback (most recent call last):
  File ""/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 972, in _do_call
    return fn(*args)
  File ""/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 954, in _run_fn
    status, run_metadata)
  File ""/home/mlabs/anaconda3/lib/python3.5/contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/errors.py"", line 463, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors.InvalidArgumentError: WhereOp: Race condition between counting the number of true elements and writing them.  When counting, saw 1363 elements; but when writing their indices, saw 12 elements.
	 [[Node: report_uninitialized_variables/boolean_mask/Where = Where[_device=""/job:worker/replica:0/task:1/cpu:0""](report_uninitialized_variables/boolean_mask/Reshape_1)]]
	 [[Node: report_uninitialized_variables/boolean_mask/Where_G11 = _Recv[client_terminated=false, recv_device=""/job:worker/replica:0/task:1/gpu:1"", send_device=""/job:worker/replica:0/task:1/cpu:0"", send_device_incarnation=-7465067838139069765, tensor_name=""edge_29_report_uninitialized_variables/boolean_mask/Where"", tensor_type=DT_INT64, _device=""/job:worker/replica:0/task:1/gpu:1""]()]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""mnist_replica.py"", line 266, in <module>
    tf.app.run()
  File ""/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 30, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""mnist_replica.py"", line 223, in main
    config=sess_config)
  File ""/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/supervisor.py"", line 722, in prepare_or_wait_for_session
    max_wait_secs=max_wait_secs)
  File ""/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/session_manager.py"", line 349, in wait_for_session
    is_ready, not_ready_msg = self._model_ready(sess)
  File ""/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/session_manager.py"", line 435, in _model_ready
    return self._ready(self._ready_op, sess, ""Model not ready"")
  File ""/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/session_manager.py"", line 404, in _ready
    ready_value = sess.run(op)
  File ""/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 717, in run
    run_metadata_ptr)
  File ""/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 915, in _run
    feed_dict_string, options, run_metadata)
  File ""/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 965, in _do_run
    target_list, options, run_metadata)
  File ""/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 985, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors.InvalidArgumentError: WhereOp: Race condition between counting the number of true elements and writing them.  When counting, saw 1363 elements; but when writing their indices, saw 12 elements.
	 [[Node: report_uninitialized_variables/boolean_mask/Where = Where[_device=""/job:worker/replica:0/task:1/cpu:0""](report_uninitialized_variables/boolean_mask/Reshape_1)]]
	 [[Node: report_uninitialized_variables/boolean_mask/Where_G11 = _Recv[client_terminated=false, recv_device=""/job:worker/replica:0/task:1/gpu:1"", send_device=""/job:worker/replica:0/task:1/cpu:0"", send_device_incarnation=-7465067838139069765, tensor_name=""edge_29_report_uninitialized_variables/boolean_mask/Where"", tensor_type=DT_INT64, _device=""/job:worker/replica:0/task:1/gpu:1""]()]]

Caused by op 'report_uninitialized_variables/boolean_mask/Where', defined at:
  File ""mnist_replica.py"", line 266, in <module>
    tf.app.run()
  File ""/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 30, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""mnist_replica.py"", line 201, in main
    global_step=global_step)
  File ""/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/supervisor.py"", line 310, in __init__
    ready_op=ready_op, ready_for_local_init_op=ready_for_local_init_op)
  File ""/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/supervisor.py"", line 399, in _init_ready_op
    ready_op = variables.report_uninitialized_variables()
  File ""/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/variables.py"", line 1167, in report_uninitialized_variables
    return array_ops.boolean_mask(variable_names_tensor, variables_mask)
  File ""/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py"", line 950, in boolean_mask
    return _apply_mask_1d(tensor, mask)
  File ""/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py"", line 925, in _apply_mask_1d
    indices = squeeze(where(mask), squeeze_dims=[1])
  File ""/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 3072, in where
    result = _op_def_lib.apply_op(""Where"", input=input, name=name)
  File ""/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 749, in apply_op
    op_def=op_def)
  File ""/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 2380, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/mlabs/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1298, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): WhereOp: Race condition between counting the number of true elements and writing them.  When counting, saw 1363 elements; but when writing their indices, saw 12 elements.
	 [[Node: report_uninitialized_variables/boolean_mask/Where = Where[_device=""/job:worker/replica:0/task:1/cpu:0""](report_uninitialized_variables/boolean_mask/Reshape_1)]]
	 [[Node: report_uninitialized_variables/boolean_mask/Where_G11 = _Recv[client_terminated=false, recv_device=""/job:worker/replica:0/task:1/gpu:1"", send_device=""/job:worker/replica:0/task:1/cpu:0"", send_device_incarnation=-7465067838139069765, tensor_name=""edge_29_report_uninitialized_variables/boolean_mask/Where"", tensor_type=DT_INT64, _device=""/job:worker/replica:0/task:1/gpu:1""]()]]

```"
6755,OSX Fail to run tensorflow/contrib/makefile/download_dependencies.sh,"Running on OSX

Run 

sh tensorflow/contrib/makefile/download_dependencies.sh in the home directory

And I get 

downloading https://bitbucket.org/eigen/eigen/get/60578b474802.tar.gz
downloading https://github.com/google/gemmlowp/archive/a6f29d8ac48d63293f845f2253eccbf86bc28321.tar.gz
downloading https://github.com/google/googletest/archive/release-1.8.0.tar.gz
downloading https://github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz
downloading https://github.com/google/re2/archive/b94b7cd42e9f02673cd748c1ac1d16db4052514c.tar.gz
sed: tensorflow/contrib/makefile/downloads/eigen/Eigen/src/Core/arch/NEON/Complex.h: No such file or directory

How can I solve this problem? Thanks a lot!!
"
6752,Matrix Vector multiply not parallelized.,"### What is the problem?

Matrix vector multiply is not parallelized. Please see my example code. No matter how I change the intra_op_parallelism_threads, the running time is always similar. I used ""top"" to confirm that only one thread was used. However, the parallel speedups for square matrix matrix multiply are quite noticeable. Again, ""top"" confirmed that multiple threads were used.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System: Ubuntu 16.04 

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2e22f1b20fdfa77b1332c518617391dc32359c5b
2. The output of `bazel version`
0.4.3

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

import tensorflow as tf
import numpy as np
import time

n = 10000

#approach 1:

matrix1 = tf.constant(np.ones(n*n), shape = [n,n])
matrix2 = tf.constant(np.ones(n*1), shape = [n,1])

product1 = tf.matmul(matrix1, matrix2)

start = time.time()
sess = tf.Session(config=tf.ConfigProto(
    inter_op_parallelism_threads=1,
    intra_op_parallelism_threads=12))
sess.run(product1)
end = time.time()
print('\n Approach 1 took: %s%%' % (end - start))

### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
6750,Error building TensorFlow within Docker Container,"I am following the instructions for using TensorFlow serving from https://tensorflow.github.io/serving/serving_inception.html
and after I clone tf within the container and build it with `./configure`

I get the following errors

```
ERROR: /serving/tensorflow/tensorflow/workspace.bzl:375:3: no such package '@junit_jar//jar': Error downloading [https://github.com/junit-team/junit4/releases/download/r4.12/junit-4.12.jar] to /root/.cache/bazel/_bazel_root/5071e8dca1385fb776f72b33971bf157/external/junit_jar/junit-4.12.jar: java.lang.RuntimeException: Unexpected error: java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty and referenced by '//external:junit'.

ERROR: /serving/tensorflow/tensorflow/workspace.bzl:375:3: no such package '@junit_jar//jar': Error downloading [https://github.com/junit-team/junit4/releases/download/r4.12/junit-4.12.jar] to /root/.cache/bazel/_bazel_root/5071e8dca1385fb776f72b33971bf157/external/junit_jar/junit-4.12.jar: java.lang.RuntimeException: Unexpected error: java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty and referenced by '//external:junit'.

ERROR: Evaluation of query ""deps(... union @bazel_tools//tools/jdk:toolchain))"" failed: errors were encountered while computing transitive closure.
```

and the `...` in the last error contains about 200 lines of additional text"
6749,sparse_placeholder no longer accepts python ints in shape argument,"Hi,

After recently updating tensorflow, sparse_placeholder stopped working correctly.  It appears that the shape argument must now be int64 in order for tensorflow to convert the shape to a tensor, so the following fails:
```
ph = tf.sparse_placeholder(dtype=tf.float32, shape=(50, 10000))
```
with error message:
```
ValueError: Tensor conversion requested dtype int64 for Tensor with dtype int32: 'Tensor(""Const:0"", shape=(2,), dtype=int32)'
```
This is inconsistent with the behavior of tf.placeholder, for which: 
```
ph = tf.placeholder(dtype=tf.float32, shape=(50, 10000))
```
succeeds. 

Thanks,
Shawn
"
6748,Got an unexpected keyword argument error when I call tf.contrib.layers.convolution2d on Tensorflow for Windows,"When I invoke tf.contrib.layers.convolution2d the tensorflow execution terminates with an error about one of the parameters used

the call is

```
layer_one = tf.contrib.layers.convolution2d(
    float_image_batch,
    num_output_channels=32,     
    kernel_size=(5,5),          
    activation_fn=tf.nn.relu,
    weight_init=tf.random_normal,
    stride=(2, 2),
    trainable=True)
```

I have tried to fix according new documentation using tf.random_normal_init but get always the same problem


OS: Windows 10 x64
Tensorflow 0.12.0 

Related Stackoverflow question: http://stackoverflow.com/questions/41539658/tensorflow-error-when-i-try-to-use-tf-contrib-layers-convolution2d"
6745,Tensor flow just stopped working all of a sudden,"My new to tensorflow and I was following the tutorial on tensorflow for poets to train an image classifier on a Docker image of gcr.io/tensorflow/tensorflow:latest-devel and I got it up and running but then yesterday I told it to classify an image that it has classified before and it threw an error now when im trying to retain it I keep getting this error


Looking for images in 'tensorflow'
No files found
Looking for images in 'tools'
No files found
Looking for images in 'git'
Traceback (most recent call last):
  File ""tensorflow/examples/image_retraining/retrain.py"", line 1012, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 43, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""tensorflow/examples/image_retraining/retrain.py"", line 757, in main
    FLAGS.validation_percentage)
  File ""tensorflow/examples/image_retraining/retrain.py"", line 148, in create_image_lists
    file_list.extend(gfile.Glob(file_glob))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/lib/io/file_io.py"", line 269, in get_matching_files
    compat.as_bytes(filename), status)]
  File ""/usr/lib/python2.7/contextlib.py"", line 24, in __exit__
    self.gen.next()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py"", line 469, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.NotFoundError: git"
6744,Bus error (core dumped) when importing TensorFlow ,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
- [Issue 2626](https://github.com/tensorflow/tensorflow/issues/2626)
- [Issue 3366](https://github.com/tensorflow/tensorflow/issues/3366)

### Environment info
Operating System:
Ubuntu 16.04

Installed version of CUDA and cuDNN: 8.0, 5.1.5
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
```-rw-r--r-- 1 root root   558720 Jan  8 20:20 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 Jan  8 20:20 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19 Jan  8 20:20 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44
-rwxr-xr-x 1 root root   415432 Jan  8 20:20 /usr/local/cuda/lib64/libcudart.so.8.0.44
-rw-r--r-- 1 root root   775162 Jan  8 20:20 /usr/local/cuda/lib64/libcudart_static.a
-rwxr-xr-x 1 root root 61062656 Jan  8 20:35 /usr/local/cuda/lib64/libcudnn.so
-rwxr-xr-x 1 root root 61062656 Jan  8 20:35 /usr/local/cuda/lib64/libcudnn.so.5
-rwxr-xr-x 1 root root 61062656 Jan  8 20:35 /usr/local/cuda/lib64/libcudnn.so.5.1.5
```

If installed from binary pip package, provide: *did not install from pip package*

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`): `ec7929b878926c39255254e9aea992f0bc65aa68`
2. The output of `bazel version`: 
```
Build label: 0.4.3
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Dec 22 12:31:25 2016 (1482409885)
Build timestamp: 1482409885
Build timestamp as int: 1482409885
```

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
```
>>> import tensorflow
I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcublas.so.8.0 locally
Bus error (core dumped)
```

### What other attempted solutions have you tried?
I have not attempted solutions because I know of none -- it seems like there is a bus error when loading the driver, and I'm not sure how to proceed

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
Not sure if the output of the core dump is going to be useful or where to find it. Help would be appreciated. 
"
6743,Is there a function which performs the opposite of tf.extract_image_patches ? ,"Hello, 

I was looking for a function in the latest tf version, which performs the opposite of tf.extract_image_patches. If I did not miss it, it would be great to have this feature in tensorflow. 
It would help me for the task I am doing.  

Thanks in advance,
Vignesh"
6742,`tensorflow.examples.tutorials.mnist` not currently downloadable,"### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

```python
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(""/tmp/data/"", one_hot=True)
```

raises `http.client.RemoteDisconnected: Remote end closed connection without response`, if you don't already have the data cached. I believe this is because the underlying data [SOURCE_URL](http://yann.lecun.com/exdb/mnist/) is currently [down](http://downforeveryoneorjustme.com/http://yann.lecun.com/exdb/mnist/)."
6741,"tf.divide handles ""name"" argument differently","If installed from binary pip package, provide:

1. A link to the pip package you installed:
https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-0.12.1-cp27-none-linux_x86_64.whl
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
0.12.head

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
```python
a = tf.get_variable('a', shape=[10])
b = tf.get_variable('b', shape=[10])
print tf.divide(a, b, name='x').name   # x/truediv:0
```

Normally one would expect (like almost every other existing function in tf) that with the name='x' option, the output op will be named 'x:0'. 
But instead it was named ""x/truediv:0""."
6740,loading text file and accessing data using tensorflow,"ubuntu 14.0.04
python 3.5

I have a text file of size (20480,8) with all float values. I want the data in 4th column in to one array. I am able to do it using python as

`file_pathname1 = os.path.join(os.path.expanduser('~'),'TF','1st_test', '20.10.22.12.09.13')`
`x= np.loadtext(file_pathname1)`
`y= x[:,4]`
`#print(np.shape(x))`
`print(np.shape(y))`

I get the size of the y as (20480,)

but I am trying to copy the same as a tensor. How to access the data

`file_pathname1 = os.path.join(os.path.expanduser('~'),'TF','1st_test', '20.10.22.12.09.13')`
`y = tf.read_file(file_pathname1)`
`sess = tf.Session()`
`sess.run(y)`
`print(y.get_shape())`

I cant understand why the loaded file is empty array  because the output is ()
once loaded how to copy the column in to array ? "
6739,"lstm_cell/weights does not exist, fail only using master code, ok for release version 0.12.1","The code like below:
` 
for i in range(max_steps):  

  with tf.variable_scope(""RNN"", reuse=True if i > 0 else None):    

     (output, state) = self.cell(last_symbol, state)
`

  File ""/home/gezi/mine/tensorflow-exp/deepiu/seq2seq/rnn_decoder.py"", line 189, in generate_sequence
    (output, state) = self.cell(last_symbol, state)
  File ""/usr/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/lstm_ops.py"", line 381, in __call__
    self._num_units * 4])
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 987, in get_variable
    custom_getter=custom_getter)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 889, in get_variable
    custom_getter=custom_getter)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 347, in get_variable
    validate_shape=validate_shape)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 332, in _true_getter
    caching_device=caching_device, validate_shape=validate_shape)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 656, in _get_single_variable
    ""VarScope?"" % name)
ValueError: Variable seq2seq/decode/RNN/lstm_cell/weights does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?

"
6738,How to train Multibox object detector included in the TF Detect Android demo,"There is a way to train a custom model for the multibox object detector that is included in the [TF Detect Android demo](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/DetectorActivity.java)? 

"
6737,tf.nn.sampled_softmax_loss fail for master code,"Build the leates tensorflow gpu version and  face this error(the code work ok for release 0.12.1) complaining float32 and int64 multiply.
tf.nn.sampled_softmax_loss(self.w_t, 
                                          self.v, 
                                          inputs, 
                                          labels, 
                                          num_sampled, 
                                          vocab_size,
                                          sampled_values=sampled_values)
                                
  File ""/home/gezi/mine/tensorflow-exp/util/melt/ops/seq2seq.py"", line 63, in sequence_loss_by_example
    crossents = softmax_loss_function(logits, targets)
  File ""/home/gezi/mine/tensorflow-exp/deepiu/seq2seq/rnn_decoder.py"", line 300, in sampled_loss
    sampled_values=sampled_values)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/nn_impl.py"", line 1180, in sampled_softmax_loss
    name=name)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/nn_impl.py"", line 963, in _compute_sampled_logits
    array_ops.reshape(true_w, new_true_w_shape))
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py"", line 329, in multiply
    return gen_math_ops._mul(x, y, name)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 1625, in _mul
    result = _op_def_lib.apply_op(""Mul"", x=x, y=y, name=name)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 522, in apply_op
    inferred_from[input_arg.type_attr]))
TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int64 of argument 'x'.
"
6736,batch_normalization layer argument name and documentation wrong in core and contrib layers,"The batch normalization layers both in core and contrib take a `center` argument that controls whether a variable beta should be created that is then **added after the normalization**. The name of the argument (`center`) and even more so the documentation (_center: If True, subtract `beta`. If False, `beta` is ignored._) indicate however that this argument controls whether the mean of the input is subtracted as part of the normalization."
6735,slim parallel_read should pass seed to string_input_producer,"Currently, the seed passed to `parallel_read` is only passed to the RandomShuffleQueue, but not to the string_input_producer. This should be fixed. Furthermore, it should also be documented that the output will never be deterministic if `num_readers` is greater than 1.

https://github.com/tensorflow/tensorflow/blob/e121667dc609de978a223c56ee906368d2c4ceef/tensorflow/contrib/slim/python/slim/data/parallel_reader.py#L212"
6734,Can not register user_ops in Android APP,"Hello!
I want to user some user ops, such as `roi_pooling.cc` in Android. I build my Android project by Bazel, just like the Tensorflow Android demo, but I seems Android does build user_ops, the logcat shows as following:
`tensorflow_inference_jni.cc:146 Could not create TensorFlow graph: Not found: Op type not registered 'RoiPooling'`
Is it a bug or how can I solve this problem?
Thank you in advance for your help."
6733,Export meta graph option in image retraining,"I was trying to tensorflow serve my retrained graph ( *.pb file ) but if I understand correctly it doesn't contain meta graph in it, so wouldn't it be nice to have export meta graph options in retrain.py ?"
6732,array operations in tensorflow,"ubunutu14.0.4
python3.5 

Hello, 

Im new using tensorflow library. Im trying to access data in text file and I can run it as python code but access the array elements using this library is not known. Please help

I have a text file that contains float and integer values
`import tensorflow as tf
import numpy as np
import os
file_pathname1 = os.path.join(os.path.expanduser('~'),'Desktop','TF','Practice Files','test')
x = np.loadtxt(file_pathname1)
#y1 = x[1, 4] # load data to another variable
print(x)

y = tf.readfile(file_pathname1)

sess = tf.Session()
print(sess.run(y))
`
how can i copy the same as array form using tensor flow

because the output of both is different
`[[ 1.3  2.5  3.5  4.4]
 [ 5.2  6.5  7.6  8.8]]`

`b'1.3 2.5 3.5 4.4\n5.2 6.5 7.6 8.8'`"
6731,Add contrib/tfprof to CMake build,"Tensorflow for windows has package missing issue
Path: tensorflow/contrib/tfprof
All the files under the path are missing,resnet model replies on these files.
Please pay attention,thank you."
6730,tf.map_fn throws error...sometimes.... ,"### Environment info
Operating System: Ubuntu 16.04

Installed version of CUDA and cuDNN: 
	libcudart.so.8.0 (libc6,x86-64) => /usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0
	libcudart.so.7.5 (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcudart.so.7.5
	libcudart.so (libc6,x86-64) => /usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so
	libcudart.so (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcudart.so
	libcuda.so.1 (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcuda.so.1
	libcuda.so.1 (libc6) => /usr/lib/i386-linux-gnu/libcuda.so.1
	libcuda.so (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcuda.so
	libcuda.so (libc6) => /usr/lib/i386-linux-gnu/libcuda.so

pip install tensorflow-gpu
 0.12.0-rc1

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
I posted to the below stackoverflow link with some code and my initial problem that I mostly fixed, but it does seem that there is a problem with the tf.map_fn()
http://stackoverflow.com/questions/41534866/image-distortion-returns-error-the-tensor-returned-for-reshape-50-was-not-vali/41536952?noredirect=1#comment70281037_41536952 

I was trying to use tf.map_fn to distort a batch of images and it throws an error sometimes... the batch starts its life as a TFRecords file of flat images. The batch is turned into 4d tensor by tf.reshape then put through tf.map_fn(image distortion function) and then reshaped back into a flat 2d tensor. Most of the time I get a reshape error but not all the time. The times I don't get the error, the code will run for all epochs...

`/home/mcamp/anaconda3/bin/python ""/media/mcamp/Local SSHD/Python Projects/GarageDoor2/train_model.py""
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: GeForce GTX 960
major: 5 minor: 2 memoryClockRate (GHz) 1.342
pciBusID 0000:01:00.0
Total memory: 3.94GiB
Free memory: 2.53GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 960, pci bus id: 0000:01:00.0)
*
Traceback (most recent call last):
  File ""/home/mcamp/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1021, in _do_call
    return fn(*args)
  File ""/home/mcamp/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1003, in _run_fn
    status, run_metadata)
  File ""/home/mcamp/anaconda3/lib/python3.5/contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""/home/mcamp/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 469, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: The tensor returned for Reshape_3:0 was not valid.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/media/mcamp/Local SSHD/Python Projects/GarageDoor2/train_model.py"", line 44, in <module>
    model.train(training_data, epochs=FLAGS.n_epochs)
  File ""/media/mcamp/Local SSHD/Python Projects/GarageDoor2/ConvNetClass.py"", line 190, in train
    training_data_dict['y_train_batch']]) #4
  File ""/home/mcamp/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 766, in run
    run_metadata_ptr)
  File ""/home/mcamp/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 964, in _run
    feed_dict_string, options, run_metadata)
  File ""/home/mcamp/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1014, in _do_run
    target_list, options, run_metadata)
  File ""/home/mcamp/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1034, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: The tensor returned for Reshape_3:0 was not valid.`
"
6729,Library not loaded: @rpath/libcudart.8.0.dylib when running TF GPU on MacOS,"Something related to linking seems to cause the build to fail on Mac OS 10.12.2 when building with CUDA.

Verbose build error is:

```
$ bazel build -c opt --config=cuda --verbose_failures //tensorflow/tools/pip_package:build_pip_package
INFO: Found 1 target...
ERROR: /Users/anton/tmp/tensorflow/tensorflow/contrib/ffmpeg/BUILD:66:1: Executing genrule //tensorflow/contrib/ffmpeg:decode_audio_op_py_pygenrule failed: bash failed: error executing command 
  (cd /private/var/tmp/_bazel_anton/d3361bfa15c75c42ab541f3d83e8eba4/execroot/tensorflow && \
  exec env - \
    PATH=/usr/local/cuda/bin:/Users/anton/.pyenv/shims:/Users/anton/.rbenv/shims:/Users/anton/.scalaenv/shims:/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/TeX/texbin:/Users/anton/unix/bin:/usr/local/cuda/bin \
    TMPDIR=/var/folders/4r/mq7ht1z11t72w5b5b6014zwm0000gn/T/ \
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/tensorflow/contrib/ffmpeg/gen_decode_audio_op_py_py_wrappers_cc 1 > bazel-out/local_darwin-py3-opt/genfiles/tensorflow/contrib/ffmpeg/ops/gen_decode_audio_op_py.py'): com.google.devtools.build.lib.shell.AbnormalTerminationException: Process terminated by signal 6.
dyld: Library not loaded: @rpath/libcudart.8.0.dylib
  Referenced from: /private/var/tmp/_bazel_anton/d3361bfa15c75c42ab541f3d83e8eba4/execroot/tensorflow/bazel-out/host/bin/tensorflow/contrib/ffmpeg/gen_decode_audio_op_py_py_wrappers_cc
  Reason: image not found
/bin/bash: line 1: 72493 Abort trap: 6           bazel-out/host/bin/tensorflow/contrib/ffmpeg/gen_decode_audio_op_py_py_wrappers_cc 1 > bazel-out/local_darwin-py3-opt/genfiles/tensorflow/contrib/ffmpeg/ops/gen_decode_audio_op_py.py
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 2.707s, Critical Path: 0.32s
```

### Related posts
It seems that Caffe had similar issues due to Apple dropping the `LD_LIBRARY_PATH` environment variable: https://github.com/BVLC/caffe/issues/3227

### Environment info
Operating System: Mac OS 10.12.2

Installed version of CUDA and cuDNN: 
```
$ ls -l /usr/local/cuda/lib/libcud*
-rwxr-xr-x  1 root  wheel  13504 Nov  3 19:39 /usr/local/cuda/lib/libcuda.dylib*
lrwxr-xr-x  1 root  wheel     45 Nov  3 19:40 /usr/local/cuda/lib/libcudadevrt.a@ -> /Developer/NVIDIA/CUDA-8.0/lib/libcudadevrt.a
lrwxr-xr-x  1 root  wheel     50 Nov  3 19:40 /usr/local/cuda/lib/libcudart.8.0.dylib@ -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.8.0.dylib
lrwxr-xr-x  1 root  wheel     46 Nov  3 19:40 /usr/local/cuda/lib/libcudart.dylib@ -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.dylib
lrwxr-xr-x  1 root  wheel     49 Nov  3 19:40 /usr/local/cuda/lib/libcudart_static.a@ -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart_static.a
lrwxr-xr-x  1 root  admin     47 Jan  8 16:48 /usr/local/cuda/lib/libcudnn.5.dylib@ -> /Developer/NVIDIA/CUDA-8.0/lib/libcudnn.5.dylib
lrwxr-xr-x  1 root  admin     45 Jan  8 16:48 /usr/local/cuda/lib/libcudnn.dylib@ -> /Developer/NVIDIA/CUDA-8.0/lib/libcudnn.dylib
lrwxr-xr-x  1 root  admin     48 Jan  8 16:48 /usr/local/cuda/lib/libcudnn_static.a@ -> /Developer/NVIDIA/CUDA-8.0/lib/libcudnn_static.a
```

##### Configuration command
```
Please specify the location of python. [Default is /Users/anton/.pyenv/shims/python]: 
Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] N
No Google Cloud Platform support will be enabled for TensorFlow
Do you wish to build TensorFlow with Hadoop File System support? [y/N] N
No Hadoop File System support will be enabled for TensorFlow
Error in sitecustomize; set PYTHONVERBOSE for traceback:
KeyError: 'PYTHONPATH'
Error in sitecustomize; set PYTHONVERBOSE for traceback:
KeyError: 'PYTHONPATH'
Error in sitecustomize; set PYTHONVERBOSE for traceback:
KeyError: 'PYTHONPATH'
Found possible Python library paths:
  /Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages
Please input the desired Python library path to use.  Default is [/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages]

Using python library path: /Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages
Do you wish to build TensorFlow with OpenCL support? [y/N] 
No OpenCL support will be enabled for TensorFlow
Do you wish to build TensorFlow with CUDA support? [y/N] Y
CUDA support will be enabled for TensorFlow
Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 
Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 
Please specify the location where CUDA  toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 
Please specify the Cudnn version you want to use. [Leave empty to use system default]: 
Please specify the location where cuDNN  library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 
libcudnn.dylib resolves to libcudnn.dylib
Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size.
[Default is: ""3.5,5.2""]: 3.5  	 
Extracting Bazel installation...
............
INFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.
.........
____Loading package: tensorflow/tools/git
____Loading package: tensorflow/contrib/opt
____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 40,960 bytes
____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 444,518 bytes
____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 743,716 bytes
____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 1,170,534 bytes
____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 1,597,352 bytes
____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 1,998,646 bytes
____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 2,251,050 bytes
____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 2,675,032 bytes
____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 3,144,390 bytes
____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 3,579,716 bytes
____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 3,788,162 bytes
INFO: All external dependencies fetched successfully.
Configuration finished
```

#### Git revision
```
$ git rev-parse HEAD
ec7929b878926c39255254e9aea992f0bc65aa68
```

#### Bazel version
```
$ bazel version
Build label: 0.4.3-homebrew
Build target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Dec 22 15:20:15 2016 (1482420015)
Build timestamp: 1482420015
Build timestamp as int: 1482420015
```

### Update
I modified `third_party/gpus/crosstool/CROSSTOOL.tpl`  the following lines 
```
  cxx_flag: ""-std=c++11""
  linker_flag: ""-Wl,-no-as-needed""
``` 
in the toolchain section to: 
```
  cxx_flag: ""-std=c++11""
  linker_flag: ""-Wl,-no-as-needed,-rpath,/usr/local/cuda/lib""
```

Now I get failures at:
```
ERROR: /Users/anton/tmp/tensorflow/tensorflow/python/BUILD:793:1: Executing genrule //tensorflow/python:control_flow_ops_pygenrule failed: bash failed: error executing command 
  (cd /private/var/tmp/_bazel_anton/d3361bfa15c75c42ab541f3d83e8eba4/execroot/tensorflow && \
  exec env - \
    PATH=/usr/local/cuda/bin:/Users/anton/.pyenv/shims:/Users/anton/.rbenv/shims:/Users/anton/.scalaenv/shims:/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/TeX/texbin:/Users/anton/unix/bin:/usr/local/cuda/bin \
    TMPDIR=/var/folders/4r/mq7ht1z11t72w5b5b6014zwm0000gn/T/ \
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/tensorflow/python/gen_control_flow_ops_py_wrappers_cc @tensorflow/python/ops/hidden_ops.txt 1 > bazel-out/local_darwin-py3-opt/genfiles/tensorflow/python/ops/gen_control_flow_ops.py'): com.google.devtools.build.lib.shell.AbnormalTerminationException: Process terminated by signal 6.
dyld: Library not loaded: @rpath/libcudart.8.0.dylib
  Referenced from: /private/var/tmp/_bazel_anton/d3361bfa15c75c42ab541f3d83e8eba4/execroot/tensorflow/bazel-out/host/bin/tensorflow/python/gen_control_flow_ops_py_wrappers_cc
  Reason: image not found
/bin/bash: line 1: 45635 Abort trap: 6           bazel-out/host/bin/tensorflow/python/gen_control_flow_ops_py_wrappers_cc @tensorflow/python/ops/hidden_ops.txt 1 > bazel-out/local_darwin-py3-opt/genfiles/tensorflow/python/ops/gen_control_flow_ops.py
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 1683.463s, Critical Path: 1587.37s
```

I got a bit further but it seems that I need to modify the link options in more files."
6728,Build with CUDA 8.0 fails on Mac OS X 10.12.2,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

"
6727, tf.contrib.learn.monitors.ValidationMonitor hangs when passed input_fn parameter,"Hello!
I have been working my way through the  tf.contrib.learn [tutorials](https://www.tensorflow.org/tutorials/) and have been attempting to integrate the tf.contrib.learn.monitors.ValidationMonitor into the 'deep' classifier in wide_n_deep.py as shown below.


### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?  I searched both Github and Stakeoverflow with the terms 'tensorflow,' 'input_fn,' and 'validationmonitor' but wasn't able to find anyone else who reported similar issues.

### Environment info
Operating System:  I ran this on Ubuntu Server 16.04 on a physical I7 with a GTX1080 gpu when i noticed the problem.  I know that i was using the GPU on the original physical box from previous tests, and because during the hang the nvidia_smi command showed considerable load on the GPU.  I was able to replicate the problem with CPU on a 16.04 VM as well.

Installed version of CUDA and cuDNN: 
```
/home/andersonjas/libcudnn5-dev_5.1.5-1+cuda8.0_amd64.deb
/home/andersonjas/libcudnn5_5.1.5-1+cuda8.0_amd64.deb
```

If installed from binary pip package, provide:

1. A link to the pip package you installed:
from Anaconda 2.7 64 bit package:
```
pip install tensorflow
```
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

```
andersonjas@ubuntu:~$ python -c ""import tensorflow; print(tensorflow.__version__)""
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
0.11.head

```
If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

```python
validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(input_fn=lambda:input_fn(df_test), 
                       every_n_steps=50)
m.fit(input_fn=lambda: input_fn(df_train), steps=151,monitors=[validation_monitor])
```

Doing this in a jupyter notebool causes the code to hang indefinitely.  To make completely sure that i don't have a bug in my own code i can make the following change:

```python
validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(input_fn=lambda:input_fn(df_test), 
                       every_n_steps=50)
m.fit(input_fn=lambda: input_fn(df_train), steps=151) #,monitors=[validation_monitor])
```

and then the code executes fine.

### What other attempted solutions have you tried?
I also built an input_fn interface to the iris and boston housing price predictor code code, each showed similar 'hangs'

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).

As a noob, i'm learning that esoteric error messages are a luxury :-),  in this case the code just hangs indefinitely.
"
6726,Error importing Tensorflow,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System: ubuntu 16.04 LTS - Tensorflow-GPU installed ( GTX 970 )

Installed version of CUDA and cuDNN:  7.5 - 4.5
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`): 

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

Error importing tensorflow.  Unless you are using bazel,
you should not try to import tensorflow from its source directory;
please exit the tensorflow source tree, and relaunch your python interpreter
from there.


### What other attempted solutions have you tried?

Tried from scratch 4 times including formatting system. 

Installed from pip 2 times, and installed with bazel 2 times. No error at during setup. 

Tried all recommendations from other developers especially faced with this one.

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
6725,text_classification_character_rnn.py is using GRU cell at old location(compile error),"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

None

### Environment info
Operating System:

Mac

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
0.12.1

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

Try to compile/run (with latest 0.12.1 TF):

https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/examples/learn/text_classification_character_rnn.py

This causes error:

AttributeError: module 'tensorflow.contrib.rnn' has no attribute 'GRUCell'

### What other attempted solutions have you tried?

Looks like GRUCell got moved from tensorflow.contrib.rnn to tf.nn.rnn_cell.RNNCell.  I tried making this change to the example code, but this leads to other errors.  It looks like the signature changed too.  

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
6724,Feature request: 1D deconvolution,"Pretty much topic tells the whole story.

For auto-encoder based on 1D convolutions, opposite operation is required to complete decoding part.
This is planned to be used for sound processing."
6722,tensorflow conv2d unexpected convolution result,"I try to migrate a `Caffe` network and model(weights) to `tensorflow`. The original first layer is a stride one convolution on 1x128x128 gray image with kernel size 5x5, output channel 96, Caffe two padding, tf 'SAME' padding.
I checked the input image and kernel weights are the same for both versions.
However, the two's upper left corner of conv1's first feature map are **different**.
Only the leftmost column is consistent. I manually calculated and checked the results, the first and the second value of Caffe's (-0.71238005 -0.74042225) are correct according to the definition of convolution, the second value in tensorflow's (-0.71238005 -0.31195271) is **incorrect**. Taking into account the padding, the first value is from the 3x3 block of the image, the second should be the 3x4 block.
Is there any subtle difference in computation model between the two version's implementation of convolution? 
I am not sure whether it is a bug of `tf` or an usage omission. So I posted the detailed version of the problem on [StackOverflow](http://stackoverflow.com/q/41529997/3863647)."
6720,tf.image.resize_images() - weird padding behaviour? ,"The tf.image.resize_images() seems to use a strange padding option, which one is not clear to me at the moment. I tried to replicate the bilinear interpolation with various padding options in for example skimage, but cant replicate the behaviour.

It would be nice to be able to set the padding option used in tf.images.resize_images(), or document what is used at least.

Example code for comparing the results of  tf.images.resize_images() and skimage transform:
Looks like  tf.images.resize_images() does some weird unsymmetrical padding!?
Using tensorflow 0.12.1:
```
import tensorflow as tf
import tensorlayer as tl
import numpy as np
import skimage
from scipy.misc import imread, imresize, imsave

sess = tf.InteractiveSession()

#create simple test image
imsize = 3
xa, ya = np.ogrid[:imsize, :imsize]
img = np.repeat((xa + ya)[..., np.newaxis], 3, 2) / float(imsize + imsize)

x = tf.placeholder(tf.float32, [1, imsize, imsize, 3])
y = tf.image.resize_images(x,(imsize*3, imsize*3))

sess.run(tf.global_variables_initializer())

upsampled_tf_result = sess.run(y, feed_dict={x: [img]})
upsampled_skimage_result = skimage.transform.rescale(img,
                                     3,
                                     mode='symmetric',
                                     cval=0,
                                     order=1,
                                     preserve_range=False)

print(np.allclose(upsampled_tf_result, upsampled_skimage_result))

imsave('upsampled_tf_result.png', np.squeeze(upsampled_tf_result))
imsave('upsampled_skimage_result.png', upsampled_skimage_result)
```"
6718,Typos in docs for expand_dims,"`dim` has been replaced by `axis` in the documentation.

(https://www.tensorflow.org/api_docs/python/array_ops/shapes_and_shaping#expand_dims)"
6717,Incorrect gradient for categorical distribution entropy,"The **Categorical** distribution class provides an awesome **entropy** operator but apparently the **gradient** calculation w.r.t. the input operators **doesn't work**.

```python
logits = tf.Variable(initial_value=[[1., 2., 3.], [2., 5., 1.]])

probabilities = tf.nn.softmax(logits)
log_probabilities = tf.nn.log_softmax(logits)
entropy = - tf.reduce_sum(probabilities * log_probabilities, axis=-1)

# using the actual distribution would be nicer but gradients seem buggy
categorical_distribution = tf.contrib.distributions.Categorical(p=probabilities)
categorical_distribution_entropy = categorical_distribution.entropy()

# initialize
init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)

# works
print(sess.run(entropy))
print(sess.run(tf.gradients(entropy, [logits])))

# apparently loses gradient information
print(sess.run(categorical_distribution_entropy))
print(sess.run(tf.gradients(categorical_distribution_entropy, [logits])))
```

In the outputs we see that the entropy calculation works but the gradients are somehow lost. This obviously also doesn't work when I try to maximize this entropy using any optimizer.
```
[ 0.83239555  0.27431309]
[array([[ 0.14181709,  0.14077036, -0.28258738],
       [ 0.13012242, -0.19513965,  0.06501719]], dtype=float32)]
[ 0.83239555  0.27431309]
[array([[ 0.,  0.,  0.],
       [ 0.,  0.,  0.]], dtype=float32)]
```"
6716,Feature request: easier access to tensor de-allocation information,"TLDR; to debug TensorFlow out-of-memory situations one needs to see tensor de-allocation info.

You can see allocation stats in timeline, but without de-allocation info you can't calculate peak memory. Currently getting peak memory is possible by:

1. Hacking TensorFlow to print deallocation messages with timestamps as [here](https://github.com/yaroslavvb/tensorflow/commit/5d4cd97c0a73e91ee37c025cd7a62fb46aae76a0)

2. A bunch of regular expression to parse `__LOG_MEMORY__` messages as in [here](https://github.com/yaroslavvb/notebooks/blob/master/saving%20memory%20by%20using%20functions.ipynb)

Since this needs building your own version of tensorflow, this is not accessible to most people. Perhaps this can be remedied by adding deallocation events to session run timeline? @michaelisard 

Some recent places this issue came up:
http://stackoverflow.com/questions/41517145/outer-product-based-conv-filters-consume-disproportionately-high-memory
http://stackoverflow.com/questions/41496251/fitting-large-matrix-calculations-into-memory-when-using-tensorflow
http://stackoverflow.com/questions/41451273/tensorflow-specifying-storage-of-layer-activations
http://stackoverflow.com/questions/40190510/tensorflow-how-to-log-gpu-memory-vram-utilization/40197094#comment70145571_40197094
https://github.com/tensorflow/tensorflow/issues/6019#issuecomment-268037463"
6714,TFSlim: evaluate multiple validation batches using `evaluation_loop`,"I'm trying to use placeholders for training with TF-Slim, as my training + evaluation data comes from outside of TF World. I've now got training working by using the workaround described in #6604 and keeping track of the last training batch to supply in the feed dict when calling `sess.run(summary_op)`.

However, evaluation using `evaluation_loop` over multiple batches seems to not be possible. There are `summary_op_feed_dict` and `eval_op_feed_dict` arguments to the `evaluation_loop` function, but if I understand correctly, if I set `num_evals` > 1, then the same feed dict (+ data) will be used for every evaluation.

I'm happy add this functionality and issue a PR – is it true that right now the ability to evaluate over multiple validation batches using placeholders is not implemented? I'm using TF 12.1."
6713,Queues don't support enqueuing SparseTensor?,"It seems that right now RandomShuffleQueue doesn't support SparseTensor. When I run this minimal example
```
import tensorflow as tf
values = tf.constant([0, 1, 2], dtype=tf.int32)
indices = tf.constant([[0, 0], [0, 1], [0, 2]], dtype=tf.int64)
shape = tf.constant([1, 3], dtype=tf.int64)

sparse = tf.SparseTensor(values=values, indices=indices, shape=shape)
queue = tf.RandomShuffleQueue(2, 1, [tf.int32])
queue.enqueue([sparse])
```
I get the following error:
```
TypeError                                 Traceback (most recent call last)
<ipython-input-12-a8e6053e53cf> in <module>()
----> 1 queue.enqueue([sparse])

/Users/astepanov/.virtualenvs/py_asr/lib/python3.5/site-packages/tensorflow/python/ops/data_flow_ops.py in enqueue(self, vals, name)
    320     with ops.name_scope(name, ""%s_enqueue"" % self._name,
    321                         self._scope_vals(vals)) as scope:
--> 322       vals = self._check_enqueue_dtypes(vals)
    323 
    324       # NOTE(mrry): Not using a shape function because we need access to

/Users/astepanov/.virtualenvs/py_asr/lib/python3.5/site-packages/tensorflow/python/ops/data_flow_ops.py in _check_enqueue_dtypes(self, vals)
    275     for i, (val, dtype) in enumerate(zip(vals, self._dtypes)):
    276       tensors.append(ops.convert_to_tensor(val, dtype=dtype,
--> 277                                            name=""component_%d"" % i))
    278 
    279     return tensors

/Users/astepanov/.virtualenvs/py_asr/lib/python3.5/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype)
    667 
    668         if ret is None:
--> 669           ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
    670 
    671         if ret is NotImplemented:

/Users/astepanov/.virtualenvs/py_asr/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)
    174                                          as_ref=False):
    175   _ = as_ref
--> 176   return constant(v, dtype=dtype, name=name)
    177 
    178 

/Users/astepanov/.virtualenvs/py_asr/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name, verify_shape)
    163   tensor_value = attr_value_pb2.AttrValue()
    164   tensor_value.tensor.CopyFrom(
--> 165       tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))
    166   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)
    167   const_tensor = g.create_op(

/Users/astepanov/.virtualenvs/py_asr/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape)
    365       nparray = np.empty(shape, dtype=np_dt)
    366     else:
--> 367       _AssertCompatible(values, dtype)
    368       nparray = np.array(values, dtype=np_dt)
    369       # check to them.

/Users/astepanov/.virtualenvs/py_asr/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py in _AssertCompatible(values, dtype)
    300     else:
    301       raise TypeError(""Expected %s, got %s of type '%s' instead."" %
--> 302                       (dtype.name, repr(mismatch), type(mismatch).__name__))
    303 
    304 

TypeError: Expected int32, got <tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x105fedef0> of type 'SparseTensor' instead.
```"
6712,"Installed succesfully, but went wrong and outputed  ‘undefined symbol: zgelsd_’ when testing","### Environment info
Operating System: Ubuntu14.04, python3.4.3

**I installed tensorflow using pip3 succesfully** from https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp34-cp34m-linux_x86_64.whl

**When I tried to test it using offical test code, it went wrong as follows:**
/usr/bin/python3.4 /home/daisy/PycharmProjects/miachaelLiang/test_tensorflow.py
Traceback (most recent call last):
  File ""/home/daisy/PycharmProjects/miachaelLiang/test_tensorflow.py"", line 9, in <module>
    import tensorflow as tf
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/__init__.py"", line 44, in <module>
    import numpy as np
  File ""/usr/local/lib/python3.4/dist-packages/numpy/__init__.py"", line 142, in <module>
    from . import add_newdocs
  File ""/usr/local/lib/python3.4/dist-packages/numpy/add_newdocs.py"", line 13, in <module>
    from numpy.lib import add_newdoc
  File ""/usr/local/lib/python3.4/dist-packages/numpy/lib/__init__.py"", line 18, in <module>
    from .polynomial import *
  File ""/usr/local/lib/python3.4/dist-packages/numpy/lib/polynomial.py"", line 20, in <module>
    from numpy.linalg import eigvals, lstsq, inv
  File ""/usr/local/lib/python3.4/dist-packages/numpy/linalg/__init__.py"", line 51, in <module>
    from .linalg import *
  File ""/usr/local/lib/python3.4/dist-packages/numpy/linalg/linalg.py"", line 29, in <module>
    from numpy.linalg import lapack_lite, _umath_linalg
### ImportError:
 **/usr/local/lib/python3.4/dist-packages/numpy/linalg/lapack_lite.cpython-34m.so: undefined symbol: zgelsd_**






"
6711,Android demo accuracy,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
I searched #1269 #504 

### Environment info
Mac OS for build and Android version 5 to run .apk demo.   

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

I followed the steps mentioned in #1269 and could able to run the example successfully, but the accuracy of the result is very low and often wrong. I have trained my systems on 25 different daily used products like soap, soup, noodles, etc. 
Where as when i run the same example using following script it give me very high accuracy (approx. 90-95%)

```
import sys
import tensorflow as tf
// change this as you see fit
image_path = sys.argv[1]

// Read in the image_data
image_data = tf.gfile.FastGFile(image_path, 'rb').read()

// Loads label file, strips off carriage return
label_lines = [line.rstrip() for line 
                   in tf.gfile.GFile(""/tf_files/retrained_labels.txt"")]

// Unpersists graph from file
with tf.gfile.FastGFile(""/tf_files/retrained_graph.pb"", 'rb') as f:
    graph_def = tf.GraphDef()
    graph_def.ParseFromString(f.read())
    _ = tf.import_graph_def(graph_def, name='')

with tf.Session() as sess:
    // Feed the image_data as input to the graph and get first prediction
    softmax_tensor = sess.graph.get_tensor_by_name('final_result:0')
    
    predictions = sess.run(softmax_tensor, \
             {'DecodeJpeg/contents:0': image_data})
    
    // Sort to show labels of first prediction in order of confidence
    top_k = predictions[0].argsort()[-len(predictions[0]):][::-1]
    
    for node_id in top_k:
        human_string = label_lines[node_id]
        score = predictions[0][node_id]
        print('%s (score = %.5f)' % (human_string, score))
```

The only difference i see here is that the model file used in the android demo is stripped because it does not support DecodeJpeg, whereas in the above code its the actually generated unstripped model. Is there any specific reason or somewhere i am wrong here? 

### What other attempted solutions have you tried?
Yes, i the above script and it gives me quite high accuracy result. 
"
6709,can't find zlib 1.2.8,"native.new_http_archive(
    name = ""zlib_archive"",
    url = ""http://zlib.net/zlib-1.2.8.tar.gz"",
    sha256 = ""36658cb768a54c1d4dec43c3116c27ed893e88b02ecfcb44f2166f9c0b7f2a0d"",
    strip_prefix = ""zlib-1.2.8"",
    build_file = str(Label(""//:zlib.BUILD"")),
)

1.2.10 is released"
6706,Bazel problem when using Tensorflow as an external dependency.,"When using the HEAD version of Tensorflow as an external Bazel dependency (like tensorflow_serving does), we run into an issue in the line:

`load(""@//third_party:common.bzl"", ""template_rule"")`

inside

`tensorflow/third_party/jpeg/jpeg.BUILD`

That line assumes that Tensorflow is the main repository. But when Tensorflow is included in another project the main repository is the main project so that common.bzl file is not found. I think that line needs to read:

`load(""@org_tensorflow//third_party:common.bzl"", ""template_rule"")`"
6702,Deadlock when decoding TFRecords,"I am storing my training examples as variable-length TFRecords using the following function for encoding:

```
def convert_to_tf_example(const_exonic_seq, const_intronic_seq,
                          alt_exonic_seq, alt_intronic_seq,
                          psi_distribution, psi_std,
                          alt_ss_position, alt_ss_type,
                          const_site_id, const_site_position,
                          n_alt_ss, event_type):
    """"""Encode a COSSMO example as a TFRecord""""""

    assert len(alt_exonic_seq) == n_alt_ss
    assert len(alt_intronic_seq) == n_alt_ss
    # assert len(psi_distribution) == n_alt_ss
    # assert len(psi_std) == n_alt_ss
    assert event_type in ['acceptor', 'donor']
    assert len(alt_ss_type) == n_alt_ss
    assert all([t in ('annotated', 'gtex', 'maxent', 'hard_negative')
                for t in alt_ss_type])

    example = tf.train.SequenceExample(
        context=tf.train.Features(feature={
            'n_alt_ss': tf.train.Feature(
                int64_list=tf.train.Int64List(value=[n_alt_ss])
            ),
            'event_type': tf.train.Feature(
                bytes_list=tf.train.BytesList(value=[event_type])
            ),
            'const_seq': tf.train.Feature(
                bytes_list=tf.train.BytesList(
                    value=[const_exonic_seq, const_intronic_seq])
            ),
            'const_site_id': tf.train.Feature(
                bytes_list=tf.train.BytesList(
                    value=[const_site_id])
            ),
            'const_site_position': tf.train.Feature(
                int64_list=tf.train.Int64List(
                    value=[const_site_position])
            )
        }),
        feature_lists=tf.train.FeatureLists(feature_list={
            'alt_seq': tf.train.FeatureList(
                feature=[tf.train.Feature(
                    bytes_list=tf.train.BytesList(value=[aes, ais])
                ) for aes, ais in
                         zip(alt_exonic_seq, alt_intronic_seq)]
            ),
            'psi': tf.train.FeatureList(
                feature=[tf.train.Feature(
                    float_list=tf.train.FloatList(value=psi))
                         for psi in psi_distribution]),
            'psi_std': tf.train.FeatureList(
                feature=[tf.train.Feature(
                    float_list=tf.train.FloatList(value=psi_sd))
                         for psi_sd in psi_std]),
            'alt_ss_position': tf.train.FeatureList(
                feature=[tf.train.Feature(
                    int64_list=tf.train.Int64List(value=[pos]))
                         for pos in alt_ss_position]),
            'alt_ss_type': tf.train.FeatureList(
                feature=[tf.train.Feature(
                    bytes_list=tf.train.BytesList(value=[t]))
                        for t in alt_ss_type])
        })
    )
    return example
```

The data stored here is genomic, but the details shouldn't matter.

When I'm training, I use the following function to decode the TFRecords:
```
def read_single_cossmo_example(serialized_example, n_tissues):
    """"""Decode a single COSSMO example""""""

    decoded_features = tf.parse_single_sequence_example(
        serialized_example,
        context_features={
            'n_alt_ss': tf.FixedLenFeature([], tf.int64),
            'event_type': tf.FixedLenFeature([], tf.string),
            'const_seq': tf.FixedLenFeature([2], tf.string),
            'const_site_id': tf.FixedLenFeature([], tf.string),
            'const_site_position': tf.FixedLenFeature([], tf.int64)
        },
        sequence_features={
            'alt_seq': tf.FixedLenSequenceFeature([2], tf.string),
            'psi': tf.FixedLenSequenceFeature([n_tissues], tf.float32),
            'psi_std': tf.FixedLenSequenceFeature([n_tissues], tf.float32),
            'alt_ss_position': tf.FixedLenSequenceFeature([], tf.int64),
            'alt_ss_type': tf.FixedLenSequenceFeature([], tf.string)
        }
    )
    return decoded_features
```

During training, I've been getting deadlocks with triple-digit CPU loads during training (running on a six-core i7) and I've isolated the problem to the above decoding function. A simple Tensorflow program like the following will reproduce the deadlock:

```
 with tf.device('/cpu:0'):
    filename_queue = tf.train.string_input_producer(
        train_files, num_epochs=num_epochs, shuffle=shuffle)
    file_reader = tf.TFRecordReader()
    tf_record_key, serialized_example = file_reader.read(filename_queue)
    decoded_example = read_single_cossmo_example(serialized_example,
                                                  n_tissues)

with tf.Session() as sess:
    sess.run(
        tf.variables_initializer(
            tf.global_variables() + tf.local_variables()
        )
    )

    # Start queue runners
    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(sess=sess, coord=coord)

    try:
        while not coord.should_stop():
            fetch_vals = sess.run(decoded_example)
    except tf.errors.OutOfRangeError:
        pass
    except KeyboardInterrupt:
        print ""Training stopped by Ctrl+C.""
    finally:
        coord.request_stop()
    coord.join(threads)
```

Now, setting `intra_op_threads = 1` and `inter_op_threads = 1` will prevent the small script from deadlocking. However, even when restricting the thread pool I have run into deadlocks when using these TFRecords in long running training sessions, so I suspect there is a deeper issue.

### Environment info
Operating System: Ubuntu 14.04

Installed version of CUDA and cuDNN: 
CUDA: 8.0
cuDNN: 5.1.5
If installed from binary pip package, provide:
https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp27-none-linux_x86_64.whl

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
6701,Possible bug: tf.reverse() does not accept bool type 'dim' argument in r0.12,"I think tf.reverse() got messed up in r0.12. Though the [documentation](https://www.tensorflow.org/api_docs/python/array_ops/slicing_and_joining#reverse) says the argument 'dim' has to be 'bool', while running it complains -- ""Value passed to parameter 'axis' has DataType bool not in list of allowed values: int32, int64"". I can see that this comes from a new op -- tf.reverse_v2(). I saw that in r0.11 this runs with no complains. I have prepared a mwe for this (link provided below).
Either the documentation needs to be updated (for r0.12) or this needs to be fixed so that dims can accept boolean. I'd, rather, opt for a fix as this might be a reason for backward incompatibility in some legacy code.

### Environment info
Operating System: Centos 7
Installed version of CUDA and cuDNN: cuda 8.0, cuDNN 5.1
If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`): e4dde23d58a10c9d0c14005d20d1ecdd599539ac
2. The output of `bazel version` - 4.2

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
I have prepared a mwe for this (Just 26 lines of code). This tries to reproduce one of the examples given in the documentation. Attached
[here](https://github.com/tensorflow/tensorflow/files/690954/mwe.txt)

### Logs or other output that would be helpful
Traceback (most recent call last):
  File ""mwe.py"", line 16, in <module>
    t, dims, t_rev = flip()
  File ""mwe.py"", line 10, in flip
    t_rev = tf.reverse(t, dims)
  File ""/scratch/virtual_envs/tensorflow_r12_env/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py"", line 2679, in reverse
    return gen_array_ops.reverse_v2(tensor, axis, name)
  File ""/scratch/virtual_envs/tensorflow_r12_env/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 2963, in reverse_v2
    name=name)
  File ""/scratch/virtual_envs/tensorflow_r12_env/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 585, in apply_op
    param_name=input_name)
  File ""/scratch/virtual_envs/tensorflow_r12_env/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 61, in _SatisfiesTypeConstraint
    "", "".join(dtypes.as_dtype(x).name for x in allowed_list)))
TypeError: Value passed to parameter 'axis' has DataType bool not in list of allowed values: int32, int64
"
6700,tensorflow docs have sigmoid_cross_entropy_with_logits args flipped,"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard5/tf.nn.sigmoid_cross_entropy_with_logits.md 

labels comes before logits, please excuse if i'm way off base, not sure where these docs are generated from"
6698,Crash: Could not create cuDNN handle when convnets are used,"Tensorflow (GPU) was imported successfully, but when running a session that involves a convolutional neural network (CNN), Python crashes with the following message:

    E tensorflow/stream_executor/cuda/cuda_dnn.cc:385] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
    E tensorflow/stream_executor/cuda/cuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM
    F tensorflow/core/kernels/conv_ops.cc:605] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms) 

The problem persists on any combination of CUDA toolkit 7.5/8.0 and Tensorflow installed from pip/source. Test sessions that do not use CNNs are run successfully.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

The issue is similar to https://github.com/tensorflow/tensorflow/issues/6586, where I first commented. But since I experience the problem on a Mac, I was suggested to open a separate issue.

### Environment info
Operating System: macOS Sierra 10.12.2
Xcode version 8.2 (8C38) (When I later tried CUDA 7.5, I installed Command Line Tools version 7.3.1 because CUDA 7.5 lacked support of the more recent compilers.)
Python 3.5.2 (anaconda)

Installed version of CUDA: tried both 8.0 (initially) and 7.5 (reported here, toolkit only -- the driver is still 8.0)
Installed version of cuDNN: 5.1 (different installations according to CUDA versions)
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

    lrwxr-xr-x  1 root   wheel        33  5 Jan 20:33 /usr/local/cuda/lib/libcuda.1.dylib -> /usr/local/cuda/lib/libcuda.dylib
    -rwxr-xr-x@ 1 root   wheel      8280 13 Apr  2016 /usr/local/cuda/lib/libcuda.dylib
    lrwxr-xr-x@ 1 root   wheel        45 13 Apr  2016 /usr/local/cuda/lib/libcudadevrt.a -> /Developer/NVIDIA/CUDA-7.5/lib/libcudadevrt.a
    lrwxr-xr-x@ 1 root   wheel        50 13 Apr  2016 /usr/local/cuda/lib/libcudart.7.5.dylib -> /Developer/NVIDIA/CUDA-7.5/lib/libcudart.7.5.dylib
    lrwxr-xr-x@ 1 root   wheel        46 13 Apr  2016 /usr/local/cuda/lib/libcudart.dylib -> /Developer/NVIDIA/CUDA-7.5/lib/libcudart.dylib
    lrwxr-xr-x@ 1 root   wheel        49 13 Apr  2016 /usr/local/cuda/lib/libcudart_static.a -> /Developer/NVIDIA/CUDA-7.5/lib/libcudart_static.a
    lrwxr-xr-x  1 root   wheel        16  5 Jan 17:14 /usr/local/cuda/lib/libcudnn.5 -> libcudnn.5.dylib
    -rwxr-xr-x@ 1 ymfa   staff  58975112 10 Jun  2016 /usr/local/cuda/lib/libcudnn.5.dylib
    lrwxr-xr-x@ 1 ymfa   staff        16 10 Jun  2016 /usr/local/cuda/lib/libcudnn.dylib -> libcudnn.5.dylib
    lrwxr-xr-x  1 root   wheel        16  5 Jan 17:14 /usr/local/cuda/lib/libcudnn5.dylib -> libcudnn.5.dylib
    -rw-r--r--@ 1 ymfa   staff  56392320 10 Jun  2016 /usr/local/cuda/lib/libcudnn_static.a

I tried both installing from pip and source. I first installed from binary pip package:

1. A link to the pip package you installed:
`tensorflow-gpu`
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
`0.12.head`

Later I installed from source (the pip package was uninstalled):

1. The commit hash (`git rev-parse HEAD`)
`d67c09d98a576e1fbf2f3609ddb842e53890f31c`
2. The output of `bazel version`

    Build label: 0.4.3-homebrew
    Build target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
    Build time: Thu Dec 22 15:20:15 2016 (1482420015)
    Build timestamp: 1482420015
    Build timestamp as int: 1482420015

### If possible, provide a minimal reproducible example

I made a minimal example by simplifying the network and reducing the training data to only twenty images and two classes for classification. [issue.zip](https://github.com/tensorflow/tensorflow/files/691561/issue.zip) contains the Python code and the data. I wrote two convolutional layers because I found the network with only one convolutional layer runs without problem.

### Complete log using CUDA 7.5 and Tensorflow compiled from source

    I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcublas.7.5.dylib locally
    I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcudnn.5.dylib locally
    I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcufft.7.5.dylib locally
    I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcuda.1.dylib locally
    I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcurand.7.5.dylib locally
    W tensorflow/core/platform/cpu_feature_guard.cc:95] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
    W tensorflow/core/platform/cpu_feature_guard.cc:95] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
    W tensorflow/core/platform/cpu_feature_guard.cc:95] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
    I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:874] OS X does not support NUMA - returning NUMA node zero
    I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
    name: GeForce GT 650M
    major: 3 minor: 0 memoryClockRate (GHz) 0.9
    pciBusID 0000:01:00.0
    Total memory: 1023.69MiB
    Free memory: 740.18MiB
    I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
    I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
    I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 650M, pci bus id: 0000:01:00.0)
    E tensorflow/stream_executor/cuda/cuda_dnn.cc:385] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
    E tensorflow/stream_executor/cuda/cuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM
    F tensorflow/core/kernels/conv_ops.cc:605] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms) 

### Complete log using CUDA 8.0 and Tensorflow installed from pip

    I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.dylib locally
    I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.dylib locally
    I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.dylib locally
    I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.1.dylib locally
    I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.dylib locally
    I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] OS X does not support NUMA - returning NUMA node zero
    I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
    name: GeForce GT 650M
    major: 3 minor: 0 memoryClockRate (GHz) 0.9
    pciBusID 0000:01:00.0
    Total memory: 1023.69MiB
    Free memory: 590.00MiB
    I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
    I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0: Y 
    I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 650M, pci bus id: 0000:01:00.0)
    E tensorflow/stream_executor/cuda/cuda_dnn.cc:385] could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED
    E tensorflow/stream_executor/cuda/cuda_dnn.cc:392] error retrieving driver version: Invalid argument: expected %d.%d or %d.%d.%d form for driver version; got """"
    E tensorflow/stream_executor/cuda/cuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM
    F tensorflow/core/kernels/conv_ops.cc:532] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)
"
6696,conv2d / Kernel Size > Input does not raise ValueError anymore,"I noticed that tensorflow 0.10.0 raises a ValueError when I pass a kernel size that is larger than the first input dimensions. The error occurred when I tried to pass data in NCHW format, while the default data_format is NHWC.

I used this code for replication of the problem:

```
import tensorflow as tf

# NHWC
input_nhwc = tf.placeholder(tf.float32, [None, 84, 84, 4])
output_nhwc = tf.contrib.layers.conv2d(input_nhwc, 32, [8, 8], 4)

# NCHW
input_nchw = tf.placeholder(tf.float32, [None, 4, 84, 84])
output_nchw = tf.contrib.layers.conv2d(input_nchw, 32, [8, 8], 4)
```

tensorflow 0.12.1 does not raise any error, whereas tensorflow 0.10.0 raises the following error:

```
Traceback (most recent call last):
  File ""conv2d_error.py"", line 5, in <module>
    output_nhwc = tf.contrib.layers.conv2d(input_nhwc, 32, [8, 8], 4)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 171, in func_with_args
    return func(*args, **current_args)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 411, in convolution2d
    padding=padding)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py"", line 394, in conv2d
    data_format=data_format, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 703, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2319, in create_op
    set_shapes_for_outputs(ret)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1711, in set_shapes_for_outputs
    shapes = shape_func(op)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.py"", line 246, in conv2d_shape
    padding)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.py"", line 184, in get2d_conv_output_size
    (row_stride, col_stride), padding_type)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.py"", line 149, in get_conv_output_size
    ""Filter: %r Input: %r"" % (filter_size, input_size))
ValueError: Filter must not be larger than the input: Filter: (8, 8) Input: (4, 84)
```

I tried to track if there's any kind of automatic detection of data formats (there is not), and further why the issue persists. All I could find were the filter and input shapes in nn_ops.py, which seem to pass compability tests in tensorflow 0.12.0.

```
NHWC
Filter shape: (8, 8, 4, 32) | input shape: (?, 84, 84, 4)
----------------
NCHW
Filter shape: (8, 8, 84, 32) | input shape: (?, 4, 84, 84)
```

I'm not sure if this is expected behavior, or if an exception should have been raised in tensorflow 0.12.1 as well.
"
6695,Cherry Pick bundle_shim to 0.12.x branch,"bundle_shim.py was added to [""allows a system to load both legacy session bundle and SavedModel bundle.""](https://github.com/tensorflow/tensorflow/commit/de1f21e98a6dd14a5e4c1d2d9c14b430f57493b3#diff-340f927179763894226603c8e38e0ec4) however this is only on master. This leads to issues saving and then loading back models, for example see: https://github.com/tensorflow/tensorflow/issues/6336.

I suggest cherry picking it to current release branch."
6694,Why cannot I import the tpprof module?,"From [this line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tfprof/python/tools/tfprof/tfprof_logger.py#L30) I get to know that the new added profile tool is imported, but it seems strange: 1) it cannot be imported from the interactive interface; 2) and when mnist is run it raise the same error saying `ImportError: cannot import name 'tfprof_log_pb2'`. What happened for the newly added feature?"
6693,"Misnamed libcuda.dylib. (CUDA 8.0, macOS 10.12)","### Misnamed `libcuda.dylib` in prebuilt binary. (CUDA 8.0, macOS 10.12).
Tensorflow fails to load `libcuda.1.dylib`, a file which does not exist for CUDA 8.0 on macOS. The filename is `libcuda.dylib`.

Resolved by `sudo ln -s /usr/local/cuda/lib/libcuda.dylib /usr/local/cuda/lib/libcuda.1.dylib`.

Preferable solution is a change to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/dso_loader.cc#L82.

I would change `FormatLibraryFileName(""cuda"", ""1"")` to `FormatLibraryFileName(""cuda"")`, but I'm not sure on which OS and CUDA versions this is the correct naming scheme. Possibly a check is needed, `if ( stoi(GetCudaVersion()) >= 8 ) {...}` and maybe `if (__APPLE__) {...}`. 

### Related issue
https://github.com/tensorflow/tensorflow/issues/2278
Solution provided [here](https://github.com/tensorflow/tensorflow/issues/2278#issuecomment-244828047) by @eagleflo, @martinianodl.
### Environment info
macOS 10.12.2
CUDA Toolkit 8.0
```
ls -l /usr/local/cuda/lib/libcud*
lrwxr-xr-x  1 root  wheel     33 Jan  6 14:34 /usr/local/cuda/lib/libcuda.1.dylib -> /usr/local/cuda/lib/libcuda.dylib
-rwxr-xr-x@ 1 root  wheel  13504 Nov  3 20:39 /usr/local/cuda/lib/libcuda.dylib
lrwxr-xr-x@ 1 root  wheel     45 Nov  3 20:40 /usr/local/cuda/lib/libcudadevrt.a -> /Developer/NVIDIA/CUDA-8.0/lib/libcudadevrt.a
lrwxr-xr-x@ 1 root  wheel     50 Nov  3 20:40 /usr/local/cuda/lib/libcudart.8.0.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.8.0.dylib
lrwxr-xr-x@ 1 root  wheel     46 Nov  3 20:40 /usr/local/cuda/lib/libcudart.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.dylib
lrwxr-xr-x@ 1 root  wheel     49 Nov  3 20:40 /usr/local/cuda/lib/libcudart_static.a -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart_static.a
lrwxr-xr-x  1 root  wheel     47 Jan  6 13:19 /usr/local/cuda/lib/libcudnn.5.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudnn.5.dylib
lrwxr-xr-x  1 root  wheel     45 Jan  6 13:19 /usr/local/cuda/lib/libcudnn.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudnn.dylib
lrwxr-xr-x  1 root  wheel     48 Jan  6 13:19 /usr/local/cuda/lib/libcudnn_static.a -> /Developer/NVIDIA/CUDA-8.0/lib/libcudnn_static.a

```
Installed as a virtualenv with the prebuilt binary.

TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-0.12.1-py3-none-any.whl
```
>>> tf.__version__
'0.12.1'
```

"
6692,Training only a subset of synaptic weights,"It is often necessary to train only a subest of synaptic weights. One should be able to specify an arbitrary list of individual weights that will be updated/kept fixed by the gradient decent optimization algorithm and by other training algorithms. 

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

There are no solutions for training only a subset of individual synaptic weights. One is only able to define an entire tensor as not trainable, but it is not possible to define individual weights within a tensor as trainable / non-trainable.


"
6691,"Feature Request: provide env vars to control resource usage (allow_growth, threads etc)","We provide GPGPU access on a cluster and to let the scheduler (HTCondor) assign the resources. Tt would be great to be able to set
- the number of theads to use
- the allow_growth options
- and the per_process_gpu_memory_fraction option
via environment variables e.g. TF_NUM_THREADS=4, TF_ALLOW_GROW=1 etc.

"
6690,"Calculation Delta, Memoization, Update Propagation and Laziness","If you just update a few element in the data structure then having to do calculations which are not affected by the change is a waste.  Also if you read only or consume un affected values then the update and calculations themselves are a waste. Also eagerly calculating this would be a waste since I might only read the result after few updates. In addition you can avoid calculating invariant items. Perhaps XLA can get Lightweight Modular Staging (LMS) functionality to do this type of analysis and optimisation.

Also see: https://github.com/tensorflow/tensorflow/issues/6690

> For example, if you took the sum of your price data, then adjusted the window, then recomputed the sum, TensorFlow, would recompute the entire sum, whereas you would probably prefer it to simply subtract the entries that were removed from the window, and add the entries that entered the window.

Also other potential optimisations (will require some research):

 - If I have a matrix inverse and I update the original matrix, what would be the optimal calculation to arrive at an updated inverse matrix
 - Similarly for matrix multiplication and other operations."
6688,Outer product based Conv filters consume disproportionately high memory,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

None
### Environment info
Operating System:

Ubuntu 14.04
Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

None, I'm using the CPU version of TF. 
If installed from binary pip package, provide:

1. A link to the pip package you installed:
https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
0.10.0rc0
If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
The minimum reproducible example is in this [github gist](https://gist.github.com/sahiliitm/bed9511d15ba0c45adf3c33210d59232). The issue is of a disproportionately large memory usage. 

I modified the cifar10 example which ships with tensorflow to use outer-product of 3 vectors as the weights of the convolutional layers. This change can be seen in this [part of the code](https://gist.github.com/sahiliitm/bed9511d15ba0c45adf3c33210d59232#file-cifar10-py-L91). 

For simplicity, i have removed all parameter training operations and even loss computations. The current model only computes logits (forward pass) again and again. 
The unmodified code (can be setting the `use_outerp` flag to `False`) uses approximately 11.4 GB RAM
whereas the modified code (with outer product of vectors used as the convolutional weight tensor) uses a disproportionately high 17 GB RAM. 

Any idea why this is the case? 
My intuition as to why this might happen is that maybe the outer product operations are being executed _every single time that the conv filter is needed_  instead of being executed exactly once in every forward pass. Is this really the case? Is there a way to fix this? 

Steps to reproduce:
1. To run the default version of the code (low memory footprint):
python train.py --use_outerp='False'

2. To run the modified version of the code (high memory footprint):
python train.py --use_outerp='True'

### What other attempted solutions have you tried?
I'm not sure what the problem is, so haven't tried anything.

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
6687,Cannot run a distributed training example with tensorflow v0.12.1,"Hi,

I was trying to run a distributed tensorflow [example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dist_test/python/mnist_replica.py
) in the official repository with v0.12.1 (current latest release).
I can run asynchronous version without problems, but when I turned on `sync_replicas` tag, some errors occurred.
(Please check the following logs in details)

This example code can be ran successfully with v0.12.0, so I guess there might be some modification from 0.12.0 to 0.12.1?
Could someone check if that's the case? Thanks.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

I haven't found others reported this issue as tensorflow v0.12.1 just released,

### Environment info
Operating System:
Ubuntu 14.04

Installed version of CUDA and cuDNN: 
CUDA 7.5, cuDNN 5.1

```
> ls -l /usr/local/cuda/lib/libcud*
-rw-r--r-- 1 root root 189170 Oct 25 22:51 /usr/local/cuda/lib/libcudadevrt.a
lrwxrwxrwx 1 root root     16 Oct 25 22:51 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5
lrwxrwxrwx 1 root root     19 Oct 25 22:51 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18
-rwxr-xr-x 1 root root 311596 Oct 25 22:51 /usr/local/cuda/lib/libcudart.so.7.5.18
-rw-r--r-- 1 root root 558020 Oct 25 22:51 /usr/local/cuda/lib/libcudart_static.a
```

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
4d924e796368163eff11a8151e8505715345f58d (Release 0.12.1)
2. The output of `bazel version`
```
Build label: 0.4.2
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Dec 7 18:47:11 2016 (1481136431)
Build timestamp: 1481136431
Build timestamp as int: 1481136431
```

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

I simply used this example [mnist_replica.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dist_test/python/mnist_replica.py) in tensorflow repository.

### Logs or other output that would be helpful

First I launched the parameter server
```
export CUDA_VISIBLE_DEVICES=0; python mnist_replica.py --ps_hosts=""localhost:50000"" --worker_hosts=""localhost:50001"" --job_name=""ps"" --task_index=0 --num_gpus=1 --sync_replicas=True
```
and then launched the worker with `sync_replicas=True` tag
```
export CUDA_VISIBLE_DEVICES=1; python mnist_replica.py --ps_hosts=""localhost:50000"" --worker_hosts=""localhost:50001"" --job_name=""worker"" --task_index=0 --num_gpus=2 --train_steps=100 --sync_replicas=True
```

but I got some errors here

```
...
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40m, pci bus id: 0000:2b:00.0)
I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:197] Initialize GrpcChannelCache for job ps -> {0 -> localhost:50000}
I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:197] Initialize GrpcChannelCache for job worker -> {0 -> localhost:50001, 1 -> localhost:50002}
I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:211] Started server with target: grpc://localhost:50002
Traceback (most recent call last):
  File ""mnist_replica.py"", line 281, in <module>
    tf.app.run()
  File ""local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 43, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""mnist_replica.py"", line 186, in main
    train_step = opt.minimize(cross_entropy, global_step=global_step)
  File ""local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py"", line 279, in minimize
    name=name)
  File ""local/lib/python2.7/site-packages/tensorflow/python/training/sync_replicas_optimizer.py"", line 751, in apply_gradients
    array_ops.reshape(self._replica_id, (1,)),
  File ""local/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 2448, in reshape
    name=name)
  File ""local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 503, in apply_op
    as_ref=input_arg.is_ref).dtype.name
  File ""local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 669, in convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""local/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.py"", line 176, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""local/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.py"", line 165, in constant
    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py"", line 360, in make_tensor_proto
    raise ValueError(""None values not supported."")
ValueError: None values not supported.
```"
6683,Tensorflow Model with CTC loss having save and restore problem,"I am using tensorflow 0.12 without GPU support. I was testing it with various models. My template structure is
```
#Load some data from file
graph=tf.Graph()
with graph.as_default():
     #Build Network
     #saver=tf.train.Saver()
with tf.Session(graph=graph) as session:
     if(sys.argv[1]==""load""):
          saver.restore(session,""weight_last"")
     else:
           initop=tf.global_variables_initializer()
           session.run(initop)
    #Continue Training
```
Now, I am facing a strange issue. When I am creating a MLP or RNN with this structure with a categorical cross entropy loss model this saving and restoring is working perfectly, i.e. after restore the loss is showing exact value that was showed during last save. But unfortunately when the network is loaded with CTC loss then after restoring the model is starting almost a new training. I am not sure what is going wrong? Any help shall be highly appreciated."
6682,Weird behavior of tf.tensordot when shapes are partly known,"I frequently use ```tf.tensordot```, for its flexibility on axes.
However, as I multiply two tensors whose shapes are partly known, it cannot infer the shape of the result tensor, i.e. shape is `<unknown>`, while other alternative functions that do the same operation infer shapes well.

For example, let `a` and `b` be
```
a = tf.placeholder('float32', shape=[None, 100])
b = tf.placeholder('float32', shape=[100, 300])
```

We can validate that the below functions infer shapes well.
```
result_matmul = tf.matmul(a, b)
result_matmul.get_shape().as_list()  # [None, 300]

result_einsum = tf.einsum('ij,jk->ik', a, b)
result_einsum.get_shape().as_list()  # [None, 300]
```

However, when `tf.tensordot` is used, the weird result occurs:
```
result_tensordot = tf.tensordot(a, b, axes=[[1], [0]])
result_tensordot.get_shape()  # TensorShape(None)
result_tensordot.get_shape().as_list()  # Error
```
Thus, I have to call `set_shape(shape)` function explicitly to make the output be able to be used for further operations.

There also exists an issue that it still uses `concat` function, where the warning that it would be deprecated after 2016-12-14 occurs, instead of `concat_v2`."
6680,"Tensorflow(GPU) build needs to be reconfigured, everytime I reboot my ubuntu","If a just configure tensorflow using ./configure and set CUDA support to YES, tensorflow is successfully built. I can make some change in source code and rebuild using command:

bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

Build after I reboot my ubuntu, and try to build tensorflow using above command, it requires me to reconfigure tensorflow, and rebuild, which takes such a long time. error message is as follows:

ERROR: /home/wolfson/.cache/bazel/_bazel_wolfson/41eaf6c788f09c81cffb135517d04fa2/external/local_config_cuda/crosstool/BUILD:4:1: Traceback (most recent call last):
	File ""/home/wolfson/.cache/bazel/_bazel_wolfson/41eaf6c788f09c81cffb135517d04fa2/external/local_config_cuda/crosstool/BUILD"", line 4
		error_gpu_disabled()
	File ""/home/wolfson/.cache/bazel/_bazel_wolfson/41eaf6c788f09c81cffb135517d04fa2/external/local_config_cuda/crosstool/error_gpu_disabled.bzl"", line 3, in error_gpu_disabled
		fail(""ERROR: Building with --config=c..."")
ERROR: Building with --config=cuda but TensorFlow is not configured to build with GPU support. Please re-run ./configure and enter 'Y' at the prompt to build with GPU support.
ERROR: no such target '@local_config_cuda//crosstool:toolchain': target 'toolchain' not declared in package 'crosstool' defined by /home/wolfson/.cache/bazel/_bazel_wolfson/41eaf6c788f09c81cffb135517d04fa2/external/local_config_cuda/crosstool/BUILD."
6679,Throw uninitialized local variable error to reduce confusion.,"I reference this [post](http://stackoverflow.com/questions/41488276/cannot-get-a-simple-tfrecord-reader-to-work/41489484#41489484).

If one does not call `tf.local_variables_initializer()` when using `tf.train.string_input_producer` with `num_epochs` set as a local variable, it will throw an `OutOfRangeError` which might mislead people into thinking that the error lies with `tf.train.string_input_producer` instead of initializing the local variable.

Throwing an uninitialized local variable error would be very helpful in this case.

@mrry "
6678,"Installation from Source Error: ""Server finished RPC without an explicit exit code""","### Environment info
Operating System: Ubuntu 14.04
GPU: NVIDIA Titan X

Installed version of CUDA and cuDNN: CUDA 8.0 and cuDNN 5.1.5
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`) f64c288ffd20819e264b80a4977b2ca8f33bfd98 (should be the latest one...)
2. The output of `bazel version` 0.4.3

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

When running ""`bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`
"", it reported ""`Server finished RPC without an explicit exit code`"" after a few minutes' running. The last few lines looks as follows:

```
...
[2,244 / 3,310] Linking tensorflow/core/libarray_ops_op_lib.pic.lo
Server finished RPC without an explicit exit code
```

### What other attempted solutions have you tried?

I managed to install tensorflow via pip installation, and everything is running correctly (ex. Tensorflow demo).

Does anyone has any suggestions? Thanks in advance! :-)
"
6677,Tensorboard not showing data,"I have installed the newest version of tensorflow. I was trying to write my code to the tensorboard, but was not able.  
This is what i got as a response .
![image](https://cloud.githubusercontent.com/assets/14088328/21702101/ded3524a-d3a9-11e6-9334-30f960adaf8c.png)

I found out was the problem was. The tensorboard program was not able to find the log files, this was because of it was saved in a pathname that uses a non ""english"" letter, in this particular instance the letter ""Å"". So maybe you cant fix this bug

Best regards 
"
6675,Creating an auxiliary CUDA Stream for an op,"I am writing an op which (sadly) requires using multiple CUDA streams to be efficient. Specifically, I need to have one auxiliary CUDA stream on whatever device the op is being executed; without this stream, the operations in this op will get entangled with operations in the rest of the graph, because the op _has_ to occasionally wait for its GPU kernels to complete before it can do things on CPU.

If I need to have an auxiliary CUDA stream besides the primary TensorFlow stream, one limited *just*  to the invocation of this op, how can I do so?

(To clarify, the CUDA stream should be long-lived across multiple invocations of the op, probably, just that it shouldn't the same stream as used by the rest of the graph)"
6674,Generic swig rules,"Looking into integrating Swig into our Bazel build all I could find was Tensorflows usage.
Is there any reason parts like `_py_wrap_cc` have not been extracted to a more general script?
Would tensorflow be willing to use such a thirdparty script should it get extracted/provided (into another github project perhaps)?"
6670,How to read Android Demo Detection Model Priors.pb file ?,"Hello ! 

I would like to look into the assets file : multibox_location_priors.pb which contains boxpriors locations.
I want to use them in a iPythonNotebook to test the model with offline images but I don't know how to access properly to the values for all locations.
Is there any easy way to parse the file and get the locations values like it is possible to do with graph.pb file using these lines 

```
with tf.gfile.FastGFile(""graph.pb"", 'rb') as f:
    graph_def = tf.GraphDef()
    graph_def.ParseFromString(f.read())
```

Thanks in advance for your help
Alex"
6669,Python3 pickle treat tf.gfile.GFile wrong,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
None

### Environment info
Operating System: macOS Sierra 10.12.2

Installed version of CUDA and cuDNN: None
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`): None

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
4d924e796368163eff11a8151e8505715345f58d

2. The output of `bazel version` 
0.4.3-homebrew

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

```
import tensorflow as tf
import pickle

f = tf.gfile.GFile(""data_batch_1"")
pickle.load(f, encoding='latin1')
```

### What other attempted solutions have you tried?
I tried to manually read all data into a variable: `s = f.read()` and unpickle it: `pickle.loads(s, encoding='latin')`. It works well.

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).

`TypeError: a bytes-like object is required, not 'str'`

I don't know why this happened. I checked the file_io code, and it returns byte object when calling `read`. So maybe this is something about pickle, any ideas?"
6668,bad zlib link in core/BUILD,"even though https://github.com/tensorflow/tensorflow/issues/6594 fixed the link to zlib in workspace.bzl, I'm getting another error looking for zlib at a bad link during the bazel build. Here's the error: 
```
ERROR: /opt/tensorflow/models/syntaxnet/tensorflow/tensorflow/core/BUILD:911:1: no such package '@zlib_archive//': Error downloading from http://zlib.net/zlib-1.2.8.tar.gz to /root/.cache/bazel/_bazel_root/9f6232783b216d8ac3748a33750481a1/external/zlib_archive: Error downloading http://zlib.net/zlib-1.2.8.tar.gz to /root/.cache/bazel/_bazel_root/9f6232783b216d8ac3748a33750481a1/external/zlib_archive/zlib-1.2.8.tar.gz: 404 Not Found: <!DOCTYPE HTML PUBLIC ""-//IETF//DTD HTML 2.0//EN"">
<html><head>
<title>404 Not Found</title>
</head><body>
<h1>Not Found</h1>
<p>The requested URL /zlib-1.2.8.tar.gz was not found on this server.</p>
<p>Additionally, a 404 Not Found
error was encountered while trying to use an ErrorDocument to handle the request.</p>
</body></html>
 and referenced by '//tensorflow/core:lib_internal'.
____Loading package: @local_config_cc//
ERROR: /opt/tensorflow/models/syntaxnet/tensorflow/tensorflow/core/BUILD:911:1: no such package '@zlib_archive//': Error downloading from http://zlib.net/zlib-1.2.8.tar.gz to /root/.cache/bazel/_bazel_root/9f6232783b216d8ac3748a33750481a1/external/zlib_archive: Error downloading http://zlib.net/zlib-1.2.8.tar.gz to /root/.cache/bazel/_bazel_root/9f6232783b216d8ac3748a33750481a1/external/zlib_archive/zlib-1.2.8.tar.gz: 404 Not Found: <!DOCTYPE HTML PUBLIC ""-//IETF//DTD HTML 2.0//EN"">
<html><head>
<title>404 Not Found</title>
</head><body>
<h1>Not Found</h1>
<p>The requested URL /zlib-1.2.8.tar.gz was not found on this server.</p>
<p>Additionally, a 404 Not Found
error was encountered while trying to use an ErrorDocument to handle the request.</p>
</body></html>
```"
6666,tf.train.Saver does not restore input queue,"**example.py**
```python
import tensorflow as tf

queue = tf.train.string_input_producer(['data.txt'])

reader = tf.TextLineReader()
_, line = reader.read(queue)

global_step = tf.Variable(0, trainable=False, name='global_step')
global_step_op = tf.assign_add(global_step, 1)

with tf.Session() as sess:
    saver = tf.train.Saver()
    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(coord=coord)

    latest_checkpoint = tf.train.latest_checkpoint('/tmp')
    if latest_checkpoint:
        saver.restore(sess, latest_checkpoint)
        print('restored from', latest_checkpoint)
    else:
        sess.run([
            tf.local_variables_initializer(),
            tf.global_variables_initializer(),
        ])

    for i in range(5):
        value, step = sess.run([line, global_step_op])

        print(value, step)

    coord.request_stop()
    coord.join(threads)

    saver.save(sess, '/tmp/model')
```

**data.txt**
```txt
line 1
line 2
line 3
line 4
line 5
line 6
```

Will output on first run `python3 example.py`:
> b'line 1' 1
> b'line 2' 2
> b'line 3' 3

Will output on second run `python3 example.py`:
> restored from /tmp/model
> b'line 1' 4
> b'line 2' 5
> b'line 3' 6

but expected:
> restored from /tmp/model
> b'line 4' 4
> b'line 5' 5
> b'line 6' 6"
6661,InvalidArgumentError: for tensor bool tensorflow==0.12.1,"Hi,

I have experimented with tensorflow==0.12.1 and I have following problem:

I am trying to run:
```
inp = tf.placeholder(tf.float32, [None, 10, 10, 3], name='inp')
training = tf.placeholder(tf.bool, name='training')
x1 = np.random.random([20, 10, 10, 3])

config = tf.ConfigProto(device_count={'GPU': 0})
with tf.Session(config=config) as sess:
    sess.run(tf.global_variables_initializer())
    result = var.eval({inp: x1, is_training: True}, sess)
```

But I'm getting following stack trace:
```
  File ""myscript.py"", line 35, in tf_run
    sess.run(tf.global_variables_initializer())
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 766, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 964, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1014, in _do_run
    target_list, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1034, in _do_call
    raise type(e)(node_def, op, message)
InvalidArgumentError: You must feed a value for placeholder tensor 'is_training' with dtype bool
	 [[Node: is_training = Placeholder[dtype=DT_BOOL, shape=[], _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

Caused by op u'training', defined at:
    File ""myscript.py"", line 30, in tf_run
    training = tf.placeholder(tf.bool, name='training')
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py"", line 1587, in placeholder
    name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py"", line 2043, in _placeholder
    name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 759, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2240, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1128, in __init__
    self._traceback = _extract_stack()
```

This problem appears on:
`Ubuntu 16.04.1 - tensorflow GPU installation`
`Mac - tensorflow CPU only installation`

When I try to downgrade to `tensorflow==0.11.0` and replace line `sess.run(tf.global_variables_initializer())` by `sess.run(tf.initialize_all_variables())`, then everything works well."
6660,using TensorFlow on Windows Server 2012,"I'm using TensorFlow on Windows Server 2012, TensorFlow version is 0.12.1
I've install CUDA (CUDA Version 8.0.44), and install cuDnn (cuDNN v5.1 (August 10, 2016), for CUDA 8.0).

that cuDnn is build for windows 10.

But when I run my tensorflow code is get some error about:
```
am_executor\cuda\cuda_dnn.cc:385] could not create cudnn handle: CUDNN_STATUS_IN
TERNAL_ERROR
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stre
am_executor\cuda\cuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_B
AD_PARAM
F c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core
\kernels\conv_ops.cc:532] Check failed: stream->parent()->GetConvolveAlgorithms(
&algorithms)
```

It seems the cuDNN version is not correct, but I tried cuDNN v5.0, also got this exception.

does anyone know how to fix this issue?
Thanks very much."
6659,Is it possible to implement Sparse Cross Entropy or Sparse Softmax Cross Entropy with Smooth Threshold to deal with large loss?,"I use `sparse_softmax_cross_entropy_with_logits` in Seq2Seq task with large vocabulary.
But sometime I got a large loss, such as 1000. It's too large for optimization. So I think to add a smooth threshold in sparse_softmax_cross_entropy_with_logits may deal with this problem.
```python
sm = tf.nn.softmax(logit)
sm = sm + threshold
ce = sparse_cross_entropy(sm, target)
```
BTW, I clip `logit` for smoothness now. But I'm not sure if it's a good idea.
```python
logit = clip(logit, -threshold, threshold)
```
Does anyone have better solution to deal with large loss or will implement Sparse Cross Entropy?
Thanks so much!

"
6656,"http://zlib.net/zlib-1.2.8.tar.gz is down, breaks configure","Need something like https://github.com/yaroslavvb/tensorflow/commit/fbc8bef52fc20d1c26290979f956e781bc863be8 to fix ./configure since ""http://zlib.net/zlib-1.2.8.tar.gz"" is down"
6654,min_pool,"There is only max_pool and avg_pool, but min_pool op is a common op in Application, I should always write it by  myself. 
Strongly suggest to add this feature."
6653,Initialize variable failure when importing graph_def,"In r0.12, I'm trying to write a graph_def at local and import it in another server. But when I `session.run(a)` after `session.run(global_variables_initializer())`, it raises a `FailedPreconditionError: Attempting to use uninitialized value a` .

### Write graph
```python
import tensorflow as tf

a = tf.Variable(1, name='a', dtype=tf.int32)
b = tf.Variable(2, name='b', dtype=tf.int32)
c = a + b
d = tf.Variable(c, name='d', dtype=tf.int32)
sess = tf.Session()
tf.train.write_graph(sess.graph, /tmp/PJT', 'test.pb', as_text=False)
```
### Import graph
```python
import tensorflow as tf
from tensorflow.core.framework import graph_pb2

graph_pb = '/home/mind/PJT/test.pb'
graph_def = graph_pb2.GraphDef()

with open(graph_pb, 'rb') as f:
    graph_def.ParseFromString(f.read())

tf.import_graph_def(graph_def, name='')
sess = tf.Session()
a = sess.graph.get_tensor_by_name('a:0')
sess.run(tf.global_variables_initializer())
sess.run(a)
```
### Error log
```
FailedPreconditionErrorTraceback (most recent call last)
<ipython-input-43-574ff7291e3a> in <module>()
      2 a = sess.graph.get_tensor_by_name('a:0')
      3 sess.run(tf.global_variables_initializer())
----> 4 sess.run(a)

/home/mind/anaconda2/envs/tf_0.12/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)
    764     try:
    765       result = self._run(None, fetches, feed_dict, options_ptr,
--> 766                          run_metadata_ptr)
    767       if run_metadata:
    768         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/home/mind/anaconda2/envs/tf_0.12/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)
    962     if final_fetches or final_targets:
    963       results = self._do_run(handle, final_targets, final_fetches,
--> 964                              feed_dict_string, options, run_metadata)
    965     else:
    966       results = []

/home/mind/anaconda2/envs/tf_0.12/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1012     if handle is None:
   1013       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,
-> 1014                            target_list, options, run_metadata)
   1015     else:
   1016       return self._do_call(_prun_fn, self._session, handle, feed_dict,

/home/mind/anaconda2/envs/tf_0.12/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)
   1032         except KeyError:
   1033           pass
-> 1034       raise type(e)(node_def, op, message)
   1035 
   1036   def _extend_graph(self):

FailedPreconditionError: Attempting to use uninitialized value a
	 [[Node: _send_a_0 = _Send[T=DT_INT32, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=784291433964173963, tensor_name=""a:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](a)]]
```

@mrry Could you have a look?
"
6652,https://www.tensorflow.org/robots.txt is disallowing all Search Engines crawling www.tensorflow.org.,"[https://www.tensorflow.org/robots.txt] 
User-agent: *
Disallow: /

Can you please allow Search Engines ?

Thanks,
Fabrice Canel
Microsoft Bing"
6650,`histogram_fixed_width` type errors and GPU issues,"With the following script:

```python
import tensorflow as tf

with tf.device('/gpu:0'):
    image = tf.zeros([200, 200, 3], dtype=tf.int32)
    hist_range = tf.constant([0, 255], dtype=tf.int32)
    hist = histogram_fixed_width(image, hist_range, nbins=16, dtype=tf.int32)

with tf.Session() as sess:
    sess.run(hist)
```

Gives the following error:

```
Traceback (most recent call last):
  File ""hist.py"", line 5, in <module>
    hist = tf.histogram_fixed_width(image, hist_range, nbins=16, dtype=tf.int32)
  File ""/home/wcrichto/.env/local/lib/python2.7/site-packages/tensorflow/python/ops/histogram_ops.py"", line 84, in histogram_fixed_width
    indices = math_ops.floor(nbins_float * scaled_values, name='indices')
  File ""/home/wcrichto/.env/local/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py"", line 805, in binary_op_wrapper
    y = ops.convert_to_tensor(y, dtype=x.dtype.base_dtype, name=""y"")
  File ""/home/wcrichto/.env/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 651, in convert_to_tensor
    as_ref=False)
  File ""/home/wcrichto/.env/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 716, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/home/wcrichto/.env/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 589, in _TensorTensorConversionFunction
    % (dtype.name, t.dtype.name, str(t)))
ValueError: Tensor conversion requested dtype float32 for Tensor with dtype float64: 'Tensor(""histogram_fixed_width/scaled_values:0"", shape=(120000,), dtype=float64)'
```

I believe the issue here is this line: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/histogram_ops.py#L75

It should say `to_double` instead of `to_float`. However, even if you fix this, you then get this error: 

```
Caused by op u'histogram_fixed_width', defined at:
  File ""hist.py"", line 79, in <module>
    hist = histogram_fixed_width(image, hist_range, nbins=16, dtype=tf.int32)
  File ""hist.py"", line 74, in histogram_fixed_width
    name=scope)
  File ""/home/wcrichto/.env/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 2952, in unsorted_segment_sum
    num_segments=num_segments, name=name)
  File ""/home/wcrichto/.env/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 763, in apply_op
    op_def=op_def)
  File ""/home/wcrichto/.env/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2392, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/wcrichto/.env/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1264, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): Cannot assign a device to node 'histogram_fixed_width': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
	 [[Node: histogram_fixed_width = UnsortedSegmentSum[T=DT_INT32, Tindices=DT_INT32, _device=""/device:GPU:0""](histogram_fixed_width/ones_like, histogram_fixed_width/Cast, histogram_fixed_width/nbins)]]
```

I ran this both with the TF version from `pip install tensorflow-gpu` as well as when building from source with `--config=cuda` and got this error. However, the source makes it seem like this should work: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/segment_reduction_ops.cc#L323
"
6649,"v0.12 outputs worse result than v0.11, running same code of lstm",
6648,wide_n_deep tutorial issues,"Please help. Running into an issue running the wide_n_deep.py tutorial.

**Tensorflow Version:**
$ python -c 'import tensorflow as tf; print(tf.__version__)'  # for Python 2
=> 0.12.1

**wide_n_deep_tutorial.py was downloaded from here:** https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/examples/learn/wide_n_deep_tutorial.py)

**Command and output**
$ python wide_n_deep_tutorial.py --model_type=wide
=>
```
Training data is downloaded to /var/folders/7y/s1y_wxt93xj5l784mcn2sf380000gn/T/tmpvWXvU2
Test data is downloaded to /var/folders/7y/s1y_wxt93xj5l784mcn2sf380000gn/T/tmpL4LH0x
model directory = /var/folders/7y/s1y_wxt93xj5l784mcn2sf380000gn/T/tmpFbBGZe
WARNING:tensorflow:The default value of combiner will change from ""sum"" to ""sqrtn"" after 2016/11/01.
WARNING:tensorflow:The default value of combiner will change from ""sum"" to ""sqrtn"" after 2016/11/01.
WARNING:tensorflow:The default value of combiner will change from ""sum"" to ""sqrtn"" after 2016/11/01.
WARNING:tensorflow:The default value of combiner will change from ""sum"" to ""sqrtn"" after 2016/11/01.
WARNING:tensorflow:The default value of combiner will change from ""sum"" to ""sqrtn"" after 2016/11/01.
WARNING:tensorflow:The default value of combiner will change from ""sum"" to ""sqrtn"" after 2016/11/01.
WARNING:tensorflow:The default value of combiner will change from ""sum"" to ""sqrtn"" after 2016/11/01.
WARNING:tensorflow:The default value of combiner will change from ""sum"" to ""sqrtn"" after 2016/11/01.
WARNING:tensorflow:The default value of combiner will change from ""sum"" to ""sqrtn"" after 2016/11/01.
WARNING:tensorflow:The default value of combiner will change from ""mean"" to ""sqrtn"" after 2016/11/01.
WARNING:tensorflow:The default value of combiner will change from ""mean"" to ""sqrtn"" after 2016/11/01.
WARNING:tensorflow:The default value of combiner will change from ""mean"" to ""sqrtn"" after 2016/11/01.
WARNING:tensorflow:The default value of combiner will change from ""mean"" to ""sqrtn"" after 2016/11/01.
WARNING:tensorflow:The default value of combiner will change from ""mean"" to ""sqrtn"" after 2016/11/01.
WARNING:tensorflow:The default value of combiner will change from ""mean"" to ""sqrtn"" after 2016/11/01.
WARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/linear.py:446 in fit.: calling fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.
Instructions for updating:
Estimator is decoupled from Scikit Learn interface by moving into
separate class SKCompat. Arguments x, y and batch_size are only
available in the SKCompat class, Estimator will only accept input_fn.
Example conversion:
  est = Estimator(...) -> est = SKCompat(Estimator(...))
WARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/linear.py:446 in fit.: calling fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.
Instructions for updating:
Estimator is decoupled from Scikit Learn interface by moving into
separate class SKCompat. Arguments x, y and batch_size are only
available in the SKCompat class, Estimator will only accept input_fn.
Example conversion:
  est = Estimator(...) -> est = SKCompat(Estimator(...))
WARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/linear.py:446 in fit.: calling fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with batch_size is deprecated and will be removed after 2016-12-01.
Instructions for updating:
Estimator is decoupled from Scikit Learn interface by moving into
separate class SKCompat. Arguments x, y and batch_size are only
available in the SKCompat class, Estimator will only accept input_fn.
Example conversion:
  est = Estimator(...) -> est = SKCompat(Estimator(...))
Traceback (most recent call last):
  File ""wide_n_deep_tutorial.py"", line 208, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 43, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""wide_n_deep_tutorial.py"", line 204, in main
    train_and_eval()
  File ""wide_n_deep_tutorial.py"", line 197, in train_and_eval
    m.fit(input_fn=lambda: input_fn(df_train), steps=FLAGS.train_steps)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/linear.py"", line 446, in fit
    max_steps=max_steps)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 191, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 355, in fit
    max_steps=max_steps)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 690, in _train_model
    features, labels = input_fn()
  File ""wide_n_deep_tutorial.py"", line 197, in <lambda>
    m.fit(input_fn=lambda: input_fn(df_train), steps=FLAGS.train_steps)
  File ""wide_n_deep_tutorial.py"", line 159, in input_fn
    for k in CATEGORICAL_COLUMNS}
  File ""wide_n_deep_tutorial.py"", line 159, in <dictcomp>
    for k in CATEGORICAL_COLUMNS}
TypeError: __init__() got an unexpected keyword argument 'dense_shape'
```

**Running on Mac Sierra**"
6647,TypeError: Cannot create initializer for non-floating point type.,"### Environment info
Operating System:
tensorflow docker(ubuntu 14.04)

### Version
tensorflow (0.12.1)/ tensorflow-gpu (0.12.1)

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

From official example:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/text_classification_character_rnn.py

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).

# tensorflow-gpu (0.12.1)

default dataset  'dbpedia' and my data set both get
```
WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpUlkI0V
INFO:tensorflow:Using default config.
INFO:tensorflow:Using config: {'save_summary_steps': 100, '_num_ps_replicas': 0, '_task_type': None, '_environment': 'local', '_is_chief': True, 'save_checkpoints_secs': 600, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f26f2317750>, 'tf_config': gpu_options {
  per_process_gpu_memory_fraction: 1
}
, '_task_id': 0, 'tf_random_seed': None, 'keep_checkpoint_every_n_hours': 10000, '_evaluation_master': '', 'save_checkpoints_steps': None, '_master': '', 'keep_checkpoint_max': 5}
WARNING:tensorflow:From <ipython-input-1-f55d1ba92119>:85 in main.: calling fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.
Instructions for updating:
Estimator is decoupled from Scikit Learn interface by moving into
separate class SKCompat. Arguments x, y and batch_size are only
available in the SKCompat class, Estimator will only accept input_fn.
Example conversion:
  est = Estimator(...) -> est = SKCompat(Estimator(...))
WARNING:tensorflow:From <ipython-input-1-f55d1ba92119>:85 in main.: calling fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.
Instructions for updating:
Estimator is decoupled from Scikit Learn interface by moving into
separate class SKCompat. Arguments x, y and batch_size are only
available in the SKCompat class, Estimator will only accept input_fn.
Example conversion:
  est = Estimator(...) -> est = SKCompat(Estimator(...))


TypeErrorTraceback (most recent call last)
<ipython-input-1-f55d1ba92119> in <module>()
     93 
     94 
---> 95 tf.app.run(main=main)

/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.pyc in run(main, argv)
     41   # Call the main function, passing through any arguments
     42   # to the final program.
---> 43   sys.exit(main(sys.argv[:1] + flags_passthrough))

<ipython-input-1-f55d1ba92119> in main(unused_argv)
     83 
     84   # Train and predict
---> 85   classifier.fit(x_train, y_train, steps=100)
     86   y_predicted = [
     87       p['class'] for p in classifier.predict(

/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.pyc in new_func(*args, **kwargs)
    189             _call_location(), decorator_utils.get_qualified_name(func),
    190             func.__module__, arg_name, date, instructions)
--> 191       return func(*args, **kwargs)
    192     new_func.__doc__ = _add_deprecated_arg_notice_to_docstring(
    193         func.__doc__, date, instructions)

/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in fit(self, x, y, input_fn, steps, batch_size, monitors, max_steps)
    353                              steps=steps,
    354                              monitors=monitors,
--> 355                              max_steps=max_steps)
    356     logging.info('Loss for final step: %s.', loss)
    357     return self

/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _train_model(self, input_fn, steps, feed_fn, init_op, init_feed_fn, init_fn, device_fn, monitors, log_every_steps, fail_on_nan_loss, max_steps)
    697       # cases, but will soon be deleted after the subclasses are updated.
    698       # TODO(b/32664904): Update subclasses and delete the else-statement.
--> 699       train_ops = self._get_train_ops(features, labels)
    700       if isinstance(train_ops, model_fn_lib.ModelFnOps):  # Default signature
    701         train_op = train_ops.train_op

/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _get_train_ops(self, features, labels)
   1050       `ModelFnOps` object.
   1051     """"""
-> 1052     return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)
   1053 
   1054   def _get_eval_ops(self, features, labels, metrics):

/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _call_model_fn(self, features, labels, mode)
   1021         model_fn_results = self._model_fn(features, labels, mode=mode)
   1022     else:
-> 1023       model_fn_results = self._model_fn(features, labels)
   1024 
   1025     if isinstance(model_fn_results, model_fn_lib.ModelFnOps):

<ipython-input-1-f55d1ba92119> in char_cnn_model(features, target)
     31     # Apply Convolution filtering on input sequence.
     32     conv1 = tf.contrib.layers.convolution2d(
---> 33         byte_list, N_FILTERS, FILTER_SHAPE1, padding='VALID')
     34     # Add a RELU for non linearity.
     35     conv1 = tf.nn.relu(conv1)

/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.pyc in func_with_args(*args, **kwargs)
    175       current_args = current_scope[key_func].copy()
    176       current_args.update(kwargs)
--> 177     return func(*args, **current_args)
    178   _add_op(func)
    179   setattr(func_with_args, '_key_op', _key_op(func))

/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.pyc in convolution(inputs, num_outputs, kernel_size, stride, padding, data_format, rate, activation_fn, normalizer_fn, normalizer_params, weights_initializer, weights_regularizer, biases_initializer, biases_regularizer, reuse, variables_collections, outputs_collections, trainable, scope)
    838                                        regularizer=weights_regularizer,
    839                                        collections=weights_collections,
--> 840                                        trainable=trainable)
    841     outputs = nn.convolution(input=inputs,
    842                              filter=weights,

/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.pyc in func_with_args(*args, **kwargs)
    175       current_args = current_scope[key_func].copy()
    176       current_args.update(kwargs)
--> 177     return func(*args, **current_args)
    178   _add_op(func)
    179   setattr(func_with_args, '_key_op', _key_op(func))

/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/variables.pyc in model_variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, device)
    242                   initializer=initializer, regularizer=regularizer,
    243                   trainable=trainable, collections=collections,
--> 244                   caching_device=caching_device, device=device)
    245 
    246 

/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.pyc in func_with_args(*args, **kwargs)
    175       current_args = current_scope[key_func].copy()
    176       current_args.update(kwargs)
--> 177     return func(*args, **current_args)
    178   _add_op(func)
    179   setattr(func_with_args, '_key_op', _key_op(func))

/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/variables.pyc in variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, device)
    206                                        trainable=trainable,
    207                                        collections=collections,
--> 208                                        caching_device=caching_device)
    209 
    210 

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc in get_variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)
   1022       collections=collections, caching_device=caching_device,
   1023       partitioner=partitioner, validate_shape=validate_shape,
-> 1024       custom_getter=custom_getter)
   1025 
   1026 

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc in get_variable(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)
    848           collections=collections, caching_device=caching_device,
    849           partitioner=partitioner, validate_shape=validate_shape,
--> 850           custom_getter=custom_getter)
    851 
    852   def _get_partitioned_variable(self,

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc in get_variable(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)
    344           reuse=reuse, trainable=trainable, collections=collections,
    345           caching_device=caching_device, partitioner=partitioner,
--> 346           validate_shape=validate_shape)
    347 
    348   def _get_partitioned_variable(

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc in _true_getter(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape)
    329           initializer=initializer, regularizer=regularizer, reuse=reuse,
    330           trainable=trainable, collections=collections,
--> 331           caching_device=caching_device, validate_shape=validate_shape)
    332 
    333     if custom_getter is not None:

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc in _get_single_variable(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape)
    675         dtype=variable_dtype,
    676         validate_shape=validate_shape,
--> 677         expected_shape=shape)
    678     self._vars[name] = v
    679     logging.vlog(1, ""Created variable %s with shape %s and init %s"", v.name,

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.pyc in __init__(self, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope)
    222           name=name,
    223           dtype=dtype,
--> 224           expected_shape=expected_shape)
    225 
    226   def __str__(self):

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.pyc in _init_from_args(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, expected_shape)
    325               # with the variable itself.
    326               self._initial_value = ops.convert_to_tensor(
--> 327                   initial_value(), name=""initial_value"", dtype=dtype)
    328               assert_expected_shape()
    329 

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc in <lambda>()
    663       else:
    664         init_val = lambda: initializer(
--> 665             shape.as_list(), dtype=dtype, partition_info=partition_info)
    666         variable_dtype = dtype.base_dtype
    667 

/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/initializers.pyc in _initializer(shape, dtype, partition_info)
    118     """"""Initializer function.""""""
    119     if not dtype.is_floating:
--> 120       raise TypeError('Cannot create initializer for non-floating point type.')
    121     # Estimating fan_in and fan_out is not possible to do perfectly, but we try.
    122     # This is the right thing for matrix multiply and convolutions.

TypeError: Cannot create initializer for non-floating point type.

```
"
6646,A suggested improvement for tf.nn.embedding_lookup_sparse() (with code),"The sp_ids and sp_weights parameters in [embedding_lookup_sparse()](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/embedding_ops.py#L203) can be merged into a single parameter sp_mat which has clearer semantics. In sp_ids and sp_weights, cols are not utilized. However, in sp_mat, we will use row as instance, col as embedding ids, and its entry/value as weight. This makes better sense and reduce parameters for the function. I have written up the function attached below (slightly modified from original code), and also provided some test cases. Hope you may consider it.

```
def embedding_lookup_sparse(params, sp_mat,
                            partition_strategy=""mod"",
                            name=None,
                            combiner=None,
                            max_norm=None):
  """"""Computes embeddings for the given ids and weights.

  This op assumes that there is at least one id for each row in the dense tensor
  represented by sp_mat (i.e. there are no rows with empty features, if so, 
  put 0.0 in sp_mat entry), and that all the indices of sp_mat are in
  canonical row-major order.

  It also assumes that all id values lie in the range [0, p0), where p0
  is the sum of the size of params along dimension 0.

  Args:
    params: A single tensor representing the complete embedding tensor,
      or a list of P tensors all of same shape except for the first dimension,
      representing sharded embedding tensors.  Alternatively, a
      `PartitionedVariable`, created by partitioning along dimension 0. Each
      element must be appropriately sized for the given `partition_strategy`.
    sp_mat: N x M SparseTensor of zero or non-zero weights, 
      where N is typically batch size and M is the embedding table size.
    partition_strategy: A string specifying the partitioning strategy, relevant
      if `len(params) > 1`. Currently `""div""` and `""mod""` are supported. Default
      is `""mod""`. See `tf.nn.embedding_lookup` for more details.
    name: Optional name for the op.
    combiner: A string specifying the reduction op. Currently ""mean"", ""sqrtn""
      and ""sum"" are supported.
      ""sum"" computes the weighted sum of the embedding results for each row.
      ""mean"" is the weighted sum divided by the total weight.
      ""sqrtn"" is the weighted sum divided by the square root of the sum of the
      squares of the weights.
    max_norm: If not None, each embedding is normalized to have l2 norm equal
      to max_norm before combining.

  Returns:
    A dense tensor representing the combined embeddings for the sparse ids. 
    For each row in the dense tensor represented by sp_mat, the op looks up 
    the embeddings for all (non-zero) ids in that row, multiplies them by the
    corresponding weight, and combines these embeddings as specified.

    In other words, if

      shape(combined params) = [p0, p1, ..., pm]

    and

      shape(sp_mat) = [d0, d1, ..., dn]

    then

      shape(output) = [d0, d1, ..., dn-1, p1, ..., pm].

    For instance, if params is a 10x20 matrix, and sp_mat is

      [0, 0]: 1.0
      [0, 1]: 3.0
      [1, 0]: 0.0
      [2, 3]: 1.0

    with `combiner`=""mean"", then the output will be a 3x20 matrix where

      output[0, :] = (params[0, :] * 1.0 + params[1, :] * 3.0) / (1.0 + 3.0)
      output[1, :] = params[0, :] * 0.0 / div_protect
      output[2, :] = params[3, :] * 1.0 / 1.0

  Raises:
    TypeError: If sp_mat is not a SparseTensor.
    ValueError: If combiner is not one of {""mean"", ""sqrtn"", ""sum""}.
  """"""
  if combiner is None:
    logging.warn(""The default value of combiner will change from \""mean\"" ""
                 ""to \""sqrtn\"" after 2016/11/01."")
    combiner = ""mean""
  if combiner not in (""mean"", ""sqrtn"", ""sum""):
    raise ValueError(""combiner must be one of 'mean', 'sqrtn' or 'sum'"")
  if isinstance(params, variables.PartitionedVariable):
    params = list(params)  # Iterate to get the underlying Variables.
  if not isinstance(params, list):
    params = [params]
  if not isinstance(sp_mat, sparse_tensor.SparseTensor):
    raise TypeError(""sp_mat must be SparseTensor"")

  with ops.name_scope(name, ""embedding_lookup_sparse"",
                      params + [sp_mat]) as name:
    segment_ids = sp_mat.indices[:, 0]
    if segment_ids.dtype != dtypes.int32:
      segment_ids = math_ops.cast(segment_ids, dtypes.int32)

    ids = sp_mat.indices[:, 1]

    embeddings = embedding_lookup(
        params, ids, partition_strategy=partition_strategy, max_norm=max_norm)

    weights = sp_mat.values
    if weights.dtype != embeddings.dtype:
      weights = math_ops.cast(weights, embeddings.dtype)

    # Reshape weights to allow broadcast
    ones = array_ops.fill(
        array_ops.expand_dims(array_ops.rank(embeddings) - 1, 0), 1)
    bcast_weights_shape = array_ops.concat_v2(
        [array_ops.shape(weights), ones], 0)

    orig_weights_shape = weights.get_shape()
    weights = array_ops.reshape(weights, bcast_weights_shape)

    # Set the weight shape, since after reshaping to bcast_weights_shape,
    # the shape becomes None.
    if embeddings.get_shape().ndims is not None:
      weights.set_shape(orig_weights_shape.concatenate(
          [1 for _ in range(embeddings.get_shape().ndims - 1)]))

    embeddings *= weights

    div_protect = 1e-32  # would not work for float16 or float8
    if combiner == ""sum"":
      embeddings = math_ops.segment_sum(embeddings, segment_ids, name=name)
    elif combiner == ""mean"":
      embeddings = math_ops.segment_sum(embeddings, segment_ids)
      weight_sum = math_ops.segment_sum(weights, segment_ids)
      embeddings = math_ops.div(embeddings, weight_sum + div_protect, name=name)
    elif combiner == ""sqrtn"":
      embeddings = math_ops.segment_sum(embeddings, segment_ids)
      weights_squared = math_ops.pow(weights, 2)
      weight_sum = math_ops.segment_sum(weights_squared, segment_ids)
      weight_sum_sqrt = math_ops.sqrt(weight_sum)
      embeddings = math_ops.div(embeddings, weight_sum_sqrt + div_protect, name=name)
    else:
      assert False, ""Unrecognized combiner""

    return embeddings
```

Test cases

```
sp_mat = [np.array([(0, 0), (0,1), (1, 0), (2, 1), (3, 0), (3, 1)]), np.array((0.5, 0.5, 1, 1, 0, 0)), (-1, -1)]
with tf.Graph().as_default():
    with tf.Session() as sess:
        idx = tf.sparse_placeholder(dtype=tf.float32)
        emb = tf.Variable(initial_value=np.random.random((100, 2)).astype('float32'))
        y = embedding_lookup_sparse(emb, idx, combiner='mean')
        sess.run(tf.global_variables_initializer())
        result = sess.run([y], feed_dict={idx: sp_mat})[0]

assert (result[0] == (result[1] + result[2]) / 2).all()
assert (result[3] == np.array([0, 0])).all()
```
"
6645,Error: Found more than one graph event per run,"Hello,

I am trying to debug an issue that I am having with a model. When I decide to kill the training process, I remove the tensorboard logs, and fix an issue. Afterwards I restart the process.

Slowly, I seem to have accumulated tensorboard graphs, because I get the following Warning (multiple times)

WARNING:tensorflow:Found more than one metagraph event per run. Overwriting the metagraph with the newest event.

WARNING:tensorflow:Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event

Any idea how to resolve this issue?"
6644,Error: Data loss: file is too short to be an sstable,"Hi there,

I'm training the Language Model code available here: https://github.com/rafaljozefowicz/lm

I am getting a ""Data loss"" error, see below for a traceback call. I was able to find the file where the error is raised, but I am having some trouble understanding exactly what is going wrong. It seems like this is related to a checkpoint file need to populate tensorboard, however I would greatly appreciate it if someone could point me in the right direction, so I may resolve this issue.

Currently, when I look at TensorBoard, I see a global_step of 0 for the entire training duration, and I'm worried that the model is not running. All of my GPUs are fully utilized, and I have no other issues. 

```
W tensorflow/core/framework/op_kernel.cc:975] Data loss: file is too short to be an sstable
Traceback (most recent call last):
  File ""single_lm_train.py"", line 38, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 43, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""single_lm_train.py"", line 34, in main
    run_eval(dataset, hps, FLAGS.logdir, FLAGS.mode, FLAGS.eval_steps)
  File ""/shareddata/s5kbjt/NLM_RNN/lm/run_utils.py"", line 111, in run_eval
    while ckpt_loader.load_checkpoint():
  File ""/shareddata/s5kbjt/NLM_RNN/lm/common.py"", line 45, in load_checkpoint
    if load_from_checkpoint(self.saver, self.logdir):
  File ""/shareddata/s5kbjt/NLM_RNN/lm/common.py"", line 27, in load_from_checkpoint
    saver.restore(sess, ckpt.model_checkpoint_path)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1388, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 766, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 964, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1014, in _do_run
    target_list, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1034, in _do_call
    raise type(e)(node_def, op, message)

tensorflow.python.framework.errors_impl.DataLossError: file is too short to be an sstable
         [[Node: save/RestoreV2_15 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_recv_save/Const_0, save/RestoreV2_15/tensor_names, save/RestoreV2_15/shape_and_slices)]]
Caused by op u'save/RestoreV2_15', defined at:  File ""single_lm_train.py"", line 38, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 43, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""single_lm_train.py"", line 34, in main
    run_eval(dataset, hps, FLAGS.logdir, FLAGS.mode, FLAGS.eval_steps)
  File ""/shareddata/s5kbjt/NLM_RNN/lm/run_utils.py"", line 98, in run_eval
    saver = tf.train.Saver(model.avg_dict)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1000, in __init__
    self.build()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1030, in build
    restore_sequentially=self._restore_sequentially)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 624, in build
    restore_sequentially, reshape)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 361, in _AddRestoreOps
    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 200, in restore_op
    [spec.tensor.dtype])[0])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 441, in restore_v2
    dtypes=dtypes, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 759, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2240, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1128, in __init__
    self._traceback = _extract_stack()

DataLossError (see above for traceback): file is too short to be an sstable
         [[Node: save/RestoreV2_15 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/repl
ica:0/task:0/cpu:0""](_recv_save/Const_0, save/RestoreV2_15/tensor_names, save/RestoreV2_15/sha
pe_and_slices)]]

```"
6643,Build works; import does not,"The latest build completes, but fails with an import error:  ImportError: No module named losses_impl

the complete message is here

[melrobin@scorpion ~]$ python
Python 2.7.11 (default, Sep 29 2016, 13:33:00) 
[GCC 5.3.1 20160406 (Red Hat 5.3.1-6)] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcurand.so.8.0 locally
W tensorflow/core/platform/cpu_feature_guard.cc:95] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:95] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:95] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:95] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/lib/python2.7/site-packages/tensorflow-0.12.1-py2.7-linux-x86_64.egg/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/usr/lib/python2.7/site-packages/tensorflow-0.12.1-py2.7-linux-x86_64.egg/tensorflow/python/__init__.py"", line 102, in <module>
    from tensorflow.python.ops.losses import losses
  File ""/usr/lib/python2.7/site-packages/tensorflow-0.12.1-py2.7-linux-x86_64.egg/tensorflow/python/ops/losses/losses.py"", line 40, in <module>
    from tensorflow.python.ops.losses.losses_impl import *
ImportError: No module named losses_impl"
6642,Tensorboard doesn't show anything in 0.12,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
I was having IOErrors but after trying in different browsers. After reading issue [#4830](https://github.com/tensorflow/tensorflow/issues/4830), I decided to upgrade tensorflow to 0.12, even when it says the 0.11 is fixed.


A [link](https://www.tensorflow.org/get_started/os_setup#pip_installation) to the pip package I installed: tensorflow-0.12.1-cp27-none-linux_x86_64.whl

### Minimal reproducible example
So, the IOErrors did dissapeared but I stil can't show any graph or scalar or summary. After following the debug everything seems fine:

```
INFO:tensorflow:TensorBoard is in debug mode.
INFO:tensorflow:Starting TensorBoard in directory /home/yunli/Documents/Research/articles/australian language/tensorFlow_use
INFO:tensorflow:TensorBoard path_to_run is: {'/home/Documents/Research/articles/australian language/tensorFlow_use/logs': None}
INFO:tensorflow:Event Multiplexer initializing.
INFO:tensorflow:Event Multiplexer done initializing
INFO:tensorflow:TensorBoard reload process beginning
INFO:tensorflow:Starting AddRunsFromDirectory: /home/Documents/Research/articles/australian language/tensorFlow_use/logs
INFO:tensorflow:Done with AddRunsFromDirectory: /home/Documents/Research/articles/australian language/tensorFlow_use/logs
INFO:tensorflow:TensorBoard reload process: Reload the whole Multiplexer
INFO:tensorflow:Beginning EventMultiplexer.Reload()
INFO:tensorflow:Finished with EventMultiplexer.Reload()
INFO:tensorflow:TensorBoard done reloading. Load took 0.001 secs
INFO:tensorflow:TensorBoard is tag: 39
Starting TensorBoard 39 on port 6006
(You can navigate to http://127.0.1.1:6006)
```
However, I try again all the browsers (Chrome, Firefox and Chromium) and nothing appear. I'll provide one of the event files I [got](https://drive.google.com/file/d/0B_CAQhE3IU1Vc1Z0SDdoa3VXdEE/view?usp=sharing).




















"
6640,not registered 'TensorArrayV2' on 12.1 on MacOS CPU,"I'm getting
`tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'TensorArrayV2'
` after upgrading to 0.12.1 on MacOS. Same code works on 0.11 on Mac and 0.12.1 on Ubuntu CPU

This op seems to be created by dynamic RNN code

```
node {
  name: ""global/RNN/TensorArray_1""
  op: ""TensorArrayV2""
  input: ""global/RNN/strided_slice_2""
  device: ""/cpu:0""
  attr {
    key: ""clear_after_read""
    value {
      b: true
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""dynamic_size""
    value {
      b: false
    }
  }
  attr {
    key: ""tensor_array_name""
    value {
      s: ""global/RNN/dynamic_rnn/input_0""
    }
  }


```
Sorry don't have a shorter repro atm, but a long repro is to follow install instructions on https://github.com/openai/universe-starter-agent and run it with Mac CPU tensorflow        

```
export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.12.1-py3-none-any.whl
pip install -I --upgrade setuptools
pip install --upgrade $TF_BINARY_URL
python train.py --num-workers 2 --env-id PongDeterministic-v3 --log-dir /tmp/pong
```

"
6639,Apply TensorFlow on ARFF,"I have ARFF file or excel sheet with binary features(0,1) and class label also (0,1) can you help me to apply convolution neural network on my file to make classification with TensorFlow .
Thanks "
6636,Tensorboard does not show any data,"On latest git head, when running the mnist_with_summaries.py, tensorboard does not show any data:

![distributions](https://cloud.githubusercontent.com/assets/5008257/21643171/2f91f656-d287-11e6-8e83-d9af45e7c978.png)
![histogram](https://cloud.githubusercontent.com/assets/5008257/21643170/2f91d996-d287-11e6-99e3-f2371a84f448.png)
![images](https://cloud.githubusercontent.com/assets/5008257/21643173/2f924f0c-d287-11e6-911e-34a7dc44a00a.png)
![scalars](https://cloud.githubusercontent.com/assets/5008257/21643172/2f91f16a-d287-11e6-9734-c258b9973832.png)

you can find the generated events here:
[events.zip](https://github.com/tensorflow/tensorflow/files/684693/events.zip)


Operating System: Linux / x86_64

**$ git rev-parse HEAD**
7c36309c37b04843030664cdc64aca2bb7d6ecaa

**$ bazel version**
Build label: 0.4.3
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Dec 22 12:31:25 2016 (1482409885)
Build timestamp: 1482409885
Build timestamp as int: 1482409885

the mnist_with_summaries.py:
./tensorflow/examples/tutorials/mnist/mnist_with_summaries.py

The --inspect output seems to find the data:
**$ tensorboard --logdir /tmp/tensorflow/mnist/logs/mnist_with_summaries/train/ --inspect**
[inspect.txt](https://github.com/tensorflow/tensorflow/files/684700/inspect.txt)
"
6635,Incorrect gradient when using tf.dynamic_stitch and tf.gather?,"In Tensorflow 0.12, I find the discrepancy of gradients in two mathematically equivalent training procedures of LSTM, probably due to the use of tf.gather and tf.dynamic_stitch.  One is the normal procedure using the whole batch of training examples to unroll the LSTM in each step. The other first uses tf.gather to select ALL the examples of the whole batch in each step, then unroll the LSTM with those examples and finally use tf.dynamic_stitch to update the corresponding states and outputs.

These two procedures should be equivalent as they both essentially use the whole batch. However, the gradients of the same variables are significantly different.

The code is as follows (the core parts are essentially `# 1.` and  `# 2.`):

```python
batch_size = 2
num_timesteps = 10
vocab_size = 10
num_embedding_nodes = 32
hidden_size = 128
n_class = 2
learning_rate = 0.001
inputs = tf.placeholder(tf.int64, [batch_size, num_timesteps])
targets = tf.placeholder(tf.int64, [batch_size])
embedding = tf.get_variable(""embedding"", [vocab_size, num_embedding_nodes])
x = tf.nn.embedding_lookup(embedding, inputs)
w_predict = tf.get_variable(""w_predict"", [hidden_size, n_class])
b_predict = tf.get_variable(""b_predict"", [n_class])

with tf.variable_scope('lstm') as lstm_scope:
  cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0, state_is_tuple=False)
  state = cell.zero_state(batch_size, dtype=tf.float32)
  state1 = cell.zero_state(batch_size, dtype=tf.float32)
  for t in range(num_timesteps):
    if t == 0:
      output, state = cell(x[:, t, :], state)
      lstm_scope.reuse_variables()
      output1, state1 = cell(x[:, t, :], state1)
    else:
      lstm_scope.reuse_variables()
      # 1. normal lstm 
      output, state = cell(x[:, t, :], state)
      # 2. lstm using tf.gather and tf.dynamic_stitch to select all samples from batch
      idx_select = tf.range(batch_size)
      tmp_output, tmp_state = cell(tf.gather(x[:, t, :], idx_select), tf.gather(state1, idx_select))
      output1 = tf.dynamic_stitch([tf.range(batch_size), idx_select], [output1, tmp_output])
      state1 = tf.dynamic_stitch([tf.range(batch_size), idx_select], [state1, tmp_state])
logits = tf.nn.xw_plus_b(output, w_predict, b_predict)
cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, targets, name=None)
logits1 = tf.nn.xw_plus_b(output1, w_predict, b_predict)
cross_entropy1 = tf.nn.sparse_softmax_cross_entropy_with_logits(logits1, targets, name=None)
trainable = tf.trainable_variables()
grad = tf.gradients(cross_entropy, trainable)
grad1 = tf.gradients(cross_entropy1, trainable)

######## gradient log
cg = []
for g in grad:
  if g is not None:
    cg += [g]
cg1 = []
for g in grad1:
  if g is not None:
    cg1 += [g]

optimizer = tf.train.AdamOptimizer(learning_rate)
train_op = optimizer.apply_gradients(zip(grad, trainable))
init_op = tf.initialize_all_variables()


"""""" Training """"""
## arbitrary synthetic data
# we use two training examples, each with length 10
my_input = np.array([[3,4,6,8,3,5,8,9,2,4], [4,2,3,8,5,2,2,3,6,1]])
my_target = np.array([0,1])
sess = tf.Session()
sess.run(init_op)

epoch = 0
while epoch < 5:
  epoch += 1
  fetches = [train_op, cg, cg1]
  outputs = sess.run(fetches, feed_dict={inputs: my_input, targets: my_target})
  gradients = outputs[1] 
  gradients1 = outputs[2] 
  print 'epoch %d:' % epoch 
    for i, g in enumerate(gradients):
      if i > 0:
	print('norm of gradient of var %d: %f' % (i, LA.norm(gradients[i])))
	print('norm of gradient1 of var %d: %f' % (i, LA.norm(gradients1[i])))
```

The sample output is:
```
epoch 1:
norm of gradient of var 1: 0.644620
norm of gradient1 of var 1: 0.644620
norm of gradient of var 2: 0.393020
norm of gradient1 of var 2: 0.393020
norm of gradient of var 3: 0.838759
norm of gradient1 of var 3: 102.815338
norm of gradient of var 4: 0.435841
norm of gradient1 of var 4: 44.867126
epoch 2:
norm of gradient of var 1: 0.613848
norm of gradient1 of var 1: 0.611423
norm of gradient of var 2: 0.355387
norm of gradient1 of var 2: 0.351987
norm of gradient of var 3: 0.797761
norm of gradient1 of var 3: 96.391121
norm of gradient of var 4: 0.397020
norm of gradient1 of var 4: 39.937107
epoch 3:
norm of gradient of var 1: 0.603118
norm of gradient1 of var 1: 0.603118
norm of gradient of var 2: 0.318260
norm of gradient1 of var 2: 0.318260
norm of gradient of var 3: 0.773661
norm of gradient1 of var 3: 93.290131
norm of gradient of var 4: 0.366684
norm of gradient1 of var 4: 36.636879
epoch 4:
norm of gradient of var 1: 0.607643
norm of gradient1 of var 1: 0.607643
norm of gradient of var 2: 0.280101
norm of gradient1 of var 2: 0.280101
norm of gradient of var 3: 0.763007
norm of gradient1 of var 3: 92.295441
norm of gradient of var 4: 0.340769
norm of gradient1 of var 4: 33.630474
epoch 5:
norm of gradient of var 1: 0.622874
norm of gradient1 of var 1: 0.619443
norm of gradient of var 2: 0.239731
norm of gradient1 of var 2: 0.235509
norm of gradient of var 3: 0.757205
norm of gradient1 of var 3: 93.335388
norm of gradient of var 4: 0.312203
norm of gradient1 of var 4: 30.536301
```

We can see that the gradient and gradient1 of var3 have significantly different norms in every epoch, which should be the same. So is var4. Those two are the trainable variables of LSTM. In fact, if the sequence length is 50 instead of 10, the discrepancy is even much larger.

Could anybody tell me why it is the case?

### Environment info
Operating System: ubuntu 14.04
"
6634,Nestable custom_getters,"In TF 0.12 (and probably in earlier versions too), only one custom_getter is active at a time:

```python
import tensorflow as tf

def postfix_name(postfix):
    def custom_getter(getter, name, *args, **kwargs):
        return getter(""{}{}"".format(name, postfix), *args, **kwargs)
    return custom_getter

with tf.Graph().as_default():
    with tf.variable_scope(""A"", custom_getter=postfix_name(""_A"")):
        with tf.variable_scope(""B""):
            var1 = tf.get_variable(""var1"", [])
        with tf.variable_scope(""C"", custom_getter=postfix_name(""_C"")):
            var2 = tf.get_variable(""var2"", [])

# Current functionality:
assert tf.VERSION == '0.12.1'
assert var1.name == 'A/B/var1_A:0'
assert var2.name == 'A/C/var2_C:0'
```

This is surprising to the user – at least to me it was – and limits the usefulness of custom_getters.

I propose that nested custom_getters are applied recursively instead, like this:

```python
# Proposal:
assert var1.name == 'A/B/var1_A:0'
assert var2.name == 'A/C/var2_C_A:0'
```

This would be useful since we often want to apply different transformations to weights before they are used. Here's a pseudo-code of a real-life use case:

```python
with tf.variable_scope(""weight_normed"", custom_getter=weight_normed):
    with tf.variable_scope(""model"", custom_getter=track_variables):
        real_out = model(x)
    with tf.variable_scope(""model"", custom_getter=simulate_variables):
        simulated_out = model(x)
```"
6633,CudnnRnnSequenceTensorDescriptor should support different sequence lengths,"According to the cuDNN docs, the functions `cudnnRNNForwardInference` / `cudnnRNNForwardTraining` get the argument `cudnnTensorDescriptor_t* xDesc`, where:

xDesc: Array of tensor descriptors. Each must have the same second dimension. The first dimension may decrease from element n to element n + 1 but may not increase.

The usage of `xDesc` is a bit non-straight-forward. I wrote about that in more detail [here](http://stackoverflow.com/questions/41461670/cudnnrnnforwardtraining-seqlength-xdesc-usage).
According to a [comment in the CNTK code](https://github.com/Microsoft/CNTK/blob/7c5fb2d7d806148b5cbd795407f7c7b6a1a64520/Source/Math/CuDnnRNN.cpp) about the dimensions of each `xDesc[t]`:

> these dimensions are what CUDNN expects: (the minibatch dimension, the data dimension, and the number 1 (because each descriptor describes one frame of data)

TensorFlow sets the same minibatch dimension for each `xDesc[t]` in `CudnnRnnSequenceTensorDescriptor`:

    int dims[] = {batch_size, data_size, 1};
    int strides[] = {dims[1] * dims[2], dims[2], 1};
    status = dynload::cudnnSetTensorNdDescriptor(
        parent, handle /*tensorDesc*/, data_type /*dataType*/,
        sizeof(dims) / sizeof(dims[0]) /*nbDims*/, dims /*dimA*/,
        strides /*strideA*/);
    CUDNN_RETURN_IF_FAIL(status, ""Failed to update tensor descriptor"");
    // Replicate handle across the number of steps.
    handles_.assign(seq_length, handle);

Also `createRnnSequenceTensorDescriptor` needs a new API to allow for that.

And I'm not sure if there are ways to prepare the input `x` for `cudnnRNNForwardTraining` so that it has all sequences contiguously behind each other, and the sequences are sorted by sequences length.
Similar as [`PackSequencesForCuDNN` in CNTK](https://github.com/Microsoft/CNTK/blob/4472649412929543d4dfe553f50be5d9b3102521/Source/ComputationNetworkLib/RNNNodes.cpp#L265).
"
6631,tf-master-win-bzl is failing due to dependency on //tensorflow/contrib/framework:framework_py,"http://ci.tensorflow.org/job/tf-master-win-bzl/245/

Many targets, including `//tensorflow/contrib/framework:framework_py`, in `//tensorflow/contrib:contrib_py` currently don't build on Windows with Bazel, it is [excluded from `//tensorflow/python:python`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/BUILD#L72)

cc0de74ff0fbaee205826f7cef23e91892fb5db2 introduced the dependency on `//tensorflow/contrib/framework:framework_py` which caused the failure.
```
$ bazel query 'somepath(//tensorflow/tools/pip_package:build_pip_package, //tensorflow/contrib/framework:python/ops/_variable_ops.so)'
//tensorflow/tools/pip_package:build_pip_package
//tensorflow/examples/tutorials/mnist:package
//tensorflow/examples/tutorials/mnist:input_data
//tensorflow/contrib/learn/python/learn/datasets:datasets
//tensorflow/contrib/framework:framework_py
//tensorflow/contrib/framework:python/ops/_variable_ops.so
```
//cc @jart @gunan "
6630,[Docs] A mismatch bug in api_docs ,"There is a mismatch bug in api_docs about [tf.graph](https://www.tensorflow.org/api_docs/python/framework/core_graph_data_structures#Graph). It's easy to find that **tf.Graph.__init__() {:#Graph.init}** is a error. And I check the original script [ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/ops.py#L2003), it seems like the `{:#Graph.init}` is added by other script. 
At the same time, it causes the index `URL#Graph.init` ineffectively."
6629,Feature request: configuration files for tensorflow,"Matplotlib and theano provide [`matplotlibrc`](http://matplotlib.org/users/customizing.html) and [`theanorc`](http://deeplearning.net/software/theano/library/config.html) files to configure the libraries, respectively. It would be great if we could add such functionality to tensorflow. 

In particular, it would be great to 

* expose a `floatX` variable such that it is straightforward to switch between 16, 32, and 64 bit representations without having to modify code
* expose the [`config.proto`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/config.proto)

A JSON representation of the configuration may be more suitable than the formats used by matplotlib and theano because it can more easily capture the hierarchical structure of the configuration."
6627,TypeError: strided_slice() takes at least 4 arguments (3 given),"I have check the error.  And I find that the bug has been fixed in github code.
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py


lastest github version:
`def strided_slice(input_,
                  begin,
                  end,
                  strides=None,`

lastest whl version:
`def strided_slice(input_,
                  begin,
                  end,
                  strides,`

However tensorflow_gpu-0.12.1-cp27-none-linux_x86_64.whl doesn't  synchronize with github.

How can I upgrade tensorflow from lastest github code?"
6624,Fail with error (rather than hanging) when init_op requires starting queues (ManagedSesssion etc),"In 0.12 and in master

`MonitoredSession` wraps the original `SessionCreator` in a `_CoordinatedSessionCreator` in an attempt to ensure queue runners are started before the session is run, however the original session has to be created first (as it is an argument to start_queue_runner). In the case of `ChiefSessionCreator`, `ChiefSessionCreator.create_session` calls `ChiefSessionCreator._get_session_manager`
which instantiates a new `SessionManager` and returns it. Then `ChiefSessionCreator.create_session` calls `SessionManager.prepare_session` which calls `sess.run(init_op)`. Thus resulting in `sess.run(init_op)` being run before `start_queue_runner` is called. 

This makes it impossible to initialize variables from queues. Even if this is intended behavior, it causes the initialization of `MonitoredSession` to stall without any logging output, and not respond to SIGTERM.

It seems that `prepare_session` may need to be split into two calls for this use case. "
6623,Where is the skflow examples ??,"https://github.com/tensorflow/skflow/tree/master/examples

It says skflow examples has moved to https://github.com/tensorflow/skflow/tree/master/examples, but this  folder is empty."
6622,Minor doc issue - Param order in nce_loss in word2vec tutorial,"In section 'Building the graph' of g3doc/tutorials/word2vec/index.md:

```python
# Compute the NCE loss, using a sample of the negative labels each time.
loss = tf.reduce_mean(
  tf.nn.nce_loss(nce_weights, nce_biases, embed, train_labels,
                 num_sampled, vocabulary_size))
```

Whereas the actual order of parameters is, as in python/ops/nn_impl.py, is defined as:

```python
def nce_loss(weights,
             biases,
             labels,
             inputs,
             ....
```

So the `embed` and the `train_labels` parameters in the code excerpt in the word2vec tutorial page should be swapped to match the definition of `tf.nn.nce_loss()`.

"
6620,CudnnLSTM doesn't work with AdamOptimizer,"I am testing how to use CudnnLSTM, there is not a lot of documentation on this. In my own experiment, I found when use AdamOptimizer with CudnnLSTM, it always raises the following Exception. 

I also found another repository using CudnnLSTM, and uploaded it here: https://github.com/boche/LM-PTB-CUDNNLSTM. It also raises the same exception.

> File ""ptb_word_lm.py"", line 465, in main
>     m = PTBModel(is_training=True, config=config, debug=FLAGS.debug)
>   File ""ptb_word_lm.py"", line 254, in __init__
>     self._train_op = optimizer.apply_gradients(zip(allgrads, allvars))
>   File ""/data/ASR1/ramons/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py"", line 409, in apply_gradients
>     self._create_slots(var_list)
>   File ""/data/ASR1/ramons/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/adam.py"", line 119, in _create_slots
>     self._zeros_slot(v, ""m"", self._name)
>   File ""/data/ASR1/ramons/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py"", line 609, in _zeros_slot
>     named_slots[var] = slot_creator.create_zeros_slot(var, op_name)
>   File ""/data/ASR1/ramons/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/slot_creator.py"", line 121, in create_zeros_slot
>     val = array_ops.zeros(primary.get_shape().as_list(), dtype=dtype)
>   File ""/data/ASR1/ramons/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py"", line 782, in as_list
>     raise ValueError(""as_list() is not defined on an unknown TensorShape."")
> ValueError: as_list() is not defined on an unknown TensorShape.

For now, using GradientDescentOptimizer is okay, but it seems all other fancy optimizers all have similar problems. I have looked up this problem on stackoverflow, and found a related thread: http://stackoverflow.com/questions/40698821/tensorflow-adamoptimizer-throws-error-when-variable-has-validate-shape-false . 

It seems the problem is that parameter buffer used in CudnnLSTM doesn't have fixed shape (validate_shape=False, defined in line 145 of https://github.com/boche/LM-PTB-CUDNNLSTM/blob/master/ptb_word_lm.py ) , which is required by AdamOptimizer.
 
Cudnn seems to be faster (less time per epoch), but if we can't use better learning algorithms with it (meaning more epochs), then the total running time may not be improved so much."
6618,Could not resolve github.com,"I am trying for tensorflow for poets and while doing the following in docker image
`cd /tensorflow`
`git pull`

error:
`fatal: unable to access 'https://github.com/tensorflow/tensorflow.git/': Could not resolve host: github.com`

### attempted solutions 
followed the issue discussion `https://github.com/discourse/discourse_docker/issues/68`
After editing `/etc/default/docker`, uncommenting the DOCKER_OPTS line makes no difference."
6616,fake_quant_with_min_max_args has odd behavior,"### Description

On a simple linear regression example, `fake_quant_with_min_max_args` is not working. If I change to `fake_quant_with_min_max_vars` with trainable quantization min/max ranges, it works just fine. 
The min/max values are the same in both approaches. 
Reproducer included below.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

None

### Environment info
Operating System: Ubuntu 14.04.5, Python 2.7

Installed version of CUDA and cuDNN: Cuda 8, CuDNN 5.1

If installed from binary pip package, provide:

1. A link to the pip package you installed: https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp27-none-linux_x86_64.whl

2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`. 0.12.1

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

Below is a small reproducer, adapted from the example at [https://www.tensorflow.org/get_started/](https://www.tensorflow.org/get_started/)

```python
import tensorflow as tf
import numpy as np

# Create 10000 phony x, y data points in NumPy, y = x * 123.456 + 78.9
x_data = np.random.rand(10000).astype(np.float32)
y_data = x_data * 123.456 + 78.9

# Try to find values for W and b that compute y_data = W * x_data + b
# (We know that W should be 123.456 and b 78.9, but TensorFlow will
# figure that out for us.)
W = tf.Variable(tf.random_uniform([1], 0.0, 255.0))
b = tf.Variable(tf.zeros([1]))

# Now we quantize the weights and bias 
# The expected result after training should be 
# y = x * 123 + 79
W = tf.fake_quant_with_min_max_args(W, min=0.0, max=255.0)
b = tf.fake_quant_with_min_max_args(b, min=0.0, max=255.0)

y = W * x_data + b # Model

loss = tf.reduce_mean(tf.square(y - y_data))
optimizer = tf.train.GradientDescentOptimizer(0.5)
train = optimizer.minimize(loss)

init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)

# Fit the line.
for step in range(201):
    sess.run(train)
    if step % 20 == 0:
        print(step, sess.run(loss), sess.run(W), sess.run(b))
```
Output is a random rounded values for W and b, depending on run, but not the expected W=123, b=79, and the loss is large.

### What other attempted solutions have you tried?

If I instead use `fake_quant_with_min_max_vars` as illustrated below, it works fine (by printing, we have verified that the quantization ranges are [0,255] for each training iteration). The loss decreases and the values for W and b are as expected. The example is adapted from [https://www.tensorflow.org/get_started/](https://www.tensorflow.org/get_started/)

```python
import tensorflow as tf
import numpy as np

# Create 10000 phony x, y data points in NumPy, y = x * 123.456 + 78.9
x_data = np.random.rand(10000).astype(np.float32)
y_data = x_data * 123.456 + 78.9

W = tf.Variable(tf.random_uniform([1], 0.0, 255.0))
b = tf.Variable(tf.zeros([1]))

# Now we quantize the weights and bias
# The expected result after training should be 
# y = x * 123 + 79   Note that we train the quantization ranges
qmin = tf.Variable(0.0)
qmax = tf.Variable(255.0)
W = tf.fake_quant_with_min_max_vars(W, min=qmin, max=qmax)

qminb = tf.Variable(0.0)
qmaxb = tf.Variable(255.0)
b = tf.fake_quant_with_min_max_vars(b, min=qminb, max=qmaxb)

y = W * x_data + b # Model

loss = tf.reduce_mean(tf.square(y - y_data))
optimizer = tf.train.GradientDescentOptimizer(0.5)
train = optimizer.minimize(loss)

init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)

# Fit the line.
for step in range(201):
    sess.run(train)
    if step % 20 == 0:
        print(step, sess.run(loss), sess.run(W), sess.run(b))

print('Quantization ranges',  sess.run(qmin), sess.run(qmax), sess.run(qminb), sess.run(qmaxb))
```

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link)."
6614,zlib has disappeared,"Obviously this isn't your doing, but http://zlib.net/zlib-1.2.8.tar.gz and http://zlib.net/zlib-1.2.9.tar.gz are no longer on that server. The new http://zlib.net/zlib-1.2.10.tar.gz released yesterday is there. 

Because dependencies are downloaded as needed this is breaking the build."
6613,tensorflow cannot download packages when run ./configure,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

when I run ./configure, below comes out:
ERROR: /home/louis/Documents/dep-tensorflow/tensorflow/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@font_roboto//': Error downloading from https://github.com/polymerelements/font-roboto/archive/v1.0.1.tar.gz to /home/louis/.cache/bazel/_bazel_louis/d4f28d83a590525a3c3f2c81d1c44f93/external/font_roboto: Error downloading https://github.com/polymerelements/font-roboto/archive/v1.0.1.tar.gz to /home/louis/.cache/bazel/_bazel_louis/d4f28d83a590525a3c3f2c81d1c44f93/external/font_roboto/v1.0.1.tar.gz: Timed out connecting to https://github.com/polymerelements/font-roboto/archive/v1.0.1.tar.gz : connect timed out and referenced by '//tensorflow/tensorboard/bower:bower'.

ERROR: /home/louis/Documents/dep-tensorflow/tensorflow/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@graphlib//': Error downloading from https://github.com/cpettitt/graphlib/archive/v1.0.7.tar.gz to /home/louis/.cache/bazel/_bazel_louis/d4f28d83a590525a3c3f2c81d1c44f93/external/graphlib: Error downloading https://github.com/cpettitt/graphlib/archive/v1.0.7.tar.gz to /home/louis/.cache/bazel/_bazel_louis/d4f28d83a590525a3c3f2c81d1c44f93/external/graphlib/v1.0.7.tar.gz: Timed out connecting to https://github.com/cpettitt/graphlib/archive/v1.0.7.tar.gz : connect timed out and referenced by '//tensorflow/tensorboard/bower:bower'.

ERROR: /home/louis/Documents/dep-tensorflow/tensorflow/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_a11y_announcer//': Error downloading from https://github.com/polymerelements/iron-a11y-announcer/archive/v1.0.4.tar.gz to /home/louis/.cache/bazel/_bazel_louis/d4f28d83a590525a3c3f2c81d1c44f93/external/iron_a11y_announcer: Error downloading https://github.com/polymerelements/iron-a11y-announcer/archive/v1.0.4.tar.gz to /home/louis/.cache/bazel/_bazel_louis/d4f28d83a590525a3c3f2c81d1c44f93/external/iron_a11y_announcer/v1.0.4.tar.gz: Timed out connecting to https://github.com/polymerelements/iron-a11y-announcer/archive/v1.0.4.tar.gz : connect timed out and referenced by '//tensorflow/tensorboard/bower:bower'.

INFO: Downloading from https://github.com/polymerelements/iron-a11y-keys-behavior/archive/v1.1.2.tar.gz: 0B

Just as it shows in INFO, it seems it can not download the package that causes the error, and i am sure the internet is ok ,because i could download the package in the web.

### Environment info
Operating System:
UBUNTU 14.04

Bazel Version:
bazel 0.3.2
Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
cuda8.0
cuDNN:5.0

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
git clone -b v0.11.0 https://github.com.................
2. The output of `bazel version`

Build label: 0.3.2- (@non-git)
Build target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Sun Jan 1 22:19:10 2017 (1483309150)
Build timestamp: 1483309150
Build timestamp as int: 1483309150
"
6611,Word2Vec Number of Steps in Example,"In the word2vec basic example (master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py) the number of steps is set to 100001. Is this an arbitrary number? 

Should there not be a len(data) // (2*skip_window + 1  + (batch_size // num_skips)) number of steps to go through the data set?"
6610,whether we have another function that have the same function as tf.select in the latest version？,"I found there is no ""tf.select"" in the latest version（tensorflow） ,so I want to know if we have another function that have the same function as tf.select"
6607,tf.contrib.learn yields error message “module has no attribute 'learn' ”,"Here is a snippet of my code taken directly from the tf.contrib.learn tutorial 

`# Load Data Sets
training_set = tf.contrib.learn.datasets.base.load_csv_with_header(
    filename = IRIS_TRAINING,
    target_dtype = np.int,
    features_dtype = `np.float32)`

Here is the error message:

`AttributeError                            Traceback (most recent call last)
<ipython-input-14-7122d1244c55> in <module>()
     11 
     12 # Load Data Sets
---> 13 training_set = tf.contrib.learn.datasets.base.load_csv_with_header(
     14     filename = IRIS_TRAINING,
     15     target_dtype = np.int,

AttributeError: 'module' object has no attribute 'learn'`

Clearly the module has the attribute learn since tensorflow has a section on learning tf.contrib.learn. What am I doing wrong? All guidance is appreciated."
6606,could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM Check failed: stream->parent()-GetConvolveAlgorithms(&algorithms) ```,"
I'm trying to use tensorflow for this project: https://github.com/ibab/tensorflow-wavenet

I've gotten to the point where when I import tensorflow, I get the messages that all the CUDA libraries are successfully opened locally.

I can run the following python code from https://www.tensorflow.org/get_started/os_setup#run_tensorflow_from_the_command_line and it works fine.

> import tensorflow as tf
> hello = tf.constant('Hello, TensorFlow!')
> sess = tf.Session()
> print(sess.run(hello))
Hello, TensorFlow!
> a = tf.constant(10)
> b = tf.constant(32)
> print(sess.run(a + b))
42
>

However when I run the wavenet project, I get the following error messages and then python crashes.

```
c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:885] Found device 0 with properties:
name: GeForce GTX 950
major: 5 minor: 2 memoryClockRate (GHz) 1.19
pciBusID 0000:01:00.0
Total memory: 2.00GiB
Free memory: 1.65GiB
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:906] DMA: 0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:916] 0:   Y
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 950, pci bus id: 0000:01:00.0)
WARNING:tensorflow:From train.py:249 in main.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
Trying to restore saved checkpoints from ./logdir\train\2017-01-02T16-17-15 ... No checkpoint found.
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\cuda\cuda_dnn.cc:385] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\cuda\cuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM
F c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\kernels\conv_ops.cc:532] Check failed: stream->parent()-GetConvolveAlgorithms(&algorithms)
```




### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
https://github.com/tensorflow/tensorflow/issues/4251


### Environment info
Operating System:
Windows

Installed version of CUDA and cuDNN: 
cuDNN v5.1 (August 10, 2016), for CUDA 8.0

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library cublas64_80.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library cudnn64_5.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library cufft64_80.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library nvcuda.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library curand64_80.dll locally
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
NameError: name 'tensor' is not defined

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`


### What other attempted solutions have you tried?

I have tried reinstalling Cuda, different versions of cudnn. Looked at different issues with same error messages but nothing seemed to help.

"
6605,Commit to enable true fully convolutional application of network,"Hello,

I have created a pull request to tensorflow/models long time ago
and just want to point out that it will make using TF-Slim easier for
Image Segmentation:
https://github.com/tensorflow/models/pull/684

Is there a chance someone can review it?

Thank you."
6604,add summary_feed_dict support to tf.train.Supervisor,"Example use case:

> We want to use `tf.summary.image` on a (computed) tensor which requires a `feed_dict` of some input tensor.

Current work around:

> Manage `summary_op` manually (inside training loop). Cannot use any higher level training helper.

I could also provide a PR on interest."
6603,Allow dynamic summary names in new summary interface,"This is a feature request connected with pull request #5558.

The new summary interface only allows Python strings as summary names, while the old one could use string tensors.

The new behaviour is considerably less flexible. Consider the case where you have train/devel/test data split, and you want to report summaries (`accuracy` for example) for each dataset individually (i.e., having them in three different graphs in TensorBoard). With the old summary interface, this can be accomplished nicely:
```python
  dataset_name = tf.placeholder(tf.string, [])
  summary = tf.scalar_summary(dataset_name+""/accuracy"", accuracy)
...
  s = session.run([summary], {dataset_name:""train"" / ""devel"" / ""test""})
```

With the new interface, there has to be three summary nodes in the Graph to accomplish this, which makes the code repetitive and more complicated to maintain (you can create the three nodes with different name scopes, but you have to store them somewhere [a dictionary?], and if you want to use these nodes after restoring a metagraph, things become unellegant).

One possible solution is #5558  -- that pull request adds a `prefix` to all summary operations. The `prefix` is added to all summary names and can be a string tensor. That would allow reporting summaries for different datasets, while being applicable to the ""new summary approach"" of naming the summary operations according to the summary names (i.e., with the new approach it is not possible for `name` to be string tensor, but a `prefix` is fine).

The change #5558 is opt-in, so nobody interested in dynamic summary names is affected, while allowing much more flexible names to those interested, and I would like to see it merged.

However, there could be other approaches how the problem could be solved. Therefore, I am opening this issue for discussion and finding the best solution."
6602,fatal error: tensorflow/stream_executor/lib/status.h: No such file or directory,"I try to write my own op and I have installed TensorFlow 0.12.0 with GPU support on Linux.

This code fails:
```
#include ""tensorflow/core/platform/stream_executor.h""
```
With error:
```
fatal error: tensorflow/stream_executor/lib/status.h: No such file or directory
```

That files does not exists.
Some more include files seem to be missing. When grepping for `DeviceMemory` in the include path, the only file it finds is `include/tensorflow/core/util/stream_executor_util.h`.
"
6599,memory leak in tensorflow_gpu 0.12.1,"**Hi, We use tensorflow for training our OCR system models. I simply train models in tensorflow 0.9 and former versions. after some upgrade in cuda and tensorflow, I see large memory leak in our server with 32GB RAM.**

### I have tried all of suggestions in [How to debug a memory leak in TensorFlow](http://stackoverflow.com/documentation/tensorflow/3883/how-to-debug-a-memory-leak-in-tensorflow#t=201612280142239281993) and some other github issues and stackoverflow posts. No of them worked.

train code:

```
#!/bin/env python
import tensorflow as tf

import Config
import Utilities
from Dataset import Dataset
from Model import Model

dataset = Dataset()

sess_config = tf.ConfigProto()
sess_config.gpu_options.allow_growth = True
sess = tf.Session(config=sess_config)
images, labels = dataset.train_images_labels()
model = Model(images, labels, training=True)
tf.train.start_queue_runners(sess=sess)


def main(global_step=0):
    if global_step == 0:
        init_op = tf.global_variables_initializer()
        sess.run(init_op)
    else:
        checkpoint_path = Utilities.get_checkpoint_path(global_step)
        model.saver.restore(sess, checkpoint_path)
        Utilities.log_checkpoint_load(checkpoint_path)

    loss = 0
    train_iterations = global_step
    while train_iterations < Config.train_max_iterations:
        # train
        l = model.train(sess)
        loss += l
        train_iterations += 1

        # show train loss
        if train_iterations % Config.display_intervals == 0:
            loss /= float(Config.display_intervals)
            Utilities.log_train_loss(train_iterations, loss)
            loss = 0

        # save checkpoint
        if train_iterations % Config.checkpoint_intervals == 0:
            checkpoint_path = model.saver.save(sess, Utilities.get_checkpoint_path(train_iterations))
            Utilities.log_checkpoint_save(checkpoint_path)


if __name__ == '__main__':
    main()
```
images, labels are queues from tf.train.shuffle_batch function. I used Graph.finalize() and I am sure no new operation added to graph in training because the size of stored models' files are equal. I guess the reason of leak is tensorflow queues.


### Environment info
Ubuntu Server 14.04.5 LTS 

Installed version of CUDA and cuDNN: 
CUDA V8.0.44, cuDNN V5.1.5.

installed tensorflow from PyPI.
1. sudo pip install --upgrade tensorflow_gpu.
2. tensorflow 0.12.1."
6598,Type Error,"i have encounter an type error issue, and tried `import sys; reload(sys); sys.setdefaultencoding('UTF8')`.but i didn't work.
here is the detail
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so.8.0 locally
E tensorflow/core/framework/op_kernel.cc:925] OpKernel ('op: ""NegTrain"" device_type: ""CPU""') for unknown op: NegTrain
E tensorflow/core/framework/op_kernel.cc:925] OpKernel ('op: ""Skipgram"" device_type: ""CPU""') for unknown op: Skipgram
Traceback (most recent call last):
  File ""Model_Multi_Gpu.py"", line 371, in <module>
    tf.app.run()
  File ""/home/rootuser/.virtualenvs/tfrc1py2/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""Model_Multi_Gpu.py"", line 355, in main
    model = Model(train_scope,4)
  File ""Model_Multi_Gpu.py"", line 85, in __init__
    avaraged_grads = self.avg_grads()
  File ""Model_Multi_Gpu.py"", line 91, in avg_grads
    averaged_grads.append(tf.reduce_mean(grads_per_var))
  File ""/home/rootuser/.virtualenvs/tfrc1py2/local/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py"", line 1329, in reduce_mean
    _ReductionDims(input_tensor, axis, reduction_indices),
  File ""/home/rootuser/.virtualenvs/tfrc1py2/local/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py"", line 1178, in _ReductionDims
    return range(0, array_ops.rank(x))
  File ""/home/rootuser/.virtualenvs/tfrc1py2/local/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py"", line 342, in rank
    return rank_internal(input, name, optimize=True)
  File ""/home/rootuser/.virtualenvs/tfrc1py2/local/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py"", line 362, in rank_internal
    input_tensor = ops.convert_to_tensor(input)
  File ""/home/rootuser/.virtualenvs/tfrc1py2/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 645, in convert_to_tensor
    as_ref=False)
  File ""/home/rootuser/.virtualenvs/tfrc1py2/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 710, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/home/rootuser/.virtualenvs/tfrc1py2/local/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.py"", line 176, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/home/rootuser/.virtualenvs/tfrc1py2/local/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.py"", line 165, in constant
    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""/home/rootuser/.virtualenvs/tfrc1py2/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py"", line 441, in make_tensor_proto
    tensor_proto.string_val.extend([compat.as_bytes(x) for x in proto_values])
  File ""/home/rootuser/.virtualenvs/tfrc1py2/local/lib/python2.7/site-packages/tensorflow/python/util/compat.py"", line 65, in as_bytes
    (bytes_or_text,))
TypeError: Expected binary or unicode string, got <tensorflow.python.framework.ops.IndexedSlices object at 0x7fa82ab1cf50>

what should i do to correct it? thanks a lot"
6597,About the implementation of attention seq2seq decoder function,"I am very pleased to hear the news that the attention decoder function is added recently to the master branch.

However, the documentation of the function does not match with the actual implementation now.

1) Clarification on 'luong' option
Inspecting the code briefly, I guess the paper that the 'luong' option referencing is the following paper:
http://www.aclweb.org/anthology/D15-1166.
- When we do not use the 'input feeding' approach, then the attention decoder is unneeded, since attention vectors depend only on the current hidden states. Thus, we can use the simple decoder and compute attention vectors afterwards.
- When we use 'input feeding' approach, a computed attention vector should be fed to the next time-step's input. This is done by the added attention decoder with 'luong' option.
I think the above information should be stated to avoid confusion.

2) A possible bug on 'bahdanau' option
When we use 'bahdanau' option, _init_attention function just creates a zero vector as an attention vector. However, according to the referencing paper (https://arxiv.org/pdf/1409.0473.pdf) an attention vector should be taken into consideration (i.e. non-zero) when computing the first hidden state. Thus, I think _init_attention should output an attention vector if attention option is designated as 'bahdanau'.

Please tell me if I understood wrong. Thanks!"
6596,[ Bug ] The 2nd Saver fails to recognize its Checkpoint State file.,"**Operating System:** macOS Sierra

**Steps to Reproduce:**
1. In a session, create two Savers.
2. Let one of the Saver save the variables.
3. Let the other Saver save the variables in a different directory from the first Saver's destination directory.

**Result:**
An Info message is displayed indicating that a Checkpoint State file does not exist (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/saver.py#L738). And TensorFlow creates a new Checkpoint State file. This happens every time the 2nd Saver saves the variables.

**Expected Result:**
The 2nd Saver's Checkpoint State file should be recognized.

**Code to Reproduce:**
```
with tf.Session(graph=graph) as sess:
    ...
    saver_best = tf.train.Saver()
    saver_hourly = tf.train.Saver(max_to_keep=None)  
    ...
    for i in range(max_step):
        ...
        if last_hourly_save + datetime.timedelta(hours=1) < datetime.datetime.now():                     
            path_checkpoint_file = saver_hourly.save(sess, 'checkpoint_directory/hourly/model', global_step=i, latest_filename='hourly_checkpoint')
        ...
        if best:
            path_checkpoint_file = saver_best.save(sess, 'checkpoint_directory/best/model', global_step=i, latest_filename='best_checkpoint')
        ...
```"
6594,http://zlib.net/zlib-1.2.8.tar.gz no longer available,"I'm attempting to install tensorflow 0.12.1 from the r0.12 branch from source.

zlib has been updated from 1.2.8 to 1.2.9 so it appears the link must be updated from:
http://zlib.net/zlib-1.2.8.tar.gz


**edit**: per later comments, this has been moved to http://zlib.net/fossils/zlib-1.2.8.tar.gz

~~one solution may be to update to:~~
~~http://zlib.net/zlib-1.2.9.tar.gz~~

here is the error I'm getting:
```
INFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.
........
ERROR: /home/ahundt/src/tensorflow/tensorflow/core/BUILD:970:1: no such package '@zlib_archive//': Error downloading [http://zlib.net/zlib-1.2.8.tar.gz] to /home/ahundt/.cache/bazel/_bazel_ahundt/beca172f341045bf57b6baf5296669b3/external/zlib_archive/zlib-1.2.8.tar.gz: GET returned 404 Not Found and referenced by '//tensorflow/core:lib_internal'.
ERROR: /home/ahundt/src/tensorflow/tensorflow/core/BUILD:970:1: no such package '@zlib_archive//': Error downloading [http://zlib.net/zlib-1.2.8.tar.gz] to /home/ahundt/.cache/bazel/_bazel_ahundt/beca172f341045bf57b6baf5296669b3/external/zlib_archive/zlib-1.2.8.tar.gz: GET returned 404 Not Found and referenced by '//tensorflow/core:lib_internal'.
ERROR: Evaluation of query ""deps((//tensorflow/... union @bazel_tools//tools/jdk:toolchain))"" failed: errors were encountered while computing transitive closure.
```

In addition to the breakage fix, perhaps it would also make sense to make a change that would prevent future breakage of this sort?"
6593,MonitoredSession Implementation [awaiting response],"I am trying to solve simple arithmetic operations on 4 separate devices which I was successful in doing. Now, I want to implement fault tolerance in my program in the event that one of the node fails. I am using the tf.train.MonitoredTrainingSession class to achieve the desired results. I am not confident about the implementation in my program as I am not getting a favorable output. Below is the program as far as I could get:-

Please help
```
import tensorflow as tf

global_step_tensor = tf.Variable(10, trainable=False, name='global_step')

cluster = tf.train.ClusterSpec({""local"": [""localhost:2222"", ""localhost:2223"",""localhost:2224"", ""localhost:2225""]})
x = tf.constant(2)

with tf.device(""/job:local/task:0""):
    y1 = x + 300

with tf.device(""/job:local/task:1""):
    y2 = x**2

with tf.device(""/job:local/task:2""):
    y3 = 5*x

with tf.device(""/job:local/task:3""):
    y0 = x - 66
    y = y0 + y1 + y2 + y3

ChiefSessionCreator = tf.train.ChiefSessionCreator(scaffold=None, master='localhost:2222', config='grpc://localhost:2222', checkpoint_dir='/home/chaitanya/tensorflow/codes/checkpoints')
saver_hook = tf.train.CheckpointSaverHook(checkpoint_dir='/home/chaitanya/tensorflow/codes/checkpoints', save_secs=10, save_steps=None, saver=y, checkpoint_basename='model.ckpt', scaffold=None)
summary_hook = tf.train.SummarySaverHook(save_steps=None, save_secs=10, output_dir='/home/chaitanya/tensorflow/codes/savepoints', summary_writer=None, scaffold=None, summary_op=y)

with tf.train.MonitoredTrainingSession(master='localhost:2222', is_chief=True, checkpoint_dir='/home/chaitanya/tensorflow/codes/checkpoints', 
    scaffold=None, hooks=[saver_hook, summary_hook], chief_only_hooks=None, save_checkpoint_secs=10, save_summaries_steps=None, config='grpc://localhost:2222') as sess:

    while not sess.should_stop():
        sess.run(model)

    while not sess.should_stop():
        print(sess.run(y0))
        print('\n')

    while not sess.should_stop():
        print(sess.run(y1))
        print('\n')

    while not sess.should_stop():
        print(sess.run(y2))
        print('\n')

    while not sess.should_stop():
        print(sess.run(y3))
        print('\n')

    while not sess.should_stop():
        result = sess.run(y)
        print(result)
```
I have also posted this on stackoverflow:-

https://stackoverflow.com/questions/41478027/tf-train-monitoredtrainingsession-arguments"
