Issue Number,Issue Title,Issue Body
23306,Broken URL in TensorFlow Tutorial for Image Retraining,"On [https://www.tensorflow.org/hub/tutorials/image_retraining](https://www.tensorflow.org/hub/tutorials/image_retraining)

The URL of this command is broken
`curl -LO http://download.tensorflow.org/example_images/flower_photos.tgz
tar xzf flower_photos.tgz`

The working URL is should be the following
`curl -O http://download.tensorflow.org/example_images/flower_photos.tgz`

It was previously discussed at
[https://github.com/tensorflow/tensorflow/issues/8459](https://github.com/tensorflow/tensorflow/issues/8459)"
23303,Error while running python code in pymanoid and openrave,"Hello. I am trying to use pymanoid which uses openrave to visualize robots. Whenever I run a python file i get something like:
[0;34mIn [[1;34m1[0;34m]: [0m

I feel like this is an error. Can anyone help me with this?Is this an error? How can I fix it?

Thank You"
23302,"Autotune seems to be ""forgetting"" after a short delay within the same session.","**System information**
- I have written custom code in Microsoft Visual Studio Pro 2017 - C# (using TensorFlowSharp v1.11 wrapper classes).
- Windows 7.
- TensorFlow installed from binary.
- TensorFlow version:  I think Nuget automatically gives me v1.4 with TensorFlowSharp v1.11?
- Python version: Not using Python.  C# instead.
- Laptop CPU only, no GPU used.

**Describe the current behavior**
I'm trying to process 128x128 pixel images real-time on a low-powered PC.  I've been profiling my code by programmatically recording millisecond timings.  We've created a CAE.pb file and it seems to run properly, and the time that the session.run() call is made in rapid succession (using the same session each time), the time it takes decreases (up to a certain point).  Specifically, when I process the first image takes 1400ms, second image 1150ms, third image 900ms... at the 7th our 8th image it levels off at 350ms per image processing.  The problem is that this speed increase only increases if I push images to TensorFlow in rapid succession.  If I pause for more than approx 350ms, then the time it takes to process the next image (with the *same session* being reused the whole time), jumps back up to 1400ms.  I'm assuming that the decreasing time is due to autotune running.  However, autotune seems to ""forget"" it's session values if I don't keep it running and fed with new data constantly.  Am I doing something wrong or is this a bug?  Is there a RunOption or SessionOption that I'm unaware of that controls how long until autotune (or what I'm presuming is autotune) ""forgets""? :P

**Describe the expected behavior**
I would have expected that autotune could pause and not ""start from scratch"" (and would stay running quickly) so long as I kept the session object around.

**Code to reproduce the issue**
This is tricky as I'm using C#, and the code is large.  Seeing as TensorFlowSharp is a pretty thin wrapper class, this seemed to be more related to the internals/usage of TensorFlow itself.
"
23300,AttributeError: module '_pywrap_tensorflow_internal' has no attribute 'TFE_DEVICE_PLACEMENT_EXPLICIT_swigconstant',"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
NA
- TensorFlow installed from (source or binary):
From pip install
- TensorFlow version (use command below):
1.11.0
- Python version:
3.6.6
- Bazel version (if compiling from source):
NA
- GCC/Compiler version (if compiling from source):
NA
- CUDA/cuDNN version:
???
- GPU model and memory:
NA

**Describe the current behavior**
After installing 1.11.0 from pip, whenever I do `import tensorflow as tf`, I get the error in title.  If I downgrade to 1.10.0, the installation appears to be successful, or at least the import command works.

**Describe the expected behavior**
1.11.0 should work out of the box when doing a pip install

**Code to reproduce the issue**
All you need is `import tensorflow as tf` to see the problem

**Other info / logs**
NA"
23299,`tf.keras.layers.Bidirectional` does not work under eager execution mode,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS X 10.12.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO
- TensorFlow installed from (source or binary): BINARY
- TensorFlow version (use command below): v1.12.0-rc0-17-g7b08198113 1.12.0-rc1
- Python version: 3.6.5
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior**

A very simple model with `Bidirectional` layer will stop work after we enable the eager execution mode.

**Describe the expected behavior**

Code should work with eager execution mode.

**Code to reproduce the issue**

The following code will be able to reprorduce this problem:

```py
import tensorflow as tf
import numpy as np

tf.enable_eager_execution()

x = tf.keras.layers.Input(shape=(2, 1))
y = tf.keras.layers.Bidirectional(
    tf.keras.layers.LSTM(1, return_state=True)
)(x)

model = tf.keras.Model(inputs=x, outputs=y)

data = np.array([0.1, 0.2]).reshape((1, -1, 1))
print(model.predict(data))
```
However, if we comment out the `tf.enable_eager_execution()`, then it will work as expected:

**Other info / logs**

The error message(with eager execution enabled) is:

```
Traceback (most recent call last):
  File ""src/t.py"", line 11, in <module>
    )(x)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/layers/wrappers.py"", line 473, in __call__
    return super(Bidirectional, self).__call__(inputs, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 769, in __call__
    output_shapes = self.compute_output_shape(input_shapes)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/utils/tf_utils.py"", line 149, in wrapper
    output_shape = fn(instance, input_shape)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/layers/wrappers.py"", line 444, in compute_output_shape
    input_shape).as_list())
AttributeError: 'list' object has no attribute 'as_list'
```

After we disable the eager execution, then the above code works as expected.

```
[array([[0.01535364, 0.04371894]], dtype=float32), array([[0.01535364]], dtype=float32), array([[0.03121366]], dtype=float32), array([[0.04371894]], dtype=float32), array([[0.09061633]], dtype=float32)]
```"
23298,Student t distribution: beta parameter in gamma,"I think the beta parameter in gamma sampling for student t distribution should be 2 instead of 0.5 : 
[https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/python/ops/distributions/student_t.py](url)


`gamma_sample = random_ops.random_gamma(
        [n],
        0.5 * df,
        beta=0.5,
        dtype=self.dtype,
        seed=distribution_util.gen_new_seed(seed, salt=""student_t""))`"
23297,what's the default activation function of cudnnlstm in tensorflow? How can I set an activation function such as relu?,"what's the default activation function of cudnnlstm in tensorflow? How can I set an activation function such as relu? Maybe it's just linear model? I read the document, but I did not find it.
for example, the code is below:

lstmcell=tf.contrib.cudnn_rnn.CudnnLSTM(1,encoder_size,direction=""bidirectional"")

hq,_ =lstmcell(query)

and i read the document in tensorflow https://www.tensorflow.org/api_docs/python/tf/contrib/cudnn_rnn/CudnnLSTM?hl=zh-cn the functon is below

init(

num_layers,
num_units,
input_mode=CUDNN_INPUT_LINEAR_MODE,
direction=CUDNN_RNN_UNIDIRECTION,
dropout=0.0,
seed=None,
dtype=tf.float32,
kernel_initializer=None,
bias_initializer=None,
name=None
)

And no keyword to set a parameter such as ""activation = ""tanh"" just like tf.nn.rnn_cell.LSTMell.

So what's the default activation function of cudnnlstm in tensorflow, and how to change it to leaky_relu."
23296,Issue building C++ Tensorflow r1.6 and r1.7 on 64bit Windows 7 with CMake,"**System information**
- OS: Windows 7 Enterprise
- TensorFlow installed from (source or binary): Source
- TensorFlow version: r1.6 and r1.7
- Python version: 3.5
- Installed using virtualenv? pip? conda?: Cmake
- Bazel version (if compiling from source): /
- GCC/Compiler version (if compiling from source): MSVC 19.14
- CUDA/cuDNN version: /
- GPU model and memory: /

The compilation of TensorFlow fails.
This issue occurs when I try to build Tensorflow with the C++ API on my machine running Windows 7 Enterprise (x64) using the provided CMake files since I need this for a standalone project.
I have tested this with the releases r1.6, r1.7, which I have pulled from the repository (for versions above r1.7 the compilation procedure fails due to other reasons; r1.12 cannot be compiled because it fails during CMake).
I have used the official documentation provided [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/README.md), as well as other guides found on the internet (e.g. the two articles found [here](https://joe-antognini.github.io/machine-learning/build-windows-tf) and [here](https://joe-antognini.github.io/machine-learning/windows-tf-project) and another one [here](https://medium.com/@shiweili/building-tensorflow-c-shared-library-on-windows-e79c90e23e6e)).

First I initialise the shell by executing in a VS2017 developer command prompt:

C:\Program Files (x86)\Microsoft Visual Studio\2017\Professional\VC\Auxiliary\Build.

Then I do

cmake .. -A x64 -T host=x64 -DCMAKE_BUILD_TYPE=Release ^
-DSWIG_EXECUTABLE=""C:\swigwin-3.0.12\swig.exe"" ^
-DPYTHON_EXECUTABLE=""C:\anaconda3\envs\tensorflow\python.exe"" ^
-DPYTHON_LIBRARIES=""C:\anaconda3\envs\tensorflow\libs\python35.lib"" ^
-Dtensorflow_BUILD_PYTHON_BINDINGS=OFF ^
-Dtensorflow_ENABLE_GRPC_SUPPORT=OFF ^
-Dtensorflow_BUILD_SHARED_LIB=ON 

and start the build process with

""C:\Program Files (x86)\Microsoft Visual Studio\2017\Professional\MSBuild\15.0\Bin\amd64\MSBuild.exe"" ^
/m:1 ^
/p:CL_MPCount=1 ^
/p:Configuration=Release ^
/p:Platform=x64 ^
/p:PreferredToolArchitecture=x64 ALL_BUILD.vcxproj ^
/filelogger


The build fails, the error messages are taken from msbuild.log:

  Creating directories for 'eigen'
  Performing download step (download, verify and extract) for 'eigen'
  -- Downloading...
     dst='C:/tensorflow_r1.6/tensorflow/contrib/cmake/build/downloads/14e1418fcf12.tar.gz'
     timeout='none'
  -- Using src='https://bitbucket.org/eigen/eigen/get/14e1418fcf12.tar.gz'
  -- [download 1% complete]
  [...]
  -- [download 100% complete]
  -- Downloading... done
  -- extracting...
       src='C:/tensorflow_r1.6/tensorflow/contrib/cmake/build/downloads/14e1418fcf12.tar.gz'
       dst='C:/tensorflow_r1.6/tensorflow/contrib/cmake/build/eigen/src/eigen'
  -- extracting... [tar xfz]
  -- extracting... [analysis]
  -- extracting... [rename]
  CMake Error at eigen-stamp/extract-eigen.cmake:51 (file):
    file RENAME failed to rename
  
   C:/tensorflow_r1.6/tensorflow/contrib/cmake/build/eigen/src/ex-eigen1234/eigen-eigen-14e1418fcf12
    to
  
   C:/tensorflow_r1.6/tensorflow/contrib/cmake/build/eigen/src/eigen
  
 because: No such file or directory
  
  
  
C:\Program Files (x86)\Microsoft Visual Studio\2017\Professional\Common7\IDE\VC\VCTargets\Microsoft.CppCommon.targets(171,5): error MSB6006: ""cmd.exe"" exited with code 1. [C:\tensorflow_r1.6\tensorflow\contrib\cmake\build\eigen.vcxproj]
Done executing task ""CustomBuild"" -- FAILED.
Done building target ""CustomBuild"" in project ""eigen.vcxproj"" -- FAILED.
Done Building Project ""C:\tensorflow_r1.6\tensorflow\contrib\cmake\build\eigen.vcxproj"" (default targets) -- FAILED.

Cheers."
23295,[Keras tensorflow] float16 doesn't work with conv2d ?,"Hello,
I have a rtx card to use RT cores (dedicated for NN, uses half-precision to my understanding) I'd like using float16 so I : 

    from tensorflow.keras import backend
    os.environ['KERAS_FLOATX'] = 'float16'
    os.environ['TF_FP16_CONV_USE_FP32_COMPUTE'] = '0'
    backend.set_floatx('float16')

but this triggers error : 
   model = Sequential([
        Conv2D(32, kernel_size=(5, 5), input_shape=(32, 32, 3), strides=(1, 1))
    ])
    model.compile(optimizer=Adam(), loss='mse')

Traceback (most recent call last):
  File ""C:\Users\xd111\Miniconda3\envs\evo\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 510, in _apply_op_helper
    preferred_dtype=default_dtype)
  File ""C:\Users\xd111\Miniconda3\envs\evo\lib\site-packages\tensorflow\python\framework\ops.py"", line 1107, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""C:\Users\xd111\Miniconda3\envs\evo\lib\site-packages\tensorflow\python\framework\ops.py"", line 944, in _TensorTensorConversionFunction
    (dtype.name, t.dtype.name, str(t)))
ValueError: Tensor conversion requested dtype float16 for Tensor with dtype float32: 'Tensor(""conv2d_sample_weights:0"", shape=(?,), dtype=float32)'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""float16.py"", line 17, in <module>
    init()
  File ""float16.py"", line 15, in init
    model.compile(optimizer=Adam(), loss='mse')
  File ""C:\Users\xd111\Miniconda3\envs\evo\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 442, in compile
    output_loss = weighted_loss(y_true, y_pred, sample_weight, mask)
  File ""C:\Users\xd111\Miniconda3\envs\evo\lib\site-packages\tensorflow\python\keras\engine\training_utils.py"", line 453, in weighted
    score_array *= weights
  File ""C:\Users\xd111\Miniconda3\envs\evo\lib\site-packages\tensorflow\python\ops\math_ops.py"", line 847, in binary_op_wrapper
    return func(x, y, name=name)
  File ""C:\Users\xd111\Miniconda3\envs\evo\lib\site-packages\tensorflow\python\ops\math_ops.py"", line 1091, in _mul_dispatch
    return gen_math_ops.mul(x, y, name=name)
  File ""C:\Users\xd111\Miniconda3\envs\evo\lib\site-packages\tensorflow\python\ops\gen_math_ops.py"", line 4758, in mul
    ""Mul"", x=x, y=y, name=name)
  File ""C:\Users\xd111\Miniconda3\envs\evo\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 546, in _apply_op_helper
    inferred_from[input_arg.type_attr]))
TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type float16 of argument 'x'.

I'm not a professional I'm a newbie to NN so excuse me if my question is not relevant.

"
23294,Dataset shard index automatic change in Estimator DistributionStrategy,"**System information**
- TensorFlow version (you are using): 1.10
- Are you willing to contribute it (Yes/No): Yes


**Describe the feature and the current behavior/state.**
According to the cluster role defined in `Estimator`, the task index of `chief` and `worker` both start from 0. This brings trouble usage for reading data shard under distributed environment. Users should either change code to distinguish the task index between `chief` and `worker_0` or modify the `TF_CONFIG` environment to read different data shards.

```
d = tf.data.TFRecordDataset(FLAGS.input_file)
d = d.shard(FLAGS.num_workers, FLAGS.worker_index)
```

The propose of `DistributionStrategy` is to make distribution easy writing with minimal changes to model function and input function. However, we currently cannot avoid the problems described above. 

Is there any solutions for this problem? I am very willing to contribute for this but want to have more discussion for the design. 

**Will this change the current api? How?**
Maybe yes.

**Who will benefit with this feature?**
DistributionStrategy users.

**Any Other info.**
Any  discussion is appreciated."
23292,"Dockerfile of tensorflow container tag ""1.10.1-gpu""","Dear all,
where can i find the Docker file of the docker container with tag ""1.10.1-gpu"" ?

Best regards. Giusy "
23291,EMA's support for DistributionStrategy introduces worse performance,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Cent OS 7
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: None
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: master
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.15
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: 7.0
- **GPU model and memory**: NA
- **Exact command to reproduce**: NA

**Describe the current behavior**
This [PR](https://github.com/benjamintanweihao/tensorflow/commit/70af5e7ddcef853adb8af6355341bb8a97dcf736) add DistributionStrategy support to moving average APIs. However, **in MirroredStrategy**, this patch's implementation will introduce expensive MEMCPYPtoP cost, as `strategy.reduce(MEAN, value, v)` will eventually call strategy.broadcast(reduced, devices), and this get even worse when there are more devices used. Is this as expected as this implementation is just prepared for GENERAL distribution strategy? Thanks in advance. 


"
23290,Object detection and vibration,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):1.9.0
- Are you willing to contribute it (Yes/No):NO

**Describe the feature and the current behavior/state.**
I want to Object detection, if  Object name is equal to stop vibration , else cancel.

**Will this change the current api? How?**  NO change

**Who will benefit with this feature?**

**Any Other info.**

my code：

 Vibrator vb = (Vibrator) getSystemService(Context.VIBRATOR_SERVICE);
                if (getLocalClassName() == ""STOP"")
                  vb.vibrate(5000);
                else if (getLocalClassName() == ""turnright"")
                  vb.cancel();

my IDE：android studio

code source：https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/DetectorActivity.java

I have build APK, but detection ""STOP""  no vibration.
"
23287,Adding quantize fake quant per channel to TFlite,"Hello! I was faced with the fact that in the current version of TFlite the operation FakeQuantWithMinMaxVarsPerChannel was not implemented when quantized by means of the utility TFLiteConverter.
In this regard, the question is, is it planned to implement this operation and this type of quantization in TFlite?
And are there any known difficulties in realizing this quantization?"
23286,Tensorflow (cpu) missing DLL on import,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64bit
- TensorFlow installed from (source or binary): pip install tensorflow; pip install --upgrade tensorflow;
pip install --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.11.0-cp36-cp36m-win_amd64.whl etc.)
- TensorFlow version: Tried multiple from 1.4.0 - 1.11.0
- Python version: Python 3.6
- Installed using virtualenv? pip? conda?: Yes to all 3
- CUDA/cuDNN version: As far as I know I don't need it for the CPU Version.
- GPU model and memory: NA



**Describe the problem**
Can't import Tensorflow, missing DLL:
`
Traceback (most recent call last):
  File ""<input>"", line 1, in <module>
  File ""C:\Program Files\JetBrains\PyCharm Community Edition 2018.2.4\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 20, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""venv\lib\site-packages\tensorflow\__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Program Files\JetBrains\PyCharm Community Edition 2018.2.4\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 20, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""venv\lib\site-packages\tensorflow\python\__init__.py"", line 52, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""C:\Program Files\JetBrains\PyCharm Community Edition 2018.2.4\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 20, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""venv\lib\site-packages\tensorflow\core\framework\graph_pb2.py"", line 6, in <module>
    from google.protobuf import descriptor as _descriptor
  File ""C:\Program Files\JetBrains\PyCharm Community Edition 2018.2.4\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 20, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""venv\lib\site-packages\google\protobuf\descriptor.py"", line 47, in <module>
    from google.protobuf.pyext import _message
  File ""C:\Program Files\JetBrains\PyCharm Community Edition 2018.2.4\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 20, in do_import
    module = self._system_import(name, *args, **kwargs)
ImportError: DLL load failed: The specified procedure could not be found.
`

**Provide the exact sequence of commands / steps that you executed before running into the problem**

- Installed vc_redist64.exe
- Open Python
`import tensorflow`

**Any other info / logs**
- I already reinstalled Python completly 
- reinstalled vc_redist multiple times 
- and tried everything I found over google. Not sure if I am messing something up.
"
23285,Can't import tensorflow," - Windows 10 1709
 - Tensorflow version 1.11 install via pip
 - Python 3.6.6 64bit
 - Have I written custom code: No
 - Bazel version : I don't have
 - CUDA/cuDNN version : I don't have
 - GPU model and memory :N\A
 - Exact command to reproduce : import tensorflow

**The problem**
Can't import tensorflow

Traceback (most recent call last):
  File ""C:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Failed to initialize a DLL.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\latit\Desktop\LanParty\LANParty1.py"", line 27, in <module>
    import tensorflow
  File ""C:\Python36\lib\site-packages\tensorflow\__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Failed to initialize a DLL.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
23284, bazel-bin/$examples_dir/cifar10/cifar10_train --   in model_pruning  for No such file or directory,"Hi,
bazel-bin/$examples_dir/cifar10/cifar10_train --pruning_hparams=name=cifar10_pruning,begin_pruning_step=10000,end_pruning_step=100000,target_sparsity=0.9,sparsity_function_begin_step=10000,sparsity_function_end_step=100000
bash: bazel-bin/contrib/model_pruning/examples/cifar10/cifar10_train: No such file or directory


Thank you for your help!"
23283,CollectiveAllReduceStrategy RecvBufResponse error in ResNet model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): custom code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6
- TensorFlow installed from (source or binary): binary(modify the code to fix the bug, pull request [link](https://github.com/tensorflow/estimator/pull/3) )
- TensorFlow version (use command below): 1.12.0-rc0
- Python version: 2.7.10
- Bazel version (if compiling from source): 0.16.1
- GCC/Compiler version (if compiling from source):  LLVM version 9.1.0(clang-902.0.39.2)
- CUDA/cuDNN version: no installed
- GPU model and memory: Intel Iris Plus Graphics

**Describe the current behavior**
using the collective_all_reduce_strategy to train cifar10_Resnet model will raise an error, but the ParameterServerStrategy won't.

**Describe the expected behavior**
This cifar10_ResNet has been updated to r1.11 and used tf.estimator.train_and_evaluate to train model, with the new feature of using distribute strategy.
It was found that the ParameterServerStrategy ran normally, but the CollectiveAllReduceStrategy raised an error in RecvBufResponse.

**Code to reproduce the issue**
Source code is attached.
[cifar10_estimator.zip](https://github.com/tensorflow/tensorflow/files/2517601/cifar10_estimator.zip)

To run the ParameterServerStrategy, type these two commands in different shells:

```
python PS_ps0.py --data-dir=./cifar-10-data --job-dir=./model_dir/ --num-gpus=0 --train-steps=1000
```

```
python PS_worker0.py --data-dir=./cifar-10-data --job-dir=./model_dir/ --num-gpus=0 --train-steps=1000
```

To run the CollectiveAllReduceStrategy(just modify TF_CONFIG and strategy), type these two commands in different shells:

```
python Collective_worker0.py --data-dir=./cifar-10-data --job-dir=./model_dir/ --num-gpus=0 --train-steps=1000
```

```
python Collective_worker1.py --data-dir=./cifar-10-data --job-dir=./model_dir/ --num-gpus=0 --train-steps=1000
```

**Other info / logs**

Error message:RecvBufResponse returned 128 bytes where to_tensor expected 64
[[node resnet/BatchNorm/beta/Momentum/Initializer/CollectiveBcastRecv (defined at /Library/Python/2.7/site-packages/tensorflow/contrib/distribute/python/collective_all_reduce_strategy.py:186) ]]

"
23278,absl warning/error when compiling from source,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.11
- Python version: 3.6
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 0.18
- GCC/Compiler version (if compiling from source): 5.4.0
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
Tensorflow cannot find absl when tensorflow is included in external project. Warning is given when compiling tensorflow.
**Provide the exact sequence of commands / steps that you executed before running into the problem**
_bazel build --verbose_failures -c opt --copt=-msse4.1 --copt=-msse4.2 //tensorflow:libtensorflow_cc.so_
^this gives the following warning:
```
WARNING: /opt/tensorflow/tensorflow/core/BUILD:2463:1: in includes attribute of cc_library rule //tensorflow/core:framework_internal_headers_lib: '../../external/com_google_absl' resolves to 'external/com_google_absl' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /opt/tensorflow/tensorflow/tensorflow.bzl:1373:20
WARNING: /opt/tensorflow/tensorflow/core/BUILD:2548:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/com_google_absl' resolves to 'external/com_google_absl' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /opt/tensorflow/tensorflow/tensorflow.bzl:1373:20

```
When compiling external project which includes tensorflow, I get the following error: 
```
In file included from /usr/local/include/google/tensorflow/tensorflow/core/framework/tensor_shape.h:26:0,
                 from /usr/local/include/google/tensorflow/tensorflow/core/framework/tensor.h:21,
                 from /usr/local/include/google/tensorflow/tensorflow/core/public/session.h:24,
                 from /share/C++/modules/xyz/include/abc/abc.h:8,
                 from /share/C++/modules/xyz/src/abc/abc.cpp:1:
/usr/local/include/google/tensorflow/tensorflow/core/lib/gtl/array_slice.h:19:29: fatal error: absl/types/span.h: No such file or directory

```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23277,Tensorflow installation,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
Alienware 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
      Windows 10
- TensorFlow installed from (source or binary):
  not sure...from source though. I do pip install tensorflow-gpu
- TensorFlow version:
   tensorflow-gpu 1.11
- Python version:
      3.6
- Installed using virtualenv? pip? conda?:
    pip
- Bazel version (if compiling from source):
    no bazel. Would this be easier?
- GCC/Compiler version (if compiling from source):
    
- CUDA/cuDNN version:
    CUDA: 9
    cuDNN: 7.3
- GPU model and memory:
   laptop
   GPU: NVIDIA GTX 1070
   CPU: I7-7700 HQ 




**Describe the problem**
The installation is just not working for some reason. I am sure that I have followed the steps pretty well.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
1)cmd
2) ""python""
3) ""import tensorflow""

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.




[Tensorflow_Issues.txt](https://github.com/tensorflow/tensorflow/files/2517308/Tensorflow_Issues.txt)

"
23274,differentiable type casting,There seems to be no way to do differentiable type casting in tensorflow. tf.cast is not differentiable and so is tf.image.convert_image_dtype() to cast image tensors. How to incorporate casting in a loss function if there are no options ?
23273,Unable to import tensorflow,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 14.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.11
- Python version: python 2.7.12
- Installed using virtualenv? pip? conda?: inside of a virtualenv installed using pip
- Bazel version (if compiling from source): not sure
- GCC/Compiler version (if compiling from source): not sure
- CUDA/cuDNN version: not sure
- GPU model and memory: not sure



**Describe the problem**
I have installed tensorflow (CPU only) for python 2.7 from source. However, when I try to import tensorflow I come across the following error

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/john/python_2_7_12/lib/python2.7/site-packages/tensorflow/__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/home/john/python_2_7_12/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/john/python_2_7_12/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/john/python_2_7_12/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/john/python_2_7_12/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/john/python_2_7_12/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: /home/john/python_2_7_12/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: PyUnicodeUCS4_FromString


**Provide the exact sequence of commands / steps that you executed before running into the problem**

python -c ""import tensorflow as tf; print(tf.__version__)""

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23272,Seg Fault when using tf.data.contrib.map_and_batch.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes. I am using a modified ResNet50 model architecture.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 16.04. Specifically I am using the AWS DL AMI 16.0 Ubunutu
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.11.0-0-gc19e29306c 1.11.0
- Python version: 3.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 9/7 I believe
- GPU model and memory: Nvidia Tesla K80 (AWS p2.8xlarge)


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
Segmentation Fault

**Describe the expected behavior**
Not a segmentation fault.

**Code to reproduce the issue**
I do not have a minimal repro case which is unfortunate. My model is built using the TF Estimator API and I use the MirroredStrategy for utilizing multiple GPUs. I don't have any custom lower level code (C++).

I have been able to trigger this behavior in 2 ways.

1. Using all the GPUs on the system. It will always occur if I use all 8 GPUs.
2. Using train_and_evaluate as opposed to train. This seems to work if I use N-2 GPUs.

I am happy to run with any additional configurations or rebuild TF to provide more specific information to help debug this problem.

**Other info / logs**
```
INFO:tensorflow:loss = 1147.4103, step = 0

Thread 73 ""python3"" received signal SIGSEGV, Segmentation fault.
[Switching to Thread 0x7ffeebfff700 (LWP 68942)]
0x00007fff7f601bf3 in std::_Function_handler<void (long, long), Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long>, 16, Eigen::MakePointer> >, Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice, true>::run(Eigen::TensorAssignOp<Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long>, 16, Eigen::MakePointer> >, Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const> const> const&, Eigen::ThreadPoolDevice const&)::{lambda(long, long)#1}>::_M_invoke(std::_Any_data const&, long, long) () from /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
(gdb) bt
#0  0x00007fff7f601bf3 in std::_Function_handler<void (long, long), Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long>, 16, Eigen::MakePointer> >, Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice, true>::run(Eigen::TensorAssignOp<Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long>, 16, Eigen::MakePointer> >, Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const> const> const&, Eigen::ThreadPoolDevice const&)::{lambda(long, long)#1}>::_M_invoke(std::_Any_data const&, long, long) () from /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#1  0x00007fff7e87f019 in std::_Function_handler<void (long, long), Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const::{lambda(long, long)#1}>::_M_invoke(std::_Any_data const&, long, long) () from /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00007fff7e87efef in std::_Function_handler<void (long, long), Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const::{lambda(long, long)#1}>::_M_invoke(std::_Any_data const&, long, long) () from /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00007fff7e87efef in std::_Function_handler<void (long, long), Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const::{lambda(long, long)#1}>::_M_invoke(std::_Any_data const&, long, long) () from /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00007fff7c32693a in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so
#5  0x00007fff7c325a02 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()
   from /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so
#6  0x00007fff9c62ec5c in std::execute_native_thread_routine_compat (__p=<optimized out>) at /opt/conda/conda-bld/compilers_linux-64_1520532893746/work/.build/src/gcc-7.2.0/libstdc++-v3/src/c++11/thread.cc:110
#7  0x00007ffff7bc16ba in start_thread (arg=0x7ffeebfff700) at pthread_create.c:333
#8  0x00007ffff78f741d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109
```"
23269,Variable Length Sequence Support for CUDNN LSTM,The Cudnn LSTM is supposed to make the training much more efficient compared to the dynamic RNN. However I am using variable length sequences with the dynamic RNN using padded batches and passing the sequence length argument to the dynamic RNN to correctly denote the length of each sequence. But this kind of support is not available in the Cudnn LSTM. Or am I missing something here?
23265,No example exporting a model from an Estimator on TensorFlow.org,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: 1.9
- Doc Link: https://www.tensorflow.org/tutorials/estimators/cnn


**Describe the documentation issue**
In this CNN tutorial example, there is no exporting savedmodel example. For users that need to save the trained model for future usage, it will be better to add exporting savedmodel part in the tutorial.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
23263, CUDA driver version is insufficient for CUDA runtime version,"Python 3.6.6 | packaged by conda-forge | (default, Oct 12 2018, 14:08:43) 
[GCC 4.8.2 20140120 (Red Hat 4.8.2-15)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
>>> import tensorflow as tf
>>> sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
2018-10-25 12:10:00.651103: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-10-25 12:10:00.738944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-10-25 12:10:00.739553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.683
pciBusID: 0000:01:00.0
totalMemory: 7.93GiB freeMemory: 7.40GiB
2018-10-25 12:10:00.739580: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/jaimeet/.conda/envs/py-dl/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1511, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File ""/home/jaimeet/.conda/envs/py-dl/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 634, in __init__
    self._session = tf_session.TF_NewSessionRef(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version



**System information**
- OS Platform and Distribution :Ubuntu 18.04 
- TensorFlow installed from (source or binary): library
- TensorFlow version: 1.11.0
- Python version: 3.6
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: Cuda compilation tools, release 9.0, V9.0.176. NVIDIA-SMI 390.87 
- GPU model and memory:  GTX 1070 


*


"
23262,TPU support for C/C++ API,"Hello,

The available Tensorflow versiosn seems to have only Python APIs for (cloud) TPU devices. Is it possible to write C/C++ programs to utilize TPU? Are there corresponding C or C++ APIs?

Thanks for any help!"
23261,Newer TF post-install python test does not expose CUDA/cuDNN driver problems,"**System information** Windows 10
- TensorFlow version: tf-nightly-gpu (Oct. 25. 2018)
- Doc Link:
https://www.tensorflow.org/install/pip


**Describe the documentation issue**
The suggested post-install test:

`python -c ""import tensorflow as tf; print(tf.__version__)""
`
passes even when there are unresolved GPU CUDA/cuDNN driver issues.

The test previously suggested exercises the CUDA/cuDNN GPU drivers and seems to be better one to suggest:

`python -c ""import tensorflow as tf; hello = tf.constant('Hello, TF'); sess = tf.Session(); print(sess.run(hello))""
`

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
yes
"
23260,Undefined reference error when trying to run tensorflow lite model on android,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): 
yes

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
ubuntu 16.04

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
Samsung A3, armeabi-v7a

I cloned from master (Oct 25), last commit is 670eff0b0f60b9fde8231e927807677a80016904

**Describe the current behavior**

When compiling the app to use the tflite model I get an error:
```
error: undefined reference to 'tflite::InterpreterBuilder::operator()(std::__ndk1::unique_ptr<tflite::Interpreter, std::__ndk1::default_delete<tflite::Interpreter> >*)'
```
I've tried
```
std::unique_ptr<tflite::FlatBufferModel> model = tflite::FlatBufferModel::BuildFromFile(model_path.c_str());
tflite::ops::builtin::BuiltinOpResolver resolver;
std::unique_ptr<tflite::Interpreter> interpreter;
tflite::InterpreterBuilder(*model, resolver)(&interpreter);
```
As per the [docs here](https://www.tensorflow.org/lite/apis) (note the difference is because they seem to be out of date). As it wasn't working, I came across [a slightly different way](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/examples/minimal/minimal.cc#L53), with
```
tflite::InterpreterBuilder builder(*model.get(), resolver);
builder(&interpreter);
```
but that also did not work. These are the relevant headers I include
```
#include ""tensorflow/contrib/lite/model.h""
#include ""tensorflow/contrib/lite/interpreter.h""
#include ""tensorflow/contrib/lite/kernels/register.h""
```

I followed [this great guide](https://stackoverflow.com/questions/49834875/problems-with-using-tensorflow-lite-c-api-in-android-studio-project) for using tensorflow lite for android. Only thing I did different was some cmake stuff (I don't actually include the flatbuffer files inside my android project, but added them through cmake).



**Describe the expected behavior**

To compile.


"
23257,Golang Tensorflow API cannot be build any more,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- TensorFlow version: 1.10.1, 1.11.0, 1.12.0-rc1
- Only tested CPU version


**Describe the problem**
I tried to build my golang project which uses the tensorflow Go API. The CI build in Gitlab failed without a change (also old builds, which finished successfully before, cannot be build any more). 

I run the following commands:
```
curl --silent -L ""https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-1.12.0-rc1.tar.gz"" | tar -C /usr/local -xz
ldconfig
go get github.com/tensorflow/tensorflow/tensorflow/go
```

Then I receive the following error:

```
# github.com/tensorflow/tensorflow/tensorflow/go
/tmp/go-build813894416/b001/_x003.o: In function `_cgo_691d94bb61b7_Cfunc_TF_ImportGraphDefOptionsSetDefaultDevice':
/tmp/go-build/cgo-gcc-prolog:198: undefined reference to `TF_ImportGraphDefOptionsSetDefaultDevice'
collect2: error: ld returned 1 exit status
```

This did not happend before with the exact same commands. It also happens if I use other versions of the C library. Therefore I assume that the tensorflow Go API is broken in the current master. "
23256,TF 1.11 build issue with CUDA 8,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Source
- TensorFlow version: TF 1.11
- Python version: 2.7
- Installed using virtualenv? pip? conda?: Docker
- Bazel version (if compiling from source): 0.15.0
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version: 8.0/7
- GPU model and memory: GeForce GTX 1060 3GB



**Describe the problem**
Attempting to build a docker container with TF 1.11 and CUDA 8 on an GeForce 1060 3GB GPU. An error keeps occurring in the build (Log attached. "".txt"" was appended to be able to upload to github). Issue 22729 (https://github.com/tensorflow/tensorflow/issues/22729) was looked at but the work around didn't work for TF 1.11 and that's what is needed. The docker file is also attached. Any help you can provide would be greatly appreciated. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
sudo docker build --no-cache . -f Dockerfile.tf-1.11-py27-gpu.txt -t tf-1.11-py27-gpu

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

[Dockerfile.tf-1.11-py27-gpu.txt](https://github.com/tensorflow/tensorflow/files/2514994/Dockerfile.tf-1.11-py27-gpu.txt)
[tf11cuda8.log.txt](https://github.com/tensorflow/tensorflow/files/2514996/tf11cuda8.log.txt)

Thank you,
Kyle"
23255,"install tensorflow from source code, run ./configure in virtuealenvwrapper due to issues with missing site.getsitepackages()","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.11.0
- Python version: 3
- Installed using virtualenv? pip? conda?: virtualenvwrapper
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 9.2/7.1
- GPU model and memory: Geforce 1060



**Describe the problem**

install tensorflow from source code, run ./configure in virtuealenvwrapper due to issues with missing site.getsitepackages()

**Provide the exact sequence of commands / steps that you executed before running into the problem**

(tensorflow) $python --version
Python 3.5.2
(tensorflow) $ which python
/home/raymond/.virtualenvs/tensorflow/bin/python
(tensorflow) raymond@raymond:~/tensorflow-1.11.0⟫ ./configure 
WARNING: Processed legacy workspace file /home/raymond/tensorflow-1.11.0/tools/bazel.rc. This file will not be processed in the next release of Bazel. Please read https://github.com/bazelbuild/bazel/issues/6319 for further information, including how to upgrade.
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by com.google.protobuf.UnsafeUtil (file:/home/raymond/.cache/bazel/_bazel_raymond/install/f1e11885a5cc7ba9947679cffb18bf94/_embedded_binaries/A-server.jar) to field java.lang.String.value
WARNING: Please consider reporting this to the maintainers of com.google.protobuf.UnsafeUtil
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.18.0 installed.
Please specify the location of python. [Default is /home/raymond/.virtualenvs/tensorflow/bin/python]: 


Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
AttributeError: module 'site' has no attribute 'getsitepackages'

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23253,"TOCO failed  Batch normalization resolution requires that mean, multiplier and offset arrays be constant.","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.9 gpu
- Python version: 27
- Bazel version (if compiling from source): no use
- GCC/Compiler version (if compiling from source): no use
- CUDA/cuDNN version: CUDA9.1    CuDNN7.0
- GPU model and memory: GTX1070 8G


**Describe the current behavior**
`toco --output_file=test.tflite --graph_def_file=freeze.pb --input_arrays=Placeholder --output_arrays=logits/BatchNorm/Reshape_1 --output_format=TFLITE --inference_type=FLOAT --std_dev_values=1 --mean_values=0`

failed

Traceback (most recent call last):
  File ""/home/icare/.local/bin/toco"", line 11, in <module>
    sys.exit(main())
  File ""/home/icare/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 320, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/home/icare/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/home/icare/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 316, in run_main
    _convert_model(tflite_flags)
  File ""/home/icare/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 121, in _convert_model
    output_data = converter.convert()
  File ""/home/icare/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/lite.py"", line 309, in convert
    allow_custom_ops=self.allow_custom_ops)
  File ""/home/icare/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/convert.py"", line 225, in toco_convert
    input_data.SerializeToString())
  File ""/home/icare/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/convert.py"", line 107, in toco_convert_protos
    (stdout, stderr))
RuntimeError: TOCO failed see console for info.
2018-10-25 17:33:59.010229: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 135 operators, 224 arrays (0 quantized)
2018-10-25 17:33:59.011116: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 127 operators, 212 arrays (0 quantized)
2018-10-25 17:33:59.012209: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 127 operators, 212 arrays (0 quantized)
2018-10-25 17:33:59.012368: F tensorflow/contrib/lite/toco/graph_transformations/resolve_batch_normalization.cc:42] Check failed: IsConstantParameterArray(*model, bn_op->inputs[1]) && IsConstantParameterArray(*model, bn_op->inputs[2]) && IsConstantParameterArray(*model, bn_op->inputs[3]) Batch normalization resolution requires that mean, multiplier and offset arrays be constant.
Aborted (core dumped)

None

**Describe the expected behavior**

 I want to use toco to transform the pb file to the tflite file

**Code to reproduce the issue**

I use the code below to generate graph.pb and ckpy file
`g = tf.get_default_graph()
 graph_def = g.as_graph_def()
 tf.train.write_graph(graph_def, ""./model"", 'graph.pb', as_text=False) 
 saver = tf.train.Saver()
 saver.save(sess, os.path.join(FLAGS.checkpoint_dir, 'mnist-conv-slim.ckpt'))`

  

and then I freezed the graph.pb successfullly

 python freeze_graph.py --input_graph=/payh/to/graph.pb --input_checkpoint=/payh/tpo/mnist-conv-slim.ckpt --output_graph=/payh/to/mobile_face/model/freeze.pb --output_node_names=logits/BatchNorm/Reshape_1 --input_binary=True



**Other info / logs**
I use the code below to watch the nodes of each layer

`import tensorflow as tf
gf = tf.GraphDef()
#gf.ParseFromString(open('/tmp/inception_v3_quantized.pb','rb').read())
gf.ParseFromString(open('./model/freeze.pb','rb').read())
for n in gf.node:
    print ( n.name +' ===> '+n.op )  `

result:


Placeholder ===> Placeholder
Placeholder_1 ===> Placeholder
Placeholder_3 ===> Placeholder
Reshape/shape ===> Const
Reshape ===> Reshape
conv1/weights ===> Const
conv1/weights/read ===> Identity
conv1/Conv2D ===> Conv2D
conv1/BatchNorm/Const ===> Const
conv1/BatchNorm/beta ===> Const
conv1/BatchNorm/beta/read ===> Identity
conv1/BatchNorm/moving_mean ===> Const
conv1/BatchNorm/moving_mean/read ===> Identity
conv1/BatchNorm/moving_variance ===> Const
conv1/BatchNorm/moving_variance/read ===> Identity
conv1/BatchNorm/cond/Switch ===> Switch
conv1/BatchNorm/cond/switch_t ===> Identity
conv1/BatchNorm/cond/pred_id ===> Identity
conv1/BatchNorm/cond/Const ===> Const
conv1/BatchNorm/cond/Const_1 ===> Const
conv1/BatchNorm/cond/FusedBatchNorm ===> FusedBatchNorm
conv1/BatchNorm/cond/FusedBatchNorm/Switch ===> Switch
conv1/BatchNorm/cond/FusedBatchNorm/Switch_1 ===> Switch
conv1/BatchNorm/cond/FusedBatchNorm/Switch_2 ===> Switch
conv1/BatchNorm/cond/FusedBatchNorm_1 ===> FusedBatchNorm
conv1/BatchNorm/cond/FusedBatchNorm_1/Switch ===> Switch
conv1/BatchNorm/cond/FusedBatchNorm_1/Switch_1 ===> Switch
conv1/BatchNorm/cond/FusedBatchNorm_1/Switch_2 ===> Switch
conv1/BatchNorm/cond/FusedBatchNorm_1/Switch_3 ===> Switch
conv1/BatchNorm/cond/FusedBatchNorm_1/Switch_4 ===> Switch
conv1/BatchNorm/cond/Merge ===> Merge
conv1/CRelu/Neg ===> Neg
conv1/CRelu/axis ===> Const
conv1/CRelu ===> ConcatV2
conv1/CRelu/Relu ===> Relu
pool1/MaxPool ===> MaxPool
conv2/weights ===> Const
conv2/weights/read ===> Identity
conv2/Conv2D ===> Conv2D
conv2/BatchNorm/Const ===> Const
conv2/BatchNorm/beta ===> Const
conv2/BatchNorm/beta/read ===> Identity
conv2/BatchNorm/moving_mean ===> Const
conv2/BatchNorm/moving_mean/read ===> Identity
conv2/BatchNorm/moving_variance ===> Const
conv2/BatchNorm/moving_variance/read ===> Identity
conv2/BatchNorm/cond/Switch ===> Switch
conv2/BatchNorm/cond/switch_t ===> Identity
conv2/BatchNorm/cond/pred_id ===> Identity
conv2/BatchNorm/cond/Const ===> Const
conv2/BatchNorm/cond/Const_1 ===> Const
conv2/BatchNorm/cond/FusedBatchNorm ===> FusedBatchNorm
conv2/BatchNorm/cond/FusedBatchNorm/Switch ===> Switch
conv2/BatchNorm/cond/FusedBatchNorm/Switch_1 ===> Switch
conv2/BatchNorm/cond/FusedBatchNorm/Switch_2 ===> Switch
conv2/BatchNorm/cond/FusedBatchNorm_1 ===> FusedBatchNorm
conv2/BatchNorm/cond/FusedBatchNorm_1/Switch ===> Switch
conv2/BatchNorm/cond/FusedBatchNorm_1/Switch_1 ===> Switch
conv2/BatchNorm/cond/FusedBatchNorm_1/Switch_2 ===> Switch
conv2/BatchNorm/cond/FusedBatchNorm_1/Switch_3 ===> Switch
conv2/BatchNorm/cond/FusedBatchNorm_1/Switch_4 ===> Switch
conv2/BatchNorm/cond/Merge ===> Merge
conv2/CRelu/Neg ===> Neg
conv2/CRelu/axis ===> Const
conv2/CRelu ===> ConcatV2
conv2/CRelu/Relu ===> Relu
pool2/MaxPool ===> MaxPool
Flatten/flatten/Shape ===> Shape
Flatten/flatten/strided_slice/stack ===> Const
Flatten/flatten/strided_slice/stack_1 ===> Const
Flatten/flatten/strided_slice/stack_2 ===> Const
Flatten/flatten/strided_slice ===> StridedSlice
Flatten/flatten/Reshape/shape/1 ===> Const
Flatten/flatten/Reshape/shape ===> Pack
Flatten/flatten/Reshape ===> Reshape
fc1/weights ===> Const
fc1/weights/read ===> Identity
fc1/MatMul ===> MatMul
fc1/BatchNorm/Reshape/shape ===> Const
fc1/BatchNorm/Reshape ===> Reshape
fc1/BatchNorm/beta ===> Const
fc1/BatchNorm/beta/read ===> Identity
fc1/BatchNorm/Const ===> Const
fc1/BatchNorm/moving_mean ===> Const
fc1/BatchNorm/moving_mean/read ===> Identity
fc1/BatchNorm/moving_variance ===> Const
fc1/BatchNorm/moving_variance/read ===> Identity
fc1/BatchNorm/cond/Switch ===> Switch
fc1/BatchNorm/cond/switch_t ===> Identity
fc1/BatchNorm/cond/pred_id ===> Identity
fc1/BatchNorm/cond/Const ===> Const
fc1/BatchNorm/cond/Const_1 ===> Const
fc1/BatchNorm/cond/FusedBatchNorm ===> FusedBatchNorm
fc1/BatchNorm/cond/FusedBatchNorm/Switch ===> Switch
fc1/BatchNorm/cond/FusedBatchNorm/Switch_1 ===> Switch
fc1/BatchNorm/cond/FusedBatchNorm/Switch_2 ===> Switch
fc1/BatchNorm/cond/FusedBatchNorm_1 ===> FusedBatchNorm
fc1/BatchNorm/cond/FusedBatchNorm_1/Switch ===> Switch
fc1/BatchNorm/cond/FusedBatchNorm_1/Switch_1 ===> Switch
fc1/BatchNorm/cond/FusedBatchNorm_1/Switch_2 ===> Switch
fc1/BatchNorm/cond/FusedBatchNorm_1/Switch_3 ===> Switch
fc1/BatchNorm/cond/FusedBatchNorm_1/Switch_4 ===> Switch
fc1/BatchNorm/cond/Merge ===> Merge
fc1/BatchNorm/Shape ===> Shape
fc1/BatchNorm/Reshape_1 ===> Reshape
fc1/CRelu/Neg ===> Neg
fc1/CRelu/axis ===> Const
fc1/CRelu ===> ConcatV2
fc1/CRelu/Relu ===> Relu
Dropout/sub/x ===> Const
Dropout/sub ===> Sub
Dropout/sub_1/x ===> Const
Dropout/sub_1 ===> Sub
Dropout/dropout_1/Shape ===> Shape
Dropout/dropout_1/random_uniform/min ===> Const
Dropout/dropout_1/random_uniform/max ===> Const
Dropout/dropout_1/random_uniform/RandomUniform ===> RandomUniform
Dropout/dropout_1/random_uniform/sub ===> Sub
Dropout/dropout_1/random_uniform/mul ===> Mul
Dropout/dropout_1/random_uniform ===> Add
Dropout/dropout_1/add ===> Add
Dropout/dropout_1/Floor ===> Floor
Dropout/dropout_1/div ===> RealDiv
Dropout/dropout_1/mul ===> Mul
logits/weights ===> Const
logits/weights/read ===> Identity
logits/MatMul ===> MatMul
logits/BatchNorm/Reshape/shape ===> Const
logits/BatchNorm/Reshape ===> Reshape
logits/BatchNorm/beta ===> Const
logits/BatchNorm/beta/read ===> Identity
logits/BatchNorm/Const ===> Const
logits/BatchNorm/moving_mean ===> Const
logits/BatchNorm/moving_mean/read ===> Identity
logits/BatchNorm/moving_variance ===> Const
logits/BatchNorm/moving_variance/read ===> Identity
logits/BatchNorm/cond/Switch ===> Switch
logits/BatchNorm/cond/switch_t ===> Identity
logits/BatchNorm/cond/pred_id ===> Identity
logits/BatchNorm/cond/Const ===> Const
logits/BatchNorm/cond/Const_1 ===> Const
logits/BatchNorm/cond/FusedBatchNorm ===> FusedBatchNorm
logits/BatchNorm/cond/FusedBatchNorm/Switch ===> Switch
logits/BatchNorm/cond/FusedBatchNorm/Switch_1 ===> Switch
logits/BatchNorm/cond/FusedBatchNorm/Switch_2 ===> Switch
logits/BatchNorm/cond/FusedBatchNorm_1 ===> FusedBatchNorm
logits/BatchNorm/cond/FusedBatchNorm_1/Switch ===> Switch
logits/BatchNorm/cond/FusedBatchNorm_1/Switch_1 ===> Switch
logits/BatchNorm/cond/FusedBatchNorm_1/Switch_2 ===> Switch
logits/BatchNorm/cond/FusedBatchNorm_1/Switch_3 ===> Switch
logits/BatchNorm/cond/FusedBatchNorm_1/Switch_4 ===> Switch
logits/BatchNorm/cond/Merge ===> Merge
logits/BatchNorm/Shape ===> Shape
logits/BatchNorm/Reshape_1 ===> Reshape

It seems that the BN para are already constant

what should I do to solver the problem?

good luck to you thanks
"
23252,Tensorflow Docker files for 1.12 don't have XLA built binaries included.,"Using `tensorflow/tensorflow:1.12.0-rc1-devel-gpu-py3` and running the [example script](https://www.tensorflow.org/performance/xla/jit) shows that the tensorflow included in the docker image isn't built with XLA.

However, if you manually pip install tensorflow: `pip install tensorflow_gpu==1.12.0rc1` then run the example script it shows XLA support is built into the binary.

Please can someone update the Docker files, I would do but it's far from obvious where the arg would be inserted.

cc @angersson 

"
23251,I want to use the beamsearch in my seq2seq architecture. But I FAILED after working for a WHOLE afternoon!!!,"[beam_search_prediction.py.zip](https://github.com/tensorflow/tensorflow/files/2514356/beam_search_prediction.py.zip)

error is about the attention wrapper i guess"
23249,ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory," Linux Ubuntu 16.04  
Python 3.5
my cuda vision is 9.2.88
cudnn is 7.1.4
only one gpu : NVIDIA 1080ti
 I use pip to install tf

when I just install tf use "" pip install tensorflow==1.8.0 ""  ,it's OK ,it can import tensorflow,  but can't use gpu
  when I  pip install tensorflow-gpu==1.8.0 ,  these is error :  ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory

use   pip install tensorflow-gpu==1.11.0  or   pip install tensorflow==1.11.0  will get the same error.


"
23248,tf.estimator.train_and_evaluation stops before it reaches max_step,"**System information**
- TensorFlow version: 1.11.0, using docker images tensorflow/tensorflow:1.11.0-gpu-py3
- Doc Link: [tf.estimator.train_and_evaluation](https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate)

**Describe the documentation issue**

According to the doc, `tf.estimator.train_and_evaluation` was meant to train forever or `max_step` if specified in `train_spec`.

However, it took me many trial and errors to realize: if the dataset returned by `input_fn` in the `train_spec` was repeated a finite number of times and that number was smaller than `max_step`, the training would stop before it reached `max_step`.

For example, I had a dataset about 120000 images. With batch size of 64, the training would stop at 1875 step (~120000/64), even though `max_step` was set at 10000 or higher.

This behavior is not written in the document. Should the document be update or was it an unexpected behavior of the function?

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**

Yes."
23247,pip install failed,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
-windows 10pro):
- TensorFlow source (source or binary):
- TensorFlow version:
- Python 3.6:
- Installed  pip:
- GCC/Compiler version (if compiling from source):
cuda 9.0
-gtx760


**Describe the problem**

PS F:\Downloads>  pip install tensorflow-gpu
Collecting tensorflow-gpu
  Using cached https://files.pythonhosted.org/packages/3d/a0/60f72b76915a7c83e336e7f9ccf3a08305c30c7262cd15fedde44e026c3f/tensorflow_gpu-1.11.0-cp36-cp36m-win_amd64.whl
Requirement already satisfied: wheel>=0.26 in f:\development\python\lib\site-packages (from tensorflow-gpu) (0.32.2)
Requirement already satisfied: six>=1.10.0 in f:\development\python\lib\site-packages (from tensorflow-gpu) (1.11.0)
Requirement already satisfied: tensorboard<1.12.0,>=1.11.0 in f:\development\python\lib\site-packages (from tensorflow-gpu) (1.11.0)
Requirement already satisfied: setuptools<=39.1.0 in f:\development\python\lib\site-packages (from tensorflow-gpu) (39.1.0)
Requirement already satisfied: numpy>=1.13.3 in f:\development\python\lib\site-packages (from tensorflow-gpu) (1.15.3)
Requirement already satisfied: keras-applications>=1.0.5 in f:\development\python\lib\site-packages (from tensorflow-gpu) (1.0.6)
Requirement already satisfied: termcolor>=1.1.0 in f:\development\python\lib\site-packages (from tensorflow-gpu) (1.1.0)
Requirement already satisfied: keras-preprocessing>=1.0.3 in f:\development\python\lib\site-packages (from tensorflow-gpu) (1.0.5)
Requirement already satisfied: gast>=0.2.0 in f:\development\python\lib\site-packages (from tensorflow-gpu) (0.2.0)
Requirement already satisfied: protobuf>=3.6.0 in f:\development\python\lib\site-packages (from tensorflow-gpu) (3.6.1)
Requirement already satisfied: grpcio>=1.8.6 in f:\development\python\lib\site-packages (from tensorflow-gpu) (1.16.0)
Requirement already satisfied: astor>=0.6.0 in f:\development\python\lib\site-packages (from tensorflow-gpu) (0.7.1)
Requirement already satisfied: absl-py>=0.1.6 in f:\development\python\lib\site-packages (from tensorflow-gpu) (0.6.0)
Requirement already satisfied: markdown>=2.6.8 in f:\development\python\lib\site-packages (from tensorboard<1.12.0,>=1.11.0->tensorflow-gpu) (3.0.1)
Requirement already satisfied: werkzeug>=0.11.10 in f:\development\python\lib\site-packages (from tensorboard<1.12.0,>=1.11.0->tensorflow-gpu) (0.14.1)
Requirement already satisfied: h5py in f:\development\python\lib\site-packages (from keras-applications>=1.0.5->tensorflow-gpu) (2.8.0)
Installing collected packages: tensorflow-gpu
Exception:
Traceback (most recent call last):
  File ""f:\development\python\lib\site-packages\pip\_internal\cli\base_command.py"", line 143, in main
    status = self.run(options, args)
  File ""f:\development\python\lib\site-packages\pip\_internal\commands\install.py"", line 366, in run
    use_user_site=options.use_user_site,
  File ""f:\development\python\lib\site-packages\pip\_internal\req\__init__.py"", line 49, in install_given_reqs
    **kwargs
  File ""f:\development\python\lib\site-packages\pip\_internal\req\req_install.py"", line 760, in install
    use_user_site=use_user_site, pycompile=pycompile,
  File ""f:\development\python\lib\site-packages\pip\_internal\req\req_install.py"", line 382, in move_wheel_files
    warn_script_location=warn_script_location,
  File ""f:\development\python\lib\site-packages\pip\_internal\wheel.py"", line 215, in move_wheel_files
    prefix=prefix,
  File ""f:\development\python\lib\site-packages\pip\_internal\locations.py"", line 153, in distutils_scheme
    d.parse_config_files()
  File ""f:\development\python\lib\distutils\dist.py"", line 395, in parse_config_files
    parser.read(filename)
  File ""f:\development\python\lib\configparser.py"", line 697, in read
    self._read(fp, filename)
  File ""f:\development\python\lib\configparser.py"", line 1080, in _read
    raise MissingSectionHeaderError(fpname, lineno, line)
configparser.MissingSectionHeaderError: File contains no section headers.
file: 'setup.cfg', line: 1
'<?xml version=""1.0"" encoding=""utf-8""?>\n'
"
23246,Grpc+RDMA problem,"Same as https://github.com/tensorflow/tensorflow/issues/18630

Only start ps, it will got error, when worker is on other node. And will waiting when worker is on the same node.

**System information**
Have I written custom code： No
OS Platform： Ubuntu 16.04.3 LTS
TensorFlow installed from： by source code
TensorFlow version：1.8.0
Bazel version: bazel release 0.16.0
CUDA/cuDNN version: 9.0
GPU model and memory: P100-PCIE 16280MiB

config 2Ps and 2 workers, but only start one ps and one worker.
Here is the ps log. It doesn't wait for the ps/worker, and failure after  retry 5 times.
2018-10-18 09:09:22.257003: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> 192.168.96.135:20002, 1 -> 192.168.96.74:20003}
2018-10-18 09:09:22.264672: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:20001, 1 -> 192.168.96.74:20004}
2018-10-18 09:09:22.264716: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> 192.168.96.135:20002, 1 -> 192.168.96.74:20003}
2018-10-18 09:09:22.277272: I tensorflow/contrib/verbs/rdma.cc:315] RoCE v2 is not configured for GID_INDEX 0
2018-10-18 09:09:22.285727: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:332] Started server with target: grpc://localhost:20001
2018-10-18 09:10:57.293232: I tensorflow/contrib/verbs/rdma_mgr.cc:128] Connected to remote node /job:worker/replica:0/task:0
2018-10-18 09:10:57.295504: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:1: Got OS Error. Retrying (1/5)...
2018-10-18 09:10:59.296411: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:1: Got OS Error. Retrying (2/5)...
2018-10-18 09:11:01.297541: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:1: Got OS Error. Retrying (3/5)...
2018-10-18 09:11:04.298136: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:1: Got OS Error. Retrying (4/5)...
2018-10-18 09:11:06.299257: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:1: Got OS Error. Retrying (5/5)...
2018-10-18 09:11:06.299300: I tensorflow/contrib/verbs/rdma_mgr.cc:128] Connected to remote node /job:worker/replica:0/task:1
2018-10-18 09:11:06.302309: I tensorflow/contrib/verbs/rdma_mgr.cc:128] Connected to remote node /job:ps/replica:0/task:1
2018-10-18 09:11:06.302436: F tensorflow/contrib/verbs/rdma_mgr.cc:142] Check failed: rc->PingPostSend() == 0 Couldn't post send  to /job:worker/replica:0/task:1 with error: Invalid argument
Aborted (core dumped)
"
23245,ModuleNotFoundError: No module named 'tensorflow',"i am using python 3.6.7 , in python 3.6.7 version not able to install tensorflow, please post the solution and guide to me.

error:         Could not find a version that satisfies the requirement tensorflow (from versions: )
No matching distribution found for tensorflow  "
23243,Using TensorRT in TensorFlow from java,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):1.11
- Are you willing to contribute it (Yes/No):


**Describe the feature and the current behavior/state.**
Using TensorRT in TensorFlow from java for inference only

**Will this change the current api? How?** 
not sure if the current java api needs to change in order to load graph and infer

**Who will benefit with this feature?**
anyone using java runtime

**Any Other info.**
"
23241,Some problems in compile tensorflow in c++ (debug mode).,"Using tensorflow 1.8 and vs2015, evertime I compile the src file in debug mode, there is a problem like this:
![image](https://user-images.githubusercontent.com/33054583/47475426-d7d5e200-d84d-11e8-934b-8e493eb52ae3.png)
Even if I use a 32G computer. Any suggestions?"
23240,"a question about ,node naming rules of model network","
```
from keras import backend as K
In [40]: input = K.placeholder(ndim=3,name='x')

In [41]: input
Out[41]: <tf.Tensor 'x:0' shape=(?, ?, ?) dtype=float32>

In [42]: input2 = K.placeholder(ndim=3,name='x')

In [43]: input2
Out[43]: <tf.Tensor 'x_1:0' shape=(?, ?, ?) dtype=float32>
```
the name 'x:0' and 'x_1:0', the ':0' what mean, how to change it?"
23238,intel mkl optimized tensorflow performance degradation,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Deep Learning VM
Version: m10
Based on: Debian GNU/Linux 9.5 (stretch) (GNU/Linux 4.9.0-8-amd64 x86_64\n)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): deep-learning image  
- TensorFlow version (use command below): 1.11
- Python version: 2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: N/A 


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
Running deep model and some wide linear models. Inference performance is very bad. 2-4x slower relative to running inference without MKL
**Describe the expected behavior**
Performance should actually improve with intel mkl instruction set .

**Code to reproduce the issue**
code for deep and wide linear model. or logistic regression example code from tensorflow example

**Other info / logs**
When running using google deep learning image version M9 on gpu machine  (image : tf-latest-cu92, version M9) . Note : the inference is only running on cpu as i turn off the visibility for cuda devices, So the tensorflow code runs only runs on cpu. The image family says they are intel optimized packages but when i rung the benchmarks with verbosity on , i do not observe any mkl related stuff.

I start another deep learning image (tf-latest-cpu , version M10): Running exact same code on this machine with environment variable (export MKL_VERBOSE=1): I can observe a lot of openMP thread settings , KMP_xxx settings and mkl instructions logged with some timing information. I didn't observe any such thing in the M9 gpu image , even though in both place when i execute command   i observe following logs: 
M9 gpu image 
Numpy + Intel(R) MKL: THREADING LAYER: (null)
Numpy + Intel(R) MKL: setting Intel(R) MKL to use INTEL OpenMP runtime
Numpy + Intel(R) MKL: preloading libiomp5.so runtime
MKL_VERBOSE Intel(R) MKL 2019.0 Product build 20180829 for Intel(R) 64 architecture Intel(R) Advanced Vector Extensions 2 (Intel(R) AVX2) enabled processors, Lnx 2.20GHz lp64 intel_thread
MKL_VERBOSE SDOT(2,0x55fd25117d40,1,0x55fd25117d40,1) 1.61ms CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:16
1.11.0

M10 cpu image : 
Numpy + Intel(R) MKL: THREADING LAYER: (null)
Numpy + Intel(R) MKL: setting Intel(R) MKL to use INTEL OpenMP runtime
Numpy + Intel(R) MKL: preloading libiomp5.so runtime

User settings:

   KMP_AFFINITY=granularity=fine,verbose,compact,1,0
   KMP_BLOCKTIME=0
   KMP_SETTINGS=1
   OMP_NUM_THREADS=32

Effective settings:

   KMP_ABORT_DELAY=0
   KMP_ADAPTIVE_LOCK_PROPS='1,1024'
   KMP_ALIGN_ALLOC=64
   KMP_ALL_THREADPRIVATE=128
   KMP_ATOMIC_MODE=2
   KMP_BLOCKTIME=0
   KMP_CPUINFO_FILE: value is not defined
   KMP_DETERMINISTIC_REDUCTION=false
   KMP_DEVICE_THREAD_LIMIT=2147483647
   KMP_DISP_HAND_THREAD=false
   KMP_DISP_NUM_BUFFERS=7
   KMP_DUPLICATE_LIB_OK=false
   KMP_FORCE_REDUCTION: value is not defined
   KMP_FOREIGN_THREADS_THREADPRIVATE=true
   KMP_FORKJOIN_BARRIER='2,2'
   KMP_FORKJOIN_BARRIER_PATTERN='hyper,hyper'
   KMP_FORKJOIN_FRAMES=true
   KMP_FORKJOIN_FRAMES_MODE=3
   KMP_GTID_MODE=3
   KMP_HANDLE_SIGNALS=false
   KMP_HOT_TEAMS_MAX_LEVEL=1
   KMP_HOT_TEAMS_MODE=0
   KMP_INIT_AT_FORK=true
   KMP_INIT_WAIT=2048
   KMP_ITT_PREPARE_DELAY=0
   KMP_LIBRARY=throughput
   KMP_LOCK_KIND=queuing
   KMP_MALLOC_POOL_INCR=1M
   KMP_NEXT_WAIT=1024
   KMP_NUM_LOCKS_IN_BLOCK=1
   KMP_PLAIN_BARRIER='2,2'
   KMP_PLAIN_BARRIER_PATTERN='hyper,hyper'
   KMP_REDUCTION_BARRIER='1,1'
   KMP_REDUCTION_BARRIER_PATTERN='hyper,hyper'
   KMP_SCHEDULE='static,balanced;guided,iterative'
   KMP_SETTINGS=true
   KMP_SPIN_BACKOFF_PARAMS='4096,100'
   KMP_STACKOFFSET=64
   KMP_STACKPAD=0
   KMP_STACKSIZE=4M
   KMP_STORAGE_MAP=false
   KMP_TASKING=2
   KMP_TASKLOOP_MIN_TASKS=0
   KMP_TASK_STEALING_CONSTRAINT=1
   KMP_TEAMS_THREAD_LIMIT=32
   KMP_TOPOLOGY_METHOD=all
   KMP_USER_LEVEL_MWAIT=false
   KMP_VERSION=false
   KMP_WARNINGS=true
   OMP_AFFINITY_FORMAT='OMP: pid %P tid %T thread %n bound to OS proc set {%a}'
   OMP_ALLOCATOR=omp_default_mem_alloc
   OMP_CANCELLATION=false
   OMP_DEBUG=disabled
   OMP_DEFAULT_DEVICE=0
   OMP_DISPLAY_AFFINITY=false
   OMP_DISPLAY_ENV=false
   OMP_DYNAMIC=false
   OMP_MAX_ACTIVE_LEVELS=2147483647
   OMP_MAX_TASK_PRIORITY=0
   OMP_NESTED=false
   OMP_NUM_THREADS='32'
   OMP_PLACES: value is not defined
   OMP_PROC_BIND='intel'
   OMP_SCHEDULE='static'
   OMP_STACKSIZE=4M
   OMP_TARGET_OFFLOAD=DEFAULT
   OMP_THREAD_LIMIT=2147483647
   OMP_TOOL=enabled
   OMP_TOOL_LIBRARIES: value is not defined
   OMP_WAIT_POLICY=PASSIVE
   KMP_AFFINITY='verbose,warnings,respect,granularity=fine,compact,1,0'

OMP: Info #212: KMP_AFFINITY: decoding x2APIC ids.
OMP: Info #210: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info
OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0-31
OMP: Info #156: KMP_AFFINITY: 32 available OS procs
OMP: Info #157: KMP_AFFINITY: Uniform topology
OMP: Info #179: KMP_AFFINITY: 1 packages x 16 cores/pkg x 2 threads/core (16 total cores)
OMP: Info #214: KMP_AFFINITY: OS proc to physical thread map:
OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 16 maps to package 0 core 0 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 17 maps to package 0 core 1 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 2 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 18 maps to package 0 core 2 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 3 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 19 maps to package 0 core 3 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 4 maps to package 0 core 4 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 20 maps to package 0 core 4 thread 1

OMP: Info #250: KMP_AFFINITY: pid 8331 tid 8331 thread 0 bound to OS proc set 0
MKL_VERBOSE Intel(R) MKL 2019.0 Product build 20180829 for Intel(R) 64 architecture Intel(R) Advanced Vector Extensions 2 (Intel(R) AVX2) enabled processors, Lnx 2.20GHz lp64 intel_thread
MKL_VERBOSE SDOT(2,0x5622b7736500,1,0x5622b7736500,1) 2.54ms CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:16
1.11.0

So i assume intel mkl is being used in M10 image where as mkl is not being used in the M9 image (Note: i have turned off visibility for cuda devices so only cpu inference is being compared) . I observe 2-4x performance degradation with intel mkl.
The mkl suggested flags are appropriate: 
KMP_AFFINITY=granularity=fine,verbose,compact,1,0
   KMP_BLOCKTIME=0
   KMP_SETTINGS=1
   OMP_NUM_THREADS=32

Any ideas on how to debug the root cause and get the maximum performance for my models.
"
23236,Unnecessary gpu allocations when reusing resource variables that are initialized from checkpoint,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
**Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
**Ubuntu 16.04**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
**binary**
- TensorFlow version (use command below):
**b'v1.11.0-rc1-0-ge4c4b20' 1.11.0-rc1**

- Python version:
**3.6.6**
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
**9.2**
- GPU model and memory:
**Tesla K80**

**Describe the current behavior**
GPU memory is very high when using init_from_checkpoint and reuse=True with resource variables, in certain cases.

**Describe the expected behavior**
GPU memory should be at most parameters+activations

**Code to reproduce the issue**

```
#!/usr/bin/env python3

""""""
# first:
python repro_bug.py --write_ckpt

# then, compare:
python repro_bug.py # good
python repro_bug.py --use_ckpt # bad
""""""

import tensorflow as tf
from tensorflow.contrib.training import HParams
import fire

def model(*, X, hparams):
    with tf.variable_scope('model'):
        wte = tf.get_variable('wte', [hparams.n_vocab, hparams.n_embd],
                             initializer=tf.random_normal_initializer(stddev=0.02))
        h = tf.gather(wte, X)
        return tf.matmul(h, wte, transpose_b=True)

def main(
        batch_size=20, 
        length=100, # increase this to leak more memory!
        write_ckpt=False, use_ckpt=False, ckpt_path='/tmp/reprobug.ckpt',
        # hparams
        n_vocab=400, n_embd=300,
):
    hparams = HParams(n_vocab=n_vocab, n_embd=n_embd)

    if write_ckpt:
        with tf.Graph().as_default() as g:
            X = tf.placeholder(shape=[batch_size], dtype=tf.int32)
            results = model(X=X, hparams=hparams)
            saver = tf.train.Saver()
            with tf.Session() as sess:
                sess.run(tf.global_variables_initializer())
                saver.save(sess, ckpt_path)
        return

    X = tf.fill([batch_size], 0) # initial value
    for _ in range(length):
        # NOTE: doing something like reuse=(i > 0) doesn't help
        with tf.variable_scope('step', reuse=tf.AUTO_REUSE, use_resource=True):
            logits_flat = model(X=X, hparams=hparams)
        X = tf.squeeze(tf.multinomial(logits_flat, num_samples=1, output_dtype=tf.int32), axis=[1])

    if use_ckpt:
        print('setting initializers')
        tf.train.init_from_checkpoint(ckpt_path, {'/': 'step/'})

    config = tf.ConfigProto()
    config.gpu_options.visible_device_list = '0'
    with tf.Session(config=config) as sess:
        sess.run(tf.global_variables_initializer())

        run_metadata = tf.RunMetadata()
        sess.run(X, feed_dict={}, options=tf.RunOptions(trace_level=tf.RunOptions.HARDWARE_TRACE), run_metadata=run_metadata)
        mbs_in_use = []
        for dev_stat in run_metadata.step_stats.dev_stats[:]:
            for node_stat in dev_stat.node_stats[:]:
                for mem in node_stat.memory:
                    if mem.allocator_bytes_in_use and mem.allocator_name.startswith(""GPU""):
                        mbs_in_use.append(mem.allocator_bytes_in_use // 1000000)
        print(""mbs in use: "" , "" -> "".join(map(str, mbs_in_use)))

if __name__ == '__main__':
    fire.Fire(main)
```

**Other info / logs**

Output looks like this:
```
setting initializers
mbs in use:  0 -> 1 -> 1 -> 8 -> 10 -> 18 -> 27 -> 27 -> 35 -> [etc...]
```
without calling initialize_from_checkpoint, it looks like:
```
mbs in use:  0 -> 0 -> 0 -> 0 -> 0 -> 0 -> 0 -> 0 -> 0 -> 0 -> [etc...]
```
"
23234,TFLiteConverter produces empty .tflite file,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Adapted code from [here](https://www.tensorflow.org/lite/convert/python_api#exporting_a_graphdef_from_file_)
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): tf-nightly-gpu
- Python version: 3.5
- CUDA/cuDNN version: 9.0
- GPU model and memory: GeForce GTX 980 8GB

**Describe the current behavior**
I am trying to convert a frozen .pb (which I have working with TF Mobile java inference interface) to .tflite format using the python TFLiteConverter API. When I run the code below, it completes without error or warning, except it produces an output file SEGNET.tflite with 0 size.

**Describe the expected behavior**
I would like it you produce a non-zero file...

**Code to reproduce the issue**
I cannot provide the .pb file, but here is the code im using
```
import tensorflow as tf

graph_def_file = ""models/SEGNET.pb""
input_arrays = [""input_1""]
output_arrays = [""output_node0""]

converter = tf.contrib.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays)
tflite_model = converter.convert()
open(""SEGNET.tflite"", ""wb"").write(tflite_model)
```
I know that both the input and output are correctly named."
23232,Broken distributed training with tf.estimator.Estimator,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): v1.11.0-0-gc19e29306c 1.11.0
- Python version: 3.6.6
- Bazel version (if compiling from source): 0.18.0 (PPA)
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
- CUDA/cuDNN version: 10.0/7.3.1
- GPU model and memory: 2x1080ti + 2x1070

When I am trying to train network with Estimator and MirroredStrategy there is naming issue in estimator's code.
I get following error:
```
Traceback (most recent call last):
  File ""/snap/pycharm-community/85/helpers/pydev/pydevd.py"", line 1664, in <module>
    main()
  File ""/snap/pycharm-community/85/helpers/pydev/pydevd.py"", line 1658, in main
    globals = debugger.run(setup['file'], None, None, is_module)
  File ""/snap/pycharm-community/85/helpers/pydev/pydevd.py"", line 1068, in run
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""/snap/pycharm-community/85/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""/home/alexandr/Code/DRIT_Tensorflow/train_est.py"", line 102, in <module>
    tf.app.run(main)
  File ""/home/alexandr/.local/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/home/alexandr/Code/DRIT_Tensorflow/train_est.py"", line 79, in main
    estimator.train(train_input_fn)
  File ""/home/alexandr/.local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 356, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/alexandr/.local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1179, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File ""/home/alexandr/.local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1309, in _train_model_distributed
    grouped_estimator_spec.training_hooks)
  File ""/home/alexandr/.local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1305, in get_hooks_from_the_first_device
    for per_device_hook in per_device_hooks
  File ""/home/alexandr/.local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1305, in <listcomp>
    for per_device_hook in per_device_hooks
AttributeError: 'Estimator' object has no attribute '_distribution'
```

Problem occurs here:
```
        def get_hooks_from_the_first_device(per_device_hooks):
          return [
              self._distribution.unwrap(per_device_hook)[0]
              for per_device_hook in per_device_hooks
          ]
```
when estimator tries to call `unwrap()` method of `_distribution` but distribution strategy is stored in `_train_distribution` field.

This is from PyCharm's debugger:
```
per_device_hooks = {tuple} <class 'tuple'>: (PerDevice({'/replica:0/task:0/device:GPU:0': <model.model_fn.<locals>.TrainOpSelectorHook object at 0x7f0189a444e0>, '/replica:0/task:0/device:GPU:1': <model.model_fn.<locals>.TrainOpSelectorHook object at 0x7f01c3331d68>, '/replica:0/tas
self = {Estimator} <tensorflow.python.estimator.estimator.Estimator object at 0x7f0c3f44f4a8>
 _config = {RunConfig} <tensorflow.python.estimator.run_config.RunConfig object at 0x7f0c3f44f4e0>
 _device_fn = {NoneType} None
 _estimator_api_names = {tuple} <class 'tuple'>: ('estimator.Estimator',)
 _estimator_api_names_v1 = {tuple} <class 'tuple'>: ('estimator.Estimator',)
 _eval_distribution = {NoneType} None
 _model_dir = {str} 'train_logs/37/checkpoints'
 _params = {dict} {'batch_size': 1, 'image_size': [216, 216], 'float_type': tf.float32, 'gradient_scale': 1.0, 'leaky_relu_alpha': 0.2, 'attribute_tensor_depth': 32, 'attribute_subsample_factor': 4, 'attribute_tensor_global_pooling': False, 'remap_attribute_with_content': True, 'weight_decay': 1e-05, 'learning_rate': 1e-05, 'content_discriminator_lr_multiplier': 0.4, 'attribute_discriminator_lr_multiplier': 0.4, 'quantize': False, 'content_discriminator_steps': 1, 'domain_discriminator_steps': 1, 'num_steps_start_lr_decay': 100000, 'max_iter': 250000}
 _session_config = {ConfigProto} 
LOOK HERE ==> _train_distribution = {MirroredStrategy} <tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x7f0c3f44f5c0>
 _warm_start_settings = {NoneType} None
 config = {RunConfig} <tensorflow.python.estimator.run_config.RunConfig object at 0x7f017d7b0f98>
 model_dir = {str} 'train_logs/37/checkpoints'
 params = {dict} {'batch_size': 1, 'image_size': [216, 216], 'float_type': tf.float32, 'gradient_scale': 1.0, 'leaky_relu_alpha': 0.2, 'attribute_tensor_depth': 32, 'attribute_subsample_factor': 4, 'attribute_tensor_global_pooling': False, 'remap_attribute_with_content': True, 'weight_decay': 1e-05, 'learning_rate': 1e-05, 'content_discriminator_lr_multiplier': 0.4, 'attribute_discriminator_lr_multiplier': 0.4, 'quantize': False, 'content_discriminator_steps': 1, 'domain_discriminator_steps': 1, 'num_steps_start_lr_decay': 100000, 'max_iter': 250000}
```


**Describe the expected behavior**
What I expect? I expect tf.estimator's code to not be broken.

**Code to reproduce the issue**
Any example with MirroredStrategy should be fine.

"
23231,TFProf is unable to parse the profile generated by profileContext.,"TFProf is unable to parse the profile generated by profileContext.


**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution : Linux Ubuntu 18.04: 
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.11
- Python version: 3.6
- Bazel version (if compiling from source): 0.16.1
- GCC/Compiler version (if compiling from source): 7.3
- CUDA/cuDNN version: 10.0
- GPU model and memory: V100, 16GB

**Describe the current behavior**
TFProf unable to parse the profile generated by profileContext. 

**Describe the expected behavior**
When I generate the profile file by using the low level approach of runOptions, runMetadata, and then writing the string generated by profiler.serialize_to_string(), I can load it with tfprof and analyze it just fine. This should happen even when using ProfileContext.

**Code to reproduce the issue**
```
with tf.contrib.tfprof.ProfileContext('./profiles', trace_steps=range(50,100), dump_steps=[100]) as pctx:

 ... Regular session.run

```
"
23229,MirroredStrategy error with Object detection retraining,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.4 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below):b'v1.11.0-0-gc19e293' 1.11.0
- Python version:3.6.5
- Bazel version (if compiling from source): 0.15.0
- GCC/Compiler version (if compiling from source):5.4.0
- CUDA/cuDNN version:9.1/7.1.2
- GPU model and memory: AWS g2.8large Grid K520 4036MiB

**Describe the current behavior**
Training stops with the attached error log when retraining Mobilenet V2 SSD model.
**Describe the expected behavior**
Trainig should start and use the available 4 GPUs
**Code to reproduce the issue**
Following is the code change I made to https://github.com/tensorflow/models/blob/master/research/object_detection/model_main.py
```
def main(unused_argv):
  flags.mark_flag_as_required('model_dir')
  flags.mark_flag_as_required('pipeline_config_path')

  distribution = tf.contrib.distribute.MirroredStrategy()
  config = tf.estimator.RunConfig(model_dir=FLAGS.model_dir,train_distribute=distribution)
```

**Other info / logs**
Error Log: 
[TF_MirroredStrategy_error.txt](https://github.com/tensorflow/tensorflow/files/2512545/TF_MirroredStrategy_error.txt)

"
23228,Bug in problem.py input_fn,"Using tensor2tensor 1.9 and tf 1.9.0, while overriding the problem.py input_fn:
Traceback (most recent call last):
  File ""/usr/local/bin/t2t-trainer"", line 33, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/usr/local/bin/t2t-trainer"", line 28, in main
    t2t_trainer.main(argv)
  File ""/usr/local/lib/python2.7/dist-packages/tensor2tensor/bin/t2t_trainer.py"", line 355, in main
    execute_schedule(exp)
  File ""/usr/local/lib/python2.7/dist-packages/tensor2tensor/bin/t2t_trainer.py"", line 321, in execute_schedule
    getattr(exp, FLAGS.schedule)()
  File ""/usr/local/lib/python2.7/dist-packages/tensor2tensor/utils/trainer_lib.py"", line 331, in continuous_train_a
nd_eval
    self._eval_spec)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.py"", line 447, in train_and_eva
luate
    return executor.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.py"", line 531, in run
    return self.run_local()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.py"", line 669, in run_local
    hooks=train_hooks)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 366, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 1119, in _train_mode
l
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 1132, in _train_mode
l_default
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py"", line 1992, in _
call_model_fn
    features, labels, mode, config)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 1107, in _call_model
_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py"", line 2212, in _
model_fn
    input_holders.generate_infeed_enqueue_ops_and_dequeue_fn())
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py"", line 1001, in g
enerate_infeed_enqueue_ops_and_dequeue_fn
    self._invoke_input_fn_and_record_structure())
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py"", line 1065, in _
invoke_input_fn_and_record_structure
    host_device, host_id))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py"", line 692, in ge
nerate_per_host_enqueue_ops_fn_for_host
    inputs = _Inputs.from_input_fn(input_fn(user_context))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py"", line 2163, in _
input_fn
    return input_fn(**kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensor2tensor/data_generators/problem.py"", line 735, in estimator_in
put_fn
    dataset_kwargs=dataset_kwargs)
  File ""/home/michaelbesc/BESC_Source/route-prediction_MMedts/training_minimal_wCat_decOnly/trainer/routes.py"", lin
e 579, in input_fn
    batch_size, padded_shapes, drop_remainder=True)
TypeError: padded_batch() got an unexpected keyword argument 'drop_remainder'


This is because in tf 1.9 the 'drop remainder' argument doesn't yet exist (becomes available tf 1.10.0)
Not sure why this issue doesn't seem to occur when input_fn is not overridden. "
23222,Compiled source in Debian Stretch: Finds GPU Card then hangs,"

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Deban stretch
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.11
- Python version: 3.5
- Installed using virtualenv? pip? conda?: pipenv
- Bazel version (if compiling from source): 0.18.0
- GCC/Compiler version (if compiling from source): 6.3.0 20170516
- CUDA/cuDNN version: 9.0, 7.0 from runfile
- GPU model and memory: NV 1050 Ti 4GB

**Describe the problem**
Compilation for GPU runs flawlessly.
Running a simple TF script fails like:

>>> import tensorflow as tf
>>> tf.test.gpu_device_name()
2018-10-24 21:26:49.111741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-10-24 21:26:49.112489: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.43
pciBusID: 0000:07:00.0
totalMemory: 3.94GiB freeMemory: 3.87GiB
2018-10-24 21:26:49.112522: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0

It does obviously find the GPU Card but then the process hangs and can only be killed. Nothing in the dmesg or system logs. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
mkdir tf_compile2
cd tf_compile2/
pipenv install --python python3
pipenv shell
pipenv install --skip-lock pip six numpy wheel mock
pipenv install --skip-lock keras_applications==1.0.5
pipenv install --skip-lock keras_preprocessing==1.0.3
./configure (every options default, only choices are CUDA and CUDA Level 3.5)
bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
./bazel-bin/tensorflow/tools/pip_package/build_pip_package ..

In a fresh pipenv I then installed the wheel 
pipenv install ../tf_compile/tensorflow-1.11.0-cp35-cp35m-linux_x86_64.whl

**Any other info / logs**
The system is a Dual Intel XEON X5680 without AVX therefore the compilation from source.

root@cuda:/home/volker/workspace/tf_test# nvidia-smi 
Wed Oct 24 21:47:49 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 410.66       Driver Version: 410.66       CUDA Version: 10.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 105...  Off  | 00000000:07:00.0  On |                  N/A |
|  0%   22C    P8    N/A /  72W |     28MiB /  4039MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0       721      G   /usr/lib/xorg/Xorg                            25MiB |
+-----------------------------------------------------------------------------+
root@cuda:/home/volker/workspace/tf_test# nvidia-smi -q

==============NVSMI LOG==============

Timestamp                           : Wed Oct 24 21:47:54 2018
Driver Version                      : 410.66
CUDA Version                        : 10.0

Attached GPUs                       : 1
GPU 00000000:07:00.0
    Product Name                    : GeForce GTX 1050 Ti
    Product Brand                   : GeForce
    Display Mode                    : Enabled
    Display Active                  : Enabled
    Persistence Mode                : Disabled
    Accounting Mode                 : Disabled
    Accounting Mode Buffer Size     : 4000
    Driver Model
        Current                     : N/A
        Pending                     : N/A
    Serial Number                   : N/A
    GPU UUID                        : GPU-54012771-5371-0894-d451-63b8bf4b46cd
    Minor Number                    : 0
    VBIOS Version                   : 86.07.42.00.99
    MultiGPU Board                  : No
    Board ID                        : 0x700
    GPU Part Number                 : N/A
    Inforom Version
        Image Version               : G001.0000.01.04
        OEM Object                  : 1.1
        ECC Object                  : N/A
        Power Management Object     : N/A
    GPU Operation Mode
        Current                     : N/A
        Pending                     : N/A
    GPU Virtualization Mode
        Virtualization mode         : None
    IBMNPU
        Relaxed Ordering Mode       : N/A
    PCI
        Bus                         : 0x07
        Device                      : 0x00
        Domain                      : 0x0000
        Device Id                   : 0x1C8210DE
        Bus Id                      : 00000000:07:00.0
        Sub System Id               : 0x37631458
        GPU Link Info
            PCIe Generation
                Max                 : 2
                Current             : 1
            Link Width
                Max                 : 16x
                Current             : 16x
        Bridge Chip
            Type                    : N/A
            Firmware                : N/A
        Replays since reset         : 0
        Tx Throughput               : 0 KB/s
        Rx Throughput               : 0 KB/s
    Fan Speed                       : 0 %
    Performance State               : P8
    Clocks Throttle Reasons
        Idle                        : Active
        Applications Clocks Setting : Not Active
        SW Power Cap                : Not Active
        HW Slowdown                 : Not Active
            HW Thermal Slowdown     : Not Active
            HW Power Brake Slowdown : Not Active
        Sync Boost                  : Not Active
        SW Thermal Slowdown         : Not Active
        Display Clock Setting       : Not Active
    FB Memory Usage
        Total                       : 4039 MiB
        Used                        : 28 MiB
        Free                        : 4011 MiB
    BAR1 Memory Usage
        Total                       : 256 MiB
        Used                        : 5 MiB
        Free                        : 251 MiB
    Compute Mode                    : Default
    Utilization
        Gpu                         : 0 %
        Memory                      : 3 %
        Encoder                     : 0 %
        Decoder                     : 0 %
    Encoder Stats
        Active Sessions             : 0
        Average FPS                 : 0
        Average Latency             : 0
    FBC Stats
        Active Sessions             : 0
        Average FPS                 : 0
        Average Latency             : 0
    Ecc Mode
        Current                     : N/A
        Pending                     : N/A
    ECC Errors
        Volatile
            Single Bit            
                Device Memory       : N/A
                Register File       : N/A
                L1 Cache            : N/A
                L2 Cache            : N/A
                Texture Memory      : N/A
                Texture Shared      : N/A
                CBU                 : N/A
                Total               : N/A
            Double Bit            
                Device Memory       : N/A
                Register File       : N/A
                L1 Cache            : N/A
                L2 Cache            : N/A
                Texture Memory      : N/A
                Texture Shared      : N/A
                CBU                 : N/A
                Total               : N/A
        Aggregate
            Single Bit            
                Device Memory       : N/A
                Register File       : N/A
                L1 Cache            : N/A
                L2 Cache            : N/A
                Texture Memory      : N/A
                Texture Shared      : N/A
                CBU                 : N/A
                Total               : N/A
            Double Bit            
                Device Memory       : N/A
                Register File       : N/A
                L1 Cache            : N/A
                L2 Cache            : N/A
                Texture Memory      : N/A
                Texture Shared      : N/A
                CBU                 : N/A
                Total               : N/A
    Retired Pages
        Single Bit ECC              : N/A
        Double Bit ECC              : N/A
        Pending                     : N/A
    Temperature
        GPU Current Temp            : 22 C
        GPU Shutdown Temp           : 102 C
        GPU Slowdown Temp           : 99 C
        GPU Max Operating Temp      : N/A
        Memory Current Temp         : N/A
        Memory Max Operating Temp   : N/A
    Power Readings
        Power Management            : Supported
        Power Draw                  : N/A
        Power Limit                 : 72.00 W
        Default Power Limit         : 72.00 W
        Enforced Power Limit        : 72.00 W
        Min Power Limit             : 52.50 W
        Max Power Limit             : 75.00 W
    Clocks
        Graphics                    : 139 MHz
        SM                          : 139 MHz
        Memory                      : 405 MHz
        Video                       : 544 MHz
    Applications Clocks
        Graphics                    : N/A
        Memory                      : N/A
    Default Applications Clocks
        Graphics                    : N/A
        Memory                      : N/A
    Max Clocks
        Graphics                    : 1936 MHz
        SM                          : 1936 MHz
        Memory                      : 3504 MHz
        Video                       : 1708 MHz
    Max Customer Boost Clocks
        Graphics                    : N/A
    Clock Policy
        Auto Boost                  : N/A
        Auto Boost Default          : N/A
    Processes
        Process ID                  : 721
            Type                    : G
            Name                    : /usr/lib/xorg/Xorg
            Used GPU Memory         : 25 MiB

"
23220,verbs: local protection error when doing rdma send,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 14.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.9.0
- Python version: 2.7
- Bazel version (if compiling from source): 0.16.1
- GCC/Compiler version (if compiling from source): 6.3
- CUDA/cuDNN version: 9.0
- GPU model and memory: TITAN Xp. 12 GB


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
When verbs support is enabled, getting

```
mlx5: 4dfe187391fa: got completion with error:
00000000 00000000 00000000 00000000
00000000 00000000 00000000 00000000
00000000 00000000 00000000 00000000
00000000 92005204 0900013b 05cc37d3
2018-09-21 23:48:04.608288: F external/org_tensorflow/tensorflow/contrib/verbs/rdma.cc:451] Check failed: wc_[i].status == IBV_WC_SUCCESS Failed status 
local protection error 4 139821989727968 82
```

**Describe the expected behavior**
Should not produce this error.

**Code to reproduce the issue**
Need substantial infrastructure (RoCE) and code to reproduce.

**Other info / logs**
The `wr`s (verbs work request) that trigger this error has lkey set to 0 and non-zero buffer length, indicating that it's trying to send unregistered memory region."
23219,CUDA 10 Support,"I am using a containerized env on CentOS with Anaconda, and a RTX 2080 Ti GPU card with 540 Tensor Core Processing Units.

According to the docs: https://www.tensorflow.org/install/gpu
it may not work due to lack of versioning support.

Also - how do I leverage the NVIDIA TPUs/is there support for that?"
23216,Failing isinstance check in tensorflow.python.keras.models._clone_functional_model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
CentOS, specifically
Linux 3.10.0-693.5.2.el7.x86_64 #1 SMP Fri Oct 20 20:32:50 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

- TensorFlow installed from (source or binary): Through conda
- TensorFlow version (use command below): 1.11.0
- Python version:
Python 2.7.15 |Anaconda, Inc.| (default, Oct 10 2018, 21:32:13)
[GCC 7.3.0] on linux2
-Bazel version: NA
-CUDA/cuDNN version: NA
-GPU model and memory: NA
-Exact command to reproduce: NA
-Mobile device: NA

**Describe the current behavior**
`tensorflow.python.keras.models._clone_functional_model `raises a `ValueError `because the model passed to it is supposedly not an instance of `Model`, even though it is.

**Code to reproduce the issue**
```
from tensorflow import keras
from keras.models import Model
from tensorflow.python.keras.engine import training
Model2 = training.Model
isinstance(Model(), Model2)
```
This returns False. Even though `keras.models.Model` should just be an alias for `tensorflow.python.keras.engine.training.Model`.

**Other info / logs**
I run into this issue when calling` tensorflow.python.keras.estimator.model_to_estimator`.
I can work around it by importing all my keras imports using the full `tensorflow.python.keras...., `but I'm not sure why that should be necessary.
"
23215,"what does  ""Device peer to peer matrix"" mean?","I have 2 system  both with 4 1080Ti. but when tensorflow startup. some information is not same: 

1. first of all, the information is printed by different code and the content is not the same. 

1. on second system the diagonal is marked as N.

does someone know what does these log means? **and more importantly, does it imply some error or performance degrade?**


**system 1:**

> 2018-09-29 10:19:05.759292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1227] Device peer to peer matrix
> 2018-09-29 10:19:05.771338: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1233] DMA: 0 1 2 3 
> 2018-09-29 10:19:05.771362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1243] 0:   Y Y Y Y 
> 2018-09-29 10:19:05.771367: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1243] 1:   Y Y Y Y 
> 2018-09-29 10:19:05.771371: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1243] 2:   Y Y Y Y 
> 2018-09-29 10:19:05.771376: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1243] 3:   Y Y Y Y 

**system 2:**

> 2018-09-29 10:54:20.831299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0, 1, 2, 3
> 2018-09-29 10:54:20.831470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
> 2018-09-29 10:54:20.831480: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 1 2 3 
> 2018-09-29 10:54:20.831486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N Y Y Y 
> 2018-09-29 10:54:20.831491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 1:   Y N Y Y 
> 2018-09-29 10:54:20.831495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 2:   Y Y N Y 
> 2018-09-29 10:54:20.831499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 3:   Y Y Y N 

"
23214,Try to Run Tensorflow on CUP clusters,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS release 6.9 
- TensorFlow version: 1.9.0-rc0
- Python version:3.6.5
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

Firstly, I want to run tensorflow on a cluster only with CPU nodes. Therefore no CUDA library is installed and only has the CPU-only version of tensorflow installed. I have many questions on the correct setting. Let's start from the very basic situation:

## 1. node with multiple CPU cores

From this [answer](https://stackoverflow.com/a/37660913/4468490), it propose to set `device_count = {'GPU': 0}` in `tf.ConfigProto` to force the code to be run on CPU only. since my platform has GPUs, so I think this step deosn't work for me. *However*, I am curious on under such setting, how the resources of CPUs are assigned. Will the code only run on only 1 CPU cores or not?

So I turned to [yaroslavvb's example](https://gist.github.com/yaroslavvb/9a5f4a0b613c79152152b35c0bc840b8#file-cpu_device_test-py-L21), but I still have some problems:

1. can I avoid using more statement `with tf.device(""/cpu:0""):`, etc?

2. what's the relation between `device_count={""CPU"": 8}` and `inter_op_parallelism_threads=3,                    intra_op_parallelism_threads=1`? From the [answer from mrry](https://stackoverflow.com/a/41233901/4468490), I think `inter` controls the distribution with threads working on independent blocks in my graph (say, multiple independent matrix operations), and `intra` works on ""internally parallelable"" tasks (say, a matrix multiplication). Am I correct?  Back to [yaroslavvb's example](https://gist.github.com/yaroslavvb/9a5f4a0b613c79152152b35c0bc840b8#file-cpu_device_test-py-L21), I think the task is assigned to 4 CPU cores, and `{""CPU"": 8}` requests 8 cores. If I am correct, why request more than needed? 

3. how the task is distributed among each CPU cores, without using `with tf.device()` statement?


After all these, here is my code, for a simple MNIST example:
```
import matplotlib.pyplot as plt
import pandas as pd
import datetime as dt
import urllib.request, json
import os, sys
import numpy as np
sys.path.append('./stanford-tensorflow-tutorials/examples')
import utils
# This code has been tested with TensorFlow 1.6
import tensorflow as tf
image_size = 28
n_channels = 1
n_classes = 10
n_train = 55000
n_valid = 5000
n_test = 10000
n_epochs = 25
batch_size = 100
mnist_folder = './MNIST_data'
utils.download_mnist(mnist_folder)
train, val, test = utils.read_mnist(mnist_folder, flatten=True)
#mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)

config = tf.ConfigProto(device_count={""CPU"": 6}, # limit to num_cpu_core CPU usage  
                inter_op_parallelism_threads = 6,   
                intra_op_parallelism_threads = 6,
                allow_soft_placement=True,                
                log_device_placement=True)  
                
                
                
                
tf.reset_default_graph()
....
```
I am not sure if this `config` setting makes me be able to use 6 CPU cores. 

Then we move further:
## 2. multiple nodes with multiple CPU cores

I think this belongs to **distributed tensorflow** concept. Is this senario comparable to multiple GPUs running?

"
23213,"When using TFLite for network inference on mobile phone, the shallow layer of same model takes more time to infer than the deep layer.","**System information**
- Have I written custom code:
N/A
- OS Platform and Distribution:
Ubuntu14.04, Win 10, Android Studio 3.0.1, sdk version 26
- Mobile device:
Gome K1 (CPU: MTK MT6757,  2.3GHZ)
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
1.11.0
- Python version:
2.7
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
I am using the [offical demo(Java interface)](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/java/demo) of TFLite on Android, which classify the frame captured by the camera.  The model is offical released [Mobilenet_v1_1.0_224 ](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md). The inference time is about 240ms in my mobile phone. Now, I need to get the feature maps of the last convolution layer, so I get the new tflite interpreter based on [TF Lite converter ](https://www.tensorflow.org/lite/convert/cmdline_examples) and the frozen graphs in the offical released  [Mobilenet_v1_1.0_224 ](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md), the comandline that I used is:
`tflite_convert --graph_def_file=mobilenet_v1_1.0_224_frozen.pb --output_file=mobilenet_v1_1.0_224.tflite --input_shapes=1,224,224,3 --input_array=input --output_arrays=MobilenetV1/MobilenetV1/Conv2d_13_pointwise/BatchNorm/FusedBatchNorm`
When I use the new tflite interpreter for inference to get the feature maps of the last conv layer, **the inference time is 320ms, which is more than the original 240ms when get the prob.** This makes me very confused, since the global avg pooling and FC is no longer needed when using the new interpreter to get the feature map of last conv layer and the compution costs is decreased, so the inference time should not increase. By the way, the code for time measurement remains unchanged, which is:
```
// Here's where the magic happens!!!
long startTime = SystemClock.uptimeMillis();
runInference();
long endTime = SystemClock.uptimeMillis();
Log.d(TAG, ""Timecost to run model inference: "" + Long.toString(endTime - startTime));
```
I also test the inference time when I get the feature maps of the middle conv layers: 
--------layer--------------------shape of feature map------------inference time(ms)
-------prob------------------------1x1001-------------------------------~240
conv2d_13_pointwise-----------7x7x1024------------------------------~320
conv2d_10_pointwise-----------14x14x512----------------------------~340
conv2d_6_pointwise------------14x14x512----------------------------~260
conv2d_5_pointwise------------28x28x256----------------------------~380
conv2d_3_pointwise------------56x56x128----------------------------~600

**It seems that the larger the feature map is, the bigger the inference time is,** and the effect of depth on inferenc time is much smaller. 
So, What is the reason? Any help would be grateful. Thanks!

**Exact command to reproduce:**
N/A
"
23212,Ambiguity with tf.nn.embedding_lookup_sparse,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow 1.9.0
- Python 3.6.6
- Are you willing to contribute it (Yes/No): Yes


**Describe the feature and the current behavior/state.**
The tf.nn.embedding_lookup_sparse need two tf.SparseTensor (sp_ids and sp_weights) but I can't figure out why. Specifically, it seems to me that the columns of the indices of sp_ids and sp_weights are useless (see the example in the doc). This could be changed to use only one SparseTensor with:
- rows of indices pointing to the index of the element of the batch.
- columns of indices pointing to the index of the target element of the embedding (params).
- values corresponding to the weights. 

Small example of two operations doing the same things:
tf.sparse_tensor_dense_matmul(sparse_tensor, embeddings)
tf.nn.embedding_lookup_sparse(embeddings, sparse_ids, sparse_weights, ""sum"")
Here sparse_ids and sparse_weights have the same indices, the values of sparse_ids are the columns of indices of sparse_tensor and the values of weights are the values of  sparse_tensor. The three sparse tensors share the same rows of indices. 

ps: It might not work in higher dimensional embedding lookups, I just wanted to point out the fact that embedding_lookup and embedding_lookup_sparse, despite their close names, have actually a very different usability. 


**Will this change the current api? How?**
I can't really say.  tf.nn.embedding_lookup_sparse(params, sp_ids, sp_weights...) could become tf.nn.embedding_lookup_sparse(params, sparse_ids...) to be more consistent with tf.nn.embedding_lookup.


**Who will benefit with this feature?**
I don't really know. 


**Any Other info.**

Have I written custom code: no
OS Platform and Distribution: linux, but not relevant
TensorFlow installed from: pip
TensorFlow version: 1.9.0
Bazel version: ?
CUDA/cuDNN version: no CUDA
GPU model and memory: no GPU
Exact command to reproduce: See above
Mobile device: not relevant
"
23210,Different inference time on my computer and server with the same gpu,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Win10

- TensorFlow installed from (source or binary):
anaconda3

- TensorFlow version (use command below):
tensorflow-gpu1.11.0

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A

- Python version:
python3.6.6

- Bazel version (if compiling from source): N/A

- GCC/Compiler version (if compiling from source): N/A

- CUDA/cuDNN version:
CUDA9.0

- GPU model and memory:
NVIDIA GeForce GTX 1080 Ti  11264MiB

**Describe the current behavior**
I have trained a 2 classified ResNet18， when I use it for inference on different machine, the time is as follow:
about 2.0s on my conputer，
about 1.6s on my workmate's computer (win10,  tensorflow-gpu of version 1.9, python 3.6.6, CUDA9.0,  NVIDIA GeForce GTX 1080 Ti 11264MiB)， 
about 0.65s on a server (Ubuntu16.04, tensorflow-gpu of version 1.9, python 3.6,2, CUDA9.0, NVIDIA GeForce GTX 1080 Ti 11171MiB), 
about 0.22s on another server (Ubuntu16.04,  tensorflow-gpu of version 1.8, python 2.7.13,  CUDA9.0, NVIDIA GeForce GTX 1080 Ti 11171MiB). 
The code is exactly the same, only change is the model path and data path. Why there is so huge difference?

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
model_path = 'D:/proj_tf/models_pb/slice_cor.pb'
image_path = 'D:/proj_tf/data/'
files_in = ['0205_256_06.raw', '0205_256_07.raw', '0205_256_08.raw']
data_image = np.zeros((3, 256, 256, 3), dtype=np.float32)
for index in range(3):
    image_file = os.path.join(image_path, files_in[index])
    data_tmp = np.fromfile(image_file, dtype=np.float32)
    data_tmp.shape = 256, 256
    min_value = data_tmp.min()
    max_value = data_tmp.max()
    data_tmp -= min_value
    data_tmp /= (max_value - min_value)
    for i in range(3):
        data_image[index, :, :, i] = data_tmp
    
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)
config.gpu_options.per_process_gpu_memory_fraction = 0.7
sess = tf.Session(config=config)
with gfile.FastGFile(model_path, 'rb') as f:
    graph_def = tf.GraphDef()
    graph_def.ParseFromString(f.read())
    sess.graph.as_default()
    tf.import_graph_def(graph_def, name="""")
    
    sess.run(tf.global_variables_initializer())
    inp = sess.graph.get_tensor_by_name('data:0')
    out = sess.graph.get_tensor_by_name('softmax:0')
    
    start_time = time.time()
    pred_slice = sess.run(out, {inp: data_image})
    end_time = time.time()
    
    print(""time for inference : "", end_time - start_time)
    sess.close()   
```

"
23209,expect delta_delta op for speech,"
delta_delta is common in speech task, expected to be implemented. And also expect to expand the feature op of audio process in future.

**System information**
- TF1.11
- No


**Describe the feature and the current behavior/state.**
add delta_detla Op to audio feature 


**Who will benefit with this feature?**
people  who doing ASR.

**Any Other info.**
"
23208,TfLiteCameraDemo.apk with NNAPI,Can someone please upload a version of TfLiteCameraDemo.apk that supports NNAPI?
23207,Add an option to apply SavedModel via command line ,"**System information**
- TensorFlow version (you are using): TF-1.11
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
By default TensorFlow has an option to store checkpoint by adding '--train_dir'. However this is good when we want to continue training, but if we want to perform inference using TensorFlowServing it becomes a challenge.  Is it possible to add some flag which will store the variables, graph, in/output signature by just providing an option similar to '--train_dir'?


**Will this change the current api? How?**
Yes, but nothing will break 

for e.g. **Old API** will store only checkpoints
tf_cnn_benchmarks.py --model resnet50 --num_epochs=2 --batch_size 64 --data_name=imagenet  --data_dir=/mnt/data **--train_dir=/mnt/checkpoint**  --variable_update horovod --horovod_device gpu --weight_decay=1e-4 --use_fp16

**New API** should store using SaveModel
tf_cnn_benchmarks.py --model resnet50 --num_epochs=2 --batch_size 64 --data_name=imagenet  --data_dir=/mnt/data **--train_dir_save_model=/mnt/checkpoint**  --variable_update horovod --horovod_device gpu --weight_decay=1e-4 --use_fp16


**Who will benefit with this feature?**
By having such option, deploying the model to production will be super fast, even for the people who doesn't know anything about training code. Currently we have to either change the training code to use SaveModel or figure out a way to export graph along with variables. 

**Any Other info.**
N/A"
23205,static_rnn has no remind,"pip show tensorflow-gpu
Name: tensorflow-gpu
Version: 1.11.0
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author: Google Inc.
Author-email: opensource@google.com
License: Apache 2.0
Location: c:\anaconda\lib\site-packages
Requires: protobuf, termcolor, wheel, keras-applications, setuptools, absl-py, grpcio, six, gast, astor, numpy, keras-preprocessing, tensorboard

Lib/site-packages/tensorflow/nn/__init__.py has something wrong

```python
from tensorflow.python.ops.nn import static_rnn  wrong
from tensorflow.python.ops.rnn import static_rnn right
```"
23202,What are the differences between cold_start_filter and filter_continuation ?,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: Master
- Doc Link:
https://www.tensorflow.org/api_docs/python/tf/contrib/timeseries/saved_model_utils/cold_start_filter

**Describe the documentation issue**

It is not clear what are the differences between  cold_start_filter and filter_continuation. And in what condition they should be used? 

And for ""Filtering refers to updating model state based on new observations."" , is the model state == model ? why one should update it  ?
"
23201,CMake can't find cublas device,"**System information**
- Windows 10 Pro
- TensorFlow installed from source
- TensorFlow 1.11
- Python 3.6
- CMake 3.10.1
- CUDA 10/cuDNN 7.3.1
- NVIDIA Geforce 980GTX


I am trying to build Tensorflow (with GPU) under Windows 10 with CUDA 10 and cuDNN 7.3.1 using CMake. 
However, when I enable GPU, it cannot find following file: cublas_device.lib. It seems, this file is not included in CUDA 10 anymore. Is there another way to build with GPU support (if possible, with CUDA 10 and not 9)?

Here's the detailed error message

```
CMake Error: The following variables are used in this project, but they are set to NOTFOUND.
Please set them or make sure they are set and tested correctly in the CMake files:
CUDA_cublas_device_LIBRARY (ADVANCED)
    linked by target ""contrib_memory_stats_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""functional_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""sdca_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""candidate_sampling_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""user_ops_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""dataset_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""contrib_image_distort_image_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""contrib_seq2seq_beam_search_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""contrib_boosted_trees_training_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""ctc_ops_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""checkpoint_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""contrib_bigquery_reader_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""contrib_tensor_forest_stats_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""data_flow_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""contrib_layers_sparse_feature_cross_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""sdca_ops_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""candidate_sampling_ops_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""control_flow_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""contrib_factorization_clustering_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""summary_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""resource_variable_ops_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""contrib_image_sirds_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""contrib_boosted_trees_prediction_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""lookup_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""script_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""tf_core_gpu_kernels"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""grpc_tensorflow_server"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""contrib_data_dataset_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""audio_ops_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""array_ops_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""tf_tutorials_example_trainer"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""batch_ops_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""bitwise_ops_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""boosted_trees_ops_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""checkpoint_ops_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""control_flow_ops_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""cudnn_rnn_ops_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""data_flow_ops_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""dataset_ops_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""decode_proto_ops_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""resource_variable_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""encode_proto_ops_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""functional_ops_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""image_ops_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""sparse_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""io_ops_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""linalg_ops_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""list_ops_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""lookup_ops_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""logging_ops_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""user_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""manip_ops_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""math_ops_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""spectral_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""nn_ops_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""no_op_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""parsing_ops_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""random_ops_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""remote_fused_graph_ops_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""debug_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""rpc_ops_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""script_ops_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""set_ops_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""sendrecv_ops_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""sparse_ops_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""spectral_ops_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""state_ops_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""stateless_random_ops_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""string_ops_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""summary_ops_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""training_ops_gen_cc"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""tf_label_image_example"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""proto_text"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""ctc_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""compare_graphs"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""transform_graph"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""summarize_graph"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""benchmark_model"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""contrib_nearest_neighbor_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""audio_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""array_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""batch_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""bitwise_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""boosted_trees_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""math_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""cudnn_rnn_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""decode_proto_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""encode_proto_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""image_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""io_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""linalg_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""list_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""logging_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""nn_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""manip_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""parsing_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""random_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""remote_fused_graph_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""rpc_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""set_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""state_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""string_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""training_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""contrib_boosted_trees_model_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""contrib_boosted_trees_split_handler_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""contrib_boosted_trees_quantiles_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""contrib_boosted_trees_stats_accumulator_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""contrib_coder_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""contrib_factorization_factorization_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""contrib_framework_variable_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""contrib_input_pipeline_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""contrib_image_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""contrib_nccl_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""contrib_periodic_resample_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""contrib_resampler_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""contrib_rnn_gru_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""contrib_rnn_lstm_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""contrib_tensor_forest_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""contrib_tensor_forest_hybrid_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""contrib_tensor_forest_model_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""contrib_text_skip_gram_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""contrib_gcs_config_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
    linked by target ""stateless_random_ops_gen_python"" in directory C:/Users/kai/Documents/Libs/tensorflow/tensorflow/contrib/cmake
```

Thanks
"
23200,"r1.12: ""No module named enum""","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- Linux Ubuntu 16.04.1
- no mobile device
- TensorFlow installed from github
- TensorFlow version: r1.12
- Python version: 2.7
- Installed using virtualenv? pip? conda?: apt-get
- Bazel version (if compiling from source): 0.17.1
- GCC/Compiler version (if compiling from source): 5.4.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**
 File ""/home/amcp011/.cache/bazel/_bazel_amcp011/2a4bcc1a1f556f9e46df7e271eeaa164/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/ops/variables.py"", line 20, in <module>
    import enum  # pylint: disable=g-bad-import-order
ImportError: No module named enum

**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

ERROR: /home/amcp011/local/tensorflow/tensorflow/BUILD:533:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1)
Traceback (most recent call last):
  File ""/home/amcp011/.cache/bazel/_bazel_amcp011/2a4bcc1a1f556f9e46df7e271eeaa164/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 27, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""/home/amcp011/.cache/bazel/_bazel_amcp011/2a4bcc1a1f556f9e46df7e271eeaa164/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 70, in <module>
    from tensorflow.python.framework.framework_lib import *  # pylint: disable=redefined-builtin
  File ""/home/amcp011/.cache/bazel/_bazel_amcp011/2a4bcc1a1f556f9e46df7e271eeaa164/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/framework/framework_lib.py"", line 52, in <module>
    from tensorflow.python.framework.importer import import_graph_def
  File ""/home/amcp011/.cache/bazel/_bazel_amcp011/2a4bcc1a1f556f9e46df7e271eeaa164/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/framework/importer.py"", line 27, in <module>
    from tensorflow.python.framework import function
  File ""/home/amcp011/.cache/bazel/_bazel_amcp011/2a4bcc1a1f556f9e46df7e271eeaa164/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/framework/function.py"", line 36, in <module>
    from tensorflow.python.ops import resource_variable_ops
  File ""/home/amcp011/.cache/bazel/_bazel_amcp011/2a4bcc1a1f556f9e46df7e271eeaa164/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/ops/resource_variable_ops.py"", line 38, in <module>
    from tensorflow.python.ops import variables
  File ""/home/amcp011/.cache/bazel/_bazel_amcp011/2a4bcc1a1f556f9e46df7e271eeaa164/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/ops/variables.py"", line 20, in <module>
    import enum  # pylint: disable=g-bad-import-order
ImportError: No module named enum
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 128.659s, Critical Path: 101.44s
INFO: 72 processes: 72 local.
FAILED: Build did NOT complete successfully
"
23199,Problem mixing tf.cast and tf.control_dependencies,"**System information**
- Windows 10 1809.
- TensorFlow installed from (source or binary): binary.
- TensorFlow version (use command below): 1.11.0 (cpu).
- Python version: 3.6.6.

**Describe the current behavior**

Returns inf.

**Describe the expected behavior**

Should return 1.0.

**Code to reproduce the issue**
```
import tensorflow as tf

with tf.Session() as sess:
    n = tf.Variable(initial_value=0, dtype=tf.int32)
    v = tf.Variable(initial_value=1., dtype=tf.float32)

    inc_n = tf.assign_add(n, 1)
    foo = 1. / tf.cast(n, tf.float32)

    with tf.control_dependencies([inc_n]):
        test = tf.assign(v, foo) # doesn't work
        # test = tf.assign(v, 1. / tf.cast(n, tf.float32)) # works

    sess.run(tf.global_variables_initializer())
    print(sess.run(test))
```

**Other info / logs**

Also tested it on ubuntu 18, tf 1.11 gpu (from source, cuda 10), works fine.
"
23198,ResourceExhaustedError (see above for traceback): OOM when allocating tensor,"Hi, 
I want to feed CNN with my database. for this aim, I use ImageDataGenerator and I put my code here. but after implementing it produces this error:  
""ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[32,32,512,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[Node: conv2d_1/convolution = Conv2D[T=DT_FLOAT, data_format=""NCHW"", dilations=[1, 1, 1, 1], padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](conv2d_1/convolution-0-TransposeNHWCToNCHW-LayoutOptimizer, conv2d_1/kernel/read)]]""

my database has 3 folders( train, test and validation). train and validation have 7 classes. train contains 564 images and validation contains 191 images. it is smal database but I do not know why it produces this error! could you please guide me about this problem? I do not know how can I solve this?

```
import numpy as np
import matplotlib.pyplot as plt
from keras.preprocessing.image import ImageDataGenerator
from sklearn.utils import shuffle
from sklearn.cross_validation import train_test_split
from keras import backend as K
#K.set_image_dim_ordering('th')

from keras.utils import np_utils
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation, Flatten
from keras.layers.convolutional import Conv2D, MaxPooling2D
from keras.optimizers import SGD,RMSprop,adam


train_datagen = ImageDataGenerator(
        rescale=1./255,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True)
#
valid_datagen = ImageDataGenerator(
        rescale=1./255,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True)
#
test_datagen = ImageDataGenerator(rescale=1./255)
#
train_generator = train_datagen.flow_from_directory(
    directory=r""E:\databasetest\train"",
    target_size=(512, 512),
    color_mode=""grayscale"",
    batch_size=32,
    class_mode=""categorical"",
    shuffle=True,
    seed=42
)
#
valid_generator = valid_datagen.flow_from_directory(
   directory=r""E:\databasetest\validation"",
    target_size=(512, 512),
    color_mode=""grayscale"",
    batch_size=32,
    class_mode=""categorical"",
    shuffle=True,
    seed=42
)
#
test_generator = test_datagen.flow_from_directory(
    directory=r""E:\databasetest\test"",
    target_size=(512, 512),
    color_mode=""grayscale"",
    batch_size=16,
    class_mode=None,
    shuffle=False,
    seed=42
)
#
## neural network model
model = Sequential()
model.add(Conv2D(32, (3,3),border_mode='same', input_shape = (512, 512, 1), activation = 'relu'))
model.add(Activation('relu'))
model.add(Conv2D(32, 3, 3))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.5))

model.add(Conv2D(64, 3, 3))
model.add(Activation('relu'))
#model.add(Convolution2D(64, 3, 3))
#model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.5))

model.add(Flatten())
model.add(Dense(64))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(7))
model.add(Activation('softmax'))

model.summary()

model.compile(loss = 'categorical_crossentropy',
              optimizer = 'rmsprop',
              metrics = ['accuracy'])
STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size
STEP_SIZE_VALID=valid_generator.n//valid_generator.batch_size
model.fit_generator(generator=train_generator,
                    steps_per_epoch=STEP_SIZE_TRAIN,
                    validation_data=valid_generator,
                    validation_steps=STEP_SIZE_VALID,
                    epochs=10
)
```"
23196,sess.run() returns inconsistent results,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): b'v1.10.1-0-g4dcfddc' 1.10.1
- Python version: Python 3.6.6 :: Anaconda, Inc.
- Bazel version (if compiling from source): Build label: 0.16.1
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
- CUDA/cuDNN version: CUDA 8.0, CuDNN 7
- GPU model and memory: GeForce GTX 1080 Ti x 4


```

== cat /etc/issue ===============================================
Linux *** 4.15.0-36-generic #39~16.04.1-Ubuntu SMP Tue Sep 25 08:59:23 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.5 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux *** 4.15.0-36-generic #39~16.04.1-Ubuntu SMP Tue Sep 25 08:59:23 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy            1.14.5   
protobuf         3.6.1    
tensorflow       1.10.1   

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.10.1
tf.GIT_VERSION = b'v1.10.1-0-g4dcfddc'
tf.COMPILER_VERSION = b'v1.10.1-0-g4dcfddc'
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda/extras/CUPTI/lib64
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Wed Oct 24 04:18:19 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 396.44                 Driver Version: 396.44                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 108...  Off  | 00000000:05:00.0  On |                  N/A |
| 47%   67C    P0    80W / 250W |    144MiB / 11175MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GTX 108...  Off  | 00000000:06:00.0 Off |                  N/A |
| 76%   87C    P2   152W / 250W |   8428MiB / 11178MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
|   2  GeForce GTX 108...  Off  | 00000000:09:00.0 Off |                  N/A |
| 43%   64C    P2    81W / 250W |   4774MiB / 11178MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  GeForce GTX 108...  Off  | 00000000:0A:00.0 Off |                  N/A |
| 26%   43C    P0    73W / 250W |     12MiB / 11178MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      1426      G   /usr/lib/xorg/Xorg                            51MiB |
|    0      3507      G   /usr/lib/xorg/Xorg                            14MiB |
|    1      9564      C   python                                      5133MiB |
|    1     14381      C   /home/***/torch/install/bin/luajit         3283MiB |
|    2      7595      C   python                                      4611MiB |
|    2     12553      C   python                                       151MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/lib/python3.5/dist-packages/torch/lib/libcudart-5d6d23a3.so.8.0.61
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.7

```

**Describe the current behavior**
Consider following two source codes
```Python
import tensorflow as tf

img = tf.random_normal((10, 10, 3), dtype=tf.float32, seed=12345)
mask = tf.random_normal((10, 10, 1), dtype=tf.float32, seed=12345)

normalization_factor = tf.reduce_sum(mask) * 3
masked_img = img * mask
masked_img_sqr = img * masked_img # = img ^ 2 * mask

mean = tf.reduce_sum(masked_img) / normalization_factor
variance = tf.reduce_sum(masked_img_sqr) / normalization_factor
variance = variance - tf.square(mean)
adjusted_variance = tf.maximum(variance, 1. / normalization_factor)
stddev = tf.sqrt(variance)

img = (img - mean) / stddev
whiten_img = img

rand = tf.random_normal(
    shape=(10, 10, 3),
    mean=0., stddev=1., dtype=tf.float32,
    seed=12345, name='random_back')
img = img * mask + rand * (1 - mask)

img = tf.concat([img, mask], axis=-1)


debug_info = {
  'mean': mean,
  'variance': variance,
  'stddev': stddev,
  'adjusted_variance': adjusted_variance,
  'threshold': 1. / normalization_factor,
  'normalization_factor': normalization_factor,
  'whiten_img_minmax': [tf.reduce_min(whiten_img[..., :3]), tf.reduce_max(whiten_img[..., :3])],
  'minmax_mask': [tf.reduce_min(img[..., 3]), tf.reduce_max(img[..., 3])],
}

with tf.Session() as sess:
  print(sess.run(debug_info))
```

```Python
import tensorflow as tf

img = tf.random_normal((10, 10, 3), dtype=tf.float32, seed=12345)
mask = tf.random_normal((10, 10, 1), dtype=tf.float32, seed=12345)

normalization_factor = tf.reduce_sum(mask) * 3
masked_img = img * mask
masked_img_sqr = img * masked_img # = img ^ 2 * mask

mean = tf.reduce_sum(masked_img) / normalization_factor
variance = tf.reduce_sum(masked_img_sqr) / normalization_factor
variance = variance - tf.square(mean)
adjusted_variance = tf.maximum(variance, 1. / normalization_factor)
stddev = tf.sqrt(variance)

img = (img - mean) / stddev
whiten_img = img

rand = tf.random_normal(
    shape=(10, 10, 3),
    mean=0., stddev=1., dtype=tf.float32,
    seed=12345, name='random_back')
img = img * mask + rand * (1 - mask)

img = tf.concat([img, mask], axis=-1)


debug_info = {
  'mean': mean,
  'variance': variance,
  'stddev': stddev + 0,
  'adjusted_variance': adjusted_variance,
  'threshold': 1. / normalization_factor,
  'normalization_factor': normalization_factor,
  'whiten_img_minmax': [tf.reduce_min(whiten_img[..., :3]), tf.reduce_max(whiten_img[..., :3])],
  'minmax_mask': [tf.reduce_min(img[..., 3]), tf.reduce_max(img[..., 3])],
}

with tf.Session() as sess:
  print(sess.run(debug_info))
```

The only difference between them is `'stddev': stddev,` and `'stddev': stddev + 0,`

But the first code (with `'stddev': stddev,`) outputs
```
{..., 'stddev': 1.1902559, ...}
```
and the second (with `'stddev': stddev + 0,`) outputs
```
{..., 'stddev': 0.8401555, ...}
```

It seems like that the correct output is `'stddev': 0.8401555,`
But in the first code, `1./stddev = 1./0.8401555 = 1.1902559` was calculated

Here is the screenshot
![image](https://user-images.githubusercontent.com/5780122/47388961-15e7de80-d746-11e8-8959-f37c46e66c8b.png)


**Describe the expected behavior**

The outputs of two codes should be correct (consistent at least) 

**Code to reproduce the issue**

**Other info / logs**
"
23195,Segfault reading dataset more than once (`make_batched_features_dataset`),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **macOS 10.14 Mojave**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **NA**
- TensorFlow installed from (source or binary): **Binary / pip**
- TensorFlow version (use command below): **`('v1.11.0-rc2-4-gc19e29306c', '1.11.0')`**
- Python version: **2.7.10**
- Bazel version (if compiling from source): **NA**
- GCC/Compiler version (if compiling from source): **NA**
- CUDA/cuDNN version: **NA**
- GPU model and memory: **NA**

[Gist of full output of `./tools/tf_env_collect.sh](https://gist.github.com/brianmartin/2b43dca69453478ed33b49f1029e03fe)

**Exact command to reproduce:**

```
$ wget https://gist.githubusercontent.com/brianmartin/4f221eb838dce8099342f829fb13253d/raw/00c2a1736b9a292f8a6fb88164d240a766468744/gistfile1.txt
$ python gistfile1.txt
```

**Describe the current behavior**

Current behavior in 1.11.0, 1.12.0rcX is that the included script segfaults (11: SIGSEGV).

**Describe the expected behavior**

Expected behavior is no segfault. Version 1.10.1, 1.10.0, and 1.9.0 do not segfault. I have not checked lower versions.

**Code to reproduce the issue**

See a full script which reproduces this issue along two different code paths here: https://gist.github.com/brianmartin/4f221eb838dce8099342f829fb13253d

The segfault occurs when reading the dataset a second time. The first read works as expected.

For reference the two code samples which produce datasets which cause a segfault on second read are:

```py
    dataset = make_batched_features_dataset(file_pattern=data_file,
                                            batch_size=1,
                                            features=feature_spec)
```

I've also dug into `make_batched_features_dataset` and minified down to this repro:

```py
    from tensorflow.contrib.data.python.ops import parsing_ops
    from tensorflow.python.data.ops import readers

    dataset = Dataset.from_tensor_slices([data_file]) \
                     .interleave(readers.TFRecordDataset, cycle_length=1) \
                     .batch(batch_size=1) \
                     .apply(parsing_ops.parse_example_dataset(feature_spec))
```

------

Please let me know if there's anything else I can provide or help with! This is blocking us from upgrading to the latest Tensorflow version in [spotify/spotify-tensorflow](https://github.com/spotify/spotify-tensorflow)."
23192,Prevent Empty Checkpointable Data Structure Restores,"@allenlavoie

Issue tracking the ""bug"" that is caused when restoring a checkpoint where the Checkpointable object has new checkpointable data structures that do not contain any checkpointable objects themselves and therefore prevent restoration when using `load_status.assert_consumed()`.

*Example Code:*
```python
import tensorflow as tf
from tensorflow.python.training.checkpointable.tracking import Checkpointable


class Model(Checkpointable):

  def __init__(self):
    self.variable = tf.get_variable(""variable"", [2, 2])
    self.dict = {
        ""test"": 1,
        ""test2"": 2,
    }
    self.dict_var = {
        ""test"": 1,
        ""test2"": tf.get_variable(""dict_var"", [2, 2])
    }
    self.dict_nested_var = {
        ""test"": 1,
        ""test2"": {
            ""var"": tf.get_variable(""dict_nested_var"", [2, 2])
        }
    }
    self.list_var = [tf.get_variable(""list_var"", [2, 2])]


print('SAVE')
with tf.Graph().as_default():

  s = tf.Session()

  m = Model()

  s.run(tf.global_variables_initializer())

  c = tf.train.Checkpoint(model=m)

  c.save('checkpoints/', session=s)

print('RESTORE')
with tf.Graph().as_default():

  s = tf.Session()

  m = Model()

  m.dict_new = {
      ""test"": 1,
      ""test2"": 2
  }

  c = tf.train.Checkpoint(model=m)

  status = c.restore('checkpoints/-1')

  status.assert_consumed().run_restore_ops(s)
```"
23191,Build from source Java,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): r1.11
- Python version:3.6
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 0.18.0
- CUDA/cuDNN version: 10.0
- GPU model and memory: nVidia970M
- Have I written custom code: N/A
- TensorFlow version: r1.11
- Exact command to reproduce: `bazel build --config=opt --config=cuda //tensorflow/java:tensorflow //tensorflow/java:libtensorflow_jni`

Error when installing tensorflow with java, generates the following error lines

```
tensorflow/java/BUILD:54:1: Building tensorflow/java/libprocessor_library-class.jar (1 source file) failed: Worker process quit or closed its stdin stream when we tried to send a WorkRequest:

---8<---8<--- Exception details ---8<---8<---
java.io.IOException:
        at com.google.devtools.build.lib.windows.WindowsSubprocess.writeStream(WindowsSubprocess.java:252)
        at com.google.devtools.build.lib.windows.WindowsSubprocess.access$000(WindowsSubprocess.java:33)
        at com.google.devtools.build.lib.windows.WindowsSubprocess$ProcessOutputStream.write(WindowsSubprocess.java:52)
        at com.google.protobuf.CodedOutputStream$OutputStreamEncoder.doFlush(CodedOutputStream.java:3003)
        at com.google.protobuf.CodedOutputStream$OutputStreamEncoder.flushIfNotAvailable(CodedOutputStream.java:2998)
        at com.google.protobuf.CodedOutputStream$OutputStreamEncoder.writeUInt32NoTag(CodedOutputStream.java:2825)
        at com.google.protobuf.CodedOutputStream$OutputStreamEncoder.writeTag(CodedOutputStream.java:2670)
        at com.google.protobuf.CodedOutputStream$OutputStreamEncoder.writeMessage(CodedOutputStream.java:2774)
        at com.google.devtools.build.lib.worker.WorkerProtocol$WorkRequest.writeTo(WorkerProtocol.java:1009)
        at com.google.protobuf.AbstractMessageLite.writeDelimitedTo(AbstractMessageLite.java:98)
        at com.google.devtools.build.lib.worker.WorkerSpawnRunner.execInWorker(WorkerSpawnRunner.java:318)
        at com.google.devtools.build.lib.worker.WorkerSpawnRunner.actuallyExec(WorkerSpawnRunner.java:160)
        at com.google.devtools.build.lib.worker.WorkerSpawnRunner.exec(WorkerSpawnRunner.java:115)
        at com.google.devtools.build.lib.exec.AbstractSpawnStrategy.exec(AbstractSpawnStrategy.java:106)
        at com.google.devtools.build.lib.exec.AbstractSpawnStrategy.exec(AbstractSpawnStrategy.java:75)
        at com.google.devtools.build.lib.exec.SpawnActionContextMaps$ProxySpawnActionContext.exec(SpawnActionContextMaps.java:362)
        at com.google.devtools.build.lib.analysis.actions.SpawnAction.internalExecute(SpawnAction.java:288)
        at com.google.devtools.build.lib.analysis.actions.SpawnAction.execute(SpawnAction.java:295)
        at com.google.devtools.build.lib.skyframe.SkyframeActionExecutor.executeActionTask(SkyframeActionExecutor.java:994)
        at com.google.devtools.build.lib.skyframe.SkyframeActionExecutor.prepareScheduleExecuteAndCompleteAction(SkyframeActionExecutor.java:923)
        at com.google.devtools.build.lib.skyframe.SkyframeActionExecutor.access$800(SkyframeActionExecutor.java:121)
        at com.google.devtools.build.lib.skyframe.SkyframeActionExecutor$ActionRunner.call(SkyframeActionExecutor.java:763)
        at com.google.devtools.build.lib.skyframe.SkyframeActionExecutor$ActionRunner.call(SkyframeActionExecutor.java:718)
        at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
        at com.google.devtools.build.lib.skyframe.SkyframeActionExecutor.executeAction(SkyframeActionExecutor.java:471)
        at com.google.devtools.build.lib.skyframe.ActionExecutionFunction.checkCacheAndExecuteIfNeeded(ActionExecutionFunction.java:505)
        at com.google.devtools.build.lib.skyframe.ActionExecutionFunction.compute(ActionExecutionFunction.java:215)
        at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:418)
        at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:368)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.base/java.lang.Thread.run(Unknown Source)
---8<---8<--- End of exception details ---8<---8<---

---8<---8<--- Start of log, file at C:/users/eduar/_bazel_eduar/gr3cukxm/bazel-workers/worker-0-Javac.log ---8<---8<---
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.
Unrecognized VM option 'CompactStrings'
---8<---8<--- End of log ---8<---8<---
INFO: Elapsed time: 281.132s, Critical Path: 94.67s
INFO: 602 processes: 602 local.
FAILED: Build did NOT complete successfully
```
"
23189,text8.zip,"Hello Guys
previously the code was working on my MacBook, now it doesn't work and I don't understand why. I have the error below:

Traceback (most recent call last):
  File ""/Users/payam/Desktop/tensorflow/tensorflow/examples/tutorials/word2vec/word2vec_basic.py"", line 73, in <module>
    filename = maybe_download('text8.zip', 31344016)
  File ""/Users/payam/Desktop/tensorflow/tensorflow/examples/tutorials/word2vec/word2vec_basic.py"", line 69, in maybe_download
    raise Exception('Failed to verify {0}. Can you get to it with a browser?'.format(local_filename))
Exception: Failed to verify /var/folders/bp/cd99npls7q7g9y8f34lhwnq80000gn/T/text8.zip. Can you get to it with a browser?
9207808

Process finished with exit code 1"
23187,Error : Check failed: IsConstantParameterArray,"**System information**

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04.5 LTS 
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.13.0-dev20181023
Python version: 
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 9.0.176 / 9
GPU model and memory: GeForce GTX 1080ti / 11175MiB

**Describe the current behavior**

I want to convert a custom graph from TF to TFLite using 'TFLiteconverter'
but I faced below error:

> tensorflow.contrib.lite.python.convert.ConverterError: TOCO failed. See console for info.
> 2018-10-23 10:13:26.786330: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 784 operators, 1332 arrays (0 quantized)
> 2018-10-23 10:13:26.797260: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 511 operators, 912 arrays (0 quantized)
> 2018-10-23 10:13:26.806327: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 511 operators, 912 arrays (0 quantized)
> 2018-10-23 10:13:26.806920: F tensorflow/contrib/lite/toco/graph_transformations/resolve_batch_normalization.cc:45] Check failed: IsConstantParameterArray(*model, bn_op->inputs[1]) && IsConstantParameterArray(*model, bn_op->inputs[2]) && IsConstantParameterArray(*model, bn_op->inputs[3]) Batch normalization resolution requires that mean, multiplier and offset arrays be constant.
> Aborted (core dumped)
> 

How can I fix it?

**Code to reproduce the issue**

```
import tensorflow as tf

graph_def_file = ""/path/graph.pb""
input_arrays = [""Placeholder""]
output_arrays = [""final""]

converter = tf.contrib.lite.TFLiteConverter.from_frozen_graph(
  graph_def_file, input_arrays, output_arrays)
converter.inference_type = tf.contrib.lite.constants.FLOAT
tflite_model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(tflite_model)
```
"
23185,Eager execution: variables can't be iterated (unlike Tensor),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): pip binary
- TensorFlow version (use command below): v1.10.0-0-g656e7a2b34 1.10.0
- Python version: 3.6
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA


**Describe the current behavior**
**Describe the expected behavior**

Variable is expected to be iterable, just as EagerTensor, but right now it raises TypeErrors

**Code to reproduce the issue**
```python
import tensorflow as tf
import numpy
tf.enable_eager_execution()
x = tf.contrib.eager.Variable(numpy.zeros([3, 4, 5]))
print(type(x + 0))
list(x + 0) # ok
print(type(x))
list(x) # TypeError: 'Variable' object is not iterable.
```

Output:
```
<class 'EagerTensor'>
<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>
```

Exact command to reproduce: 
```
python script.py
```
(to please tensorflowbuttler)"
23184,CUDA 9.1 and Tensorflow GPU 1.5.0 Visual Studio 2017 and Python 3.6 error,"I installed CUDA 9.1, cuDNN 7.31 for CUDA 9.1 and I have installed Visual Studio 2017. However when I run the CUDA samples I get these following outputs:
![capture](https://user-images.githubusercontent.com/24243687/47340754-d2955d80-d6bc-11e8-8f8c-c4dad2375cf6.PNG)
![3](https://user-images.githubusercontent.com/24243687/47340755-d45f2100-d6bc-11e8-8223-2e3146168de0.PNG)

But when I try to fit my model using tensorflow-gpu it show ""python.exe"" stopped working and then when I click Debug, I get this output inside my Visual Studio 2017:
![2](https://user-images.githubusercontent.com/24243687/47340798-ee006880-d6bc-11e8-8197-b50eb52d57c9.PNG)

Any idea what might fix this issue?"
23183,Provide Clear Instructions to build tensorflow from source,"Hi
  The instructions to build tensorflow from source are not clear, simple or easy to understand.

I would like a concise set of steps to build tensorflow from source for ubuntu 16.04

Thanks+"
23182,Different results got using different batch sizes (cannot fully fit into GPU memory therefore evaluating different batch each time),"**System information**
- Have I written custom code: **Yes**
- OS Platform and Distribution: **Linux Ubuntu 18.04.1** 
- Mobile device: N/A
- TensorFlow installed from: **binary, tensorflow-gpu from anaconda**
- TensorFlow version: **b'unknown', 1.11.0**
- Python version: **3.6.6**
- Bazel version: N/A
- GCC/Compiler version: N/A
- CUDA/cuDNN version: **10.0, Driver Version: 410.66**
- GPU model and memory: **TITAN Xp, 12195MiB**

**Describe the current behavior**
I'm trying to batch evaluate MLP (with same structure but different parameter) on GPU. Since the whole batch is too big to fit into GPU memory, I have to create smaller batches from the whole batch. Since the size of whole batch is not divisible by the size of little batch, the last little batch will be smaller than usual. It turns out the results in the last small batch is way smaller than previous batches.

**Describe the expected behavior**
The result should be the same since batch size should not affect the result itself.

**Exact command to reproduce**
Put any of the following examples in a `jupyter` cell, and run it
Put any of the following examples in `xxx.py`, and run `python xxx.py` should also be fine

**Code to reproduce the issue**
A small example:

    import tensorflow as tf
    import numpy as np
    with tf.Session():
        x = tf.Variable(np.random.rand(43, 60000).astype('f2'))
        w = tf.Variable(np.ones((58624, 43), 'f2'))
        b = tf.Variable(np.ones((58624, 1), 'f2'))
        v1 = w @ x + b
        w = tf.Variable(np.ones((5376, 43), 'f2'))
        b = tf.Variable(np.ones((5376, 1), 'f2'))
        v2 = w @ x + b
        tf.global_variables_initializer().run()
        print(v1.eval(), v2.eval()) # they should have the same value while they dont

The actual example:

    import tensorflow as tf
    import numpy as np
    with tf.Session():
        x = tf.Variable(np.ones((43, 60000), 'f2'))
        y = tf.Variable(np.ones((8, 60000), 'f2'))
        w = tf.Variable(np.ones((916* 64, 43), 'f2'))
        b = tf.Variable(np.ones((916* 64, 1), 'f2'))
        v = tf.Variable(np.ones((916, 8, 64), 'f2'))
        c = tf.Variable(np.ones((916, 8, 1), 'f2'))
        r = tf.linalg.norm(tf.cast(v@tf.reshape(tf.tanh(w@x+b),(916,64,60000))+c,'float32'),axis=(1,2))# prevent overflow
        w = tf.Variable(np.ones((84* 64, 43), 'f2'))
        b = tf.Variable(np.ones((84* 64, 1), 'f2'))
        v = tf.Variable(np.ones((84, 8, 64), 'f2'))
        c = tf.Variable(np.ones((84, 8, 1), 'f2'))
        R = tf.linalg.norm(tf.cast(v@tf.reshape(tf.tanh(w@x+b),(84,64,60000))+c,'float32'),axis=(1,2)) # prevent overflow
        tf.global_variables_initializer().run()
        print(r.eval()[::100], R.eval()[::10]) # summary, they should have the same value while they dont

**Other info / logs**
I created a stack overflow question (https://stackoverflow.com/questions/52921884/tensorflow-result-inconsistent-across-batch-size) but since no body solved the problem, I submitted an issue here. It seems that this problem does not occur with small batches, it sometimes is not stably reproducible "
23181,"how to get the Projective transform matrix ,which will be used to tf.contrib.image.transform??","i wanna do Projective transform in tensorflow, but tf.contrib.image.transform needs some args as follows:
'''
images,
transforms,
interpolation='NEAREST',
name=None
''''
i dont know how to get the transforms matrix.
OPENCV has this function: getPerspectiveTransform(pts0,pts1),but i cant find Similar function in tensorflow.
"
23179,Introduce SamplingDatasetOp to TensorFlow,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (1.10):
- Are you willing to contribute it (Yes):



**Describe the feature and the current behavior/state.**
I propose to introduce a SamplingDatasetOp to TensorFlow
**Will this change the current api? How?**
Yes.
User would be able to create sampling dataset that output a subset of input dataset that is uniformed sampled from input dataset, with sample rate supplied as parameter.
`dataset = dataset.sampling(rate)`
**Who will benefit with this feature?**
Minigo training have very slow filling rate. https://github.com/tensorflow/minigo/issues/520
This is due to Minigo use a FilterDatasetOp to filter out subset of input data, which has low efficiency.  With sampling dataset, we can implement the sample action inside the sampling dataset op, without call predicate and go through lambda function explicitly, which improves training performance by 1.7x.  More details can be seen in the issue for minigo above.
**Any Other info.**
"
23178,"Cannot build tensorflow from source, Ubuntu 16.04","uname
Linux olympus 4.15.0-36-generic #39~16.04.1-Ubuntu SMP Tue Sep 25 08:59:23 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

CUDA 10

Trying to run ./configure to build tensorflow for cuda 10 but it doesn't work. bazel is complaining about many things..

The function 'set' has been removed in favor of 'depset', please use the latter. You can temporarily refer to the old 'set' constructor from unexecuted code by using --incompatible_disallow_uncalled_set_constructor=false

ERROR: error loading package '': Extension 'closure/filegroup_external.bzl' has errors

What are the exact steps to build successfully from source for linux?"
23177,"in mnist_with_summaries.py,the function act(), what mean,,where define","https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py#L97
      activations = act(preactivate, name='activation')

"
23176,keras.layers.Concatenate could support list inputs with length 1,"**System information**
- TensorFlow version (you are using):
```
$ pip freeze | grep tensorflow
tensorflow==1.12.0rc1
tensorflow-estimator==1.10.12
```
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
Currently, the `tf.keras.layers.Concatenate` only supports [list inputs with length >= 2](https://github.com/tensorflow/tensorflow/blob/e5c17aef836f8b85591cdcae31fbb66ddcf8185a/tensorflow/python/keras/layers/merge.py#L378).

Considering that the vanilla `tf.concat` works with inputs of length 1, it would be nice if the keras layers implemented the same behavior.

```python
import numpy as np
import tensorflow as tf


def test_tensorflow_concatenate(inputs):
    tf.concat(inputs, axis=-1)

    print(""tf.concat works with {} inputs"".format(len(inputs)))


def test_concatenate_layer_with_inputs(inputs):
    model = tf.keras.Sequential((
        tf.keras.layers.Concatenate(axis=-1),
        tf.keras.layers.Dense(32)))

    feed_dict = {
        input_: np.random.uniform(
            0, 1, (3, *input_.shape[1:].as_list()))
        for input_ in inputs
    }
    output = model(inputs)
    output_eval = tf.keras.backend.get_session().run(
        output, feed_dict=feed_dict)
    output_np = model.predict([feed_dict[key] for key in inputs])

    assert np.allclose(output_eval, output_np)

    print(""tf.keras.layers.Concatenate with {} inputs"".format(len(inputs)))


def main():
    input1 = tf.keras.layers.Input((1, ))
    input2 = tf.keras.layers.Input((2, ))

    test_tensorflow_concatenate([input1, input2])
    test_tensorflow_concatenate([input1])

    test_concatenate_layer_with_inputs([input1, input2])
    test_concatenate_layer_with_inputs([input1])


if __name__ == '__main__':
    main()
```


**Will this change the current api? How?**
No ValueErrors would be raised when calling the `tf.keras.layers.Concatenate` with input list of length 1.

**Who will benefit with this feature?**
Anyone who dynamically creates multi-input/-output keras models.

**Any Other info.**
n/a"
23172,Error using cohen_kappa metric with tf.contrib.distribute.MirroredStrategy configured estimator,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): ('v1.11.0-0-gc19e29306c', '1.11.0')
- Python version: 2.7.12
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 9.0.176/7.2.1.38
- GPU model and memory: 4x Nvidia V100 16GB

**Describe the current behavior**
When using the `tf.contrib.metrics.cohen_kappa` metric with an estimator, a `TypeError` is raised when calling the `evaluate` method on the estimator if the evaluation is configured to be distributed with `tf.contrib.distribute.MirroredStrategy`. The evaluation is successful if the estimator is not configured for distributed evaluation.

**Describe the expected behavior**
The metric `tf.contrib.metrics.cohen_kappa` should be calculated successfully in both distributed and non-distributed evaluations.

**Code to reproduce the issue**
test.py:
```python
import tensorflow as tf

DO_EVAL_DIST = True

tf.logging.set_verbosity(tf.logging.INFO)

def model_fn(features, labels, mode):
    layer = tf.layers.Dense(2)
    logits = layer(features)

    if mode == tf.estimator.ModeKeys.PREDICT:
        predictions = {""logits"": logits}
        return tf.estimator.EstimatorSpec(mode, predictions=predictions)

    loss = tf.losses.mean_squared_error(
        labels=labels, predictions=tf.reshape(logits, [-1]))

    if mode == tf.estimator.ModeKeys.EVAL:
        class_targets = tf.reshape(tf.argmax(labels, axis=-1, name='class_targets'), [-1])
        preds = tf.reshape(tf.argmax(logits, axis=-1, name='preds'), [-1])
        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops={
            'cohen_k': tf.contrib.metrics.cohen_kappa(class_targets, preds, 2),
        })

    if mode == tf.estimator.ModeKeys.TRAIN:
        train_op = tf.train.GradientDescentOptimizer(0.2).minimize(loss, global_step=tf.train.get_global_step())
        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)

def input_fn():
    features = tf.data.Dataset.from_tensors([[1.]]).repeat()
    labels = tf.data.Dataset.from_tensors([1., 0.]).repeat()
    return tf.data.Dataset.zip((features, labels))

# Configured the estimator for either distributed or non-distributed evaluation
# Training is always distributed
if DO_EVAL_DIST:
    distribution = tf.contrib.distribute.DistributeConfig(
        train_distribute=tf.contrib.distribute.MirroredStrategy(),
        eval_distribute=tf.contrib.distribute.MirroredStrategy()
    )
    config = tf.estimator.RunConfig(experimental_distribute=distribution)
else:
    distribution = tf.contrib.distribute.MirroredStrategy()
    config = tf.estimator.RunConfig(train_distribute=distribution)

classifier = tf.estimator.Estimator(model_fn=model_fn, config=config, model_dir='out_test')

print('********* start train **********')
classifier.train(input_fn=input_fn, steps=1000)
print('********* end train/start eval **********')
classifier.evaluate(input_fn=input_fn, steps=1000)
print('********* end eval **********')
```

**Other info / logs**
Error thrown when using distributed evaluation, when `DO_EVAL_DIST = True`:
```python
Traceback (most recent call last):
  File ""test.py"", line 49, in <module>
    classifier.evaluate(input_fn=input_fn, steps=1000)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 474, in evaluate
    return _evaluate()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 469, in _evaluate
    output_dir=self.eval_dir(name))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 1528, in _evaluate_run
    config=self._session_config)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/evaluation.py"", line 212, in _evaluate_once
    session.run(eval_ops, feed_dict)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 783, in __exit__
    self._close_internal(exception_type)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 816, in _close_internal
    h.end(self._coordinated_creator.tf_sess)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/basic_session_run_hooks.py"", line 941, in end
    self._final_ops, feed_dict=self._final_ops_feed_dict)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 887, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1095, in _run
    self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 429, in __init__
    self._fetch_mapper = _FetchMapper.for_fetch(fetches)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 249, in for_fetch
    return _DictFetchMapper(fetch)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 387, in __init__
    _FetchMapper.for_fetch(fetch) for fetch in fetches.values()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 255, in for_fetch
    return _ElementFetchMapper(fetches, contraction_fn)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 288, in __init__
    (fetch, type(fetch), str(e)))
TypeError: Fetch argument PerDevice({'/replica:0/task:0/device:GPU:0': <tf.Tensor 'cohen_kappa/value/Merge:0' shape=() dtype=float64>, '/replica:0/task:0/device:GPU:1': <tf.Tensor 'tower_1/cohen_kappa/value/Merge:0' shape=() dtype=float64>, '/replica:0/task:0/device:GPU:2': <tf.Tensor 'tower_2/cohen_kappa/value/Merge:0' shape=() dtype=float64>, '/replica:0/task:0/device:GPU:3': <tf.Tensor 'tower_3/cohen_kappa/value/Merge:0' shape=() dtype=float64>}) has invalid type <class 'tensorflow.contrib.distribute.python.values.PerDevice'>, must be a string or Tensor. (Can not convert a PerDevice into a Tensor or Operation.)
```"
23170,Switch from Deprecated to Current Docker Images on Docker Hub,"Tensorflow Dockerfiles are [currently located](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/dockerfiles/dockerfiles) at `tensorflow/tensorflow/tools/dockerfiles/dockerfiles/`. The parent directory (`tensorflow/tensorflow/tools/dockerfiles/`) includes instructions to build the images from that directory. This is done so the Dockerfiles have the `bashrc` file in their build context to `COPY` into containers at runtime. 

There are another set of Dockerfiles at `tensorflow/tensorflow/tools/docker/` which are marked deprecated/for-deletion. These deprecated Dockerfiles are the ones currently hosted on [Docker Hub](https://hub.docker.com/r/tensorflow/tensorflow/).

I propose Tensorflow switch their publicly available Docker images from the deprecated set to current ones. With minimal upfront effort, all eight current images could be hosted on Docker Hub and automatically rebuilt with each repo update.

This transition could be achieved in two main steps:

1 Move `tensorflow/tensorflow/tools/dockerfiles/bashrc` into the `dockerfiles` child directory at `tensorflow/tensorflow/tools/dockerfiles/dockerfiles/` to give the Dockerfiles access to the file in their default build context.
 - This will require a few minor relative path updates, such as changing the `COPY` path in `tensorflow/tensorflow/tools/dockerfiles/assembler.Dockerfile`

```diff
- COPY  bashrc /etc/bash.bashrc
+ COPY dockerfiles/bashrc /etc/bash.bashrc
```

2 Now, Docker Hub can be easily configured to link to the Tensorflow account, host the current Docker images and rebuild them whenever they're changed. Using this Docker Hub Build Setting image for context:

![graph_nets_dockerhub_build_settings](https://user-images.githubusercontent.com/21133719/47311058-03f72600-d5f6-11e8-8827-8411c2fb641b.png)

I recommend Tensorflow set up Docker Hub automatic build settings to the `tensorflow/tensorflow` repo with the following location:tag pairs:

| Dockerfile Location                                                                          |  Docker Tag Name          |
|----------------------------------------------------------------------------------------|----------------------------------|
| `tools/dockerfiles/dockerfiles/cpu.Dockerfile`                                  | latest                               |
| `tools/dockerfiles/dockerfiles/cpu-devel.Dockerfile`                        | latest-devel                      |
| `tools/dockerfiles/dockerfiles/cpu-jupyter.Dockerfile`                      | latest-jupyter                    |
| `tools/dockerfiles/dockerfiles/cpu-devel-jupyter.Dockerfile`            | latest-devel-jupyter          |
| `tools/dockerfiles/dockerfiles/nvidia.Dockerfile`                              | latest-gpu                        |
| `tools/dockerfiles/dockerfiles/nvidia-devel.Dockerfile`                    | latest-gpu-devel               |
| `tools/dockerfiles/dockerfiles/nvidia-jupyter.Dockerfile`                  | latest-gpu-jupyter             |
| `tools/dockerfiles/dockerfiles/nvidia-devel-jupyter.Dockerfile`        | latest-gpu-devel-jupyter   |

I have already signed the individual CLA for a previous commit and would be happy to implement these code changes -- someone with access to Tensorflow's Docker Hub account would have to enter the automated build settings."
23166,"""Invalid loop structure. Mismatched parent frames""","Computing the gradient through a batch normalization layer as implemented in keras with a batch size of None while in a tf.while loop can cause the following error:

""tensorflow.python.framework.errors_impl.InvalidArgumentError: Invalid loop structure: Mismatched parent frames for ""while/while_context"": ""while/while_context"" vs """". This is an internal bug, please file a bug report with instructions on how to reproduce the error""

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution: Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): Reproducible using the latest release [v1.12.0-rc1-0-g7b08198113 1.12.0-rc1] through back to at least 1.10.0.
- Python version: 3.5.2
- CUDA/cuDNN version: Reproducible without CUDA.
- GPU model and memory: Reproducible without any GPUs.

Current Behavior: running the below script throws a really bad error.

Expected Behavior: it crashes, but not with this error.

**Code to reproduce the issue**

This is the minimal example that I can get to cause a crash. It isn't ""correct"" code in that it shouldn't work, but it shouldn't crash because of mismatched parent frames. Code that I believe is actually correct also causes the crash, but is much longer.

```python
import numpy as np
import tensorflow as tf
### MUST be keras, not from tensorflow import keras
import keras

if __name__ == ""__main__"":
  sess = keras.backend.get_session()

  ### I can't reproduce this with tf.layers.batch_normalization
  model = keras.models.Sequential([keras.layers.BatchNormalization(input_shape=(1,))])
  ### This next line MUST have the None, otherwise it doesn't crash
  x = tf.placeholder(tf.float32, (None, 1))

  eta = tf.zeros(tf.shape(x))
  def cond(i, _):
    return tf.less(i, 10)
  def body(i, e):
    preds = model(x+e)
    return i + 1, tf.gradients(preds, x)[0]

  _, eta = tf.while_loop(cond, body, [tf.zeros([]), eta])

  sess.run(eta, {x: np.zeros((128,1))})
```

**Other info / logs**

```
2018-10-22 17:21:30.132455: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Traceback (most recent call last):
  File ""/home/ncarlini/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1334, in _do_call
    return fn(*args)
  File ""/home/ncarlini/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/home/ncarlini/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InternalError: Invalid loop structure: Mismatched parent frames for ""while/while_context"": ""while/while_context"" vs """". The node giving this error: {{node while/gradients/while/sequential_1/batch_normalization_1/cond/batchnorm/mul_1_grad/Shape/Enter}}This is an internal bug, please file a bug report with instructions on how to reproduce the error.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""bug.py"", line 26, in <module>
    sess.run(eta, {x: np.zeros((128,1))})
  File ""/home/ncarlini/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 929, in run
    run_metadata_ptr)
  File ""/home/ncarlini/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/ncarlini/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1328, in _do_run
    run_metadata)
  File ""/home/ncarlini/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Invalid loop structure: Mismatched parent frames for ""while/while_context"": ""while/while_context"" vs """". The node giving this error: node while/gradients/while/sequential_1/batch_normalization_1/cond/batchnorm/mul_1_grad/Shape/Enter (defined at bug.py:22) This is an internal bug, please file a bug report with instructions on how to reproduce the error.
```"
23165,TensorRT engine binding error,"*System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Jetson TX2, Linux4Tegra Xenial 16.04
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 1.8.0
- Python version: 2.7.12
- Bazel version (if compiling from source): 0.18.0
- GCC/Compiler version (if compiling from source): gcc5
- CUDA/cuDNN version: 9.0/7.1.5
- GPU model and memory: Jetson tx2 8GB


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**:

I have frozen a graph that contains the following operations:

* tf.layers.conv2d(*args, **kwargs)
* tf.layers.batch_normalization(net, **batch_norm)

I will try to provide a minimal graph that have this problem.
**Describe the expected behavior**:
I create a trt_graph by using the function trt.create_inference_graph and it creates a graph successfully  but whenever I try to make an inference I encounter:

```
2018-10-22 15:29:17.537843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with│
 363 MB memory) -> physical GPU (device: 0, name: NVIDIA Tegra X2, pci bus id: 0000:00:00.0, compute capability: 6.2)                                            │
2018-10-22 15:29:20.426303: F tensorflow/contrib/tensorrt/shape_fn/trt_shfn.cc:76] TensorRT engine cannot find binding: ModelBase/Conv2dBatchNorm/Relu     
Aborted (core dumped)
```
Any guideline on how to provide a better log issue?

Thanks


"
23164,Error exporting to TFlite,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.1 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 1.10.1
- Python version: 
- Bazel version (if compiling from source):0.16.1
- GCC/Compiler version (if compiling from source):.3.0
- CUDA/cuDNN version: 9.1/7.1.2
- GPU model and memory: AWS EC2 g2.2 K520

**Describe the current behavior**
I want to reuse one of the classification checkpoints in mobilenet v2 as feature extractor in the object detection pipeline.

I started with a classification checkpoint mobilenet_v2_1.0_224 for object detection retraining.
I made modifications to ssd_mobilenet_v2_coco pipeline.config. Specifically changed `fine_tune_checkpoint_type` from `detection` to `classification` 
I uploaded the modified [pipeline.config](https://github.com/tensorflow/tensorflow/files/2502108/pipeline.config.txt) file.

I am able to retrain and I see the inference images in Tensorboard during the training process. The loss values converge.

The problem is when exporting to TFLite using `object_detection/export_tflite_ssd_graph.py` script.

Command to export to TFlite:
`python object_detection/export_tflite_ssd_graph.py 
                  --pipeline_config_path=$CONFIG_FILE 
--trained_checkpoint_prefix=$CHECKPOINT_PATH 
--output_directory=$OUTPUT_DIR 
--add_postprocessing_op=true`

Error:
InvalidArgumentError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

Assign requires shapes of both tensors to match. lhs shape= [3,3,256,24] rhs shape= [3,3,1280,24]
     [[Node: save/Assign_15 = Assign[T=DT_FLOAT, _class=[""loc:@BoxPredictor_1/BoxEncodingPredictor/weights""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](BoxPredictor_1/BoxEncodingPredictor/weights, save/RestoreV2:15)]]

**Describe the expected behavior**
The retraining was successful, so I should get a tflite file successfully after exporting.

**Code to reproduce the issue**
N/A

**Other info / logs**
I am able to start with a detection checkpoint like ssd_mobilenet_v2_coco, retrrain it, export to TFLite and run inferece on Android."
23163,"ModuleNotFoundError: No module named 'tensorflow_estimator', and other import errors on TFE side","Hello,

I think there's a bug in the master branch of TF, where it's trying to import a module `tensorflow_estimator` which doesn't seem to exist? I can load TF as-is all good, but when I try to load TFP, I get this error (posted below my System specs):


**System information**
- OS Platform and Distribution: Debian Stretch
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): Master branch
- Python version: 3.6.5
- Bazel version (if compiling from source):0.18.0
- GCC/Compiler version (if compiling from source): 8.2.0
- CUDA/cuDNN version: NA
- GPU model and memory: NA
- Mobile device: No
- Exact command to reproduce:

```bash 
echo ""deb [arch=amd64] http://storage.googleapis.com/bazel-apt stable jdk1.8"" | sudo tee /etc/apt/sources.list.d/bazel.list && \
    curl https://bazel.build/bazel-release.pub.gpg | sudo apt-key add - && \
    apt-get update && apt-get install -y bazel  && rm -rf /var/lib/apt/lists/* && \
    ldconfig && \
    pip uninstall -y tensorflow-tensorboard tb-nightly tf-nightly tensorflow && \
    cd /opt && \
    git clone https://github.com/tensorflow/tensorflow.git && \
    cd /opt/tensorflow && \
    # sed -i 's/2018\.0\.3\.20180406/2019\.0\.20180710/g' tensorflow/contrib/cmake/external/mkl.cmake && \
    /bin/bash ./configure \
    && \
    bazel build \
    --config=mkl --config=opt --config=verbs \
    --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" \
    --copt=-msse4.2 --copt=-msse4.1 --copt=-mavx --copt=-msse2 --copt=-msse3  \
    --copt=-O3 --copt=-mfpmath=both \
    --copt=""-DMKL_LP64"" \
    --copt=""-fPIC"" \
    --linkopt=""-lmkl_gf_lp64"" \
    --linkopt=""-lmkl_gnu_thread"" \
    --linkopt=""-dl"" \
    --linkopt=""-lpthread"" \
    --linkopt=""-lmkl_core"" \
    --linkopt=""-lm"" \
    --linkopt=""-lmkl_rt"" \
    --linkopt=""-lmkldnn"" \
    tensorflow/tools/pip_package:build_pip_package && \
    bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip && \
    pip install --no-deps /tmp/pip/tensorflow-*.whl && \
    cd /opt && rm -rf /opt/tensorflow /tmp/* 

cd /opt && \
    pip uninstall -y tfp-nightly && \
    git clone https://github.com/tensorflow/probability.git && \
    cd /opt/probability && \
    echo """" >> /opt/probability/tensorflow_probability/python/sts/internal/__init__.py && \
    bazel build \
    --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" \
    --copt=-mavx --copt=-msse2 --copt=-msse3 --copt=-msse4.1 --copt=-msse4.2 \
    --copt=-O3 --copt=-mfpmath=both \
    --copt=""-DMKL_LP64"" \
    --copt=""-fPIC"" \
    --linkopt=""-lmkl_gf_lp64"" \
    --linkopt=""-lmkl_gnu_thread"" \
    --linkopt=""-dl"" \
    --linkopt=""-lpthread"" \
    --linkopt=""-lmkl_core"" \
    --linkopt=""-lm"" \
    --linkopt=""-lmkl_rt"" \
    --linkopt=""-lmkldnn"" \
    :pip_pkg && \
    PKGDIR=$(mktemp -d) && \
    ./bazel-bin/pip_pkg $PKGDIR && \
    pip install --no-deps $PKGDIR/*.whl && \
    cd /opt && rm -rf /opt/probability /tmp/* && \
    python -c ""import tensorflow_probability""
```

## Error trace:

```python

In [1]: import tensorflow_probability as tfp
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-1-41494c8c96ff> in <module>
----> 1 import tensorflow_probability as tfp

/opt/probability/tensorflow_probability/__init__.py in <module>
     19
     20 # from tensorflow_probability.google import staging  # DisableOnExport
---> 21 from tensorflow_probability.python import *  # pylint: disable=wildcard-import
     22 from tensorflow_probability.python.version import __version__

/opt/probability/tensorflow_probability/python/__init__.py in <module>
     19 from __future__ import print_function
     20
---> 21 from tensorflow_probability.python import bijectors
     22 from tensorflow_probability.python import distributions
     23 from tensorflow_probability.python import edward2

/opt/probability/tensorflow_probability/python/bijectors/__init__.py in <module>
     21 # pylint: disable=unused-import,wildcard-import,line-too-long,g-importing-member
     22
---> 23 from tensorflow_probability.python.bijectors.absolute_value import AbsoluteValue
     24 from tensorflow_probability.python.bijectors.affine import Affine
     25 from tensorflow_probability.python.bijectors.affine_linear_operator import AffineLinearOperator

/opt/probability/tensorflow_probability/python/bijectors/absolute_value.py in <module>
     20
     21 import tensorflow as tf
---> 22 from tensorflow_probability.python.bijectors import bijector
     23 from tensorflow.python.ops import control_flow_ops
     24

/opt/probability/tensorflow_probability/python/bijectors/bijector.py in <module>
     29 import tensorflow as tf
     30
---> 31 from tensorflow_probability.python.internal import distribution_util
     32 from tensorflow.python.framework import tensor_util
     33

/opt/probability/tensorflow_probability/python/internal/distribution_util.py in <module>
     24 import tensorflow as tf
     25
---> 26 from tensorflow_probability.python.internal import dtype_util
     27 from tensorflow_probability.python.internal import reparameterization
     28 from tensorflow.python.framework import smart_cond

/opt/probability/tensorflow_probability/python/internal/dtype_util.py in <module>
     22 import tensorflow as tf
     23
---> 24 from tensorflow.contrib import framework as contrib_framework
     25
     26

/opt/conda/lib/python3.6/site-packages/tensorflow/contrib/__init__.py in <module>
     38 from tensorflow.contrib import data
     39 from tensorflow.contrib import deprecated
---> 40 from tensorflow.contrib import distribute
     41 from tensorflow.contrib import distributions
     42 from tensorflow.contrib import estimator

/opt/conda/lib/python3.6/site-packages/tensorflow/contrib/distribute/__init__.py in <module>
     32 from tensorflow.contrib.distribute.python.parameter_server_strategy import ParameterServerStrategy
     33 from tensorflow.contrib.distribute.python.step_fn import *
---> 34 from tensorflow.contrib.distribute.python.tpu_strategy import TPUStrategy
     35 from tensorflow.python.distribute.distribute_config import DistributeConfig
     36 from tensorflow.python.distribute.distribute_coordinator import run_standard_tensorflow_server

/opt/conda/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/tpu_strategy.py in <module>
     25 from tensorflow.contrib.distribute.python import one_device_strategy
     26 from tensorflow.contrib.distribute.python import values
---> 27 from tensorflow.contrib.tpu.python.ops import tpu_ops
     28 from tensorflow.contrib.tpu.python.tpu import tpu
     29 from tensorflow.contrib.tpu.python.tpu import tpu_system_metadata as tpu_system_metadata_lib

/opt/conda/lib/python3.6/site-packages/tensorflow/contrib/tpu/__init__.py in <module>
     71 from tensorflow.contrib.tpu.python.tpu.bfloat16 import *
     72 from tensorflow.contrib.tpu.python.tpu.device_assignment import *
---> 73 from tensorflow.contrib.tpu.python.tpu.keras_support import tpu_model as keras_to_tpu_model
     74 from tensorflow.contrib.tpu.python.tpu.keras_support import TPUDistributionStrategy
     75 from tensorflow.contrib.tpu.python.tpu.topology import *

/opt/conda/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py in <module>
     60 from tensorflow.contrib.tpu.python.ops import tpu_ops
     61 from tensorflow.contrib.tpu.python.tpu import keras_tpu_variables
---> 62 from tensorflow.contrib.tpu.python.tpu import tpu
     63 from tensorflow.contrib.tpu.python.tpu import tpu_function
     64 from tensorflow.contrib.tpu.python.tpu import tpu_optimizer

/opt/conda/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/tpu.py in <module>
     22 from six.moves import xrange  # pylint: disable=redefined-builtin
     23
---> 24 from tensorflow.contrib.compiler import xla
     25 from tensorflow.contrib.framework.python.framework import experimental
     26 from tensorflow.contrib.tpu.python.ops import tpu_ops

/opt/conda/lib/python3.6/site-packages/tensorflow/contrib/compiler/xla.py in <module>
     25 from tensorflow.compiler.jit.ops import xla_ops
     26 from tensorflow.core.framework import attr_value_pb2
---> 27 from tensorflow.python.estimator import model_fn as model_fn_lib
     28 from tensorflow.python.framework import ops
     29 from tensorflow.python.ops import array_ops

/opt/conda/lib/python3.6/site-packages/tensorflow/python/estimator/__init__.py in <module>
     24 from __future__ import print_function
     25
---> 26 from tensorflow_estimator.python import estimator
     27
     28 # Include attrs that start with single underscore.

ModuleNotFoundError: No module named 'tensorflow_estimator'
```
"
23162,Can not run traing comand Object Detectioon API,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04

- TensorFlow installed from (source or binary):conda
- TensorFlow version (use command below):1.9
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A
- Python version:3.6.6
 Bazel version (if compiling from source):N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version: CUDA 9/ cuDNN 7.
- GPU model and memory: Tesla K80 , 11GB

**Describe the current behavior**
I trying to train a model(Faster R-CNN with Inception v2) but there is an error massage ""TypeError: merge_external_params_with_configs() got an unexpected keyword argument 'train_steps'"" while tracing the issue i have print the train_steps and is't equal to 200,000.
**The Output**
```
Train steps
200000
<class 'int'>
```

**My command is :**
```
PIPELINE_CONFIG_PATH=/home/dabastany_gmail_com/miniconda3/envs/od/lib/python3.6/site-packages/tensorflow/models/research/object_detection/train/pipeline_od.config
MODEL_DIR=/home/dabastany_gmail_com/miniconda3/envs/od/lib/python3.6/site-packages/tensorflow/models/research/object_detection/train/models/model
NUM_TRAIN_STEPS=200000
SAMPLE_1_OF_N_EVAL_EXAMPLES=1
python object_detection/model_main.py \
    --pipeline_config_path=${PIPELINE_CONFIG_PATH} \
    --model_dir=${MODEL_DIR} \
    --num_train_steps=${NUM_TRAIN_STEPS} \
    --sample_1_of_n_eval_examples=$SAMPLE_1_OF_N_EVAL_EXAMPLES \
    --alsologtostderr

```

**The error message:**

```
Traceback (most recent call last):
  File ""object_detection/model_main.py"", line 101, in <module>
    tf.app.run()
  File ""/home/dabastany_gmail_com/miniconda3/envs/od/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""object_detection/model_main.py"", line 62, in main
    eval_steps=FLAGS.num_eval_steps)
  File ""/home/dabastany_gmail_com/miniconda3/envs/od/lib/python3.6/site-packages/tensorflow/models/research/object_detection/model_lib.py"", line 509, in create_estimator_and_inputs
    retain_original_images_in_eval=False if use_tpu else True)
TypeError: merge_external_params_with_configs() got an unexpected keyword argument 'train_steps'
```

**My configuration File:**
 ```
Faster R-CNN with Inception v2, configuration for MSCOCO Dataset.
 Users should configure the fine_tune_checkpoint field in the train config as
 well as the label_map_path and input_path fields in the train_input_reader and
 eval_input_reader. Search for ""PATH_TO_BE_CONFIGURED"" to find the fields that
should be configured.


model {
  faster_rcnn {
    num_classes: 9
    image_resizer {
      keep_aspect_ratio_resizer {
        min_dimension: 600
        max_dimension: 1024
      }
    }
    feature_extractor {
      type: 'faster_rcnn_inception_v2'
      first_stage_features_stride: 16
    }
    first_stage_anchor_generator {
      grid_anchor_generator {
        scales: [0.25, 0.5, 1.0, 2.0]
        aspect_ratios: [0.5, 1.0, 2.0]
        height_stride: 16
        width_stride: 16
      }
    }
    first_stage_box_predictor_conv_hyperparams {
      op: CONV
      regularizer {
        l2_regularizer {
          weight: 0.0
        }
      }
      initializer {
        truncated_normal_initializer {
          stddev: 0.01
        }
      }
    }
    first_stage_nms_score_threshold: 0.0
    first_stage_nms_iou_threshold: 0.7
    first_stage_max_proposals: 300
    first_stage_localization_loss_weight: 2.0
    first_stage_objectness_loss_weight: 1.0
    initial_crop_size: 14
    maxpool_kernel_size: 2
    maxpool_stride: 2
    second_stage_box_predictor {
      mask_rcnn_box_predictor {
        use_dropout: false
        dropout_keep_probability: 1.0
        fc_hyperparams {
          op: FC
          regularizer {
            l2_regularizer {
              weight: 0.0
            }
          }
          initializer {
            variance_scaling_initializer {
              factor: 1.0
              uniform: true
              mode: FAN_AVG
            }
          }
        }
      }
    }
    second_stage_post_processing {
      batch_non_max_suppression {
        score_threshold: 0.0
        iou_threshold: 0.6
        max_detections_per_class: 100
        max_total_detections: 300
      }
      score_converter: SOFTMAX
    }
    second_stage_localization_loss_weight: 2.0
    second_stage_classification_loss_weight: 1.0
  }
}

train_config: {
  batch_size: 32
  optimizer {
    momentum_optimizer: {
      learning_rate: {
        manual_step_learning_rate {
          initial_learning_rate: 0.0002
          schedule {
            step: 900000
            learning_rate: .00002
          }
          schedule {
            step: 1200000
            learning_rate: .000002
          }
        }
      }
      momentum_optimizer_value: 0.9
    }
    use_moving_average: false
  }
  gradient_clipping_by_norm: 10.0
  fine_tune_checkpoint: ""/home/dabastany_gmail_com/miniconda3/envs/od/lib/python3.6/site-packages/tensorflow/models/research/object_detection/train/faster_rcnn_inception_v2_coco_2018_01_28/model.ckpt""
  from_detection_checkpoint: true
   Note: The below line limits the training process to 200K steps, which we
   empirically found to be sufficient enough to train the COCO dataset. This
   effectively bypasses the learning rate schedule (the learning rate will
   never decay). Remove the below line to train indefinitely.
  num_steps: 200000
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
}

train_input_reader: {
  tf_record_input_reader {
    input_path: ""/home/dabastany_gmail_com/miniconda3/envs/od/lib/python3.6/site-packages/tensorflow/models/research/object_detection/train/data/train.record""
  }
  label_map_path: ""/home/dabastany_gmail_com/miniconda3/envs/od/lib/python3.6/site-packages/tensorflow/models/research/object_detection/train/data/mscoco_label_map_dataturk.pbtxt""
}

eval_config: {
  num_examples: 83
   Note: The below line limits the evaluation process to 10 evaluations.
   Remove the below line to evaluate indefinitely.
  max_evals: 10
}

eval_input_reader: {
  tf_record_input_reader {
    input_path: ""/home/dabastany_gmail_com/miniconda3/envs/od/lib/python3.6/site-packages/tensorflow/models/research/object_detection/train/data/eval.record""
  }
  label_map_path: ""/home/dabastany_gmail_com/miniconda3/envs/od/lib/python3.6/site-packages/tensorflow/models/research/object_detection/train/data/mscoco_label_map_dataturk.pbtxt""
  shuffle: false
  num_readers: 1
}


```"
23160,tf.layers.dense activity_regularizer= tf.contrib.layers.l2_regularizer(0.0) causes error.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary (pip)
- TensorFlow version (use command below): 1.10
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 9.0 / 7
- GPU model and memory: Tesla k80

**Describe the current behavior**
Setting the activity regularizer of a tf.layers.dense layer to tf.contrib.layers.l2_regularizer(0.0) gives the error:
ValueError: None values not supported.

**Describe the expected behavior**
No error should be thrown - the regularizer should be disabled.
**Code to reproduce the issue**
x = tf.placeholder(tf.float32, (64,100))
layer = tf.layers.dense(
    x,
    100,
    activation=None,
    kernel_initializer=tf.contrib.layers.xavier_initializer(),
    activity_regularizer=tf.contrib.layers.l2_regularizer(scale=0.0)
)

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Trace:
Traceback (most recent call last):
  File ""<stdin>"", line 7, in <module>
  File ""/tensorflow/python/layers/core.py"", line 189, in dense
    return layer.apply(inputs)
  File ""/tensorflow/python/keras/engine/base_layer.py"", line 805, in apply
    return self.__call__(inputs, *args, **kwargs)
  File ""/tensorflow/python/layers/base.py"", line 362, in __call__
    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)
  File ""/tensorflow/python/keras/engine/base_layer.py"", line 758, in __call__
    self._handle_activity_regularization(inputs, outputs)
  File ""/tensorflow/python/keras/engine/base_layer.py"", line 640, in _handle_activity_regularization
    self.add_loss(activity_regularization, inputs=inputs)
  File ""/tensorflow/python/layers/base.py"", line 134, in add_loss
    super(Layer, self).add_loss(losses, inputs=inputs)
  File ""/tensorflow/python/keras/engine/base_layer.py"", line 411, in add_loss
    if not tensor_util.is_tensor(loss) else loss for loss in losses]
  File ""/tensorflow/python/keras/engine/base_layer.py"", line 411, in <listcomp>
    if not tensor_util.is_tensor(loss) else loss for loss in losses]
  File ""/tensorflow/python/framework/ops.py"", line 998, in convert_to_tensor
    as_ref=False)
  File ""/tensorflow/python/framework/ops.py"", line 1094, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/tensorflow/python/framework/constant_op.py"", line 217, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/tensorflow/python/framework/constant_op.py"", line 196, in constant
    value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""/tensorflow/python/framework/tensor_util.py"", line 424, in make_tensor_proto
    raise ValueError(""None values not supported."")
"
23159,Infinite run or optimizer fail,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.9.0
- Python version: 3.6.0
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version:9.1
- GPU model and memory: P100 16GB

**Describe the current behavior**
sess.run() executes infinitely and will never return
In the alternative version, sess.run() produces expected results but error is produced by the optimizer (see log).

The bug only occurs when 
1. On GPU. The code executes properly on CPU.
2. The condition of tf.cond holds False, i.e. when tf.gradients is not to be executed. Otherwise correct gradients could be calculated.
3. There're loop structures, e.g. RNN.  


**Code to reproduce the issue**

> X = tf.placeholder(shape=[None, None, 10], dtype=tf.float32)
> Y, _ = tf.nn.bidirectional_dynamic_rnn(LSTMCell(20), LSTMCell(20), X, dtype=tf.float32)
> 
> loss = tf.reduce_mean(Y)
> vars = tf.trainable_variables()
> grads = tf.cond(tf.shape(X)[0] < 3, lambda :tf.gradients(loss, vars), lambda :[np.nan] * len(vars))
> 
> sess = tf.Session()
> sess.run(tf.global_variables_initializer())
> print(sess.run(grads, feed_dict={X: np.random.random([30, 10, 10])}))


Alternative version:

> X = tf.constant(np.random.random([30, 10, 10]), dtype=tf.float32)
> Y, _ = tf.nn.bidirectional_dynamic_rnn(LSTMCell(20), LSTMCell(20), X, dtype=tf.float32)
> 
> loss = tf.reduce_mean(Y)
> vars = tf.trainable_variables()
> grads = tf.cond(tf.shape(X)[0] < 3, lambda :tf.gradients(loss, vars), lambda :[np.nan] * len(vars))
> 
> sess = tf.Session()
> sess.run(tf.global_variables_initializer())
> print(sess.run(grads))


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

> 2018-10-22 17:32:36.399614: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:581] Iteration = 0, topological sort failed with message: The graph couldn't be sorted in topological order.
> 2018-10-22 17:32:36.401378: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:581] Iteration = 1, topological sort failed with message: The graph couldn't be sorted in topological order.
> "
23158,Mysterious infinite run,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.9.0
- Python version: 3.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 9.1.85
- GPU model and memory: P100 16GB


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

**Update: I've managed to make a minimal example here: https://github.com/tensorflow/tensorflow/issues/23159**
Please close this issue.


        check_vars = model.trainable_variables()  

        check_grad = tf.cond(cond,  

                                  lambda :[g if isinstance(g, tf.Tensor) else 0.0 for g in  

                                           tf.gradients(model.loss, check_vars)],  

                                  lambda : [0.0] * len(check_vars))  

        sess.run(check_grad)  
In my own model (a Tacotron2 implementation), the last line will fall into infinite run and never return when tf is run on GPU and cond holds False, i.e. it should directly return a tensor of zeros, while the results will be correct when tf is run on CPU, or cond holds True.
**Describe the expected behavior**

**Code to reproduce the issue**
I didn't manage to reproduce the case on a model simple enough.
**Other info / logs**

"
23157,Supported input types for tf.nn.quantized_conv2d,"**System information**
- TensorFlow version: 1.11.0
- Doc Link: https://www.tensorflow.org/api_docs/python/tf/nn/quantized_conv2d
- Have I written custom code: Check [this question](https://stackoverflow.com/questions/52922980/error-when-using-quantized-conv2d-with-tf-qint8-inputs).
- OS Platform and Distribution: macOS Mojave
- TensorFlow installed from: pip binary
- Bazel version: N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A
- Exact command to reproduce: N/A
- Mobile device: N/A


**Describe the documentation issue**

The documentation for [tf.nn.quantized_conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/quantized_conv2d) says that `input` and `filter` must be one of the following types: `qint8`, `quint8`, `qint32`, `qint16`, `quint16`.

However, the only registered type for these arguments is `tf.quint8`. See also [this question](https://stackoverflow.com/questions/52922980/error-when-using-quantized-conv2d-with-tf-qint8-inputs).
"
23156,"axis argument for FFT ops (tf.signal.fft, tf.signal.fft2d, etc.)","<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.11
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
`tf.spectral.fft3d` as of now is restricted to apply FFT to the last 3 dimensions of the input, while the majority of the operations, e.g. `tf.nn.conv3d`, handle data in NDHWC format by default. In order to circumvent this limitation, users have to permute (transpose) the data just for the FFT operation, then permute them back for other operations that follow thereafter. This not only makes the code less readable, but more importantly incurs extra overhead.

**Will this change the current api? How?**
This will add another argument `data_format` to `tf.spectral.fft3d` which lets users decide which dimensions should be used for FFT. `data_format` should be either `""NDHWC""` or `""NCDHW""`, and defaults to `""NCDHW""` for backward compatibility. If `""NDHWC""` is chosen as the `data_format`, the second, third and fourth dimensions will be used instead. 

**Who will benefit with this feature?**
Any users using `fft3d` are expected to benefit from this, especially when they use FFT with other convolutional layers that accept inputs in NDHWC format.

**Any Other info.**
Other operations such as `tf.spectral.fft2d` may also be modified for consistency.
"
23155,Support for Truncated Backpropagation Through Time with tf.data.TFRecordDataset API,"I am aware that there is support on Tensorflow for truncated backpropagation through time with the tf.contrib.training.batch_sequences_with_states method and the state saving RNN. However this requires the conventional method using the queue runners. I am using the new tf.data.TFRecordDataset API for my my work which does not require the developer to explicitly call queue runners. But apparently there is no support for truncated backpropagation through time when I use this API.

Tensorflow Version: 1.11.0  "
23154,Support for Truncated Backpropagation Through Time with tf.data.TFRecordDataset API,"I am aware that there is support on Tensorflow for truncated backpropagation through time with the tf.contrib.training.batch_sequences_with_states method and the state saving RNN. However this requires the conventional method using the queue runners. I am using the new tf.data.TFRecordDataset API for my my work which does not require the developer to explicitly call queue runners. But apparently there is no support for truncated backpropagation through time when I use this API.

Tensorflow Version: 1.11.0  "
23153,rasa core,"while running rasa core in ubuntu im getting thiss error

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

from tensorflow.python.keras._impl.keras.backend import abs
ImportError: cannot import name abs

tried  uninstalling and installing the tensorflow and also tensorflow-gpu

------------------------

rasa core version : 
Version: 0.11.12

python version: 2.7.14

"
23150,TPU model doesn't work with tensorflow.python.keras learning rate scheduler.,"When I convert Keras model into TPU model using `tensorflow.contrib.tpu.keras_to_tpu_model` and try to fit it with learning rate scheduler callback, the training crashes with error `ValueError: Optimizer must have a ""lr"" attribute.`. I guess the problem is that the keras and tensorflow optimizers use different learning rate variable name, which makes the keras callback fail. The callback in `tensorflow.python.keras` should be therefore corrected to use the proper variable.

**System information**
- Google Colab TPU runtime
- TF 1.12.0rc1

**Code fragment**
```
    strategy = tensorflow.contrib.tpu.TPUDistributionStrategy(
        tensorflow.contrib.cluster_resolver.TPUClusterResolver(tpu = 'grpc://' + os.environ['COLAB_TPU_ADDR'])
    )
    model = tensorflow.contrib.tpu.keras_to_tpu_model(model, strategy=strategy)
    model.compile(
        loss=tensorflow.keras.losses.categorical_crossentropy,
        optimizer=tensorflow.train.GradientDescentOptimizer(learning_rate=lr_schedule(0)),
        metrics=['accuracy']
    )
    (...)
   callbacks = [tensorflow.python.keras.callbacks.LearningRateScheduler(lr_schedule_function)]
```

**Describe the current behavior**
```
Traceback (most recent call last)
<ipython-input-12-b6a9470abb3b> in <module>()
    441                         validation_data=(x_test, y_test),
    442                         epochs=epochs, verbose=1, workers=4,
--> 443                         callbacks=callbacks)
    444 
    445 # Score trained model.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   2175         use_multiprocessing=use_multiprocessing,
   2176         shuffle=shuffle,
-> 2177         initial_epoch=initial_epoch)
   2178 
   2179   def evaluate_generator(self,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py in fit_generator(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
    141       for m in model.stateful_metric_functions:
    142         m.reset_states()
--> 143       callbacks.on_epoch_begin(epoch)
    144       steps_done = 0
    145       batch_index = 0

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in on_epoch_begin(self, epoch, logs)
    198     logs = logs or {}
    199     for callback in self.callbacks:
--> 200       callback.on_epoch_begin(epoch, logs)
    201     self._delta_t_batch = 0.
    202     self._delta_ts_batch_begin = deque([], maxlen=self.queue_length)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in on_epoch_begin(self, epoch, logs)
    768   def on_epoch_begin(self, epoch, logs=None):
    769     if not hasattr(self.model.optimizer, 'lr'):
--> 770       raise ValueError('Optimizer must have a ""lr"" attribute.')
    771     try:  # new API
    772       lr = float(K.get_value(self.model.optimizer.lr))

ValueError: Optimizer must have a ""lr"" attribute.
```"
23148,Doc Request: Better Documentation for TFRecords,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: 1.11.0
- Doc Link: NA


**Describe the documentation issue**
With the introduction of `tf.data` and deprecation of `QueueRunner`, TFRecords seem to play a major role if you want to effeciently use accelerators while training on large datasets. Unfortunately, the current `tf.data` [documentation](https://www.tensorflow.org/guide/datasets) starts off assuming we already have `TFRecords` on disk without even linking how we could convert images, say `tiny imagenet` or even arbitrary images to the `TFRecord`s format. 

To be precise, there is no documentation for `TFRecord` , `TFExample` or even the readers and writers. Moreover, the examples do not cover how these things would work in eager execution mode. It would be great if these could be fixed as it would improve adoption of TFRecords and help more people use the tf.data pipelines.

PS, I am aware of code examples in the repo such as [1](https://github.com/tensorflow/models/blob/1af55e018eebce03fb61bba9959a04672536107d/research/deeplab/datasets/build_voc2012_data.py) and [2](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/convert_to_records.py) but would prefer to have better documentation and maybe a guide for this. 

Thanks

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?** no
"
23147,"Trying to connect from process to Tensorflow Server, cannot use frozen graph","Hi everyone, I have an issue which can possibly be a bug, for which I am trying to find solution for quite some time:

Stack Overflow:
https://stackoverflow.com/questions/52700621/tensorflow-server-i-dont-want-to-initialize-global-variables-for-every-session

Discuss:
https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/rARPP1_ilNA

My story in short is that I create processes dynamically in Python and do model predictions using a session for Tensorflow Server `tf.train.Server`. Graph is saved in global scope, as I hope that default python 'fork' mode will not create any overhead as long as data in RAM is not changed (""copy-on-change"" policy). Anyway, it's too long and expensive for me to reload model for each new process. Perhaps there might be a more efficient way to define model on server side, but I don't know it.

The problem is that somewhy I need to run initialization of global variables (`sess.run(tf.global_variables_initializer())`) each time I create new process and open new session. I tried to do dummy sun of the model before locking graph so that all the variables must be initialized, but no success - I still get the uninitialized variable error. Can anyone help me with that?

I have a small reproducible example of the problem compatible with Tensorflow 1.11:

https://github.com/hcl14/Tensorflow-server-launched-from-child-process

The places in the code are marked with `""PROBLEM""` comment.

Also, maybe I am doing everything wrong and there is a better way to quickly do model predictions from dynamically created processes?
"
23146,"The same code which is fine on Android 6.0.1, but an error(""The model is not a valid Flatbuffer file"") is reported on Android 8.0.0","**OS Platform and Distribution**
Win 10
Android Studio 3.0.1
sdk version 26
**TensorFlow installed from**
N/A
**TensorFlow version**
N/A
**Bazel version**
N/A
**CUDA/cuDNN version**
N/A
**GPU model and memory**
N/A
**Exact command to reproduce**
N/A
**Mobile devic**
Huawei Honor 8 & Gome K1

**Describe the problem**
I am using the TFLite demo for Android. Since there are two ways to initialize an Interpreter with a model file:
The Interpreter can be initialized with a model file using the constructor:
`public Interpreter(@NotNull File modelFile);`
or with a MappedByteBuffer:
`public Interpreter(@NotNull MappedByteBuffer mappedByteBuffer);`
The official TFLite demo uses the second way(MappedByteBuffer) to initialize an Interpreter. I change it to the first way(using File) and The modified code is as follows(the constructor of ImageClassifier):
```
//tflite = new Interpreter(loadModelFile(activity));  //official code. The second way to initialize.
File modelFile = new File(""/storage/emulated/0/DCIM/"" + getModelPath());   //the first way
tflite = new Interpreter(modelFile);            //the first way
```
When I run it on Gome K1 mobile phone(Android 6.0.1), it works fine. However, when I run it on Huawei Honor 8(Android 8.0.0), an error occurred:
`java.lang.RuntimeException: Unable to start activity ComponentInfo{com.example.tflite_pure/com.example.tflite_pure.MainActivity}: java.lang.IllegalArgumentException: Contents of /storage/emulated/0/DCIM/mobilenet_v1_1.0_224.tflite does not encode a valid TensorFlowLite model: Could not open '/storage/emulated/0/DCIM/mobilenet_v1_1.0_224.tflite'.The model is not a valid Flatbuffer file`

I am not sure if this problem is caused by different versions of Android(6.0.1 vs 8.0.0). So any help would grateful. Thanks.
"
23145,Could not initialize a memory descriptor when using softmax layer,"I have both CPU and GPU version installed by Miniconda, each with a unique environment. While GPU version works fine, the CPU version seems to throw an error when I try to add a softmax layer after a convolution layer.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Manjaro 4.14.74
- TensorFlow installed from (source or binary): binary from Miniconda
- TensorFlow version (use command below): 1.11.0
- Python version: Python 3.6.6 :: Anaconda, Inc.
- CUDA/cuDNN version: CPU version, no CUDA/cuDNN
- Bazel version: N/A
- GPU model and memory: N/A
- Mobile device: N/A
- Exact command to reproduce: python code.py

**Describe the current behavior**

Run the test code, the program throws AbortedError, info is:

```
AbortedError (see above for traceback): Operation received an exception:Status: 3, message: could not initialize a memory descriptor, in file tensorflow/core/kernels/mkl_softmax_op.cc:163
	 [[{{node Softmax}} = _MklSoftmax[T=DT_FLOAT, _kernel=""MklOp"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](conv2d/BiasAdd, conv2d/BiasAdd:2)]]
```

**Describe the expected behavior**

The program should finish with no error.

**Code to reproduce the issue**

```
import tensorflow as tf
import numpy as np

sess = tf.Session()
inputs = tf.placeholder(dtype=tf.float32, shape=(1, 300, 300, 3))
net = tf.layers.Conv2D(filters=2, kernel_size=3)(inputs)
net = tf.nn.softmax(net, axis=-1)
sess.run(tf.global_variables_initializer())
sess.run(net, feed_dict={inputs: np.zeros(shape=(1, 300, 300, 3), dtype=np.float32)})
```

**Other info / logs**

* I set up the environment by ```conda create -n xxx pip python=3 tensorflow```

* Traceback is:

```
Traceback (most recent call last):
  File ""/home/kwy/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1292, in _do_call
    return fn(*args)
  File ""/home/kwy/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1277, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/home/kwy/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1367, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.AbortedError: Operation received an exception:Status: 3, message: could not initialize a memory descriptor, in file tensorflow/core/kernels/mkl_softmax_op.cc:163
         [[{{node Softmax}} = _MklSoftmax[T=DT_FLOAT, _kernel=""MklOp"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](conv2d/BiasAdd, conv2d/BiasAdd:2)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""test.py"", line 10, in <module>
    sess.run(net, feed_dict={inputs: np.zeros(shape=(1, 300, 300, 3), dtype=np.float32)})
  File ""/home/kwy/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 887, in run
    run_metadata_ptr)
  File ""/home/kwy/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1110, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/kwy/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1286, in _do_run
    run_metadata)
  File ""/home/kwy/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1308, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.AbortedError: Operation received an exception:Status: 3, message: could not initialize a memory descriptor, in file tensorflow/core/kernels/mkl_softmax_op.cc:163
         [[{{node Softmax}} = _MklSoftmax[T=DT_FLOAT, _kernel=""MklOp"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](conv2d/BiasAdd, conv2d/BiasAdd:2)]]

Caused by op 'Softmax', defined at:
  File ""test.py"", line 7, in <module>
    net = tf.nn.softmax(net, axis=-1)
  File ""/home/kwy/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/home/kwy/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 1746, in softmax
    return _softmax(logits, gen_nn_ops.softmax, axis, name)
  File ""/home/kwy/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 1685, in _softmax
    return compute_op(logits, name=name)
  File ""/home/kwy/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 7138, in softmax
    ""Softmax"", logits=logits, name=name)
  File ""/home/kwy/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/kwy/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/home/kwy/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3272, in create_op
    op_def=op_def)
  File ""/home/kwy/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1768, in __init__
    self._traceback = tf_stack.extract_stack()

AbortedError (see above for traceback): Operation received an exception:Status: 3, message: could not initialize a memory descriptor, in file tensorflow/core/kernels/mkl_softmax_op.cc:163
         [[{{node Softmax}} = _MklSoftmax[T=DT_FLOAT, _kernel=""MklOp"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](conv2d/BiasAdd, conv2d/BiasAdd:2)]]
```

* GPU version works fine.

* If i set axis to 0, 1 or 2, the program finishes with no error, but with it set to  -1 or 3, the error occurs.

* If the softmax layer is added after a dense layer, it also works fine.

* I've also tested on another server with CentOS 7 and a Quadro P2000, the problem still occurs. (GPU version works fine while CPU version not)

* This code still not work:
```
net = tf.layers.Conv2D(filters=2, kernel_size=3, activation=tf.nn.softmax)(inputs)
```"
23143,Remove output tensors 1 - 4 from FusedBatchNorm,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.10.0
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
Downloaded frozen inference model MobileNet_v1_1.0_224 from https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md

FusedBatchNorm has five outputs. 1-4 seem to be related to training. How do I remove them?

I tried the following code:

			if op.type == 'FusedBatchNorm':
				new_node = node_def_pb2.NodeDef()
				new_node.name = op.node_def.name
				new_node.op = op.node_def.op
				for i in op.node_def.input:
					new_node.input.extend([i])
				for a in op.node_def.attr:
					new_node.attr[a].CopyFrom(op.node_def.attr[a])
				output_graph.node.extend([new_node])

			else:
				new_node = node_def_pb2.NodeDef()
				new_node.CopyFrom(op.node_def)
				output_graph.node.extend([new_node])

It still has the output tensors 1 - 4.

Can this feature be added to Graph Transform tool?

**Will this change the current api? How?**
It won't. It's a new transformation in Graph Transformation tool.

**Who will benefit with this feature?**
Mobilenet users

**Any Other info.**
"
23142,CAN'T BUILD TENSORFLOW,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):WINDOWS 7 64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: r1.11
- Python version: 3.7
- Installed using virtualenv? pip? conda?: PIP
- Bazel version (if compiling from source): 0.18.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:  not using
- GPU model and memory:



I cant build tensorflow because happens some error about@png_archive

`C:\Users\00\tensorflow>bazel build --config=opt //tensorflow/tools/pip_package:
uild_pip_package
WARNING: Processed legacy workspace file c:\users\00\tensorflow/tools/bazel.rc.
This file will not be processed in the next release of Bazel. Please read https
//github.com/bazelbuild/bazel/issues/6319 for further information, including ho
 to upgrade.
WARNING: Option 'experimental_shortened_obj_file_path' is deprecated
Loading:
Loading: 0 packages loaded
INFO: Build options have changed, discarding analysis cache.
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (0 packages
oaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (0 packages
oaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (0 packages
oaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (0 packages
oaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (0 packages
oaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (0 packages
oaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (0 packages
oaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (0 packages
oaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (0 packages
oaded)
ERROR: C:/users/00/tensorflow/tensorflow/core/platform/default/build_config/BUI
D:188:1: no such package '@png_archive//': Traceback (most recent call last):
        File ""C:/users/00/tensorflow/third_party/repo.bzl"", line 106
                _apply_patch(ctx, ctx.attr.patch_file)
        File ""C:/users/00/tensorflow/third_party/repo.bzl"", line 73, in _apply_
atch
                _execute_and_check_ret_code(ctx, cmd)
        File ""C:/users/00/tensorflow/third_party/repo.bzl"", line 52, in _execut
_and_check_ret_code
                fail(""Non-zero return code({1}) when ...))
Non-zero return code(127) when executing 'E:\MSYS2\usr\bin\bash.exe -l -c patch
-p1 -d C:/users/00/_bazel_00/xjibpket/external/png_archive -i C:/users/00/tenso
flow/third_party/png_fix_rpi.patch':
Stdout:
Stderr: /usr/bin/bash: patch: comando n├úo encontrado
 and referenced by '//tensorflow/core/platform/default/build_config:png'
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (0 packages
oaded)
ERROR: C:/users/00/tensorflow/tensorflow/core/platform/default/build_config/BUI
D:188:1: no such package '@png_archive//': Traceback (most recent call last):
        File ""C:/users/00/tensorflow/third_party/repo.bzl"", line 106
                _apply_patch(ctx, ctx.attr.patch_file)
        File ""C:/users/00/tensorflow/third_party/repo.bzl"", line 73, in _apply_
atch
                _execute_and_check_ret_code(ctx, cmd)
        File ""C:/users/00/tensorflow/third_party/repo.bzl"", line 52, in _execut
_and_check_ret_code
                fail(""Non-zero return code({1}) when ...))
Non-zero return code(127) when executing 'E:\MSYS2\usr\bin\bash.exe -l -c patch
-p1 -d C:/users/00/_bazel_00/xjibpket/external/png_archive -i C:/users/00/tenso
flow/third_party/png_fix_rpi.patch':
Stdout:
Stderr: /usr/bin/bash: patch: comando n├úo encontrado
 and referenced by '//tensorflow/core/platform/default/build_config:png'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' fa
led; build aborted: Analysis failed
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (0 packages
oaded)
ERROR: C:/users/00/tensorflow/tensorflow/core/platform/default/build_config/BUI
D:188:1: no such package '@png_archive//': Traceback (most recent call last):
        File ""C:/users/00/tensorflow/third_party/repo.bzl"", line 106
                _apply_patch(ctx, ctx.attr.patch_file)
        File ""C:/users/00/tensorflow/third_party/repo.bzl"", line 73, in _apply_
atch
                _execute_and_check_ret_code(ctx, cmd)
        File ""C:/users/00/tensorflow/third_party/repo.bzl"", line 52, in _execut
_and_check_ret_code
                fail(""Non-zero return code({1}) when ...))
Non-zero return code(127) when executing 'E:\MSYS2\usr\bin\bash.exe -l -c patch
-p1 -d C:/users/00/_bazel_00/xjibpket/external/png_archive -i C:/users/00/tenso
flow/third_party/png_fix_rpi.patch':
Stdout:
Stderr: /usr/bin/bash: patch: comando n├úo encontrado
 and referenced by '//tensorflow/core/platform/default/build_config:png'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' fa
led; build aborted: Analysis failed
INFO: Elapsed time: 9,864s
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (0 packages
oaded)
ERROR: C:/users/00/tensorflow/tensorflow/core/platform/default/build_config/BUI
D:188:1: no such package '@png_archive//': Traceback (most recent call last):
        File ""C:/users/00/tensorflow/third_party/repo.bzl"", line 106
                _apply_patch(ctx, ctx.attr.patch_file)
        File ""C:/users/00/tensorflow/third_party/repo.bzl"", line 73, in _apply_
atch
                _execute_and_check_ret_code(ctx, cmd)
        File ""C:/users/00/tensorflow/third_party/repo.bzl"", line 52, in _execut
_and_check_ret_code
                fail(""Non-zero return code({1}) when ...))
Non-zero return code(127) when executing 'E:\MSYS2\usr\bin\bash.exe -l -c patch
-p1 -d C:/users/00/_bazel_00/xjibpket/external/png_archive -i C:/users/00/tenso
flow/third_party/png_fix_rpi.patch':
Stdout:
Stderr: /usr/bin/bash: patch: comando n├úo encontrado
 and referenced by '//tensorflow/core/platform/default/build_config:png'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' fa
led; build aborted: Analysis failed
INFO: Elapsed time: 9,864s
INFO: 0 processes.
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (0 packages
oaded)
ERROR: C:/users/00/tensorflow/tensorflow/core/platform/default/build_config/BUI
D:188:1: no such package '@png_archive//': Traceback (most recent call last):
        File ""C:/users/00/tensorflow/third_party/repo.bzl"", line 106
                _apply_patch(ctx, ctx.attr.patch_file)
        File ""C:/users/00/tensorflow/third_party/repo.bzl"", line 73, in _apply_
atch
                _execute_and_check_ret_code(ctx, cmd)
        File ""C:/users/00/tensorflow/third_party/repo.bzl"", line 52, in _execut
_and_check_ret_code
                fail(""Non-zero return code({1}) when ...))
Non-zero return code(127) when executing 'E:\MSYS2\usr\bin\bash.exe -l -c patch
-p1 -d C:/users/00/_bazel_00/xjibpket/external/png_archive -i C:/users/00/tenso
flow/third_party/png_fix_rpi.patch':
Stdout:
Stderr: /usr/bin/bash: patch: command not found
 and referenced by '//tensorflow/core/platform/default/build_config:png'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' fa
led; build aborted: Analysis failed
INFO: Elapsed time: 9,864s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (0 packages
oaded)
ERROR: C:/users/00/tensorflow/tensorflow/core/platform/default/build_config/BUI
D:188:1: no such package '@png_archive//': Traceback (most recent call last):
        File ""C:/users/00/tensorflow/third_party/repo.bzl"", line 106
                _apply_patch(ctx, ctx.attr.patch_file)
        File ""C:/users/00/tensorflow/third_party/repo.bzl"", line 73, in _apply_
atch
                _execute_and_check_ret_code(ctx, cmd)
        File ""C:/users/00/tensorflow/third_party/repo.bzl"", line 52, in _execut
_and_check_ret_code
                fail(""Non-zero return code({1}) when ...))
Non-zero return code(127) when executing 'E:\MSYS2\usr\bin\bash.exe -l -c patch
-p1 -d C:/users/00/_bazel_00/xjibpket/external/png_archive -i C:/users/00/tenso
flow/third_party/png_fix_rpi.patch':
Stdout:
Stderr: /usr/bin/bash: patch: command not found
 and referenced by '//tensorflow/core/platform/default/build_config:png'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' fa
led; build aborted: Analysis failed
INFO: Elapsed time: 9,864s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (0 packages
oaded)
ERROR: C:/users/00/tensorflow/tensorflow/core/platform/default/build_config/BUI
D:188:1: no such package '@png_archive//': Traceback (most recent call last):
        File ""C:/users/00/tensorflow/third_party/repo.bzl"", line 106
                _apply_patch(ctx, ctx.attr.patch_file)
        File ""C:/users/00/tensorflow/third_party/repo.bzl"", line 73, in _apply_
atch
                _execute_and_check_ret_code(ctx, cmd)
        File ""C:/users/00/tensorflow/third_party/repo.bzl"", line 52, in _execut
_and_check_ret_code
                fail(""Non-zero return code({1}) when ...))
Non-zero return code(127) when executing 'E:\MSYS2\usr\bin\bash.exe -l -c patch
-p1 -d C:/users/00/_bazel_00/xjibpket/external/png_archive -i C:/users/00/tenso
flow/third_party/png_fix_rpi.patch':
Stdout:
Stderr: /usr/bin/bash: patch: command not found
 and referenced by '//tensorflow/core/platform/default/build_config:png'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' fa
led; build aborted: Analysis failed
INFO: Elapsed time: 9,864s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (0 packages
oaded)
ERROR: C:/users/00/tensorflow/tensorflow/core/platform/default/build_config/BUI
D:188:1: no such package '@png_archive//': Traceback (most recent call last):
        File ""C:/users/00/tensorflow/third_party/repo.bzl"", line 106
                _apply_patch(ctx, ctx.attr.patch_file)
        File ""C:/users/00/tensorflow/third_party/repo.bzl"", line 73, in _apply_
atch
                _execute_and_check_ret_code(ctx, cmd)
        File ""C:/users/00/tensorflow/third_party/repo.bzl"", line 52, in _execut
_and_check_ret_code
                fail(""Non-zero return code({1}) when ...))
Non-zero return code(127) when executing 'E:\MSYS2\usr\bin\bash.exe -l -c patch
-p1 -d C:/users/00/_bazel_00/xjibpket/external/png_archive -i C:/users/00/tenso
flow/third_party/png_fix_rpi.patch':
Stdout:
Stderr: /usr/bin/bash: patch: command not found
 and referenced by '//tensorflow/core/platform/default/build_config:png'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' fa
led; build aborted: Analysis failed
INFO: Elapsed time: 9,864s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
`
"
23140,openrave gives me segfaults,"Hello Everyone. First of all I have to apologize if my question may not be clear. Because I'm a newbie in Ubuntu.  I tried to install pymanoid package and I was using openrave to test Inverse Kinematic example. After everything was installed correctly when I opened Inverse Kinematic python example I could see the robot but there was something that I thought was an error in terminal:
[0;34mIn [[1;34m1[0;34m]: [0m

I searched about it and someone was suggesting to install lib32ncurses5-dev. After that whenver I open the same python file(IK example) I get segmentation fault and I can't solve the problem. I tried to remove lib32ncurses5-dev and its dependencies, but still I have the same problem. 
Does anyone know how can I fix it?

This is what happens:
Python 2.7.12 (default, Dec  4 2017, 14:50:18) 
Type ""copyright"", ""credits"" or ""license"" for more information.

IPython 2.4.1 -- An enhanced Interactive Python.
?         -> Introduction and overview of IPython's features.
%quickref -> Quick reference.
help      -> Python's own help system.
object?   -> Details about 'object', use 'object??' for extra details.
Segmentation fault (core dumped)

"
23137,`ImportError: No module named 'tensorboardX'`,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux dlvm 4.9.0-8-amd64 #1 SMP Debian 4.9.110-3+deb9u5 (2018-09-30) x86_64 GNU/Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
Google Cloud VM
- TensorFlow installed from (source or binary):
pip3 install --upgrade tensorflow
- TensorFlow version: 1.11.0
- Python version: 3.5.3
- Installed using virtualenv? pip? conda?: pip3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the problem**

Trying to import the tensorboardX module in a cell within Python 3 Jupyter Notebook
`from tensorboardX import SummaryWriter`

Received the following error:
`ImportError: No module named 'tensorboardX'`

Ran a `pip3 freeze` to check if the packages were installed:
```
tensorboard==1.11.0
tensorboardX==1.4
tensorflow==1.11.0
```
"
23136,Estimators Siamese Model Peformance Issue ,"tf.esitmators producing poor performance vs TF & Keras.  

* System information
    OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
    TensorFlow installed from (source or binary):
    pip3 install --upgrade tensorflow-gpu==1.12.0-rc1
    TensorFlow version: 1.12.0-rc1
    Python version: Python 3.6.5 
    Installed using virtualenv? pip? conda?: pip3
    CUDA/cuDNN version: 9.0 / 7.0.5
    GPU model and memory: nvidia gtx 1050 (Lenovo Laptop)

* Code to reproduce performance issue:

```python
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from keras.datasets import mnist

import numpy as np
import tensorflow as tf

import random
tf.logging.set_verbosity(tf.logging.INFO)


def cnn_cnn_model(features, reuse=False):
    """"""Model function for CNN.""""""

    # Flat 1
    flat_1 = tf.layers.flatten(
        features,
        name=""flattern"",
    )

    # dense 1
    dense_1 = tf.layers.dense(
        flat_1,
        1024,
        activation=tf.nn.relu,
        reuse=reuse,
        name='dense_1',
    )

    # drop 1
    drop_1 = tf.layers.dropout(
        dense_1,
        rate=0.1,
        name=""drop_1"",
    )

    # dense 2
    dense_2 = tf.layers.dense(
        drop_1,
        512,
        activation=tf.nn.relu,
        reuse=reuse,
        name='dense_2',
    )

    # drop 2
    drop_2 = tf.layers.dropout(
        dense_2,
        rate=0.1,
        name=""drop_2"",
    )

    # dense 3
    dense_3 = tf.layers.dense(
        drop_2,
        128,
        activation=None,
        reuse=reuse,
        name='dense_3',
    )

    return dense_3


def accuracy(y_true, y_pred):
    '''Compute classification accuracy with a fixed threshold on distances.
    '''

    return tf.metrics.mean(tf.equal(y_true, tf.cast(y_pred < tf.cast(0.5, 'float64'), y_true.dtype)))

def compute_accuracy(y_true, y_pred):
    '''Compute classification accuracy with a fixed threshold on distances.
    '''
    pred = y_pred.ravel() < 0.5
    return np.mean(pred == y_true)


def my_loss(dist, y_true, margin=1.0):

    y_true = tf.cast(y_true, 'float64')
    margin = tf.cast(margin, 'float64')
    dist = tf.cast(dist, 'float64')

    # Loss function
    loss_pos = tf.multiply(y_true, tf.pow(dist, 2), name='constrastive_loss_1')
    loss_neg = tf.multiply(tf.subtract(tf.cast(1.0, 'float64'), y_true),
                           tf.pow(tf.maximum(tf.subtract(margin, dist), 0), 2),
                           name='constrastive_loss_2')
    loss = tf.reduce_mean(tf.add(loss_neg, loss_pos), name='constrastive_loss')

    return loss


def euclidean_distance(x1, x2):
    x1 = tf.cast(x1, 'float64')
    x2 = tf.cast(x2, 'float64')
    epsilon = tf.cast(1e-7, 'float64')

    eucd2 = tf.pow(tf.subtract(x1, x2), 2, name='eucd2')
    eucd2 = tf.reduce_sum(eucd2, 1)

    eucd2 = tf.maximum(eucd2, epsilon)
    eucd = tf.sqrt(eucd2, name='eucd')

    return eucd



def create_siamese(features, labels, mode):
    # Input Layers
    input_layer_a = tf.reshape(features[""x_1""], [-1, 28, 28])
    input_layer_b = tf.reshape(features[""x_2""], [-1, 28, 28])

    with tf.variable_scope(""siamese"") as scope:

        network_a = cnn_cnn_model(input_layer_a, reuse=False)
        network_b = cnn_cnn_model(input_layer_b, reuse=True)

    if mode == tf.estimator.ModeKeys.TRAIN:
        dist = euclidean_distance(network_a, network_b)
        loss = my_loss(dist, labels)

        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)
        train_op = optimizer.minimize(
            loss=loss,
            global_step=tf.train.get_global_step())
        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)

    elif mode == tf.estimator.ModeKeys.PREDICT:
        predictions = {
            ""eucl"": euclidean_distance(network_a, network_b)
        }

        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)


def create_pairs(x, digit_indices, num_classes):
    '''Positive and negative pair creation.
    Alternates between positive and negative pairs.
    '''
    pairs = []
    labels = []
    n = min([len(digit_indices[d]) for d in range(num_classes)]) - 1
    for d in range(num_classes):
        for i in range(n):
            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]
            pairs += [[x[z1], x[z2]]]
            inc = random.randrange(1, num_classes)
            dn = (d + inc) % num_classes
            z1, z2 = digit_indices[d][i], digit_indices[dn][i]
            pairs += [[x[z1], x[z2]]]
            labels += [1, 0]
    return np.array(pairs), np.array(labels)

def main(unused_argv):

    path_ = '/media/imran/bigboy/datasets/my_smallVoxCeleb/updated_datasets/split_datasets/3d_cnn/smallest_dataset.hdf5'
    num_classes = 10

    # the data, split between train and test sets
    (x_train, y_train), (x_test, y_test) = mnist.load_data()
    x_train = x_train.astype('float64')
    x_test = x_test.astype('float64')
    y_train = y_train.astype('float64')
    y_test = y_test.astype('float64')


    x_train /= 255
    x_test /= 255

    # create training+test positive and negative pairs
    digit_indices = [np.where(y_train == i)[0] for i in range(num_classes)]
    tr_pairs, tr_y = create_pairs(x_train, digit_indices, num_classes)

    digit_indices = [np.where(y_test == i)[0] for i in range(num_classes)]
    te_pairs, te_y = create_pairs(x_test, digit_indices, num_classes)


    # Create the Estimator
    mnist_classifier = tf.estimator.Estimator(
      model_fn=create_siamese)

    # Train the model
    train_input_fn = tf.estimator.inputs.numpy_input_fn(
      x={""x_1"": tr_pairs[:, 0],
         ""x_2"": tr_pairs[:, 1]},
      y=tr_y,
      batch_size=128,
      num_epochs=1,
      shuffle=True)

    test_input_fn_train = tf.estimator.inputs.numpy_input_fn(
        x={""x_1"": tr_pairs[:, 0],
           ""x_2"": tr_pairs[:, 1]},
        shuffle=False
       )

    test_input_fn_test = tf.estimator.inputs.numpy_input_fn(
        x={""x_1"": te_pairs[:, 0],
           ""x_2"": te_pairs[:, 1]},
        shuffle=False
       )

    mnist_classifier.train(
        input_fn=train_input_fn,
        steps=None)

    predictions_test = list(mnist_classifier.predict(input_fn=test_input_fn_test))
    predictions_train = list(mnist_classifier.predict(input_fn=test_input_fn_train))

    preds_test = []
    for item in predictions_test:
        preds_test.append(item['eucl'])

    preds_train = []
    for item in predictions_train:
        preds_train.append(item['eucl'])

    print(""Accuracy on train set: "", compute_accuracy(tr_y, np.array(preds_train)))
    print(""Accuracy on test set: "", compute_accuracy(te_y, np.array(preds_test)))


if __name__ == ""__main__"":
    tf.app.run()
```

With Keras model.fit(), this yields:
```
Accuracy on training set: 95.70%
Accuracy on test set: 95.33%
```
With TF this yields:
```
Accuracy on training set: 94.56%
Accuracy on test set: 94.53%
```
With tf.estimators this yields:
```
Accuracy on training set: 61.32%
Accuracy on test set: 62.74%
```

The code for the benchmark code for Keras and TF can be found here:
+ Keras:
https://gist.github.com/imranparuk/25963eb3b2db1540ead684271de6f5a8
+ TF:
https://gist.github.com/imranparuk/4fe48323a006ff030bf2037136db7868

"
23135,start Jupyter Notebook in nightly-devel tag,When running `docker run -it -p 8888:8888 tensorflow/tensorflow:nightly-devel` the docker runs in bash. How can I run the same tag but with Jupyter notebook as in the Dockerfile 'cpu-devel-jupyter.Dockerfile'?
23134,AttributeError: 'CuDNNLSTM' object has no attribute '_num_inputs' | Tflite,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 (I think), I used the nightly docker: 
docker run --runtime=nvidia -it tensorflow/tensorflow:nightly-gpu-py3 bash
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): docker:
docker run --runtime=nvidia -it tensorflow/tensorflow:nightly-gpu-py3 bash
- TensorFlow version (use command below): nightly (v1.12.0-rc0-1214-g6802f29084' 1.13.0-dev20181020)
- Python version: py3
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: 
- GPU model and memory: 


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
I am trying to convert a model.h5 keras model using tflite. Command used: 
`tflite_convert --output_file=./model_tflite --keras_model_file=./model.h5
`
Error: 
`root@7580fbf40640:/DNA_Compression/encoder_decoder# tflite_convert --output_file=./model_tflite --keras_model_file=./model.h5
Traceback (most recent call last):
  File ""/usr/local/bin/tflite_convert"", line 11, in <module>
    sys.exit(main())
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 412, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 408, in run_main
    _convert_model(tflite_flags)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 100, in _convert_model
    converter = _get_toco_converter(flags)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 87, in _get_toco_converter
    return converter_fn(**converter_kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/lite.py"", line 368, in from_keras_model_file
    keras_model = _keras.models.load_model(model_file)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/saving.py"", line 230, in load_model
    model = model_from_config(model_config, custom_objects=custom_objects)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/saving.py"", line 310, in model_from_config
    return deserialize(config, custom_objects=custom_objects)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/serialization.py"", line 64, in deserialize
    printable_module_name='layer')
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/utils/generic_utils.py"", line 173, in deserialize_keras_object
    list(custom_objects.items())))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/sequential.py"", line 340, in from_config
    model.add(layer)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/checkpointable/base.py"", line 474, in _method_wrapper
    method(self, *args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/sequential.py"", line 175, in add
    output_tensor = layer(self.outputs[0])
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/recurrent.py"", line 706, in __call__
    self._num_inputs)
AttributeError: 'CuDNNLSTM' object has no attribute '_num_inputs'
`
**Describe the expected behavior**
Ideally there should be no error, as it is a model which works fine in keras (TF backend)
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
`root@7580fbf40640:/DNA_Compression/encoder_decoder# tflite_convert --output_file=./model_tflite --keras_model_file=./model.h5'
(I can upload the model somewhere, if needed!)
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23133,Keras to Estimators Siamese Issue,"Hi there, I wounder if anyone could assist me. Forgive me if this isn't the correct place for this or it is not in the perfect format...

I am trying to convert the Keras Siamese example into an estimator, however it does not seem to be working...

* System information
    OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
    TensorFlow installed from (source or binary):
    pip3 install --upgrade tensorflow-gpu==1.12.0-rc1
    TensorFlow version: 1.12.0-rc1
    Python version: Python 3.6.5 
    Installed using virtualenv? pip? conda?: pip3
    CUDA/cuDNN version: 9.0 / 7.0.5
    GPU model and memory: nvidia gtx 1050 (Lenovo Laptop)

```python
'''Trains a Siamese MLP on pairs of digits from the MNIST dataset.
It follows Hadsell-et-al.'06 [1] by computing the Euclidean distance on the
output of the shared network and by optimizing the contrastive loss (see paper
for mode details).
# References
- Dimensionality Reduction by Learning an Invariant Mapping
    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf
Gets to 97.2% test accuracy after 20 epochs.
2 seconds per epoch on a Titan X Maxwell GPU
'''
from __future__ import absolute_import
from __future__ import print_function
import numpy as np

import random
import tensorflow as tf
#from tf.keras.models import Sequential  # This does not work!
from tensorflow.python.keras.datasets import mnist

from tensorflow.python.keras.models import Model
from tensorflow.python.keras.layers import Input, Flatten, Dense, Dropout, Lambda
from tensorflow.python.keras.callbacks import ModelCheckpoint, Callback
from tensorflow.python.keras.utils import HDF5Matrix, to_categorical
from tensorflow.python.keras.losses import categorical_crossentropy
from tensorflow.python.keras.optimizers import RMSprop
from tensorflow.python.keras import backend as K

num_classes = 10
epochs = 20
tf.logging.set_verbosity(tf.logging.INFO)


def euclidean_distance(vects):
    x, y = vects
    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)
    return K.sqrt(K.maximum(sum_square, K.epsilon()))


def eucl_dist_output_shape(shapes):
    shape1, shape2 = shapes
    return (shape1[0], 1)


def contrastive_loss(y_true, y_pred):
    '''Contrastive loss from Hadsell-et-al.'06
    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf
    '''
    margin = 1
    sqaure_pred = K.square(y_pred)
    margin_square = K.square(K.maximum(margin - y_pred, 0))
    return K.mean(y_true * sqaure_pred + (1 - y_true) * margin_square)


def create_pairs(x, digit_indices):
    '''Positive and negative pair creation.
    Alternates between positive and negative pairs.
    '''
    pairs = []
    labels = []
    n = min([len(digit_indices[d]) for d in range(num_classes)]) - 1
    for d in range(num_classes):
        for i in range(n):
            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]
            pairs += [[x[z1], x[z2]]]
            inc = random.randrange(1, num_classes)
            dn = (d + inc) % num_classes
            z1, z2 = digit_indices[d][i], digit_indices[dn][i]
            pairs += [[x[z1], x[z2]]]
            labels += [1, 0]
    return np.array(pairs), np.array(labels)


def create_base_network(input_shape):
    '''Base network to be shared (eq. to feature extraction).
    '''

    input = Input(shape=input_shape)
    x = Flatten()(input)
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.1)(x)
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.1)(x)
    x = Dense(128, activation='relu')(x)
    return Model(input, x)


def compute_accuracy(y_true, y_pred):
    '''Compute classification accuracy with a fixed threshold on distances.
    '''
    pred = y_pred.ravel() < 0.5
    return np.mean(pred == y_true)


def accuracy(y_true, y_pred):
    '''Compute classification accuracy with a fixed threshold on distances.
    '''
    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))


# the data, split between train and test sets
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255
input_shape = x_train.shape[1:]

# create training+test positive and negative pairs
digit_indices = [np.where(y_train == i)[0] for i in range(num_classes)]
tr_pairs, tr_y = create_pairs(x_train, digit_indices)

digit_indices = [np.where(y_test == i)[0] for i in range(num_classes)]
te_pairs, te_y = create_pairs(x_test, digit_indices)


# network definition
base_network = create_base_network(input_shape)

input_a = Input(shape=input_shape, name='input_a')
input_b = Input(shape=input_shape, name='input_b')

# because we re-use the same instance `base_network`,
# the weights of the network
# will be shared across the two branches
processed_a = base_network(input_a)
processed_b = base_network(input_b)

distance = Lambda(euclidean_distance,
                  output_shape=eucl_dist_output_shape)([processed_a, processed_b])

model = Model([input_a, input_b], distance)

# train
rms = RMSprop()
model.compile(loss=contrastive_loss, optimizer=rms, metrics=[accuracy])

# model.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,
#           batch_size=128,
#           epochs=1,
#           validation_data=([te_pairs[:, 0], te_pairs[:, 1]], te_y))

estimata = tf.keras.estimator.model_to_estimator(keras_model=model)


input_name1 = model.input_names[0]
input_name2 = model.input_names[1]



train_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={input_name1: tr_pairs[:,0],
           input_name2: tr_pairs[:,1]},
        y=tr_y,
        batch_size=128,
        num_epochs=1,
        shuffle=True)
#
estimata.train(
    input_fn=train_input_fn,
    steps=None)  # ,

eval_input_fn = tf.estimator.inputs.numpy_input_fn(
    x={input_name1: te_pairs[:, 0],
       input_name2: te_pairs[:, 1]},
    y=te_y,
    num_epochs=1,
    shuffle=False)

eval_results = estimata.evaluate(input_fn=eval_input_fn)

# compute final accuracy on training and test sets
y_pred = model.predict([tr_pairs[:, 0], tr_pairs[:, 1]])
tr_acc = compute_accuracy(tr_y, y_pred)

y_pred = model.predict([te_pairs[:, 0], te_pairs[:, 1]])
te_acc = compute_accuracy(te_y, y_pred)

print('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))
print('* Accuracy on test set: %0.2f%%' % (100 * te_acc))
```

With Keras model.fit(), this yields:
```
Accuracy on training set: 95.70%
Accuracy on test set: 95.33%
```

However With Estimators tf.estimator.train() the results are much worse:
```
Accuracy on training set: 53.11%
Accuracy on test set: 53.80%
```"
23132,multiplication of IndexedSlices with Dense Tensors,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):v1.3.0-rc2-20-g0787eee 1.3.0
- Are you willing to contribute it (Yes/No):
No


**Describe the feature and the current behavior/state.**
I wanted to implement LambdaRank algorithm which requires a point-wise multiplication of \lambda (dense tensor) with grads obtained by tf.gradients.  In tf, the grads of embeddings extraction will be IndexedSlices. It could be fed into optimizer's apply_grad() directly which is good but not for customized operations. Here I want to multiply dense tensor with IndexedSlices, but in vain. I have to convert IndexedSlices into dense and then multiply, which resulted in OOM  
**Will this change the current api? How?**
No
**Who will benefit with this feature?**
People interested in Learning-to-rank algorithm, and more generally whoever wants to tweak the sparse grads
**Any Other info.**
None

Have I written custom code: No, I only call tf API functions
OS Platform and Distribution: Debian 8.11
TensorFlow installed from: anaconda 
Bazel version: N/A
CUDA/cuDNN version: CUDA 8.0.44, cuDNN 6.0.21
GPU model and memory: Nvidia Titan Xp
Exact command to reproduce: N/A
Mobile device: N/A"
23131,Failed to achieve MobileNet V2 reported latency on Pixel Phone,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
Host: Mac OS High Sierra / Ubuntu 16.04
Phone: Google Pixel, Android 9.0
Measure tool: build from [TF-Benchmark](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/tools/benchmark)
Model: [mobilenet_v2_1.0_224 from TF-Slim](https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet)
  
**Describe the current behavior**

Net   | Paper Report | Measured
--- | --- | ---
MobileNet v2 1.0 224 | 73.8ms | 93.9ms



**Code to reproduce the issue**
```
adb push $bin /data/local/tmp
adb shell chmod +x /data/local/tmp/$bin

bin=benchmark_tflite_model
model=model.tflite
adb shell rm /data/local/tmp/$model
adb push $model /data/local/tmp

adb shell /data/local/tmp/$bin \
    --graph=/data/local/tmp/$model \
    --input_layer=""input"" \
    --input_layer_shape=""1,224,224,3"" \
    --input_layer_type=""float"" \
    --num_runs=200 --warmup_runs=50
```

**Other info / logs**
```
➜  mobilenet_v2_1.0_224 adb shell
sailfish:/ $ getprop | grep -e 'model' -e 'version.sdk' -e 'manufacturer' -e 'hardware' -e 'platform' -e 'revision' -e 'serialno' -e 'product.name' -e 'brand'
[media.recorder.show_manufacturer_and_model]: [true]
[ro.board.platform]: [msm8996]
[ro.boot.hardware]: [sailfish]
[ro.boot.hardware.color]: [SLV00]
[ro.boot.hardware.ddr]: [4096MB,Samsung,LPDDR4]
[ro.boot.hardware.revision]: [PVT]
[ro.boot.hardware.ufs]: [32GB,Samsung]
[ro.boot.serialno]: [FA68X0302645]
[ro.build.version.sdk]: [28]
[ro.frp.pst]: [/dev/block/platform/soc/624000.ufshc/by-name/frp]
[ro.hardware]: [sailfish]
[ro.hardware.power]: [marlin]
[ro.product.brand]: [google]
[ro.product.manufacturer]: [Google]
[ro.product.model]: [Pixel]
[ro.product.name]: [sailfish]
[ro.product.vendor.brand]: [google]
[ro.product.vendor.manufacturer]: [Google]
[ro.product.vendor.model]: [Pixel]
[ro.revision]: [0]
[ro.serialno]: [FA68X0302645]



➜  mobilenet_v2_1.0_224 bin=benchmark_tflite_model
model=model.tflite
adb push $model /data/local/tmp
adb shell /data/local/tmp/$bin \
    --graph=/data/local/tmp/$model \
    --input_layer=""input"" \
    --input_layer_shape=""1,224,224,3"" \
    --input_layer_type=""float"" \
    --num_runs=200 --warmup_runs=50
model.tflite: 1 file pushed. 27.7 MB/s (13978596 bytes in 0.481s)
STARTING!
Num runs: [200]
Inter-run delay (seconds): [-1]
Num threads: [1]
Benchmark name: []
Output prefix: []
Warmup runs: [50]
Graph: [/data/local/tmp/model.tflite]
Input layers: [input]
Input shapes: [1,224,224,3]
Use nnapi : [0]
Loaded model /data/local/tmp/model.tflite
resolved reporter
Initialized session in 34.432ms
Running benchmark for 50 iterations
count=50 first=117086 curr=93140 min=93027 max=117086 avg=94640.8 std=3362

Running benchmark for 200 iterations
count=200 first=93564 curr=93170 min=92931 max=96389 avg=93925 std=963

============================== Run Order ==============================
	             [node type]	  [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                 CONV_2D	    0.000	    4.325	    4.320	  4.601%	  4.601%	     0.000	        1	[MobilenetV2/Conv/Relu6]
	       DEPTHWISE_CONV_2D	    4.321	    2.222	    2.304	  2.455%	  7.056%	     0.000	        1	[MobilenetV2/expanded_conv/depthwise/Relu6]
	                 CONV_2D	    6.626	    2.075	    2.017	  2.148%	  9.204%	     0.000	        1	[MobilenetV2/expanded_conv/project/BatchNorm/FusedBatchNorm]
	                 CONV_2D	    8.643	    6.103	    6.153	  6.554%	 15.758%	     0.000	        1	[MobilenetV2/expanded_conv_1/expand/Relu6]
	       DEPTHWISE_CONV_2D	   14.798	    3.621	    3.705	  3.946%	 19.704%	     0.000	        1	[MobilenetV2/expanded_conv_1/depthwise/Relu6]
	                 CONV_2D	   18.503	    1.865	    1.898	  2.021%	 21.725%	     0.000	        1	[MobilenetV2/expanded_conv_1/project/BatchNorm/FusedBatchNorm]
	                 CONV_2D	   20.402	    3.190	    3.193	  3.401%	 25.126%	     0.000	        1	[MobilenetV2/expanded_conv_2/expand/Relu6]
	       DEPTHWISE_CONV_2D	   23.595	    2.271	    2.255	  2.402%	 27.528%	     0.000	        1	[MobilenetV2/expanded_conv_2/depthwise/Relu6]
	                 CONV_2D	   25.851	    2.882	    2.875	  3.062%	 30.591%	     0.000	        1	[MobilenetV2/expanded_conv_2/project/BatchNorm/FusedBatchNorm]
	                     ADD	   28.728	    0.068	    0.052	  0.056%	 30.647%	     0.000	        1	[MobilenetV2/expanded_conv_2/add]
	                 CONV_2D	   28.780	    3.169	    3.180	  3.387%	 34.034%	     0.000	        1	[MobilenetV2/expanded_conv_3/expand/Relu6]
	       DEPTHWISE_CONV_2D	   31.961	    0.811	    0.837	  0.891%	 34.925%	     0.000	        1	[MobilenetV2/expanded_conv_3/depthwise/Relu6]
	                 CONV_2D	   32.799	    1.275	    1.261	  1.343%	 36.268%	     0.000	        1	[MobilenetV2/expanded_conv_3/project/BatchNorm/FusedBatchNorm]
	                 CONV_2D	   34.060	    1.281	    1.243	  1.324%	 37.592%	     0.000	        1	[MobilenetV2/expanded_conv_4/expand/Relu6]
	       DEPTHWISE_CONV_2D	   35.303	    0.708	    0.723	  0.770%	 38.362%	     0.000	        1	[MobilenetV2/expanded_conv_4/depthwise/Relu6]
	                 CONV_2D	   36.027	    1.714	    1.642	  1.749%	 40.110%	     0.000	        1	[MobilenetV2/expanded_conv_4/project/BatchNorm/FusedBatchNorm]
	                     ADD	   37.669	    0.022	    0.022	  0.023%	 40.133%	     0.000	        1	[MobilenetV2/expanded_conv_4/add]
	                 CONV_2D	   37.691	    1.229	    1.260	  1.343%	 41.476%	     0.000	        1	[MobilenetV2/expanded_conv_5/expand/Relu6]
	       DEPTHWISE_CONV_2D	   38.952	    0.723	    0.738	  0.786%	 42.262%	     0.000	        1	[MobilenetV2/expanded_conv_5/depthwise/Relu6]
	                 CONV_2D	   39.690	    1.719	    1.633	  1.739%	 44.001%	     0.000	        1	[MobilenetV2/expanded_conv_5/project/BatchNorm/FusedBatchNorm]
	                     ADD	   41.325	    0.026	    0.021	  0.023%	 44.024%	     0.000	        1	[MobilenetV2/expanded_conv_5/add]
	                 CONV_2D	   41.346	    1.223	    1.258	  1.340%	 45.364%	     0.000	        1	[MobilenetV2/expanded_conv_6/expand/Relu6]
	       DEPTHWISE_CONV_2D	   42.604	    0.221	    0.225	  0.240%	 45.604%	     0.000	        1	[MobilenetV2/expanded_conv_6/depthwise/Relu6]
	                 CONV_2D	   42.829	    0.695	    0.693	  0.738%	 46.342%	     0.000	        1	[MobilenetV2/expanded_conv_6/project/BatchNorm/FusedBatchNorm]
	                 CONV_2D	   43.523	    1.173	    1.135	  1.208%	 47.550%	     0.000	        1	[MobilenetV2/expanded_conv_7/expand/Relu6]
	       DEPTHWISE_CONV_2D	   44.658	    0.328	    0.334	  0.356%	 47.906%	     0.000	        1	[MobilenetV2/expanded_conv_7/depthwise/Relu6]
	                 CONV_2D	   44.992	    1.253	    1.258	  1.340%	 49.246%	     0.000	        1	[MobilenetV2/expanded_conv_7/project/BatchNorm/FusedBatchNorm]
	                     ADD	   46.251	    0.014	    0.013	  0.014%	 49.260%	     0.000	        1	[MobilenetV2/expanded_conv_7/add]
	                 CONV_2D	   46.264	    1.104	    1.123	  1.196%	 50.456%	     0.000	        1	[MobilenetV2/expanded_conv_8/expand/Relu6]
	       DEPTHWISE_CONV_2D	   47.387	    0.407	    0.352	  0.375%	 50.831%	     0.000	        1	[MobilenetV2/expanded_conv_8/depthwise/Relu6]
	                 CONV_2D	   47.740	    1.249	    1.252	  1.334%	 52.164%	     0.000	        1	[MobilenetV2/expanded_conv_8/project/BatchNorm/FusedBatchNorm]
	                     ADD	   48.993	    0.010	    0.012	  0.012%	 52.177%	     0.000	        1	[MobilenetV2/expanded_conv_8/add]
	                 CONV_2D	   49.004	    1.103	    1.127	  1.200%	 53.377%	     0.000	        1	[MobilenetV2/expanded_conv_9/expand/Relu6]
	       DEPTHWISE_CONV_2D	   50.131	    0.336	    0.344	  0.366%	 53.743%	     0.000	        1	[MobilenetV2/expanded_conv_9/depthwise/Relu6]
	                 CONV_2D	   50.475	    1.295	    1.250	  1.331%	 55.074%	     0.000	        1	[MobilenetV2/expanded_conv_9/project/BatchNorm/FusedBatchNorm]
	                     ADD	   51.726	    0.013	    0.013	  0.014%	 55.088%	     0.000	        1	[MobilenetV2/expanded_conv_9/add]
	                 CONV_2D	   51.739	    1.107	    1.128	  1.201%	 56.289%	     0.000	        1	[MobilenetV2/expanded_conv_10/expand/Relu6]
	       DEPTHWISE_CONV_2D	   52.867	    0.333	    0.340	  0.362%	 56.652%	     0.000	        1	[MobilenetV2/expanded_conv_10/depthwise/Relu6]
	                 CONV_2D	   53.208	    1.818	    1.780	  1.896%	 58.548%	     0.000	        1	[MobilenetV2/expanded_conv_10/project/BatchNorm/FusedBatchNorm]
	                 CONV_2D	   54.989	    2.735	    2.701	  2.877%	 61.424%	     0.000	        1	[MobilenetV2/expanded_conv_11/expand/Relu6]
	       DEPTHWISE_CONV_2D	   57.690	    0.539	    0.547	  0.582%	 62.007%	     0.000	        1	[MobilenetV2/expanded_conv_11/depthwise/Relu6]
	                 CONV_2D	   58.238	    2.642	    2.653	  2.825%	 64.832%	     0.000	        1	[MobilenetV2/expanded_conv_11/project/BatchNorm/FusedBatchNorm]
	                     ADD	   60.891	    0.015	    0.015	  0.016%	 64.848%	     0.000	        1	[MobilenetV2/expanded_conv_11/add]
	                 CONV_2D	   60.906	    2.671	    2.713	  2.889%	 67.737%	     0.000	        1	[MobilenetV2/expanded_conv_12/expand/Relu6]
	       DEPTHWISE_CONV_2D	   63.620	    0.502	    0.513	  0.546%	 68.284%	     0.000	        1	[MobilenetV2/expanded_conv_12/depthwise/Relu6]
	                 CONV_2D	   64.133	    2.640	    2.648	  2.820%	 71.104%	     0.000	        1	[MobilenetV2/expanded_conv_12/project/BatchNorm/FusedBatchNorm]
	                     ADD	   66.782	    0.016	    0.019	  0.020%	 71.124%	     0.000	        1	[MobilenetV2/expanded_conv_12/add]
	                 CONV_2D	   66.801	    2.710	    2.721	  2.898%	 74.022%	     0.000	        1	[MobilenetV2/expanded_conv_13/expand/Relu6]
	       DEPTHWISE_CONV_2D	   69.523	    0.146	    0.149	  0.159%	 74.181%	     0.000	        1	[MobilenetV2/expanded_conv_13/depthwise/Relu6]
	                 CONV_2D	   69.672	    1.130	    1.156	  1.231%	 75.412%	     0.000	        1	[MobilenetV2/expanded_conv_13/project/BatchNorm/FusedBatchNorm]
	                 CONV_2D	   70.829	    2.117	    2.120	  2.258%	 77.670%	     0.000	        1	[MobilenetV2/expanded_conv_14/expand/Relu6]
	       DEPTHWISE_CONV_2D	   72.950	    0.201	    0.207	  0.221%	 77.891%	     0.000	        1	[MobilenetV2/expanded_conv_14/depthwise/Relu6]
	                 CONV_2D	   73.157	    2.105	    2.092	  2.228%	 80.119%	     0.000	        1	[MobilenetV2/expanded_conv_14/project/BatchNorm/FusedBatchNorm]
	                     ADD	   75.250	    0.007	    0.008	  0.009%	 80.128%	     0.000	        1	[MobilenetV2/expanded_conv_14/add]
	                 CONV_2D	   75.258	    2.048	    2.100	  2.237%	 82.365%	     0.000	        1	[MobilenetV2/expanded_conv_15/expand/Relu6]
	       DEPTHWISE_CONV_2D	   77.360	    0.201	    0.204	  0.217%	 82.583%	     0.000	        1	[MobilenetV2/expanded_conv_15/depthwise/Relu6]
	                 CONV_2D	   77.564	    2.113	    2.094	  2.231%	 84.813%	     0.000	        1	[MobilenetV2/expanded_conv_15/project/BatchNorm/FusedBatchNorm]
	                     ADD	   79.659	    0.007	    0.008	  0.008%	 84.821%	     0.000	        1	[MobilenetV2/expanded_conv_15/add]
	                 CONV_2D	   79.667	    2.118	    2.096	  2.232%	 87.054%	     0.000	        1	[MobilenetV2/expanded_conv_16/expand/Relu6]
	       DEPTHWISE_CONV_2D	   81.764	    0.196	    0.199	  0.212%	 87.266%	     0.000	        1	[MobilenetV2/expanded_conv_16/depthwise/Relu6]
	                 CONV_2D	   81.963	    4.196	    4.249	  4.526%	 91.792%	     0.000	        1	[MobilenetV2/expanded_conv_16/project/BatchNorm/FusedBatchNorm]
	                 CONV_2D	   86.213	    6.016	    6.110	  6.507%	 98.299%	     0.000	        1	[MobilenetV2/Conv_1/Relu6]
	         AVERAGE_POOL_2D	   92.324	    0.034	    0.035	  0.037%	 98.336%	     0.000	        1	[MobilenetV2/Logits/AvgPool]
	                 CONV_2D	   92.359	    1.408	    1.524	  1.623%	 99.960%	     0.000	        1	[MobilenetV2/Logits/Conv2d_1c_1x1/BiasAdd]
	                 RESHAPE	   93.884	    0.003	    0.003	  0.003%	 99.963%	     0.000	        1	[MobilenetV2/Logits/Squeeze]
	                 SOFTMAX	   93.887	    0.034	    0.035	  0.037%	100.000%	     0.000	        1	[MobilenetV2/Predictions/Reshape_1]

============================== Top by Computation Time ==============================
	             [node type]	  [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                 CONV_2D	    8.643	    6.103	    6.153	  6.554%	  6.554%	     0.000	        1	[MobilenetV2/expanded_conv_1/expand/Relu6]
	                 CONV_2D	   86.213	    6.016	    6.110	  6.507%	 13.061%	     0.000	        1	[MobilenetV2/Conv_1/Relu6]
	                 CONV_2D	    0.000	    4.325	    4.320	  4.601%	 17.663%	     0.000	        1	[MobilenetV2/Conv/Relu6]
	                 CONV_2D	   81.963	    4.196	    4.249	  4.526%	 22.189%	     0.000	        1	[MobilenetV2/expanded_conv_16/project/BatchNorm/FusedBatchNorm]
	       DEPTHWISE_CONV_2D	   14.798	    3.621	    3.705	  3.946%	 26.135%	     0.000	        1	[MobilenetV2/expanded_conv_1/depthwise/Relu6]
	                 CONV_2D	   20.402	    3.190	    3.193	  3.401%	 29.536%	     0.000	        1	[MobilenetV2/expanded_conv_2/expand/Relu6]
	                 CONV_2D	   28.780	    3.169	    3.180	  3.387%	 32.923%	     0.000	        1	[MobilenetV2/expanded_conv_3/expand/Relu6]
	                 CONV_2D	   25.851	    2.882	    2.875	  3.062%	 35.985%	     0.000	        1	[MobilenetV2/expanded_conv_2/project/BatchNorm/FusedBatchNorm]
	                 CONV_2D	   66.801	    2.710	    2.721	  2.898%	 38.884%	     0.000	        1	[MobilenetV2/expanded_conv_13/expand/Relu6]
	                 CONV_2D	   60.906	    2.671	    2.713	  2.889%	 41.773%	     0.000	        1	[MobilenetV2/expanded_conv_12/expand/Relu6]

Number of nodes executed: 66
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	                 CONV_2D	       36	    79.637	    84.851%	    84.851%	     0.000	       36
	       DEPTHWISE_CONV_2D	       17	    13.970	    14.885%	    99.736%	     0.000	       17
	                     ADD	       10	     0.177	     0.189%	    99.924%	     0.000	       10
	                 SOFTMAX	        1	     0.035	     0.037%	    99.962%	     0.000	        1
	         AVERAGE_POOL_2D	        1	     0.034	     0.036%	    99.998%	     0.000	        1
	                 RESHAPE	        1	     0.002	     0.002%	   100.000%	     0.000	        1

Timings (microseconds): count=200 first=93531 curr=93135 min=92902 max=96356 avg=93888.1 std=962
Memory (bytes): count=0
66 nodes observed


Average inference timings in us: Warmup: 94640.8, Init: 34432, no stats: 93925
```"
23130,Keras-GPU Fail to create cudnn handle,"Hi,

Just quick question regarding using Keras-GPU on Windows. Essentially I was playing with implementation of one of the Deep Reinforcement Learning algorithms (DQN) using Keras with Tensorflow-GPU backend for AirSim self-driving car simulation. All good but when it comes to training the NN the TF Fails with the error below. I tried to see if there is something wrong with my script by running simple MNSIT NN example and it fails with exact same reason. What I've noticed is that this ONLY happens when the environment (AirSim application) is running in the background, nothing fails if its not running. Additionally (since I have access to more than one GPU), when the GPUs are not SLI connected and work as 2 separate units it also works just fine by using GPU1 for application and GPU2 for training. Already posted this on AirSim git but was redirected here. Anyone else encountered this issue before?

I'm using CUDA 9 with and TF 1.5.0 on Windows 10 if that helps, updating TF didn't help etc, as I said all works when the app is not running

Epoch 1/1
2018-09-28 15:02:31.247055: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:03:00.0, compute capability: 6.1)

2018-09-28 15:02:33.168438: E C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\stream_executor\cuda\cuda_dnn.cc:385] could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED

2018-09-28 15:02:33.173026: E C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\stream_executor\cuda\cuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM

2018-09-28 15:02:33.178209: F C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\kernels\conv_ops.cc:717] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo(), &algorithms)

Cheers"
23129,ModuleNotFoundError: No module named 'tensorflow.core.protobuf',"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10 Pro 1803 (OS BUILD:17134.112
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:no
- TensorFlow installed from (source or binary):pip,using conda. Followed instructions from [this page](https://www.tensorflow.org/install/pip)
- TensorFlow version:1.11.0
- Python version:3.6.6
- Installed using virtualenv? pip? conda?:conda and pip [using this page](https://www.tensorflow.org/install/pip)
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2017 NVIDIA Corporation
Built on Fri_Sep__1_21:08:32_Central_Daylight_Time_2017
Cuda compilation tools, release 9.0, V9.0.176
- GPU model and memory: nvidia Geforce GTX 1060 3GB



**Describe the problem**
![image](https://user-images.githubusercontent.com/36691630/47257924-eb7df300-d4b1-11e8-9733-68079dbdf3fd.png)
Executing the first line shows an error

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Nothing, just tried to execute the line for my model and it started showing an error. Googled the problem but the solutions were mainly for ""google.protobuf""and [this one](https://github.com/tensorflow/tensorflow/issues/890). Both didn't work.
Tried to create a new conda env and followed the above installation link, activated the environment, opened jupyter notebook, and still it is showing the error.


**Any other info / logs**
![image](https://user-images.githubusercontent.com/36691630/47257981-9db5ba80-d4b2-11e8-8749-e59d27d1157c.png)
The above image is of the new environment I tried to create, but still it showed the error.
Below packages is of the original env(the one which opens when you first open anaconda prompt, idk what it is called)
![image](https://user-images.githubusercontent.com/36691630/47258011-f38a6280-d4b2-11e8-9d1f-62b3709b1e8c.png)
![image](https://user-images.githubusercontent.com/36691630/47258027-13ba2180-d4b3-11e8-9d4b-7f9293a66ee7.png)
![image](https://user-images.githubusercontent.com/36691630/47258035-1fa5e380-d4b3-11e8-9b51-44de97595079.png)
![image](https://user-images.githubusercontent.com/36691630/47258040-2a607880-d4b3-11e8-9745-11a955402ab2.png)
![image](https://user-images.githubusercontent.com/36691630/47258047-449a5680-d4b3-11e8-8251-155899f74e41.png)
![image](https://user-images.githubusercontent.com/36691630/47258059-5c71da80-d4b3-11e8-842b-811186508874.png)


"
23128,Difference in higher order tf.gradients(),"**System information**

- OS: OS X 10.13.6
- TensorFlow installed from (source or binary): Anaconda
- TensorFlow version (use command below): 1.11
- Python version: 3.6.6
- Have I written custom code: No
- Bazel version: N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A
- Exact command to reproduce: See below.
- Mobile device: N/A

**Describe the current behavior**
I have a cost function based on the derivative of the output of the network c w.r.t to the inputs of the network x and t.  I wish to replace x and t by one input tensor X=[x,t]. To test my code, I have the written the following function which returns the gradients:

`def cost_diffusion_test(x, t, X ,c):
    c_t = tf.gradients(c, t)[0]
    c_x = tf.gradients(c, x)[0]
    c_xx = tf.gradients(c_x, x)[0]

    dc = tf.gradients(c, X)[0]
    d2c = tf.gradients(dc[:, 0:1], X)[0]

    return c_t, c_x, c_xx, dc, d2c`

c_t and dc[:,1:2], c_x and dc[:,0:1] are the same as expected through np.array_equal(). c_xx and d2c[:,0:1] are not. When I study the difference d2c[:,0:1]-c_xx the I observe many zeros and a few terms on the order of e-08. Intriguingly, the maximum and minimum difference is 5.9604645e-08. If this would be a tensorflow-> numpy error, why is the first order derivative correct?

**Describe the expected behavior**
np.array_equal(d2c[:,0:1], c_xx)==True
"
23127,Problems when building tensorflow with CUDA,"When I built tensorflow from source with CUDA using bazel,I got the following errors:

    external/com_google_absl/absl/strings/string_view.h(496): warning: expression has no effect
    external/com_google_absl/absl/strings/str_cat.h(259): error: expression must have a constant value
    external/com_google_absl/absl/strings/str_cat.h(259): error: expression must have a constant value

I add some codes like `std::cout<<""using cpu for Launch<Device>""<<std::endl;`.Is there anything wrong? Thanks for any help.
"
23126,"Tensorflow - Disaccording TF METRICS (Accuracy, Precision, Recall)","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.4
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.10.1
- Python version: 2.7
- GCC/Compiler version (if compiling from source): 5.4.0
- CUDA/cuDNN version: release 7.5
- GPU model and memory: Tesla K80 - 11.17GiB
- Bazel version: N/A
- Exact command to reproduce: N/A
- Mobile device: N/A

I am working with a CNN for an instrument recognition problem, using **IRMAS** dataset. 

The **IRMAS** dataset has a clear **training** set, based only on singular instruments, therefore each instrument (guitar, violin, etc) has its own directory with multiple samples (*multi-classification*). The **testing** set, differently, contains polyphonic music, therefore multiple classes of instruments may appear in the same song (*multi-classification*, *multi-label*).

This is the final part of my network with the prediction and loss calculation, I am using the `Estimator API`.

    

    ### previous part of network is omitted
    
    # logits layer
    logits = tf.layers.dense(inputs=dropout, units=labels.shape[1])

    print('Shape Logits:', logits.shape)
       
   
    predictions = {
        ""classes"": tf.argmax(input=logits, axis=1),
        ""probabilities"": tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(labels, tf.float32), logits=tf.cast(logits, tf.float32), name=""sigmoid_tensor"")
    }

    if mode == tf.estimator.ModeKeys.PREDICT:
        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)

    loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=labels, logits=logits)

    if mode == tf.estimator.ModeKeys.TRAIN:
        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)
        #optimizer = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.96)
        train_op = optimizer.minimize(
                                  loss=loss,
                                  global_step=tf.train.get_global_step())
        logging_hook = tf.train.LoggingTensorHook({""loss"" : loss}, every_n_iter=10)
        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op, training_hooks = [logging_hook])

    eval_metric_ops = {
        ""accuracy"": tf.metrics.accuracy(
        labels=tf.argmax(input=labels, axis=1),
        predictions=predictions[""classes""]),

        ""recall"": tf.metrics.recall(
        labels=tf.argmax(input=labels, axis=1),
        predictions=predictions[""classes""]),

        ""precision"": tf.metrics.precision(
        labels=tf.argmax(input=labels, axis=1),
        predictions=predictions[""classes""])
    }


After few epochs, the training process seems to work since the loss goes down quite smoothly. However, the tensorflow metrics needed to evaluate the model does not seem to agree.

These are my results on the evaluation set (15% of the training set):

`{'recall': 0.9989496, 'accuracy': 0.40656063, 'global_step': 1428, 'precision': 0.95004994, 'loss': 0.2432195}`

These are the results obtained from the testing set:

`{'recall': 0.9981618, 'accuracy': 0.26530612, 'global_step': 1428, 'precision': 0.96533334, 'loss': 0.46097097}`

In my opinion, the accuracy value seems to be the only right, since decreases in the testing set, which is composed by polyphonic music rather than singular instruments. I don't understand why I get these results for precision and recall, they should be lower compared to the accuracy I get. 

I've tried to display the FP, FN, TN and TP and they seem to be in accord with precision and recall. In fact, by computing the accuracy with them I got around 0.97 (too high for this classification problem).

My question is why the metrics of tensorflow are so in disaccord? I could not find any useful information in the documentation.

Thank you in advance
"
23123,TensorFlow 1.11 CUDA 9.0 with CUDNN7.3 CUDA_ERROR_LAUNCH_TIMEOUT,"Here is my information first:
- cuda: CUDA 9.0
- cudnn: cudnn7.3 for CUDA9.0
- Nvidia GTX 1080Ti
- TensorFlow 1.11 from pip3 with GPU support

The library can work and tensorflow successfully import from python. **but everytime inference or train a dataset only a while it will got this error**:

```
2018-10-20 16:47:07.055721: E tensorflow/stream_executor/cuda/cuda_driver.cc:981] failed to synchronize the stop event: CUDA_ERROR_LAUNCH_TIMEOUT: the launch timed out and was terminated
2018-10-20 16:47:07.055749: E tensorflow/stream_executor/cuda/cuda_timer.cc:55] Internal: error destroying CUDA event in context 0xff40eb0: CUDA_ERROR_LAUNCH_TIMEOUT: the launch timed out and was terminated
2018-10-20 16:47:07.055755: E tensorflow/stream_executor/cuda/cuda_timer.cc:60] Internal: error destroying CUDA event in context 0xff40eb0: CUDA_ERROR_LAUNCH_TIMEOUT: the launch timed out and was terminated
2018-10-20 16:47:07.055789: F tensorflow/stream_executor/cuda/cuda_dnn.cc:211] Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream.
[1]    3276 abort (core dumped)  python3 demo.py

```

Is this a bug or the driver issue? I have ever got this error before, if anyone got this error leave a comment below to let me know!! I am stuck here right now"
23122,Failing to build tensorflow c++ api on raspberry pi model 3 b+,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspbian Stretch kernel 4.14
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): used this command git clone --recurse-submodules https://github.com/tensorflow/tensorflow.git
- TensorFlow version:r1.11
- Python version:3.5.3
- Installed using virtualenv? pip? conda?: 
- Bazel version (if compiling from source): 0.18.0
- GCC/Compiler version (if compiling from source):6.3.0
- CUDA/cuDNN version: NA
- GPU model and memory: NA



**Describe the problem**
When I follow this guide (https://gist.github.com/EKami/9869ae6347f68c592c5b5cd181a3b205) and attempt to build the c++ api using bazel, it fails.
**Provide the exact sequence of commands / steps that you executed before running into the problem**
I followed the guide above except for the portion where it says to replace:
native.new_http_archive(
      name = ""eigen_archive"",
      urls = [
          ""http://mirror.bazel.build/bitbucket.org/eigen/eigen/get/f3a22f35b044.tar.gz"",
          ""https://bitbucket.org/eigen/eigen/get/f3a22f35b044.tar.gz"",
      ],
      sha256 = ""ca7beac153d4059c02c8fc59816c82d54ea47fe58365e8aded4082ded0b820c4"",
      strip_prefix = ""eigen-eigen-f3a22f35b044"",
      build_file = str(Label(""//third_party:eigen.BUILD"")),
  )
because doing so gave me problems. I also made sure to use the newest versions of the software involved for bazel and other dependent software. I then attempted to build tensorflow using the command: bazel build -c opt --config=monolithic --local_resources 1024,1.0,1.0 --verbose_failures //tensorflow:libtensorflow_cc.so


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
ERROR: /home/pi/tf/tensorflow/tensorflow/BUILD:558:1: Linking of rule '//tensorflow:libtensorflow_cc.so' failed (Exit 1): gcc failed: error executing command 
  (cd /home/pi/.cache/bazel/_bazel_pi/4770c5ca1786316d370c900c0b614a6d/execroot/org_tensorflow && \
  exec env - \
    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/games:/usr/games \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python \
    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \
    TF_DOWNLOAD_CLANG=0 \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL_SYCL=0 \
  /usr/bin/gcc -shared -o bazel-out/arm-opt/bin/tensorflow/libtensorflow_cc.so -z defs -Wl,--version-script tensorflow/tf_version_script.lds '-Wl,-rpath,$ORIGIN/' -Wl,-soname,libtensorflow_cc.so -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread '-fuse-ld=gold' -Wl,-no-as-needed -Wl,-z,relro,-z,now -B/usr/bin -B/usr/bin -pass-exit-codes -Wl,--gc-sections -Wl,@bazel-out/arm-opt/bin/tensorflow/libtensorflow_cc.so-2.params)
bazel-out/arm-opt/bin/tensorflow/core/kernels/_objs/list_kernels/list_kernels.pic.o:list_kernels.cc:function tensorflow::TensorListStack<Eigen::ThreadPoolDevice, tensorflow::bfloat16>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'void tensorflow::ConcatCPU<tensorflow::bfloat16>(tensorflow::DeviceBase*, std::vector<std::unique_ptr<tensorflow::TTypes<tensorflow::bfloat16, 2, int>::ConstMatrix, std::default_delete<tensorflow::TTypes<tensorflow::bfloat16, 2, int>::ConstMatrix> >, std::allocator<std::unique_ptr<tensorflow::TTypes<tensorflow::bfloat16, 2, int>::ConstMatrix, std::default_delete<tensorflow::TTypes<tensorflow::bfloat16, 2, int>::ConstMatrix> > > > const&, tensorflow::TTypes<tensorflow::bfloat16, 2, int>::Matrix*)'
bazel-out/arm-opt/bin/tensorflow/core/kernels/_objs/list_kernels/list_kernels.pic.o:list_kernels.cc:function tensorflow::TensorListGather<Eigen::ThreadPoolDevice, tensorflow::bfloat16>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'void tensorflow::ConcatCPU<tensorflow::bfloat16>(tensorflow::DeviceBase*, std::vector<std::unique_ptr<tensorflow::TTypes<tensorflow::bfloat16, 2, int>::ConstMatrix, std::default_delete<tensorflow::TTypes<tensorflow::bfloat16, 2, int>::ConstMatrix> >, std::allocator<std::unique_ptr<tensorflow::TTypes<tensorflow::bfloat16, 2, int>::ConstMatrix, std::default_delete<tensorflow::TTypes<tensorflow::bfloat16, 2, int>::ConstMatrix> > > > const&, tensorflow::TTypes<tensorflow::bfloat16, 2, int>::Matrix*)'
collect2: error: ld returned 1 exit status
Target //tensorflow:libtensorflow_cc.so failed to build
INFO: Elapsed time: 21254.436s, Critical Path: 1956.26s
INFO: 2708 processes: 2708 local.
FAILED: Build did NOT complete successfully
"
23119,global name 'cuda_toolkit_path_full' is not defined,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Ubuntu 18.04.1 LTS  64 bit
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**:   Source
- **TensorFlow Version**:  r.1.11  
- **Python version**:  3.6
- **Bazel version (if compiling from source)**:  0.15.2 (tried/same problem with 0.15.2, 1.16.1, 0.18.0)
- **GCC/Compiler version (if compiling from source)**:  7.3.0
- **CUDA/cuDNN version**:  10.0 / 7.7.3
- **GPU model and memory**: RTX 2080 TI 11GB
- **Exact command to reproduce**:  ./configure with answers below

### Describe the problem
configure fails with following error:

anatolii@workstation:~/tensorflow$ ./configure
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.15.2 installed.
Please specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3


Found possible Python library paths:
  /usr/lib/python3/dist-packages
  /usr/local/lib/python3.6/dist-packages
Please input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]

Do you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: 
jemalloc as malloc support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: 
Google Cloud Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Hadoop File System support? [Y/n]: 
Hadoop File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Amazon AWS Platform support? [Y/n]: 
Amazon AWS Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Apache Kafka Platform support? [Y/n]: 
Apache Kafka Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with XLA JIT support? [y/N]: 
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with GDR support? [y/N]: 
No GDR support will be enabled for TensorFlow.

Do you wish to build TensorFlow with VERBS support? [y/N]: 
No VERBS support will be enabled for TensorFlow.

Do you wish to build TensorFlow with nGraph support? [y/N]: 
No nGraph support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: 
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]: CUDA 10.0


Please specify the location where CUDA CUDA 10.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 


Traceback (most recent call last):
  File ""./configure.py"", line 1592, in <module>
    main()
  File ""./configure.py"", line 1527, in main
    set_tf_cuda_version(environ_cp)
  File ""./configure.py"", line 855, in set_tf_cuda_version
    (tf_cuda_version, cuda_toolkit_path_full))
NameError: global name 'cuda_toolkit_path_full' is not defined

"
23118,Pure Virtual Method called - build for ARMv7 ,"**System information**
- OS Platform and Distribution:  Linux Debian 8 Jessie
- TensorFlow installed from: source
- TensorFlow version: r1.12
- Python version: 3.4.2
- compiled and built as per instructions on Ubuntu 18.04.1 cross-compile for Raspberry Pi: https://www.tensorflow.org/install/source_rpi


**Problem**
When running tensorflow on my device, the following error is thrown when accessing any cameras using the tensorflow.python.keras.layers.Input() method:
```
pure virtual method called
terminate called without an active exception
Aborted
```
The device uses an R8 ARMv7 processor 


**Commands / Steps**
```
from tensorflow.python.keras.layers import Input
```

```
img_in = Input(shape=(120, 160, 3), name='img_in')
```


**Any other info / logs**

Complete system output:
```
chip@chip:~/mycar$ python3
Python 3.4.2 (default, Sep 26 2018, 05:38:50)
[GCC 4.9.2] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from tensorflow.python.keras.layers import Input
>>> img_in = Input(shape=(120, 160, 3), name='img_in')
pure virtual method called
terminate called without an active exception
Aborted
```

We have done some research on this issue and have not found too many similar cases.  We found one similar issue that stated that removing the marsh flag from compile would resolve the issue, however, this was not the case.

Any help would be greatly appreciated."
23117,Build fail on Windows with compute capability 6.1,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:  source 
- **TensorFlow version (use command below)**: master
- **Python version**: 3.5.4
- **Bazel version (if compiling from source)**: 1.18.0
- **GCC/Compiler version (if compiling from source)**: Visual C++ Build Tools 2015
- **CUDA/cuDNN version**: CUDA 9.2, cuDNN 7.2.1
- **GPU model and memory**: NVIDIA 1080 GTX 
- **Exact command to reproduce**: bazel build --config=opt --config=cuda --copt=-nvcc_options=disable-warnings --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
I am trying to build tensorflow from source. 
Build fail if compiling with compute capability 6.1 (but build fine with compute capability 3.7).
I am getting the following error when I run bazel build command.

### Source code / logs
```
ERROR: C:/tensorflow/tensorflow/core/kernels/BUILD:4714:1: C++ compilation of rule '//tensorflow/core/kernels:multinomial_op_gpu' failed (Exit 1): msvc_wrapper_for_nvcc.bat failed: error executing command
  cd C:/users/em/_bazel_em/xv6zejqw/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.2
    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.2
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE;C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt;C:\Program Files (x86)\Windows Kits\8.1\include\shared;C:\Program Files (x86)\Windows Kits\8.1\include\um;C:\Program Files (x86)\Windows Kits\8.1\include\winrt;
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\LIB\amd64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.10240.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\8.1\lib\winv6.3\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\amd64;C:\WINDOWS\Microsoft.NET\Framework64\v4.0.30319;C:\WINDOWS\Microsoft.NET\Framework64\;C:\Program Files (x86)\Windows Kits\8.1\bin\x64;C:\Program Files (x86)\Windows Kits\8.1\bin\x86;;C:\WINDOWS\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/em/AppData/Local/Programs/Python/Python35/python.exe
    SET PYTHON_LIB_PATH=C:/Users/em/AppData/Local/Programs/Python/Python35/lib/site-packages
    SET TEMP=C:\Users\em\AppData\Local\Temp
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1
    SET TF_CUDA_VERSION=9.2
    SET TF_CUDNN_VERSION=7
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
    SET TMP=C:\Users\em\AppData\Local\Temp
  external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.bat /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Ibazel-out/x64_windows-opt/bin /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/genfiles/external/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/genfiles/external/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/bazel_tools /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/nsync /Ibazel-out/x64_windows-opt/genfiles/external/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/gif_archive /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive /Ibazel-out/x64_windows-opt/bin/external/gif_archive /Iexternal/jpeg /Ibazel-out/x64_windows-opt/genfiles/external/jpeg /Ibazel-out/x64_windows-opt/bin/external/jpeg /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/genfiles/external/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/genfiles/external/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/genfiles/external/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/genfiles/external/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/genfiles/external/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/gif_archive/lib /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/lib /Ibazel-out/x64_windows-opt/bin/external/gif_archive/lib /Iexternal/gif_archive/windows /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/windows /Ibazel-out/x64_windows-opt/bin/external/gif_archive/windows /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/local_config_cuda/cuda/cuda/include/crt /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cuda/include/crt /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include/crt /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DTF_USE_SNAPPY /showIncludes /MD /O2 /DNDEBUG -w /arch:AVX2 -nvcc_options=disable-warnings -DGOOGLE_CUDA=1 -DTENSORFLOW_MONOLITHIC_BUILD /DPLATFORM_WINDOWS /DEIGEN_HAS_C99_MATH /DTENSORFLOW_USE_EIGEN_THREADPOOL /DEIGEN_AVOID_STL_ARRAY /Iexternal/gemmlowp /wd4018 /wd4577 /DNOGDI /DTF_COMPILE_LIBRARY -x cuda -DGOOGLE_CUDA=1 -nvcc_options=relaxed-constexpr -nvcc_options=ftz=true /Fobazel-out/x64_windows-opt/bin/tensorflow/core/kernels/_objs/multinomial_op_gpu/multinomial_op_gpu.cu.o /c tensorflow/core/kernels/multinomial_op_gpu.cu.cc
c:\users\em\_bazel_em\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\eigen\src/Core/arch/CUDA/Half.h(212): error: more than one instance of overloaded function ""__hadd"" matches the argument list:
            function ""__hadd(int, int)""
            function ""__hadd(__half, __half)""
            argument types are: (const Eigen::half, const Eigen::half)

1 error detected in the compilation of ""C:/Users/em/AppData/Local/Temp/nvcc_inter_files_tmp_dir/multinomial_op_gpu.cu.cpp1.ii"".
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 772.580s, Critical Path: 147.05s
INFO: 1931 processes: 1931 local.
FAILED: Build did NOT complete successfully
```"
23115,Cannot build Tensorflow android example with Android Studio,"When I build the project in android studio it goes through with just 1 warning asking me to not specify the JDK version in the manifest file. I don't think this is the cause of the error, and for now I have left it as is.

When I run the project on a virtual device (tried pixel 2 with Android Pie, and Nexus 5X with Android 7), both give the same error stack. Here it is below. Help would be GREATLY appreciated. I have tried compiling the example on both mac and linux, and both seem to fail. 

```
Executing tasks: [:assembleDebug]

:buildInfoDebugLoader
:checkDebugClasspath UP-TO-DATE
:preBuild UP-TO-DATE
:preDebugBuild UP-TO-DATE
:compileDebugAidl NO-SOURCE
:compileDebugRenderscript UP-TO-DATE
:checkDebugManifest UP-TO-DATE
:generateDebugBuildConfig UP-TO-DATE
:prepareLintJar UP-TO-DATE
:mainApkListPersistenceDebug UP-TO-DATE
:generateDebugResValues UP-TO-DATE
:generateDebugResources UP-TO-DATE
:mergeDebugResources UP-TO-DATE
:createDebugCompatibleScreenManifests UP-TO-DATE
:processDebugManifest
:splitsDiscoveryTaskDebug UP-TO-DATE
:processDebugResources
:generateDebugSources
:javaPreCompileDebug
/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/SpeechActivity.java:54: error: package org.tensorflow.contrib.android does not exist
import org.tensorflow.contrib.android.TensorFlowInferenceInterface;
                                     ^
/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/SpeechActivity.java:95: error: cannot find symbol
  private TensorFlowInferenceInterface inferenceInterface;
          ^
  symbol:   class TensorFlowInferenceInterface
  location: class SpeechActivity
/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/StylizeActivity.java:57: error: package org.tensorflow.contrib.android does not exist
import org.tensorflow.contrib.android.TensorFlowInferenceInterface;
                                     ^
/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/StylizeActivity.java:117: error: cannot find symbol
  private TensorFlowInferenceInterface inferenceInterface;
          ^
  symbol:   class TensorFlowInferenceInterface
  location: class StylizeActivity
/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowImageClassifier.java:30: error: cannot find symbol
import org.tensorflow.Operation;
                     ^
  symbol:   class Operation
  location: package org.tensorflow
/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowImageClassifier.java:31: error: package org.tensorflow.contrib.android does not exist
import org.tensorflow.contrib.android.TensorFlowInferenceInterface;
                                     ^
/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowImageClassifier.java:57: error: cannot find symbol
  private TensorFlowInferenceInterface inferenceInterface;
          ^
  symbol:   class TensorFlowInferenceInterface
  location: class TensorFlowImageClassifier
/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowMultiBoxDetector.java:32: error: cannot find symbol
import org.tensorflow.Graph;
                     ^
  symbol:   class Graph
  location: package org.tensorflow
/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowMultiBoxDetector.java:33: error: cannot find symbol
import org.tensorflow.Operation;
                     ^
  symbol:   class Operation
  location: package org.tensorflow
/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowMultiBoxDetector.java:34: error: package org.tensorflow.contrib.android does not exist
import org.tensorflow.contrib.android.TensorFlowInferenceInterface;
                                     ^
/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowMultiBoxDetector.java:63: error: cannot find symbol
  private TensorFlowInferenceInterface inferenceInterface;
          ^
  symbol:   class TensorFlowInferenceInterface
  location: class TensorFlowMultiBoxDetector
/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowObjectDetectionAPIModel.java:31: error: cannot find symbol
import org.tensorflow.Graph;
                     ^
  symbol:   class Graph
  location: package org.tensorflow
/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowObjectDetectionAPIModel.java:32: error: cannot find symbol
import org.tensorflow.Operation;
                     ^
  symbol:   class Operation
  location: package org.tensorflow
/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowObjectDetectionAPIModel.java:33: error: package org.tensorflow.contrib.android does not exist
import org.tensorflow.contrib.android.TensorFlowInferenceInterface;
                                     ^
/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowObjectDetectionAPIModel.java:62: error: cannot find symbol
  private TensorFlowInferenceInterface inferenceInterface;
          ^
  symbol:   class TensorFlowInferenceInterface
  location: class TensorFlowObjectDetectionAPIModel
/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowYoloDetector.java:26: error: package org.tensorflow.contrib.android does not exist
import org.tensorflow.contrib.android.TensorFlowInferenceInterface;
                                     ^
/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowYoloDetector.java:87: error: cannot find symbol
  private TensorFlowInferenceInterface inferenceInterface;
          ^
  symbol:   class TensorFlowInferenceInterface
  location: class TensorFlowYoloDetector
/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/SpeechActivity.java:152: error: cannot find symbol
    inferenceInterface = new TensorFlowInferenceInterface(getAssets(), MODEL_FILENAME);
                             ^
  symbol:   class TensorFlowInferenceInterface
  location: class SpeechActivity
/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/StylizeActivity.java:365: error: cannot find symbol
    inferenceInterface = new TensorFlowInferenceInterface(getAssets(), MODEL_FILE);
                             ^
  symbol:   class TensorFlowInferenceInterface
  location: class StylizeActivity
/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowImageClassifier.java:103: error: cannot find symbol
    c.inferenceInterface = new TensorFlowInferenceInterface(assetManager, modelFilename);
                               ^
  symbol:   class TensorFlowInferenceInterface
  location: class TensorFlowImageClassifier
/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowImageClassifier.java:106: error: cannot find symbol
    final Operation operation = c.inferenceInterface.graphOperation(outputName);
          ^
  symbol:   class Operation
  location: class TensorFlowImageClassifier
/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowMultiBoxDetector.java:90: error: cannot find symbol
    d.inferenceInterface = new TensorFlowInferenceInterface(assetManager, modelFilename);
                               ^
  symbol:   class TensorFlowInferenceInterface
  location: class TensorFlowMultiBoxDetector
/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowMultiBoxDetector.java:92: error: cannot find symbol
    final Graph g = d.inferenceInterface.graph();
          ^
  symbol:   class Graph
  location: class TensorFlowMultiBoxDetector
/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowMultiBoxDetector.java:99: error: cannot find symbol
    final Operation inputOp = g.operation(inputName);
          ^
  symbol:   class Operation
  location: class TensorFlowMultiBoxDetector
/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowMultiBoxDetector.java:108: error: cannot find symbol
    final Operation outputOp = g.operation(outputScoresName);
          ^
  symbol:   class Operation
  location: class TensorFlowMultiBoxDetector
/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowObjectDetectionAPIModel.java:91: error: cannot find symbol
    d.inferenceInterface = new TensorFlowInferenceInterface(assetManager, modelFilename);
                               ^
  symbol:   class TensorFlowInferenceInterface
  location: class TensorFlowObjectDetectionAPIModel
/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowObjectDetectionAPIModel.java:93: error: cannot find symbol
    final Graph g = d.inferenceInterface.graph();
          ^
  symbol:   class Graph
  location: class TensorFlowObjectDetectionAPIModel
/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowObjectDetectionAPIModel.java:100: error: cannot find symbol
    final Operation inputOp = g.operation(d.inputName);
          ^
  symbol:   class Operation
  location: class TensorFlowObjectDetectionAPIModel
/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowObjectDetectionAPIModel.java:107: error: cannot find symbol
    final Operation outputOp1 = g.operation(""detection_scores"");
          ^
  symbol:   class Operation
  location: class TensorFlowObjectDetectionAPIModel
/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowObjectDetectionAPIModel.java:111: error: cannot find symbol
    final Operation outputOp2 = g.operation(""detection_boxes"");
          ^
  symbol:   class Operation
  location: class TensorFlowObjectDetectionAPIModel
/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowObjectDetectionAPIModel.java:115: error: cannot find symbol
    final Operation outputOp3 = g.operation(""detection_classes"");
          ^
  symbol:   class Operation
  location: class TensorFlowObjectDetectionAPIModel
/Users/spandanmadan/Desktop/android/src/org/tensorflow/demo/TensorFlowYoloDetector.java:107: error: cannot find symbol
    d.inferenceInterface = new TensorFlowInferenceInterface(assetManager, modelFilename);
                               ^
  symbol:   class TensorFlowInferenceInterface
  location: class TensorFlowYoloDetector
Note: Some input files use or override a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
32 errors
:compileDebugJavaWithJavac FAILED
:buildInfoGeneratorDebug

FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':compileDebugJavaWithJavac'.
> Compilation failed; see the compiler error output for details.

* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.

* Get more help at https://help.gradle.org

BUILD FAILED in 1s
17 actionable tasks: 6 executed, 11 up-to-date

```"
23114,Can tf.sparse_reduce_sum infer shape while keepdims=True?,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.11
- Are you willing to contribute it (Yes/No): 
No. Sorry, I'm not familiar with the TF codebase.

**Describe the feature and the current behavior/state.**
tf.sparse_reduce_sum should be able to infer the shape if the input has known shape.
```pythonIn [1]: import tensorflow as tf

In [2]: x = tf.SparseTensor(indices=[[0,0],[1,1]], values=[1,2], dense_shape=[2,2])

In [3]: tf.sparse_reduce_sum(x)
Out[3]: <tf.Tensor 'SparseReduceSum:0' shape=<unknown> dtype=int32>

In [4]: tf.sparse_reduce_sum(x, axis=1, keepdims=True)
Out[4]: <tf.Tensor 'SparseReduceSum_1:0' shape=<unknown> dtype=int32>
```


**Will this change the current api? How?** 
No.
**Who will benefit with this feature?**
Users who use TensorFlow to do graph learning
**Any Other info.**

Hi, please check the following information, thanks!

Have I written custom code
N/A

OS Platform and Distribution
Windows 10

TensorFlow installed from
conda -c anaconda tensorflow-gpu

Bazel version
N/A

CUDA/cuDNN version
9.0/7.1.4

GPU model and memory
Quadro M520 2GB memory

Exact command to reproduce
describe in the original post

Mobile device
N/A"
23113,testing tflite quantized model on an input image,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): tf_nightly
- Python version:  3.5.2
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source):  5.4.0
- CUDA/cuDNN version: 9.0/7.1
- GPU model and memory: GeFORCE 1080 Ti and 12 GB


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
 NA

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.


`import numpy as np
import tensorflow as tf
import pathlib
import cv2
from keras.preprocessing import image
from keras.applications.resnet50 import preprocess_input, decode_predictions
img_path = 'elephant.jpg'
img = image.load_img(img_path, target_size=(299, 299))

x = image.img_to_array(img)
input_data = np.expand_dims(x, axis=0)

#print(input_data.type)
#x = preprocess_input(x)

archive_path = tf.keras.utils.get_file(""resnet_v2_101.tgz"", ""https://storage.googleapis.com/download.tensorflow.org/models/tflite_11_05_08/resnet_v2_101.tgz"", extract=True)
archive_path = pathlib.Path(archive_path)
archive_dir = str(archive_path.parent)
print(archive_dir)


graph_def_file = pathlib.Path(archive_path).parent/""resnet_v2_101_299_frozen.pb""
input_arrays = [""input""] 
output_arrays = [""output""]


converter = tf.contrib.lite.TFLiteConverter.from_frozen_graph(
  str(graph_def_file), input_arrays, output_arrays, input_shapes={""input"":[1,299,299,3]})

converter.post_training_quantize = False
resnet_tflite_file = graph_def_file.parent/""resnet_v2_101.tflite""
resnet_tflite_file.write_bytes(converter.convert())


converter.post_training_quantize = True
resnet_tflite_file = graph_def_file.parent/""resnet_v2_101_quantized.tflite""
resnet_tflite_file.write_bytes(converter.convert())


#!ls -lh {archive_dir}/*.tflite

# Load TFLite model and allocate tensors.
interpreter = tf.contrib.lite.Interpreter(model_path=""/home/sumeet/.keras/datasets/resnet_v2_101_quantized.tflite"")
interpreter.allocate_tensors()

# Get input and output tensors.
input_index= interpreter.get_input_details()
print(input_index)
#output_details = interpreter.get_output_details()
output_index = interpreter.get_output_details()
print(output_index)

# Test model on random input data.
input_shape = input_index[0]['shape']
#input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
#input_data = np.array(X(input_shape), dtype=np.float32)
interpreter.set_tensor(input_index[0]['index'],input_data)

interpreter.invoke()
predictions = interpreter.get_tensor(output_index[0]['index'])
predictions[0]['name']
print(predictions[0,0])
#print('Predicted:', decode_predictions(predictions, top=3)[0])`



How to make predictions on a custom input image like the code given below
from keras.applications.resnet50 import ResNet50
from keras.preprocessing import image
from keras.applications.resnet50 import preprocess_input, decode_predictions
import numpy as np

model = ResNet50(weights='imagenet')

img_path = 'elephant.jpg'
img = image.load_img(img_path, target_size=(224, 224))
x = image.img_to_array(img)
x = np.expand_dims(x, axis=0)
x = preprocess_input(x)

preds = model.predict(x)
print(preds.shape)
# decode the results into a list of tuples (class, description, probability)
# (one such list for each sample in the batch)
print('Predicted:', decode_predictions(preds, top=3)[0])
# Predicted: [(u'n02504013', u'Indian_elephant', 0.82658225), (u'n01871265', u'tusker', 0.1122357), (u'n02504458', u'African_elephant', 0.061040461)
"
23112,ImportError: /home/pi/.virtualenvs/keras_tf/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so: cannot open shared object file: No such file or directory,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspbian
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 0.11
- Python version: 3.5.3
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): 6.3.0 20170516
- CUDA/cuDNN version: N/A
- GPU model and memory: Nvidia GTX 1050, 4 gb



**Describe the problem**
I am new learner in deep learning and do not know much about it. I was trying to install tensorflow in Raspbian following a blog [https://www.pyimagesearch.com/2016/11/14/installing-keras-with-tensorflow-backend/](url). After installing the tensorflow i got an issue while importing tensorflow, which i am not able to solve. I searched lots of blog and asked many for help but i did not get solution.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
$ mkvirtualenv keras_tf -p python3
$ workon keras_tf
$ pip install --upgrade tensorflow
$ python
>>> import tensorflow
--

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Here is the error which i am getting

>>> import tensorflow
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/pi/.virtualenvs/keras_tf/lib/python3.5/site-packages/tensorflow/__init__.py"", line 23, in <module>
    from tensorflow.python import *
  File ""/home/pi/.virtualenvs/keras_tf/lib/python3.5/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/pi/.virtualenvs/keras_tf/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""/home/pi/.virtualenvs/keras_tf/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
  File ""/home/pi/.virtualenvs/keras_tf/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/pi/.virtualenvs/keras_tf/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: /home/pi/.virtualenvs/keras_tf/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so: cannot open shared object file: No such file or directory"
23108,I get error when I want to load my Model,"I am building a deep CNN and I get my graph files by running my code on a GPU on cluster. The training procedure work perfect. Then I move my graph files to my laptop to build this network on my laptop which has CPU. But when I try to load the model I receive the following error:

Traceback (most recent call last): File ""dev_test.py"", line 28, in new_saver = tf.train.import_meta_graph('./3/Model_Arch3/Deep_CNN_Color_Arch8.ckpt-178000.meta') File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1810, in import_meta_graph **kwargs) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/meta_graph.py"", line 660, in import_scoped_meta_graph producer_op_list=producer_op_list) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py"", line 285, in import_graph_def raise ValueError('No op named %s in defined operations.' % node.op) ValueError: No op named ParseSingleExample in defined operations.

The strange thing is that when I train this network on my laptop using my CPU, then I can load the model without any problem!

Also I can build the graph and load the model successfully on the GPU cluster where I trained it! 

I tried the dir(tf.contrib) solution that was suggested in other question, but it did not work for me.

I really appreciate if someone helps me"
23107,Number of elements was larger than representable by 32-bit output type,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.5.1804 (Core)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 1.11.0,  v1.11.0-0-gc19e29306c
- Python version: Python 3.7.0 (default, Jun 28 2018, 13:15:42)
- Bazel version (if compiling from source): Build label: 0.18.0- (@non-git)
- GCC/Compiler version (if compiling from source): gcc version 4.8.5 20150623 (Red Hat 4.8.5-28) (GCC)
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

I'm training a 3D U-Net model with a tensor size of 128x128x128x1 in order to develop models for brain MRIs. It works for a batch size of 8 and less. When I move to a batch size of 16, the size is larger than the int32 index can handle. I have sufficient RAM (384 GB) to allocate the tensors because I can run 2 such processes on the machine without a problem. The error is:

```
tensorflow.python.framework.errors_impl.InvalidArgumentError: Number of elements was larger than representable by 32-bit output type
            [[{{node training/Adam/gradients/batch_normalization_2/moments/mean_grad/Prod}} = Size[T=DT_FLOAT, _class=[""loc:@training/Adam/gradients/batch_normalization_2/moments/mean_grad/truediv""], out_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](conv1b/add, ^training/Adam/gradients/batch_normalization_2/moments/mean_grad/Shape)]]
```
**Describe the expected behavior**
It should be able to handle larger tensors and batch sizes.

**Exact command to reproduce**
`python benchmark_model.py --dim_length 128 --bz 16`

**Code to reproduce the issue**
I have test code here:  https://github.com/NervanaSystems/topologies/tree/master/3D_UNet

`python benchmark_model.py --dim_length 128 --bz 8` will work
`python benchmark_model.py --dim_length 128 --bz 16` will give error

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```...which was originally created as op 'batch_normalization_2/moments/mean', defined at:
  File ""benchmark_model.py"", line 108, in <module>
    print_summary=args.print_model)
  File ""/home/bduser/topologies/3D_UNet/model.py"", line 96, in define_model
    conv1 = K.layers.BatchNormalization()(conv1)
  File ""/home/bduser/anaconda3/envs/tf/lib/python3.6/site-packages/keras/engine/base_layer.py"", line 457, in __call__
    output = self.call(inputs, **kwargs)
  File ""/home/bduser/anaconda3/envs/tf/lib/python3.6/site-packages/keras/layers/normalization.py"", line 185, in call
    epsilon=self.epsilon)
  File ""/home/bduser/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py"", line 1869, in normalize_batch_in_training
    epsilon=epsilon)
  File ""/home/bduser/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py"", line 1750, in _regular_normalize_batch_in_training
    None, None, False)
  File ""/home/bduser/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py"", line 690, in moments
    mean = math_ops.reduce_mean(y, axes, keepdims=True, name=""mean"")
  File ""/home/bduser/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/home/bduser/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py"", line 1492, in reduce_mean
    name=name))
  File ""/home/bduser/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 4778, in mean
    name=name)
  File ""/home/bduser/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/bduser/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/home/bduser/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3272, in create_op
    op_def=op_def)
  File ""/home/bduser/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1768, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): Number of elements was larger than representable by 32-bit output type
	 [[{{node gradients/batch_normalization_2/moments/mean_grad/Prod}} = Size[T=DT_FLOAT, out_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](conv1b/add, ^gradients/batch_normalization_2/batchnorm/mul_1_grad/Shape)]]
```
"
23104,Bazel will not compile under Windows when the user folder has spaces,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: master
- Python version: 3.6
- Installed using virtualenv? pip? conda?: no
- Bazel version (if compiling from source): 0.18.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 7
- GPU model and memory: GTX 1080 Ti 11Gb
- Have I written custom code: no
- Exact command to reproduce: bazel build

The initial state build files usually call a _wrap_bash_cmd function, which does not handle spaces in the paths. I changed line 71 of repo.bzl from:
`[""patch"", ""-p1"", ""-d"", ctx.path("".""), ""-i"", ctx.path(patch_file)]`
to
`[""patch"", ""-p1"", ""-d"", '""' + str(ctx.path(""."")) + '""', ""-i"", ctx.path(patch_file)]`

and it helped, but there are a lot of other files where this also happens and the build keeps failing."
23102,compile from source fails,"**System information**
- Ubuntu 18.04
- From source / r1.12
- TensorFlow version:
- Python 3.6.6
- Inside virtualenv
- Bazel 0.18.0
- gcc 6.4.0
- Cuda 9 / Cudnn 7

**The problem**
Building from source, I end up with: 

```
ERROR: /home/dev/tensorflow/tensorflow/core/BUILD:319:1: undeclared inclusion(s) in rule '//tensorflow/core:platform_base':
this rule is missing dependency declarations for the following files included by 'tensorflow/core/platform/env_time.cc':
  '/usr/lib/gcc/x86_64-linux-gnu/6/include/stdint.h'
  '/usr/lib/gcc/x86_64-linux-gnu/6/include/stddef.h'
  '/usr/lib/gcc/x86_64-linux-gnu/6/include/stdarg.h'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 0.460s, Critical Path: 0.26s
INFO: 0 processes.
FAILED: Build did NOT complete successfully
```
This is when I run: 

`bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`

Also note that if I repeat the command several times, I don't always get the same error message. After I do `bazel clean` I get: 

```
ERROR: /home/.cache/bazel/_bazel_nnnnn/75c0d842b4eca8fbdb48dc37e31275de/external/protobuf_archive/BUILD:659:1: C++ compilation of rule '@protobuf_archive//:python/google/protobuf/pyext/_message.so' failed (Exit 1)
In file included from external/protobuf_archive/python/google/protobuf/pyext/map_container.cc:33:0:
external/protobuf_archive/python/google/protobuf/pyext/map_container.h:34:20: fatal error: Python.h: No such file or directory
 #include <Python.h>
                    ^
compilation terminated.

```
"
23101,Possible bug in the design of `tf.keras.layers.Conv`,"**System information**
- TensorFlow version (you are using): 1.10
- Are you willing to contribute it (Yes/No): No (Don't know tf's code base well enough.)

**Describe the feature and the current behavior/state.**
Current behaviour of `tf.layers.Conv2D` is that
```
import tensorflow as tf

example = tf.zeros([1, 32, 32, 1])
example2 = tf.zeros([1, 41, 41, 1])

# Using a dilated convolution layer
convolution_op = tf.layers.Conv2D(4, 3, padding=""SAME"", dilation_rate=4)

print(convolution_op( example ))
print(convolution_op( example2 ))
```
causes the following exception
```
ValueError: Dimension size must be evenly divisible by 4 but is 49 for 'conv2d_2/SpaceToBatchND_1' (op: 'SpaceToBatchND') with input shapes: [1,41,41,1], [2], [2,2] and with computed input tensors: input[1] = <4 4>, input[2] = <[4 4][4 4]>.
```
The reason for this is that in the `build` function for `tf.keras.layers.Conv` (https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/python/keras/layers/convolutional.py) the convolution operation is created - however, the *dilated* convolution operation also contains padding dependent on the size of the first input to `call`.
There's no reason for this, as the padding operation (i.e. the call to `tf.nn.conv2d`) could simply be different for each of the two `call`s.
Note that with the functional layer API it is still possible to do the reuse of the layers:
```
conv1 = tf.layers.conv2d(example, 4, 3, padding=""SAME"", dilation_rate=4, name=""test"")
conv2 = tf.layers.conv2d(example2, 4, 3, padding=""SAME"", dilation_rate=4, name=""test"", reuse=True)
```
because this causes two separate layer creations. I am proposing that the behaviour should be similar to this.

The consequence of this is that at the moment, to my knowledge, there is no good way of doing weight reuse of dilated convolution weights using `tf.keras.layers`. I think that the above is possibly a bug in the implementation - the convolution operation should maybe be create in the `call` function of `tf.keras.layers.Conv`?




**Will this change the current api? How?**
This should not raise an error:
```
import tensorflow as tf

example = tf.zeros([1, 32, 32, 1])
example2 = tf.zeros([1, 41, 41, 1])

convolution_op = tf.layers.Conv2D(4, 3, padding=""SAME"", dilation_rate=4)

print(convolution_op( example ))
print(convolution_op( example2 ))

```
**Who will benefit with this feature?**
Everyone, as this would be a more consistent behavior of the API, and would allow things that right now are not possible.

**Any Other info.**
More information is also in https://github.com/tensorflow/tensorflow/issues/23019 - I had raised it but it was closed without a relevant reply.

Have I written custom code 
as above.
OS Platform and Distribution
N/A
TensorFlow installed from
pip3
Bazel version
N/A
CUDA/cuDNN version
N/A
GPU model and memory
N/A
Exact command to reproduce
as above
Mobile device
N/A
"
23100,Upgrade to protobuf 3.6.1  ,"An incompatible change in Bazel: https://github.com/bazelbuild/bazel/issues/6384 breaks Tensorflow on Windows: https://buildkite.com/bazel/bazel-with-downstream-projects-bazel/builds/498#3e15d20b-30a5-4071-8c3f-ce7425ce5f35. The breakage comes from using protobuf 3.6.0.

Protobuf 3.6.1 is compatible with the change."
23099,"Running rename_protobuf.sh causes sed: can't read s%#include \([<""]\)google/protobuf/%#include \1google/protobuf3/%: No such file or directory","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: https://github.com/tensorflow/tensorflow/commit/93bc2e2072e0daccbcff7a90d397b704a9e8f778/
- Python version: python3
- Installed using virtualenv? pip? conda?: 
- Bazel version (if compiling from source): 0.16.1
- GCC/Compiler version (if compiling from source): 5.4.0
- CUDA/cuDNN version: 10.0 / 7.1.4
- GPU model and memory: 



**Describe the problem**
https://stackoverflow.com/questions/52886112/running-rename-protobuf-sh

**Provide the exact sequence of commands / steps that you executed before running into the problem**
sudo bash tensorflow/contrib/makefile/download_dependencies.sh
sudo bash tensorflow/contrib/makefile/rename_protobuf.sh

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.


The error message:
sed: can't read s%#include \([<""]\)google/protobuf/%#include \1google/protobuf3/%: No such file or directory

"
23098,TFRecordDataset entries shuffle between examples,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave 10.14 (18A391)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.11.0
- Python version: 3.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A
- Have I written custom code: N/A
- Exact command to reproduce: Check Code to reproduce the issue

**Describe the problem**
When I load several examples into a TFRecordDataset and parse the serialised examples using `parse_single_example`, the entry values of the examples are interchanged randomly. For example, if I save two examples `{""a"": [1, 2, 3], ""b"": [10, 20, 30]}` and `{""a"": [3, 4, 5], ""b"": [30, 40, 50]}` into a `tfrecord` and then load and parse it using `TFRecordDataset` and `parse_single_example`, the loaded examples will become `{""a"": [3, 4, 5], ""b"": [10, 20, 30]}` and `{""a"": [1, 2, 3], ""b"": [30, 40, 50]}`, while it will be correct if I use `Example.ParseFromString` to parse the evaluated string.

**Code to reproduce the issue**
Code to write the tfrecord
```
import tensorflow as tf

test = tf.train.Features(feature={
    'a': tf.train.Feature(float_list=tf.train.FloatList(value=[1,2,3])),
    'b': tf.train.Feature(float_list=tf.train.FloatList(value=[10,20,30]))
})
test_2 = tf.train.Features(feature={
    'a': tf.train.Feature(float_list=tf.train.FloatList(value=[3,4,5])),
    'b': tf.train.Feature(float_list=tf.train.FloatList(value=[30,40,50]))
})

example = tf.train.Example(features=test)
example_2 = tf.train.Example(features=test_2)

with tf.python_io.TFRecordWriter('test.tfrecord') as writer:
    writer.write(example.SerializeToString())
    writer.write(example_2.SerializeToString())
```
Code to load the tfrecord. Example parsed via `ParseFromString` is printed first, and then the one parsed via `parse_single_example`
```
dataset = tf.data.TFRecordDataset('test.tfrecord')
dataset = dataset.repeat()
examples = dataset.make_one_shot_iterator()
with tf.Session() as sess:
    for _ in range(2):
        serialized_example = examples.get_next()
        correct_example = tf.train.Example()
        correct_example.ParseFromString(serialized_example.eval())
        print(correct_example)
        features = {
            ""a"": tf.FixedLenFeature([3], tf.float32),
            ""b"": tf.FixedLenFeature([3], tf.float32),
        }
        example = tf.parse_single_example(serialized=serialized_example, features=features)
        for name, tensor in example.items():
            print('{}: {}'.format(name, tensor.eval()))
```

**Other info / logs**
The log I've got from the second snippet
```
features {
  feature {
    key: ""a""
    value {
      float_list {
        value: 1.0
        value: 2.0
        value: 3.0
      }
    }
  }
  feature {
    key: ""b""
    value {
      float_list {
        value: 10.0
        value: 20.0
        value: 30.0
      }
    }
  }
}

a: [3. 4. 5.]
b: [10. 20. 30.]
features {
  feature {
    key: ""a""
    value {
      float_list {
        value: 3.0
        value: 4.0
        value: 5.0
      }
    }
  }
  feature {
    key: ""b""
    value {
      float_list {
        value: 30.0
        value: 40.0
        value: 50.0
      }
    }
  }
}

a: [1. 2. 3.]
b: [30. 40. 50.]
```"
23096,[Source Installation] Unable to install TF due multiple errors. Info as extensive as possible,"**System information**
- **OS Platform and Distribution** (e.g., Linux Ubuntu 16.04): Mac OS High Sierra 10.13.6 (17G65)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- **TensorFlow installed from** (source or binary): Source
- **TensorFlow version**: r1.10 / r1.11
- **Python version**: 2.7.10
- **Installed using virtualenv? pip? conda?**: Installed inside a virtualenv using pip
- **Bazel version** (if compiling from source): 
Build label: 0.16.1
Build target: bazel-out/darwin-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Mon Aug 13 13:42:50 2018 (1534167770)
Build timestamp: 1534167770
Build timestamp as int: 1534167770

- **GCC/Compiler version** (if compiling from source):
➜  Tensorflow-SDK git:(r1.10) gcc --version
Configured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/usr/include/c++/4.2.1
Apple LLVM version 10.0.0 (clang-1000.11.45.2)
Target: x86_64-apple-darwin17.7.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin
- CUDA/cuDNN version: Not using Cuda but CUDA v9.0.176 and Cuddn v7.0
- GPU model and memory: Unsure.

### Describe the problem
**Attempt one** 
I had Bazel v  0.18.0-homebrew installed and I was trying to compile Tensorflow r1.10 from source. 
I followed the following steps : 
* ./configure (followed everything written [here](https://www.tensorflow.org/install/source))
* Everytime I executed `bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/contrib/lite/...` it gave me the following error so I just decided to skip it and execute the build command.

```
ERROR: /Users/daksh_s/Development/Sources/ARCore/Resources/TFObjectDetection/Tensorflow-SDK/tensorflow/python/eager/BUILD:10:1: C++ compilation of rule '//tensorflow/python/eager:pywrap_tfe_lib' failed (Exit 1)
In file included from tensorflow/python/eager/pywrap_tfe_src.cc:18:
In file included from ./tensorflow/python/eager/pywrap_tfe.h:22:
In file included from ./tensorflow/core/lib/core/status.h:23:
In file included from bazel-out/host/genfiles/tensorflow/core/lib/core/error_codes.pb.h:9:
In file included from external/protobuf_archive/src/google/protobuf/stubs/common.h:39:
In file included from /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/iostream:38:
In file included from /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/ios:216:
fatal error: too many errors emitted, stopping now [-ferror-limit=]
```
* On building, I'm able to compile about 3k files when it eventually fails stating `fatal error: too many errors emitted, stopping now [-ferror-limit=]`

```
ERROR: /Users/daksh_s/Development/Sources/ARCore/Resources/TFObjectDetection/Tensorflow-SDK/tensorflow/core/kernels/BUILD:145:1: error while parsing .d file: /private/var/tmp/_bazel_daksh_s/b40f90ee3cec22f597caca95f90fbe9c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/core/kernels/_objs/concat_lib_gpu/concat_lib_gpu_impl.cu.pic.d (No such file or directory)
nvcc fatal   : Unsupported gpu architecture 'compute_70'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```
At this point, I stopped using CUDA and built for CPU only. Also, I downgraded by Bazel from 0.18 to 0.16.1. The following are my observations : 
* Even though I give ""N"" as the answer to majority of the questions in `./configure` command, the following is generated in `.bazelrc`:

```
build --action_env PYTHON_BIN_PATH=""/Users/daksh_s/Development/Sources/ARCore/Resources/TFObjectDetection/bin/python""
build --action_env PYTHON_LIB_PATH=""/Users/daksh_s/Development/Sources/ARCore/Resources/TFObjectDetection/lib/python2.7/site-packages""
build --python_path=""/Users/daksh_s/Development/Sources/ARCore/Resources/TFObjectDetection/bin/python""
build:gcp --define with_gcp_support=true
build --define with_hdfs_support=true
build:aws --define with_aws_support=true
build:kafka --define with_kafka_support=true
build:xla --define with_xla_support=true
build:gdr --define with_gdr_support=true
build:verbs --define with_verbs_support=true
build --action_env TF_NEED_OPENCL_SYCL=""0""
build --action_env TF_NEED_CUDA=""0""
build --action_env TF_DOWNLOAD_CLANG=""1""
build --config=download_clang
test --config=download_clang
build --define grpc_no_ares=true
build:opt --copt=-march=native
build:opt --host_copt=-march=native
build:opt --define with_default_optimizations=true
build --strip=always
```
Since I did not want Kafka/aws... etc, I manually changed them all to false and re-executed.
* The bulild fails 100% of the time if `TF_DOWNLOAD_CLANG=0` citing the following. Apparently, its not optional. 
```
ERROR: no such package '@local_config_download_clang//': cc_download_clang_toolchain rule //external:local_config_download_clang must create a directory
```
* If I let TF download it by keeping `TF_DOWNLOAD_CLANG=1`, I get the following build error : 
```
fatal error: too many errors emitted, stopping now [-ferror-limit=]
20 errors generated.
Error in child process '/usr/bin/xcrun'. 1
ERROR: /Users/daksh_s/Development/Sources/ARCore/Resources/TFObjectDetection/Tensorflow-SDK/tensorflow/python/eager/BUILD:10:1: output 'tensorflow/python/eager/_objs/pywrap_tfe_lib/pywrap_tfe_src.o' was not created
ERROR: /Users/daksh_s/Development/Sources/ARCore/Resources/TFObjectDetection/Tensorflow-SDK/tensorflow/python/eager/BUILD:10:1: not all outputs were created or valid
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```

**Any other info / logs**
You may download the terminal saved output from [here](https://1drv.ms/u/s!AlU4ab7adLFlgfZNsL1IHuby1vknnA)"
23091,latest cpu version tensorflow report error when run slim model,"**System information**
- use tensorlow master branch commit 0918aa74153664dbc61604af3cf0d66eb334c7ce build the code, build command is : bazel build -c opt --copt=-L/home/guizili/tool/gcc6.3/lib64 //tensorflow/tools/pip_package:build_pip_package
- OS Platform and Distribution (CentOS Linux release 7.4.1708 (Core)):
- Python version 2.7.5:
- Bazel version (0.15.0):
- GCC/Compiler version (6.3.0):
- CUDA/cuDNN version: NA
- GPU model and memory: NA

run train command in tensorlfow model: https://github.com/tensorflow/models
command is:
train_dir=""./train_dir""
    rm -rf $train_dir
    mkdir -p $train_dir
    python research/slim/train_image_classifier.py \
    --dataset_name=imagenet \
    --train_dir=$train_dir \
    --dataset_dir=/lustre/dataset/tensorflow/imagenet \
    --dataset_split_name=train \
    --model_name=inception_v3 \
    --batch_size=64 \
    --max_number_of_steps=220 \
    --clone_on_cpu=True

error log:
2018-10-19 09:29:57.155023: E tensorflow/core/framework/node_def_util.cc:110] Error in the node: {{node InceptionV3/InceptionV3/Mixed_7c/Branch_1/Conv2d_0b_1x3/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=""NHWC"", dilations=[1, 1, 1, 1], padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](InceptionV3/InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/Relu, InceptionV3/Mixed_7c/Branch_1/Conv2d_0b_1x3/weights/read)
2018-10-19 09:29:57.179106: E tensorflow/core/framework/node_def_util.cc:110] Error in the node: {{node InceptionV3/InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=""NHWC"", dilations=[1, 1, 1, 1], padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](InceptionV3/InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/Relu, InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/weights/read)
2018-10-19 09:29:57.204720: E tensorflow/core/framework/node_def_util.cc:110] Error in the node: {{node InceptionV3/InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=""NHWC"", dilations=[1, 1, 1, 1], padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](InceptionV3/InceptionV3/Mixed_7b/concat, InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/weights/read)
2018-10-19 09:29:57.228912: E tensorflow/core/framework/node_def_util.cc:110] Error in the node: {{node InceptionV3/InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=""NHWC"", dilations=[1, 1, 1, 1], padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](InceptionV3/InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/Relu, InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/weights/read)

attach the log while run mobilenet.
[log.txt](https://github.com/tensorflow/tensorflow/files/2494158/log.txt)

"
23088,Trying to install tensorflow in windows 10 and got error when trying to import,"
(tfp3.6) C:\WINDOWS\system32>python
Python 3.6.6 |Anaconda, Inc.| (default, Jun 28 2018, 11:27:44) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""C:\Users\Dell\Anaconda3\envs\tfp3.6\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Dell\Anaconda3\envs\tfp3.6\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Dell\Anaconda3\envs\tfp3.6\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Dell\Anaconda3\envs\tfp3.6\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Dell\Anaconda3\envs\tfp3.6\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Dell\Anaconda3\envs\tfp3.6\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Dell\Anaconda3\envs\tfp3.6\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Dell\Anaconda3\envs\tfp3.6\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Dell\Anaconda3\envs\tfp3.6\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Dell\Anaconda3\envs\tfp3.6\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Dell\Anaconda3\envs\tfp3.6\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Dell\Anaconda3\envs\tfp3.6\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Dell\Anaconda3\envs\tfp3.6\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
23087,How to interpret feature weights from bucketized and crossed columns from a trained model in TensorFlow,"```
Have I written custom code: NA
OS Platform and Distribution: Mac
TensorFlow installed from: pip
TensorFlow version: 1.11
Bazel version: NA
CUDA/cuDNN version: NA 
GPU model and memory: NA
Exact command to reproduce: NA 
Mobile device: NA
```


I'm currently running into a problem where I'm not able to fetch features' weight from a linear model in TensorFlow. I know how to do it when the feature columns only consist of ``numeric_column`` or ``categorical_columns_with_vocabulary_list``, unless ``bucketized_column`` or ``crossed_column`` are added additionally, especially because of **hash bins** introduced in ``crossed_column``.

The problem looks like this:

```
import tensorflow as tf
import tensorflow.feature_column as fc

x1 = [1,2,3,4,5,6,7,8]
x2 = [10,20,30,40,50]

x1_fc = fc.bucketized_column('x1', bins=[3,6])
x2_fc = fc.bucketized_column('x2', bins=[20,40])
x1_x_x2_fc = fc.indicator_column(fc.crossed_column([x1_fc, x2_fc], 7))
feature_columns = [x1_fc, x2_fc, x1_x_x2_fc]

# constuct the linear model
model = tf.estimator.LinearClassifier(feature_columns)
```

The question is that when model is trained and ready to do inferences, is there any way to inspect the predictions by looking into the model's each weight of those features (both bucketized and crossed features)? I mean the weights' values can be extracted without an issue, but I struggle to interpret those values associated with my original inputs.

Besides, for my experiences, when I only use ``numeric_column`` in the model, I do notice that the features are ordered alphabetically in the weights layer, which is somewhat weird to me.

[This answer][1] does not seem very natural, and can be hard to implement when feature number grows or the **hash bins** gets large.


  [1]: https://stackoverflow.com/questions/47282229/tensorflow-inspecting-hash-buckets-for-categorical-and-feature-columns"
23086,Float16 support for log_uniform_candidate_sampler/uniform_candidate_sampler,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.11.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**

Float16 architectures are promising for significantly increase the model size / training speed using the same amount of memory. So I am requesting Float16 support for `log_uniform_candidate_sampler` and `uniform_candidate_sampler`. Currently, in the returned tuple,  `true_expected_count` and `sampled_expected_count` as returned as float32. This causes issues when using `sampled_softmax_loss` since everything needs to be in the same datatype. 

I was able to find a hack work around

https://stackoverflow.com/questions/52711895/how-to-run-define-tensorflow-graph-were-all-variables-are-in-float16-instead-ins

However, this actually increases the training speed; I am not sure why (more details below)

I have tried to develop a replacement function, however, I was unable to interpret the machine generated code where `log_uniform_candidate_sampler` and `uniform_candidate_sampler` are defined. I describe my attempt to interpret the code here

https://stackoverflow.com/questions/52734709/how-to-interpret-this-machine-generated-python-code

**Will this change the current api? How?**

Yes, most likely the best way is to have an optional arg for what datatype the user would like use, ie datatype=tf.float16. Otherwise defaults to float32

**Who will benefit with this feature?**

Those who are developing embedding items (ie Word2Vec) using float16 embeddings. 

**Any Other info.**

>Have I written custom code

Here is the code I used, for convenience it's in a Google Colab notebook so you can just run the code in the notebook. 

https://drive.google.com/open?id=1bnlSfezPqtwapvURRTAqGR4mkcCl9QZl

I wrote a custom function to replace `true_expected_count` and `sampled_expected_count` generated from `uniform_candidate_sampler` because I wanted to isolate the issue if the slow-down is due to using float16 instead of float32. The specific code to replace those variables is  

```
    LogUniformCandidateSampler = namedtuple(""namedtuple"", [""sampled_candidates"", ""true_expected_count"", ""sampled_expected_count""]) 
    sampled_values = tf.nn.uniform_candidate_sampler(
          true_classes=tf.cast(train_labels, tf.int64), num_sampled=num_sampled,
          num_true=1,
          unique=True,
          range_max=vocabulary_size,
          seed=None)
    
    ray= tf.fill( dims =[num_sampled] ,  value= (num_sampled/vocabulary_size)   ) 
    fillvalue2 = tf.cast(ray, testDataType)

    jay= tf.fill( dims =[ batch_size  ,1] ,  value= (num_sampled/vocabulary_size)   ) 
    fillvalue = tf.cast(jay, testDataType)

    true_expected_count2 = tf.get_variable( 'this1' ,dtype=testDataType, initializer = fillvalue  )
    sampled_expected_count2 = tf.get_variable( 'this2' , dtype=testDataType, initializer = fillvalue2   )

    sampled_value_16 = LogUniformCandidateSampler(
        sampled_values.sampled_candidates,
        true_expected_count2,
        sampled_expected_count2 )
```

You can switch between float16 and float32 by changing the `testDataType` variable at the top. You can also change the embedding size at the top by changing the `embedding_size` variable, which I recommend for switching between TPU mode and GPU mode in Google Colab. 

After you select which datatype and embedding size you want to use, select 'Runtime' -> 'Run all', and you can check the time it takes between steps at the training code at the bottom. 

In GPU mode, 100 steps takes about 11 seconds for float32, but takes 15 seconds for float16. I imagine float16 would have been faster. For GPU mode I used an embedding size of 52 (highest embedding size I could use before the system would crash during training for float32).

In TPU mode, 100 steps takes about 10 seconds for float32, but takes 6 minutes 5 seconds for float16. So 36x slower for float16 than float32. For TPU mode I used an embedding size of 154 (highest embedding size I could use before the system would crash during training for float32).

>OS Platform and Distribution

Python 3. Not sure about the rest, whatever Google Colab uses. 

>TensorFlow installed from

Not sure, whatever Google Colab uses. 

>CUDA/cuDNN version

Not sure, whatever Google Colab uses. 

>GPU model and memory

I believe Colab for GPU uses a Tesla k80, and for TPU they are using V2. 

>Exact command to reproduce

Same code as linked above 

https://drive.google.com/open?id=1bnlSfezPqtwapvURRTAqGR4mkcCl9QZl

>Mobile device

N/A"
23085,Problem when try to decode some bmp images with tf.image.decode_bmp,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NA
- TensorFlow installed from (source or binary): PIP
- TensorFlow version (use command below):1.12
- Python version:3.6
- Bazel version (if compiling from source):No
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version NO
- GPU model and memory: Not using GPU


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
Hi,
I have written a python script to read bmp image from local drive using tensorflow and trying to display the image. 

First case:
I am getting error when i try to use the 
below command - tf.image.decode_bmp(image_string,channels=3)

The error is as below:

Traceback (most recent call last):
  File ""/home/venkatesh/anaconda3/envs/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1334, in _do_call
    return fn(*args)
  File ""/home/venkatesh/anaconda3/envs/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/home/venkatesh/anaconda3/envs/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: channels attribute 3 does not match bits per pixel from file 4
	 [[{{node DecodeBmp}} = DecodeBmp[channels=3](ReadFile)]]
	 [[{{node IteratorGetNext}} = IteratorGetNext[output_shapes=[[?,?,3]], output_types=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](OneShotIterator)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/venkatesh/Desktop/Image_processing_thesis/Thesis_deep_learning/problem_tensorfile.py"", line 38, in <module>
    res = sess.run(patches)
  File ""/home/venkatesh/anaconda3/envs/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 929, in run
    run_metadata_ptr)
  File ""/home/venkatesh/anaconda3/envs/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/venkatesh/anaconda3/envs/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1328, in _do_run
    run_metadata)
  File ""/home/venkatesh/anaconda3/envs/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: channels attribute 3 does not match bits per pixel from file 4
	 [[{{node DecodeBmp}} = DecodeBmp[channels=3](ReadFile)]]
	 [[node IteratorGetNext (defined at /home/venkatesh/Desktop/Image_processing_thesis/Thesis_deep_learning/problem_tensorfile.py:35)  = IteratorGetNext[output_shapes=[[?,?,3]], output_types=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](OneShotIterator)]]

Process finished with exit code 1

Second case:
If i use the command tf.image.decode_bmp(image_string) instead of tf.image.decode_bmp(image_string,channels=3) 

I am not getting error but when i see the shape , it's showing the no of channels =4 and not 3.  I want to know why I am getting shape =4 for the second case and why getting error for some images for the first case.

For example consider 00001.bmp Image(attached in attachments) size:
Height=351
Width=572
channels=3
is getting shape(351,572,4) for the second case.

Additionally
All the images works fine without tensorflow
For example: I used scipy to read the image and try to print the shapes of all images in which the no of channels are 3 and not 4.

The image which i used is one of the images form waterloo exploratory dataset. In the first 50 images i have faced the above issue for 5 images and tf.image.decode_bmp works for remaining 45 images.

Please use the below link if you like to download the waterloo exploratory dataset :
[https://ece.uwaterloo.ca/~k29ma/exploration/](url)
**Describe the expected behavior**
Requires information about why the above error is getting displayed for some of bmp images.

**Code to reproduce the issue**
import tensorflow as tf
import numpy as np
from scipy import misc
import glob
import os
#Command to extract all file names
list_ground_truth=glob.glob(""Truth_Images/*.bmp"")  

num_parallel_calls=4

def parse_fn(filename):
    """"""Decode the jpeg image from the filename and convert to [0, 1].""""""
    image_string = tf.read_file(filename)
    image_decoded = tf.image.decode_bmp(image_string,channels=3)
    #print(image_decoded.get_shape().dims)
    # This will convert to float values in [0, 1]
    image = tf.image.convert_image_dtype(image_decoded, tf.float32)
    return image


# Create a dataset
dataset = (tf.data.Dataset.from_tensor_slices(list_ground_truth)
           .map(parse_fn, num_parallel_calls=num_parallel_calls)
           .prefetch(1))


iterator = dataset.make_one_shot_iterator()
patches = iterator.get_next()
sess = tf.Session()
res = sess.run(patches)
print(res.shape)
misc.imshow(res)
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

1. Load the above script. 
2. Change the location in the below command to where the image file which i have attached 
list_ground_truth=glob.glob(""Truth_Images/*.bmp"")  
3.Run the script and if it runs without error.

[problem_files.zip](https://github.com/tensorflow/tensorflow/files/2493751/problem_files.zip)

**Other info / logs**
Additional information:
I am trying the process as part of initial data preparation for training a convolutional network. To make it simple i have removed the unnecessary steps for recreating the issue.

Thanks,
Venkatesh.S
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23083,Documentation for learning_rate_power in the FTRL optimizer,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>

**System information**
- TensorFlow version: 1.6.0
- Doc Link: https://www.tensorflow.org/api_docs/python/tf/train/FtrlOptimizer

**Describe the documentation issue**
This is probably a silly, small issue, but the docs for the FTRL optimizer do not describe what `learning_rate_power` and how it interacts with `learning_rate`.  I tried to figure it out based on the source, but couldn't actually find [where learning_rate_power is used in the code.](https://github.com/tensorflow/tensorflow/blob/1c7bc899dbb86cec70a2c11207a9ce8acf30c13b/tensorflow/python/training/ftrl.py#L41)

The `l2_shrinkage_regularization_strength` has a very detailed explanation and an equation.  Something similar for `learning_rate_power` would be nice."
23082,Error while calling TFLite interpreter (Similar to #21574),"**Tensorflow Version: 1.11.0
Python Version: 3.4 (Also tried with 2.7)**

I tried to install Tensorflow **using PIP install** on my Raspberry Pi3 B+ (Raspbian Stretch - June 2018 Version) and when I tried to run the sample [label_image.py ](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/examples/python/label_image.py)example with TFLite model file I am getting this error -

```
Traceback (most recent call last):
  File ""label_image.py"", line 37, in <module>
    interpreter = tf.contrib.lite.Interpreter(model_path=args.model_file)
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/lite/python/interpreter.py"", line 52, in __init__
    _interpreter_wrapper.InterpreterWrapper_CreateWrapperCPPFromFile(
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/util/lazy_loader.py"", line 53, in __getattr__
    module = self._load()
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/util/lazy_loader.py"", line 42, in _load
    module = importlib.import_module(self.__name__)
  File ""/usr/lib/python3.4/importlib/__init__.py"", line 109, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 2254, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 2237, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 2226, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 1200, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 1129, in _exec
  File ""<frozen importlib._bootstrap>"", line 1471, in exec_module
  File ""<frozen importlib._bootstrap>"", line 321, in _call_with_frames_removed
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py"", line 28, in <module>
    _tensorflow_wrap_interpreter_wrapper = swig_import_helper()
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_tensorflow_wrap_interpreter_wrapper', fp, pathname, description)
  File ""/usr/lib/python3.4/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
ImportError: /usr/local/lib/python3.4/dist-packages/tensorflow/contrib/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so: undefined symbol: _ZN6tflite12tensor_utils24NeonVectorScalarMultiplyEPKaifPf

```

I also tried another way and build the **Cross Compile Package** using latest Tensorflow code from master branch and install the package on pi. After running the [same example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/examples/python/label_image.py) I am facing this error -

```
Traceback (most recent call last):
  File ""label_image.py"", line 37, in <module>
    interpreter = tf.contrib.lite.Interpreter(model_path=args.model_file)
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/util/lazy_loader.py"", line 53, in __getattr__
    module = self._load()
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/util/lazy_loader.py"", line 42, in _load
    module = importlib.import_module(self.__name__)
  File ""/usr/lib/python3.4/importlib/__init__.py"", line 109, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 2254, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 2237, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 2226, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 1200, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 1129, in _exec
  File ""<frozen importlib._bootstrap>"", line 1471, in exec_module
  File ""<frozen importlib._bootstrap>"", line 321, in _call_with_frames_removed
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/__init__.py"", line 48, in <module>
    from tensorflow.contrib import distribute
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/distribute/__init__.py"", line 34, in <module>
    from tensorflow.contrib.distribute.python.tpu_strategy import TPUStrategy
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/distribute/python/tpu_strategy.py"", line 27, in <module>
    from tensorflow.contrib.tpu.python.ops import tpu_ops
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/tpu/__init__.py"", line 69, in <module>
    from tensorflow.contrib.tpu.python.ops.tpu_ops import *
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/tpu/python/ops/tpu_ops.py"", line 39, in <module>
    resource_loader.get_path_to_datafile(""_tpu_ops.so""))
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/util/loader.py"", line 56, in load_op_library
    ret = load_library.load_op_library(path)
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/load_library.py"", line 60, in load_op_library
    lib_handle = py_tf.TF_LoadLibrary(library_filename)

tensorflow.python.framework.errors_impl.InvalidArgumentError: Invalid name: 

An op that loads optimization parameters into HBM for embedding. Must be
preceded by a ConfigureTPUEmbeddingHost op that sets up the correct
embedding table configuration. For example, this op is used to install
parameters that are loaded from a checkpoint before a training loop is
executed.
```
So none of the provided methods are working for me."
23078,Building the pip package on Windows,"Hi,

I'm trying to build tensorflow from source. I managed to successfully finish the bazel build step (CPU-only), but when I run the command 

bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

what happens is that the build_pip_package process spawns itself and the bash process multiple times (like a 1000 times) and the whole process ends with an out of memory error. Here's the output of the command:

bazel-bin\tensorflow\tools\pip_package\build_pip_package .\tfbuild
/usr/bin/bash: warning: shell level (1000) too high, resetting to 1
LAUNCHER ERROR: Cannot launch process: ""C:/msys64/usr/bin/bash.exe
Reason: (error: 8): Not enough memory resources are available to process this command.

Is there a way to limit the number of spawned processes? I cannot find any help online.

Update:
**Have I written custom code: NO
OS Platform and Distribution: Windows 10
TensorFlow installed from: N/A
TensorFlow version: latest sources
Bazel version: 0.18.0 x64 (2018-10-15)
CUDA/cuDNN version: N/A
GPU model and memory: Intel HD graphics
Exact command to reproduce: bazel-bin\tensorflow\tools\pip_package\build_pip_package .\tfbuild
Mobile device: NO**

Thank You"
23077,Slow training speed and incompatible ops issue in tf.keras.utils.multi_gpu_model,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes. See https://github.com/kami93/PredRNN
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A
- TensorFlow installed from (source or binary):Source
- TensorFlow version (use command below):v1.11.0
- Python version: Anaconda Python 3.6.6
- Bazel version (if compiling from source):0.17.2
- GCC/Compiler version (if compiling from source): GCC 7.3.0
- CUDA/cuDNN version: CUDA 9.2, cuDNN 7.3
- GPU model and memory: Gefroce 1080Ti * 4 (4-way GPU), 11,178 MiB each.

**Describe the current behavior**
Hi. I have built a TensorFlow Keras Sequential model (See [predRNN.py](https://github.com/kami93/PredRNN/blob/master/predRNN.py)) which consists of several Keras layers including my custom layer (See [keras_custom/layers/STLSTM.py](https://github.com/kami93/PredRNN/blob/master/keras_custom/layers/STLSTM.py)).

The model by itself runs very well when tf.keras.utils.multi_gpu_model is not applied. The model can be created, compiled, and perform the training (by Model.fit method) without any warning or error.

However, if tf.keras.utils.multi_gpu_model is applied to replicate the model on several GPUs for multi gpu training, warnings like the followings are raised. 

```
No node-device colocations were active during op 'replica_2/sequential/stlst_m2d/while/convolution_14' creation.
Device assignments active during op 'replica_2/sequential/stlst_m2d/while/convolution_14' creation:
  with tf.device(/gpu:2): </home/simon/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/utils/multi_gpu_utils.py:221>
```
```
WARNING:tensorflow:Tried to colocate op 'training/Adam/gradients/replica_3/sequential/stlst_m2d/while/convolution_14_grad/ShapeN/Const' (defined at predRNN.py:142) having device '/device:CPU:1' with op 'replica_3/sequential/stlst_m2d/while/convolution_14' (defined at /home/simon/Desktop/git/PredRNN/keras_custom/layers/STLSTM.py:969) which had an incompatible device '/device:GPU:3'.
```
I think here ""tf.keras.utils.multi_gpu_model"" is not being able to properly assign the convolution ops to other devices which they are initially created for the ""model_creation_device"" (See [line 97 in predRNN.py](https://github.com/kami93/PredRNN/blob/master/predRNN.py#L97)).

Actually, the moodel behaves different with the different selection of ""model_creation_device"".

**Case 1**
If model_creation_device == '/cpu:0', the CPU usage is almost 100% in every core. For the GPU usages, gpu:0, gpu:1, gpu:2 and gpu:3 are all underutilized below 30%. The training is extremely slow compared to other cases. I think some convolution ops are performed by 'CPU:0' here.

![cpu_usage](https://user-images.githubusercontent.com/20102/47162030-33740d00-d32e-11e8-8dc4-b5fd40be6c09.png)
![nvidia-smi_cpu0](https://user-images.githubusercontent.com/20102/47163769-a3d05d80-d331-11e8-8e39-d8fa27ec9542.png)

##################################################################################

**Case 2**
If model_creation_device == '/cpu:1', the CPU usage is normal below 30% in every core. However memory shortage warnings such as the followings are raised.
```
2018-10-18 23:14:56.079586: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.04GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-10-18 23:14:56.082410: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.06GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
```
For the GPUs usage, only the 'gpu:0' is highly utilized around 100% all the time. Others are mostly underutilized around 30%. However, the training speed is fastest amongst the cases. 

![cpu_usage_cpu1](https://user-images.githubusercontent.com/20102/47166363-fc562980-d336-11e8-88d8-1c4b7708a135.png)
![cpu1](https://user-images.githubusercontent.com/20102/47166330-e7799600-d336-11e8-862d-de21e67fc211.png)

##################################################################################

**Case 3**
If model_creation_device == '/gpu:0', '/gpu:1', '/gpu:2', or '/gpu:3', the CPU usage is normal below 30% in every core. However, memory shortage warnings are raised.

The GPUs usage is somewhat strange. Only the ""model_creation_device"" is highly utilized around 100% all the time, and others are mostly underutilized around 30%. 

![cpu_usage_gpus](https://user-images.githubusercontent.com/20102/47165523-1989f880-d335-11e8-9728-424b649c967c.png)
![gpu1](https://user-images.githubusercontent.com/20102/47165150-64efd700-d334-11e8-83e1-abc2cf41b02f.png)
The training speed is way faster than model_creation_device == '/cpu:0' case. However, slightly slower than model_creation_device == '/cpu:1' case.

**Describe the expected behavior**
Multi GPU training of ""PredRNN"" keras model using ""tf.keras.utils.multi_gpu_model"" works without any warning related ops compatibility with devices. All CPU core's usage is below 30% and GPUs usage are around 100% equally for all GPUs while training.

**Code to reproduce the issue**
Run predRNN.py in https://github.com/kami93/PredRNN.

Modify the ""model_creation_device"" to reproduce issues.

**Other info / logs**
Model source code: https://github.com/kami93/PredRNN
logs when model_creation_device = '/cpu:0'
https://pastebin.com/esjt8ZLa
logs when model_creation_device = '/cpu:1'
https://pastebin.com/yRfm9yW8
logs when model_creation_device = '/gpu:0'
https://pastebin.com/CB6Wbzn7
logs when model_creation_device = '/gpu:1'
https://pastebin.com/c917nU53
logs when model_creation_device = '/gpu:2'
https://pastebin.com/rbMSPaWN
logs when model_creation_device = '/gpu:3'
https://pastebin.com/j3xGDvrw"
23076,prediction from tensorflow differs on broadwell and on SkyLake,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.11.0
- Python version: 3.6
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): 0.18.0
- GCC/Compiler version (if compiling from source): 7.3
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**
I have experimented a bit with CNNs for semantic segmentation and it's predictions and I have noticed that for exactly the same network predictions are different regarding the architecture where I deploy the network. I currently see the following state:

`pip install tensorflow` - correct result
`pip install tensorflow-gpu` - correct result (P100)
`bazel build -c opt --copt=-march=broadwell --copt=-mfpmath=both -k //tensorflow/tools/pip_package:build_pip_package` - correct result
`bazel build -c opt --copt=-march=native --copt=-mfpmath=both -k //tensorflow/tools/pip_package:build_pip_package` - different (1-2% mismatch on my tested images) result on SkyLake (but faster prediction than with `--copt=-march=broadwell`)

**NOTE:**
I'm using 32core Google Cloud machines in the us-central1-f zone with CPU option SkyLake.

**Question:**
Is expected the prediction on SkyLake to be different from the prediction on broadwell and GPU on purpose, or should it be exactly the same and I'm just very unlucky here regarding rounding in some libraries underneath the tensorflow?"
23075,"bazel-bin problem.help,help,help~~~","Hello,I don`t know what the problem it is? it can`t generate a new pb file.thank you.

bazel-bin/tensorflow/tools/graph_transforms/transform_graph --in_graph=/media/long/data/android/PoseEstimationForMobile/release/cpm_model/model.pb --out_graph=/media/long/data/android/PoseEstimationForMobile/release/cpm_model/graph_opt1.pb --inputs=inputs/X --outputs=output/predict --transforms='strip_unused_nodes(type=float, shape=""256*64"") remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true) fold_batch_norms fold_old_batch_norms'
2018-10-18 21:49:33.990823: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
2018-10-18 21:49:33.991322: E tensorflow/tools/graph_transforms/transform_graph.cc:263] Input node output/predict not found in graph
2018-10-18 21:49:33.991366: E tensorflow/tools/graph_transforms/transform_graph.cc:264] usage: bazel-bin/tensorflow/tools/graph_transforms/transform_graph
Flags:
	--in_graph=""""                    	string	input graph file name
	--out_graph=""""                   	string	output graph file name
	--inputs=""""                      	string	inputs
	--outputs=""""                     	string	outputs
	--transforms=""""                  	string	list of transforms
	--output_as_text=false           	bool	whether to write the graph in text protobuf format

Transforms are:
add_default_attributes
backport_concatv2
backport_tensor_array_v3
flatten_atrous_conv
fold_batch_norms
fold_constants
fold_old_batch_norms
freeze_requantization_ranges
fuse_pad_and_conv
fuse_remote_graph
fuse_resize_and_conv
fuse_resize_pad_and_conv
insert_logging
merge_duplicate_nodes
obfuscate_names
place_remote_graph_arguments
quantize_nodes
quantize_weights
remove_attribute
remove_control_dependencies
remove_device
remove_nodes
rename_attribute
rename_op
rewrite_quantized_stripped_model_for_hexagon
round_weights
set_device
sort_by_execution_order
sparsify_gather
strip_unused_nodes"
23074,Gradients with respect to a TensorArray,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):
1.10.0
- Are you willing to contribute it (Yes/No):
No (not enough expertise)


**Describe the feature and the current behavior/state.**
It appears that one cannot use the `tf.gradients` function for taking derivatives with respect to TensorArrays. I would like there to be the feature to return a TensorArray with the relevant gradients

**Will this change the current api? How?**
Slightly but in a non-breaking manner (hopefully). It would be an addition to the capabilities of `tf.gradients`

**Who will benefit with this feature?**
This is a feature that is crucial if we wish to take derivatives with respect to intermediate states in a recurrent neural network.

Also related: issue #5412 (currently closed due to no response)

**Any Other info.**
"
23073,tensorflow-gpu install from source. Bazel tests failed,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
Ubuntu 16.04 x86_64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: 
no
- TensorFlow installed from (source or binary): 
source
- TensorFlow version:
1.11
- Python version:
2.7.12
- Installed using virtualenv? pip? conda?:
virtualenv, all dependency packages using pip
- Bazel version (if compiling from source):
0.18.0
- GCC/Compiler version (if compiling from source):
gcc 5.4.0 
- CUDA/cuDNN version: 
9.0 / 7.3.1
- GPU model and memory: 
Nvidia Tesla P100 12Gb

**Describe the problem**
Hello! This is my first post on github, and I apologize in advance if my English is not clear.
I am new to Tensorflow and I try to install it (with gpu support) from source according to the [instructions](https://www.tensorflow.org/install/source#common_installation_problems)
Nvidia 384.145 drivers were pre-installed, and other dependencies too (cuda, cudnn). LD_LIBRARY_PATH and CUDA_HOME variables also set.
The whole installation takes place in virtualenv

**Provide the exact sequence of commands / steps that you executed before running into the problem**

stuck at this stage:
```
git checkout r1.11
bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/contrib/lite/...
```

**Any other info / logs**
some output after 
> bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/contrib/lite/...

```
//tensorflow/python/kernel_tests:rnn_test                  (9/10 cached) FAILED in 1 out of 10 in 5.3s
  Stats over 10 runs: max = 5.3s, min = 1.8s, avg = 3.2s, dev = 1.2s
  /home/iict01/.cache/bazel/_bazel_iict01/d9bbc19c8526907df6f203d5223f36ca/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/kernel_tests/rnn_test/shard_5_of_10/test.log
//tensorflow/contrib/lookup:lookup_ops_test                              FAILED in 4.6s
  /home/iict01/.cache/bazel/_bazel_iict01/d9bbc19c8526907df6f203d5223f36ca/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/contrib/lookup/lookup_ops_test/test.log
//tensorflow/contrib/rpc/python/kernel_tests:rpc_op_test                 FAILED in 0.2s
  /home/iict01/.cache/bazel/_bazel_iict01/d9bbc19c8526907df6f203d5223f36ca/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/contrib/rpc/python/kernel_tests/rpc_op_test/test.log
//tensorflow/core:util_tensor_slice_set_test                             FAILED in 0.1s
  /home/iict01/.cache/bazel/_bazel_iict01/d9bbc19c8526907df6f203d5223f36ca/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/core/util_tensor_slice_set_test/test.log
//tensorflow/go:test                                                     FAILED in 0.1s
  /home/iict01/.cache/bazel/_bazel_iict01/d9bbc19c8526907df6f203d5223f36ca/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/go/test/test.log
//tensorflow/python:build_info_test                                      FAILED in 0.8s
  /home/iict01/.cache/bazel/_bazel_iict01/d9bbc19c8526907df6f203d5223f36ca/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/build_info_test/test.log
//tensorflow/python:flags_test                                           FAILED in 0.8s
  /home/iict01/.cache/bazel/_bazel_iict01/d9bbc19c8526907df6f203d5223f36ca/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/flags_test/test.log
//tensorflow/python:stacktrace_handler_test                              FAILED in 0.8s
  /home/iict01/.cache/bazel/_bazel_iict01/d9bbc19c8526907df6f203d5223f36ca/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/stacktrace_handler_test/test.log
//tensorflow/python/debug:dist_session_debug_grpc_test                   FAILED in 0.7s
  /home/iict01/.cache/bazel/_bazel_iict01/d9bbc19c8526907df6f203d5223f36ca/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/debug/dist_session_debug_grpc_test/test.log
//tensorflow/python/debug:grpc_large_data_test                           FAILED in 0.7s
  /home/iict01/.cache/bazel/_bazel_iict01/d9bbc19c8526907df6f203d5223f36ca/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/debug/grpc_large_data_test/test.log
//tensorflow/python/debug:session_debug_grpc_test                        FAILED in 0.8s
  /home/iict01/.cache/bazel/_bazel_iict01/d9bbc19c8526907df6f203d5223f36ca/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/debug/session_debug_grpc_test/test.log
//tensorflow/python/debug:source_remote_test                             FAILED in 0.8s
  /home/iict01/.cache/bazel/_bazel_iict01/d9bbc19c8526907df6f203d5223f36ca/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/debug/source_remote_test/test.log
//tensorflow/python/keras:image_test                                     FAILED in 5.8s
  /home/iict01/.cache/bazel/_bazel_iict01/d9bbc19c8526907df6f203d5223f36ca/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/keras/image_test/test.log
```
log details of failed tests:
[1_rnn_test.log](https://github.com/tensorflow/tensorflow/files/2491474/1_rnn_test.log)
[2_lookup_ops_test.log](https://github.com/tensorflow/tensorflow/files/2491475/2_lookup_ops_test.log)
[3_rpc_op_test.log](https://github.com/tensorflow/tensorflow/files/2491477/3_rpc_op_test.log)
[4_util_tensor_slice_set.log](https://github.com/tensorflow/tensorflow/files/2491478/4_util_tensor_slice_set.log)
[5_go_test.log](https://github.com/tensorflow/tensorflow/files/2491479/5_go_test.log)
[6_build_info_test.log](https://github.com/tensorflow/tensorflow/files/2491480/6_build_info_test.log)
[7_flags_test.log](https://github.com/tensorflow/tensorflow/files/2491481/7_flags_test.log)
[8_stacktrace_handler_trace.log](https://github.com/tensorflow/tensorflow/files/2491482/8_stacktrace_handler_trace.log)
[9_dist_sess_debug_grpc_test.log](https://github.com/tensorflow/tensorflow/files/2491483/9_dist_sess_debug_grpc_test.log)
[10_grpc_large_data_test.log](https://github.com/tensorflow/tensorflow/files/2491484/10_grpc_large_data_test.log)
[11_sess_debug_grpc_test.log](https://github.com/tensorflow/tensorflow/files/2491485/11_sess_debug_grpc_test.log)
[12_source_remote_test.log](https://github.com/tensorflow/tensorflow/files/2491486/12_source_remote_test.log)
[13_image_test.log](https://github.com/tensorflow/tensorflow/files/2491487/13_image_test.log)

PS: some of theese logs contain error about:

> Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMAF

I hope for your help in solving this issue. Thanks!

"
23072,Runmetadata and graph definitions are not mached,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
source
- TensorFlow version (use command below):
1.6
- Python version:
Python 2.7.12
- Bazel version (if compiling from source):
0.13.0
- GCC/Compiler version (if compiling from source):
5.4.0 2
- CUDA/cuDNN version:
9.0, 7.0
- GPU model and memory:
TitanXP, 12G

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
I was trying to match information between graph definition and timeline but they didn't match.
For example, below is a node in graph definition. 
`node {
    name: ""AutoParallel-Replica-0-Accum-Apply/model/gradients/model/lm/sampled_softmax_loss/embedding_lookup/Gather_3_grad/Reshape""
    op: ""SparseAccumulatorApplyGradient""
    input: ""AutoParallel-Accum/model/gradients/model/lm/sampled_softmax_loss/embedding_lookup/Gather_3_grad/Reshape""
    input: ""Const_35""
    input: ""AutoParallel-Replica-0/model/gradients/model/lm/sampled_softmax_loss/embedding_lookup/Gather_3_grad/Reshape_1""
    input: ""AutoParallel-Replica-0/model/gradients/model/lm/sampled_softmax_loss/embedding_lookup/Gather_3_grad/Reshape""
    input: ""ToInt64_107""
    device: ""/job:worker/task:0/device:CPU:0""
    attr {
      key: ""_class""
      value {
        list {
          s: ""loc:@AutoParallel-Accum/model/gradients/model/lm/sampled_softmax_loss/embedding_lookup/Gather_3_grad/Reshape""
        }
      }
    }
    attr {
      key: ""dtype""
      value {
        type: DT_FLOAT
      }
    }
    attr {
      key: ""has_known_shape""
      value {
        b: true
      }
    }
  }
`
However, Runmetadata contains below node stat for the node. The inputs in timeline_label are different with information in the graph node. I want to know the policy or rules to match two different information.
` node_stats {
      node_name: ""AutoParallel-Replica-0-Accum-Apply/model/gradients/model/lm/sampled_softmax_loss/embedding_lookup/Gather_3_grad/Reshape""
      all_start_micros: 1539680850138092
      op_start_rel_micros: 10
      op_end_rel_micros: 12405
      all_end_rel_micros: 12418
      memory {
        allocator_name: ""cpu_rdma_bfc""
        total_bytes: 2574336
        peak_bytes: 2574336
        allocator_bytes_in_use: 260048896
        allocation_records {
          alloc_micros: 1539680850142050
          alloc_bytes: 2574336
        }
        allocation_records {
          alloc_micros: 1539680850150477
          alloc_bytes: -2574336
        }
      }
      timeline_label: ""[cpu_rdma_bfc 2.5MB 2.5MB] AutoParallel-Replica-0-Accum-Apply/model/gradients/model/lm/sampled_softmax_loss/embedding_lookup/Gather_3_grad/Reshape = SparseAccumulatorApplyGradient(AutoParallel-Accum/model/gradients/model/lm/sampled_softmax_loss/embedding_lookup/Gather_3_grad/Reshape, Const, AutoParallel-Replica-0/model/lm/sampled_softmax_loss/embedding_lookup/DynamicPartition:3, AutoParallel-Replica-0/model/gradients/model/lm/sampled_softmax_loss/embedding_lookup_grad/Gather_3_G818, ToInt64_2)""
      scheduled_micros: 1539680850138009
      memory_stats {
      }
    }
`
**Describe the expected behavior**
Input names in graph definition and timeline label in metadata have the same names

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23071,Error while importing tensorflow after installation ,"Please help me to solve this problem i am trying from several days. i get this error when i import tensorflow after installation , i am using windows 10 OS. 

>>> import tensorflow as tf
RuntimeError: module compiled against API version 0xc but this version of numpy is 0xa
ImportError: numpy.core.multiarray failed to import
ImportError: numpy.core.umath failed to import
ImportError: numpy.core.umath failed to import
2018-10-18 18:24:15.573678: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr"
23069,TensorFlow Docker image contains old Jupyter release,"**System information**
- TensorFlow version (you are using): 1.11 (tensorflow/tensorflow:1.11.0-gpu-py3), amd64
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
Latest release of the Docker TensorFlow image contains Jupyter 4.4.0 which is pretty old (latest release is 5.5.0)

**Will this change the current api? How?**
No changes to the API.

**Who will benefit with this feature?**
Users that use Jupyter latest features (e.g. ""delete not empty folder from web interface"") and bug fixes.
"
23068,TFLite benchmark_model cannot be compiled successfully,"**System information**
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from: source
- TensorFlow version: 1.11.0
- Python version: 3.6.6
- Bazel version: 0.18.0
- GCC/Compiler version: Android Clang NDK 18

**Describe the problem**

When use command in README for benchmark, the compiling failed.

bazel build -c opt \
  --config=android_arm \
  --cxxopt='--std=c++11' \
  tensorflow/contrib/lite/tools/benchmark:benchmark_model

**Any other info / logs**

ERROR: /data/tensorflow/contrib/lite/tools/benchmark/BUILD:19:1: Linking of rule '//tensorflow/contrib/lite/tools/benchmark:benchmark_model' failed (Exit 1)
/usr/local/google/buildbot/src/android/ndk-release-r18/external/libcxx/include/__locale:51: error: undefined reference to 'uselocale'
/usr/local/google/buildbot/src/android/ndk-release-r18/external/libcxx/include/__locale:56: error: undefined reference to 'uselocale'
/usr/local/google/buildbot/src/android/ndk-release-r18/external/libcxx/include/__locale:56: error: undefined reference to 'uselocale'
/usr/local/google/buildbot/src/android/ndk-release-r18/external/libcxx/src/locale.cpp:76: error: undefined reference to 'newlocale'
/usr/local/google/buildbot/src/android/ndk-release-r18/external/libcxx/include/__locale:51: error: undefined reference to 'uselocale'
/usr/local/google/buildbot/src/android/ndk-release-r18/external/libcxx/src/locale.cpp:5439: error: undefined reference to 'freelocale'
/usr/local/google/buildbot/src/android/ndk-release-r18/external/libcxx/include/__bsd_locale_fallbacks.h:53: error: undefined reference to 'wcsnrtombs'
/usr/local/google/buildbot/src/android/ndk-release-r18/external/libcxx/include/__bsd_locale_fallbacks.h:68: error: undefined reference to 'mbsnrtowcs'
/usr/local/google/buildbot/src/android/ndk-release-r18/external/libcxx/include/__bsd_locale_fallbacks.h:83: error: undefined reference to 'mbtowc'
/usr/local/google/buildbot/src/android/ndk-release-r18/external/libcxx/src/locale.cpp:1531: error: undefined reference to 'freelocale'
/usr/local/google/buildbot/src/android/ndk-release-r18/external/libcxx/include/locale:739: error: undefined reference to 'strtoll_l'
/usr/local/google/buildbot/src/android/ndk-release-r18/external/libcxx/include/locale:739: error: undefined reference to 'strtoll_l'
/usr/local/google/buildbot/src/android/ndk-release-r18/external/libcxx/include/locale:779: error: undefined reference to 'strtoull_l'
/usr/local/google/buildbot/src/android/ndk-release-r18/external/libcxx/include/locale:779: error: undefined reference to 'strtoull_l'
/usr/local/google/buildbot/src/android/ndk-release-r18/external/libcxx/include/locale:779: error: undefined reference to 'strtoull_l'
/usr/local/google/buildbot/src/android/ndk-release-r18/external/libcxx/include/locale:779: error: undefined reference to 'strtoull_l'
/usr/local/google/buildbot/src/android/ndk-release-r18/external/libcxx/include/locale:820: error: undefined reference to 'strtold_l'
clang: error: linker command failed with exit code 1 (use -v to see invocation)"
23067,"QueueRunner going towards deprecation, but tf.data does not replace all usecases?","In my code I get QueueRunner deprecation warning:

```
QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
```

Did a bit of investigation [here](https://www.tensorflow.org/guide/datasets) and it seems that tf.data is a new thing that is faster and better than the old queue stuff. But, as I understand, it it not replacing queues because data is expected to be static, (as far as I understood and of all the examples I saw), modification / appending new training data to the dataset is not supported, so I don't know how this change applies to the reinforcement learning, which learns on new data environment returns all the time. So, as far as I see, deprecation of queue runner is coming and I don't see tf.data replacing it for this usecase?

Any kind of explanation and example is welcome! Thanks!

PS. Started out this question [on stack overflow](https://stackoverflow.com/questions/52855870/queuerunner-going-towards-deprecation-but-tf-data-does-not-replace-all-usecases) first, but didn't get any solutions."
23065,DLL load failed for tensorflow,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.10
- Python version: 3.6
- Installed using virtualenv: Canopy pip: 1.18 
- Bazel version (if compiling from source):NA
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:NA
- GPU model and memory:NA



**Describe the problem**

```
ImportError                               Traceback (most recent call last)
C:\Users\Dell\AppData\Local\Enthought\Canopy\edm\envs\User\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>()
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

C:\Users\Dell\AppData\Local\Enthought\Canopy\edm\envs\User\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in <module>()
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

C:\Users\Dell\AppData\Local\Enthought\Canopy\edm\envs\User\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

C:\Users\Dell\AppData\Local\Enthought\Canopy\edm\envs\User\lib\imp.py in load_module(name, file, filename, details)
    241         else:
--> 242             return load_dynamic(name, filename, file)
    243     elif type_ == PKG_DIRECTORY:

C:\Users\Dell\AppData\Local\Enthought\Canopy\edm\envs\User\lib\imp.py in load_dynamic(name, path, file)
    341             name=name, loader=loader, origin=path)
--> 342         return _load(spec)
    343 

ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-7-6b0f4483b0d5> in <module>()
----> 1 import tensorflow as tf
      2 
      3 a = tf.Variable(1, name=""a"")
      4 b = tf.Variable(2, name=""b"")
      5 f = a + b

C:\Users\Dell\AppData\Local\Enthought\Canopy\edm\envs\User\lib\site-packages\tensorflow\__init__.py in <module>()
     20 
     21 # pylint: disable=g-bad-import-order
---> 22 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
     23 
     24 try:

C:\Users\Dell\AppData\Local\Enthought\Canopy\edm\envs\User\lib\site-packages\tensorflow\python\__init__.py in <module>()
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 # Protocol buffers

C:\Users\Dell\AppData\Local\Enthought\Canopy\edm\envs\User\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>()
     72 for some common reasons and solutions.  Include the entire stack trace
     73 above this error message when asking for help."""""" % traceback.format_exc()
---> 74   raise ImportError(msg)
     75 
     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""C:\Users\Dell\AppData\Local\Enthought\Canopy\edm\envs\User\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Dell\AppData\Local\Enthought\Canopy\edm\envs\User\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Dell\AppData\Local\Enthought\Canopy\edm\envs\User\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Dell\AppData\Local\Enthought\Canopy\edm\envs\User\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Dell\AppData\Local\Enthought\Canopy\edm\envs\User\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```python
import tensorflow as tf

a = tf.Variable(1, name=""a"")
b = tf.Variable(2, name=""b"")
f = a + b

init = tf.global_variables_initializer()
with tf.Session() as s:
    init.run()
    print( f.eval() )
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
``"
23064,Is there something wrong with AttentionWrapper when use BahdanauAttention or LuongAttention? ,"When using attention model, we need to get a AttentionWrapper object,  which defined in tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py. When focused on the func call, i find something wrong. 

tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py: In class AttentionWrapper:
  def call(self, inputs, state):
    """"""Perform a step of attention-wrapped RNN.
    - Step 1: Mix the `inputs` and previous step's `attention` output via
      `cell_input_fn`.
    - Step 2: Call the wrapped `cell` with this input and its previous state.
    - Step 3: Score the cell's output with `attention_mechanism`.
    - Step 4: Calculate the alignments by passing the score through the
      `normalizer`.
    - Step 5: Calculate the context vector as the inner product between the
      alignments and the attention_mechanism's values (memory).
    - Step 6: Calculate the attention output by concatenating the cell output
      and context through the attention layer (a linear layer with
      `attention_layer_size` outputs).
....
    # Step 1: Calculate the true inputs to the cell based on the
    # previous attention value.
    cell_inputs = self._cell_input_fn(inputs, state.attention)
    cell_state = state.cell_state
    cell_output, next_cell_state = self._cell(cell_inputs, cell_state)
.....
    for i, attention_mechanism in enumerate(self._attention_mechanisms):
      attention, alignments, next_attention_state = _compute_attention(
          attention_mechanism, cell_output, previous_attention_state[i],
          self._attention_layers[i] if self._attention_layers else None)
      alignment_history = previous_alignment_history[i].write(
          state.time, alignments) if self._alignment_history else ()

      all_attention_states.append(next_attention_state)
      all_alignments.append(alignments)
      all_attentions.append(attention)
      maybe_all_histories.append(alignment_history)
.....

The code is just copyed.  In _compute_attention(...),  we get alignment and attention, but here the second parameter  we used is  cell_ouput , which was calculted from self._cell(cell_inputs, cell_state). 

But In paper: neuaral machine translation by  jonintly learning to align and trainslate(https://arxiv.org/pdf/1409.0473.pdf), the BahdanauAttention, which should be calculated by cell_state,  not cell_output.

In paper: effective Approaches to Attention-based Neural Machine Translation(https://arxiv.org/pdf/1508.04025.pdf), i cannot  find the LuongAttention used to calculate current cell state and output, so, how should step 1 occured? The default self._cell_input_fn(inputs, state.attention) just cancat input and stattion.

Is there someting wrong?
"
23062,Train the Model in android app,"I just finished three courses in CodeLab about Tensorflow.
I'm finding the way ***how can I train the Model on Android device instead of training from the computer and use that Model on Android***
"
23061,Xla and Ignite support always true from configure.py,"**System information**
- OS Platform and Distribution: Ubuntu 18.04
- TensorFlow installed from source
- TensorFlow version: 1.12.0rc-1
- Python version: 3.6
- Installed using pip3
- Bazel version 0.18.0
- GCC/Compiler version 7.3.0
- CUDA/cuDNN version 7.3.1
- GPU model and memory: GTX Titan X (Maxwell w/12GB)
- Have I written custom code: No
- Mobile device: None
- Exact command to reproduce: ./configure  (see below)


**Describe the problem**
It looks as if the configure.py script is always setting ignite and xla support true.  
To see this, run configure and answer N to the options for ignite and xla...
```
Do you wish to build TensorFlow with Apache Ignite support? [Y/n]: n
No Apache Ignite support will be enabled for TensorFlow.
Do you wish to build TensorFlow with XLA JIT support? [Y/n]: n
No XLA JIT support will be enabled for TensorFlow.
```
Afterwards .tf_configure.bazelrc has the following lines:
```
build:ignite --define with_ignite_support=true
build:xla --define with_xla_support=true
```


**Code**
It looks to me like the following lines in configure.py are the issue:
Under def set_build_var (line 368)
```
  if var == '1':
    write_to_bazelrc('build --define %s=true' % option_name)
  elif bazel_config_name is not None:
    write_to_bazelrc(
        'build:%s --define %s=true' % (bazel_config_name, option_name))
```
In the above, if var is 0 and there's a bazel_config_name, the option still gets set to true.  That doesn't seem like the behavior we want.

"
23060,Attributes values not inferred by TFE C API (eager mode),"I'm trying to enable eager execution mode in Java based on the`TFE_*` methods exposed by TensorFlow C API and I have noticed that those method do not infer attribute values the same way that graph execution does.

For example, the `NodeDefBuilder` class (used in graph execution mode) will infer type attribute values automatically based on the type of the tensors the node receives in input (see [`NodeDefBuilder::SingleInput`](https://github.com/tensorflow/tensorflow/blob/7e0257d953401288bc10dc11d07b418371bbc56d/tensorflow/core/framework/node_def_builder.cc#L123). Eager classes do not. Also, in graph mode, default values are assigned automatically when some attributes are not provided (see [`NodeDefBuilder::Finalize`](https://github.com/tensorflow/tensorflow/blob/7e0257d953401288bc10dc11d07b418371bbc56d/tensorflow/core/framework/node_def_builder.cc#L245)). Again, eager classes do not.

My question is are those features missing by design or should they be part of the eager C API as well? 

I can do a work around in the Java client, as @eaplatanios did for Scala, but I'm tempted to think that this should be implemented in the C API and that the graph and eager methods should be more symmetric to avoid moving this complexity in all the clients.

-----------

Have I written custom code: N/A
OS Platform and Distribution: N/A
TensorFlow installed from: N/A
TensorFlow version: 1.11
Bazel version: N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: N/A
Mobile device: N/A"
23059,tf.argmax docs don't say which argument is deprecated,"**System information**
- TensorFlow version: 1.11
- Doc Link: https://www.tensorflow.org/api_docs/python/tf/argmax

**Describe the documentation issue**

The doc says: ""SOME ARGUMENTS ARE DEPRECATED. They will be removed in a future version. Instructions for updating: Use the axis argument instead""

I believe both the dimension and output_type args are deprecated? The update instructions only mention the to, but not the from. This may be a problem with the `deprecated_args` decorator.
"
23058,dll load failed in tensorflow install using pip on windows 10,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10
- TensorFlow installed from (source or binary): pip
- TensorFlow version: 1.11
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip and venv
- CUDA/cuDNN version: 8 | 5.1/6
- GPU model and memory: nvidia geforce gtx 960m
- Have I written custom code: N/A
- Bazel version: N/A
- Exact command to reproduce: N/A
- Mobile device: N/A




**Describe the problem**

I did look at these issues and applied the fixes but still not working.
https://github.com/tensorflow/tensorflow/issues/10033
https://github.com/tensorflow/tensorflow/issues/5949

Then i found this https://gist.github.com/mrry/ee5dbcfdd045fa48a27d56664411d41c
turns out there were some version issues like it needed cuda 8 instead of 9. so I fixed all those things and its still not working please help


**Any other info / logs**
```
Traceback (most recent call last):
  File ""C:\Users\luthr\rl\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\luthr\rl\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\luthr\rl\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\luthr\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\luthr\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Users\luthr\rl\lib\site-packages\tensorflow\__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\luthr\rl\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\luthr\rl\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\luthr\rl\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\luthr\rl\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\luthr\rl\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\luthr\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\luthr\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```"
23057,tf.arg_max docs suggest nonexistent page in 1.12,"**System information**
- TensorFlow version: 1.12
- Doc Link: https://www.tensorflow.org/versions/r1.12/api_docs/python/tf/arg_max

**Describe the documentation issue**

the 1.12 page for arg_max mentions this function is deprecated in favor of argmax: ""THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use argmax instead""

However, argmax is not documented for 1.12 (it is fine in 1.11)."
23056,Losses collection is not thread local so it can't be used inside model_fn call when using MirroredStrategy,"**System information**

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 'v1.12.0-rc0-0-g1a6dea3' 1.12.0-rc0
- Python version: 3.6
- Bazel version (if compiling from source): 0.18.0
- GCC/Compiler version (if compiling from source): gcc-6 (Ubuntu 6.4.0-17ubuntu1) 6.4.0 20180424
- CUDA/cuDNN version: 10.0/7.3.1.20
- GPU model and memory: GeForce GTX 1080 Ti (11GB)

**Describe the current behavior**

When calling `tf.losses.add_loss` inside model_fn in Estimator API, it is added to the `tf.GraphKeys.LOSSES` collection. `tf.losses.get_total_loss` is aggregating all the losses from the `tf.GraphKeys.LOSSES` collection.

Unfortunately, when using `tf.contrib.distribute.MirroredStrategy` as a distribute strategy, collection is updated from all concurrent `model_fn` calls. This leads to tower losses being aggregated to total loss in other towers as well.

So if we have 4 GPUs with losses L1, L2, L3, L4, Estimator will report total loss as `a * L1 + b * L2 + c * L3 + d* L4` where a,b,c,d depends on the races encountered.

**Describe the expected behavior**

I would expect total loss to be `L1 + L2 + L3 + L4`.

**Code to reproduce the issue**

When running this code, second call to `model_fn` causes assertion that losses collection is empty to fail.

```python
import tensorflow as tf


def model_fn(features, labels, mode):
    loss = tf.abs(features + tf.get_variable('foo', shape=()) - labels)

    assert len(tf.get_collection(tf.GraphKeys.LOSSES)) == 0
    tf.losses.add_loss(loss)

    loss = tf.losses.get_total_loss()
    train_op = tf.train.GradientDescentOptimizer(0.0).minimize(loss)
    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)


def input_fn():
    return tf.data.Dataset.zip((
        tf.data.Dataset.from_tensors(0.).repeat(100),
        tf.data.Dataset.from_tensors(0.).repeat(100)
    ))


distribution = tf.contrib.distribute.MirroredStrategy(num_gpus=2)
config = tf.estimator.RunConfig(train_distribute=distribution)
estimator = tf.estimator.Estimator(model_fn=model_fn, config=config)
estimator.train(input_fn=input_fn)
```


**Other info / logs**

Script output:

```
INFO:tensorflow:Initializing RunConfig with distribution strategies.
INFO:tensorflow:Not using Distribute Coordinator.
WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpq6v9w4nh
INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpq6v9w4nh', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x7f27bd939240>, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f27bd939f60>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_distribute_coordinator_mode': None}
INFO:tensorflow:Device is available but not used by distribute strategy: /device:CPU:0
INFO:tensorflow:Device is available but not used by distribute strategy: /device:GPU:2
INFO:tensorflow:Device is available but not used by distribute strategy: /device:GPU:3
INFO:tensorflow:Configured nccl all-reduce.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Error reported to Coordinator: 
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception
    yield
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 795, in run
    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py"", line 1195, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""<ipython-input-9-57173fdb3adc>"", line 7, in model_fn
    assert len(tf.get_collection(tf.GraphKeys.LOSSES)) == 0
AssertionError
```
"
23055,"Can't build the demo app, run this bazel command from the tensorflow directory:","Can't figure out how to build the demo app. Tried different ways doesn't work. 
I have tried:

**`bazel build -c opt --cxxopt='--std=c++11' --fat_apk_cpu=armeabi-v7a //tensorflow/contrib/l
ite/examples/android:tflite_demo`**


**`bazel build -c opt --cxxopt='--std=c++11' --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a 
//tensorflow/contrib/lite/examples/android:tflite_demo`**


**`bazel build -c opt --config=android_arm{,64} --cxxopt='--std=c++11' \
//tensorflow/contrib/lite/examples/android:tflite_demo`
`**

**System information**
 Ubuntu 16.04 on GCP 
run  Docker

**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

`kaisenaiko@tpu-cv-app:~$ **sudo docker run --rm -it --privileged -p 6006:6006 detect-tf**
root@c567d9e05859:/tensorflow# 
root@c567d9e05859:/tensorflow# bazel build -c opt --config=android_arm{,64} --cxxopt='--std=c++11' \
> //tensorflow/contrib/lite/examples/android:tflite_demo
WARNING: The following configs were expanded more than once: [android]. For repeatable flags, repeats are counted twice and
 may lead to unexpected behavior.
WARNING: option '--crosstool_top' was expanded to from both option '--config=download_clang' (source /tensorflow/.tf_config
ure.bazelrc) and option '--config=android_arm' (source command line options)
WARNING: option '--cpu' was expanded to from both option '--config=android_arm' (source command line options) and option '-
-config=android_arm64' (source command line options)
WARNING: option '--fat_apk_cpu' was expanded to from both option '--config=android_arm' (source command line options) and o
ption '--config=android_arm64' (source command line options)
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdow
n"".
ERROR: No default_toolchain found for cpu 'arm64-v8a'. Valid cpus are: [
  k8,
  local,
  armeabi-v7a,
  x64_windows,
  x64_windows_msvc,
  x64_windows_msys,
  s390x,
  ios_x86_64,
]
INFO: Elapsed time: 5.567s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (3 packages loaded)
root@c567d9e05859:/tensorflow# **bazel build -c opt --config=android_arm{,32} --cxxopt='--std=c++11' //tensorflow/contrib/lit
e/examples/android:tflite_demo**
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=123
INFO: Reading rc options for 'build' from /etc/bazel.bazelrc:
  'build' options: --spawn_strategy=standalone --genrule_strategy=standalone
INFO: Reading rc options for 'build' from /tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=t
rue --define=grpc_no_ares=true --spawn_strategy=standalone --genrule_strategy=standalone -c opt --define=grpc_no_ares=true 
--define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include
INFO: Reading rc options for 'build' from /tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python --action_env PYTHON_LIB_PATH=/usr/lib/python2.7/dist-packag
es --python_path=/usr/bin/python --config=xla --action_env TF_NEED_OPENCL_SYCL=0 --action_env TF_NEED_ROCM=0 --action_env T
F_NEED_CUDA=0 --action_env TF_DOWNLOAD_CLANG=1 --config=download_clang
INFO: Found applicable config definition build:xla in file /tensorflow/.tf_configure.bazelrc: --define with_xla_support=tru
e
INFO: Found applicable config definition build:download_clang in file /tensorflow/.bazelrc: --crosstool_top=@local_config_d
ownload_clang//:toolchain --define=using_clang=true
INFO: Found applicable config definition build:android_arm in file /tensorflow/.bazelrc: --config=android --cpu=armeabi-v7a
 --fat_apk_cpu=armeabi-v7a
INFO: Found applicable config definition build:android in file /tensorflow/.bazelrc: --crosstool_top=//external:android/cro
`
`
root@c567d9e05859:/tensorflow# **bazel build -c opt --cxxopt='--std=c++11' --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a \
> //tensorflow/contrib/lite/examples/android:tflite_demo**
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdow
n"".
Unhandled exception thrown during build; message: //tensorflow/contrib/lite/examples/android:tflite_demo BuildConfiguration
Value.Key[347ce3beb85cc56b9abdec126d2f85bd] false -> ErrorInfo{exception=com.google.devtools.build.lib.analysis.config.Inva
lidConfigurationException: No default_toolchain found for cpu 'arm64-v8a'. Valid cpus are: [
  k8,
  local,
  armeabi-v7a,
  x64_windows,
  x64_windows_msvc,
  x64_windows_msys,
  s390x,
  ios_x86_64,
], rootCauses={BuildConfigurationValue.Key[785f82d034ad1259081f051c32361d64]}, cycles=[], isCatastrophic=false, rootCauseOf
Exception=BuildConfigurationValue.Key[785f82d034ad1259081f051c32361d64], isDirectlyTransient=false, isTransitivelyTransient
=false}
INFO: Elapsed time: 5.027s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (13 packages loaded)
    currently loading: tensorflow/contrib/lite/java
java.lang.IllegalStateException: //tensorflow/contrib/lite/examples/android:tflite_demo BuildConfigurationValue.Key[347ce3b
eb85cc56b9abdec126d2f85bd] false -> ErrorInfo{exception=com.google.devtools.build.lib.analysis.config.InvalidConfigurationE
xception: No default_toolchain found for cpu 'arm64-v8a'. Valid cpus are: [
  k8,
  local,
  armeabi-v7a,
  x64_windows,
  x64_windows_msvc,
  x64_windows_msys,
  s390x,
  ios_x86_64,
], rootCauses={BuildConfigurationValue.Key[785f82d034ad1259081f051c32361d64]}, cycles=[], isCatastrophic=false, rootCauseOf
`
`root@c567d9e05859:/tensorflow# **bazel build -c opt --cxxopt='--std=c++11' --fat_apk_cpu=armeabi-v7a   //tensorflow/contrib/l
ite/examples/android:tflite_demo**
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
ERROR: /tensorflow/tensorflow/contrib/lite/kernels/internal/BUILD:666:1: no such package '@androidndk//': The repository could not be resolved and referenced by '//tensorflow/contrib/lite/kernels/internal:cpu_check'
ERROR: Analysis of target '//tensorflow/contrib/lite/examples/android:tflite_demo' failed; build aborted: Analysis failed
INFO: Elapsed time: 10.508s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (59 packages loaded)
    Fetching https://mirror.bazel.build/github.com/google/gemmlowp/archive/38ebac7b059e84692f53e5938f97a9943c120d98.zip
    Fetching https://mirror.bazel.build/bitbucket.org/eigen/eigen/get/fd6845384b86.tar.gz
    Fetching https://mirror.bazel.build/.../ARM_NEON_2_x86_SSE/archive/0f77d9d182265259b135dad949230ecbf1a2633d.tar.gz
root@c567d9e05859:/tensorflow# `"
23054,tanh on CPU exceeds range (and is inconsistent),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04.1 LTS
- **TensorFlow installed from (source or binary)**: binary (or source)
- **TensorFlow version (use command below)**: ('v1.10.0-0-g656e7a2b34', '1.10.0')
- **Python version**: Python 2.7.15rc1
- **CUDA/cuDNN version**: N/A, CPU problem
- **GPU model and memory**: N/A
- **Exact command to reproduce**: 
```
import numpy
import tensorflow
sess = tensorflow.Session()
sess.run(tensorflow.tanh(numpy.arange(8.8,9.0,0.01,dtype=numpy.float32)))
```

### Describe the problem
tanh on float32s can return values outside of the range [-1,1] when run on a CPU, which suggests a bug in its implementation.  This occurs across multiple machines and builds (but does not seem to occur for me when running on a GPU, or when using numpy instead of tensorflow).  On the system specs given above, the commands produce two instances of 1.0000001:
```
>>> sess.run(tensorflow.tanh(numpy.arange(8.8,9.0,0.01,dtype=numpy.float32)))
array([1.        , 1.        , 0.99999994, 1.0000001 , 1.        ,
       0.99999994, 0.9999998 , 0.99999994, 0.9999998 , 1.        ,
       1.0000001 , 0.9999998 , 1.        , 0.99999994, 0.9999998 ,
       0.99999994, 1.        , 1.        , 0.9999998 , 1.        ],
      dtype=float32)
```

Of lesser concern, return values also differ on the same system when specifying overlapping ranges (here 8.8 changed to 8.9 does not match second half of above):
```
>>> sess.run(tensorflow.tanh(numpy.arange(8.9,9.0,0.01,dtype=numpy.float32)))
array([1.        , 0.99999994, 1.        , 0.99999994, 1.0000001 ,
       0.9999998 , 1.        , 1.0000001 , 1.        , 1.        ],
      dtype=float32)
```"
23053,Autograph example crashes,"The autograph example from the documentation crashes.
https://www.tensorflow.org/guide/autograph#automatically_convert_python_control_flow

I'm running in docker. The same issues occur with tag latest and nightly.

Code:
```python
import tensorflow as tf
from tensorflow.contrib import autograph

@autograph.convert()
def square_if_positive(x):
  if x > 0:
    x = x * x
  else:
    x = 0.0
  return x

print(square_if_positive(tf.constant(10)))
```

Execution:
```bash
root@ab48c4411db3:/notebooks# python tmp.py
Traceback (most recent call last):
  File ""tmp.py"", line 12, in <module>
    print(square_if_positive(tf.constant(10)))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/autograph/impl/api.py"", line 68, in wrapper
    return converted_call(f, recursive, verbose, True, {}, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/autograph/impl/api.py"", line 211, in converted_call
    return converted_f(*effective_args, **kwargs)
  File ""/tmp/tmpaCFtJC.py"", line 21, in square_if_positive
    ag__.rewrite_graph_construction_error(ag_source_map__)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/autograph/core/errors.py"", line 137, in rewrite_graph_construction_error
    raise new_error
tensorflow.contrib.autograph.core.errors.GraphConstructionError: Traceback (most recent call last):
  File ""tmp.py"", line 7, in square_if_positive
    if x > 0:
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/autograph/utils/multiple_dispatch.py"", line 50, in run_cond
    return control_flow_ops.cond(condition, true_fn, false_fn)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 2126, in cond
    (val_x.dtype.name, val_y.dtype.name))

Outputs of true_fn and false_fn must have the same type: int32, float32
```"
23052,Tensorflow not Compatible with Python 3.7 ,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OS 10.13.4
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source (whole package)
- **TensorFlow version (use command below)**: 1.10.1
- **Python version**: 3.7
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**:

Apple LLVM version 9.1.0 (clang-902.0.39.1)
Target: x86_64-apple-darwin17.5.0
Thread model: posix
InstalledDir: /Library/Developer/CommandLineTools/usr/bin

- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A


- **Exact command to reproduce**: 

run the following below:

import numpy as np
import tensorflow as tf
import tensorflow 

from tensorflow import keras
from keras.models import Sequential
from keras.layers import Embedding

model = Sequential()
model.add(Embedding(5, 2, input_length=5))



You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

So it appears Tensorflow doesn't play nicely with Python 3.7 - there is a source issue when I run ANY tensorflow or keras command using python 3.7 (but runs perfectly fine with earlier versions of python - reverted back and everything ran perfectly fine). 

Here are all the relevant files with environment (it actually errors out when running in p3.7) 

files using 3.7:

[broken-old-tf_env.txt](https://github.com/tensorflow/tensorflow/files/2488938/broken-old-tf_env.txt)
[broken-tf_env.txt](https://github.com/tensorflow/tensorflow/files/2488939/broken-tf_env.txt)

normal env:

[old-tf_env.txt](https://github.com/tensorflow/tensorflow/files/2488954/old-tf_env.txt)
[tf_env.txt](https://github.com/tensorflow/tensorflow/files/2488958/tf_env.txt)




### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

Here is the trace error message - this exact message occurs when attempting to run ANY keras or tf command using python 3.7:

(for test case, honestly even the most simple lines will cause the break - try;

embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))

Or for keras:

model = Sequential()
model.add(Embedding(10000, 50, input_length=3334))


Error message:
---------------------------------------------------------------------------
UnboundLocalError                         Traceback (most recent call last)
<ipython-input-2-e17e94177b7f> in <module>()
      6 embedding_size = 100
      7 
----> 8 embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))
      9 nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size)))
     10 nce_biases = tf.Variable(tf.zeros([vocabulary_size]))

~/.local/share/virtualenvs/embedded-feature-extraction-o8pNKOHv/lib/python3.7/site-packages/tensorflow/python/ops/random_ops.py in random_uniform(shape, minval, maxval, dtype, seed, name)
    233   with ops.name_scope(name, ""random_uniform"", [shape, minval, maxval]) as name:
    234     shape = _ShapeTensor(shape)
--> 235     minval = ops.convert_to_tensor(minval, dtype=dtype, name=""min"")
    236     maxval = ops.convert_to_tensor(maxval, dtype=dtype, name=""max"")
    237     seed1, seed2 = random_seed.get_seed(seed)

~/.local/share/virtualenvs/embedded-feature-extraction-o8pNKOHv/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype)
    996       name=name,
    997       preferred_dtype=preferred_dtype,
--> 998       as_ref=False)
    999 
   1000 

~/.local/share/virtualenvs/embedded-feature-extraction-o8pNKOHv/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)
   1092 
   1093     if ret is None:
-> 1094       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1095 
   1096     if ret is NotImplemented:

~/.local/share/virtualenvs/embedded-feature-extraction-o8pNKOHv/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)
    215                                          as_ref=False):
    216   _ = as_ref
--> 217   return constant(v, dtype=dtype, name=name)
    218 
    219 

~/.local/share/virtualenvs/embedded-feature-extraction-o8pNKOHv/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name, verify_shape)
    194   tensor_value.tensor.CopyFrom(
    195       tensor_util.make_tensor_proto(
--> 196           value, dtype=dtype, shape=shape, verify_shape=verify_shape))
    197   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)
    198   const_tensor = g.create_op(

~/.local/share/virtualenvs/embedded-feature-extraction-o8pNKOHv/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape)
    534     raise TypeError(
    535         ""Element type not supported in TensorProto: %s"" % numpy_dtype.name)
--> 536   append_fn(tensor_proto, proto_values)
    537 
    538   return tensor_proto

tensorflow/python/framework/fast_tensor_util.pyx in tensorflow.python.framework.fast_tensor_util.AppendFloat32ArrayToTensorProto()

~/.local/share/virtualenvs/embedded-feature-extraction-o8pNKOHv/lib/python3.7/site-packages/google/protobuf/internal/containers.py in append(***failed resolving arguments***)
    249   def append(self, value):
    250     """"""Appends an item to the list. Similar to list.append().""""""
--> 251     self._values.append(self._type_checker.CheckValue(value))
    252     if not self._message_listener.dirty:
    253       self._message_listener.Modified()

UnboundLocalError: local variable 'self' referenced before assignment
"
23050,Batch Normalization with virtual_batch_size not equal to None not implemented correctly for inference time,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.10.1-0-g4dcfddc5d1 1.10.1
- **Python version**: Python 3.6.2
- **Exact command to reproduce**: python main.py
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Mobile device**: N/A
- **Bazel version**: N/A

### Describe the problem
The batch normalization implementation respects the virtual_batch_size parameter in both train and inference modes. As such you are unable to do inference with batch sizes that are not multiples of the virtual_batch_size. Algorithm 1 in the [ghost batch norm paper](https://arxiv.org/pdf/1705.08741.pdf) makes it clear that virtual_batch_size should only be respected in train mode.

Relevant tf source code: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/normalization.py

### Source code / logs

```python
import tensorflow as tf
import numpy as np

virtual_batch_size = 32

training = tf.placeholder(tf.bool, shape=[])
x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])
x_norm = tf.layers.batch_normalization(x, training=training, virtual_batch_size=virtual_batch_size)
update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)

sess = tf.Session()

sess.run(tf.global_variables_initializer())


for _ in range(10):
    sess.run([update_ops, x_norm], feed_dict={x : np.random.random(size=[4*virtual_batch_size, 28, 28, 1]), training : True})

sess.run(x_norm, feed_dict={x : np.random.random(size=[virtual_batch_size, 28, 28, 1]), training : False})

# fails after this line
sess.run(x_norm, feed_dict={x : np.random.random(size=[1, 28, 28, 1]), training : False})    
```
"
23049,Ghost Batch Normalization performance,"GBN seems to be at least twice as slow as regular BN. This is the case even when using the full batch for normalization in GBN mode (`virtual_batch_size=batch_size`).

You can reproduce that with tensorflow/models cifar10 example by running the script below, also available at (https://gist.github.com/MustafaMustafa/a12485746e4c877620a818d227982e1c):

```bash

git clone git@github.com:tensorflow/models.git slow_GBN_issue
cd slow_GBN_issue/tutorials/image/cifar10_estimator

# use tf.layer.batch_normalization
sed -i  's/tf.contrib.layers.batch_norm/tf.layers.batch_normalization/' model_base.py
sed -i  's/decay=/momentum=/' model_base.py
sed -i  's/is_training=/training=/' model_base.py
sed -i  '/data_format=data_format)/c\axis=-1 if data_format==""NHWC"" else 1)' model_base.py

# make val dataset a multiple of batchsize (multiple of 128)
sed -i  's/return 1000/return 9984/' cifar10.py

python generate_cifar10_tfrecords.py --data-dir=${PWD}/cifar-10-data
if [ -d ./cifar10_logs ]
then
  rm -rf ./cifar10_logs
fi
python3 cifar10_main.py --data-dir=${PWD}/cifar-10-data --job-dir=./cifar10_logs --num-gpus=1 --eval-batch-size 128 --train-steps=500 2>&1 | tee bn.log

# use virtual_batch_size
sed -i '/fused/a virtual_batch_size=128,' model_base.py
if [ -d ./cifar10_logs ]
then
  rm -rf ./cifar10_logs
fi
python3 cifar10_main.py --data-dir=${PWD}/cifar-10-data --job-dir=./cifar10_logs --num-gpus=1 --eval-batch-size 128 --train-steps=500 2>&1 | tee gbn.log

# show samples of timing prints
echo ""BN:""
grep ""sec)"" bn.log | tail -1
echo ""GBN:""
grep ""sec)"" gbn.log | tail -1
```

Output:
```
BN:
INFO:tensorflow:learning_rate = 0.1, loss = 2.09511 (4.351 sec)
GBN:
INFO:tensorflow:learning_rate = 0.1, loss = 2.37345 (9.071 sec)
```
------------------------

### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow version (use command below)**: v1.12.0-rc0-0-g1a6dea3 1.12.0-rc0
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: 0.16.0
- **GCC/Compiler version (if compiling from source)**: 7.1.0
- **CUDA/cuDNN version**: 9.2/7.1.4
- **GPU model and memory**: Titan X Pascal 12GB
- **Have I written custom code**: No
- **TensorFlow installed from**: Compiled from source
- **Exact command to reproduce**: bash script for how to reproduce provided in the description.
- **Mobile device**: N/A"
23048,Bug in function: MutableGraphView::ReplaceInput(..) ?,"------------------------
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: v1.11
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: 0.16
- **GCC/Compiler version (if compiling from source)**: 5.4.0 20160609
- **CUDA/cuDNN version**:  N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:


### Describe the problem
I am modifying the Tensorflow source to add a new `tensorflow::grappler::CustomGraphOptimizer` pass, and I am having a hard time of using the fuction: `MutableGraphView::ReplaceInput(..)`. I think there is a bug inside this function, (the master branch version is the same as the r1.11 version), in that the new input is not specified with the `output_port_id`:

### Source code / logs
The description of the function seems pretty clear in the header file:
```c++
  // Replaces the input for the output nodes of 'old_input' with a port
  // `output_port_id` with 'new_input'.
  //
  // E.g: We have 2 nodes that use 'bar' node outputs as inputs:
  // foo(bar:0, bar:1),  foo2(other:0, bar:0)
  // Calling ReplaceInput(bar, new, 0) changes every occurrence of bar:0 for
  // new:0.  Result:
  // foo(new:0, bar:1),  foo2(other:0, new:0)
  void ReplaceInput(const NodeDef& old_input, const NodeDef& new_input,
                    int output_port_id = 0);

```
However in the .cc code of the function body, I don't see where we set the output port index to the new_input:
```c++
void MutableGraphView::ReplaceInput(const NodeDef& old_input,
                                    const NodeDef& new_input,
                                    const int output_port_id) {
  GraphView::OutputPort output_port =
      GetOutputPort(old_input.name(), output_port_id);
  auto fanout = GetFanout(output_port);
  for (auto& input_port : fanout) {
    // Isn't that here we should have something like: strings::StrCat(new_input.name(), "":"", output_port_id); 
   // instead of just put the new_input.name() below!?
    input_port.node->set_input(input_port.port_id, new_input.name());  
    AddFanouts(input_port.node);
  }
}
```

Also I have a related question, if the output port indexes are different between the new and old input, how can I properly replace the input? e.g. something like
```c++
  // foo(bar:0, bar:1),  foo2(other:0, bar:0)
  // Calling ReplaceInput(bar, 0, new, 2) changes every occurrence of bar:0 for
  // new:2.  Result:
  // foo(new:2, bar:1),  foo2(other:0, new:2)
```

Thanks a lot!

"
23047,Adam implement details about tf.train.Adamoptimizer,"Hello，recent days I want to implement a Adam optimizer to compare with the tf.train.Adamoptimizer ,I use the formula mentioned in A DAM : A M ETHOD FOR S TOCHASTIC O PTIMIZATION,(the end of the section 2), but when I use the optimizer to train language model, I found the update speed of trainable variables using original Adamoptimer is more fast than mine. I just wonder the implement details about the tf.train.Adamoptimizer , is there any improvement , or any other revised manners ? If you have some advice ,please !!!!"
23046,Java API Performance & Multi-Threaded Latency,Question removed as answer was found...
23045,tf.contrib.layers.layer_norm fails in tf.estimator.train_and_evaluate() if drop_remainder==False,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Red Hat Enterprise Linux 7.5, 
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: unknown
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 3.5.4+
- **Bazel version (if compiling from source)**: 0.15.2- (@non-git)
- **GCC/Compiler version (if compiling from source)**: 6.4.1
- **CUDA/cuDNN version**: V9.2.148
- **GPU model and memory**: None
- **Exact command to reproduce**: None


### Describe the problem
I have a model using  `tf.contrib.layers.layer_norm` that runs fine throughout training but fails as soon as evaluation starts with the error displayed below (the input to  `tf.contrib.layers.layer_norm` has undefined rank). I suspect this to arise as soon as tensorflow uses a dynamic batch size during evaluation, at least when I set `drop_remainder==True` the problem disappears. The exact line of code causing the error is:
`output = tf.contrib.layers.layer_norm(
            tf.add(first_sublayer, pos_layer_2)
        )`
Any help/hints/ideas?

### Source code / logs
`  tf.add(first_sublayer, pos_layer_2)
  File ""/hpc_modules/at10.0-based/deeplearning/tensorflow/tensorflow-v1.8.0_openmpi-3.1.2/lib64/python3.5/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 183, in func_with_args
    return func(*args, **current_args)
  File ""/hpc_modules/at10.0-based/deeplearning/tensorflow/tensorflow-v1.8.0_openmpi-3.1.2/lib64/python3.5/site-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 2135, in layer_norm
    raise ValueError('Inputs %s has undefined rank.' % inputs.name)
ValueError: Inputs smiles_attention/Add_1:0 has undefined rank.
`
"
23044,Failed to allocate tensors.,"- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPhone 6s
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): master
- Python version: 2.7
- Bazel version (if compiling from source): 0.15.0
- GCC/Compiler version (if compiling from source): None
- CUDA/cuDNN version: 7.0
- GPU model and memory: None
- Exact command to reproduce: None

I get this error - ""tensorflow/contrib/lite/kernels/conv.cc:201 filter->type != data_type (3 != 1)
Node 1 failed to prepare.""

in this part of code:
```
    if (interpreter->AllocateTensors() != kTfLiteOk) {
        NSLog(@""Failed to allocate tensors."");
        exit(-1);
    }
```

For float model everything work correctly, but for quant I get this issue.

How could I Fix it?"
23043,[Bug or Not?] Variable and VarHandleOp has different initial value in creating slots in ExponentialMovingAverage ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 7
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: None
- **TensorFlow installed from (source or binary)**: source 
- **TensorFlow version (use command below)**:  master
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.15.0
- **GCC/Compiler version (if compiling from source)**:  None
- **CUDA/cuDNN version**: 7.0
- **GPU model and memory**: None
- **Exact command to reproduce**: None

### Describe the problem
Variable and ResourceVariable has different initial value here. Is it a bug or expect action?
```
        # For variables: to lower communication bandwidth across devices we keep
        # the moving averages on the same device as the variables. For other
        # tensors, we rely on the existing device allocation mechanism.
        with ops.init_scope():
          if isinstance(var, variables.Variable):
            avg = slot_creator.create_slot(var,
                                           var.initialized_value(),
                                           self.name,
                                           colocate_with_primary=True)
            # NOTE(mrry): We only add `tf.Variable` objects to the
            # `MOVING_AVERAGE_VARIABLES` collection.
            ops.add_to_collection(ops.GraphKeys.MOVING_AVERAGE_VARIABLES, var)
          else:
            avg = slot_creator.create_zeros_slot(
                var,
                self.name,
                colocate_with_primary=(var.op.type in [""Variable"",
                                                       ""VariableV2"",
                                                       ""VarHandleOp""]))
            if self._zero_debias:
              zero_debias_true.add(avg)
        self._averages[var] = avg
```
 
"
23041,[Feature Request] tf.keras expose align_corners,"----------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
N/A
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
N/A
- **TensorFlow installed from (source or binary)**:
N/A
- **TensorFlow version (use command below)**:
N/A
- **Python version**:
N/A
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
N/A

### Describe the problem
We need to expose `align_corners` in this Keras upsample
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/convolutional.py#L1935

See also https://github.com/keras-team/keras/pull/10994#issuecomment-430305648
"
23040,Can tflite model implementation on FPGA ?,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
23039,Broken links to API docs,"### Describe the problem
This is a docs bug.
Several (maybe all?) links to API docs are broken.
eg. on the page [summaries_and_tensorboard](https://www.tensorflow.org/guide/summaries_and_tensorboard) the links to the summary operations go to https://www.tensorflow.org/api_guides/python/summary, but I presume they should point to https://www.tensorflow.org/api_docs/python/tf/summary
The same pattern happens across other pages that link to the Python API too.

I wanted to submit a PR but I couldn't figure out the schema for URLs of pages in [site/en/api_docs](https://github.com/tensorflow/docs/tree/master/site/en/api_docs). 

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
MacOS High Sierra 10.13.6
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
n/a
- **TensorFlow installed from (source or binary)**:
`pip`
- **TensorFlow version (use command below)**:
1.10
- **Python version**:
3
- **Bazel version (if compiling from source)**:
n/a
- **GCC/Compiler version (if compiling from source)**:
n/a
- **CUDA/cuDNN version**:
n/a
- **GPU model and memory**:
n/a
- **Exact command to reproduce**:
https://www.tensorflow.org/guide/faq the link 'See also the API documentation on building graphs.'
or:
https://www.tensorflow.org/guide/summaries_and_tensorboard the links to 'summary operations'.


"
23038,AssertAllCloseToAccordingToType()  Error,"

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
no
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
1.08
- **Python version**:
3.0
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
CPU

Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
Comparing two results from division (Array-a / Array-b), if I use self.AssertAllEqual(a,b), it works perfectly, however, if I use self.AssertAllCloseAccordingToType(a , b), it gives me an error, saying:
not close where =  (array([], dtype=int64),)
not close lhs =  []
not close rhs =  []
not close dif =  []
not close tol =  []

I figured that there is a NaN in the array, but AssertAllEqual handles the NaN perfectly. 
"
23037,How to save model for Tensorflow Serving while using Tensorflow distributed training?,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
23036,[Tflite Model]The tflite model inception_resnet_v2.tflite(2018.04.27) lacks softmax operation,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
I use [intel/webml-polyfill](https://github.com/intel/webml-polyfill) to parse [this model](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md#image-classification-float-models) and do inference, but get the strange output result as the following:

order | Label | Probability
-- | -- | --
1 | bee eater | 1093.54%
2 | jay | 369.31%
3 | brambling | 363.70%

After add a softmax layer, the output result seems more normal:

order | Label | Probability
-- | -- | --
1 | bee eater | 96.99%
2 | jay | 0.07%
3 | brambling | 0.07%

So if the model need another ""softmax"" operation after ""fullyconnect"" for the image classification task? 


### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
23034,TensorFlow doesn't match  cuda9.1( libcublas 9.1 or later version) ?,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
23033,Cannot calculate tf.gradients wrt embedding_matrix,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian 8.1
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.3.0-rc2-20-g0787eee 1.3.0
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**: 
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:  8/6.0.21
- **GPU model and memory**: Titan Xp
- **Exact command to reproduce**: 

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I used tensorflow to implement an end-to-end lambdaRank retrieval model. There are 3 modules: rep_module, inter_module, and L2R_module. The embeddings (emb_mat) was defined in L2R_module, and pass as params to rep_module and inter_module: So the aggregating L2R module will have:

self.emb_mat = tf.get_variable(""emb_mat"",
        shape=[self.vocab_size, self.emb_dim], dtype=tf.float32)
self.rep_mod = RepModule(...., emb_mat=self.emb_mat)
self.inter_mod = InterModule(..., emb_mat=self.emb_mat)
The goal is to let the emb_mat shared by both rep and inter modules, and learn it jointly with those 2 modules. This L2R module will output a batch of scores: score=(Batch_size, 1)

Then I have a another higher-level lambdaRank module to calculate the gradients by hand (I cannot use built-in off the self optimizer, since I have to get the grads and multiply that with the things in lambda rank). I have a _jacobian(y, x) function as follows:

  def _jacobian(self, y_flat, x):
    """"""
    https://github.com/tensorflow/tensorflow/issues/675
    for ranknet and lambdarank
    """"""

    loop_vars = [
        tf.constant(0, tf.int32),
        tf.TensorArray(tf.float32, size=self.batch_size),
    ]

    _, jacobian = tf.while_loop(
        lambda j, _: j < self.batch_size,
        lambda j, result: (j + 1, result.write(j, tf.gradients(y_flat[j], x))),
        loop_vars)

    return jacobian.stack()  
which will calculate the grad of each element of y wrt x, and reassemble them together. I can get Jacobians of all score wrt all other variables(model parameters, if I use a fixed embedding, so emb_mat is no longer in trainable_variables()) except the emb_mat. My other variables are like tf.layers.. variables like

tf.Variable 'conv1/conv1d/kernel:0' shape=(3, 300, 256) dtype=float32_ref,
but it cannot calculate the gradients wrt the emb_mat. It returned something like : TypeError: Failed to convert object of type <class 'list'> to Tensor. Contents: [<tensorflow.python.framework.ops.IndexedSlices object at 0x7f025e161f28>] and TypeError: Expected binary or unicode string, got <tensorflow.python.framework.ops.IndexedSlices object at 0x7f025e161f28>
The whole traceback is attached in logs section
### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
Class L2R_Model(object):
    .....
    self.emb_mat = tf.get_variable(""emb_mat"",
            shape=[self.vocab_size, self.emb_dim], dtype=tf.float32,
            initializer=tf.orthogonal_initializer(1.0))
    self.rep_mod = RepModule(...., emb_mat=self.emb_mat)
    self.inter_mod = InterModule(..., emb_mat=self.emb_mat)

def _jacobian(self, y_flat, x):
        """"""
        https://github.com/tensorflow/tensorflow/issues/675
        for ranknet and lambdarank
        """"""

        loop_vars = [
            tf.constant(0, tf.int32),
            tf.TensorArray(tf.float32, size=self.batch_size),
        ]

        _, jacobian = tf.while_loop(
            lambda j, _: j < self.batch_size,
            lambda j, result: (j + 1, result.write(j, tf.gradients(y_flat[j], x))),
            loop_vars)

        return jacobian.stack()

Traceback (most recent call last):
  File ""/u/nieyifan/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py"", line 460, in make_tensor_proto
    str_values = [compat.as_bytes(x) for x in proto_values]
  File ""/u/nieyifan/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py"", line 460, in <listcomp>
    str_values = [compat.as_bytes(x) for x in proto_values]
  File ""/u/nieyifan/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/compat.py"", line 65, in as_bytes
    (bytes_or_text,))
TypeError: Expected binary or unicode string, got <tensorflow.python.framework.ops.IndexedSlices object at 0x7f025e161f28>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""main_lambda.py"", line 234, in <module>
    debug()
  File ""main_lambda.py"", line 230, in debug
    inter_param_dict=inter_param_dict, resume=r_flag)
  File ""/u/nieyifan/projects/L2R_RM/L2R_LambdaRank.py"", line 25, in __init__
    self.loss, self.num_pairs, self.score, self.train_op = self._build_model()
  File ""/u/nieyifan/projects/L2R_RM/L2R_LambdaRank.py"", line 215, in _build_model
    grads = [self._get_derivative(score, Wk, lambda_ij) for Wk in vars]
  File ""/u/nieyifan/projects/L2R_RM/L2R_LambdaRank.py"", line 215, in <listcomp>
    grads = [self._get_derivative(score, Wk, lambda_ij) for Wk in vars]
  File ""/u/nieyifan/projects/L2R_RM/L2R_LambdaRank.py"", line 112, in _get_derivative
    dsi_dWk = self._jacobian(score, Wk)  # (BS, )
  File ""/u/nieyifan/projects/L2R_RM/L2R_LambdaRank.py"", line 98, in _jacobian
    loop_vars)
  File ""/u/nieyifan/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2775, in while_loop
    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)
  File ""/u/nieyifan/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2604, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File ""/u/nieyifan/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2554, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File ""/u/nieyifan/projects/L2R_RM/L2R_LambdaRank.py"", line 97, in <lambda>
    lambda j, result: (j + 1, result.write(j, tf.gradients(y_flat[j], x))),
  File ""/u/nieyifan/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py"", line 175, in wrapped
    return _add_should_use_warning(fn(*args, **kwargs))
  File ""/u/nieyifan/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py"", line 302, in write
    value = ops.convert_to_tensor(value, name=""value"")
  File ""/u/nieyifan/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 611, in convert_to_tensor
    as_ref=False)
  File ""/u/nieyifan/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 676, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/u/nieyifan/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 121, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/u/nieyifan/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 102, in constant
    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""/u/nieyifan/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py"", line 464, in make_tensor_proto
    ""supported type."" % (type(values), values))
TypeError: Failed to convert object of type <class 'list'> to Tensor. Contents: [<tensorflow.python.framework.ops.IndexedSlices object at 0x7f025e161f28>]. Consider casting elements to a supported type."
23032,"Multiple gpus (1080Ti) do not speed up training, test on cifar10_estimator code","I was trying to test the performance of multi GPUs version of [cifar10_estimator][1] on 2 or 3 1080Ti, but received no speed-up.

I find some useful information about hardware [here][2], but still confused how to solve it.

My environment:
 - docker image tensorflow/tensorflow:latest-gpu-py3
 - Ubuntu VERSION=16.04.5 LTS
 - Python3
 - CUDA_VERSION=9.0.176
 - tensorflow-gpu=1.11.0


GPU info:

    nvidia-smi topo -m
    
    	GPU0	GPU1	GPU2	GPU3	GPU4	GPU5	GPU6	GPU7	CPU Affinity
    GPU0	 X 	PIX	PHB	PHB	SYS	SYS	SYS	SYS	0-7
    GPU1	PIX	 X 	PHB	PHB	SYS	SYS	SYS	SYS	0-7
    GPU2	PHB	PHB	 X 	PIX	SYS	SYS	SYS	SYS	0-7
    GPU3	PHB	PHB	PIX	 X 	SYS	SYS	SYS	SYS	0-7
    GPU4	SYS	SYS	SYS	SYS	 X 	PIX	PHB	PHB	8-15
    GPU5	SYS	SYS	SYS	SYS	PIX	 X 	PHB	PHB	8-15
    GPU6	SYS	SYS	SYS	SYS	PHB	PHB	 X 	PIX	8-15
    GPU7	SYS	SYS	SYS	SYS	PHB	PHB	PIX	 X 	8-15

    Legend:
    
      X    = Self
      SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)
      NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node
      PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)
      PXB  = Connection traversing multiple PCIe switches (without traversing the PCIe Host Bridge)
      PIX  = Connection traversing a single PCIe switch
      NV#  = Connection traversing a bonded set of # NVLinks


1 gpu bach_size=128

    INFO:tensorflow:loss = 2.2576141, step = 200 (3.729 sec)
    INFO:tensorflow:learning_rate = 0.1, loss = 2.2576141 (3.729 sec)
    INFO:tensorflow:Average examples/sec: 2821.06 (2858.65), step = 200
    INFO:tensorflow:Average examples/sec: 2847.23 (3496.06), step = 210
    INFO:tensorflow:Average examples/sec: 2857.91 (3102.29), step = 220
    INFO:tensorflow:Average examples/sec: 2867.04 (3083.62), step = 230
    INFO:tensorflow:Average examples/sec: 2889.21 (3514.15), step = 240
    INFO:tensorflow:Average examples/sec: 2913.15 (3636.28), step = 250
    INFO:tensorflow:Average examples/sec: 2915.99 (2988.94), step = 260
    INFO:tensorflow:Average examples/sec: 2901.94 (2578.95), step = 270
    INFO:tensorflow:Average examples/sec: 2888.87 (2575.46), step = 280
    INFO:tensorflow:Average examples/sec: 2892.13 (2986.66), step = 290
    INFO:tensorflow:global_step/sec: 24.25

2 gpu bach_size=256 

    INFO:tensorflow:loss = 2.4630964, step = 200 (5.971 sec)
    INFO:tensorflow:learning_rate = 0.1, loss = 2.4630964 (5.971 sec)
    INFO:tensorflow:Average examples/sec: 3255.68 (4296.71), step = 200
    INFO:tensorflow:Average examples/sec: 3297.51 (4437.93), step = 210
    INFO:tensorflow:Average examples/sec: 3332.15 (4275.33), step = 220
    INFO:tensorflow:Average examples/sec: 3363.86 (4254.65), step = 230
    INFO:tensorflow:Average examples/sec: 3395.09 (4316.94), step = 240
    INFO:tensorflow:Average examples/sec: 3418.44 (4094.23), step = 250
    INFO:tensorflow:Average examples/sec: 3447.17 (4364.24), step = 260
    INFO:tensorflow:Average examples/sec: 3474.56 (4379.02), step = 270
    INFO:tensorflow:Average examples/sec: 3492.73 (4067.13), step = 280
    INFO:tensorflow:Average examples/sec: 3514.19 (4244.23), step = 290
    INFO:tensorflow:global_step/sec: 16.6026

3 gpu bach_size=384

    INFO:tensorflow:loss = 2.0980535, step = 200 (9.329 sec)
    INFO:tensorflow:learning_rate = 0.1, loss = 2.0980535 (9.329 sec)
    INFO:tensorflow:Average examples/sec: 3214.65 (4165.7), step = 200
    INFO:tensorflow:Average examples/sec: 3272.85 (5130.99), step = 210
    INFO:tensorflow:Average examples/sec: 3324.15 (4955.13), step = 220
    INFO:tensorflow:Average examples/sec: 3376.65 (5174.76), step = 230
    INFO:tensorflow:Average examples/sec: 3425.48 (5132.15), step = 240
    INFO:tensorflow:Average examples/sec: 3468.29 (4954.35), step = 250
    INFO:tensorflow:Average examples/sec: 3509.91 (5014.23), step = 260
    INFO:tensorflow:Average examples/sec: 3544.29 (4755.56), step = 270
    INFO:tensorflow:Average examples/sec: 3579.69 (4901.39), step = 280
    INFO:tensorflow:Average examples/sec: 3617.84 (5156.66), step = 290
    INFO:tensorflow:global_step/sec: 13.1009


[![enter image description here][3]][3]


  [1]: https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10_estimator
  [2]: https://github.com/rossumai/keras-multi-gpu/blob/master/blog/docs/hardware.md
  [3]: https://i.stack.imgur.com/yqIZX.png"
23031,"Bazel Build fails - ""ERROR: missing input file '//tensorflow/core/api_def:python_api/api_def_DatasetToSingleElement.pbtxt'""","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 WSL on Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: r1.8
- **Python version**: 3.5
- **Bazel version (if compiling from source)**:0.10.1
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I am trying to build tensorflow from source. I am getting the following error when I run bazel build command.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
ERROR: missing input file '//tensorflow/core/api_def:python_api/api_def_DatasetToSingleElement.pbtxt':/mnt/c/workspace/tensorflow/tensorflow/core/api_def/python_api/api_def_DatasetToSingleElement.pbtxt (No such file or directory)
ERROR: /mnt/c/workspace/tensorflow/tensorflow/python/BUILD:1594:1: //tensorflow/python:user_ops_pygenrule: missing input file '//tensorflow/core/api_def:python_api/api_def_DatasetToSingleElement.pbtxt'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /mnt/c/workspace/tensorflow/tensorflow/python/BUILD:1594:1 1 input file(s) do not exist
INFO: Elapsed time: 26.125s, Critical Path: 0.03s
FAILED: Build did NOT complete successfully
```

The file do exist and I am not sure why it is complaining that api_def_DatasetToSingleElement.pbtx file is missing. 

![screen shot 2018-10-16 at 6 08 56 pm](https://user-images.githubusercontent.com/12464203/47052588-a898e400-d16e-11e8-85ac-3cb4746ac3b5.png)"
23030,Error using kernel_regularizer with tf.layers.Dense and tf.contrib.distribute.MirroredStrategy TF v1.12.0rc0 regression,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: ('v1.12.0-rc0-0-g1a6dea36de', '1.12.0-rc0')
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0.176/7.2.1.38
- **GPU model and memory**: Nvidia V100 16GB
- **Exact command to reproduce**: See below

### Describe the problem
Attempting to use a `tf.contrib.layers.l2_regularizer` for the `kernel_regularizer` param of `tf.layers.Dense` causes an `AssertionError` when using with `tf.contrib.distribute.MirroredStrategy` under Tensorflow v1.12.0rc0, but not under Tensorflow v1.11.0. This also occurs with several other layers including `tf.layers.dense` and `tf.layers.conv2d`.

### Source code / logs
Source of test.py:
```python
import tensorflow as tf

DO_L2_REG = False

tf.logging.set_verbosity(tf.logging.INFO)

def model_fn(features, labels, mode):
    regularizer = tf.contrib.layers.l2_regularizer(0.001) if DO_L2_REG else None
    layer = tf.layers.Dense(1, kernel_regularizer=regularizer)
    logits = layer(features)

    if mode == tf.estimator.ModeKeys.PREDICT:
        predictions = {""logits"": logits}
        return tf.estimator.EstimatorSpec(mode, predictions=predictions)

    loss = tf.losses.mean_squared_error(
        labels=labels, predictions=tf.reshape(logits, []))

    loss += tf.losses.get_regularization_loss()

    if mode == tf.estimator.ModeKeys.EVAL:
        return tf.estimator.EstimatorSpec(mode, loss=loss)

    if mode == tf.estimator.ModeKeys.TRAIN:
        train_op = tf.train.GradientDescentOptimizer(0.2).minimize(loss, global_step=tf.train.get_global_step())#loss_fn())
        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)

def input_fn():
    features = tf.data.Dataset.from_tensors([[1.]]).repeat()
    labels = tf.data.Dataset.from_tensors(1.).repeat()
    return tf.data.Dataset.zip((features, labels))

distribution = tf.contrib.distribute.MirroredStrategy()
config = tf.estimator.RunConfig(train_distribute=distribution)
classifier = tf.estimator.Estimator(model_fn=model_fn, config=config, model_dir='out_test')
print('********* start train **********')
classifier.train(input_fn=input_fn, steps=1000)
print('********* end train/start eval **********')
classifier.evaluate(input_fn=input_fn, steps=1000)
print('********* end eval **********')
```

### Results of testing this code with different version of Tensorflow and `DO_L2_REG` values:
* Calling this with Tensorflow v1.11.0 with `DO_L2_REG` set to either `True` or `False` works as expected, both training and evaluation run successfully.
* Calling this with Tensorflow v1.12.0rc0 with `DO_L2_REG` set to `False` works as expected, both training and evaluation run successfully.
* Calling this with Tensorflow v1.12.0rc0 with `DO_L2_REG` set to `True` results in the following error when the `classifier.train` is called:
```
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Error reported to Coordinator:
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception
    yield
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 795, in run
    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 1195, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""test.py"", line 10, in model_fn
    logits = layer(features)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py"", line 374, in __call__
    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 746, in __call__
    self.build(input_shapes)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/layers/core.py"", line 944, in build
    trainable=True)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py"", line 293, in add_weight
    self._handle_weight_regularization(name, variable, regularizer)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 638, in _handle_weight_regularization
    self.add_loss(functools.partial(_loss_for_variable, variable))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py"", line 142, in add_loss
    loss_tensor = regularizer()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 434, in _tag_unconditional
    loss = loss()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 629, in _loss_for_variable
    with ops.colocate_with(v):
  File ""/usr/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 4094, in _colocate_with_for_gradient
    with self.colocate_with(op, ignore_existing):
  File ""/usr/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 4146, in colocate_with
    op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1307, in internal_convert_to_tensor_or_indexed_slices
    value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1146, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/values.py"", line 439, in _tensor_conversion_mirrored
    assert not as_ref
AssertionError
Traceback (most recent call last):
  File ""test.py"", line 35, in <module>
    classifier.train(input_fn=input_fn, steps=1000)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 354, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 1205, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 1316, in _train_model_distributed
    self.config)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/distribute.py"", line 721, in call_for_each_tower
    return self._call_for_each_tower(fn, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 556, in _call_for_each_tower
    return _call_for_each_tower(self, fn, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 183, in _call_for_each_tower
    coord.join(threads)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py"", line 389, in join
    six.reraise(*self._exc_info_to_raise)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception
    yield
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 795, in run
    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 1195, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""test.py"", line 10, in model_fn
    logits = layer(features)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py"", line 374, in __call__
    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 746, in __call__
    self.build(input_shapes)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/layers/core.py"", line 944, in build
    trainable=True)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py"", line 293, in add_weight
    self._handle_weight_regularization(name, variable, regularizer)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 638, in _handle_weight_regularization
    self.add_loss(functools.partial(_loss_for_variable, variable))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py"", line 142, in add_loss
    loss_tensor = regularizer()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 434, in _tag_unconditional
    loss = loss()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 629, in _loss_for_variable
    with ops.colocate_with(v):
  File ""/usr/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 4094, in _colocate_with_for_gradient
    with self.colocate_with(op, ignore_existing):
  File ""/usr/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 4146, in colocate_with
    op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1307, in internal_convert_to_tensor_or_indexed_slices
    value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1146, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/values.py"", line 439, in _tensor_conversion_mirrored
    assert not as_ref
AssertionError
```
"
23029,Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?,"_Originally posted by @tensorflowbutler in https://github.com/tensorflow/tensorflow/issues/17336#issuecomment-374545205_

Yes please. Can you guys add the axis feature to the scatter_update. I need to mutate certain columns of weight matrices at each layer and then substitute them back. I can make the changes using tf.gather but have no idea how to substitute them back. I tried to work around by taking a a transpose of variable but apparently taking a transpose of a variable effects its mutability. tf.scatter_update gives an error.

TypeError: 'ScatterUpdate' Op requires that input 'ref' be a mutable tensor (e.g.: a tf.Variable) "
23027,rtx2080ti  python3.6 tensorflow1.10 cuda9.0+cudnn7.0  Blas xGEMMBatched launch failed,"I have try tensorflow1.8-1.11  but all failed!

```
import tensorflow as tf
import numpy as np

import os
os.environ[""CUDA_VISIBLE_DEVICES""]=""0""

config = tf.ConfigProto()
config.gpu_options.allow_growth=True
tf.Session(config=config).close()

def calc():
    N = 15 # works for N <= 14
    a = 16
    b = 8
    X = np.random.rand(N, 11520, b, 1).astype(np.float32)
    print(X.nbytes*1e-6, ""MB"")
    W = np.random.rand(N, 11520, a, b).astype(np.float32)
    print(W.nbytes*1e-6, ""MB"")
    X_ = tf.constant(X, name=""X-constant"", dtype=tf.float32)
    W_ = tf.constant(W, name=""W-constant"", dtype=tf.float32)
    return W_ @ X_

tf.reset_default_graph()
a = calc()
sess = tf.Session()
sess.run(tf.global_variables_initializer())
b = sess.run(a)
sess.close()
print(b.shape)
```
"
23026,Installation failed,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  failed at import tensorflow
- **OS Platform and Distribution: Ubuntu 18.04.1 LTS
- **TensorFlow installed from : Anaconda
- **TensorFlow version (use command below)**:  1.11.0
- **Python version**: 3.6.6
- **Bazel version (if compiling from source)**: not using
- **GCC/Compiler version (if compiling from source)**: [GCC 7.3.0] on linux
- **CUDA/cuDNN version**: 9.1.85 /  not sure was automatically download
- **GPU model and memory**: GeForce 840m


### Describe the problem
basically it doesn't work when i run ""import tensorflow""

### Source code / logs
Python 3.6.6 |Anaconda, Inc.| (default, Oct  9 2018, 12:34:16) 
[GCC 7.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""/home/herooooooooo/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/herooooooooo/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/herooooooooo/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/herooooooooo/anaconda3/envs/tf_gpu/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/herooooooooo/anaconda3/envs/tf_gpu/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: libcuda.so.1: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/herooooooooo/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/home/herooooooooo/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/herooooooooo/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/herooooooooo/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/herooooooooo/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/herooooooooo/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/herooooooooo/anaconda3/envs/tf_gpu/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/herooooooooo/anaconda3/envs/tf_gpu/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: libcuda.so.1: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

"
23025,Keras saved_model.simple_save model bigger and bigger,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  Custom code
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Mint 19 Tara
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10.1
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**: python script.py


### Describe the problem
When saving successively a keras model using `tf.get_default_graph()`, the size of `saved_model.pb` increase.

![image](https://user-images.githubusercontent.com/7363034/47020409-982a3e80-d159-11e8-80db-e39f47278f03.png)


### Source code / logs
script.py
```
import time

import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import *

if __name__ == '__main__':

    sess = tf.keras.backend.get_session()

    model = Sequential()

    model.add(Dense(units=64, activation='relu', input_dim=4))
    model.add(Dense(units=4, activation='linear'))

    opt = RMSprop(lr=0.1)
    model.compile(loss='mse', optimizer=opt)

    graph = tf.get_default_graph()  # Use the same graph as global thread

    model.predict(np.array([[1, 1, 1, 1], [1, 1, 1, 1]]))

    for _ in range(100):
        time.sleep(1)
        temp_export_path = '/tmp/models/' + str(time.time()).split(""."")[0]

        with graph.as_default():
            tf.saved_model.simple_save(
                sess,
                temp_export_path,
                inputs={'state': model.input},
                outputs={t.name: t for t in model.outputs})

```
"
23023,Hyperspectral image classification using tensorflow,"
Delivery Status Notification (Failure)Application toolbar
Message Body
Hello sandeep.ladi@gitam.edu,

We're writing to let you know that the group you tried to contact (discuss) may not exist, or you may not have permission to post messages to the group. A few more details on why you weren't able to post:

 * You might have spelled or formatted the group name incorrectly.
 * The owner of the group may have removed this group.
 * You may need to join the group before receiving permission to post.
 * This group may not be open to posting.

If you have questions related to this or any other Google Group, visit the Help Center at https://support.google.com/a/tensorflow.org/bin/topic.py?topic=25838.

Thanks,

tensorflow.org admins



----- Original message -----

X-Google-Smtp-Source: ACcGV625ngDxZGFv6zxaJTNRD5PlUDFyJNQJK/RowOuC79uaJbT7E4TVqkiThwkQIV8Yc/WPsFZ0
X-Received: by 2002:a5d:44ce:: with SMTP id z14-v6mr19096788wrr.286.1539685981354;
        Tue, 16 Oct 2018 03:33:01 -0700 (PDT)
ARC-Seal: i=1; a=rsa-sha256; t=1539685981; cv=none;
        d=google.com; s=arc-20160816;
        b=E/S3lH6dW8jBpv80GEP3mZyz1KyIDntxI9nZGVOdAbeJS3yOgQ3nq4bp7hre3N/HU8
         9HqA9XkD3dHukXpz06r+2R5F3i2Rbio9TsIfnjdh9b454Ps75DUM7gm1nuxEGojtfeZd
         2RKUTWWKH3a/1dOWecoz0fRibWJd0ByhBNTYW2F+p9hMdg9TAP3m9nuxpLl/bF/r/VAw
         62gIT4ydaIgySfEH6uXHh5dYAab8+96sPnw5wZiAHzO1LpUt766K85dv/6asxCHoMMOR
         M5lzAq1FK+XOfca7bJqn7lt2Vu2YIU7wJbIJs2vQKtLwZcl7SYf20rkFeVGrNQ4zmonY
         zuwg==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=user-agent:message-id:disposition-notification-to:return-receipt-to
         :subject:to:from:date:mime-version;
        bh=OsFzSe3J2uxfPu0m4Xtt3CiGAfo46dWIPdy+i82VhII=;
        b=ePJSnFaP1g6RcKwXYOLK4F+lXEJ8Ul1ycwocH3iAPT6Ga19fNe5HJXhaFcbexJywfr
         G+86clKs6GOA2baW3nqnAl/4rAVGMjQyWIUMBPFHdei28lIjvMo3gyXX3ocxM0XMPP2z
         2vQ2qCJZQBHZegkhMO/z+d1/8a/bjqScVAH+gZOXS0hbQIp23fsx9CsPeM6iX2boAlDB
         q8zh8LkYwMUxA+p0EDdmrflNA2hs9mqqiYaCJJQ2oD7VvqHpUsqdMeczcmck75SvB0xO
         uvPdhrFHdp26RDN2JKj36FInwYHZTnTEKw69UFchUoLBSgzOCdCnhBQlIWyEyDX6LGYQ
         8fUg==
ARC-Authentication-Results: i=1; mx.google.com;
       spf=pass (google.com: domain of sandeep.ladi@gitam.edu designates 103.23.29.142 as permitted sender) smtp.mailfrom=sandeep.ladi@gitam.edu
Return-Path: <sandeep.ladi@gitam.edu>
Received: from mail.gitam.edu (mail.gitam.edu. [103.23.29.142])
        by mx.google.com with ESMTP id s1-v6si12238610wrf.2.2018.10.16.03.33.00
        for <discuss@tensorflow.org>;
        Tue, 16 Oct 2018 03:33:00 -0700 (PDT)
Received-SPF: pass (google.com: domain of sandeep.ladi@gitam.edu designates 103.23.29.142 as permitted sender) client-ip=103.23.29.142;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of sandeep.ladi@gitam.edu designates 103.23.29.142 as permitted sender) smtp.mailfrom=sandeep.ladi@gitam.edu
Received: from mail.gitam.edu (webmail.gitam.edu [192.168.23.28])
    by mail.gitam.edu (Postfix) with ESMTPA id B765722BCC5
    for <discuss@tensorflow.org>; Tue, 16 Oct 2018 16:02:46 +0530 (IST)
MIME-Version: 1.0
Content-Type: multipart/alternative;
 boundary=""=_8b103ec7c4f10c51b775e6343c7573ca""
Date: Tue, 16 Oct 2018 16:02:46 +0530
From: sandeep.ladi@gitam.edu
To: discuss@tensorflow.org
Subject: Command to Input Hyperspectral Image dataset to TensorFlow.
Return-Receipt-To: sandeep.ladi@gitam.edu
Disposition-Notification-To: sandeep.ladi@gitam.edu
Message-ID: <e885520003a6c5c626388950aeb89319@gitam.edu>
X-Sender: sandeep.ladi@gitam.edu
User-Agent: GITAM Webmail/1.2.2
X-gitam-MailScanner-Information: Please contact the ISP for more information
X-gitam-MailScanner-ID: B765722BCC5.A15A0
X-gitam-MailScanner: Found to be clean
X-gitam-MailScanner-SpamScore: s
X-gitam-MailScanner-From: sandeep.ladi@gitam.edu
X-Spam-Status: No

--=_8b103ec7c4f10c51b775e6343c7573ca
Content-Transfer-Encoding: 7bit
Content-Type: text/plain; charset=US-ASCII

Dear sir/Madam, 

Greetings of the day. 

I am Mr.Ladi Sandeep Kumar,working as an Assistant Professor in GITAM
University,Visakhapatnam,India.I and my project students are working on
a Project based on Hyperspectral image classification which is an
emerging research topic.The opensource Hyperspectral image datasets are
in .mat files containing reflectance values.We are going to use CNN
technique for image classification using Hyperspectral image dataset of 
Indian pines which is a dataset of 145x145x200 where 145x145 are height
and width of image and 200 is number of channels.There are lot of
resources and guide for using tensorflow CNN toolbox for color image.Can
you please tell me the way and send the code to input the Indian pines
dataset of 145x145 x200 as input to the CNN. 

I will use the tensorFlow toolbox in my  research as well as projects 
to teach my students and quote your website in my publications wherever
I apply tensorflow module.
"
23021,I am getting the same prediction value for all of my test data,"I tried with many combinations of hyper parameters, optimizers and cost functions but I'm getting R2 = 0.33 at max. Please help me finding a solution.
[Python code.txt](https://github.com/tensorflow/tensorflow/files/2482551/Python.code.txt)
[Gholamnejad_dataset1.xlsx](https://github.com/tensorflow/tensorflow/files/2482564/Gholamnejad_dataset1.xlsx)

"
23020,Error in C++ Tensorflow to load model trained by python,"Using tensorflow1.8 in C++,  I load the model (freezed in *.pb) and creat session with graph. Than when I run the session, error occurs as ""Error: RUN FAILED...Invalid argument: Expects arg[0] to be bool but float is peovided"".
Anyone suggestions is needed, Please help me!"
23019,`tf.layers.Conv2D`'s padding doesn't allow variable reuse in some cases,"Minimal code to reproduce:
```python
import tensorflow as tf

example_image = tf.zeros([1, 32, 32, 1])
example_image2 = tf.zeros([1, 41, 41, 1])


convolution_op = tf.layers.Conv2D(32, 3, padding=""SAME"", dilation_rate=4)

convolution_op(example_image)
convolution_op(example_image2)
```
Error:
```
InvalidArgumentError                      Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs)
   1575   try:
-> 1576     c_op = c_api.TF_FinishOperation(op_desc)
   1577   except errors.InvalidArgumentError as e:

InvalidArgumentError: Dimension size must be evenly divisible by 4 but is 49 for 'conv2d/SpaceToBatchND_1' (op: 'SpaceToBatchND') with input shapes: [1,41,41,1], [2], [2,2] and with computed input tensors: input[1] = <4 4>, input[2] = <[4 4][4 4]>.

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-1-05cd8b9e8ce2> in <module>()
      8 
      9 convolution_op(example_image)
---> 10 convolution_op(example_image2)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)
    360 
    361       # Actually call layer
--> 362       outputs = super(Layer, self).__call__(inputs, *args, **kwargs)
    363 
    364     if not context.executing_eagerly():

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    734 
    735       if not in_deferred_mode:
--> 736         outputs = self.call(inputs, *args, **kwargs)
    737         if outputs is None:
    738           raise ValueError('A layer\'s `call` method should return a Tensor '

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/convolutional.py in call(self, inputs)
    184 
    185   def call(self, inputs):
--> 186     outputs = self._convolution_op(inputs, self.kernel)
    187 
    188     if self.use_bias:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py in __call__(self, inp, filter)
    866 
    867   def __call__(self, inp, filter):  # pylint: disable=redefined-builtin
--> 868     return self.conv_op(inp, filter)
    869 
    870 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py in __call__(self, inp, filter)
    518 
    519   def __call__(self, inp, filter):  # pylint: disable=redefined-builtin
--> 520     return self.call(inp, filter)
    521 
    522 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py in _with_space_to_batch_call(self, inp, filter)
    501     crops = _with_space_to_batch_adjust(crops, 0, spatial_dims)
    502     input_converted = array_ops.space_to_batch_nd(
--> 503         input=inp, block_shape=dilation_rate, paddings=paddings)
    504 
    505     result = self.op(input_converted, filter)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py in space_to_batch_nd(input, block_shape, paddings, name)
   7568     _, _, _op = _op_def_lib._apply_op_helper(
   7569         ""SpaceToBatchND"", input=input, block_shape=block_shape,
-> 7570         paddings=paddings, name=name)
   7571     _result = _op.outputs[:]
   7572     _inputs_flat = _op.inputs

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    785         op = g.create_op(op_type_name, inputs, output_types, name=scope,
    786                          input_types=input_types, attrs=attr_protos,
--> 787                          op_def=op_def)
    788       return output_structure, op_def.is_stateful, op
    789 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)
    452                 'in a future version' if date is None else ('after %s' % date),
    453                 instructions)
--> 454       return func(*args, **kwargs)
    455     return tf_decorator.make_decorator(func, new_func, 'deprecated',
    456                                        _add_deprecated_arg_notice_to_docstring(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in create_op(***failed resolving arguments***)
   3153           input_types=input_types,
   3154           original_op=self._default_original_op,
-> 3155           op_def=op_def)
   3156       self._create_op_helper(ret, compute_device=compute_device)
   3157     return ret

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)
   1729           op_def, inputs, node_def.attr)
   1730       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,
-> 1731                                 control_input_ops)
   1732 
   1733     # Initialize self._outputs.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs)
   1577   except errors.InvalidArgumentError as e:
   1578     # Convert to ValueError for backwards compatibility.
-> 1579     raise ValueError(str(e))
   1580 
   1581   return c_op

ValueError: Dimension size must be evenly divisible by 4 but is 49 for 'conv2d/SpaceToBatchND_1' (op: 'SpaceToBatchND') with input shapes: [1,41,41,1], [2], [2,2] and with computed input tensors: input[1] = <4 4>, input[2] = <[4 4][4 4]>.

```

A different error happens if the order of calls to `convolution_op` is inverted:
```
import tensorflow as tf

example_image = tf.zeros([1, 32, 32, 1])
example_image2 = tf.zeros([1, 41, 41, 1])


convolution_op = tf.layers.Conv2D(32, 3, padding=""SAME"", dilation_rate=4)

convolution_op(example_image2)
convolution_op(example_image)
```
which is in short
```
InvalidArgumentError: Dimension size must be evenly divisible by 4 but is 43 for 'conv2d/SpaceToBatchND_1' (op: 'SpaceToBatchND') with input shapes: [1,32,32,1], [2], [2,2] and with computed input tensors: input[1] = <4 4>, input[2] = <[4 7][4 7]>.

```

I don't think this is the correct behaviour. I understand that the object oriented tf.layers classes such as `tf.layers.Conv2D` are meant to be the preferred way of doing variable reuse, but the padding seems to be set with the first call to the function, reducing utility of those functions.


Have I written custom code
N/A
OS Platform and Distribution
N/A
TensorFlow installed from
pip
TensorFlow version
1.10.1
Bazel version 
N/A
CUDA/cuDNN version
N/A
GPU model and memory
N/A
Exact command to reproduce
N/A
Mobile device
N/A
"
23018,.,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
23017,tf.estimator.train's incompatibility with distributed training on Cloud ML Engine is not well-documented,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Fedora 28
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.11.0
- **Python version**: 3.5.0
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

The page https://cloud.google.com/ml-engine/docs/tensorflow/distributed-training-details#tensorflow-config notes that ""The tf.estimator.train method doesn't work with distributed training on Cloud ML Engine. Please use train_and_evaluate instead."". This is not documented on the Estimator page (https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator) or anywhere else I've seen. I believe it would be helpful to document it more prominently, as I can't be the only one who didn't read the ""Using TF_CONFIG for Distributed Training Details"" page and wasted time debugging why a model wouldn't work when distributed.

If possible, it would be even more helpful to make tf.estimator.train raise an exception when run in a distributed ML Engine context, or log a warning. It's unreasonable to expect the user to figure this out themselves as from an API perspective there's no reason to expect `train_and_evaluate` would work where `train` fails (one might reasonably assume `train_and_evaluate` calls `train`)."
23016,ImportError: DLL load failed: The specified module could not be found | lib\site-packages\tensorflow\python\pywrap_tensorflow.py,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Win10 x64
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: -
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.11.0
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**: -
- **GCC/Compiler version (if compiling from source)**: -
- **CUDA/cuDNN version**: CUDA9 / cuDNN7.0.5
- **GPU model and memory**: Nvidia GeForce 1050 Ti
- **Exact command to reproduce**:
pip install tensorflow-gpu
python
import tensorflow as tf

### Describe the problem
I'm trying to install TensorFlow for my study. I tried a few version of python and CUDA/cuDNN but I am still getting the same error. This [blog](https://medium.com/@lmoroney_40129/installing-tensorflow-with-gpu-on-windows-10-3309fec55a00) is used to install properly. I have done each steps exactly the same with the blog. I have read and tried solutions from the tensorflow error list and almost all solution provided on the internet. Result is the same.

### Source code / logs
Traceback (most recent call last):
  File ""C:\Users\Umit Kilic\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Umit Kilic\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Umit Kilic\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Umit Kilic\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Umit Kilic\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Umit Kilic\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Umit Kilic\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Umit Kilic\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Umit Kilic\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Umit Kilic\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Umit Kilic\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Umit Kilic\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Umit Kilic\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found

"
23014,Tensorflow Eager Execution Gradient Calculation slower each epoch,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04
- **TensorFlow installed from (source or binary)**: 1.10 binary
- **Python version**: 3.6
- **CUDA/cuDNN version**: 9.0 / 7.2
- **GPU model and memory**: 1080Ti - 11 Gb
- Bazel version - NA
- Exact command to reproduce - NA
- Mobile device - NA
### Describe the problem

Hi to all!

I have been implementing Temporal Ensembling for Semi-Supervised Learning by Laine et al. with eager execution and a couple of GitHub users noticed that the computations of the gradients is taking gradually more time each epoch. I don't see what is causing this. After benchmarking I could confirm this issue, but I have no idea why this should be happening. Is anyone faced this problem with `tf.GradientTape()` ? Or is this is an issue related to eager execution?

The code for the loss and gradients is the following:

```
def temporal_ensembling_loss(X_train_labeled, y_train_labeled, X_train_unlabeled, model, unsupervised_weight, ensembling_targets):
    """""" Gets the loss for the temporal ensembling model
    Arguments:
        X_train_labeled {tensor} -- labeled samples
        y_train_labeled {tensor} -- labeled train labels
        X_train_unlabeled {tensor} -- unlabeled samples 
        model {tf.keras.Model} -- temporal ensembling model
        unsupervised_weight {float} -- weight of the unsupervised loss
        ensembling_targets {np.array} --  ensembling targets
    Returns:
        {tensor} -- predictions for the ensembles
        {tensor} -- loss value
    """"""

    z_labeled = model(X_train_labeled)
    z_unlabeled = model(X_train_unlabeled)

    current_predictions = tf.concat([z_labeled, z_unlabeled], 0)

    return current_predictions, tf.losses.softmax_cross_entropy(
        y_train_labeled, z_labeled) + unsupervised_weight * (
            tf.losses.mean_squared_error(ensembling_targets, current_predictions))


def temporal_ensembling_gradients(X_train_labeled, y_train_labeled, X_train_unlabeled, model, unsupervised_weight, ensembling_targets):
    """""" Gets the gradients for the temporal ensembling model
    Arguments:
        X_train_labeled {tensor} -- labeled samples
        y_train_labeled {tensor} -- labeled train labels
        X_train_unlabeled {tensor} -- unlabeled samples 
        model {tf.keras.Model} -- temporal ensembling model
        unsupervised_weight {float} -- weight of the unsupervised loss
        ensembling_targets {np.array} --  ensembling targets
    Returns:
        {tensor} -- predictions for the ensembles
        {tensor} -- loss value
        {tensor} -- gradients for each model variables
    """"""

    with tf.GradientTape() as tape:
        ensemble_precitions, loss_value = temporal_ensembling_loss(X_train_labeled, y_train_labeled, X_train_unlabeled,
                                                                   model, unsupervised_weight, ensembling_targets)

    return ensemble_precitions, loss_value, tape.gradient(loss_value, model.variables)
```

I tested it in multiple machines (and in tensorflow 1.10 and 1.11) and the problem persists. 

The gradient calculation takes gradually more time each epoch. Is something related to my code that is causing this or is this a bug with eager execution?

Best Regards"
23013,LSTM weights not loaded correctly in tf.keras,"### Description
Running into this problem when I:
- build model A containing an LSTM layer
- save the model weights with `tf.keras.save_weights()` in TensorFlow format
- at some later point create an identical model A' and load the previously saved weights

Inspection by eye shows the lstm weights differ between model A and A' while weights of other layers (e.g. dense) are loaded correctly from file. When running through the same procedure as above but saving the weights in HDF5 format everything works as expected.

### System Information:
- Have I written custom code: yes
- OS Platform and Distribution: Linux Ubuntu 16.04
- Mobile device: NA
- TensorFlow installed from (source or binary): binary (pip package)
- TensorFlow version: 1.10.0
- Python version: 3.6.6
- Bazel version: NA
- GCC/Compiler version: NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA
- Exact command to reproduce:
```
import tensorflow as tf

input_layer = tf.keras.layers.Input(shape=(None, 5))
lstm_layer = tf.keras.layers.LSTM(units=2)(input_layer)
output_layer = tf.keras.layers.Dense(units=2, activation='softmax')(lstm_layer)

my_model = tf.keras.Model(input_layer, output_layer)

my_model.save_weights(""weights_test/my_weights"")

print('\nlstm:')
print(my_model.layers[1].get_weights())
print('\ndense:')
print(my_model.layers[2].get_weights())

#lstm:
#[array([[ 0.38913965,  0.4081726 , -0.23971727,  ...
#dense:
#[array([[-0.5567662 , -0.38940358],
#       [ 0.16435587,  1.1092712 ]], dtype=float32), array([0., 0.], dtype=float32)]

new_input_layer = tf.keras.layers.Input(shape=(None, 5))
new_lstm_layer = tf.keras.layers.LSTM(units=2)(new_input_layer)
new_output_layer = tf.keras.layers.Dense(units=2, activation='softmax')(new_lstm_layer)

my_new_model = tf.keras.Model(new_input_layer, new_output_layer)
my_new_model.load_weights(""weights_test/my_weights"")

print('\nlstm:')
print(my_new_model.layers[1].get_weights())
print('\ndense:')
print(my_new_model.layers[2].get_weights())

#lstm:
#[array([[-0.41712055,  0.67933106, -0.5161066 , ...
#dense:
#[array([[-0.5567662 , -0.38940358],
#       [ 0.16435587,  1.1092712 ]], dtype=float32), array([0., 0.], dtype=float32)]
```"
23012,tf.make_tensor_proto doesn't work as expected,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.13.6
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: none
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.11.0-rc2-4-gc19e29306c 1.11.0
- **Python version**: Python 3.6.4
- **Bazel version (if compiling from source)**: none
- **GCC/Compiler version (if compiling from source)**: none
- **CUDA/cuDNN version**: none
- **GPU model and memory**: none
- **Exact command to reproduce**:
`print(tf.make_tensor_proto([1,2,3,4], dtype=tf.uint32))`


### Describe the problem

According to TensorProto message definition there are specific fields for different DataType e.g. for DT_UINT32 (https://github.com/tensorflow/tensorflow/blob/903a6399aab19b549fefd0ead836af644f3d00f8/tensorflow/core/framework/tensor.proto#L79-L80) 
However, `make_tensor_proto` puts content for most numeric types into `tensor_content` field.

In addition to that there is same inconsistency in `tf.make_ndarray` function: it tries to deserealize tensor_content and then it searches for correct fields with DataType. https://github.com/tensorflow/tensorflow/blob/903a6399aab19b549fefd0ead836af644f3d00f8/tensorflow/python/framework/tensor_util.py#L548

### Source code / logs

```python
import tensorflow as tf
from tensorflow.core.framework import tensor_pb2
from tensorflow.python.framework import dtypes

current = tf.make_tensor_proto([1,2,3,4], dtype=tf.uint32)
print(current)
#dtype: DT_UINT32
#tensor_shape {
#  dim {
#    size: 4
#  }
#}
#tensor_content: ""\001\000\000\000\002\000\000\000\003\000\000\000\004\000\000\000""

expected = tensor_pb2.TensorProto(dtype=dtypes.uint32.as_datatype_enum, uint32_val=[1,2,3,4])
print(expected)
#dtype: DT_UINT32
#uint32_val: 1
#uint32_val: 2
#uint32_val: 3
#uint32_val: 4
```
"
23010,TensorFlow Lite installation on Raspberry Pi,"Hi,

I've tried stackoverflow but thus far I've not had a reply - so feel free to close off is you feel this is unnecessary.

[https://stackoverflow.com/questions/52804669/tensor-flow-lite-raspberry-pi-installation](url)

I'm just looking from some clarity. I've build TF Lite on my RPi3 and the static lib's been created without issue.

At the moment python is returning the error:

> Traceback (most recent call last): 
>    File ""label_image.py"", line 23, in
>       <module> import tensorflow as tf

What I'm uncertain of is if it pip install TF then how can I know/be certain that I will get/be using TF Lite as apposed to the full version?

Also are there any stats re: TF Lite on the Pi vs full version for image classification?

Thanks & Regards

Mark
"
23007,tf.estimator package not installed,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 7
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.11.0
- **Python version**: 3.6.6
- **Bazel version**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Mobile device**: N/A
- **Exact command to reproduce**: 
`pip install --upgrade tensorflow`
`import tensorflow as tf` 

### Describe the problem
I installed tensorflow via pip in a conda environment. Trying to `import tensorflow` results in a `tf.estimator package not installed.` error. I had a look a #22342 and checked my pandas version to be 0.23.4. 

```python
>>> import tensorflow as tf
tf.estimator package not installed.
```

Interestingly enough if I import pandas before importing tensorflow there is no error
```python
>>> import pandas as pd 
>>> import tensorflow as tf 
# No Error
```

On an added note I can't import pandas after I import tensorflow 
```python
>>> import tensorflow as tf
tf.estimator package not installed.
>>> import pandas as pd
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home1/irteam/anaconda3/envs/umap/lib/python3.6/site-packages/pandas/__init__.py"", line 42, in <module>
    from pandas.core.api import *
  File ""/home1/irteam/anaconda3/envs/umap/lib/python3.6/site-packages/pandas/core/api.py"", line 10, in <module>
    from pandas.core.groupby.groupby import Grouper
  File ""/home1/irteam/anaconda3/envs/umap/lib/python3.6/site-packages/pandas/core/groupby/__init__.py"", line 2, in <module>
    from pandas.core.groupby.groupby import (
  File ""/home1/irteam/anaconda3/envs/umap/lib/python3.6/site-packages/pandas/core/groupby/groupby.py"", line 49, in <module>
    from pandas.core.frame import DataFrame
  File ""/home1/irteam/anaconda3/envs/umap/lib/python3.6/site-packages/pandas/core/frame.py"", line 74, in <module>
    from pandas.core.series import Series
  File ""/home1/irteam/anaconda3/envs/umap/lib/python3.6/site-packages/pandas/core/series.py"", line 67, in <module>
    import pandas.core.ops as ops
AttributeError: module 'pandas' has no attribute 'core'
```"
23005,tf.gradients vs. np.gradient,"Please go to Stack Overflow f

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
Is there a way to get tensorflow to give the same gradient value as numpy when the relationship between two arrays is not known? For example:

import numpy as np
x = np.linspace(0, 1, 10)
y = np.linspace(1, 2, 10)
grad = np.gradient(y, x)

gives [1., 1. ........1.].

Is there a way to do this in tensorflow?

"
23003,Problem on running tensorflow,"Traceback (most recent call last):
  File ""retrain.py"", line 132, in <module>
    import tensorflow as tf
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 72, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
23001,Add Support for Apache Arrow in TensorFlow Dataset,"Apache Arrow is a standard format for in-memory columnar data. It provides a cross-language platform for systems to communicate and operate on data efficiently.

Adding Arrow support in TensorFlow Dataset will allow systems to interface with TensorFlow in a well defined way, without the need to develop custom converters, serialize data, or write to specialized files.

It would be straightforward to add a base layer of Arrow support that works on Arrow record batches (a common struct for Arrow IPC) and extend that layer to support different kinds of Arrow Ops:

* Python memory / Pandas DataFrames
* Arrow Feather files
* Parquet files
* Socket / Pipes

A slightly more involved Op could use Arrow Flight - Arrow-based messaging over gRPC.  Additionally, it would possible to define Ops to connect directly to other systems that can export Arrow data.

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: master branch
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.17.1
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 9.1
- **GPU model and memory**: Quadro M1000M 4G
- **Exact command to reproduce**: N/A
"
22999,Tensor is not an element of this graph.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10, Version 10.0.17134.285.
- **TensorFlow installed from (source or binary)**: Binary.
- **TensorFlow version (use command below)**: b'v1.11.0-rc2-4-gc19e29306c' 1.11.0.
- **Python version**: Python 3.6.6 (v3.6.6:4cf1f54eb7, Jun 27 2018, 03:37:03) [MSC v.1900 64 bit (AMD64)] on win32.
- **CUDA/cuDNN version**: CUDA V9.0.176, cuDNN v7.3.1.
- **GPU model and memory**: Nvidia GTX 970 4 GB.
- **Bazel version**: N/A.
- **Mobile platform**: N/A.
- **Exact command to reproduce**: `python test.py training_dry.wav training_wet.wav validation_dry.wav validation_wet.wav`

### Description

I'm probably doing something horribly wrong with regard to graph/session handling. I tried to copy examples from the documentation but my code is still failing with the following error:

```
C:\Users\Markov\AppData\Local\Programs\Python\Python36\lib\site-packages\scipy\io\wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.
  WavFileWarning)
2018-10-16 00:11:07.807782: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2018-10-16 00:11:08.164098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties:
name: GeForce GTX 970 major: 5 minor: 2 memoryClockRate(GHz): 1.253
pciBusID: 0000:01:00.0
totalMemory: 4.00GiB freeMemory: 3.31GiB
2018-10-16 00:11:08.172047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2018-10-16 00:11:09.229492: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-10-16 00:11:09.234965: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0
2018-10-16 00:11:09.237217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N
2018-10-16 00:11:09.240415: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3023 MB memory) -> physical GPU (device: 0, name: GeForce GTX 970, pci bus id: 0000:01:00.0, compute capability: 5.2)
Commencing training.
Iteration 1
Traceback (most recent call last):
  File ""C:\Users\Markov\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1050, in _run
    subfeed, allow_tensor=True, allow_operation=False)
  File ""C:\Users\Markov\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\ops.py"", line 3488, in as_graph_element
    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
  File ""C:\Users\Markov\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\ops.py"", line 3567, in _as_graph_element_locked
    raise ValueError(""Tensor %s is not an element of this graph."" % obj)
ValueError: Tensor Tensor(""Const:0"", shape=(24085052,), dtype=float32) is not an element of this graph.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""train.py"", line 92, in <module>
    train(dry_training_wav, wet_training_wav, dry_validation_wav, wet_validation_wav)
  File ""train.py"", line 66, in train
    run_operation(dry_training_wav, wet_training_wav, batch_size, minimize, session)
  File ""train.py"", line 26, in run_operation
    operation_output = session.run(operation, feed)
  File ""C:\Users\Markov\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 887, in run
    run_metadata_ptr)
  File ""C:\Users\Markov\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1053, in _run
    'Cannot interpret feed_dict key as Tensor: ' + e.args[0])
TypeError: Cannot interpret feed_dict key as Tensor: Tensor Tensor(""Const:0"", shape=(24085052,), dtype=float32) is not an element of this graph.
```

### Reproduction
```
import sys

import scipy.io.wavfile
import tensorflow as tf

def read_wav(path):
	rate, data = scipy.io.wavfile.read(path)
	return tf.convert_to_tensor(data, dtype = tf.float32)

def get_batch(data, offset, batch_size):
	return data[offset : offset + batch_size]

def get_length(tensor):
	return tensor.shape[0].value

def run_operation(dry_data, wet_data, batch_size, operation, session):
	offset = 0
	output = []
	while offset + batch_size < get_length(dry_training_wav):
		dry_batch = get_batch(dry_data, offset, batch_size)
		wet_batch = get_batch(wet_data, offset, batch_size)
		feed = {
			dry_data: dry_batch,
			wet_data: wet_batch
		}
		operation_output = session.run(operation, feed)
		output.append(operation_output)
		offset += batch_size
	return output

def get_graph():
	graph = tf.Graph()
	with graph.as_default():
		frame_count = 96
		lstm_layers = 64

		frame_shape = [frame_count]

		dry_data = tf.placeholder(tf.float32, frame_shape, 'dry_data')
		wet_data = tf.placeholder(tf.float32, frame_shape, 'wet_data')

		lstm = tf.contrib.cudnn_rnn.CudnnLSTM(lstm_layers, frame_count)
		reshaped_dry_data = tf.reshape(dry_data, [1, 1, frame_count])
		lstm_output, _ = lstm(reshaped_dry_data)
		flat_lstm_output = tf.reshape(lstm_output, frame_shape)
		prediction = tf.nn.elu(flat_lstm_output)

		loss = tf.sqrt(tf.losses.mean_squared_error(prediction, wet_data), name = 'loss')
		optimizer = tf.train.AdamOptimizer()
		minimize = optimizer.minimize(loss, name = 'minimize')
	return graph

def train(dry_training_wav, wet_training_wav, dry_validation_wav, wet_validation_wav):
	graph = get_graph()
	with tf.Session(graph = graph) as session:
		initializer = tf.global_variables_initializer()
		session.run(initializer)
		iteration = 1
		dry_data_placeholder = graph.get_operation_by_name('dry_data')
		batch_size = dry_data_placeholder.outputs[0].shape[0].value
		loss = graph.get_operation_by_name('loss')
		minimize = graph.get_operation_by_name('minimize')
		print('Commencing training.')
		while True:
			print(f'Iteration {iteration}')
			run_operation(dry_training_wav, wet_training_wav, batch_size, minimize, session)
			losses = run_operation(dry_validation_wav, wet_validation_wav, batch_size, loss, session)
			validation_loss = sum(losses)
			print(f'Validation: {validation_loss}')
			iteration += 1

if len(sys.argv) != 5:
	print('Usage:')
	print(f'{sys.argv[0]} <dry training WAV file> <wet training WAV file> <dry validation WAV file> <wet validation WAV file>')
	sys.exit(1)

dry_training_wav_path = sys.argv[1]
wet_training_wav_path = sys.argv[2]

dry_validation_wav_path = sys.argv[3]
wet_validation_wav_path = sys.argv[4]

dry_training_wav = read_wav(dry_training_wav_path)
wet_training_wav = read_wav(wet_training_wav_path)

dry_validation_wav = read_wav(dry_validation_wav_path)
wet_validation_wav = read_wav(wet_validation_wav_path)

if get_length(dry_training_wav) != get_length(wet_training_wav) or get_length(dry_validation_wav) != get_length(wet_validation_wav):
	raise Exception('Dry and wet WAVs must be same length.')

train(dry_training_wav, wet_training_wav, dry_validation_wav, wet_validation_wav)
```"
22997,Undefined reference to stream_executor::Stream::ThenBlasGemm in _gru_ops.so,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Adjustments for build
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux <HOST> 2.6.32-573.7.1.el6.centos.plus.x86_64 #1 SMP Wed Sep 23 03:02:55 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: source git rev: ac7b84de8803edbb2d4da573b3f8704e9fad8fa8
- **TensorFlow version (use command below)**: b'v1.9.0-rc2-5328-gac7b84d' 1.11.0-rc1
- **Python version**: Python 3.6.6 :: Anaconda, Inc.
- **Bazel version (if compiling from source)**: bazel-0.17.1
- **GCC/Compiler version (if compiling from source)**: gcc version 4.9.3 (GCC)
- **CUDA/cuDNN version**: cuda-9.0, libcudnn.so.7.3.1
- **GPU model and memory**:
- **Exact command to reproduce**: `python -c ""import tensorflow.contrib`



### Describe the problem

Problem seems similar to the now closed #19840

#### Modifications to Tensorflow source
From the checked out source I made the following modifications:

- added `-lrt` to linkopts  in `tensorflow.bzl` for compatibility with CentOS following https://github.com/tensorflow/tensorflow/issues/15129

- Added `use_default_shell_env = True to `ctx.action()` in  in `tensorflow.bzl` for problems with swig following https://github.com/aiqu/devsetting/commit/f927890e05e06382ef1606decf38f192a75bac71

- compiled with --monolithic because of an issue with `load_library` throwing a `ByteSizeConsistencyError` when serializing `_bigtable.so` perhaps among others when running on my compute cluster vs. workstation where tensorflow is compiled.

- changed the paths for `gcc` to point to `v4.9` installed in `~/opt` in various `CROSSTOOL` config files.

Here is a [gist](https://gist.github.com/momeara/2edb2d606ea9490d3bbb358ed2b57855) of the diffs.


#### Set up the environment
```
export CC=/mnt/nfs/home/momeara/opt/bin/gcc
export CXX=/mnt/nfs/home/momeara/opt/bin/g++
export PYTHON_BIN_PATH=/nfs/ex9/work/momeara/tools/anaconda3/envs/DeepSEA/bin/python
export PYTHON_LIB_PATH=/nfs/ex9/work/momeara/tools/anaconda3/envs/DeepSEA/lib/python3.6/site-packages
export TF_NEED_AWS=0
export TF_NEED_KAFKA=0
export TF_NEED_NGRAPH=0
export TF_NEED_IGNITE=0
export TF_NEED_ROCM=0
export TF_NEED_TENSORRT=0
export TF_NEED_JEMALLOC=0
export TF_NEED_GCP=0
export TF_NEED_HDFS=0
export TF_NEED_S3=0
export TF_ENABLE_XLA=0
export TF_NEED_GDR=0
export TF_NEED_VERBS=0
export TF_NEED_OPENCL=0
export TF_NEED_OPENCL_SYCL=0
export TF_NCCL_VERSION=""2.3""
export NCCL_INSTALL_PATH=/mnt/nfs/ex9/work/momeara/collaborations/DeepSEA/nccl/nccl_2.3.5-2+cuda9.0_x86_64
export TF_SET_ANDROID_WORKSPACE=0
export TF_NEED_CUDA=1
export TF_CUDA_VERSION=10.0
export CUDA_TOOLKIT_PATH=/nfs/soft/cuda-10.0
export CUDA_HOME=/nfs/soft/cuda-10.0
export TF_CUDNN_VERSION=7.3.1
export CUDNN_INSTALL_PATH=/mnt/nfs/ex9/work/momeara/collaborations/DeepSEA/cuda
export TF_CUDA_COMPUTE_CAPABILITIES=5.2
export TF_CUDA_CLANG=0
export TF_NEED_MPI=0
export GCC_HOST_COMPILER_PATH=/mnt/nfs/home/momeara/opt/bin/gcc
export CC_OPT_FLAGS='-march=native'
# path to /nfs/soft is for /nfs/soft/cuda/include/cudnn.h
export CPLUS_INCLUDE_PATH=/mnt/nfs/home/momeara/opt/include:/nfs/soft:/nfs/soft/cuda/include
export C_INCLUDE_PATH=/mnt/nfs/home/momeara/opt/include:/nfs/soft:/nfs/soft/cuda/include
export LIBRARY_PATH=/nfs/soft/cuda-10.0/lib64:/nfs/soft/cuda-10.0/extras/CUPTI/lib64:/mnt/nfs/home/momeara/opt/lib:/mnt/nfs/home/momeara/opt/lib64:/nfs/soft/cuda/nfs/soft/cuda/lib64
export LD_LIBRARY_PATH=/nfs/soft/cuda-10.0/lib64:/nfs/ex9/work/momeara/collaborations/DeepSEA/cuda/lib64:/nfs/soft/cuda-10.0/extras/CUPTI/lib64:/mnt/nfs/home/momeara/opt/lib:/mnt/nfs/home/momeara/opt/lib64:/nfs/soft/cuda/nfs/soft/cuda/lib64
export PATH=/nfs/soft/cuda-10.0/bin:/mnt/nfs/ex9/work/momeara/collaborations/DeepSEA/tensorflow/bazel-0.17.1/output:/nfs/home/momeara/opt/ncbi-blast-2.2.30+/bin:/nfs/ex9/work/momeara/tools/anaconda3/envs/DeepSEA/bin:/nfs/ex9/work/momeara/tools/anaconda3/bin:/mnt/nfs/home/momeara/.local/bin:/mnt/nfs/home/momeara/opt/node-v4.5.0-linux-x64/bin:/mnt/nfs/home/momeara/opt/bin:/usr/lib64/qt-3.3/bin:/usr/kerberos/sbin:/usr/kerberos/bin:/usr/local/bin:/bin:/usr/bin
```


#### Build and install `tensorflow`
```
./configure
bazel \
  --output_user_root=/nfs/ex9/work/momeara/collaborations/DeepSEA/tensorflow/.cache \
  build \
  --config=cuda \
  --config=opt \
  --config=monolithic \
  //tensorflow/tools/pip_package:build_pip_package

./bazel-bin/tensorflow/tools/pip_package/build_pip_package \
  --src /scratch/momeara/tensorflow_pkg_dbg_monolithic_src \
  /scratch/momeara/tensorflow_pkg_dbg_monolithic
pip install --upgrade /scratch/momeara/tensorflow_pkg_dbg_monolithic/tensorflow-*.whl

python -c ""import tensorflow.contrib""
```
gives this error (see below for full backtrace)

```
tensorflow.python.framework.errors_impl.NotFoundError: /nfs/ex9/work/momeara/tools/anaconda3/envs/DeepSEA/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/_gru_ops.so: undefined symbol: _ZN15stream_executor6Stream12ThenBlasGemmENS_4blas9TransposeES2_yyyfRKNS_12DeviceMemoryIfEEiS6_ifPS4_i
```

#### Differential testing
- The same problem only arises with the `--config=monolithic` flags.
- Including debug flags `-c dbg --copt=-DNDEBUG --strip=never` makes no difference.

### Source code / logs

```
python -c ""import tensorflow.contrib""
# ...generic warnings...
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/nfs/ex9/work/momeara/tools/anaconda3/envs/DeepSEA/lib/python3.6/site-packages/tensorflow/contrib/__init__.py"", line 45, in <module>
    from tensorflow.contrib import cudnn_rnn
  File ""/nfs/ex9/work/momeara/tools/anaconda3/envs/DeepSEA/lib/python3.6/site-packages/tensorflow/contrib/cudnn_rnn/__init__.py"", line 34, in <module>
    from tensorflow.contrib.cudnn_rnn.python.layers import *
  File ""/nfs/ex9/work/momeara/tools/anaconda3/envs/DeepSEA/lib/python3.6/site-packages/tensorflow/contrib/cudnn_rnn/python/layers/__init__.py"", line 23, in <module>
    from tensorflow.contrib.cudnn_rnn.python.layers.cudnn_rnn import *
  File ""/nfs/ex9/work/momeara/tools/anaconda3/envs/DeepSEA/lib/python3.6/site-packages/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py"", line 20, in <module>
    from tensorflow.contrib.cudnn_rnn.python.ops import cudnn_rnn_ops
  File ""/nfs/ex9/work/momeara/tools/anaconda3/envs/DeepSEA/lib/python3.6/site-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py"", line 22, in <module>
    from tensorflow.contrib.rnn.python.ops import lstm_ops
  File ""/nfs/ex9/work/momeara/tools/anaconda3/envs/DeepSEA/lib/python3.6/site-packages/tensorflow/contrib/rnn/__init__.py"", line 92, in <module>
    from tensorflow.contrib.rnn.python.ops.gru_ops import *
  File ""/nfs/ex9/work/momeara/tools/anaconda3/envs/DeepSEA/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/gru_ops.py"", line 33, in <module>
    resource_loader.get_path_to_datafile(""_gru_ops.so""))
  File ""/nfs/ex9/work/momeara/tools/anaconda3/envs/DeepSEA/lib/python3.6/site-packages/tensorflow/contrib/util/loader.py"", line 56, in load_op_library
    ret = load_library.load_op_library(path)
  File ""/nfs/ex9/work/momeara/tools/anaconda3/envs/DeepSEA/lib/python3.6/site-packages/tensorflow/python/framework/load_library.py"", line 60, in load_op_library
    lib_handle = py_tf.TF_LoadLibrary(library_filename)
tensorflow.python.framework.errors_impl.NotFoundError: /nfs/ex9/work/momeara/tools/anaconda3/envs/DeepSEA/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/_gru_ops.so: undefined symbol: _ZN15stream_executor6Stream12ThenBlasGemmENS_4blas9TransposeES2_yyyfRKNS_12DeviceMemoryIfEEiS6_ifPS4_i
```

For what it's worth here the symbols in `_gru_ops.so` missing containing `stream_executor`:

```
nm -C -u /nfs/ex9/work/momeara/tools/anaconda3/envs/DeepSEA/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/_gru_ops.so | grep stream_executor                                                                                                
                 U stream_executor::Stream::ThenBlasGemm(stream_executor::blas::Transpose, stream_executor::blas::Transpose, unsigned long long, unsigned long long, unsigned long long, double, stream_executor::DeviceMemory<double> const&, int, stream_executor::DeviceMemory<double> const&, int, double, stream_executor::DeviceMemory<double>*, int)
                 U stream_executor::Stream::ThenBlasGemm(stream_executor::blas::Transpose, stream_executor::blas::Transpose, unsigned long long, unsigned long long, unsigned long long, float, stream_executor::DeviceMemory<float> const&, int, stream_executor::DeviceMemory<float> const&, int, float, stream_executor::DeviceMemory<float>*, int)
```
"
22996,tensboard not working on google colab   it shows a error :-No dashboards are active for the current data set.,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22995,Model is still floating point after Post-training quantization ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Android 7
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
Samsung Galaxy S6
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.11.0 on PC, 'org.tensorflow:tensorflow-lite:0.0.0-nightly' on Android 
- **Python version**: 2.7.15
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**: tfLite.run(imgData, labelProb);

### Describe the problem
After applying post-training quantization, my custom CNN model was shrinked to 1/4 of its original size (from 56.1MB to 14MB). I put the image(100x100x3) that is to be predicted into ByteBuffer as 100x100x3=30,000 bytes. However, I got the following error during inference:

```
**java.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 120000 bytes and a ByteBuffer with 30000 bytes.**
        at org.tensorflow.lite.Tensor.throwExceptionIfTypeIsIncompatible(Tensor.java:221)
        at org.tensorflow.lite.Tensor.setTo(Tensor.java:93)
        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:136)
        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:216)
        at org.tensorflow.lite.Interpreter.run(Interpreter.java:195)
        at gov.nih.nlm.malaria_screener.imageProcessing.TFClassifier_Lite.recongnize(TFClassifier_Lite.java:102)
        at gov.nih.nlm.malaria_screener.imageProcessing.TFClassifier_Lite.process_by_batch(TFClassifier_Lite.java:145)
        at gov.nih.nlm.malaria_screener.Cells.runCells(Cells.java:269)
        at gov.nih.nlm.malaria_screener.CameraActivity.ProcessThinSmearImage(CameraActivity.java:1020)
        at gov.nih.nlm.malaria_screener.CameraActivity.access$600(CameraActivity.java:75)
        at gov.nih.nlm.malaria_screener.CameraActivity$8.run(CameraActivity.java:810)
        at java.lang.Thread.run(Thread.java:762) 
```

The imput image size to the model is: 100x100x3. I'm currently predicting one image at a time. So, if I'm making the Bytebuffer: 100x100x3 = 30,000 bytes. However, the log info above says the TensorFlowLite buffer has 120,000 bytes. This makes me suspect that the converted tflite model is still in float format. Is this expected behavior? How can I get a quantized model that take input image in 8 pit precision like it does in the [example](https://github.com/tensorflow/tensorflow/blob/307c83106445ab2c52847f08d35a66c51aff19d9/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java#L163) from TensorFlow official repository ?

In the example code, the ByteBuffer used as input for tflite.run() is in 8 bit precision for the quantized model. 
But I also read from the google doc saying, ""At inference, weights are converted from 8-bits of precision to floating-point and computed using floating point kernels."" This two instances seems to contradict each other. 

### Source code / logs

```
private static final int BATCH_SIZE = 1;

private static final int DIM_IMG_SIZE = 100;

private static final int DIM_PIXEL_SIZE = 3;

private static final int BYTE_NUM = 1;

imgData = ByteBuffer.allocateDirect(BYTE_NUM * BATCH_SIZE * DIM_IMG_SIZE * DIM_IMG_SIZE * DIM_PIXEL_SIZE);
imgData.order(ByteOrder.nativeOrder());

... ...

int pixel = 0;

        for (int i = 0; i < DIM_IMG_SIZE; ++i) {
            for (int j = 0; j < DIM_IMG_SIZE; ++j) {

                final int val = intValues[pixel++];

                imgData.put((byte)((val >> 16) & 0xFF));
                imgData.put((byte)((val >> 8) & 0xFF));
                imgData.put((byte)(val & 0xFF));

//                imgData.putFloat(((val >> 16) & 0xFF) / 255.0f);
//                imgData.putFloat(((val >> 8) & 0xFF) / 255.0f);
//                imgData.putFloat((val & 0xFF) / 255.0f);

            }
        }

... ...

tfLite.run(imgData, labelProb);
```

Post-training quantization code:
```
import tensorflow as tf
import sys
import os

saved_model_dir = '/home/yuh5/Downloads/malaria_thinsmear.h5.pb'

input_arrays = [""input_2""]

output_arrays = [""output_node0""]

converter = tf.contrib.lite.TocoConverter.from_frozen_graph(saved_model_dir, input_arrays, output_arrays)

converter.post_training_quantize = True

tflite_model = converter.convert()
open(""thinSmear_100.tflite"", ""wb"").write(tflite_model)
```

"
22994,Tensorflow freeze_graph.py: NodeDef mentions attr 'Truncate' not in Op,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOs High Sierra, MacBook Pro, 3.1 GHz Intel Core i5, 8 GB 2133 MHz LPDDR3, Intel Iris Plus Graphics 650 1536 MB
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: pip install tensorflow==1.10
- **TensorFlow version (use command below)**: v1.10.0-rc1-19-g656e7a2b34 1.10.0
- **Python version**: python 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: 
python3 /Users/duckhahwang/anaconda3/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py --input_graph=/Users/duckhahwang/Downloads/resnet_imagenet_v2_20180928/graph.pbtxt --input_binary=false --input_checkpoint=/Users/duckhahwang/Downloads/resnet_imagenet_v2_20180928/model.ckpt-56286 --input_binary=false --output_graph=/Users/duckhahwang/Downloads/frozen_graph.pb --output_node_names=softmax


You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I downloaded the pre-trained tensorflow model (ResNet-50 v2 (fp32)) from the below link

http://download.tensorflow.org/models/official/20181001_resnet/checkpoints/resnet_imagenet_v2_fp32_20181001.tar.gz

And using freeze_graph.py to freeze the pre-trained model. but I received the error message and not working. 

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

DUCKHAs-MacBook-Pro:site-packages duckhahwang$ python3 /Users/duckhahwang/anaconda3/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py --input_graph=/Users/duckhahwang/Downloads/resnet_imagenet_v2_20180928/graph.pbtxt --input_binary=false --input_checkpoint=/Users/duckhahwang/Downloads/resnet_imagenet_v2_20180928/model.ckpt-56286 --input_binary=false --output_graph=/Users/duckhahwang/Downloads/frozen_graph.pb --output_node_names=softmax

/Users/duckhahwang/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
/Users/duckhahwang/anaconda3/lib/python3.6/site-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.23) or chardet (3.0.4) doesn't match a supported version!
  RequestsDependencyWarning)
Traceback (most recent call last):
  File ""/Users/duckhahwang/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 418, in import_graph_def
    graph._c_graph, serialized, options)  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef mentions attr 'Truncate' not in Op<name=Cast; signature=x:SrcT -> y:DstT; attr=SrcT:type; attr=DstT:type>; NodeDef: resnet_model/conv2d/kernel_cast = Cast[DstT=DT_HALF, SrcT=DT_FLOAT, Truncate=false, _output_shapes=[[7,7,3,64]]](resnet_model/conv2d/kernel_cast/ReadVariableOp). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/duckhahwang/anaconda3/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py"", line 382, in <module>
    run_main()
  File ""/Users/duckhahwang/anaconda3/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py"", line 379, in run_main
    app.run(main=my_main, argv=[sys.argv[0]] + unparsed)
  File ""/Users/duckhahwang/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/Users/duckhahwang/anaconda3/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py"", line 378, in <lambda>
    my_main = lambda unused_args: main(unused_args, flags)
  File ""/Users/duckhahwang/anaconda3/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py"", line 272, in main
    flags.saved_model_tags, checkpoint_version)
  File ""/Users/duckhahwang/anaconda3/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py"", line 254, in freeze_graph
    checkpoint_version=checkpoint_version)
  File ""/Users/duckhahwang/anaconda3/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py"", line 99, in freeze_graph_with_def_protos
    _ = importer.import_graph_def(input_graph_def, name="""")
  File ""/Users/duckhahwang/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 454, in new_func
    return func(*args, **kwargs)
  File ""/Users/duckhahwang/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 422, in import_graph_def
    raise ValueError(str(e))
ValueError: NodeDef mentions attr 'Truncate' not in Op<name=Cast; signature=x:SrcT -> y:DstT; attr=SrcT:type; attr=DstT:type>; NodeDef: resnet_model/conv2d/kernel_cast = Cast[DstT=DT_HALF, SrcT=DT_FLOAT, Truncate=false, _output_shapes=[[7,7,3,64]]](resnet_model/conv2d/kernel_cast/ReadVariableOp). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.)."
22993,TOCO failed see console for info. ,"INFO:
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
TensorFlow installed from (source or binary): Anaconda 
TensorFlow version (use command below): 1.11
Python version: 3.5
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:
Exact command to reproduce:

Hi, 
 I've used the example given in [converter python api guide](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/convert/python_api.md)

import numpy as np
import tensorflow as tf
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(2, input_shape=(3,)))
model.add(tf.keras.layers.RepeatVector(3))
model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(3)))
model.compile(loss=tf.keras.losses.MSE,
              optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),
              metrics=[tf.keras.metrics.categorical_accuracy],
              sample_weight_mode='temporal')
x = np.random.random((1, 3))
y = np.random.random((1, 3, 3))
model.train_on_batch(x, y)
model.predict(x)
keras_file = ""keras_model.h5""
tf.keras.models.save_model(model, keras_file)
converter = tf.contrib.lite.TFLiteConverter.from_keras_model_file(keras_file)
tflite_model = converter.convert()`


I ran this code and this is the error I get:

> RuntimeError: TOCO failed see console for info.
b'Traceback (most recent call last):\r\n  File ""C:\\Users\\sgavvala\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\contrib\\lite\\toco\\python\\tensorflow_wrap_toco.py"", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module(\'_tensorflow_wrap_toco\', [dirname(__file__)])\r\n  File ""c:\\users\\sgavvala\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\imp.py"", line 297, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named \'_tensorflow_wrap_toco\'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File ""c:\\users\\sgavvala\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\runpy.py"", line 193, in _run_module_as_main\r\n    ""__main__"", mod_spec)\r\n  File ""c:\\users\\sgavvala\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\runpy.py"", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File ""C:\\Users\\sgavvala\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\Scripts\\toco_from_protos.exe\\__main__.py"", line 5, in <module>\r\n  File ""C:\\Users\\sgavvala\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\contrib\\lite\\toco\\python\\toco_from_protos.py"", line 22, in <module>\r\n    from tensorflow.contrib.lite.toco.python import tensorflow_wrap_toco\r\n  File ""C:\\Users\\sgavvala\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\contrib\\lite\\toco\\python\\tensorflow_wrap_toco.py"", line 28, in <module>\r\n    _tensorflow_wrap_toco = swig_import_helper()\r\n  File ""C:\\Users\\sgavvala\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\contrib\\lite\\toco\\python\\tensorflow_wrap_toco.py"", line 20, in swig_import_helper\r\n    import _tensorflow_wrap_toco\r\nImportError: No module named \'_tensorflow_wrap_toco\'\r\n'
None

My plan is to convert a keras model with my custom layers into tflite quantized version. Figured I would start with a given example but that doesn't seem to run."
22992,Deployment Guides,"It would be helpful to have a guide that describes Tensorflow Serving Deployment best practices.For example:

1. Run time environment scaling.
2. Best practices for maintaining a native installation of Tensorflow Serving
3. Best practices for maintaining Tensorflow Serving in a Docker environment.
4. Best practices for maintaining a model library for both a native installation and a Docker image."
22991,tf.contrib.feature_column.sequence_numeric_column normalize_fn argument not working,"I am using tf.contrib.feature_column.sequence_numeric_column to normalize a sparse tensor through normalize_fn. Here is a minimal example of the error that I get:
```
import tensorflow as tf
import numpy as np
a = np.reshape(np.arange(12), (3, 4, 1))
with tf.Session() as sess:
    a_t = tf.constant(a)
    idx = tf.where(tf.not_equal(a_t, 0))
    sparse = tf.SparseTensor(idx, tf.gather_nd(a_t, idx), a_t.get_shape())
    seq_cols = tf.contrib.feature_column.sequence_numeric_column('test', default_value=.0, normalizer_fn=lambda x: (x - 2.) / .3)
    seq_features, sequence_length = tf.contrib.feature_column.sequence_input_layer({'test': sparse}, [seq_cols])

    b = sess.run(seq_features)
```
Rises:

```
Traceback (most recent call last):
  File ""/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2878, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-12-137e79726dfc>"", line 9, in <module>
    seq_features, sequence_length = tf.contrib.feature_column.sequence_input_layer({'test': sparse}, [seq_cols])
  File ""/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/feature_column/python/feature_column/sequence_feature_column.py"", line 120, in sequence_input_layer
    trainable=trainable)
  File ""/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/feature_column/python/feature_column/sequence_feature_column.py"", line 448, in _get_sequence_dense_tensor
    sp_tensor = inputs.get(self)
  File ""/anaconda2/lib/python2.7/site-packages/tensorflow/python/feature_column/feature_column.py"", line 2263, in get
    transformed = column._transform_feature(self)  # pylint: disable=protected-access
  File ""/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/feature_column/python/feature_column/sequence_feature_column.py"", line 435, in _transform_feature
    input_tensor = self.normalizer_fn(input_tensor)
  File ""<ipython-input-12-137e79726dfc>"", line 8, in <lambda>
    seq_cols = tf.contrib.feature_column.sequence_numeric_column('test', default_value=.0, normalizer_fn=lambda x: (x - 2.) / .3)
TypeError: unsupported operand type(s) for -: 'SparseTensor' and 'float'
```
Note that tf.contrib.feature_column.sequence_numeric_column is actually meant for sparse tensors so moving to dense is not an option here.

I am running tf 1.11.0 on python2.7"
22988,2x slower using post-training quantization from tensorflow-lite,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:  `pip install -U tf-nightly`
- **TensorFlow version (use command below)**: `'1.12.0-dev20181012'`
- **Python version**: 2.7.12

### Describe the problem
I read and run the post-training quantization code from official tensorflow Medium (https://medium.com/tensorflow/introducing-the-model-optimization-toolkit-for-tensorflow-254aca1ba0a3). 
The post said the technique can result in up to 3x faster execution for relevant machine learning models. But I found during inference is 2x slower. 

### Source code / logs
I run the Colab notebook (https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/tutorials/post_training_quant.ipynb) and compare inference time before and after quantization from the *Evaluate the models* section. 

The log before quantization: 
```
Accuracy after 500 images: 0.976000
Accuracy after 1000 images: 0.966000
Accuracy after 1500 images: 0.958667
Accuracy after 2000 images: 0.956500
Accuracy after 2500 images: 0.952400
Accuracy after 3000 images: 0.956667
Accuracy after 3500 images: 0.957429
Accuracy after 4000 images: 0.955000
Accuracy after 4500 images: 0.955333
Accuracy after 5000 images: 0.954800
Accuracy after 5500 images: 0.958182
Accuracy after 6000 images: 0.959000
Accuracy after 6500 images: 0.960462
Accuracy after 7000 images: 0.961571
Accuracy after 7500 images: 0.963600
Accuracy after 8000 images: 0.965250
Accuracy after 8500 images: 0.966471
Accuracy after 9000 images: 0.968222
Accuracy after 9500 images: 0.969158
Accuracy after 10000 images: 0.968000
0.968
42.6 sec
```

The log after quantization: 
```
Accuracy after 500 images: 0.976000
Accuracy after 1000 images: 0.967000
Accuracy after 1500 images: 0.959333
Accuracy after 2000 images: 0.957000
Accuracy after 2500 images: 0.952800
Accuracy after 3000 images: 0.957000
Accuracy after 3500 images: 0.957714
Accuracy after 4000 images: 0.955500
Accuracy after 4500 images: 0.955778
Accuracy after 5000 images: 0.955200
Accuracy after 5500 images: 0.958545
Accuracy after 6000 images: 0.959333
Accuracy after 6500 images: 0.960769
Accuracy after 7000 images: 0.961857
Accuracy after 7500 images: 0.963867
Accuracy after 8000 images: 0.965500
Accuracy after 8500 images: 0.966706
Accuracy after 9000 images: 0.968444
Accuracy after 9500 images: 0.969368
Accuracy after 10000 images: 0.968300
0.9683
74.5 sec
```

The accuracy is basically the same, but it's much slower. "
22987,Segmentation Fault (SIGSEGV) in middle of Training due to Runtime-statistics calculation ops.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Custom code. The full code can be accessed [here at GitHub](https://github.com/grasseau/HAhRD/tree/master/GSOC18)
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
CentOS Linux release 7.5.1804 (Core)
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
('v1.8.0-0-g93bc2e2072', '1.8.0')
- **Python version**:
Python 2.7.5 (default, Apr 11 2018, 07:36:10)
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
cuda-9.0-cudnn-7
- **GPU model and memory**:
2 Tesla V100 Nvidia GPUs, ~15 Gb each
- **Exact command to reproduce**:
Please go through this instruction [here](https://github.com/BigBang0072/HAhRD/wiki/GSOC-18-Work-Summary) to run the code.
The training runs without error on small dataset. The problem arises when number of minibatches are
more per epoch keeping the batch size constant. (i.e more number of sess.run call per epoch)

### Describe the problem
The training crashes with the error **Segmentation Fault (Core Dumped) in the middle of the training after around 14-18 epochs (with ~800 minibatches in each)** even though I have sufficient RAM (~12-20% utilization holding steadily before crash).

![screenshot from 2018-10-14 16-46-38](https://user-images.githubusercontent.com/17550410/46916113-c4b44e00-cfd3-11e8-842b-686a42b2c486.png)

**Possible Memory Leak Checked**
Initially, I suspected a memory leak due to the addition of new ops after each iteration, so I finalized the graph in the training session. But this was not the source of the problem. No new nodes were added at each iteration.

**Attempt 2: Locating the problem**
After that, I ran the training on gdb (see the stack trace in logs section) and along with some commenting of the code I have almost pinpointed the source of error.  
Currently every 30 minibatch training I am saving the summary here in the code. The full code can be found [here](https://github.com/grasseau/HAhRD/blob/master/GSOC18/train_multi_gpu.py#L406)
![screenshot from 2018-10-14 17-13-59](https://user-images.githubusercontent.com/17550410/46916189-c29ebf00-cfd4-11e8-9a94-08f67c12198d.png)


The train_track_op in line number 415 is a list of which looks like this:  
[gradient_update_op, loss_GPU1, loss_GPU2, merged_summary_op]

If I comment out the section in lines 406-438 the training runs without error.

**Attempt 3: Exact Location** 
Now I comment out line 422 to 438,(the part where I save timeline and summary). I have checked there is no problem due to these lines.

Now if, I run merged_summary op along with rest of the training op and **comment out line 418 and 419,
i.e removing the run_options and run_metadata, the training goes without error**. And if I leave these two lines uncommented the Segfault comes back.  
![screenshot from 2018-10-15 17-41-41](https://user-images.githubusercontent.com/17550410/46951781-93588280-d0a6-11e8-8173-f52dfce1c3c6.png)

**So, It seems some memory is being leaked when calculating the runtime statistics using  run_options and run_metadata when doing the full trace of the graph.**


### Source code / logs
**Stack Trace**

Program received signal SIGSEGV, Segmentation fault.
[Switching to Thread 0x7ff2d3fff700 (LWP 138819)]
0x00007ff2b8b00d6f in ?? () from /usr/local/cuda-9.0/extras/CUPTI/lib64/libcupti.so.9.0

(gdb) backtrace
#0  0x00007ff2b8b00d6f in ?? () from /usr/local/cuda-9.0/extras/CUPTI/lib64/libcupti.so.9.0
#1  0x00007ff2b8b04fd0 in ?? () from /usr/local/cuda-9.0/extras/CUPTI/lib64/libcupti.so.9.0
#2  0x00007ff2b8d10c39 in ?? () from /usr/local/cuda-9.0/extras/CUPTI/lib64/libcupti.so.9.0
#3  0x00007ffff77fae25 in start_thread () from /lib64/libpthread.so.0
#4  0x00007ffff6e1bbad in clone () from /lib64/libc.so.6

"
22986,tensorflow keras random seeds not working,"Hello,
I am using Tensorflow 1.5 and its embedded Keras on CPU.
The following simple program does not produce the same results at every run despite setting the random seeds for keras and TF

    import numpy as np
    np.random.seed(123)

    import tensorflow as tf
    tf.set_random_seed(123)
    from tensorflow.python.keras.models import Sequential
    from tensorflow.python.keras.layers import Dense


    def create_model(input_dim, output_dim, training=True):
        from tensorflow.python.keras.models import Sequential
        from tensorflow.python.keras.layers import Dense
        hidden_dim = 8 
        model = Sequential()
        model.add(Dense(input_dim=input_dim, units=hidden_dim))
        model.add(Dense(units=output_dim, activation='linear'))
        model.compile(loss='mse', optimizer='adam')
        return model

    N = 100000
    X = np.random.normal(loc=0.0, scale=1.0, size=(N, 20))
    Y = np.random.normal(loc=0.0, scale=1.0, size=(N, 2)) 
    input_dim = X.shape[1]
    output_dim = Y.shape[1]

    tf.reset_default_graph()
    tf.set_random_seed(123)
    model = create_model(input_dim, output_dim)
    model.fit(X, Y, batch_size=100, epochs=1, verbose=1)

    Y_pred = model.predict(X, batch_size=1)

    print Y_pred[0:5]
"
22985,Difference in implementation of batch normalization between theano and tensorflow,"Dear Sir: 
  I have a misunderstanding in the process of batch normalization. e.g.
  The out put of convolution layer 
  [[[102. 138.]
  [246. 282.]]]
  [[[ 507.  624.]
  [ 975. 1092.]]]
  [[[ 912. 1110.]
  [1704. 1902.]]]
  I set the following config of batch norm
  gamma  = [1,2,3]
  beta = [0,0,0]
  mean = [2,3,4]
  Inv = [1/3,1/4,1/5]


  Norm.append(gamma)
  Norm.append(beta)
  Norm.append(mean)
  Norm.append(inv)
 

  model.add(tf.keras.layers.BatchNormalization( momentum=1.0, epsilon=0,center = True,scale = True,trainable=False,renorm_momentum=1.0))
  model.layers[1].set_weights(Norm)
   
   According to the definition of batch_norm , (x - mean/ inv)*gamma + beta, it should generate： 
   
  [[[[   300.    408.]
   [   732.    840.]]

  [[  4032.   4968.]
   [  7776.   8712.]]

  [[ 13620.  16590.]
   [ 25500.  28470.]]]]
  
   However, the final result is
   [[[173.20248 235.55537]
  [422.61404 484.96695]]]
[[[2015.9596 2483.9502]
  [3887.922  4355.9126]]]
[[[ 6090.8965  7419.088 ]
  [11403.661  12731.853 ]]]

  Could anyone tell me what is wrong and how could I get the correct answer?

Best Regards
Allen
"
22983,Feature request: reduce_random,"Just as `tf.reduce_max`, `tf.reduce_random` is to randomly return the elements across dimensions of a tensor with some probabilities, such as respect to the value of the tensor at those dimensions.

Or any other ways to implement it?"
22982,lose some early epoch checkpoint file during training,"Have I written custom code: Y
OS Platform and Distribution: centos release 6.5
TensorFlow installed from: pip install
TensorFlow version: 1.40
Bazel version: N/A
CUDA/cuDNN version: Cuda 8.0
GPU model and memory: N/A
Exact command to reproduce: N/A
Mobile device: N/A
I found myself losing some checkpoint in the target dir after training. For example, I lose 1-13 epoch checkpoint files after 60-epoch training.
I used tf.train.Saver and tf.train.Supervisor in my code:
`tf.train.Supervisor(
            is_chief=is_chief,
            logdir=FLAGS.log_dir,
            global_step=global_step,
            saver=tf.train.Saver(var_list=var_list,max_to_keep=80,sharded=True),
            save_model_secs=FLAGS.save_model_secs,
            save_summaries_secs=FLAGS.save_summary_secs,
            init_op=tf.global_variables_initializer(),
            local_init_op=tf.group(tf.local_variables_initializer(),tf.tables_initializer())`
where save_model_secs=1800, save_summary_secs=1800
FYI, it costs me about 30 minutes for training 1 epoch.WHY?"
22981,"def TFE_ContextOptionsSetAsync(arg1, async):                                              ","Problem with installing tensorflow in Python 3.7.

Referred to [#20444](https://github.com/tensorflow/tensorflow/issues/20444) but to no avail. :( "
22980,Unable to build libtensorflow_cc.so,"I'm trying to build libtensorflow_cc.so so I can utilize the C++ API of TensorFlow in my own project. But when I try to do that using the following command:

`bazel build -c opt //tensorflow:libtensorflow_cc.so`

I eventually get the error:

```
2 errors detected in the compilation of ""/tmp/tmpxft_000056f1_00000000-7_spacetobatch_functor_gpu.cu.cpp1.ii"".
ERROR: /home.net/aa17gil/tensorflow_from_source/tensorflow/tensorflow/core/kernels/BUILD:3825:1: output 'tensorflow/core/kernels/_objs/batch_space_ops_gpu/spacetobatch_functor_gpu.cu.pic.o' was not created
ERROR: /home.net/aa17gil/tensorflow_from_source/tensorflow/tensorflow/core/kernels/BUILD:3825:1: not all outputs were created or valid
Target //tensorflow:libtensorflow_cc.so failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1219.021s, Critical Path: 81.53s
INFO: 4992 processes: 4992 local.
FAILED: Build did NOT complete successfully

```

As stated in other issues, there is practically no official documentation or guide for how to build this library. I did go through many older issues, but the solutions proposed there don't work for me.

------------------------

### System information
- **Have I written custom code: No**:
- **OS Platform and Distribution: Ubuntu 14.04**:
- **Mobile device: N/A**:
- **TensorFlow installed from source**:
- **TensorFlow version: cloned from master branch on 10.10.2018**:
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.17.2
- **GCC/Compiler version (if compiling from source)**: GCC 4.8.4
- **CUDA/cuDNN version**: CUDA 8.0, CuDNN 6.0.21
- **GPU model and memory**: Nvidia GTX1050Ti
- **Exact command to reproduce**: `bazel build -c opt //tensorflow:libtensorflow_cc.so`"
22979,"""Unknwon layer"" error while using custom Keras layer with tf.contrib.distribute.MirroredStrategy","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes. See STLSTM.py in https://gist.github.com/kami93/c49f0cdd19b318f3e2699df13971f7b1
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04.1
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
 N/A
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.11.0
- **Python version**: Anaconda Python 3.6.6
- **Bazel version (if compiling from source)**: 0.17.2
- **GCC/Compiler version (if compiling from source)**: 7.3.0
- **CUDA/cuDNN version**:9.2
- **GPU model and memory**:Gefroce 1080Ti * 4 (4-way GPU)
- **Exact command to reproduce**: Run predRNN.py in https://gist.github.com/kami93/c49f0cdd19b318f3e2699df13971f7b1

### Describe the problem
Hi. I am getting an ""Unknown layer"" error while running a custom keras layer with tf.contrib.distribute.MirroredStrategy turned on. The trackback is in the Source code / logs section.

The error is not raised when I omit the ""distribute"" argument from model compile method call (line 75 of the predRNN.py) so that the training runs without distribution strategy and only a single GPU works.

Breif explanation on predRNN.py:
predRNN.py builds input pipeline, makes, compiles and fits the Keras sequential model which includes the STLSTM2D custom layer.
The training data can be downloaded from http://www.cs.toronto.edu/~nitish/unsupervised_video/ (Moving MNIST dataset)

Breif explanation on STLSTM.py:
STLSTM is a sophisticated convolutional RNN model suggested from PredRNN (Wang et al. 2017, https://papers.nips.cc/paper/6689-predrnn-recurrent-neural-networks-for-predictive-learning-using-spatiotemporal-lstms).

STLSTM.py contains three classes - STLSTM2D, StackedSTLSTMCells, STLSTMCell which inherit from the Keras RNN class(in tensorflow/python/keras/layers/recurrent.py), Keras Layer class, and Keras Layer class, repectively. 

- STLSTM2D  is based on ConvRNN2D class in tensorflow/python/keras/layers/convolutional_recurrent.py. It forms a Keras RNN layer that performs recurrent functions with the STLSTM Cell.

- StackedSTLSTMCells is a wrapper to make a list of multiple STLSTMCells look like a single cell.

- STLSTMCell is based on ConvLSTM2DCell in tensorflow/python/keras/layers/convolutional_recurrent.py. The modification is performed such that STLSTM has an additional hidden state (The M-state) and different operation procedure compared to the ConvLSTM2D.

### Source code / logs
Source code : https://gist.github.com/kami93/c49f0cdd19b318f3e2699df13971f7b1
error traceback : https://pastebin.com/6dPKZrzX"
22978,fatal error: absl/strings/string_view.h: No such file or directory,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22977,gradients of tf.fake_quant_with_min_max_vars function.,"### System information
- **OS Platform and Distribution : Linux Ubuntu 16.04**
- **TensorFlow version : 1.6.0**
- **Python version : 3.6**
- **CUDA/cuDNN version : CUDA Version 9.0.176/ CUDNN 7.0.5**:
- **GPU model and memory : TITAN Xp/12196MiB**

### Describe the problem
I assume that **tf.fake_quant_with_min_max_vars function** can not be differentiable due to the fact that quantization should be working based on threshold, such as round, or sign function. And it means that we can not get the gradient of variables due to the nature of chain rule.

In these days, several tricks are researched, and one of the most popular method is called **'straight-through-estimator'**, literally passing through the gradient itself.

So I tested **tf.fake_quant_with_min_max_vars** to apply **'straight-through-estimator'** functionality. Most results get a gradient 1. This means it works well. Sometimes, however, the result is incorrect. More detailed errors are described below.

### Source code / logs
Here's some snippet code
`x = tf.cast(np.random.normal(0, 1, (10), tf.float32)`
`x_q = tf.fake_quant_with_min_max_vars(x, min=tf.reduce_min(x), max=tf.reduce_max(x), num_bits=3)`
`grad = tf.gradients(x_q, x)`

In that case, sometimes **grad** get weird results:
`[array([1., 1., 1., 0., 1., 1., 2., 1., 1., 1.], dtype=float32)]`
And non-zero gradient values have a common quantization value. In this case, the **x_q** is:
`array([0., 0., 0., -2.289673, 2.289673, 0., -2.289673, 0., 4.570346, 0.], dtype=float32)`"
22976,ValueError: Unknown activation function:relu6,"tf.keras.layers.Activation(tf.nn.relu6)(x)

when i transform my model to tflite, this bug will happen"
22975,Intermittent very long latency in XRT operations,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: mater
- **Python version**: N/A
- **Bazel version (if compiling from source)**: 0.16.1
- **GCC/Compiler version (if compiling from source)**: 7.2.0
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem

On occasion, I've seen XRT operations take significantly longer than I'd expect and then they usually do (into the 10s of seconds). Attaching GDB while this is happening reveals that it is spending most of its time in TF Graph level optimization passes, particularly EncapsulateXlaComputations (which don't really make any sense in conjunction with XRT ops). I don't have an exact reproducer for when this happens, but it appears that it's more likely to happen upon the first XRTAllocate after reconnecting after a client crashed. I believe, I managed to capture a log  at VLOG level 2 while this was happening (see gist at https://gist.github.com/Keno/3a5e0dc86d3829f5712c5e1a5f65161b). Hopefully that should aid in figuring out what's going on. At the end of the gist, it tried to dump the serialized representation of the ~500MB sized model into the log, so I interrupted it there. I can try to reproduce it again, letting it finish dumping the serialized model if that would be helpful.

cc @michaelisard"
22973,Installation issue with all versions tried,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10 17134.345

- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
N/A
- **TensorFlow installed from (source or binary)**:
Using pip install

- **TensorFlow version (use command below)**:
1.11-GPU
 but error occurred when uninstalled and reinstalled with 1.8-GPU as well (slightly different one, noted below)

- **Python version**:
3.6.0

- **Bazel version (if compiling from source)**:
N/A

- **GCC/Compiler version (if compiling from source)**:
N/A

- **CUDA/cuDNN version**:
CUDA 9.0
cuDNN 7.0.5

- **GPU model and memory**:
NVIDIA GTX 980M 8 GB memory

- **Exact command to reproduce**:
py -c ""import tensorflow as tf; print(tf.__version.__)""


### Describe the problem
I have tried several times to install tensorflow with various versions but get the same DLL error and another issue with one of them

I tried searching for the issue, and found similar ones with no real answer.
One thing I have noticed from looking at those threads is that my ""pywrap_tensorflow_internal"", and perhaps other DLLs, are missing from    ....../site-packages/tensorflow/python.

Also
cudart64_90.dll' is stated to be missing when I use tensorflow-GPU 1.8
and when I check my cuda/bin folder I only see cudart64_7, so that makes sense.

I am not getting this error with 1.11, though, as you may note below.

This error seems simple but is frustrating me, and I would greatly appreciate all help in getting this done quickly, thanks!

### Source code / logs
ERROR WITH 1.11:
C:\WINDOWS\system32>py -c ""import tensorflow as tf;print(tf.__version__)""
Traceback (most recent call last):
  File ""C:\Users\Josh\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Josh\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Josh\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Josh\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Josh\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Users\Josh\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Josh\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Josh\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Josh\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Josh\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Josh\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Josh\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Josh\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.



ERROR WITH 1.8:
C:\WINDOWS\system32>py -c ""import tensorflow as tf;print(tf.__version.__)""
Traceback (most recent call last):
  File ""C:\Users\Josh\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\platform\self_check.py"", line 75, in preload_check
    ctypes.WinDLL(build_info.cudart_dll_name)
  File ""C:\Users\Josh\AppData\Local\Programs\Python\Python36\lib\ctypes\__init__.py"", line 344, in __init__
    self._handle = _dlopen(self._name, mode)
OSError: [WinError 126] The specified module could not be found

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Users\Josh\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Josh\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Josh\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 30, in <module>
    self_check.preload_check()
  File ""C:\Users\Josh\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\platform\self_check.py"", line 82, in preload_check
    % (build_info.cudart_dll_name, build_info.cuda_version_number))
ImportError: Could not find 'cudart64_90.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 9.0 from this URL: https://developer.nvidia.com/cuda-toolkit"
22972,Can't convert Keras model using tflite_convert,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS Mojave
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: Binary (via Pip)
- **TensorFlow version (use command below)**: 1.11.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: n/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: tflite_convert --keras_model_file ./mymodel.h5 --output_file ./mymodel.tflite

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I'm trying to convert a Keras model to TFLite, using the tflite_convert tool. However, I'm getting the below error, using Keras 2.2.4 and Tensorflow 1.11.0. I believe this to be a bug in the tool as this is just a standard H5 file which I've been using in production for months.


### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.


```
$ tflite_convert --keras_model_file ./mymodel.h5 --output_file ./mymodel.tflite
2018-10-14 19:11:59.960314: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Traceback (most recent call last):
  File ""~/anaconda3/envs/tf/bin/tflite_convert"", line 11, in <module>
    sys.exit(main())
  File ""~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 401, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 397, in run_main
    _convert_model(tflite_flags)
  File ""~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 100, in _convert_model
    converter = _get_toco_converter(flags)
  File ""~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 87, in _get_toco_converter
    return converter_fn(**converter_kwargs)
  File ""~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/contrib/lite/python/lite.py"", line 356, in from_keras_model_file
    keras_model = _keras.models.load_model(model_file)
  File ""~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/engine/saving.py"", line 251, in load_model
    training_config['weighted_metrics'])
KeyError: 'weighted_metrics'
```
"
22971,"Compile on debian with cuda 9.1, older CPU, cuda:cuda-extras failed (Segmentation fault): bash failed: error executing command","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux jet 4.18.0-2-amd64 #1 SMP Debian 4.18.10-1 (2018-09-30) x86_64 GNU/Linux
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: v1.11.0
- **Python version**: Python 3.6.7rc1
- **Bazel version (if compiling from source)**: 0.17.2 (compiled)
- **GCC/Compiler version (if compiling from source)**: gcc-4.8
- **CUDA/cuDNN version**: 7.3.1
- **GPU model and memory**: GeForce GTX 1060 6GB – cuda 6.1
- **Exact command to reproduce**: time bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures
- **CPU and RAM**: AMD Phenom(tm) II X6 1090T Processor, 16Gb

### Describe the problem

I have to compile tensorflow myself, because of the AVX CPU extension problem (found the details about that in an [amikelive article](https://tech.amikelive.com/node-887/how-to-resolve-error-illegal-instruction-core-dumped-when-running-import-tensorflow-in-a-python-program/))

It worked compiling and then running tensorflow **without** cuda Support but **compiling with cuda** fails with the following **segmentation fault**.

```
…
ERROR: /home/dedeibel/.cache/bazel/_bazel_dedeibel/216944cd492986130e450b1bf5cd7b5e/external/local_config_cuda/cuda/BUILD:638:1: Executing genrule @local_config_cuda//cuda:cuda-extras failed (Segmentation fault): bash failed: error executing command
…
```

See below for full output.

Commands I ran after the successful compile without cuda.

```
bazel clean
./configure (see below)
time bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures
```

### Source code / logs

```
build --action_env PYTHON_BIN_PATH=""/usr/bin/python3""
build --action_env PYTHON_LIB_PATH=""/usr/local/lib/python3.6/dist-packages""
build --python_path=""/usr/bin/python3""
build --define with_jemalloc=true
build:gcp --define with_gcp_support=true
build:hdfs --define with_hdfs_support=true
build:aws --define with_aws_support=true
build:kafka --define with_kafka_support=true
build:xla --define with_xla_support=true
build:gdr --define with_gdr_support=true
build:verbs --define with_verbs_support=true
build:ngraph --define with_ngraph_support=true
build --action_env TF_NEED_OPENCL_SYCL=""0""
build --action_env TF_NEED_CUDA=""1""
build --action_env CUDA_TOOLKIT_PATH=""/usr/local/cuda""
build --action_env TF_CUDA_VERSION=""9.1""
build --action_env CUDNN_INSTALL_PATH=""/usr/local/cuda""
build --action_env TF_CUDNN_VERSION=""7""
build --action_env NCCL_INSTALL_PATH=""/usr/local/cuda""
build --action_env TF_NCCL_VERSION=""2""
build --action_env TF_CUDA_COMPUTE_CAPABILITIES=""6.1""
build --action_env TF_CUDA_CLANG=""0""
build --action_env GCC_HOST_COMPILER_PATH=""/usr/bin/gcc-4.8""
build --config=cuda
test --config=cuda
build --define grpc_no_ares=true
build:opt --copt=-march=native
build:opt --host_copt=-march=native
build:opt --define with_default_optimizations=true
```

_(Btw. why is build:kafka --define with_kafka_support=true when I said ""n""?)_

Compile output:

```
time bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures        
Starting local Bazel server and connecting to it...
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
DEBUG: /home/dedeibel/src/tensorflow-gpu/tensorflow/version_check.bzl:42:5: 
Current Bazel is not a release version, cannot check for compatibility.
DEBUG: /home/dedeibel/src/tensorflow-gpu/tensorflow/version_check.bzl:43:5: Make sure that you are running at least Bazel 0.15.0.
DEBUG: /home/dedeibel/.cache/bazel/_bazel_dedeibel/216944cd492986130e450b1bf5cd7b5e/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5: 
Auto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\Windows\Temp' as default
WARNING: /home/dedeibel/.cache/bazel/_bazel_dedeibel/216944cd492986130e450b1bf5cd7b5e/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/dedeibel/.cache/bazel/_bazel_dedeibel/216944cd492986130e450b1bf5cd7b5e/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: /home/dedeibel/.cache/bazel/_bazel_dedeibel/216944cd492986130e450b1bf5cd7b5e/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/dedeibel/.cache/bazel/_bazel_dedeibel/216944cd492986130e450b1bf5cd7b5e/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: /home/dedeibel/.cache/bazel/_bazel_dedeibel/216944cd492986130e450b1bf5cd7b5e/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/dedeibel/.cache/bazel/_bazel_dedeibel/216944cd492986130e450b1bf5cd7b5e/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: /home/dedeibel/src/tensorflow-gpu/tensorflow/core/BUILD:2463:1: in includes attribute of cc_library rule //tensorflow/core:framework_internal_headers_lib: '../../external/com_google_absl' resolves to 'external/com_google_absl' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/dedeibel/src/tensorflow-gpu/tensorflow/tensorflow.bzl:1373:20
WARNING: /home/dedeibel/src/tensorflow-gpu/tensorflow/core/BUILD:2548:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/com_google_absl' resolves to 'external/com_google_absl' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/dedeibel/src/tensorflow-gpu/tensorflow/tensorflow.bzl:1373:20
WARNING: /home/dedeibel/src/tensorflow-gpu/tensorflow/core/BUILD:2562:1: in includes attribute of cc_library rule //tensorflow/core:stream_executor_headers_lib: '../../external/com_google_absl' resolves to 'external/com_google_absl' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/dedeibel/src/tensorflow-gpu/tensorflow/tensorflow.bzl:1373:20
WARNING: /home/dedeibel/src/tensorflow-gpu/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: /home/dedeibel/src/tensorflow-gpu/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
WARNING: /home/dedeibel/src/tensorflow-gpu/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/dedeibel/src/tensorflow-gpu/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:73:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/dedeibel/src/tensorflow-gpu/tensorflow/contrib/timeseries/python/timeseries/BUILD:354:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries:ar_model: target '//tensorflow/contrib/timeseries/python/timeseries:ar_model' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/dedeibel/src/tensorflow-gpu/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:230:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/dedeibel/src/tensorflow-gpu/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/dedeibel/src/tensorflow-gpu/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (310 packages loaded).
INFO: Found 1 target...
Slow read: a 2353-byte read from /home/dedeibel/src/tensorflow-gpu/tensorflow/core/distributed_runtime/rpc/grpc_util.cc took 5849 ms.
Slow read: a 1998-byte read from /home/dedeibel/src/tensorflow-gpu/tensorflow/core/kernels/scatter_op_gpu.cu.cc took 5849 ms.
ERROR: /home/dedeibel/.cache/bazel/_bazel_dedeibel/216944cd492986130e450b1bf5cd7b5e/external/local_config_cuda/cuda/BUILD:638:1: Executing genrule @local_config_cuda//cuda:cuda-extras failed (Segmentation fault): bash failed: error executing command 
  (cd /home/dedeibel/.cache/bazel/_bazel_dedeibel/216944cd492986130e450b1bf5cd7b5e/execroot/org_tensorflow && \
  exec env - \
    PATH=/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/opt/groovy/bin:/opt/grails/bin:/opt/gradle/bin:/home/dedeibel/bin:/home/dedeibel/ruby_bin:/home/dedeibel/perl_bin:/my/games/bin:/opt/bin:/home/dedeibel/opt/android/platform-tools:/home/dedeibel/go/bin:/opt/groovy/bin:/opt/grails/bin:/opt/gradle/bin:/home/dedeibel/bin:/home/dedeibel/ruby_bin:/home/dedeibel/perl_bin:/my/games/bin:/opt/bin:/home/dedeibel/opt/android/platform-tools:/home/dedeibel/go/bin \
  /bin/bash bazel-out/host/genfiles/external/local_config_cuda/cuda/cuda-extras.genrule_script.sh): bash failed: error executing command 
  (cd /home/dedeibel/.cache/bazel/_bazel_dedeibel/216944cd492986130e450b1bf5cd7b5e/execroot/org_tensorflow && \
  exec env - \
    PATH=/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/opt/groovy/bin:/opt/grails/bin:/opt/gradle/bin:/home/dedeibel/bin:/home/dedeibel/ruby_bin:/home/dedeibel/perl_bin:/my/games/bin:/opt/bin:/home/dedeibel/opt/android/platform-tools:/home/dedeibel/go/bin:/opt/groovy/bin:/opt/grails/bin:/opt/gradle/bin:/home/dedeibel/bin:/home/dedeibel/ruby_bin:/home/dedeibel/perl_bin:/my/games/bin:/opt/bin:/home/dedeibel/opt/android/platform-tools:/home/dedeibel/go/bin \
  /bin/bash bazel-out/host/genfiles/external/local_config_cuda/cuda/cuda-extras.genrule_script.sh)
Slow read: a 54035688-byte read from /home/dedeibel/.cache/bazel/_bazel_dedeibel/216944cd492986130e450b1bf5cd7b5e/execroot/org_tensorflow/bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda/cuda/lib/libcublas.so.9.1 took 5918 ms.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 2592.039s, Critical Path: 23.55s
INFO: 58 processes: 58 local.
FAILED: Build did NOT complete successfully
bazel build --config=opt --config=cuda  --verbose_failures  0.28s user 0.17s system 0% cpu 43:15.42 total
```

Since I don't know what to do differently I suspect a bug and would love to get some advice. If it is my mistake, I am sorry to bother you."
22970,protobuf not seem to be recognized but it is installed,"(base) D:\FINAL YEAR\Final Project Fanally\Project\Workshop\TensorFlow\models\object_detection>python legacy/train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v2_coco.config
Traceback (most recent call last):
  File ""legacy/train.py"", line 49, in <module>
    from object_detection.builders import dataset_builder
  File ""D:\FINAL YEAR\Final Project Fanally\Project\Workshop\TensorFlow\models\object_detection\builders\dataset_builder.py"", line 27, in <module>
    from object_detection.data_decoders import tf_example_decoder
  File ""D:\FINAL YEAR\Final Project Fanally\Project\Workshop\TensorFlow\models\object_detection\data_decoders\tf_example_decoder.py"", line 24, in <module>
    from object_detection.protos import input_reader_pb2
  File ""D:\FINAL YEAR\Final Project Fanally\Project\Workshop\TensorFlow\models\object_detection\protos\input_reader_pb2.py"", line 11, in <module>
    from google.protobuf import descriptor_pb2
  File ""C:\ProgramData\Anaconda3\lib\site-packages\google\protobuf\descriptor_pb2.py"", line 1114, in <module>
    serialized_options=None, file=DESCRIPTOR),
  File ""C:\ProgramData\Anaconda3\lib\site-packages\google\protobuf\descriptor.py"", line 534, in __new__
    return _message.default_pool.FindFieldByName(full_name)
KeyError: ""Couldn't find field google.protobuf.FileOptions.php_metadata_namespace""

(base) D:\FINAL YEAR\Final Project Fanally\Project\Workshop\TensorFlow\models\object_detection>python legacy/train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v2_coco.config
Traceback (most recent call last):
  File ""legacy/train.py"", line 49, in <module>
    from object_detection.builders import dataset_builder
  File ""D:\FINAL YEAR\Final Project Fanally\Project\Workshop\TensorFlow\models\object_detection\builders\dataset_builder.py"", line 27, in <module>
    from object_detection.data_decoders import tf_example_decoder
  File ""D:\FINAL YEAR\Final Project Fanally\Project\Workshop\TensorFlow\models\object_detection\data_decoders\tf_example_decoder.py"", line 24, in <module>
    from object_detection.protos import input_reader_pb2
  File ""D:\FINAL YEAR\Final Project Fanally\Project\Workshop\TensorFlow\models\object_detection\protos\input_reader_pb2.py"", line 11, in <module>
    from google.protobuf import descriptor_pb2
  File ""C:\ProgramData\Anaconda3\lib\site-packages\google\protobuf\descriptor_pb2.py"", line 1114, in <module>
    serialized_options=None, file=DESCRIPTOR),
  File ""C:\ProgramData\Anaconda3\lib\site-packages\google\protobuf\descriptor.py"", line 534, in __new__
    return _message.default_pool.FindFieldByName(full_name)
KeyError: ""Couldn't find field google.protobuf.FileOptions.php_metadata_namespace""

Please F1 aka Help
"
22969,"ValueError: sequence_length must be a vector of length batch_size, but saw shape: (24, 1, 2)","import tensorflow as tf

I am working with lstm using tensor flow when I am running the code it is showing me the error. the code is running fine but when I am running the function tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float64) it is showing Value ERROR

wordsList = np.load('urduwords.npy')
wordVectors = np.load('urduwordsMatrix.npy')

batchSize = 24
lstmUnits = 64
numClasses = 2
iterations = 10000

tf.reset_default_graph()


labels = tf.placeholder(tf.float32, [batchSize, numClasses])
input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])

print(labels)

data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)
print(data)


data = tf.nn.embedding_lookup(wordVectors,input_data)
print(data)


lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)
lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.1)

value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float64)

data_mask = tf.cast(data, tf.bool)
data_len = tf.reduce_sum(tf.cast(data_mask, tf.float32), axis=1)
tf.nn.dynamic_rnn(lstmCell, data, sequence_length=data_len, initial_state=initial_state)

sequence_length must be a vector of length batch_size, but saw shape: (24, 1, 2)"
22967,libcublas.so.9.0: cannot open shared object file: No such file or directory,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22965,I installed tensorflow-gpu using command  pip3 install tensorflow-gpu   but when I import tensorflow it throws error like this  ImportError: libcuda.so.9: cannot open shared object file: No such file or directory,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22963,"Subclassing the tf.keras.model.Model class, throw ""ValueError: You tried to call `count_params` on dense_81, but the layer isn't built. You can build it manually via: `dense_81.build(batch_input_shape)`.""","The example code :
```python
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Input

class MyModel(Model):
    def __init__(self):
        super().__init__()
        self.dense = Dense(4)
    
    def call(self, inputs):
  
        return self.dense(inputs)
    
model = MyModel()
# inputs = Input(shape=(None, 10))
model.build((None, 10))
model.summary()
```
```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-103-b62d1ffe41d5> in <module>()
     14 # inputs = Input(shape=(None, 10))
     15 model.build((None, 10))
---> 16 model.summary()

~\AppData\Roaming\Python\Python35\site-packages\tensorflow\python\keras\engine\network.py in summary(self, line_length, positions, print_fn)
   1551                               line_length=line_length,
   1552                               positions=positions,
-> 1553                               print_fn=print_fn)
   1554 
   1555 

~\AppData\Roaming\Python\Python35\site-packages\tensorflow\python\keras\utils\layer_utils.py in print_summary(model, line_length, positions, print_fn)
    221   for i in range(len(layers)):
    222     if sequential_like:
--> 223       print_layer_summary(layers[i])
    224     else:
    225       print_layer_summary_with_connections(layers[i])

~\AppData\Roaming\Python\Python35\site-packages\tensorflow\python\keras\utils\layer_utils.py in print_layer_summary(layer)
    177     name = layer.name
    178     cls_name = layer.__class__.__name__
--> 179     fields = [name + ' (' + cls_name + ')', output_shape, layer.count_params()]
    180     print_row(fields, positions)
    181 

~\AppData\Roaming\Python\Python35\site-packages\tensorflow\python\keras\engine\base_layer.py in count_params(self)
   1324                          ', but the layer isn\'t built. '
   1325                          'You can build it manually via: `' + self.name +
-> 1326                          '.build(batch_input_shape)`.')
   1327     weight_shapes = [w.shape.as_list() for w in self.weights]
   1328     return int(sum([np.prod(w) for w in weight_shapes]))

ValueError: You tried to call `count_params` on dense_84, but the layer isn't built. You can build it manually via: `dense_84.build(batch_input_shape)`.
```

>The environment in below:
OS Platform: Window10 64bit
Distribution: N
tools: Jupyter Notebook
Python:  python3.5
Tensorflow: tensorflow-gpu1.10 by pip installed
Bazel version: N
CUDA: 9.0
cuDNN: 7.0
GPU: Quadro M2000/4G
Mobile device: N

In **Jupyter** result thrown ValueError, but result show as below in **Colab**
```
_______________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_2 (Dense)              multiple                  44        
=================================================================
Total params: 44
Trainable params: 44
Non-trainable params: 0
_________________________________________________________________
```"
22962,How is loss (and average loss) computed in Canned estimators?,"### System information
- **Largely irrelevant, the issue concerns documentation**
- **Tensorflow 1.11 (stable)**


Documentation only specifies:

> loss (mean loss per mini-batch) and the average_loss (mean loss per sample)

I understand that this might seem like enough, but since there are several, very different algorithms for computing loss (absolute, relative, square etc.), this can make a big difference.

Especially if you are comparing your custom estimator, with canned alternatives.

For example, I make my own DNN classifier, I run it and get eval dict with keys loss, accuracy, and global_step.  But does the `loss` key represent the same thing it does in canned equivalent? And even if it does, which algorithm I should pick if I want to compute average loss? 

If you are using the canned estimators as a baseline, and many beginners do, then making wrong assumptions about metrics is the worst possible option.

"
22960,Tensorflow Compile error,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: Master
- **Python version**: Python 3.6.7rc1
- **Bazel version (if compiling from source)**: Build label: 0.17.2
Build target: bazel-out/x64_windows-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Sep 21 10:31:06 2018 (1537525866)
Build timestamp: 1537525866
Build timestamp as int: 1537525866
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 10.0 / 7.3.1
- **GPU model and memory**: GeForce 1050
- **Exact command to reproduce**:
INFO: From Linking tensorflow/core/liblib_internal_impl.a:
android_armv7a_cpu_utils_helper.o : warning LNK4221: This object file does not define any previously undefined public symbols, so it will not be used by any link operation that consumes this library
random_distributions.o : warning LNK4221: This object file does not define any previously undefined public symbols, so it will not be used by any link operation that consumes this library
ERROR: D:/tensorflow/tensorflow/core/kernels/BUILD:4420:1: C++ compilation of rule '//tensorflow/core/kernels:scatter_nd_op_gpu' failed (Exit 1): msvc_wrapper_for_nvcc.bat failed: error executing command
  cd C:/users/sirto/_bazel_sirto/26orbg4z/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0
    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\INCLUDE;C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\include\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\winrt;
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\LIB\amd64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\LIB\amd64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.10240.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.10240.0\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\amd64;C:\WINDOWS\Microsoft.NET\Framework64\v4.0.30319;C:\WINDOWS\Microsoft.NET\Framework64\;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Windows Kits\10\bin\x86;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.6.1 Tools\x64\;;C:\WINDOWS\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=D:/Program Files/Python36/python.exe
    SET PYTHON_LIB_PATH=D:/Program Files/Python36/lib/site-packages
    SET TEMP=C:\Users\sirto\AppData\Local\Temp
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1
    SET TF_CUDA_VERSION=10.0
    SET TF_CUDNN_VERSION=7
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
    SET TMP=C:\Users\sirto\AppData\Local\Temp
  external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.bat /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Ibazel-out/x64_windows-opt/bin /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/genfiles/external/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/genfiles/external/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/bazel_tools /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/nsync /Ibazel-out/x64_windows-opt/genfiles/external/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/gif_archive /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive /Ibazel-out/x64_windows-opt/bin/external/gif_archive /Iexternal/jpeg /Ibazel-out/x64_windows-opt/genfiles/external/jpeg /Ibazel-out/x64_windows-opt/bin/external/jpeg /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/genfiles/external/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/genfiles/external/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/genfiles/external/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/genfiles/external/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/genfiles/external/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/gif_archive/lib /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/lib /Ibazel-out/x64_windows-opt/bin/external/gif_archive/lib /Iexternal/gif_archive/windows /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/windows /Ibazel-out/x64_windows-opt/bin/external/gif_archive/windows /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/local_config_cuda/cuda/cuda/include/crt /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cuda/include/crt /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include/crt /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DTF_USE_SNAPPY /showIncludes /MD /O2 /DNDEBUG -w /arch:AVX -nvcc_options=disable-warnings -DGOOGLE_CUDA=1 -DTENSORFLOW_MONOLITHIC_BUILD /DPLATFORM_WINDOWS /DEIGEN_HAS_C99_MATH /DTENSORFLOW_USE_EIGEN_THREADPOOL /DEIGEN_AVOID_STL_ARRAY /Iexternal/gemmlowp /wd4018 /wd4577 /DNOGDI /DTF_COMPILE_LIBRARY -x cuda -DGOOGLE_CUDA=1 -nvcc_options=relaxed-constexpr -nvcc_options=ftz=true /Fobazel-out/x64_windows-opt/bin/tensorflow/core/kernels/_objs/scatter_nd_op_gpu/scatter_nd_op_gpu.cu.o /c tensorflow/core/kernels/scatter_nd_op_gpu.cu.cc
c:\users\sirto\_bazel_sirto\26orbg4z\execroot\org_tensorflow\external\eigen_archive\eigen\src/Core/arch/CUDA/Half.h(212): error: more than one instance of overloaded function ""__hadd"" matches the argument list:
            function ""__hadd(int, int)""
            function ""__hadd(__half, __half)""
            argument types are: (const Eigen::half, const Eigen::half)

1 error detected in the compilation of ""C:/Users/sirto/AppData/Local/Temp/nvcc_inter_files_tmp_dir/scatter_nd_op_gpu.cu.cpp1.ii"".
Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: D:/tensorflow/tensorflow/tools/pip_package/BUILD:124:1 C++ compilation of rule '//tensorflow/core/kernels:scatter_nd_op_gpu' failed (Exit 1): msvc_wrapper_for_nvcc.bat failed: error executing command
  cd C:/users/sirto/_bazel_sirto/26orbg4z/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0
    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\INCLUDE;C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\include\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\winrt;
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\LIB\amd64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\LIB\amd64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.10240.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.10240.0\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\amd64;C:\WINDOWS\Microsoft.NET\Framework64\v4.0.30319;C:\WINDOWS\Microsoft.NET\Framework64\;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Windows Kits\10\bin\x86;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.6.1 Tools\x64\;;C:\WINDOWS\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=D:/Program Files/Python36/python.exe
    SET PYTHON_LIB_PATH=D:/Program Files/Python36/lib/site-packages
    SET TEMP=C:\Users\sirto\AppData\Local\Temp
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1
    SET TF_CUDA_VERSION=10.0
    SET TF_CUDNN_VERSION=7
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
    SET TMP=C:\Users\sirto\AppData\Local\Temp
  external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.bat /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Ibazel-out/x64_windows-opt/bin /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/genfiles/external/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/genfiles/external/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/bazel_tools /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/nsync /Ibazel-out/x64_windows-opt/genfiles/external/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/gif_archive /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive /Ibazel-out/x64_windows-opt/bin/external/gif_archive /Iexternal/jpeg /Ibazel-out/x64_windows-opt/genfiles/external/jpeg /Ibazel-out/x64_windows-opt/bin/external/jpeg /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/genfiles/external/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/genfiles/external/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/genfiles/external/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/genfiles/external/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/genfiles/external/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/gif_archive/lib /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/lib /Ibazel-out/x64_windows-opt/bin/external/gif_archive/lib /Iexternal/gif_archive/windows /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/windows /Ibazel-out/x64_windows-opt/bin/external/gif_archive/windows /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/local_config_cuda/cuda/cuda/include/crt /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cuda/include/crt /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include/crt /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DTF_USE_SNAPPY /showIncludes /MD /O2 /DNDEBUG -w /arch:AVX -nvcc_options=disable-warnings -DGOOGLE_CUDA=1 -DTENSORFLOW_MONOLITHIC_BUILD /DPLATFORM_WINDOWS /DEIGEN_HAS_C99_MATH /DTENSORFLOW_USE_EIGEN_THREADPOOL /DEIGEN_AVOID_STL_ARRAY /Iexternal/gemmlowp /wd4018 /wd4577 /DNOGDI /DTF_COMPILE_LIBRARY -x cuda -DGOOGLE_CUDA=1 -nvcc_options=relaxed-constexpr -nvcc_options=ftz=true /Fobazel-out/x64_windows-opt/bin/tensorflow/core/kernels/_objs/scatter_nd_op_gpu/scatter_nd_op_gpu.cu.o /c tensorflow/core/kernels/scatter_nd_op_gpu.cu.cc
INFO: Elapsed time: 653.206s, Critical Path: 181.89s
INFO: 2220 processes: 2220 local.
FAILED: Build did NOT complete successfully"
22957,Freezing network with batch norm does not work with TRT,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
16.04
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:
1.10.1
- **Python version**:
3.5
- **CUDA/cuDNN version**:
9.0
- **Bazel version**:
N/A
- **GPU model and memory**:
N/A
- **Mobile device**:
N/A
- **Exact command to reproduce**:
```
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
import numpy as np
import tensorflow.contrib.tensorrt as trt
import tensorflow as tf
from tensorflow.python.platform import gfile
from tensorflow.keras import backend as K

path         = ""/tmp""
output_trt_pb    = os.path.join(path, ""output_trt.pb"")

np.random.seed(0)

X, Y = np.random.rand(1000, 100, 100, 3), np.random.rand(1000, 100, 100, 16)

with K.get_session() as sess:
    
    inp = tf.keras.layers.Input(shape=(100,100,3), name=""input"")
    x = tf.keras.layers.Conv2D(16, (3,3), padding=""same"", kernel_initializer=""ones"", name=""conv2d"", use_bias=False)(inp)
    x = tf.keras.layers.BatchNormalization(name=""bn"", fused=True)(x)
    x = tf.keras.layers.Activation(""relu"")(x)
    model = tf.keras.models.Model(inp, x)
    model.compile(""adam"", ""mse"")
    model.fit(X, Y, epochs=3, verbose=True)
    
    # fix nodes (from https://github.com/tensorflow/tensorflow/issues/3628) here doesn't help
    graph_def = sess.graph_def
    
    output_graph_def = tf.graph_util.convert_variables_to_constants(
        sess,                                     # The session is used to retrieve the weights
        graph_def,                                # The graph_def is used to retrieve the nodes
        [i.name[:-2] for i in model.outputs]      # The output node names are used to select the useful nodes
    )
    
    trt_graph = trt.create_inference_graph(output_graph_def, 
                                           [i.name[:-2] for i in model.outputs],
                                           max_batch_size=1,
                                           max_workspace_size_bytes= 1256 << 20,
                                           precision_mode=""FP32"") 
```

### Describe the problem

When I freeze a protobuf that contains batch normalization and then try to use it with TRT, it fails with the error 

`InvalidArgumentError: Input 0 of node bn/cond/ReadVariableOp/Switch_1 was passed float from bn/gamma_1:0 incompatible with expected resource.`

It seems like this is an issue in other threads, like https://github.com/tensorflow/tensorflow/issues/3628 and in my code I tried to include the suggested fixes, but this does not help.
I tried using both fused=True and fused=False, and also tried trainable=False/True in BatchNormalization.
If you comment out
`x = tf.keras.layers.BatchNormalization(name=""bn"", fused=True)(x)`
then everything works fine.

Any comment would be appreciated.
"
22955,How to convert Tensorrt Optimized graph to Tensorrt engine,"I need to convert my tensorflow model to tensorrt engine. However, since there are some unsupported layers in the model, I decided to use create_inference_graph to skip those operations. Although it works well with Python, I need to write it to *.engine file so that I can read it in C++. I tried to convert it to uff model using uff_model_from_tensorflow_frozen_model, but got key error. Is there a way I can convert the optimized graph to an engine? Any suggestion would be appreciated.

### System information

- **OS Platform and Distribution : Ubuntu 16.04
- **TensorFlow installed from (source or binary) : source
- **TensorFlow version 1.7 ~ 1.11
- **Python version**: 3.5
- **CUDA/cuDNN version**: CUDA 8.0, 9.0; CUDNN 7.0, 7.3
- Have I written custom code : No
- Bazel version : NA
- GPU model and memory : NA
- Mobile device : NA
- **Exact command to reproduce**:
```
def load_graph(file):
    with tf.gfile.GFile(file, 'rb') as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())
    with tf.Graph().as_default() as graph:
        tf.import_graph_def(graph_def)
    return graph, graph_def

graph = load_graph('tensorflow_model.pb')
tensorrt_graph = trt.create_inference_graph(graph_def, outputs=['output_node'], max_batch_size=1, precision_mode='FP32', max_workspace_size_bytes=1<<33)

with tf.gfile.GFile('tensorrt_model.pb', 'wb') as f:
    f.write(tensorrt_graph.SerializeToString())

loaded_tensorrt_graph, loaded_tensorrt_graph_def = load_graph('tensorrt_model.pb')
uff_model = uff.from_tensorflow_frozen_model(loaded_tensorrt_graph_def)
```

Traceback (most recent call last):
File ""<stdin>"", line 1, in <module>
File ""/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/conversion_helpers.py"", line 149, in from_tensorflow_frozen_model
return from_tensorflow(graphdef, output_nodes, preprocessor, **kwargs)
File ""/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/conversion_helpers.py"", line 120, in from_tensorflow
name=""main"")
File ""/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py"", line 76, in convert_tf2uff_graph
uff_graph, input_replacements)
File ""/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py"", line 63, in convert_tf2uff_node
op, name, tf_node, inputs, uff_graph, tf_nodes=tf_nodes)
File ""/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py"", line 38, in convert_layer
fields = cls.parse_tf_attrs(tf_node.attr)
File ""/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py"", line 209, in parse_tf_attrs
for key, val in attrs.items()}
File ""/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py"", line 209, in <dictcomp>
for key, val in attrs.items()}
File ""/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py"", line 204, in parse_tf_attr_value
return cls.convert_tf2uff_field(code, val)
File ""/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py"", line 189, in convert_tf2uff_field
'type': 'dtype', 'list': 'list'}
KeyError: 'shape'
"
22954,Error Building from source on Windows / my CPU doesn't have AVX ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.11
- **Python version**:3.6.6
- **Bazel version (if compiling from source)**: 0.17.2
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:  bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
I can't build from source as it gives me the error
ERROR: C:/users/ivo/_bazel_ivo/xv6zejqw/external/protobuf_archive/BUILD:266:1: Executing genrule @protobuf_archive//:generate_js_well_known_types_embed failed (Illegal instruction): bash.exe failed: error executing command
Perhaps because my CPU doesnt have AVX instructions set

on my CPU Supported Instructions sets	MMX, SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2, EM64T
Lack of AVX don't allow me to pip install tf>1.5
My question is how to install from source without AVX instructions set?

### Source code / logs

> C:\tensorflow>bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
> Starting local Bazel server and connecting to it...
> WARNING: Option 'experimental_shortened_obj_file_path' is deprecated
> WARNING: C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/bazel/grpc_build_system.bzl:172:12
> WARNING: C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/bazel/grpc_build_system.bzl:172:12
> WARNING: C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/bazel/grpc_build_system.bzl:172:12
> WARNING: C:/tensorflow/tensorflow/python/BUILD:2823:1: in py_library rule //tensorflow/python:standard_ops: target '//tensorflow/python:standard_ops' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
> WARNING: C:/tensorflow/tensorflow/contrib/metrics/BUILD:16:1: in py_library rule //tensorflow/contrib/metrics:metrics_py: target '//tensorflow/contrib/metrics:metrics_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
> WARNING: C:/tensorflow/tensorflow/python/BUILD:73:1: in py_library rule //tensorflow/python:no_contrib: target '//tensorflow/python:no_contrib' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
> WARNING: C:/tensorflow/tensorflow/contrib/gan/BUILD:137:1: in py_library rule //tensorflow/contrib/gan:losses_impl: target '//tensorflow/contrib/gan:losses_impl' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
> WARNING: C:/tensorflow/tensorflow/contrib/gan/BUILD:33:1: in py_library rule //tensorflow/contrib/gan:train: target '//tensorflow/contrib/gan:train' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
> WARNING: C:/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
> WARNING: C:/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
> WARNING: C:/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:230:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
> WARNING: C:/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:73:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
> WARNING: C:/tensorflow/tensorflow/contrib/timeseries/python/timeseries/BUILD:354:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries:ar_model: target '//tensorflow/contrib/timeseries/python/timeseries:ar_model' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
> WARNING: C:/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
> WARNING: C:/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
> WARNING: C:/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
> WARNING: C:/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
> INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (303 packages loaded).
> INFO: Found 1 target...
> INFO: From Compiling external/com_google_absl/absl/base/dynamic_annotations.cc:
> cl : Command line warning D9025 : overriding '/w' with '/W3'
> ERROR: C:/users/ivo/_bazel_ivo/xv6zejqw/external/protobuf_archive/BUILD:266:1: Executing genrule @protobuf_archive//:generate_js_well_known_types_embed failed (Illegal instruction): bash.exe failed: error executing command
>   cd C:/users/ivo/_bazel_ivo/xv6zejqw/execroot/org_tensorflow
>   SET PATH=C:\msys64\usr\bin;C:\msys64\bin;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Users\ivo\AppData\Local\Microsoft\WindowsApps;C:\Users\ivo\AppData\Local\Programs\Python\Python36;C:\Users\ivo\AppData\Local\Programs\Python\Python36\Scripts;C:\bazel;C:\msys64\usr\bin
>     SET PYTHON_BIN_PATH=C:/Users/ivo/AppData/Local/Programs/Python/Python36/python.exe
>     SET PYTHON_LIB_PATH=C:/Users/ivo/AppData/Local/Programs/Python/Python36/lib/site-packages
>     SET TF_DOWNLOAD_CLANG=0
>     SET TF_NEED_CUDA=0
>     SET TF_NEED_OPENCL_SYCL=0
>     SET TF_NEED_ROCM=0
>   C:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt/bin/external/protobuf_archive/js_embed.exe external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/any.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/struct.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/timestamp.js > bazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types_embed.cc: bash.exe failed: error executing command
>   cd C:/users/ivo/_bazel_ivo/xv6zejqw/execroot/org_tensorflow
>   SET PATH=C:\msys64\usr\bin;C:\msys64\bin;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Users\ivo\AppData\Local\Microsoft\WindowsApps;C:\Users\ivo\AppData\Local\Programs\Python\Python36;C:\Users\ivo\AppData\Local\Programs\Python\Python36\Scripts;C:\bazel;C:\msys64\usr\bin
>     SET PYTHON_BIN_PATH=C:/Users/ivo/AppData/Local/Programs/Python/Python36/python.exe
>     SET PYTHON_LIB_PATH=C:/Users/ivo/AppData/Local/Programs/Python/Python36/lib/site-packages
>     SET TF_DOWNLOAD_CLANG=0
>     SET TF_NEED_CUDA=0
>     SET TF_NEED_OPENCL_SYCL=0
>     SET TF_NEED_ROCM=0
>   C:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt/bin/external/protobuf_archive/js_embed.exe external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/any.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/struct.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/timestamp.js > bazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types_embed.cc
> /usr/bin/bash: line 1:  7128 Illegal instruction     bazel-out/x64_windows-opt/bin/external/protobuf_archive/js_embed.exe external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/any.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/struct.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/timestamp.js > bazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types_embed.cc
> Target //tensorflow/tools/pip_package:build_pip_package failed to build
> INFO: Elapsed time: 74.199s, Critical Path: 2.64s
> INFO: 42 processes: 42 local.
> FAILED: Build did NOT complete successfully
"
22952,[tf.keras] Release 1.12.0 issue in model.predict_generator,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: na
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.12.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: na
- **GCC/Compiler version (if compiling from source)**: na
- **CUDA/cuDNN version**: na
- **GPU model and memory**: na
- **Exact command to reproduce**: na

### Describe the problem
Calling ```model.predict_generator()``` on a loaded model or an uncompiled model produces an error. This does not happen for ``model.predict`` or previous releases of TensorFlow.

### Source code / logs
```python
import numpy as np
import tensorflow as tf


def create_model():
    inputs = tf.keras.layers.Input(shape=(1,))
    outputs = tf.keras.layers.Dense(1)(inputs)
    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)
    return model


model = create_model()

X = np.array([[1]])
print(model.predict(X))

class gen(object):
    """""" Dummy data generator. """"""

    def run(self):
        while True:
            yield np.array([[1]])

print(model.predict_generator(generator=gen().run(), steps=1))
```

```python
line 23, in <module>
    print(model.predict_generator(generator=gen().run(), steps=1))
  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 2298, in predict_generator
    verbose=verbose)
  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py"", line 354, in predict_generator
    model._make_test_function()
  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 716, in _make_test_function
    raise RuntimeError('You must compile your model before using it.')
```"
22948,MacOSX & Bazel build errors,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS High Sierra 10.13.6
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: 2.7.5
- **Bazel version (if compiling from source)**: 0.17.2
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Mobile device**: Samsung Note 9
- **Exact command to reproduce**: bazel build -c opt --cxxopt=--std=c++11 //tensorflow/contrib/lite/experimental/c:libtensorflowlite_c.so

### Describe the problem
I am trying to implement the TensorFlowLite Experimental Unity Plug-in. I am currently building tensorflow from the source. I am using android ndk version 13. When I run bazel build -c opt --cxxopt=--std=c++11 //tensorflow/contrib/lite/experimental/c:libtensorflowlite_c.so, I get the errors below. 

Sometimes, when I run the above command, I get an undeclared inclusion(s) rule on 'tensorflow/contrib/lite/profiling/time.cc' or another file without changing the statement.

All I need to do is use the tensorflowlite experimental unity plug-in so if there is a workaround to this or a solution, that'd be great.

### Source code / logs
INFO: Analysed target //tensorflow/contrib/lite/experimental/c:libtensorflowlite_c.so (0 packages loaded).
INFO: Found 1 target...
ERROR: /Users/sarahanson/tensorflow/voyager/tensorflow/tensorflow/contrib/lite/kernels/internal/BUILD:440:1: undeclared inclusion(s) in rule '//tensorflow/contrib/lite/kernels/internal:kernel_utils':
this rule is missing dependency declarations for the following files included by 'tensorflow/contrib/lite/kernels/internal/kernel_utils.cc':
  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/stdint.h'
  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/sys/_types/_int8_t.h'
  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/sys/_types/_int16_t.h'
  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/sys/_types/_int32_t.h'
  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/sys/_types/_int64_t.h'
  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/_types/_uint8_t.h'
  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/_types/_uint16_t.h'
  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/_types/_uint32_t.h'
  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/_types/_uint64_t.h'
  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/sys/_types.h'
  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/sys/cdefs.h'
  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/sys/_symbol_aliasing.h'
  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/sys/_posix_availability.h'
  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/machine/_types.h'
  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/i386/_types.h'
  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/sys/_pthread/_pthread_types.h'
  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/sys/_types/_intptr_t.h'
  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/machine/types.h'
  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/i386/types.h'
  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/sys/_types/_u_int8_t.h'
  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/sys/_types/_u_int16_t.h'
  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/sys/_types/_u_int32_t.h'
  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/sys/_types/_u_int64_t.h'
  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/sys/_types/_uintptr_t.h'
  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/_types/_intmax_t.h'
  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/_types/_uintmax_t.h'
In file included from tensorflow/contrib/lite/kernels/internal/kernel_utils.cc:15:
In file included from ./tensorflow/contrib/lite/kernels/internal/kernel_utils.h:18:
./tensorflow/contrib/lite/c/builtin_op_data.h:144:9: warning: empty struct has size 0 in C, size 1 in C++ [-Wextern-c-compat]
typedef struct {
        ^
./tensorflow/contrib/lite/c/builtin_op_data.h:147:9: warning: empty struct has size 0 in C, size 1 in C++ [-Wextern-c-compat]
typedef struct {
        ^
./tensorflow/contrib/lite/c/builtin_op_data.h:210:9: warning: empty struct has size 0 in C, size 1 in C++ [-Wextern-c-compat]
typedef struct {
        ^
./tensorflow/contrib/lite/c/builtin_op_data.h:213:9: warning: empty struct has size 0 in C, size 1 in C++ [-Wextern-c-compat]
typedef struct {
        ^
./tensorflow/contrib/lite/c/builtin_op_data.h:252:9: warning: empty struct has size 0 in C, size 1 in C++ [-Wextern-c-compat]
typedef struct {
        ^
5 warnings generated.
Target //tensorflow/contrib/lite/experimental/c:libtensorflowlite_c.so failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 0.374s, Critical Path: 0.22s
INFO: 0 processes.
FAILED: Build did NOT complete successfully
"
22947,[tf.keras] Ominous Load Warning,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: na
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.11.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: na
- **GCC/Compiler version (if compiling from source)**: na
- **CUDA/cuDNN version**: na
- **GPU model and memory**:na 
- **Exact command to reproduce**:na

### Describe the problem
Calling 
```tf.keras.models.model.save(...)```
```model.load_weights(...)```

Produces an ominous warning:

```
Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?
```

### Source code / logs
```python
import numpy as np
import tensorflow as tf

def create_model():
    inputs = tf.keras.layers.Input(shape=(1,))
    outputs = tf.keras.layers.Dense(1)(inputs)
    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)
    return model

X = np.array([[1]])

model = create_model()
model.compile(""Adam"", loss=""mse"")
model.save(""test.hdf5"")
print (model.predict(X))
del model

model = create_model()
model.load_weights(""test.hdf5"")
print (model.predict(X))
```

```
[[-1.1461306]]
2018-10-12 13:05:33.052964: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ./test.hdf5: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?
[[-1.1461306]]
```

The predictions match, the warning isn't actually an issue."
22946,Convolutional LSTM raises errors,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
No
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.9
- **Python version**:
2.7
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
CUDA 9
cuDNN 7.1

- **GPU model and memory**:
GeForce GTX 1070
8GB

- **Exact command to reproduce**:
Listed in source code

### Describe the problem

Using convolutional LSTM cell with dynamic_rnn raises errors:
```
2018-10-12 14:24:01.651741: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:581] Iteration = 0, topological sort failed with message: The graph couldn't be sorted in topological order.
2018-10-12 14:24:01.652168: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:581] Iteration = 1, topological sort failed with message: The graph couldn't be sorted in topological order.
```
The Code is listed below, it works on tf1.8. But tf1.9 and tf1.10 give errors above.

### Source code / logs

```
import tensorflow as tf
cell = tf.contrib.rnn.ConvLSTMCell(conv_ndims=2, input_shape=[28, 28, 3], output_channels=32, kernel_shape=[3, 3], use_bias=True)
c_input = tf.placeholder(tf.float32, [None] + list(cell.state_size.c))
h_input = tf.placeholder(tf.float32, [None] + list(cell.state_size.h))
state_in = tf.nn.rnn_cell.LSTMStateTuple(c_input, h_input)
x = tf.placeholder(tf.float32, [None, None, 28, 28, 3])
lstm_output, next_state = tf.nn.dynamic_rnn(cell = cell, inputs = x, initial_state = state_in, dtype = tf.float32, time_major = False)
import numpy as np
v_x = np.ones([1, 1, 28, 28, 3])
c_v = np.ones([1, 28, 28, 32])
h_v = np.ones([1, 28, 28, 32])
feed_dict = {x: v_x, c_input: c_v, h_input: h_v}
init_op = tf.variables_initializer(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))
sess = tf.Session()
sess.run(init_op)
test = sess.run(lstm_output, feed_dict = feed_dict)
```
"
22945,Can't install form pip on MacOS Mojave and build from source failed,"@tesorflowbutler asked nicely so I added this section :)

Have I written custom code: N/A
OS Platform and Distribution:
MacOS Mojave
TensorFlow installed from: 
- source, tag v1.11.0
- source, branch master 
- pip, 1.11
TensorFlow version: latest or 1.11.0
Bazel version: 
0.17.2 via brew
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: I’ve tried a couple of things, listed below.
Mobile device: N/A

—-

When I tried `pip3 install --user --upgrade tensorflow`
it didn't worked see

```
pip3 install --user --upgrade tensorflow

Collecting tensorflow
  Could not find a version that satisfies the requirement tensorflow (from versions: )
No matching distribution found for tensorflow
```

when trying `bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`

it fails for

```
9 warnings generated.
ERROR: /private/var/tmp/_bazel_kobi.kadosh/2314413b591ef152edf49d9023c8bc65/external/protobuf_archive/BUILD:659:1: C++ compilation of rule '@protobuf_archive//:python/google/protobuf/pyext/_message.so' failed (Exit 1)
external/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc:172:13: error: assigning to 'char *' from incompatible type 'const char *'
        if (PyString_AsStringAndSize(key, &name, &name_size) < 0) {
            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc:69:22: note: expanded from macro 'PyString_AsStringAndSize'
       ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \
                     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc:189:13: error: assigning to 'char *' from incompatible type 'const char *'
        if (PyString_AsStringAndSize(key, &camelcase_name, &name_size) < 0) {
            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc:69:22: note: expanded from macro 'PyString_AsStringAndSize'
       ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \
                     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2 errors generated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 218.358s, Critical Path: 49.61s
INFO: 1348 processes: 1348 local.
FAILED: Build did NOT complete successfully
```

the only thing allowed me to get it installed was: 

`pip3 install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.11.0-py3-none-any.whl`

which I saw here
https://qiita.com/nahshi/items/fcf4898f7c45f11a5c63

running this

`python3 -c ""import tensorflow as tf; print(tf.__version__)""`

output that it didn't match.. I guess that my only option is to compile.. but I can't get it to work..

```
/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7
  return f(*args, **kwds)
1.11.0
```

Any help here would be greatly appreciated ^_^"
22942,"Keras tensorflow : Freeze calibrated mode, extract graph, input and output tensors","I create a tensorflow medel using keras embedded in tensorflow:

    import tensorflow as tf
    from tensorflow.python.keras.models import Sequential
    from tensorflow.python.keras.layers import Dense 
    model = Sequential()
    model.add(Dense(input_dim=input_dim, units=hidden_dim))
    model.add(Dense(units=output_dim, activation='linear'))
    model.compile(loss='mse', optimizer='adam')

After fitting the model, I want to freeze it and to extract the graph, the input tensor, and the output tensor. 
    
    session = tf.keras.backend.get_session()
    output_node_names = [node.op.name for node in model.outputs]
    graphdef = tf.graph_util.convert_variables_to_constants(session, session.graph_def, 
    output_node_names)

    with tf.Graph().as_default() as graph:
        tf.import_graph_def(graphdef)
        input_tensor = graph.get_tensor_by_name(graph.get_operations()[0].name+':0')
        output_tensor = graph.get_tensor_by_name(graph.get_operations()[-1].name+':0')


So that after the above code, the variable `graph` will contain the graph, and `input_tensor` and `output_tensor` will contain respectively the  input and output of the graph.

The graph does not offer a method to get its input and output tensors, hence my complicated method above. But it does not work.

Can you help please or advise a better way to do it?

Using Tensorflow 1.5"
22940,Tensorflow 1.11.0 is still looking for cublas 9.0 (and other libraries) despite of having installed CUDA 10,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 18.04

- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:

pip3 install tensorflow-gpu==1.11.0

- **TensorFlow version (use command below)**:
- **Python version**:

I use a local installation: /home/ivan/Python/Python-3.6.5

- **Bazel version (if compiling from source)**:

- **GCC/Compiler version (if compiling from source)**:

- **CUDA/cuDNN version**:
CUDA 10.0 and CuDNN 7.3.1 compatible with CUDA 10.0

- **GPU model and memory**:

NVIDIA Corporation GM107GL [Quadro K2200] with 65 GB of RAM

- **Exact command to reproduce**:
import tensorflow as tf

It produces this error message;

Traceback (most recent call last):
  File ""Main_55_Lithofacies_Classification.py"", line 19, in <module>
    import tensorflow as tf
  File ""/home/imarroquin/Python/Python-3.6.5/lib/python3.6/site-packages/tensorflow/__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/home/imarroquin/Python/Python-3.6.5/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/imarroquin/Python/Python-3.6.5/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/imarroquin/Python/Python-3.6.5/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/imarroquin/Python/Python-3.6.5/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/imarroquin/Python/Python-3.6.5/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/imarroquin/Python/Python-3.6.5/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/imarroquin/Python/Python-3.6.5/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory


You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

Collecting system information...
Traceback (most recent call last):
  File ""/home/imarroquin/Python/Python-3.6.5/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/imarroquin/Python/Python-3.6.5/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/imarroquin/Python/Python-3.6.5/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/imarroquin/Python/Python-3.6.5/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/imarroquin/Python/Python-3.6.5/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Tensorflow 1.11.0 is trying to load cublas from CUDA 9.0. Unfortunately, I can't install CUDA 9.0 on Ubuntu 18.04 because is not supported by Nvidia (https://devtalk.nvidia.com/default/topic/1036640/cuda-setup-and-installation/cuda-9-0-install-on-ubuntu-18-04-quot-unmet-dependencies-quot-/ )

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22939,LSTMBlockFusedCell not supported by optimize_for_inference?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes, https://github.com/mozilla/DeepSpeech/blob/963edc12deeaa9d2307c3c352d7471a97057e6f2/DeepSpeech.py#L385-L480 and https://github.com/mozilla/DeepSpeech/blob/963edc12deeaa9d2307c3c352d7471a97057e6f2/DeepSpeech.py#L1786-L1824
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian/Sid
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary and source
- **TensorFlow version (use command below)**: r1.11 and current master (9e0fa9578638f9147c0b180e6ea89d67d5c0bae3)
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.15.2
- **GCC/Compiler version (if compiling from source)**: 8.2 (Debian Sid)
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: 

### Describe the problem
 - Train a model using DeepSpeech
 - Optimize with: `python tensorflow/python/tools/optimize_for_inference.py --frozen_graph true --input ~/tmp/deepspeech-fr/model/output_graph.pb --input_names input_node,input_lengths,previous_state_c,
previous_state_h --output_names logits --output ~/tmp/deepspeech-fr/model/output_graph_opt.pb`
 - `optimize_for_inference.py` runs without any issue
 - Load and run the model for inference results in `Invalid argument: Input 0 of node lstm_fused_cell/Max was passed float from input_lengths:0 incompatible with expected int32.`"
22938,RuntimeError: TOCO failed see console for info.,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22936,Invalid loop strucure,"I had converted a h5 file to pb file, then while loading the pb file I get the following error.

Traceback (most recent call last):
  File ""import_model.py"", line 244, in <module>
    detections = sess.run(detectionsT, feed_dict={img_ph: molded_images, img_meta_ph: image_metas})
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 877, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1272, in _do_run
    run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Invalid loop structure: Loop ""mrcnn_detection/map/while/while_context"" has more than one LoopCond node: ""mrcnn_detection/map/while/LoopCond_1"" and ""mrcnn_detection/map/while/LoopCond"". This is an internal bug, please file a bug report with instructions on how to reproduce the error.


Have I written custom code - No, forked from - https://github.com/GustavZ/Mobile_Mask_RCNN
OS Platform and Distribution- linux ubuntu 16.04
TensorFlow installed from - using pip
Bazel version- N/A
CUDA/cuDNN version- N/A
GPU model and memory- N/A
Exact command to reproduce - convert the h5 file to pb and then load the pb file
Mobile device - N/A"
22935,"(Feature request) Add synthetic gradient training per ""Decoupled Neural Interfaces using Synthetic Gradients""","## Feature request
### Related papers
https://arxiv.org/abs/1703.00522
### Related blog : https://deepmind.com/blog/decoupled-neural-networks-using-synthetic-gradients/



**System information**
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No

**OS Platform and Distribution (e.g., Linux Ubuntu 16.04):**

VERSION=""16.04.4 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

**Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:**
No

**TensorFlow installed from (source or binary):**
source

**TensorFlow version (use command below):**
tf.VERSION = 1.9.0

**Python version:** python 2.7

**Bazel version (if compiling from source):**
0.11.1

**GCC/Compiler version (if compiling from source):**
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609

**CUDA/cuDNN version:**
9.1

**GPU model and memory:**
Tesla K80

**Exact command to reproduce:**
( No command)


"
22933,Tensor .eval() on loaded tensor,"Hi, I can convert tensor to numpy array, when creating a new const
np_array = np.random.rand(3, 2)
with tf.Session() as sess:
    tensor = tf.constant(np_array)
    numpy_array_2 = tensor.eval()
    print(numpy_array_2)

works fine, everything is ok, but when I load a saved tensor - this no longer works

loaded_graph = tf.Graph()

with tf.Session(graph=loaded_graph) as sess:
    loader = tf.train.import_meta_graph('checkpoints/vgg_16/test_animals/model-18000.meta')
    loader.restore(sess, 'checkpoints/vgg_16/test_animals/model-18000')
        
    tensor = loaded_graph.get_tensor_by_name('conv1_1/Conv2D:0')
    #tf.train.start_queue_runners(sess)    
    arr = tensor.eval()

at this point it freezes, I have tried running .start_queue_runners(sess) before, but it says that session is closed, if I run it before .import_meta_graph(path), .eval() is still frozen.

Is there any other way to get numbers from that tensor except sending it to numpy array?
What should I do to get .eval() working?"
22931,Reindex broken in tf docker image (v1.11.0) -> Broken numpy dependency!,"# TLDR

The pandas, which is distributed with the tf docker image is broken!

# Description

I'm new to tf and I'm walking thru the [Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/ml-intro) at the moment. [First Steps with TF](https://developers.google.com/machine-learning/crash-course/first-steps-with-tensorflow/programming-exercises) point me to the [Quick Introduction to pandas](https://colab.research.google.com/notebooks/mlcc/intro_to_pandas.ipynb?utm_source=mlcc&utm_campaign=colab-external&utm_medium=referral&utm_content=pandas-colab&hl=de) notebook. 

## Run example on colab

 Based on this. I created the following MWE on colab (actually, it is only pandas code):

https://colab.research.google.com/drive/19uDE_H4AtpLaEL6INrRrDMXkdANsNr69

It basically just does the following: 

1) Import the california housing data set
2) reorder the data set
3) prints the merged dataset

If you run my notebook, you can see that step 3 prints up to 20 different values for the data set (makred with `left only` and `right only`):

```
`   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \
0      -114.3      34.2                15.0       5612.0          1283.0   
1      -114.5      34.4                19.0       7650.0          1901.0   
2      -114.6      33.7                17.0        720.0           174.0   
3      -114.6      33.6                14.0       1501.0           337.0   
4      -114.6      33.6                20.0       1454.0           326.0   
..        ...       ...                 ...          ...             ...   
15     -117.3      33.2                13.0       3619.0           791.0   
16     -118.3      33.8                25.0       4177.0           832.0   
17     -117.7      34.0                25.0       1859.0           463.0   
18     -118.1      34.0                50.0       1146.0           238.0   
19     -118.5      34.0                41.0       1240.0           320.0   

    population  households  median_income  median_house_value      _merge  
0       1015.0       472.0            1.5             66900.0   left_only  
1       1129.0       463.0            1.8             80100.0   left_only  
2        333.0       117.0            1.7             85700.0   left_only  
3        515.0       226.0            3.2             73400.0   left_only  
4        624.0       262.0            1.9             65500.0   left_only  
..         ...         ...            ...                 ...         ...  
15      1759.0       806.0            2.8             98500.0  right_only  
16      2123.0       789.0            5.1            446800.0  right_only  
17      1070.0       374.0            2.5            187500.0  right_only  
18       579.0       213.0            3.0            172600.0  right_only  
19       711.0       304.0            3.3            318100.0  right_only  
```

## Run example on local python

In have Python 3.6.6 and I install tf with `pip install  tensorflow`. 

Now I run the same code in a python script as follows (on Windows 10):

```
python Reorder.py
```

The reordered data is exactly the same as the imported data (all are marked with `both`):

```
longitude  latitude  housing_median_age  total_rooms  total_bedrooms  population  households  median_income  median_house_value _merge
0     -114.3      34.2                15.0       5612.0          1283.0      1015.0       472.0            1.5             66900.0   both
1     -114.5      34.4                19.0       7650.0          1901.0      1129.0       463.0            1.8             80100.0   both
2     -114.6      33.7                17.0        720.0           174.0       333.0       117.0            1.7             85700.0   both
3     -114.6      33.6                14.0       1501.0           337.0       515.0       226.0            3.2             73400.0   both
4     -114.6      33.6                20.0       1454.0           326.0       624.0       262.0            1.9             65500.0   both
5     -114.6      33.6                29.0       1387.0           236.0       671.0       239.0            3.3             74000.0   both
6     -114.6      33.6                25.0       2907.0           680.0      1841.0       633.0            2.7             82400.0   both
7     -114.6      34.8                41.0        812.0           168.0       375.0       158.0            1.7             48500.0   both
8     -114.6      33.6                34.0       4789.0          1175.0      3134.0      1056.0            2.2             58400.0   both
9     -114.6      34.8                46.0       1497.0           309.0       787.0       271.0            2.2             48100.0   both
```

## Run example in tf docker container

Now I run the same python script in the [tf docker container](https://hub.docker.com/r/tensorflow/tensorflow/) (v1.11.0) as follows (on Windows 10):

```bash
docker run --rm -it -v C:\Users\boldt\docker\tf\scripts\:/scripts tensorflow/tensorflow:1.11.0-py3 python /scripts/Reorder.py
```

The result is the same. The reordered data is exactly the same as the imported data (all are marked with `both`):

# Conclusion

Because of this, I am unable to randomize the data set and split it in a training and a test set using the tf docker container. Currently, my more complex example always results the same training and test sets."
22930,absl not installed as it is included in stringpiece.h,https://github.com/tensorflow/tensorflow/blob/8201ea4dc1cdd9283121f58b48ba7237a05b220e/tensorflow/core/lib/core/stringpiece.h#L34
22928,Tensorflow setup,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22927,Feature request: Import position of dense_to_sparse and sparse_to_dense.,"- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 7 
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10.1
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: No
- **GCC/Compiler version (if compiling from source)**: No
- **CUDA/cuDNN version**: cuda9.0/cuDNN7.0
- **GPU model and memory**: 1080ti
- **Exact command to reproduce**: No

---- 


First of all, I have to appreciate your development. I have a trivial request related to python interface.
Current implementation is the following,

```python
tf.sparse_to_dense
tf.contrib.layers.dense_to_sparse
```

However, This implementation is counterintuitive. I think the following code is more intuitive.

```python
tf.sparse_to_dense
tf.dense_to_sparse
```
"
22926,Feature Request: GPUOptions for Go binding,"Current implementation of Go binding can not specify options.

GPUOptions struct is in internal package. And `go generate` doesn't work for protobuf directory. So we can't specify GPUOptions for `NewSession`.
"
22925,"""#warning ""crt/link.stub is an internal header"" error when compiling TensorFlow w/ CUDA 10","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 53faa313b7628cd8c9fbb836544cc6482cafb7a4
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.15.2
- **GCC/Compiler version (if compiling from source)**: 7.3.0
- **CUDA/cuDNN version**: CUDA 10.0, cudnn 7.3
- **GPU model and memory**: V100
- **Exact command to reproduce**:

```
yes '' | TF_NEED_CUDA=1 TF_CUDA_VERSION=10.0 CUDNN_INSTALL_PATH=/usr/local/cudnn NCCL_INSTALL_PATH=/usr/local/nccl_2.3.4-1+cuda10.0_x86_64  ./configure
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
```

### Describe the problem
Starting at commit 53faa313b7628cd8c9fbb836544cc6482cafb7a4, when trying to compile TensorFlow with CUDA 10.0, I get an error stating ""#warning ""crt/link.stub is an internal header file and must not be used directly"". The full error output is:

```
ERROR: /home/reedwm/.cache/bazel/_bazel_reedwm/f53335a0bee8ade861f80a7955d79beb/external/nccl_archive/BUILD.bazel:139:1: C++ compilation of rule '@nccl_archive//:device_code' failed (Exit 1)
bazel-out/k8-opt/bin/external/nccl_archive/device_code.cc:19:2: warning: #warning ""crt/link.stub is an internal header file and must not be used directly.  Please use cuda_runtime_api.h or cuda_runtime.h instead."" [-Wcpp]
 #warning ""crt/link.stub is an internal header file and must not be used directly.  Please use cuda_runtime_api.h or cuda_runtime.h instead.""
  ^~~~~~~
bazel-out/k8-opt/bin/external/nccl_archive/device_code.cc: In function 'void __cudaRegisterLinkedBinary(const __fatBinC_Wrapper_t*, void (*)(void**), void*)':
bazel-out/k8-opt/bin/external/nccl_archive/device_code.cc:140:5: error: '__NV_EXTRA_INITIALIZATION' was not declared in this scope
     __NV_EXTRA_INITIALIZATION
     ^~~~~~~~~~~~~~~~~~~~~~~~~
bazel-out/k8-opt/bin/external/nccl_archive/device_code.cc:144:5: error: '__NV_EXTRA_FINALIZATION' was not declared in this scope
     __NV_EXTRA_FINALIZATION
     ^~~~~~~~~~~~~~~~~~~~~~~
bazel-out/k8-opt/bin/external/nccl_archive/device_code.cc:145:23: warning: statement has no effect [-Wunused-value]
     for (__i = 0; __i < NUM_PRELINKED_OBJECTS; ++__i) {
bazel-out/k8-opt/bin/external/nccl_archive/device_code.cc:145:53: error: expected ';' before ')' token
     for (__i = 0; __i < NUM_PRELINKED_OBJECTS; ++__i) {
                                                     ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.

```
This does not occur before 53faa313b7628cd8c9fbb836544cc6482cafb7a4. Letting TensorFlow download and build NCCL does not resolve this issue.

@chsigg, can you resolve this?"
22920,LocalMaster WaitForNotification cause master to hang,"Could not understand why here we need to WaitForNotification after timeout?

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/local_master.cc#L39

This wait for notification could cause master or worker could not exit normally, the later exit one will hang at this step, because nobody will give them a message.

The comment says that the call has borrowed pointers to request and response messages, I can not understand the call means what? which object and which class? and where it borrows its pointers?"
22918,"build failed, ubuntu 16.04 with python 3.7","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 16.04.4 x64
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.12.0rc0
- **Python version**: 3.7
- **Bazel version (if compiling from source)**: 0.18.0rc9
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 10.0 / 7.3
- **GPU model and memory**: gtx1080Ti GDDR5X 11GB X 7
- **Exact command to reproduce**: 
configure and build


### Describe the problem
configure and build

### Source code / logs
ERROR: /home/wmind5/.cache/bazel/_bazel_wmind5/86e1c308a3d29b47174eb34b7539b75b/external/protobuf_archive/BUILD:659:1: C++ compilation of rule '@protobuf_archive//:python/google/protobuf/pyext/_message.so' failed (Exit 1)
external/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc: In function âbool google::protobuf::python::descriptor::_GetItemByKey(google::protobuf::python::PyContainer*, PyObject*, const void**)â:
external/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc:69:45: error: invalid conversion from âconst char*â to âchar*â [-fpermissive]
        ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \
                                             ^
external/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc:172:13: note: in expansion of macro âPyString_AsStringAndSizeâ
         if (PyString_AsStringAndSize(key, &name, &name_size) < 0) {
             ^
external/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc:69:45: error: invalid conversion from âconst char*â to âchar*â [-fpermissive]
        ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \
                                             ^
external/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc:189:13: note: in expansion of macro âPyString_AsStringAndSizeâ
         if (PyString_AsStringAndSize(key, &camelcase_name, &name_size) < 0) {
             ^
At global scope:
cc1plus: warning: unrecognized command line option â-Wno-writable-stringsâ
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /home/wmind5/repo/tensorflow/tensorflow/python/estimator/api/BUILD:12:1 C++ compilation of rule '@protobuf_archive//:python/google/protobuf/pyext/_message.so' failed (Exit 1)
INFO: Elapsed time: 142.995s, Critical Path: 18.81s
INFO: 482 processes: 482 local.
FAILED: Build did NOT complete successfully

"
22908,r1.5 tf_version_script.lds not found,"### System information
== cat /etc/issue ===============================================
Linux sc-phy-370909L 4.15.0-34-generic #37~16.04.1-Ubuntu SMP Tue Aug 28 10:44:06 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.5 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux sc-phy-370909L 4.15.0-34-generic #37~16.04.1-Ubuntu SMP Tue Aug 28 10:44:06 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.15.1)
numpydoc (0.6.0)
protobuf (3.6.0)
tensorflow (1.10.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""tensorflow/python/pywrap_tensorflow.py"", line 25, in <module>
    from tensorflow.python.platform import self_check
ImportError: No module named platform

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tools/tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================

### Describe the problem
A stock tensorflow r1.5 build fails with ""tf_version_script.lds not found.""

### Source code / logs
ERROR: /home/amcp011/Documents/tensorflow/tensorflow/python/BUILD:3059:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed (Exit 1): gcc failed: error executing command 
  (cd /home/amcp011/.cache/bazel/_bazel_amcp011/39f5672abd825ffe3e88afa17f90eb9e/execroot/org_tensorflow && \
  exec env - \
    PATH=/home/amcp011/local/bin:/home/amcp011/bin:/home/amcp011/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/amcp011/anaconda3/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/local/lib/python3.5/dist-packages \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL_SYCL=0 \
  /usr/bin/gcc -shared -o bazel-out/k8-py3-opt/bin/tensorflow/python/_pywrap_tensorflow_internal.so '-Wl,-rpath,$ORIGIN/../../_solib_k8/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow' -Lbazel-out/k8-py3-opt/bin/_solib_k8/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow -Wl,--version-script //tensorflow:tf_version_script.lds '-Wl,-rpath,$ORIGIN/,-rpath,$ORIGIN/..' -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread '-fuse-ld=gold' -Wl,-no-as-needed -Wl,-z,relro,-z,now -B/usr/bin -B/usr/bin -pass-exit-codes -Wl,--gc-sections -Wl,@bazel-out/k8-py3-opt/bin/tensorflow/python/_pywrap_tensorflow_internal.so-2.params)
gcc: error: //tensorflow:tf_version_script.lds: No such file or directory
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 97.480s, Critical Path: 10.96s
INFO: 2 processes: 2 local.
FAILED: Build did NOT complete successfully

"
22903,tfcompile of a fitted keras model,"Hello,
I want to define a simple model in keras, fit it, then tfcompile it.

    import tensorflow as tf
    from tensorflow.python.keras.models import Sequential
    from tensorflow.python.keras.layers import Dense
    model = Sequential()
    model.add(Dense(input_dim=X.shape[1], units=hidden_dim))
    model.add(Dense(units=Y.shape[1], activation='linear'))
    model.compile(loss='mse', optimizer='adam')
    model.fit(X, Y, batch_size=100)

Can you please give an example of how to tfcopile this fitted model, or at least give some steps (for example, should the graph be frozen before using tfcompile?)


"
22902,Linker error when compiling from head of master on MacOS,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS Mojave and High Sierra
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: master
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.17..2-homebrew
- **GCC/Compiler version (if compiling from source)**: 
```
clang -v
Apple LLVM version 10.0.0 (clang-1000.11.45.2)
```
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: 
```
bazel test --test_output=all --nocache_test_results --config=opt tensorflow/compiler/tests:cpu_tests
```
(Configured with XLA support enabled)
### Describe the problem
When trying to run the above command I get:
```
ld: can't open -exported_symbols_list file: -filelist
clang: error: linker command failed with exit code 1 (use -v to see invocation)
```

### Source code / logs
After applying this diff:
```diff --git a/tensorflow/tensorflow.bzl b/tensorflow/tensorflow.bzl
index df15914233..8de9c7cfdd 100644
--- a/tensorflow/tensorflow.bzl
+++ b/tensorflow/tensorflow.bzl
@@ -1636,8 +1636,7 @@ def tf_py_wrap_cc(
     )
     extra_linkopts = select({
         ""@local_config_cuda//cuda:darwin"": [
-            ""-Wl,-exported_symbols_list"",
-            ""$(location %s.lds)"" % vscriptname,
+            ""-Wl,-exported_symbols_list,$(location %s.lds)"" % vscriptname,
         ],
         clean_dep(""//tensorflow:windows""): [],
         ""//conditions:default"": [
```

and running `bazel test --test_output=all --nocache_test_results --config=opt tensorflow/compiler/tests:cpu_tests`

I get lots of warnings during linking:
```
ld: warning: cannot export hidden symbol std::__1::shared_ptr<tensorflow::AWSSha256HMACOpenSSLImpl>::__enable_weak_this(...) from bazel-out/darwin-opt/bin/tensorflow/core/platform/s3/libaws_crypto.lo(aws_crypto.o)
ld: warning: cannot export hidden symbol std::__1::shared_ptr<tensorflow::AWSLogSystem>::__enable_weak_this(...) from bazel-out/darwin-opt/bin/tensorflow/core/platform/s3/libaws_logging.lo(aws_logging.o)
ld: warning: cannot export hidden symbol std::__1::shared_ptr<tensorflow::Notification>::__enable_weak_this(...) from bazel-out/darwin-opt/bin/tensorflow/core/grappler/libutils.a(utils.o)
```

And then when the tests are ran I get errors such as:

```
2018-10-11 13:02:47.136859: I tensorflow/compiler/xla/service/service.cc:157]   StreamExecutor device (0): <undefined>, <undefined>
*** Received signal 10 ***
*** BEGIN MANGLED STACK TRACE ***
0   libtensorflow_framework.so          0x00000001200870a7 _ZN10tensorflow7testingL17StacktraceHandlerEiP9__siginfoPv + 183
1   libsystem_platform.dylib            0x00007fff66049b3d _sigtramp + 29
2   ???                                 0x0000000000000000 0x0 + 0
3   _pywrap_tensorflow_internal.so      0x00000001143583a9 _ZNSt3__110__function6__funcIZN3xla3cpu13CpuExecutable24ExecuteAsyncOnStreamImplEPKNS2_27ServiceExecutableRunOptionsEN4absl4SpanIKPKNS2_12ShapedBufferEEEPNS2_19HloExecutionProfileEE12AsyncRunTaskNS_9allocatorISH_EEFvvEEclEv + 73
4   libtensorflow_framework.so          0x000000012032bd09 _ZNSt3__110__function6__funcIZN15stream_executor4host10HostStream11EnqueueTaskENS_8functionIFvvEEEE12NotifiedTaskNS_9allocatorIS8_EES6_EclEv + 25
5   libtensorflow_framework.so          0x00000001200641da _ZN5Eigen26NonBlockingThreadPoolTemplIN10tensorflow6thread16EigenEnvironmentEE10WorkerLoopEi + 618
6   libtensorflow_framework.so          0x0000000120063e6f _ZNSt3__110__function6__funcIZN10tensorflow6thread16EigenEnvironment12CreateThreadENS_8functionIFvvEEEEUlvE_NS_9allocatorIS8_EES6_EclEv + 47
7   libtensorflow_framework.so          0x0000000120088a30 _ZNSt3__114__thread_proxyINS_5tupleIJNS_10unique_ptrINS_15__thread_structENS_14default_deleteIS3_EEEENS_8functionIFvvEEEEEEEEPvSB_ + 48
8   libsystem_pthread.dylib             0x00007fff6605233d _pthread_body + 126
9   libsystem_pthread.dylib             0x00007fff660552a7 _pthread_start + 70
10  libsystem_pthread.dylib             0x00007fff66051425 thread_start + 13
*** END MANGLED STACK TRACE ***

*** Begin stack trace ***
	tensorflow::CurrentStackTrace()
	tensorflow::testing::StacktraceHandler(int, __siginfo*, void*)
	_sigtramp

	std::__1::__function::__func<xla::cpu::CpuExecutable::ExecuteAsyncOnStreamImpl(xla::ServiceExecutableRunOptions const*, absl::Span<xla::ShapedBuffer const* const>, xla::HloExecutionProfile*)::AsyncRunTask, std::__1::allocator<xla::cpu::CpuExecutable::ExecuteAsyncOnStreamImpl(xla::ServiceExecutableRunOptions const*, absl::Span<xla::ShapedBuffer const* const>, xla::HloExecutionProfile*)::AsyncRunTask>, void ()>::operator()()
	std::__1::__function::__func<stream_executor::host::HostStream::EnqueueTask(std::__1::function<void ()>)::NotifiedTask, std::__1::allocator<stream_executor::host::HostStream::EnqueueTask(std::__1::function<void ()>)::NotifiedTask>, void ()>::operator()()
	Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int)
	std::__1::__function::__func<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'(), std::__1::allocator<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'()>, void ()>::operator()()
	void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, std::__1::function<void ()> > >(void*)
	_pthread_body
	_pthread_start
	thread_start
*** End stack trace ***
```

Note that `cc_wrapper.sh` calls `gcc` however I haven't changed it so it still points at Apple LLVM.

Note that there is an issue #22759 however that one uses custom code, where as this is HEAD of master."
22901,"We have removed this restriction. Please check out the code at head, or in the upcoming 1.4 RC. Please reopen if there are issues with it.",_Originally posted by @martinwicke in https://github.com/tensorflow/tensorflow/issues/12367#issuecomment-335320352_
22900,RTX 2080 issue with Tensorflow,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NaN
- **TensorFlow installed from (source or binary)**: Both 
- **TensorFlow version (use command below)**: 1.12.0-rc0-devel-gpu-py3 
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: Latest
- **GCC/Compiler version (if compiling from source)**: 5.x
- **CUDA/cuDNN version**: 9.0 / 10.0 (Tried Both)    | cuDNN : 7.2.x / 7.3.1
- **GPU model and memory**: RTX Geforce 2080
- **Exact command to reproduce**:  Running the most basic mnist dataset using keras with Tf backend

When I try running the code it starts and ends with a segmentation fault even without start of single epoch. The command nvidia-smi shows maximum usage of its volatile memory. I also restricted the gpu usage by a fraction of 0.5 still the problem persists.  I tried building the source files with cuda 9.0 / cudnn 7.2.1 in the docker environment yet the installed whl package produces the same error."
22899,Error reported to Coordinator: libnccl.so.2: cannot open shared object file: No such file or directory,"When I use tf-version 1.11, I found above error for distribute strategy on multi-gpus"
22898,Error building on Windows // no such package '@flatbuffers,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: from source
- **TensorFlow version (use command below)**: 1.11
- **Python version**: 3.6.6
- **Bazel version (if compiling from source)**: 0.17.2
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**: bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
I followed the instructions from https://www.tensorflow.org/install/source_windows 
When I tried to make a cpu only build with the command
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

I get the error:

C:\tensorflow>bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
Starting local Bazel server and connecting to it...
WARNING: Option 'experimental_shortened_obj_file_path' is deprecated
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@flatbuffers//': C:/users/xxxx/_bazel_xxxx/xv6zejqw/external/flatbuffers/tests (Directory not empty)
INFO: Elapsed time: 106.904s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (246 packages loaded)
"
22897,Conversion from pb to tflite fails,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: binary through pip
- **TensorFlow version (use command below)**: tf-nightly and tf 1.11 (updated today)
- **Python version**: 3.6.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: CPU only
- **GPU model and memory**:
- **Exact command to reproduce**:


### Describe the problem
I have a trained model and exportet it to a frozen pb file. After that, I optimized this frozen graph for inference. Now I want to convert this graph to a tflite model to use on Android.
The code I used to convert it is:
```
import tensorflow as tf

graph_def_file = ""graph_optimized.pb""
input_arrays = [""Placeholder""]
output_arrays = [""output""]

converter = tf.contrib.lite.TocoConverter.from_frozen_graph(
    graph_def_file, input_arrays, output_arrays, input_shapes={""Placeholder"" : [1, 227, 227, 3]})
tflite_model = converter.convert()
open(""save_path/converted_model.tflite"", ""wb"").write(tflite_model)
```

This fails with an error. I tried it both with my existing tensorflow installation and with a clean installation in a new virtualenv with tf-nightly like it was suggested in issue #22617 . Still the same errors happen
### Source code / logs

```
WARNING:tensorflow:From C:/Users/User/Documents/Test_export/export.py:8: TocoConverter.from_frozen_graph (from tensorflow.contrib.lite.python.lite) is deprecated and will be removed in a future version.
Instructions for updating:
Use `lite.TFLiteConverter.from_frozen_graph` instead.
2018-10-11 11:57:49.338944: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
Traceback (most recent call last):
  File ""C:/Users/User/Documents/Test_export/export.py"", line 9, in <module>
    tflite_model = converter.convert()
  File ""C:\Users\User\venv-tf-nightly\lib\site-packages\tensorflow\contrib\lite\python\lite.py"", line 453, in convert
    **converter_kwargs)
  File ""C:\Users\User\venv-tf-nightly\lib\site-packages\tensorflow\contrib\lite\python\convert.py"", line 348, in toco_convert_impl
    input_data.SerializeToString())
  File ""C:\Users\User\venv-tf-nightly\lib\site-packages\tensorflow\contrib\lite\python\convert.py"", line 135, in toco_convert_protos
    (stdout, stderr))
RuntimeError: TOCO failed see console for info.
b'Traceback (most recent call last):\r\n  File ""c:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\lite\\toco\\python\\tensorflow_wrap_toco.py"", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module(\'_tensorflow_wrap_toco\', [dirname(__file__)])\r\n  File ""c:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\imp.py"", line 297, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named \'_tensorflow_wrap_toco\'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File ""c:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\runpy.py"", line 193, in _run_module_as_main\r\n    ""__main__"", mod_spec)\r\n  File ""c:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\runpy.py"", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File ""C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\Scripts\\toco_from_protos.exe\\__main__.py"", line 5, in <module>\r\n  File ""c:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\lite\\toco\\python\\toco_from_protos.py"", line 22, in <module>\r\n    from tensorflow.contrib.lite.toco.python import tensorflow_wrap_toco\r\n  File ""c:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\lite\\toco\\python\\tensorflow_wrap_toco.py"", line 28, in <module>\r\n    _tensorflow_wrap_toco = swig_import_helper()\r\n  File ""c:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\lite\\toco\\python\\tensorflow_wrap_toco.py"", line 20, in swig_import_helper\r\n    import _tensorflow_wrap_toco\r\nModuleNotFoundError: No module named \'_tensorflow_wrap_toco\'\r\n'
None


Process finished with exit code 1
```
"
22894,"Can't use CTCBeamSearchDecoder in c++, LINK ERROR occur,BUG in CTCBeamSearchDecoder 's source code","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Pro 1803
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:N/A
- **TensorFlow installed from (source or binary)**: from source
- **TensorFlow version (use command below)**: r1.4  and r1.10
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: cmake 3.12.2
- **GCC/Compiler version (if compiling from source)**: VS 2015
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:N/A

### Describe the problem
 I want to use [CTCBeamSearchDecoder ](https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/core/util/ctc/ctc_beam_search.h), but there is a link error when I call CTCBeamSearchDecoder  constructor  function(`CTCBeamSearchDecoder<> decoder(num_classes, 10 * top_paths, &default_scorer);`).  And I comment the constructor  function, the error disappear.  I test the r1.4 tensorflow.lib complied by myself with cmake and r1.10 [download from github](https://github.com/fo40225/tensorflow-windows-wheel/tree/master/1.10.0/cpp) . **So I think there might be a bug in CTCBeamSearchDecoder  's source code.**	
### Source code / logs
This is the test code which is from [ctc_beam_search_test.cc](https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/core/util/ctc/ctc_beam_search_test.cc). The code is very simple.
```
#include ""third_party/eigen3/Eigen/Core""
//#include ""tensorflow/core/platform/env.h""
#include ""tensorflow/core/util/ctc/ctc_beam_search.h""
#include <vector>
void test()
{
	using tensorflow::ctc::CTCBeamSearchDecoder;
	using tensorflow::ctc::CTCDecoder;
	const int batch_size = 1;
	const int timesteps = 5;
	const int top_paths = 3;
	const int num_classes = 6;

	// Plain decoder using hibernating beam search algorithm.
	CTCBeamSearchDecoder<>::DefaultBeamScorer default_scorer;
        //COMMENT AND NO ERROR
	CTCBeamSearchDecoder<> decoder(num_classes, 10 * top_paths, &default_scorer);

	// Dictionary decoder, allowing only two dictionary words : {3}, {3, 1}.
	//DictionaryBeamScorer dictionary_scorer;
	//CTCBeamSearchDecoder<HistoryBeamState> dictionary_decoder(num_classes, top_paths, &dictionary_scorer);

	// Raw data containers (arrays of floats, ints, etc.).
	int sequence_lengths[batch_size] = { timesteps };
	float input_data_mat[timesteps][batch_size][num_classes] = {
		{ { 0, 0.6, 0, 0.4, 0, 0 } },
		{ { 0, 0.5, 0, 0.5, 0, 0 } },
		{ { 0, 0.4, 0, 0.6, 0, 0 } },
		{ { 0, 0.4, 0, 0.6, 0, 0 } },
		{ { 0, 0.4, 0, 0.6, 0, 0 } } };

	// The CTCDecoder works with log-probs.
	for (int t = 0; t < timesteps; ++t) {
		for (int b = 0; b < batch_size; ++b) {
			for (int c = 0; c < num_classes; ++c) {
				input_data_mat[t][b][c] = std::log(input_data_mat[t][b][c]);
			}
		}
	}

	// Plain output, without any additional scoring.
	std::vector<CTCDecoder::Output> expected_output = {
		{ { 1, 3 },{ 1, 3, 1 },{ 3, 1, 3 } },
	};

	// Dictionary outputs: preference for dictionary candidates. The
	// second-candidate is there, despite it not being a dictionary word, due to
	// stronger probability in the input to the decoder.
	std::vector<CTCDecoder::Output> expected_dict_output = {
		{ { 3 },{ 1, 3 },{ 3, 1 } },
	};

	// Convert data containers to the format accepted by the decoder, simply
	// mapping the memory from the container to an Eigen::ArrayXi,::MatrixXf,
	// using Eigen::Map.
	Eigen::Map<const Eigen::ArrayXi> seq_len(&sequence_lengths[0], batch_size);
	std::vector<Eigen::Map<const Eigen::MatrixXf>> inputs;
	inputs.reserve(timesteps);
	for (int t = 0; t < timesteps; ++t) {
		inputs.emplace_back(&input_data_mat[t][0][0], batch_size, num_classes);
	}

	// Prepare containers for output and scores.
	std::vector<CTCDecoder::Output> outputs(top_paths);
	for (CTCDecoder::Output& output : outputs) {
		output.resize(batch_size);
	}
	float score[batch_size][top_paths] = { { 0.0 } };
	Eigen::Map<Eigen::MatrixXf> scores(&score[0][0], batch_size, top_paths);
}
int main()
{
	test();
	return 0;
}

```
This is complier 's error log.
```
error LNK2001 unresolved external symbol  ""void __cdecl tensorflow::internal::MakeCheckOpValueString<unsigned char>(class std::basic_ostream<char,struct std::char_traits<char> > *,unsigned char const &)"" (??$MakeCheckOpValueString@E@internal@tensorflow@@YAXPEAV?$basic_ostream@DU?$char_traits@D@std@@@std@@AEBE@Z)	test1	C:\Users\46099\Desktop\testtf\test1\test1\main.obj	1
```
"
22893,Strange issue converting Tensorflow model to TFLite using toco,"

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.5 LTS
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: Tensorflow v1.9.0-rc2-5420-g411b9ba 1.11.0-rc1 
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

I have a tensorflow model, in two formats, the first in a frozen graph definition (pb) file, and the second in a Keras model (h5) file.

Via pb file:
```
toco --graph_def_file=./model.pb --output_file=./model.tflite --inference_type=FLOAT --input_type=FLOAT --input_arrays=input_1 --output_arrays=out0 --allow_custom_operators
```
Yields:
```
2018-10-09 12:24:55.039391: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 5705 operators, 7732 arrays (0 quantized)
2018-10-09 12:24:55.664965: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 5705 operators, 7732 arrays (0 quantized)
2018-10-09 12:24:56.867861: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 4908 operators, 9078 arrays (0 quantized)
2018-10-09 12:24:57.400037: F tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:1442] Check failed: axis < input_shape.dimensions_count() (90746776 vs. 4)
Aborted (core dumped)'
None
```
Interestingly the 90746776 (axis) value in the error changes between successive calls

Via the h5 file
```
toco --keras_model_file=./model.h5 --output_file=./model.tflite --inference_type=FLOAT --input_type=FLOAT --input_arrays=input_1 --output_arrays=out0 --allow_custom_ops
```
Yields:
```
Traceback (most recent call last):
  File ""/home/joseph/.local/bin/toco"", line 11, in <module>
    sys.exit(main())
  File ""/home/joseph/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 412, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/home/joseph/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 408, in run_main
    _convert_model(tflite_flags)
  File ""/home/joseph/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 100, in _convert_model
    converter = _get_toco_converter(flags)
  File ""/home/joseph/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 87, in _get_toco_converter
    return converter_fn(**converter_kwargs)
  File ""/home/joseph/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/lite.py"", line 368, in from_keras_model_file
    keras_model = _keras.models.load_model(model_file)
  File ""/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/saving.py"", line 230, in load_model
    model = model_from_config(model_config, custom_objects=custom_objects)
  File ""/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/saving.py"", line 310, in model_from_config
    return deserialize(config, custom_objects=custom_objects)
  File ""/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/keras/layers/serialization.py"", line 64, in deserialize
    printable_module_name='layer')
  File ""/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/keras/utils/generic_utils.py"", line 173, in deserialize_keras_object
    list(custom_objects.items())))
  File ""/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/network.py"", line 1298, in from_config
    process_layer(layer_data)
  File ""/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/network.py"", line 1284, in process_layer
    layer = deserialize_layer(layer_data, custom_objects=custom_objects)
  File ""/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/keras/layers/serialization.py"", line 64, in deserialize
    printable_module_name='layer')
  File ""/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/keras/utils/generic_utils.py"", line 173, in deserialize_keras_object
    list(custom_objects.items())))
  File ""/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/sequential.py"", line 340, in from_config
    model.add(layer)
  File ""/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/training/checkpointable/base.py"", line 474, in _method_wrapper
    method(self, *args, **kwargs)
  File ""/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/sequential.py"", line 175, in add
    output_tensor = layer(self.outputs[0])
  File ""/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 746, in __call__
    self.build(input_shapes)
  File ""/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/keras/layers/wrappers.py"", line 213, in build
    self.layer.build(tuple(child_input_shape))
  File ""/home/joseph/.local/lib/python3.5/site-packages/tensorflow/python/keras/layers/core.py"", line 933, in build
    raise ValueError('The last dimension of the inputs to `Dense` '
ValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`.
```"
22892,How to parse the caffe-generated LMDB datasets with tf.LMDBReader,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
PC
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
1.9.0
- **Python version**:
2.7.15
- **Bazel version (if compiling from source)**:
Not compiled from source
- **GCC/Compiler version (if compiling from source)**:
Not compiled from source
- **CUDA/cuDNN version**:
CUDA 9.0
- **GPU model and memory**:
GTX 1060 6G

### Describe the problem
After importation through the tf.LMDBReader(), we are able to get the key and value of the .mdb. But we cannot further parse the value through the tf.parse_single_example API as no sub-keys like ""image_raw"" or ""label"" existed in the Caffe-generated .mdb dataset. As in Caffe, the properties of the image or the label data are available with this data block converted to Caffe-defined Datum structure and we can get the width through methods like Datum.width(). So what we should do in tensorflow?

### Source code / logs
filename_queue = tf.train.string_input_producer(['path_to_the_data.mdb'], num_epochs=None)
reader = tf.LMDBReader()
cur_key_val, serialized_example = reader.read(filename_queue)
features = tf.parse_single_example(
        serialized_example,
        features={
             '?????': tf.VarLenFeature(tf.string),
             '?????': tf.VarLenFeature(tf.string)
         })
label = features['?????']
image = features['?????']"
22891,Cannot obtain the output feature map value of certain tensor in tensorflow-lite,"Hello, I am analysing each layer of mobilenet-ssd in tensorflow-lite. I am using python interpreter.
here is the sample of python script for accessing certain tensor of detect.tflite.

```
interpreter = interpreter_wrapper.Interpreter(model_path=resource_loader.get_path_to_datafile(""detect.tflite""))
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
interpreter.set_tensor(input_details[0]['index'], input_data)
interpreter.invoke()
tensorIndex  = 173
tmpTensor = interpreter._get_tensor_details(tensorIndex)
value = interpreter.get_tensor(tensorIndex)
valueShape = value.shape
tensorName = interpreter._get_tensor_details(tensorIndex)
LayerNames = tensorName['name']
plt.imshow(value)
plt.show()
```
in this way I want to check the output feature map of certain tensor. However, the output feature map seems show trash values all the time.

> Note: given python script can show the image_tensor (input image) and cannot obtain any detection results

How to check the other output feature map of certain tensor in tensorflow lite?

Thank you."
22890,Failed to load the native TensorFlow runtime.,"please help me

pygame 1.9.4
Hello from the pygame community. https://www.pygame.org/contribute.html
2018-10-11 18:04:48.044220: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2018-10-11 18:04:48.293316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties:
name: GeForce GTX 1060 major: 6 minor: 1 memoryClockRate(GHz): 1.6705
pciBusID: 0000:01:00.0
totalMemory: 6.00GiB freeMemory: 4.97GiB
2018-10-11 18:04:48.305478: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2018-10-11 18:04:49.064648: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-10-11 18:04:49.075159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0
2018-10-11 18:04:49.079229: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N
2018-10-11 18:04:49.084288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4699 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-10-11 18:05:04.242432: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.59G (4927577088 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
WARNING:tensorflow:From D:\Project\practice_pysc2\RDRL\rl\model.py:63: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
W1011 18:05:41.685944 12328 tf_logging.py:125] From D:\Project\practice_pysc2\RDRL\rl\model.py:63: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
Traceback (most recent call last):
  File ""D:\Engineer\Anaconda\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""D:\Engineer\Anaconda\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""D:\Engineer\Anaconda\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""D:\Engineer\Anaconda\Anaconda3\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""D:\Engineer\Anaconda\Anaconda3\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: 页面文件太小，无法完成操作。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""D:\Engineer\Anaconda\Anaconda3\lib\multiprocessing\spawn.py"", line 105, in spawn_main
    exitcode = _main(fd)
  File ""D:\Engineer\Anaconda\Anaconda3\lib\multiprocessing\spawn.py"", line 114, in _main
    prepare(preparation_data)
  File ""D:\Engineer\Anaconda\Anaconda3\lib\multiprocessing\spawn.py"", line 225, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File ""D:\Engineer\Anaconda\Anaconda3\lib\multiprocessing\spawn.py"", line 277, in _fixup_main_from_path
    run_name=""__mp_main__"")
  File ""D:\Engineer\Anaconda\Anaconda3\lib\runpy.py"", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File ""D:\Engineer\Anaconda\Anaconda3\lib\runpy.py"", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File ""D:\Engineer\Anaconda\Anaconda3\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""D:\Project\practice_pysc2\RDRL\main.py"", line 3, in <module>
    import tensorflow as tf
  File ""D:\Engineer\Anaconda\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""D:\Engineer\Anaconda\Anaconda3\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""D:\Engineer\Anaconda\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""D:\Engineer\Anaconda\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""D:\Engineer\Anaconda\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""D:\Engineer\Anaconda\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""D:\Engineer\Anaconda\Anaconda3\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""D:\Engineer\Anaconda\Anaconda3\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: 页面文件太小，无法完成操作。


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
2018-10-11 18:05:48.218204: W tensorflow/core/framework/op_kernel.cc:1275] OP_REQUIRES failed at constant_op.cc:207 : Resource exhausted: OOM when allocating tensor with shape[76800,256] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu"
22889,train with multi-gpu with MirroredStrategy will hang-up,"### System information
Have I written custom code: N/A
OS Platform and Distribution: CentOS Linux release 7.3.1611
TensorFlow installed from:  (pip install tf-nightly-gpu)
TensorFlow version: Tensorflow('v1.9.0-rc2-5345-g57d31aa599', '1.12.0-dev20181005')
Bazel version: N/A
GPU model and memory: Tesla P40 24G
Exact command to reproduce: N/A
Mobile device: N/A
CUDA/cuDNN version: cuda 9.0 with cudnn7.1.4

I train with tensorflow for multi-gpu with MirroredStrategy and estimator. I got the problem:
when I set the distribute mode with the following code it will got stuck after runing some training steps:
```
distribution = tf.contrib.distribute.MirroredStrategy()
config = tf.estimator.RunConfig(train_distribute=distribution)
estimator = tf.estimator.Estimator(model_fn=mymodel_fn, model_dir='logs',
        config=config)
```
bug when I run without distribute mode like this:
```
distribution = tf.contrib.distribute.MirroredStrategy()
config = tf.estimator.RunConfig()
estimator = tf.estimator.Estimator(model_fn=mymodel_fn, model_dir='logs',
        config=config)
```
It runs ok. Why? 
Is that a bug of MirroredStrategy?
"
22887,[TensorRT] use op_name as op_type_name.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: None
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: centos 7.2
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: None
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.11.0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:  0.16.1
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: cudnn 7.0.4
- **GPU model and memory**: p40 & 20GB
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem

1. run tensorflow 1.11.0 with tensorrt 4.0.6.
2. TRTEngineOp is registered. 
3. I find the tf  use `op_name` as `op_type_name`.

```
Not found: Op type not registered 'TitleLSTM/my_trt_op_2_native_segment' in binary
```
I think  'TitleLSTM/my_trt_op_2_native_segment' is a op_name, this op's type is 'TRTEngineOp'

### Source code / logs
2018-09-30 17:23:37.482200: I external/org_tensorflow/tensorflow/core/framework/op.cc:103] Not found: Op type not registered 'TitleLSTM/my_trt_op_2_native_segment' in binary running on jshd_40_73. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed."
22886,ImportError: DLL load failed: The specified module could not be found.,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No this is just a simple installation and import of tensorflow
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
Desktop PC
- **TensorFlow installed from (source or binary)**:
pip install tensorflow-gpu
- **TensorFlow version (use command below)**:
tensorflow-gpu 1.11.0
- **Python version**:
python 3.5.5
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: CUDA v10.0 / cudnn-10.0-windows10-x64-v7.3.1.20
- **GPU model and memory**:
Geforce 770 2GB
- **Exact command to reproduce**:
import tensorflow as tf

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

When I import tensorflow I immediately get an error (see below). I have looked at discussions on stack overflow and seem to have the following path variables to get at the necessary DLLs including msvcp140.dll

The problem began after I upgraded my tensorflow, CUDA, and cuDNN

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

Python 3.5.5 | packaged by conda-forge | (default, Feb 13 2018, 06:15:35) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\Ulric\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Ulric\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Ulric\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Ulric\Anaconda3\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Ulric\Anaconda3\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Ulric\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Ulric\Anaconda3\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Ulric\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Ulric\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Ulric\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Ulric\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Ulric\Anaconda3\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Ulric\Anaconda3\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
"
22885,Segmentation fault running layers.conv2d,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Centos 7.5
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA
- **TensorFlow installed from (source or binary)**: Anaconda
- **TensorFlow version (use command below)**: 1.10.0
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: python test.py

### Describe the problem
I am trying to implement a CNN using tensorflow, but I started running into segmentation fault with increasing input size to layers.conv2d. In the provided simple test script, I can successfully run it if n_H=3000. But I got a segmentation fault when I set n_H=4000. In addition, if I run it without layars.conv2d by setting with_conv=False, the script runs successfully. I don't know if this is my lack of understanding with layers.conv2d or there is something more to this.

### Source code / logs
Here is the output when I get a segmentation fault:
```
$ python test.py
(100, 4000, 100, 1) (100, 8)
2018-10-10 11:57:23.825704: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2
2018-10-10 11:57:23.827653: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
Fatal Python error: Segmentation fault

Thread 0x00007fea9af1a740 (most recent call first):
  File ""/home/seng/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1350 in _call_tf_sessionrun
  File ""/home/seng/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1263 in _run_fn
  File ""/home/seng/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1278 in _do_call
  File ""/home/seng/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1272 in _do_run
  File ""/home/seng/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1100 in _run
  File ""/home/seng/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 877 in run
  File ""test.py"", line 43 in <module>
Segmentation fault (core dumped)
```

Here is the simple test script test.py:
```
import faulthandler; faulthandler.enable()
import numpy as np
import tensorflow as tf

seed = 42
tf.reset_default_graph()
tf.set_random_seed(seed)
np.random.seed(seed)

num_examples = 100
n_H = 4000
with_conv = True

# generate training data
X_train = np.random.randn(num_examples*n_H*100).reshape(num_examples, n_H, 100, 1)
Y_train = np.random.randn(num_examples*8).reshape(num_examples, 8)    
print(X_train.shape, Y_train.shape)

# create placeholders
X = tf.placeholder(tf.float32, shape=(None, n_H, 100, 1))
Y = tf.placeholder(tf.float32, shape=(None, 8))

# build graph
if (with_conv):
    conv1 = tf.layers.conv2d(X, filters=64, kernel_size=[5, 5],strides = 1, padding='valid',activation = tf.nn.relu)    
    pool1 = tf.layers.max_pooling2d(conv1, pool_size=[2, 2], strides=2, padding='valid')
else:
    pool1 = tf.layers.max_pooling2d(X, pool_size=[2, 2], strides=2, padding='valid')    
pool1_flat = tf.layers.flatten(pool1)
dense2 = tf.layers.dense(pool1_flat, units=256, activation=tf.nn.relu)
H = tf.layers.dense(dense2, units=8, activation=tf.nn.relu)

# compute cost
cost = tf.reduce_mean(tf.square(Y - H))

# initialize variables
init = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init)
    a = sess.run(cost, feed_dict = {X: X_train, Y:Y_train})
    print(a)

```
"
22884,Weird crash when using tensorflow C++ API on Android,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:

Follow the link https://medium.com/jim-fleming/loading-a-tensorflow-graph-with-the-c-api-4caaff88463f to load graph with C++ API:

namespace tf = tensorflow;
Inference::loadGraph( const std::vector<unsigned char>& graph ) 
{
 
    tf::SessionOptions* options = new tf::SessionOptions();
    tensorflow::Session* session = nullptr;
    tf::Status status = tf::NewSession( *options, &session );
    if ( status.ok() ) {
        m_session.reset( session );
    } else {
        return false;
    }

    tf::GraphDef tensorflowGraph;

    // The following graph parsing codes are copied from ReadBinaryProto in core/platform/env.cc
    std::unique_ptr<::tensorflow::protobuf::io::CodedInputStream> coded_stream =
            std::unique_ptr<::tensorflow::protobuf::io::CodedInputStream>(
                new ::tensorflow::protobuf::io::CodedInputStream(
                        (const google::protobuf::uint8*)graph.data(),
                        (int)graph.size()));

    if (!coded_stream.get()) {
        return false;
    }

    // Total bytes hard limit / warning limit are set to 1GB and 512MB
    // respectively.
    coded_stream->SetTotalBytesLimit(1024LL << 20, 512LL << 20);

    if (!tensorflowGraph.ParseFromCodedStream(coded_stream.get())) {
        return false;
    }

    status = m_session->Create( tensorflowGraph );
    if (!status.ok()) {
        return false;
    }

#ifdef IOS
    // Notes: Crash in SessionOptions destructor on Android for unknown reason.
    // All of data member get corrupt on Android in destructor
    // Use stack variable tf::SessionOptions options crash as well.
    // The object may be deleted already by others on Android
    // But no crash on iOS.
    delete options;
#endif

    return true;
}

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 18.04 or Mac OS

- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
Samsung S9 or HuaWei P20. 

- **TensorFlow installed from (source or binary)**:
Tensorflow mobile, build for Android from source with makefile, following official build steps.

./tensorflow/contrib/makefile/build_all_android.sh -a armeabi-v7a -s tensorflow/contrib/makefile/sub_makefiles/android/Makefile.in -t ""libtensorflow_inference.so libtensorflow_demo.so all""


- **TensorFlow version (use command below)**:
r1.10

- **Python version**:
3.5

- **Bazel version (if compiling from source)**:
1.5.0.  Not used because makefile is used

- **GCC/Compiler version (if compiling from source)**:
Android NDK r15c


- **CUDA/cuDNN version**:
Not enabled

- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
I created a set C++ APIs for iOS and Android with JNI wrapper on top of it. Not using Tensorflow Java API since I want the same C++ API shared between iOS and Android.

Everything works on iOS.  

As comments mentioned in the code above, the SessionOptions can't be deleted on Android. It causes crash.  If the options object not deleted, load graph works on Android as well.  Please let me know what I did wrong. Thanks


"
22883,Weird crash when using C++ ,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22882,custom,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22881,How to quantize the format of h5 by API tf.keras ?,"import numpy as np
import tensorflow as tf

**the code only generate a float tflite, but i want to get an int8 tflite, i dont't kown how to do** 
if somebody can solve this issue [https://github.com/tensorflow/tensorflow/issues/22880](url) , maybe this problem can be solved.

# Generate tf.keras model.
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(2, input_shape=(3,)))
model.add(tf.keras.layers.RepeatVector(3))
model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(3)))
model.compile(loss=tf.keras.losses.MSE,
              optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),
              metrics=[tf.keras.metrics.categorical_accuracy],
              sample_weight_mode='temporal')

x = np.random.random((1, 3))
y = np.random.random((1, 3, 3))
model.train_on_batch(x, y)
model.predict(x)

# Save tf.keras model in HDF5 format.
keras_file = ""keras_model.h5""
tf.keras.models.save_model(model, keras_file)

# Convert to TensorFlow Lite model.
converter = tf.contrib.lite.TFLiteConverter.from_keras_model_file(keras_file)
tflite_model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(tflite_model)"
22880,"how to get  “tf. fake_quant_with_min_max_args”  by  API  ""tf.keras""","I want to quantize a h5 model, but i can't find “tf. fake_quant_with_min_max_args”  by   API tf.keras  "
22879,Integrating imported graphs into Dataset pipelines,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
n/a
- **TensorFlow installed from (source or binary)**:
binary (pip)
- **TensorFlow version (use command below)**:
1.5.0
- **Python version**:
3.5.5
- **Bazel version (if compiling from source)**:
NA
- **GCC/Compiler version (if compiling from source)**:
NA
- **CUDA/cuDNN version**:
cuda 9.0/cudnn 7.0
- **GPU model and memory**:
gtx 1080, 12GB
- **Exact command to reproduce**

### Describe the problem
I'd like to build parts of my `dataset` by importing pre-existing graphs. It looks like a way to do this is to import the graph from within a `dataset.map` operation as in the source code below. But this approach fails with a placeholder-not-mapped error (please see below). I know, e.g., that it is possible to import and stick a graph _after_ the iterator at the end of the dataset. But I'd like to stick them _in_ the dataset pipeline. Is this currently infeasible using datasets, or am I doing something wrong?

### Source code
Here is my minimal source code. It works when `USE_GRAPH` is set to `False` and fails (see below for trace) when set to `True`:
```
import tensorflow as tf
USE_GRAPH = True
INT = tf.int64


def sq_fun(x):
    if USE_GRAPH:
        g = tf.Graph()
        with g.as_default() as g:
            d = tf.placeholder(INT, shape=(), name='d')
            _ = tf.pow(d, 2, name='e')
        xx = tf.import_graph_def(
            g.as_graph_def(),
            input_map={'d': x},
            return_elements=['e:0'])
    else:
        xx = tf.pow(x, 2, name='e')
    return xx


dataset = tf.data.Dataset.range(10)
dataset = dataset.map(sq_fun)
next_element = dataset.make_one_shot_iterator().get_next()

with tf.Session() as sess:
    for i in range(10):
        value = sess.run(next_element)
        print(value)
```

### Error trace
Here is the error trace when the above code is run as is (wiith `USE_GRAPH` set to `True`):

```
(tensorflow) matthai@matthai-z400:~/projects/w4ml/src/python/w4ml$ python test1.py 
/home/matthai/install/anaconda/anaconda3-4.4.0/envs/tensorflow/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
2018-10-10 18:05:42.047846: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2
2018-10-10 18:05:42.177377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-10-10 18:05:42.177855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:0f:00.0
totalMemory: 11.90GiB freeMemory: 7.23GiB
2018-10-10 18:05:42.177883: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:0f:00.0, compute capability: 6.1)
2018-10-10 18:05:42.577441: W tensorflow/core/framework/op_kernel.cc:1198] Invalid argument: You must feed a value for placeholder tensor 'import/d' with dtype int64
	 [[Node: import/d = Placeholder[dtype=DT_INT64, shape=[]]()]]
Traceback (most recent call last):
  File ""/home/matthai/install/anaconda/anaconda3-4.4.0/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1350, in _do_call
    return fn(*args)
  File ""/home/matthai/install/anaconda/anaconda3-4.4.0/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1329, in _run_fn
    status, run_metadata)
  File ""/home/matthai/install/anaconda/anaconda3-4.4.0/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'import/d' with dtype int64
	 [[Node: import/d = Placeholder[dtype=DT_INT64, shape=[]]()]]
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[]], output_types=[DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](OneShotIterator)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""test1.py"", line 28, in <module>
    value = sess.run(next_element)
  File ""/home/matthai/install/anaconda/anaconda3-4.4.0/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 895, in run
    run_metadata_ptr)
  File ""/home/matthai/install/anaconda/anaconda3-4.4.0/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/matthai/install/anaconda/anaconda3-4.4.0/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1344, in _do_run
    options, run_metadata)
  File ""/home/matthai/install/anaconda/anaconda3-4.4.0/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1363, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'import/d' with dtype int64
	 [[Node: import/d = Placeholder[dtype=DT_INT64, shape=[]]()]]
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[]], output_types=[DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](OneShotIterator)]]
```


"
22878,[Bug] Strange FIFOQueue behavior in distributed tensorflow. Is this a bug?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: pip install tensorflow 
- **TensorFlow version (use command below)**: 1.8
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: No CUDA
- **GPU model and memory**: No GPU
- **Exact command to reproduce**: Check code below


------------------------
### Describe the problem
There seems to be very little documentation on distributed tensorflow, so I am unsure of whether or not this is a bug or a logical error on my part. But the behavior is still a little strange.  Essentially, the issue seems to arise due to tf.train.SummarySaverHook(), it's interaction with tf.train.MoniteredTrainingSession() and the chief worker. If you look at my code below, you will see that there are two workers in this model. The chief worker is associated with ""task:0"". I've set up the model such that the queue and it operations live on ""task:1""(not the chief worker). There is a simple sum operation on ""task:0"". Once I launch the 3 nodes(ps, task0, task1), task0 is able to complete one simple sum operation and then all tasks hang indefinitely.  

For the moment, I've bypassed this issue by passing the summary hook object to the non-chief worker(task:0). By doing this, there don't seem to be any issues and the code works fine. I'm unsure of why this bypasses the issue. If chief workers handle summary write operations, shouldn't it still hang?

The strange behavior that I was talking about earlier occurs when you pass both the summary hook to the chief worker (Doing this will cause all workers to hang). You can get past the issue by simple removing the tf.FIFOqueue().dequeue() operation. It seems that this operation is getting called by ""task:0"" despite the fact that I've specified its location as ""task:1"" using tf.device. It is also being called despite not being an input requirement for any part of the graph that lives on ""task:0"". 

My question is essentially: Is this a bug or a logical error on my part?

### Source code / logs


```
import tensorflow as tf 
import sys

s_name = str(sys.argv[1])
t_num = int(sys.argv[2])


#Cluster details
cl_spec = tf.train.ClusterSpec({
    ""worker"": [
        ""localhost:2223"",
        ""localhost:2224""
    ],
    ""ps"": [
        ""localhost:2222""
    ]
})

config = tf.ConfigProto()
config.gpu_options.allow_growth = True


#Decides whether parameter server or worker
if s_name == ""ps"":
    server = tf.train.Server(cl_spec,job_name=""ps"",task_index=0,config=config)
    server.join()
else:
    server = tf.train.Server(cl_spec,job_name=""worker"",task_index=t_num,config=config)

    with tf.device(""/job:worker/replica:0/task:0""):
        a = tf.Variable(tf.constant(5))
        b = tf.Variable(tf.constant(6))
        z = tf.add(a,b)
        z_p = tf.Print([z],[z],""sum: "")
        tf.summary.histogram(""t0sum"",z_p)

    with tf.device(""/job:ps/replica:0/task:0""):    
        with tf.name_scope(""train_place_holder""):
            x = tf.placeholder(tf.uint8,shape=[1,1],name=""s_img1"")
            y = tf.placeholder(tf.uint8,shape=[1,1],name=""s_a"")    
        
        global_step = tf.train.create_global_step()
    
    with tf.device(""/job:worker/replica:0/task:1""):
        q = tf.FIFOQueue(capacity=25,
                         dtypes= (tf.uint8,tf.uint8),
                         shapes= (tf.TensorShape([1,1]),tf.TensorShape([1,1])),
                         name=""tq"",shared_name=""train_queue"")
        
        enqueue_op = q.enqueue((x,y),name=""enqueue"")
        xx,yy = q.dequeue(name=""dq"")
        zz = tf.add(xx,yy)
        tf.summary.histogram(""t1sum: "",zz)

    summ = tf.summary.merge_all()

    lap_dir = r'C:\Users\Vishnu\Documents\EngProj\bug_tf\log'

    summary_hook = tf.train.SummarySaverHook(   save_steps=1,save_secs=None,
                                                    output_dir=lap_dir,summary_writer=None,
                                                    scaffold=None,summary_op=summ)

    if (t_num == 0):
        # THis is chief worker
        saver_hook = tf.train.CheckpointSaverHook(  checkpoint_dir=lap_dir,
                                                    save_secs=3600,save_steps=None,
                                                    saver=tf.train.Saver(),checkpoint_basename='model.ckpt',
                                                    scaffold=None)

        with tf.train.MonitoredTrainingSession(master=server.target,is_chief=True,
                                                hooks=[saver_hook,summary_hook], config=config) as sess:   
           while True:
               sess.run([z,z_p])

    else:

        with tf.train.MonitoredTrainingSession(master=server.target,is_chief=False,
                                                save_summaries_steps=1,config=config) as sess:
            while not sess.should_stop():
                sess.run([zz])

    
```


You can run the above file and start 3 workers using the following command line commands.

```
python [file_name] worker 1

python [file_name] worker 0

python [file_name] ps 0
```"
22872,"import tensorflow failed, ""ImportError: DLL load failed: The specified module could not be found""","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Home
- **TensorFlow installed from (source or binary)**: binary ( pip install tensorflow-gpu )
- **TensorFlow version (use command below)**: 1.11.0
- **Python version**: python-3.5.4-amd64
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: cuda_10.0.130_411.31_win10
- **GPU model and memory**: Nvidia Geforece GTX 1060 6G
- **Exact command to reproduce**: python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Cannot import tensorflow successfully.
Try ""visual studio 2015, Microsoft Visual C++ 2015 Redistributable Update 3"" but the update in my laptop is newer than this, so I cannot install this update.

### Source code / logs
The error message after the command above
Traceback (most recent call last):
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Program Files\Python35\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Program Files\Python35\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Program Files\Python35\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Program Files\Python35\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

Please let me know if any question or suggestion.
"
22871,Dataset.shuffle gives same order with any seed if reshuffle_each_iteration=False,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OS X 10.11.6
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary (anaconda)
- **TensorFlow version (use command below)**: b'unknown' 1.10.0
- **Python version**:
Python 3.6.6 |Anaconda custom (64-bit)| (default, Jun 28 2018, 11:07:29) 
[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:
in python interactive session

```
>>> op1 = tf.data.Dataset.from_tensor_slices(list(range(100))).shuffle(100, seed=0, reshuffle_each_iteration=False).repeat().make_one_shot_iterator().get_next()
>>> op2 = tf.data.Dataset.from_tensor_slices(list(range(100))).shuffle(100, seed=1, reshuffle_each_iteration=False).repeat().make_one_shot_iterator().get_next()
>>> sess = tf.InteractiveSession()
2018-10-10 18:56:24.996286: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
>>> sess.run([op1, op2])
[85, 85]
>>> sess.run([op1, op2])
[1, 1]
>>> sess.run([op1, op2])
[54, 54]
```

One can see that op1 and op2 fetch elements in same order. If reshuffle_each_iteration is set to True, the element order is different as expected. I believe different seed values should result in different element order for op1 and op2, no matter what the value of reshuffle_each_iteration is."
22870,Transfer Learning help,"I am working on a sketch based image retrieval problem using Siamese network and have written the code in TensorFlow. Some people suggested to train the network on pretrained weights rather than training from scratch, but I am not sure how to implement it in TensorFlow. Online resources couldn't clear up my doubts completely. The repo link is given below:

https://github.com/ArkaJU/Sketch-Retrieval---Siamese/blob/master/SketchRetrieval.ipynb

I want to change the architecture to GoogLeNet and start with pretrained weights.
Any help is appreciated, thank you."
22869,Tensorflow C API: SessionRun batch size (how to properly set),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: no
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.10
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.16.1
- **GCC/Compiler version (if compiling from source)**: 7.3.0
- **CUDA/cuDNN version**: 9.2 / 7.3
- **GPU model and memory**: Nvidia GTX 1070ti


Dear sirs,
I am struggling to find the correct way to properly set the batch size for a SessionRun call in the C API. 
What I am trying to do is passing multiple images in a single SessionRun to a CNN (VGG16).
I am wondering which is the correct way to do so and to retrieve the appropriate correlated output values.
I have also come to believe that a single SessionRun call can only accept a single image as an input, so a proper batched input could only be achieved by having multiple SessionRun calls (for an example inside a ""for"" loop) with a single image each. Am I correct? Is there not a way to batch multiple images inside a single SessionRun call for a single-input CNN?

I have searched thoroughly your documentation but I could not find any evidence of a proper indication on how to do it.

Best regards"
22866,From <ipython-input-11-8a10d6bf5a6d>:51: calling import_graph_def (from tensorflow.python.framework.importer) with op_dict is deprecated and will be removed in a future version.,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22865,"ValueError: Invalid tensors 'num_detections,detection_classes,detection_scores,detection_boxes' were found.","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22864,check_numerics,
22863,"Tensorflow C++ 1.9.0 ~1.11.0 failed call to cuInit: CUresult(-1)  but libcuda.so is ok, where is the problem?","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution Linux kernel 3.10.104:
- **TensorFlow installed from source 1.9.0 build with gpu
       configure with open cuda support and build with command as 
      ""bazel build -c opt --config=mkl --config=cuda --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.2 //tensorflow:libtensorflow_cc.so""
- **TensorFlow version 1.9.0:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
Using built-in specs.
COLLECT_GCC=gcc
COLLECT_LTO_WRAPPER=/usr/libexec/gcc/x86_64-redhat-linux/4.8.5/lto-wrapper
Target: x86_64-redhat-linux
Configured with: ../configure --prefix=/usr --mandir=/usr/share/man --infodir=/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-bootstrap --enable-shared --enable-threads=posix --enable-checking=release --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --with-linker-hash-style=gnu --enable-languages=c,c++,objc,obj-c++,java,fortran,ada,go,lto --enable-plugin --enable-initfini-array --disable-libgcj --with-isl=/builddir/build/BUILD/gcc-4.8.5-20150702/obj-x86_64-redhat-linux/isl-install --with-cloog=/builddir/build/BUILD/gcc-4.8.5-20150702/obj-x86_64-redhat-linux/cloog-install --enable-gnu-indirect-function --with-tune=generic --with-arch_32=x86-64 --build=x86_64-redhat-linux
Thread model: posix
gcc version 4.8.5 20150623 (Red Hat 4.8.5-4) (GCC) 

- **CUDA/cuDNN version:/usr/local/cuda-9.0/
- **GPU model and memory**:
Wed Oct 10 20:14:44 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.81                 Driver Version: 384.81                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla P40           On   | 00000000:04:00.0 Off |                    0 |
| N/A   51C    P0   136W / 250W |  16817MiB / 22912MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla P40           On   | 00000000:06:00.0 Off |                    0 |
| N/A   50C    P0   143W / 250W |  16817MiB / 22912MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla P40           On   | 00000000:07:00.0 Off |                    0 |
| N/A   49C    P0    93W / 250W |  16817MiB / 22912MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla P40           On   | 00000000:08:00.0 Off |                    0 |
| N/A   50C    P0   138W / 250W |  16817MiB / 22912MiB |     98%      Default |
+-------------------------------+----------------------+----------------------+
|   4  Tesla P40           On   | 00000000:0C:00.0 Off |                    0 |
| N/A   40C    P0    56W / 250W |   2435MiB / 22912MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   5  Tesla P40           On   | 00000000:0D:00.0 Off |                    0 |
| N/A   26C    P8    10W / 250W |      0MiB / 22912MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   6  Tesla P40           On   | 00000000:0E:00.0 Off |                    0 |
| N/A   25C    P8    10W / 250W |      0MiB / 22912MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   7  Tesla P40           On   | 00000000:0F:00.0 Off |                    0 |
| N/A   28C    P8    10W / 250W |      0MiB / 22912MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|

my problem is  that when i use c++ api  to inference，but get error as follows:
2018-10-10 20:19:47.332240: E tensorflow/stream_executor/cuda/cuda_driver.cc:397] failed call to cuInit: CUresult(-1)
2018-10-10 20:19:47.332371: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: 100-88-66-85
2018-10-10 20:19:47.332402: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: 100-88-66-85
2018-10-10 20:19:47.332545: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 384.81.0
2018-10-10 20:19:47.332661: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 384.81.0
2018-10-10 20:19:47.332691: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 384.81.0
2018-10-10 20:19:47.349695: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.

the c++ code as follows:

int main(int argc, char const *argv[]) {
    const char* devices = getenv(""CUDA_VISIBLE_DEVICES"");
    if(devices!=NULL)
        LOG_INFO(""devices:%s"", devices);
    std::string model_path = ""wavenet.pb"";
    tensorflow::GraphDef graph;
    tensorflow::Status load_graph_status = ReadBinaryProto(tensorflow::Env::Default(), model_path.c_str(), &graph);
    if (!load_graph_status.ok()) {
        LOG_ERROR(""Failed to load pb graph:%s"", load_graph_status.error_message().c_str());
        return load_graph_status.code();
    }
    // config
    tensorflow::SessionOptions options;
    int num_threads_1 = 4;
    int num_threads_2 = 4;
    if (num_threads_1 > 0) {
        options.config.set_intra_op_parallelism_threads(num_threads_1);
        options.config.set_inter_op_parallelism_threads(num_threads_2);
	options.config.mutable_gpu_options()->set_visible_device_list(""0"");
    }
    // create session
    std::unique_ptr<tensorflow::Session> session;
    session.reset(tensorflow::NewSession(options));
    tensorflow::Status session_create_status = session->Create(graph);
    if (!session_create_status.ok()) {
        LOG_ERROR(""Failed to create tensorflow graph: %s"", session_create_status.error_message().c_str());
        return load_graph_status.code();
    }

    const std::string input_tensor_name = ""Predict/mel:0"";
    const std::string output_tensor_name = ""Predict/wav:0"";

    int32_t batch_size = 1;
    //int32_t frame_nums = (int)(24000 / 256);
    int32_t frame_nums = 1;
    int32_t dims = 80;
    std::vector<float> spectrum;
    std::vector<float> wave;
    spectrum.resize(frame_nums, 0);

    LOG_INFO(""fill data to input tensor begin"");
    tensorflow::Tensor input_tensor(tensorflow::DT_FLOAT, tensorflow::TensorShape({batch_size, frame_nums, dims}));
    auto input_tensor_matrix = input_tensor.tensor<float, 3>();
    for (int32_t x = 0; x < batch_size; ++x) {
        for (int y = 0; y < frame_nums; ++y) {
            for (int z = 0; z < dims; ++z) {
                int32_t index = x * frame_nums * dims + y * frame_nums + z;
                input_tensor_matrix(x, y, z) = spectrum[index];
            }
        }
    }
    LOG_INFO(""fill data to input tensor end"");
    std::vector<std::pair<std::string, tensorflow::Tensor>> inputs;
    std::vector<tensorflow::Tensor> outputs;
    inputs.push_back(std::pair<std::string, tensorflow::Tensor>(input_tensor_name, input_tensor));
    LOG_INFO(""session run begin"");
    tensorflow::Status run_status = session->Run(inputs, {output_tensor_name}, {}, &outputs);
    LOG_INFO(""session run end"");
    if (!run_status.ok()) {
        LOG_ERROR(""Running model failed: %s"", run_status.error_message().c_str());
        return load_graph_status.code();
    }
    auto output_c = outputs[0].tensor<float, 3>();
    int32_t length = outputs[0].dim_size(2);
    wave.resize(batch_size * 1 * length);
    for (int32_t x = 0; x < batch_size; ++x) {
        for (int y = 0; y < 1; ++y) {
            for (int z = 0; z < length; ++z) {
                int32_t index = x * 1 * length + y * 1 + z;
                wave[index] = output_c(x, y, z);
            }
        }
    }
    session->Close();
    return 0;
}



"
22862,Tf keras functional model with tf dataset input,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
no
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.11
- **Python version**:
3.5.3
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
I'm new to tensorflow keras and dataset. Can anyone help me understand why the following code doesn't work?

    import tensorflow as tf
    import tensorflow.keras as keras
    import numpy as np
    from tensorflow.python.data.ops import dataset_ops
    from tensorflow.python.data.ops import iterator_ops
    from tensorflow.python.keras.utils import multi_gpu_model
    from tensorflow.python.keras import backend as K


    data = np.random.random((1000,32))
    labels = np.random.random((1000,10))
    dataset = tf.data.Dataset.from_tensor_slices((data,labels))
    print( dataset)
    print( dataset.output_types)
    print( dataset.output_shapes)
    dataset.batch(10)
    dataset.repeat(100)

    inputs = keras.Input(shape=(32,))  # Returns a placeholder tensor

    # A layer instance is callable on a tensor, and returns a tensor.
    x = keras.layers.Dense(64, activation='relu')(inputs)
    x = keras.layers.Dense(64, activation='relu')(x)
    predictions = keras.layers.Dense(10, activation='softmax')(x)

    # Instantiate the model given inputs and outputs.
    model = keras.Model(inputs=inputs, outputs=predictions)

    # The compile step specifies the training configuration.
    model.compile(optimizer=tf.train.RMSPropOptimizer(0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

    # Trains for 5 epochs
    model.fit(dataset, epochs=5, steps_per_epoch=100)

It failed with the following error:

    model.fit(x=dataset, y=None, epochs=5, steps_per_epoch=100)
    File ""/home/wuxinyu/pyEnv/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py"", line 1510, in fit
    validation_split=validation_split)
    File ""/home/wuxinyu/pyEnv/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py"", line 994, in _standardize_user_data
    class_weight, batch_size)
    File ""/home/wuxinyu/pyEnv/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py"", line 1113, in _standardize_weights
    exception_prefix='input')
    File ""/home/wuxinyu/pyEnv/lib/python3.5/site-packages/tensorflow/python/keras/engine/training_utils.py"", line 325, in standardize_input_data
    'with shape ' + str(data_shape))
    ValueError: Error when checking input: expected input_1 to have 2 dimensions, but got array with shape (32,)


According to tf.keras guide, I should be able to directly pass the dataset to model.fit, as this example shows:

> # Input tf.data datasets

> Use the Datasets API to scale to large datasets or multi-device training. Pass a tf.data.Dataset instance to the fit method:

    # Instantiates a toy dataset instance:
    dataset = tf.data.Dataset.from_tensor_slices((data, labels))
    dataset = dataset.batch(32)
    dataset = dataset.repeat()

> # Don't forget to specify `steps_per_epoch` when calling `fit` on a dataset.
> model.fit(dataset, epochs=10, steps_per_epoch=30)
Here, the fit method uses the steps_per_epoch argument—this is the number of training steps the model runs before it moves to the next epoch. Since the Dataset yields batches of data, this snippet does not require a batch_size.

> Datasets can also be used for validation:

    dataset = tf.data.Dataset.from_tensor_slices((data, labels))
    dataset = dataset.batch(32).repeat()

    val_dataset = tf.data.Dataset.from_tensor_slices((val_data, val_labels))
    val_dataset = val_dataset.batch(32).repeat()

    model.fit(dataset, epochs=10, steps_per_epoch=30,
          validation_data=val_dataset,
          validation_steps=3)

What's the problem with my code, and what's the correct way of doing it?"
22861,Variable names created by tf.kera.Model.build() is inconsistent with that by tf.keras.Model.call(),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04.4 LTS
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:  v1.11.0-0-gc19e29306c 1.11.0
- **Python version**: Python 3.5.2
- **Bazel version (if compiling from source)**: No
- **GCC/Compiler version (if compiling from source)**: No
- **CUDA/cuDNN version**: 7.2.1
- **GPU model and memory**: GTX 1060 6GB
- **Exact command to reproduce**: Please see the below

### Describe the problem
tf.keras.Model makes Variables of its weights, when its build() or call() is called first.

While I found call() makes Variables with the prefix ""MyModel/"", build() make Variables without any prefix.

That is inconvenient to manage non-object based checkpoint.

```python
import tensorflow as tf
import tensorflow.keras as keras

layers = keras.layers

class Model(keras.Model):
    def __init__(self, name=""MyModel""):
        super().__init__(name=name)
        self.conv1 = layers.Conv2D(32, [5,5], activation=tf.nn.relu, name=""conv1"")
        self.conv2 = layers.Conv2D(64, [5,5], activation=tf.nn.relu, name=""conv2"")

    def call(self, images):
        featmap = self.conv1(images)
        featmap = self.conv2(featmap)
        return featmap

    def inference(self, images):
        return self.__call__(images)


model = Model()
flags = ""build""

if flags == ""build"":
    model.build(input_shape=tf.TensorShape([None, 32, 32, 3]))

    for w in model.weights:
        print (w.op.name)
else:
    dummy = tf.zeros([1, 32, 32, 3])
    model(dummy)

    for w in model.weights:
        print (w.op.name)
```
"
22859,Feature request: Provide pre-compiled binary file of libtensorflow.so for RaspberryPi,"Thanks to your great works for Raspberry Pi.

https://www.tensorflow.org/install/source_rpi

I hope to use libtensorflow.so for Raspberry Pi too. Could you please provide pre-compiled binary?

https://www.tensorflow.org/install/lang_c"
22857,tf.estimator.train_and_evaluate auc value is different from tf.estimator.evaluate with same model,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:no 
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**: 'v1.11.0-0-gc19e29306c', '1.11.0'
- **Python version**:2.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:


### Describe the problem

I use tf.Estimator for training my models.
Here is part of my source code:

### Source code/log
**tf.estimator.train_and_evaluate**


```
 for n in range(num_epochs):
    train_spec = tf.estimator.TrainSpec(input_fn=lambda: input_fn(...))
    eval_spec = tf.estimator.EvalSpec(input_fn =lambda: input_fn(...))
    tf.estimator.train_and_evaluate(model, train_spec, eval_spec)


INFO:tensorflow:Saving dict for global step 18621: auc = 0.6128701, global_step = 18621, loss = 0.17829555
INFO:tensorflow:Saving dict for global step 37242: auc = 0.6212857, global_step = 37242, loss = 0.17826068
INFO:tensorflow:Saving dict for global step 55863: auc = 0.6261746, global_step = 55863, loss = 0.17617485
INFO:tensorflow:Saving dict for global step 74484: auc = 0.630533, global_step = 74484, loss = 0.17646796
INFO:tensorflow:Saving dict for global step 93105: auc = 0.63444453, global_step = 93105, loss = 0.17552255
INFO:tensorflow:Saving dict for global step 111726: auc = 0.63371396, global_step = 111726, loss = 0.17790207
INFO:tensorflow:Saving dict for global step 130347: auc = 0.62660295, global_step = 130347, loss = 0.17602158
INFO:tensorflow:Saving dict for global step 148968: auc = 0.6290031, global_step = 148968, loss = 0.17708226
INFO:tensorflow:Saving dict for global step 167589: auc = 0.6234657, global_step = 167589, loss = 0.17724389
INFO:tensorflow:Saving dict for global step 186210: auc = 0.62800914, global_step = 186210, loss = 0.17702612
```

### Source code/log

**tf.estimator.evaluate**

```
ckpts = [18621, 37242, 55863, 74484, 93105, 111726, 130347, 148968, 167589, 186210]
        for epoch, ckpt in enumerate(ckpts):
            model.evaluate(
                input_fn=lambda: input_fn(...),
                checkpoint_path=FLAGS.model_dir + ""/model.ckpt-"" + str(ckpt))

INFO:tensorflow:Saving dict for global step 18621: auc = 0.6088399, global_step = 18621, loss = 0.1782806
INFO:tensorflow:Saving dict for global step 37242: auc = 0.61857945, global_step = 37242, loss = 0.17766058
INFO:tensorflow:Saving dict for global step 55863: auc = 0.6241194, global_step = 55863, loss = 0.17692827
INFO:tensorflow:Saving dict for global step 74484: auc = 0.627106, global_step = 74484, loss = 0.17647153
INFO:tensorflow:Saving dict for global step 93105: auc = 0.62898576, global_step = 93105, loss = 0.17620046
INFO:tensorflow:Saving dict for global step 111726: auc = 0.62996054, global_step = 111726, loss = 0.17710653
INFO:tensorflow:Saving dict for global step 130347: auc = 0.63045925, global_step = 130347, loss = 0.17653853
INFO:tensorflow:Saving dict for global step 148968: auc = 0.6306327, global_step = 148968, loss = 0.17709586
INFO:tensorflow:Saving dict for global step 167589: auc = 0.6306394, global_step = 167589, loss = 0.1774284
INFO:tensorflow:Saving dict for global step 186210: auc = 0.6304847, global_step = 186210, loss = 0.17686236
```

my question is why same model but different auc value. To validate which is right. I also try auc api by sklearn.The result is that auc value same with tf.estimator.evaluate 

### Source code/log
```
 ckpts = [18621, 37242, 55863, 74484, 93105, 111726, 130347, 148968, 167589, 186210]
        for epoch, ckpt in enumerate(ckpts):
            for data_from, data_mode in zip([FLAGS.valid_data], ['test']):
                preds = model.predict(
                    input_fn=lambda: input_fn.(...),
                    checkpoint_path=FLAGS.model_dir + ""/model.ckpt-"" + str(ckpt))
                label_list = []
                pred_list = []
                for i, prob in enumerate(preds):
                    pred_list.append(prob['prob'][0])
                    label_list.append(prob['true_label'])
                auc_local = sklearn.metrics.roc_auc_score(label_list, pred_list)
                print(""%s sklearn.auc is :"", (epoch, data_mode, auc_local))


%s auc is : (0, 'test', 0.608910481886984)
%s auc is : (1, 'test', 0.6186364169158124)
%s auc is : (2, 'test', 0.6242379534016801)
%s auc is : (3, 'test', 0.6272265511554851)
%s auc is : (4, 'test', 0.6291364474305191)
%s auc is : (5, 'test', 0.6300167648655547)
%s auc is : (6, 'test', 0.6305495577793957)
%s auc is : (7, 'test', 0.6307263729168348)
%s auc is : (8, 'test', 0.6307389062228584)
%s auc is : (9, 'test', 0.6306225969185617)
```








"
22855,"[TF1.10][TRT5.0] TFTRT is still using libnvinfer.so.4 not 5, importing tensorflow.contrib.tensorrt failed","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 7.0
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NO
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 1.10
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: NO
- **GCC/Compiler version (if compiling from source)**: NO
- **CUDA/cuDNN version**: CUDA9.0 cuDNN 7.0.5
- **GPU model and memory**: TitanXP
- **Exact command to reproduce**: 'import tensorflow.contrib.tensorrt as trt'

### Describe the problem
New TensorRT version 5.0 is released from Nvidia official site. I downloaded it and try to import it inside tensorflow, which is called ""TFTRT"".
But below error popped out:
```
/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
**** Failed to initialize TensorRT. This is either because the TensorRT installation path is not in LD_LIBRARY_PATH, or because you do not have it installed. If not installed, please go to https://developer.nvidia.com/tensorrt to download and install TensorRT ****
Traceback (most recent call last):
  File ""run_uff.py"", line 4, in <module>
    from tensorflow.contrib.tensorrt import tensorrt as trt
  File ""/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/contrib/tensorrt/__init__.py"", line 34, in <module>
    raise e
tensorflow.python.framework.errors_impl.NotFoundError: libnvinfer.so.4: cannot open shared object file: No such file or directory

```
Seems the tensorflow is not updated for new TensorRT. Hope @samikama can take a look into this issue.

Thanks,"
22854,TensorRT INT8 calibration doesn't work with TF r1.12 and TRT 5RC,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:n/a
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a
- **TensorFlow installed from (source or binary)**: source 
- **TensorFlow version (use command below)**: r1.12
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: 0.16.1
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**:9/7.1
- **GPU model and memory**:1080ti
- **Exact command to reproduce**:

Follow workflow from here,  https://devblogs.nvidia.com/tensorrt-integration-speeds-tensorflow-inference/
Error at the following piece of code
`trt_graph=trt.calib_graph_to_infer_graph(calibGraph)
`
Log:
File ""/home/dhingratul/.virtualenvs/tf_trt_source_trt5rc_tf1_12/local/lib/python3.5/site-packages/tensorflow/contrib/tensorrt/python/trt_convert.py"", line 349, in calib_graph_to_infer_graph
for n in calibration_graph_def.node:
AttributeError: 'Graph' object has no attribute 'node'


You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22853,tf.keras.Model overrides 'self' behaviour with lists,"- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 18.04.1 LTS
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
N/A
- **TensorFlow installed from (source or binary)**:
pip3 install tensorflow-gpu
- **TensorFlow version (use command below)**:
v1.11.0-0-gc19e29306c 1.11.0
- **Python version**:
Python 3.6.5
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
V9.0.176
- **GPU model and memory**:
GTX 1070, 6GB VRAM
- **Exact command to reproduce**:
python3 codebelow.py

### Describe the problem
I am not familiar with ListWrapper(), but it is being applied to all list variables created with self when my class inherits from tf.keras.Model. This is bad because it is causing an IndexError when I use it in certain functions, or even by just passing it through my Tensorflow model (I am using eager execution). I don't know whether this is a bug or if it is intended, but if this is supposed to happen, I think documentation should be added to explain this at https://www.tensorflow.org/api_docs/python/tf/keras/models/Model. I have included a smallest working example below.

### Source code / logs
Code:

```
import tensorflow as tf

class my_class(tf.keras.Model):

    def __init__(self):
        super(my_class, self).__init__()

        self.x = [0]
        print(self.x)

model = my_class()
```

Output:
`ListWrapper([0])`
"
22852,TensorFlow C++ API is significantly slower than Python API in inference,"System information
***Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
***OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Kernel 4.4.103, LUbuntu 16.04
***TensorFlow installed from (source or binary):
Python API is the official release;  C++ API was complied via Makefile with all available optimization flags (linked as a static library)
***TensorFlow version (use command below):
1.10.1
***Python version:
2.7
***CUDA/cuDNN version:
Only CPU no GPU
***Bazel version:
did not use Bazel to build 
***GPU model and memory:
did not use GPU for inference 
***Mobile device:
platform with a RK3399 ARM processor

Steps to reproduce:
1. Trained a MobileNetSSDv1 model via Google Detection API (Python);
2. Froze the graph via the official tool provided by Detection API with batch_size = 1, and the input tensor size was [1, 150, 150, 3];
3. Used the frozen graph (.pb file) along with TensorFlow Python and C++ API for inference, respectively. All C++ optimization flags on;
4. The inference time with Python API was ~200 ms while C++ API takes ~550 ms. 

Any thoughts would be appreciated!"
22850,Tensorflow 1.11 breaks simple keras estimator: global step does not increment,"

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: running on colab.sandbox.google.com
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.11
- **Python version**: 3.6

### Describe the problem

The below code illustrates a minimal keras model that is then converted to an estimator using tf.keras.estimator.model_to_estimator() and and run using tf.estimator.train_and_evaluate().

The code runs properly up until Tensorflow 1.10, however upon upgrading to Tensorflow 1.11 the global step fails to increment, resulting in training never ending. 

You can reproduce by the running the below code on colab.sandbox.google.com, observe it doesn't increment. Then downgrade to 1.10 and observe it works as expected.

### Source code / logs

```python
import shutil
import numpy as np
import tensorflow as tf
print(tf.__version__)

from tensorflow import keras
from tensorflow.python.keras.layers import Dense

#Hyperparameters
units=64
num_classes=3
batch_size=128
model_dir='model_files'


### 1. Generate Data
data = np.random.random((1000, 100))
labels = np.random.randint(3, size=(1000, 1))

### 2. Input Function
def input_fn(features, labels, batch_size, mode):
    dataset = tf.data.Dataset.from_tensor_slices((features, labels))
      
    if mode == tf.estimator.ModeKeys.TRAIN:
      dataset = dataset.repeat()

    # Shuffle, repeat, and batch the examples.
    return dataset.batch(batch_size)

### 3. Model Code
model = keras.models.Sequential()
model.add(Dense(units=units, input_shape=(100,), activation='relu',))
model.add(Dense(units=num_classes, activation='softmax'))
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])


### 4. Estimator Code
shutil.rmtree(model_dir, ignore_errors=True) # start fresh each time
estimator = tf.keras.estimator.model_to_estimator(keras_model=model, model_dir=model_dir)


# Create TrainSpec
train_spec = tf.estimator.TrainSpec(
    input_fn=lambda: input_fn(
        data,
        labels,
        batch_size,
        mode=tf.estimator.ModeKeys.TRAIN),
    max_steps=100
)

# Create EvalSpec
eval_spec = tf.estimator.EvalSpec(
    input_fn=lambda: input_fn(
        data,
        labels,
        batch_size,
        mode=tf.estimator.ModeKeys.EVAL),
    steps=None
)

# Start training
tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
```"
22847,Failed to load the native TensorFlow runtime.,"I'm Try run tensorflow.

Last version for Bazel, CUDA, pip... using python 3.6 and 3.7 with PyCharm IDE or Windows cmd.

System Win 7 x64, processor AMD + Radeon Graphics GPU.

Traceback (most recent call last):
  File ""C:\Users\Ramon\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Ramon\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Ramon\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Ramon\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Ramon\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed with error code -1073741795

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:/Users/Ramon/.PyCharmCE2018.2/config/scratches/scratch.py"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Users\Ramon\AppData\Roaming\Python\Python36\site-packages\tensorflow\__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Ramon\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Ramon\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Ramon\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Ramon\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Ramon\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Ramon\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Ramon\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed with error code -1073741795


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
22846,TensorFlow Lite for Microcontrollers needs to follow alignment while allocating/access memory,"@petewarden, we have cross compiled TensorFlow Lite for  Microcontrollers for our audio platforms, while running ""micro_speech_test"" our processor throw below LoadStoreAlignmentCause exception.
***WARNING* Unhandled user exception: LoadStoreAlignmentCause (0x57ffe02b)**
In general our processor expects to be 4 byte aligned while accessing memory.
After debugging code we found AllocateMemory() is not following memory aligned, Following patch is fixing the issue,however this will have some penalty for cycles in our hardware.

Original code:
uint8_t* SimpleTensorAllocator::AllocateMemory(size_t size) {
  if ((data_size_ + size) > data_size_max_) {
    // TODO(petewarden): Add error reporting beyond returning null!
    return nullptr;
  }
  uint8_t* result = data_;
  data_ += size;
  data_size_ += size;
  return result;
}

After aligned to 4 bytes:
 uint8_t* SimpleTensorAllocator::AllocateMemory(size_t size) {
       int align = 4;
       uint8_t *result = (uint8_t*)((intptr_t)(data_ + align - 1) & (~(align - 1)));
       int aligned_size = result - data_ + size;
  if ((data_size_ + aligned_size) > data_size_max_) {
     // TODO(petewarden): Add error reporting beyond returning null!
     return nullptr;
   }
  data_ += aligned_size;
  data_size_ += aligned_size;
  return result;
 }

It is good idea to write code for TensorFlow Lite for Microcontrollers follows memory alignment with an argument default to 4bytes and allow overwrite to custom number may be 8 and so on..

 "
22845,Importing Tensorflow - DLL load failed: the specified procedure could not be found,"I'm creating a basic project in PyCharm 2018.2.4, importing tensor flow (version 1.10.0 installed using pip) , and I get this error:

`Traceback (most recent call last):
  File ""C:/Users/sandr/Desktop/TensorProj/testing.py"", line 1, in <module>
    import tensorflow
  File ""C:\Users\sandr\.virtualenvs\TensorProj-uBuGaj5e\lib\site-packages\tensorflow\__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\sandr\.virtualenvs\TensorProj-uBuGaj5e\lib\site-packages\tensorflow\python\__init__.py"", line 52, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""C:\Users\sandr\.virtualenvs\TensorProj-uBuGaj5e\lib\site-packages\tensorflow\core\framework\graph_pb2.py"", line 6, in <module>
    from google.protobuf import descriptor as _descriptor
  File ""C:\Users\sandr\.virtualenvs\TensorProj-uBuGaj5e\lib\site-packages\google\protobuf\descriptor.py"", line 47, in <module>
    from google.protobuf.pyext import _message
ImportError: DLL load failed: The specified procedure could not be found.`

Tried a whole bunch of different versions, but nothing is working. Could anyone help me out?"
22843,"Error with transform_graph tool:  ""Failed to parse --transform argument""","
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X High Sierra
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: - 
- **TensorFlow installed from (source or binary)**: PIP INSTALLED
- **TensorFlow version (use command below)**: 1.10.1
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: - 
- **GCC/Compiler version (if compiling from source)**: -
- **CUDA/cuDNN version**: - 
- **GPU model and memory**: - 
- **Exact command to reproduce**: 
bazel run transform_graph -- --in_graph=/Users/Documents/modelsmaster/research/object_detection/inference_graphX/frozen_inference_graph.pb --out_graph=/Users/Documents/modelsmaster/research/object_detection/inference_graphX/out_graph.pb --inputs='image_tensor' --outputs='detection_boxes,detection_scores,detection_classes,num_detections' --transforms='
strip_unused_nodes(type=float,shape=“1,299,299,3”)
fold_constants(ignore_errors=true)
fold_batch_norms
fold_old_batch_norms'

### Describe the problem

I have used the Object Detection API to create a transfer learning model based upon MobileNet-V2-Coco. I have used ""export_for_inference.py"" on my checkpoint files to create a frozen model.

I am now trying to optimise the frozen model as described here:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#introduction

However, I am receiving an error:

tensorflow/tools/graph_transforms/transform_graph.cc:241] **Failed to parse --transform argument, error was Looking for parameter name, but found “1,299,299,3”)
fold_constants(ignore_errors=true)
fold_batch_norms
fold_old_batch_norms**


### Source code / logs

TRACEBACK

Mac-Air:graph_transforms mac$ bazel run transform_graph -- --in_graph=/Users/Documents/modelsmaster/research/object_detection/inference_graphX/frozen_inference_graph.pb --out_graph=/Users/Documents/modelsmaster/research/object_detection/inference_graphX/out_graph.pb --inputs='image_tensor' --outputs='detection_boxes,detection_scores,detection_classes,num_detections' --transforms='
strip_unused_nodes(type=float,shape=“1,299,299,3”)
fold_constants(ignore_errors=true)
fold_batch_norms
fold_old_batch_norms'
INFO: Analysed target //tensorflow/tools/graph_transforms:transform_graph (0 packages loaded).
INFO: Found 1 target...
Target //tensorflow/tools/graph_transforms:transform_graph up-to-date:
  bazel-bin/tensorflow/tools/graph_transforms/transform_graph
INFO: Elapsed time: 0.554s, Critical Path: 0.00s
INFO: 0 processes.
INFO: Build completed successfully, 1 total action
INFO: Running command line: bazel-bin/tensorflow/tools/graph_transforms/transform_graph '--in_graph=/Users/Documents/modelsmaster/research/object_detection/inference_graphX/frozen_inference_graph.pb' '--out_graph=/Users/Documents/modelsmaster/research/object_detection/inference_graphX/out_graph.pb' '--inputs=image_tensor' '--outputs=detection_boxes,detection_scores,detection_classes,num_detections' '--transforms=
strip_unused_nodes(type=float,shape=“1,299,299,3”)
fold_constants(ignore_errors=true)
fold_batch_norms
INFO: Build completed successfully, 1 total action
2018-10-09 22:25:41.764013: E tensorflow/tools/graph_transforms/transform_graph.cc:241] Failed to parse --transform argument, error was Looking for parameter name, but found “1,299,299,3”)
fold_constants(ignore_errors=true)
fold_batch_norms
fold_old_batch_norms


Please help me in sorting out this issue.
"
22842,Is Tensorflow Distributed Model supporting NCCL 2.0 inter-node AllReduce features?,"Hi, allow me to ask for a question about Tensorflow feature:

Firstly, I uses the nearly-latest Tensorflow version v1.10 and the system has NCCL 2.0 installed.

As is known that NCCL 2.0 has already supported Cross-Node multiple GPU training with fully NCCL management, but I found that Tensorflow benchmark script doesn't support NCCL in Tensorflow distributed mode, and it said NCCL mode is only allowed and designed for in single-node mode, so I don't know whether it is not supported by Tensorflow engine, or just not supported by the Tensorflow benchmark script? If it is just not implemented by the Tensorflow benchmark script, is there documentation or example to enable Distributed Tensorflow to use NCCL 2.0 to manage cross-node Multi GPUs? Thanks.

The official Tensorflow benchmark:
https://github.com/tensorflow/benchmarks"
22840,Compiling tensorflow 1.10.0 in mingw-w64(msys2 32bit),"After patching some code(mostly in cmake), I have compiled successfully tensorflow 1.10.0 in mingw-w64(msys2 32bit). Although some unit test not passed, I think some code should be commit to master. So now what should i do?
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: win7 32bit
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.10.0
- **Python version**: 2.7.15
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: gcc with mingw-w64(msys2 32bit)
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:
### Describe the problem
For some reasons, i need to using tensorflow in mingw-w64. 
After patching some code(mostly in cmake and little in header files), I have compiled successfully tensorflow source 1.10.0 in win7 mingw-w64(msys2 32bit) with cmake. 
Although some unit test not passed, I think some code should be commit to master to check. 
I think this patchs could make a mingw-w64 port.
Because i never use github to commit code, so i want to know how should i commit this patchs to master to check?"
22838,Wav to Spectogram wrong expected height,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: pip install tensorflow
- **TensorFlow version (use command below)**:1.11.0
- **Python version**: 2.7.15rc1
- **Bazel version (if compiling from source)**: 0.17.2
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**: bazel run tensorflow/examples/wav_to_spectrogram:wav_to_spectrogram -- --input_wav=/home/lukas/Downloads/music.wav --output_image=/home/lukas/Downloads/spectrogram.png



### Describe the problem
Hi I have problem with build this example from tensorflow guide.
 bazel run tensorflow/examples/wav_to_spectrogram:wav_to_spectrogram 
I get error ""Expected height 18711 but got 18712"" 
I tried three differenr wav files and got same error with different height. ""Expected height x but got x + 1""
When I tried with default image spectrogram_test_data/short_test_segment.wav so it works good 

### Source code / logs
bazel run tensorflow/examples/wav_to_spectrogram:wav_to_spectrogram -- --input_wav=/home/lukas/Downloads/music.wav --output_image=/home/lukas/Downloads/spectrogram.png
INFO: Analysed target //tensorflow/examples/wav_to_spectrogram:wav_to_spectrogram (65 packages loaded).
INFO: Found 1 target...
Target //tensorflow/examples/wav_to_spectrogram:wav_to_spectrogram up-to-date:
  bazel-bin/tensorflow/examples/wav_to_spectrogram/wav_to_spectrogram
INFO: Elapsed time: 149.081s, Critical Path: 4.61s
INFO: 0 processes.
INFO: Build completed successfully, 1 total action
INFO: Running command line: bazel-bin/tensorflow/examples/wav_to_spectrogram/wav_to_spectrogram '--input_wav=/home/lukas/Downloads/music.wav' INFO: Build completed successfully, 1 total action
2018-10-09 17:44:25.818539: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX
2018-10-09 17:44:26.909131: E tensorflow/examples/wav_to_spectrogram/main.cc:61] WavToSpectrogram failed with Invalid argument: Spectrogram size calculation failed: Expected height 18711 but got 18712
   [[Node: spectrogram = AudioSpectrogram[magnitude_squared=false, stride=128, window_size=256, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](wav_decoder)]]
"
22837,model.save() returning a 'NotImplementedError',"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 1803
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: From pip
- **TensorFlow version (use command below)**: 1.11.0
- **Python version**: 3.6.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:
```python

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))
model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))
model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

model.fit(x_train, y_train, epochs=3)

model.save('epic_num_reader.model')
```

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I'm receiving a `NotImplementedError` when trying to save a new model. The code above written above creates the error. This code is from a tutorial i've been following here [Python Programming (Sentdex)](https://pythonprogramming.net/introduction-deep-learning-python-tensorflow-keras/).

It's on a fresh machine with a fresh install of python and tensorflow.

Talking with some others on a Discord chat, i'm not the first person who's encountered this recently.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
<ipython-input-11-7b7b789f82a6> in <module>
----> 1 model.save('epic_num_reader.model')

c:\users\james\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\keras\engine\network.py in save(self, filepath, overwrite, include_optimizer)
   1356     """"""
   1357     if not self._is_graph_network:
-> 1358       raise NotImplementedError
   1359 
   1360     from tensorflow.python.keras.models import save_model  # pylint: disable=g-import-not-at-top

NotImplementedError:
```"
22834,TensorFlow 1.11.0 fails on aarch64 platform,"Building TensorFlow 1.11.0 from source fails on aarch64 platform
Ubuntu 16.04.5 LTS (GNU/Linux 4.4.77 aarch64)

------------------------

### System information
- **Have I written custom code**: N/A (a build problem)
- **OS Platform and Distribution**: Ubuntu 16.04.5 LTS (GNU/Linux 4.4.77 aarch64)
- **Mobile device**: N/A
- **TensorFlow installed from**: source
- **TensorFlow version**: 1.11
- **Python version**: 2.7.12
- **Bazel version**: 0.16.1
- **GCC/Compiler version**: gcc (Ubuntu/Linaro 7.3.0-21ubuntu1~16.04) 7.3.0
- **CUDA/cuDNN version**: Mobile device
- **GPU model and memory**: N/A
- **Exact command to reproduce**: Install dependencies and build via [CK-TensorFlow](https://github.com/ctuning/ck-tensorflow):

```bash
$ sudo apt install liblapack-dev libatlas-dev
$ sudo pip install enum34 mock pillow wheel absl-py scipy ck
$ ck pull repo:ck-tensorflow
$ ck install ck-env:package:tool-bazel-0.16.1-linux
$ ck install package:lib-tensorflow-1.11.0-src-cpu --env.CK_HOST_CPU_NUMBER_OF_PROCESSORS=1
```
**NB**: Restricting the number of building processes to 1 is necessary to prevent running out of memory on the platform with 4 GB and no swap enabled or similar.
### Describe the problem
Building TensorFlow 1.11.0 from source fails on aarch64 platform. Similar instruction for x86_64 platform works well:
```
$ ck install package:lib-tensorflow-1.7.0-src-cpu --env.CK_HOST_CPU_NUMBER_OF_PROCESSORS=1
```

### Logs:
```
ERROR: /home/ivan/CK-TOOLS/lib-tensorflow-src-cpu-1.11.0-compiler.python-2.7.12-linux-64/src/tensorflow/BUILD:592:1: Executing genrule //tensorflow:tensorflow_python_api_gen failed (Exit 1)
Traceback (most recent call last):
  File ""/home/ivan/.cache/bazel/_bazel_ivan/6cdff2b5ba48d82461ef4df735b65391/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 27, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""/home/ivan/.cache/bazel/_bazel_ivan/6cdff2b5ba48d82461ef4df735b65391/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 81, in <module>
    from tensorflow.python import keras
  File ""/home/ivan/.cache/bazel/_bazel_ivan/6cdff2b5ba48d82461ef4df735b65391/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/keras/__init__.py"", line 25, in <module>
    from tensorflow.python.keras import applications
  File ""/home/ivan/.cache/bazel/_bazel_ivan/6cdff2b5ba48d82461ef4df735b65391/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/keras/applications/__init__.py"", line 22, in <module>
    import keras_applications
ImportError: No module named keras_applications
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 29255.108s, Critical Path: 262.73s
INFO: 5693 processes: 5693 local.
FAILED: Build did NOT complete successfully
```"
22833,Internal error while saving an image to GCS (during evaluation),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian (c2-deeplearning-tf-1-11-cu100-20181001)
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.11.0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 10.0
- **GPU model and memory**: Nvidia V100 , 16GB
- **Exact command to reproduce**: models/research/object_detection/legacy/eval.py

### Describe the problem
While evaluating my model using the [object detection API](https://github.com/tensorflow/models/tree/master/research/object_detection) , I am saving images with the export_dir flag. The images are saved to GCS (google cloud bucket).
Once in a while, I encounter an InternalError that I traced back to [here](https://github.com/tensorflow/tensorflow/blob/ce1cdd52eda4b40ff8fb8c09bc178210883b3773/tensorflow/core/platform/cloud/gcs_file_system.cc#L410)
I am unable the reproduce it, since this happens unexpectedly, and on rare occasions, but it stops the evaluation job and requires restarting the machine.

Might be related - just before this error, I recieve a RuntimeWarning about true_divide

### Source code / logs
`
models/research/object_detection/utils/metrics.py:142: RuntimeWarning: invalid value encountered in true_divide
  num_images_correctly_detected_per_class / num_gt_imgs_per_class)
Traceback (most recent call last):
  File ""object_detection/legacy/eval.py"", line 150, in <module>
    tf.app.run()
  File ""models/.env/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""models/.env/local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 306, in new_func
    return func(*args, **kwargs)
  File ""object_detection/legacy/eval.py"", line 146, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""models/research/object_detection/legacy/evaluator.py"", line 276, in evaluate
    losses_dict=losses_dict)
  File ""models/research/object_detection/eval_util.py"", line 438, in repeated_checkpoint_run
    losses_dict=losses_dict)
  File ""models/research/object_detection/eval_util.py"", line 309, in _run_checkpoint_once
    tensor_dict, sess, batch, counters, losses_dict=losses_dict)
  File ""models/research/object_detection/legacy/evaluator.py"", line 238, in _process_batch
    keep_image_id_for_visualization_export=eval_config.
  File ""models/research/object_detection/eval_util.py"", line 196, in visualize_detection_results
    vis_utils.save_image_array_as_png(image, export_path)
  File ""models/research/object_detection/utils/visualization_utils.py"", line 76, in save_image_array_as_png
    image_pil.save(fid, 'PNG')
  File ""models/.env/local/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py"", line 208, in __exit__
    self.close()
  File ""models/.env/local/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py"", line 240, in close
    pywrap_tensorflow.Set_TF_Status_from_Status(status, ret_status)
  File ""models/.env/local/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py"", line 526, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.InternalError: Could not write to the internal temporary file.`"
22832,Can GANEstimator execute a forward pass in the discriminator after training?,"Hi, 

I am looking at GANEstimator and am trying to pass a new data point `X` to `discriminator_fn`. I have a class `GAN` that has an attribute `self.model = tfgan.estimator.GANEstimator(...)`. Is there a way to do this? It looks like for now, there is a session problem (in order to have something like `sess.run([self.discriminator_fn(X)])`).

Thanks"
22831,conv.cc:260 real_multiplier < 1.0 was not true.Node 0 failed to prepare.,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 14.0.4
- **Mobile device if the issue happens on mobile device**: MI 3s Prime
- **TensorFlow installed from (source or binary)**: pip install
- **TensorFlow version (use command below)**: 1.9.0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.16
- **CUDA/cuDNN version**: 8
- **GPU**: Nvidia GeForce GTX 1080 Ti
- **Exact command to reproduce**:
bazel-bin/tensorflow/contrib/lite/toco/toco --input_file=tflite_graph.pb --output_file=detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --inference_type=QUANTIZED_UINT8  --mean_values=128 --std_dev_values=128 --allow_custom_ops



### Problem
App gets crashed while loading model with following error.

### logs
Caused by: java.lang.NullPointerException: Internal error: Cannot allocate memory for the interpreter: tensorflow/contrib/lite/kernels/conv.cc:260 real_multiplier < 1.0 was not true.Node 0 failed to prepare."
22830,can tensorflow tensorRT be used for  optimization of object detection model,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22829,can tensorflow tensorRT (with multiple outputs) be used for object dectection? can ,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22828,aws: tensorflow_model_server: /lib64/libm.so.6: version `GLIBC_2.23' not found,"Hi team, 

I am trying to build tensorflow_model_server from source, I follow https://www.tensorflow.org/serving/setup and finish the building step. 

But when I run it, I get the error message:

```
bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server

bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server)
```

The platform is Amazon Linux 14, 
gcc (GCC) 7.2.1 20170915 (Red Hat 7.2.1-2)
g++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-28)


I am trying to build glibc 2.23 to get libm.so, but can not configure

```
loading cache ./config.cache
checking host system type... x86_64-unknown-linux-gnu
checking sysdep dirs... configure: error: The x86_64 is not supported.
```

Any idea? Thanks!"
22827,tensorflow c++ api session->Run segv for batch size>1,"Hi,

  i build tensorflow 1.4.0 to .so and call c++ session api Run to predict, when the input batch size is 1, the  Run api works well, but  batch_size    > 1, the Run will segv as follows:


#12 0x00002b12e0122c72 in tensorflow::LaunchMatMulBase<Eigen::ThreadPoolDevice, float>::launch(tensorflow::OpKernelContext*, tensorflow::Tensor const&, tensorflow::Tensor const&, Eigen::array<Eigen::IndexPair<long>, 1ul> const&, std::vector<long long, std::allocator<long long> >*, bool, tensorflow::Tensor*) () from lib/64bit/libtensorflow_cc.so                                                                                                                                 
#13 0x00002b12e0123231 in tensorflow::MatMulOp<Eigen::ThreadPoolDevice, float, false>::Compute(tensorflow::OpKernelContext*) () from lib/64bit/libtensorflow_cc.so                                                                                                                                           
#14 0x00002b12e20e6a4c in tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) () from lib/64bit/libtensorflow_framework.so                                                                                                                                            
#15 0x00002b12e20b7669 in tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) () from lib/64bit/libtensorflow_framework.so                                                                                                    
#16 0x00002b12e20a6290 in std::_Function_handler<void ()(), std::_Bind<std::_Mem_fn<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)> ()(tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)> >::_M_invoke(std::_Any_data const&) () from lib/64bit/libtensorflow_framework.so                                                  
#17 0x00002b12e1d576b7 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) () from lib/64bit/libtensorflow_framework.so                                                                                                                                              
#18 0x00002b12e1d565a2 in std::_Function_handler<void ()(), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from lib/64bit/libtensorflow_framework.so                                                                       
#19 0x00002b12e7726f50 in execute_native_thread_routine_compat () from /icd/dlsh_t2b/luther/tf.integ2.dev/tools.lnx86/lib/64bit/libstdc++.so.6                                
#20 0x0000000004385269 in create_head(void*) ()                                                                                                                               
#21 0x00002b12e71309d1 in start_thread () from /lib64/libpthread.so.0        
    
*** Stack trace in log file.
free model
terminate called after throwing an instance of 'std::system_error'
  what():  Resource deadlock avoided



the predict code :
   std::vector<tensorflow::Tensor> outputs;
    tensorflow::Status run_status = session_->Run({input},{model_outputs_name_}, {}, &outputs);

After i upgrade tensorflow to 1.8.0, crash too, please help to review this issue.

chao
"
22826,"Win10 C++ TF1.9, error LNK2001, build by bazel  !","I have generated the TensorFlowV1.9's .so and .lib file successfully on Win10,  but when I use this in VS2017, it has errors as bellow :

MFCTestTF1.9.obj : error LNK2001: 无法解析的外部符号 ""char const * __cdecl tensorflow::core::GetVarint32PtrFallback(char const *,char const *,unsigned int *)"" (?GetVarint32PtrFallback@core@tensorflow@@YAPBDPBD0PAI@Z)
1>D:\ProgramData\VS2017 Project\MFCTestTF1.9\Release\MFCTestTF1.9.exe : fatal error LNK1120: 1 个无法解析的外部命令
1>已完成生成项目“MFCTestTF1.9.vcxproj”的操作 - 失败。


And I also build TensorFlowV1.8 with CMAKE, it work OK without LNK error.  But V1.9 can not build by CMAKE."
22825,Second order derivative not supported for LRN (tf.nn.lrn),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: binary 
- **TensorFlow version (use command below)**: 1.11.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:  CUDA 9.0, cuDNN - 7
- **GPU model and memory**: Titan X
- **Exact command to reproduce**:

### Describe the problem
I have a network architecture which has a LRN layer (tf.nn.lrn). I am using the same network definition from the CIFAR 10 tutorial: https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10.py .
I have a loss term which involves the gradient of the cross entropy loss with respect to the input. This throws an error during graph construction:
LookupError: No gradient defined for operation 'gradients/norm1_grad/LRNGrad' (op type: LRNGrad)


### Source code
I have reused the same code from the CIFAR10 tutorial with minimal change to the loss function.
I have modified the loss function to have an additional term which penalizes the L2 norm of the gradient of the cross entropy loss with respect to the input.
Following is the change to the loss function made to the cifar10.py from the tutorial.
```python
def loss(images, logits, labels):
  """"""Please note that the caller needs to feed the input images as well.
  Args:
    images: Input images to be used for gradient computation
    logits: Logits from inference().
    labels: Labels from distorted_inputs or inputs(). 1-D tensor
            of shape [batch_size]
  Returns:
    Loss tensor of type float.
  """"""
  labels_onehot = tf.one_hot(labels, depth=10, off_value=0.0, on_value=1.0, dtype=tf.float32)
  cross_entropy = tf.nn.softmax_cross_entropy_with_logits(
      labels=labels_onehot, logits=logits, name='cross_entropy_per_example')
  cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')
  
  cross_entropy_grads, = tf.gradients(cross_entropy_mean, images)
  xent_grad_norm = tf.nn.l2_loss(cross_entropy_grads)

  tf.add_to_collection('losses', cross_entropy_mean)
  tf.add_to_collection('losses', xent_grad_norm)

  return tf.add_n(tf.get_collection('losses'), name='total_loss')
```
### Logs:
```
Traceback (most recent call last):
  File ""/home/vipin/anaconda3/envs/py36-tf1.11.0-test/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 730, in _GradientsHelper
    grad_fn = ops.get_gradient_function(op)
  File ""/home/vipin/anaconda3/envs/py36-tf1.11.0-test/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2460, in get_gradient_function
    return _gradient_registry.lookup(op_type)
  File ""/home/vipin/anaconda3/envs/py36-tf1.11.0-test/lib/python3.6/site-packages/tensorflow/python/framework/registry.py"", line 93, in lookup
    ""%s registry has no entry for: %s"" % (self._name, name))
LookupError: gradient registry has no entry for: LRNGrad

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
..............................................
..............................................
  File ""/home/vipin/anaconda3/envs/py36-tf1.11.0-test/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 596, in gradients
    gate_gradients, aggregation_method, stop_gradients)
  File ""/home/vipin/anaconda3/envs/py36-tf1.11.0-test/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 734, in _GradientsHelper
    (op.name, op.type))
LookupError: No gradient defined for operation 'gradients/norm1_grad/LRNGrad' (op type: LRNGrad)
```

"
22824,Sampled softmax in tf keras,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
no
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.11
- **Python version**:
3.5.3
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
I want to do sampled softmax loss in tf keras.  I defined my own model by subclassing keras Model.  In __init__, I specify the layers I need including the last Dense projection layer.  But this Dense layer shouldn't be called in training as I want to do sampled softmax and only to use it's weights and biases.  Then I define the loss function like this:

    class SampledSoftmax(Layer):
        def __init__(self,
                           num_sampled,
                           num_classes,
                           projection,
                           bias,
                           hidden_size):
            self.weights = tf.transpose(projection)
            self.bias = bias
            self.num_classes = num_classes
            self.num_sampled = num_sampled
            self.hidden_size = hidden_size

        def __call__(self, y_true, input):
            """""" reshaping of y_true and input to make them fit each other """"""
            input = tf.reshape(input, (-1,self.hidden_size))
            y_true = tf.reshape(y_true, (-1,1))

            return tf.nn.sampled_softmax_loss(
                weights=self.weights,
                biases=self.bias,
                labels=y_true,
                inputs=input,
                num_sampled=self.num_sampled,
                num_classes=self.num_classes,
                partition_strategy='div')

It takes in the necessary parameters to initialize and the class call will be the needed sampled softmax loss function.  The catch is that to add loss to model compile I need the weights etc of the last Dense.  But 1) in training Dense is not included in the model, and 2) even if it does, the Dense layer would only be hooked up with input and thus get its input dimensions etc in __call__ of my custom model.  In short, the weights etc won't be available before compiling model.  Can anyone offer some help to point me to the right direction? 

BTW, the loss defined above would work in small test cases like the following.

    x = Input(shape=(10,), name='input_x')
    emb_out = Embedding(10000,200,input_length=10)(x)
    lstm_out = LSTM(200, return_sequences=True)(emb_out)

    dense = Dense(10000, activation='sigmoid')
    output = dense(lstm_out)

    sl = SampledSoftmax(10, 10000, dense.kernel, dense.bias)

    model = Model(inputs=x, outputs=lstm_out)
    model.compile(optimizer='adam', loss=sl)
    model.summary()
    model.fit(dataset, epochs=20, steps_per_epoch=5)"
22823,Error in importing tensorflow,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Home
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: pip install tf-nightly-gpu
- **TensorFlow version (use command below)**: Can't see (error while importing)
- **Python version**: 3.6.0
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: Cuda 9.1; cuDNN 7.2.1
- **GPU model and memory**: GTX 1060 6Gb
- **Exact command to reproduce**: import tensorflow as tf

### Describe the problem
Error while importing tensorflow.
I have successfully installed via pip, but while I'm trying to import I get the log listed error.
Unfortunately, the error dosen't describe which DLL is missing, and all tickets I've seen doesn't solved my problem.

### Source code / logs

Python 3.6.0 (v3.6.0:41df79263a11, Dec 23 2016, 08:06:12) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""D:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""D:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""D:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""D:\Program Files\Python36\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""D:\Program Files\Python36\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""D:\Program Files\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""D:\Program Files\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""D:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""D:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""D:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""D:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""D:\Program Files\Python36\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""D:\Program Files\Python36\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
22821,tfrecords-only size-1 arrays can be converted to Python scalars error,"
![tfrecords_error](https://user-images.githubusercontent.com/26739210/46628024-f358a180-cb59-11e8-82e0-37f7eb7e8289.png)


"
22819,"Unable to disable build of AWS, HDFS, Kafka and GCP","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: any
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: any
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: master
- **Python version**: N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: ./configure

### Describe the problem
When building TensorFlow as a library (and in our case using `--config=monolithic`), we used to remove as much as possible of the not used features. Recently, this was removed from master in https://github.com/tensorflow/tensorflow/commit/d56c298f1ef14b5a738e1e0b7bbc66fcd736be3e for AWS, HDFS, Kafka and GCP.

I understand that that having to deal with multiple optionnally-disabled features can be a huge burden for the future, but is it possible to have some way around ?"
22818,Is Latest Tensorflow not compatible with latest cuda release  i.e CUDA 10??,"Hi Team,
I have new  RTX 2080 Graphics card with cuda 10.0 and cudnn 7.3.1. But tensor flow-gpu doesn't works. Can you please tell whether  tensor flow is compatible with it or not. Because as per tensorflow.org installation it is only compatible with cuda 9. I there any way out for this??
Or you guys are planning to release tensorflow with cuda 10 compatibility.??

**Have I written custom code : NO
- **OS : Ubuntu 18.04
- **TensorFlow installed from (source or binary)**: Using pip
- **TensorFlow version (use command below)**: TensorFlow 1.11
- **Python version**: 3.6
- **CUDA/cuDNN version**: CUDA 10.0/ cuDNN 7.3
- **GPU model and memory**: RTX 2080/ 8GB
- **Exact command to reproduce**: import tensorflow"
22816,The usage of CUDA_VISIBLE_DEVICES.,"I cannot figure out an ambigious definition of the env variable ""CUDA_VISIBLE_DEVICES"". As we know, the command `export CUDA_VISIBLE_DEVICES = 0`  means that we can only find the GPU:0. However, what's the defination of the command `export CUDA_VISIBLE_DEVICES = '' `? It means that we can either find all gpus or mask all gpus in our system?

And another odd thing is that I am running a BiLSTM-CRF model to do some NER tasks with a 8-gpu server. Without using the command `export CUDA_VISIBLE_DEVICES = '' `, the model runs a bit slower than expected, and we can see the program running with all 8 gpus through the command `nvidia-smi`. However, the model runs extremely faster than before when I use the command `export CUDA_VISIBLE_DEVICES = '' `, in addition, I cannot find any program information with `nvidia-smi`. 

Did I run the model with CPU? But the CPU usage changed only a little. And there seems no posibility that the CPU runs extremely faster than GPU under BiLSTM-CRF model. What's the matter?  If any information is needed, I'll update later cuz I don't know what information to display now."
22815,Object detection api training issue,"### System information
OS WIN 10
Processor : i5-4440 3.10GHz
RAM 8

- **TensorFlow installed from (source or binary)**: pip install
- **TensorFlow version (use command below)**: b'v1.10.0-rc1-19-g656e7a2b34' 1.10.0
- **Python version**: 3.6.6

### Describe the problem
I am trying to train object detection api using (faster_rcnn_inception_v2_coco_2018_01_28)

configuration file
[config.docx](https://github.com/tensorflow/tensorflow/files/2455380/config.docx)
 - 
I am facing with issue with object detection training
after 400~ steps the training is stop because a path problem while the script create the path
the path - C:\Users\z\Anaconda3\envs\tensorflow\Lib\site-packages\tensorflow\models\research\object_detection\training_dataturk\models\model\export\Servo\temp-b'1538897129'
----the error screen shot that happend twice before stopped
![1](https://user-images.githubusercontent.com/38852155/46601079-f6dc2200-caa0-11e8-8c11-e3af22ff602d.PNG)



"
22814,PYTHON_LIB_PATH not set during compilation of tensorflow_python_api_gen from source,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: RHEL 7.5
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NO
- **TensorFlow installed from (source or binary)**: YES
- **TensorFlow version (use command below)**: 1.11
- **Python version**: 3.5
- **Bazel version (if compiling from source)**:1.17.2
- **GCC/Compiler version (if compiling from source)**: gcc version 6.4.1 20170720 (Advance-Toolchain-at10.0) IBM AT 10 branch, based on subversion id 250395. (GCC)
- **CUDA/cuDNN version**: CUDA 9.2, CUDNN 7.2.1
- **GPU model and memory**: V100
- **Exact command to reproduce**: Installation of Tensorflow - see below

### Describe the problem
During installation from source, env variables are not loaded for the  compilation of:

```//tensorflow:tensorflow_python_api_gen```

These are the variables that are missing:

```
    CUDA_TOOLKIT_PATH=/usr/local/cuda-9.2 \
    CUDNN_INSTALL_PATH=/usr/local/cudnn-9.2-v7.2.1 \
    GCC_HOST_COMPILER_PATH=/opt/at10.0/bin/gcc \
    NCCL_INSTALL_PATH=/usr/local/nccl-9.2-v2.2.13 \
    OMP_NUM_THREADS=1 \
    PATH=/hpc_modules/at10.0-based/deeplearning/tensorflow/tensorflow-v1.11.0_openmpi-3.1.2/bin:/hpc_modules/os-based/deeplearning/tensorflow/bazel-0.17.2/bin:/hpc_modules/at10.0-based/python-packages-3.5-mpi4py-3.0.0_openmpi-3.1.2/bin:/hpc_modules/at10.0-based/mpi/openmpi-3.1.2/bin:/hpc_modules/at10.0-based/python-packages-3.5/bin:/opt/at10.0/bin:/usr/local/cuda-9.2/nvvm/bin:/usr/local/cuda-9.2/bin:/usr/lib/jvm/java-openjdk/bin:/shared/lsf/10.1/linux3.10-glibc2.17-ppc64le/etc:/shared/lsf/10.1/linux3.10-glibc2.17-ppc64le/bin:/u/acm/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/ibutils/bin:/afs/zurich.ibm.com/usr/bin \
    PYTHONPATH=/hpc_modules/at10.0-based/deeplearning/tensorflow/tensorflow-v1.11.0_openmpi-3.1.2/lib64/python3.5/site-packages:/hpc_modules/at10.0-based/python-packages-3.5-mpi4py-3.0.0_openmpi-3.1.2/lib64/python3.5/site-packages:/hpc_modules/at10.0-based/python-packages-3.5-mpi4py-3.0.0_openmpi-3.1.2/lib/python3.5/site-packages:/hpc_modules/at10.0-based/python-packages-3.5/lib64/python3.5/site-packages:/hpc_modules/at10.0-based/python-packages-3.5/lib/python3.5/site-packages:/hpc_modules/at10.0-based/python-packages-3.5/lib64/python3.5/site-packages \
    PYTHON_BIN_PATH=/opt/at10.0/bin/python3.5 \
    PYTHON_LIB_PATH=/opt/at10.0/lib64/python3.5/site-packages \
    TF_CUDA_CLANG=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=7.0 \
    TF_CUDA_VERSION=9.2 \
    TF_CUDNN_VERSION=7 \
    TF_NCCL_VERSION=2 \
    TF_NEED_CUDA=1 \
    TF_NEED_OPENCL_SYCL=0 \
```

In my case, the lack of `PYTHON_LIB_PATH` caused the issue with numpy. A manual execution of the command with those env variable set solve the problem - however I cannot continue installation.

### Source code / logs
```
SUBCOMMAND: # //tensorflow:tensorflow_python_api_gen [action 'Executing genrule //tensorflow:tensorflow_python_api_gen']
(cd /ibm/gpfs-dataP/hpc_modules/rhel7-P9-2018-09-14/at10.0-based/deeplearning/tensorflow/tensorflow-v1.11.0_openmpi-3.1.2/bazel_tmp/_bazel_acm/89da28a486992b186bd01e39111a422f/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda-9.2 \
    CUDNN_INSTALL_PATH=/usr/local/cudnn-9.2-v7.2.1 \
    GCC_HOST_COMPILER_PATH=/opt/at10.0/bin/gcc \
    LD_LIBRARY_PATH=/usr/local/nccl-9.2-v2.2.13/lib:/opt/ibm/xlmass/9.1.0/lib:/usr/lib64/libibverbs:/hpc_modules/at10.0-based/mpi/openmpi-3.1.2/lib:/opt/at10.0/lib:/usr/local/cudnn-9.2-v7.2.1/lib64:/usr/local/cuda-9.2/extras/CUPTI/lib64:/usr/local/cuda-9.2/nvvm/lib64:/usr/lib64/nvidia:/usr/local/cuda-9.2/lib64:/usr/lib64/jna:/shared/lsf/10.1/linux3.10-glibc2.17-ppc64le/lib \
    NCCL_INSTALL_PATH=/usr/local/nccl-9.2-v2.2.13 \
    OMP_NUM_THREADS=1 \
    PATH=/hpc_modules/at10.0-based/deeplearning/tensorflow/tensorflow-v1.11.0_openmpi-3.1.2/bin:/hpc_modules/os-based/deeplearning/tensorflow/bazel-0.17.2/bin:/hpc_modules/at10.0-based/python-packages-3.5-mpi4py-3.0.0_openmpi-3.1.2/bin:/hpc_modules/at10.0-based/mpi/openmpi-3.1.2/bin:/hpc_modules/at10.0-based/python-packages-3.5/bin:/opt/at10.0/bin:/usr/local/cuda-9.2/nvvm/bin:/usr/local/cuda-9.2/bin:/usr/lib/jvm/java-openjdk/bin:/shared/lsf/10.1/linux3.10-glibc2.17-ppc64le/etc:/shared/lsf/10.1/linux3.10-glibc2.17-ppc64le/bin:/u/acm/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/ibutils/bin:/afs/zurich.ibm.com/usr/bin \
    PYTHONPATH=/hpc_modules/at10.0-based/deeplearning/tensorflow/tensorflow-v1.11.0_openmpi-3.1.2/lib64/python3.5/site-packages:/hpc_modules/at10.0-based/python-packages-3.5-mpi4py-3.0.0_openmpi-3.1.2/lib64/python3.5/site-packages:/hpc_modules/at10.0-based/python-packages-3.5-mpi4py-3.0.0_openmpi-3.1.2/lib/python3.5/site-packages:/hpc_modules/at10.0-based/python-packages-3.5/lib64/python3.5/site-packages:/hpc_modules/at10.0-based/python-packages-3.5/lib/python3.5/site-packages:/hpc_modules/at10.0-based/python-packages-3.5/lib64/python3.5/site-packages \
    PYTHON_BIN_PATH=/opt/at10.0/bin/python3.5 \
    PYTHON_LIB_PATH=/opt/at10.0/lib64/python3.5/site-packages \
    TF_CUDA_CLANG=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=7.0 \
    TF_CUDA_VERSION=9.2 \
    TF_CUDNN_VERSION=7 \
    TF_NCCL_VERSION=2 \
    TF_NEED_CUDA=1 \
    TF_NEED_OPENCL_SYCL=0 \
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/tensorflow/create_tensorflow.python_api --root_init_template=tensorflow/api_template.__init__.py --apidir=bazel-out/ppc-opt/genfiles/tensorflow --apiname=tensorflow --apiversion=1 --package=tensorflow.python --output_package=tensorflow bazel-out/ppc-opt/genfiles/tensorflow/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/app/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/bitwise/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/compat/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/data/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/debugging/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/distributions/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/dtypes/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/errors/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/feature_column/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/gfile/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/graph_util/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/image/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/io/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/initializers/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/keras/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/keras/activations/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/keras/applications/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/keras/applications/densenet/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/keras/applications/inception_resnet_v2/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/keras/applications/inception_v3/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/keras/applications/mobilenet/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/keras/applications/mobilenet_v2/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/keras/applications/nasnet/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/keras/applications/resnet50/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/keras/applications/vgg16/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/keras/applications/vgg19/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/keras/applications/xception/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/keras/backend/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/keras/callbacks/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/keras/constraints/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/keras/datasets/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/keras/datasets/boston_housing/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/keras/datasets/cifar10/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/keras/datasets/cifar100/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/keras/datasets/fashion_mnist/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/keras/datasets/imdb/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/keras/datasets/mnist/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/keras/datasets/reuters/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/keras/estimator/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/keras/initializers/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/keras/layers/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/keras/losses/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/keras/metrics/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/keras/models/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/keras/optimizers/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/keras/preprocessing/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/keras/preprocessing/image/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/keras/preprocessing/sequence/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/keras/preprocessing/text/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/keras/regularizers/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/keras/utils/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/keras/wrappers/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/keras/wrappers/scikit_learn/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/layers/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/linalg/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/logging/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/losses/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/manip/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/math/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/metrics/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/nn/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/nn/rnn_cell/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/profiler/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/python_io/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/quantization/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/resource_loader/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/strings/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/saved_model/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/saved_model/builder/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/saved_model/constants/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/saved_model/loader/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/saved_model/main_op/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/saved_model/signature_constants/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/saved_model/signature_def_utils/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/saved_model/tag_constants/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/saved_model/utils/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/sets/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/sparse/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/spectral/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/summary/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/sysconfig/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/test/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/train/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/train/queue_runner/__init__.py bazel-out/ppc-opt/genfiles/tensorflow/user_ops/__init__.py')
SUBCOMMAND: # //tensorflow:tensorflow_python_api_gen [action 'Executing genrule //tensorflow:tensorflow_python_api_gen [for host]']
(cd /ibm/gpfs-dataP/hpc_modules/rhel7-P9-2018-09-14/at10.0-based/deeplearning/tensorflow/tensorflow-v1.11.0_openmpi-3.1.2/bazel_tmp/_bazel_acm/89da28a486992b186bd01e39111a422f/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/nccl-9.2-v2.2.13/lib:/opt/ibm/xlmass/9.1.0/lib:/usr/lib64/libibverbs:/hpc_modules/at10.0-based/mpi/openmpi-3.1.2/lib:/opt/at10.0/lib:/usr/local/cudnn-9.2-v7.2.1/lib64:/usr/local/cuda-9.2/extras/CUPTI/lib64:/usr/local/cuda-9.2/nvvm/lib64:/usr/lib64/nvidia:/usr/local/cuda-9.2/lib64:/usr/lib64/jna:/shared/lsf/10.1/linux3.10-glibc2.17-ppc64le/lib \
    PATH=/hpc_modules/at10.0-based/deeplearning/tensorflow/tensorflow-v1.11.0_openmpi-3.1.2/bin:/hpc_modules/os-based/deeplearning/tensorflow/bazel-0.17.2/bin:/hpc_modules/at10.0-based/python-packages-3.5-mpi4py-3.0.0_openmpi-3.1.2/bin:/hpc_modules/at10.0-based/mpi/openmpi-3.1.2/bin:/hpc_modules/at10.0-based/python-packages-3.5/bin:/opt/at10.0/bin:/usr/local/cuda-9.2/nvvm/bin:/usr/local/cuda-9.2/bin:/usr/lib/jvm/java-openjdk/bin:/shared/lsf/10.1/linux3.10-glibc2.17-ppc64le/etc:/shared/lsf/10.1/linux3.10-glibc2.17-ppc64le/bin:/u/acm/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/ibutils/bin:/afs/zurich.ibm.com/usr/bin \
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/tensorflow/create_tensorflow.python_api --root_init_template=tensorflow/api_template.__init__.py --apidir=bazel-out/host/genfiles/tensorflow --apiname=tensorflow --apiversion=1 --package=tensorflow.python --output_package=tensorflow bazel-out/host/genfiles/tensorflow/__init__.py bazel-out/host/genfiles/tensorflow/app/__init__.py bazel-out/host/genfiles/tensorflow/bitwise/__init__.py bazel-out/host/genfiles/tensorflow/compat/__init__.py bazel-out/host/genfiles/tensorflow/data/__init__.py bazel-out/host/genfiles/tensorflow/debugging/__init__.py bazel-out/host/genfiles/tensorflow/distributions/__init__.py bazel-out/host/genfiles/tensorflow/dtypes/__init__.py bazel-out/host/genfiles/tensorflow/errors/__init__.py bazel-out/host/genfiles/tensorflow/feature_column/__init__.py bazel-out/host/genfiles/tensorflow/gfile/__init__.py bazel-out/host/genfiles/tensorflow/graph_util/__init__.py bazel-out/host/genfiles/tensorflow/image/__init__.py bazel-out/host/genfiles/tensorflow/io/__init__.py bazel-out/host/genfiles/tensorflow/initializers/__init__.py bazel-out/host/genfiles/tensorflow/keras/__init__.py bazel-out/host/genfiles/tensorflow/keras/activations/__init__.py bazel-out/host/genfiles/tensorflow/keras/applications/__init__.py bazel-out/host/genfiles/tensorflow/keras/applications/densenet/__init__.py bazel-out/host/genfiles/tensorflow/keras/applications/inception_resnet_v2/__init__.py bazel-out/host/genfiles/tensorflow/keras/applications/inception_v3/__init__.py bazel-out/host/genfiles/tensorflow/keras/applications/mobilenet/__init__.py bazel-out/host/genfiles/tensorflow/keras/applications/mobilenet_v2/__init__.py bazel-out/host/genfiles/tensorflow/keras/applications/nasnet/__init__.py bazel-out/host/genfiles/tensorflow/keras/applications/resnet50/__init__.py bazel-out/host/genfiles/tensorflow/keras/applications/vgg16/__init__.py bazel-out/host/genfiles/tensorflow/keras/applications/vgg19/__init__.py bazel-out/host/genfiles/tensorflow/keras/applications/xception/__init__.py bazel-out/host/genfiles/tensorflow/keras/backend/__init__.py bazel-out/host/genfiles/tensorflow/keras/callbacks/__init__.py bazel-out/host/genfiles/tensorflow/keras/constraints/__init__.py bazel-out/host/genfiles/tensorflow/keras/datasets/__init__.py bazel-out/host/genfiles/tensorflow/keras/datasets/boston_housing/__init__.py bazel-out/host/genfiles/tensorflow/keras/datasets/cifar10/__init__.py bazel-out/host/genfiles/tensorflow/keras/datasets/cifar100/__init__.py bazel-out/host/genfiles/tensorflow/keras/datasets/fashion_mnist/__init__.py bazel-out/host/genfiles/tensorflow/keras/datasets/imdb/__init__.py bazel-out/host/genfiles/tensorflow/keras/datasets/mnist/__init__.py bazel-out/host/genfiles/tensorflow/keras/datasets/reuters/__init__.py bazel-out/host/genfiles/tensorflow/keras/estimator/__init__.py bazel-out/host/genfiles/tensorflow/keras/initializers/__init__.py bazel-out/host/genfiles/tensorflow/keras/layers/__init__.py bazel-out/host/genfiles/tensorflow/keras/losses/__init__.py bazel-out/host/genfiles/tensorflow/keras/metrics/__init__.py bazel-out/host/genfiles/tensorflow/keras/models/__init__.py bazel-out/host/genfiles/tensorflow/keras/optimizers/__init__.py bazel-out/host/genfiles/tensorflow/keras/preprocessing/__init__.py bazel-out/host/genfiles/tensorflow/keras/preprocessing/image/__init__.py bazel-out/host/genfiles/tensorflow/keras/preprocessing/sequence/__init__.py bazel-out/host/genfiles/tensorflow/keras/preprocessing/text/__init__.py bazel-out/host/genfiles/tensorflow/keras/regularizers/__init__.py bazel-out/host/genfiles/tensorflow/keras/utils/__init__.py bazel-out/host/genfiles/tensorflow/keras/wrappers/__init__.py bazel-out/host/genfiles/tensorflow/keras/wrappers/scikit_learn/__init__.py bazel-out/host/genfiles/tensorflow/layers/__init__.py bazel-out/host/genfiles/tensorflow/linalg/__init__.py bazel-out/host/genfiles/tensorflow/logging/__init__.py bazel-out/host/genfiles/tensorflow/losses/__init__.py bazel-out/host/genfiles/tensorflow/manip/__init__.py bazel-out/host/genfiles/tensorflow/math/__init__.py bazel-out/host/genfiles/tensorflow/metrics/__init__.py bazel-out/host/genfiles/tensorflow/nn/__init__.py bazel-out/host/genfiles/tensorflow/nn/rnn_cell/__init__.py bazel-out/host/genfiles/tensorflow/profiler/__init__.py bazel-out/host/genfiles/tensorflow/python_io/__init__.py bazel-out/host/genfiles/tensorflow/quantization/__init__.py bazel-out/host/genfiles/tensorflow/resource_loader/__init__.py bazel-out/host/genfiles/tensorflow/strings/__init__.py bazel-out/host/genfiles/tensorflow/saved_model/__init__.py bazel-out/host/genfiles/tensorflow/saved_model/builder/__init__.py bazel-out/host/genfiles/tensorflow/saved_model/constants/__init__.py bazel-out/host/genfiles/tensorflow/saved_model/loader/__init__.py bazel-out/host/genfiles/tensorflow/saved_model/main_op/__init__.py bazel-out/host/genfiles/tensorflow/saved_model/signature_constants/__init__.py bazel-out/host/genfiles/tensorflow/saved_model/signature_def_utils/__init__.py bazel-out/host/genfiles/tensorflow/saved_model/tag_constants/__init__.py bazel-out/host/genfiles/tensorflow/saved_model/utils/__init__.py bazel-out/host/genfiles/tensorflow/sets/__init__.py bazel-out/host/genfiles/tensorflow/sparse/__init__.py bazel-out/host/genfiles/tensorflow/spectral/__init__.py bazel-out/host/genfiles/tensorflow/summary/__init__.py bazel-out/host/genfiles/tensorflow/sysconfig/__init__.py bazel-out/host/genfiles/tensorflow/test/__init__.py bazel-out/host/genfiles/tensorflow/train/__init__.py bazel-out/host/genfiles/tensorflow/train/queue_runner/__init__.py bazel-out/host/genfiles/tensorflow/user_ops/__init__.py')
ERROR: /ibm/gpfs-dataP/hpc_modules/rhel7-P9-2018-09-14/at10.0-based/deeplearning/tensorflow/tensorflow-v1.11.0_openmpi-3.1.2/tensorflow-git/tensorflow/BUILD:592:1: Executing genrule //tensorflow:tensorflow_python_api_gen failed (Exit 1): bash failed: error executing command
  (cd /ibm/gpfs-dataP/hpc_modules/rhel7-P9-2018-09-14/at10.0-based/deeplearning/tensorflow/tensorflow-v1.11.0_openmpi-3.1.2/bazel_tmp/_bazel_acm/89da28a486992b186bd01e39111a422f/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/nccl-9.2-v2.2.13/lib:/opt/ibm/xlmass/9.1.0/lib:/usr/lib64/libibverbs:/hpc_modules/at10.0-based/mpi/openmpi-3.1.2/lib:/opt/at10.0/lib:/usr/local/cudnn-9.2-v7.2.1/lib64:/usr/local/cuda-9.2/extras/CUPTI/lib64:/usr/local/cuda-9.2/nvvm/lib64:/usr/lib64/nvidia:/usr/local/cuda-9.2/lib64:/usr/lib64/jna:/shared/lsf/10.1/linux3.10-glibc2.17-ppc64le/lib \
    PATH=/hpc_modules/at10.0-based/deeplearning/tensorflow/tensorflow-v1.11.0_openmpi-3.1.2/bin:/hpc_modules/os-based/deeplearning/tensorflow/bazel-0.17.2/bin:/hpc_modules/at10.0-based/python-packages-3.5-mpi4py-3.0.0_openmpi-3.1.2/bin:/hpc_modules/at10.0-based/mpi/openmpi-3.1.2/bin:/hpc_modules/at10.0-based/python-packages-3.5/bin:/opt/at10.0/bin:/usr/local/cuda-9.2/nvvm/bin:/usr/local/cuda-9.2/bin:/usr/lib/jvm/java-openjdk/bin:/shared/lsf/10.1/linux3.10-glibc2.17-ppc64le/etc:/shared/lsf/10.1/linux3.10-glibc2.17-ppc64le/bin:/u/acm/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/ibutils/bin:/afs/zurich.ibm.com/usr/bin \
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/tensorflow/create_tensorflow.python_api --root_init_template=tensorflow/api_template.__init__.py --apidir=bazel-out/host/genfiles/tensorflow --apiname=tensorflow --apiversion=1 --package=tensorflow.python --output_package=tensorflow bazel-out/host/genfiles/tensorflow/__init__.py bazel-out/host/genfiles/tensorflow/app/__init__.py bazel-out/host/genfiles/tensorflow/bitwise/__init__.py bazel-out/host/genfiles/tensorflow/compat/__init__.py bazel-out/host/genfiles/tensorflow/data/__init__.py bazel-out/host/genfiles/tensorflow/debugging/__init__.py bazel-out/host/genfiles/tensorflow/distributions/__init__.py bazel-out/host/genfiles/tensorflow/dtypes/__init__.py bazel-out/host/genfiles/tensorflow/errors/__init__.py bazel-out/host/genfiles/tensorflow/feature_column/__init__.py bazel-out/host/genfiles/tensorflow/gfile/__init__.py bazel-out/host/genfiles/tensorflow/graph_util/__init__.py bazel-out/host/genfiles/tensorflow/image/__init__.py bazel-out/host/genfiles/tensorflow/io/__init__.py bazel-out/host/genfiles/tensorflow/initializers/__init__.py bazel-out/host/genfiles/tensorflow/keras/__init__.py bazel-out/host/genfiles/tensorflow/keras/activations/__init__.py bazel-out/host/genfiles/tensorflow/keras/applications/__init__.py bazel-out/host/genfiles/tensorflow/keras/applications/densenet/__init__.py bazel-out/host/genfiles/tensorflow/keras/applications/inception_resnet_v2/__init__.py bazel-out/host/genfiles/tensorflow/keras/applications/inception_v3/__init__.py bazel-out/host/genfiles/tensorflow/keras/applications/mobilenet/__init__.py bazel-out/host/genfiles/tensorflow/keras/applications/mobilenet_v2/__init__.py bazel-out/host/genfiles/tensorflow/keras/applications/nasnet/__init__.py bazel-out/host/genfiles/tensorflow/keras/applications/resnet50/__init__.py bazel-out/host/genfiles/tensorflow/keras/applications/vgg16/__init__.py bazel-out/host/genfiles/tensorflow/keras/applications/vgg19/__init__.py bazel-out/host/genfiles/tensorflow/keras/applications/xception/__init__.py bazel-out/host/genfiles/tensorflow/keras/backend/__init__.py bazel-out/host/genfiles/tensorflow/keras/callbacks/__init__.py bazel-out/host/genfiles/tensorflow/keras/constraints/__init__.py bazel-out/host/genfiles/tensorflow/keras/datasets/__init__.py bazel-out/host/genfiles/tensorflow/keras/datasets/boston_housing/__init__.py bazel-out/host/genfiles/tensorflow/keras/datasets/cifar10/__init__.py bazel-out/host/genfiles/tensorflow/keras/datasets/cifar100/__init__.py bazel-out/host/genfiles/tensorflow/keras/datasets/fashion_mnist/__init__.py bazel-out/host/genfiles/tensorflow/keras/datasets/imdb/__init__.py bazel-out/host/genfiles/tensorflow/keras/datasets/mnist/__init__.py bazel-out/host/genfiles/tensorflow/keras/datasets/reuters/__init__.py bazel-out/host/genfiles/tensorflow/keras/estimator/__init__.py bazel-out/host/genfiles/tensorflow/keras/initializers/__init__.py bazel-out/host/genfiles/tensorflow/keras/layers/__init__.py bazel-out/host/genfiles/tensorflow/keras/losses/__init__.py bazel-out/host/genfiles/tensorflow/keras/metrics/__init__.py bazel-out/host/genfiles/tensorflow/keras/models/__init__.py bazel-out/host/genfiles/tensorflow/keras/optimizers/__init__.py bazel-out/host/genfiles/tensorflow/keras/preprocessing/__init__.py bazel-out/host/genfiles/tensorflow/keras/preprocessing/image/__init__.py bazel-out/host/genfiles/tensorflow/keras/preprocessing/sequence/__init__.py bazel-out/host/genfiles/tensorflow/keras/preprocessing/text/__init__.py bazel-out/host/genfiles/tensorflow/keras/regularizers/__init__.py bazel-out/host/genfiles/tensorflow/keras/utils/__init__.py bazel-out/host/genfiles/tensorflow/keras/wrappers/__init__.py bazel-out/host/genfiles/tensorflow/keras/wrappers/scikit_learn/__init__.py bazel-out/host/genfiles/tensorflow/layers/__init__.py bazel-out/host/genfiles/tensorflow/linalg/__init__.py bazel-out/host/genfiles/tensorflow/logging/__init__.py bazel-out/host/genfiles/tensorflow/losses/__init__.py bazel-out/host/genfiles/tensorflow/manip/__init__.py bazel-out/host/genfiles/tensorflow/math/__init__.py bazel-out/host/genfiles/tensorflow/metrics/__init__.py bazel-out/host/genfiles/tensorflow/nn/__init__.py bazel-out/host/genfiles/tensorflow/nn/rnn_cell/__init__.py bazel-out/host/genfiles/tensorflow/profiler/__init__.py bazel-out/host/genfiles/tensorflow/python_io/__init__.py bazel-out/host/genfiles/tensorflow/quantization/__init__.py bazel-out/host/genfiles/tensorflow/resource_loader/__init__.py bazel-out/host/genfiles/tensorflow/strings/__init__.py bazel-out/host/genfiles/tensorflow/saved_model/__init__.py bazel-out/host/genfiles/tensorflow/saved_model/builder/__init__.py bazel-out/host/genfiles/tensorflow/saved_model/constants/__init__.py bazel-out/host/genfiles/tensorflow/saved_model/loader/__init__.py bazel-out/host/genfiles/tensorflow/saved_model/main_op/__init__.py bazel-out/host/genfiles/tensorflow/saved_model/signature_constants/__init__.py bazel-out/host/genfiles/tensorflow/saved_model/signature_def_utils/__init__.py bazel-out/host/genfiles/tensorflow/saved_model/tag_constants/__init__.py bazel-out/host/genfiles/tensorflow/saved_model/utils/__init__.py bazel-out/host/genfiles/tensorflow/sets/__init__.py bazel-out/host/genfiles/tensorflow/sparse/__init__.py bazel-out/host/genfiles/tensorflow/spectral/__init__.py bazel-out/host/genfiles/tensorflow/summary/__init__.py bazel-out/host/genfiles/tensorflow/sysconfig/__init__.py bazel-out/host/genfiles/tensorflow/test/__init__.py bazel-out/host/genfiles/tensorflow/train/__init__.py bazel-out/host/genfiles/tensorflow/train/queue_runner/__init__.py bazel-out/host/genfiles/tensorflow/user_ops/__init__.py')
Traceback (most recent call last):
  File ""/ibm/gpfs-dataP/hpc_modules/rhel7-P9-2018-09-14/at10.0-based/deeplearning/tensorflow/tensorflow-v1.11.0_openmpi-3.1.2/bazel_tmp/_bazel_acm/89da28a486992b186bd01e39111a422f/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 27, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""/ibm/gpfs-dataP/hpc_modules/rhel7-P9-2018-09-14/at10.0-based/deeplearning/tensorflow/tensorflow-v1.11.0_openmpi-3.1.2/bazel_tmp/_bazel_acm/89da28a486992b186bd01e39111a422f/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 47, in <module>
    import numpy as np
ImportError: No module named 'numpy'```"
22813,The model is killing Automatically after 3 epochs,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22812,"Windows 10 c++, bazel build successfully， where is the .lib file？","I have build both TensorFlow1.11 and 1.10 's libtensorflow_cc.so file successfully, but where is the .lib file for Visual Studio ?"
22811,centos6 compile source code error,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  centos6.4
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: r1.9
- **TensorFlow version (use command below)**:
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.16.1
- **GCC/Compiler version (if compiling from source)**: 2.12
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**: 

### Describe the problem
Compile tensorflow with source code, but get some erros.
command : 
bazel build --config=opt --conlyopt=-std=c99  --verbose_failures //tensorflow/tools/pip_package:build_pip_package

### Source code / logs
Starting local Bazel server and connecting to it...
... still trying to connect to local Bazel server after 10 seconds ...
... still trying to connect to local Bazel server after 20 seconds ...
WARNING: /home/rcdev/.cache/bazel/_bazel_rcdev/0bbd005cd428560db8b92113dbd10a39/external/grpc/BUILD:1960:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/rcdev/.cache/bazel/_bazel_rcdev/0bbd005cd428560db8b92113dbd10a39/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: /home/rcdev/.cache/bazel/_bazel_rcdev/0bbd005cd428560db8b92113dbd10a39/external/grpc/BUILD:1960:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/rcdev/.cache/bazel/_bazel_rcdev/0bbd005cd428560db8b92113dbd10a39/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: /home/rcdev/.cache/bazel/_bazel_rcdev/0bbd005cd428560db8b92113dbd10a39/external/grpc/BUILD:1960:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/rcdev/.cache/bazel/_bazel_rcdev/0bbd005cd428560db8b92113dbd10a39/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: /home/rcdev/soft/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: /home/rcdev/soft/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (287 packages loaded).
INFO: Found 1 target...
INFO: From Compiling external/nsync/internal/counter.c:
cc1plus: warning: command line option '-std=c99' is valid for C/ObjC but not for C++
INFO: From Compiling tensorflow/contrib/lite/kernels/internal/tensor_utils.cc:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:0,
                 from tensorflow/contrib/lite/kernels/internal/tensor_utils.cc:16:
external/arm_neon_2_x86_sse/NEON_2_SSE.h: In function 'void vst1q_lane_f32(float32_t*, float32x4_t, int)':
external/arm_neon_2_x86_sse/NEON_2_SSE.h:9673:31: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]
     *(ptr) =  *((float*)&ilane);
                               ^
external/arm_neon_2_x86_sse/NEON_2_SSE.h: In function 'float32_t vgetq_lane_f32(float32x4_t, int)':
external/arm_neon_2_x86_sse/NEON_2_SSE.h:11912:22: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]
     return *(float*)&ilane;
                      ^
ERROR: /home/rcdev/.cache/bazel/_bazel_rcdev/0bbd005cd428560db8b92113dbd10a39/external/gif_archive/BUILD.bazel:8:1: C++ compilation of rule '@gif_archive//:gif' failed (Exit 1): gcc failed: error executing command 
  (cd /home/rcdev/.cache/bazel/_bazel_rcdev/0bbd005cd428560db8b92113dbd10a39/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/home/rcdev/soft/binutils/lib/:/home/rcdev/soft/gcc-4.9/lib/:/home/rcdev/soft/gcc-4.9/lib64/:/usr/local/CDH5/hadoop/lib/native/:/usr/local/lib: \
    PATH=/home/rcdev/soft/python3/bin:/home/rcdev/soft/bazel-0.16.1/output:/usr/local/java/jdk1.8.0/bin:/home/rcdev/soft/binutils/bin:/home/rcdev/soft/python2.7/bin:/home/rcdev/soft/gcc-4.9/bin:/home/rcdev/soft/python3/bin:/usr/local/CDH5/hadoop/bin:/usr/local/ruby/bin:/usr/local/CDH5/hive/bin:/usr/local/jdk1.7.0/bin:/usr/local/jdk1.7.0/jre/bin:/bin:/usr/local/jdk1.7.0/bin:/usr/local/apache-maven-3.0.4/bin:/usr/local/mysql/bin:/usr/local/jdk1.7.0/bin:/usr/local/jdk1.7.0/jre/bin:/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/usr/local/apache-flume/bin:/bin:/usr/local/R-3.1/bin:/usr/local/apache-flume/bin:/usr/local/CDH5/hadoop/bin:/usr/local/apache-maven-3.0.4/bin:/usr/local/CDH5/hbase/bin:/usr/local/CDH5/pig/bin:/usr/local/CDH5/zookeeper-3.4.5-cdh5.0.1/bin:/usr/local/scala-2.10.3/bin:/home/rcdev/bin:/home/rcdev/storm/bin:/home/rcdev/.local/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/home/rcdev/soft/python2.7/bin/python \
    PYTHON_LIB_PATH=/home/rcdev/soft/python2.7/lib/python2.7/site-packages \
    TF_DOWNLOAD_CLANG=0 \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL_SYCL=0 \
  /home/rcdev/soft/gcc-4.9/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/home/rcdev/soft/gcc-4.9/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -MD -MF bazel-out/k8-opt/bin/external/gif_archive/_objs/gif/egif_lib.pic.d -fPIC -iquote external/gif_archive -iquote bazel-out/k8-opt/genfiles/external/gif_archive -iquote bazel-out/k8-opt/bin/external/gif_archive -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -iquote bazel-out/k8-opt/bin/external/bazel_tools -isystem external/gif_archive/lib -isystem bazel-out/k8-opt/genfiles/external/gif_archive/lib -isystem bazel-out/k8-opt/bin/external/gif_archive/lib '-march=native' '-std=c99' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/gif_archive/lib/egif_lib.c -o bazel-out/k8-opt/bin/external/gif_archive/_objs/gif/egif_lib.pic.o)
external/gif_archive/lib/egif_lib.c: In function 'EGifOpenFileName':
external/gif_archive/lib/egif_lib.c:62:6: error: 'S_IREAD' undeclared (first use in this function)
      S_IREAD | S_IWRITE);
      ^
external/gif_archive/lib/egif_lib.c:62:6: note: each undeclared identifier is reported only once for each function it appears in
external/gif_archive/lib/egif_lib.c:62:16: error: 'S_IWRITE' undeclared (first use in this function)
      S_IREAD | S_IWRITE);
                ^
external/gif_archive/lib/egif_lib.c: In function 'EGifOpenFileHandle':
external/gif_archive/lib/egif_lib.c:119:5: warning: implicit declaration of function 'fdopen' [-Wimplicit-function-declaration]
     f = fdopen(FileHandle, ""wb"");    /* Make it into a stream: */
     ^
external/gif_archive/lib/egif_lib.c:119:7: warning: assignment makes pointer from integer without a cast
     f = fdopen(FileHandle, ""wb"");    /* Make it into a stream: */
       ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 46.012s, Critical Path: 14.30s
INFO: 327 processes: 327 local.
FAILED: Build did NOT complete successfully
You have mail in /var/spool/mail/rcdev

"
22810,Segfault when loading libtensorflow_cc.so for a second time,"

See

https://stackoverflow.com/questions/52683649/libtensorflow-cc-so-initialised-a-second-time-causes-segfault

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem."
22809,The virtual env install and verification fails,"On Mac OS X latest and following the exact steps on Tensorflow install page using the virtual env and it fails with weird error... any ideas?

pip3 --version
pip 18.1

python --version
Python 3.7.0

Following all steps exactly here....
https://www.tensorflow.org/install/pip

To validate at end it is failing....

python -c ""import tensorflow as tf; print(tf.__version__)""
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 114
    def TFE_ContextOptionsSetAsync(arg1, async):"
22806,tfp.bijectors.BatchNormalization ask for shape of tensor at graph construction,"i am building a customized bijector with tfp.bijectors.BatchNormalization. when i construct the graph, the dimension of input tensor for forward function cannot be possibly fully defined, because of the unknown batch size. But bijector BatchNormalization keeps raising error message that input must have shape known at graph construction. The code is [here](https://github.com/breadbread1984/glow-flow/blob/3c5645b1fa25e03110ec6d6553cb793eaf758b3f/GlowStep.py#L18). is it a bug or i am using it wrong?

call stack when error occurs
>  File ""C:\Users\yi.xie\Documents\glow-flow\GlowStep.py"", line 28, in _forward
    return self.flow.forward(x);
  File ""C:\Users\yi.xie\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\distributions\bijector_impl.py"", line 764, in forward
    return self._call_forward(x, name)
  File ""C:\Users\yi.xie\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\distributions\bijector_impl.py"", line 745, in _call_forward
    mapping = mapping.merge(y=self._forward(x, **kwargs))
  File ""C:\Users\yi.xie\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_probability\python\bijectors\chain.py"", line 263, in _forward
    x = b.forward(x, **kwargs.get(b.name, {}))
  File ""C:\Users\yi.xie\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\distributions\bijector_impl.py"", line 764, in forward
    return self._call_forward(x, name)
  File ""C:\Users\yi.xie\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\distributions\bijector_impl.py"", line 745, in _call_forward
    mapping = mapping.merge(y=self._forward(x, **kwargs))
  File ""C:\Users\yi.xie\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_probability\python\bijectors\batch_normalization.py"", line 223, in _forward
    return self._de_normalize(x)
  File ""C:\Users\yi.xie\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_probability\python\bijectors\batch_normalization.py"", line 214, in _de_normalize
    broadcast_fn = self._get_broadcast_fn(x)
  File ""C:\Users\yi.xie\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_probability\python\bijectors\batch_normalization.py"", line 188, in _get_broadcast_fn
    raise ValueError(""Input must have shape known at graph construction."")
ValueError: Input must have shape known at graph construction.

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:n/a
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.11
- **Python version**:3.6
- **Bazel version (if compiling from source)**:n/a
- **GCC/Compiler version (if compiling from source)**:n/a
- **CUDA/cuDNN version**:n/a
- **GPU model and memory**:n/a
- **Exact command to reproduce**:n/a

"
22805,InternalError: Failed to create session.,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10
- **TensorFlow installed from (source or binary)**:
source (conda install tensorflow)
- **TensorFlow version (use command below)**:
1.10.0
- **Python version**:
3.6.5
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
NO CUDA
- **GPU model and memory**:
NO GPU

### Describe the problem
Traceback (most recent call last):

  File ""<ipython-input-4-e4380738388e>"", line 11, in <module>
    model.fit(x_train, y_train, epochs=5)

  File ""C:\Users\DELL\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 1363, in fit
    validation_steps=validation_steps)

  File ""C:\Users\DELL\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training_arrays.py"", line 264, in fit_loop
    outs = f(ins_batch)

  File ""C:\Users\DELL\Anaconda3\lib\site-packages\tensorflow\python\keras\backend.py"", line 2876, in __call__
    session = get_session()

  File ""C:\Users\DELL\Anaconda3\lib\site-packages\tensorflow\python\keras\backend.py"", line 440, in get_session
    _SESSION = session_module.Session(config=config)

  File ""C:\Users\DELL\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)

  File ""C:\Users\DELL\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)

InternalError: Failed to create session.

### Source code / logs
import tensorflow as tf
mnist = tf.keras.datasets.mnist

(x_train, y_train),(x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(512, activation=tf.nn.relu),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5)
model.evaluate(x_test, y_test)
"
22804,timeout in //tensorflow/python/keras:data_utils_test when building from source,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
```
$ uname -a
Linux precision 4.15.0-34-generic #37~16.04.1-Ubuntu SMP Tue Aug 28 10:44:06 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

$ cat /etc/lsb-release
DISTRIB_ID=LinuxMint
DISTRIB_RELEASE=18.2
DISTRIB_CODENAME=sonya
DISTRIB_DESCRIPTION=""Linux Mint 18.2 Sonya""
```

CPU: Intel(R) Xeon(R) CPU E3-1505M v6 @ 3.00GHz
Mem: 32GB
Storage: 500GB

- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
No

- **TensorFlow installed from (source or binary)**:
Building from source

- **TensorFlow version (use command below)**:

    Issue occurs on both the latest 1.11.0 release branch and on latest master (573985c currently)

- **Python version**:
```
$ /usr/bin/python3 -V
Python 3.5.2
```

- **Bazel version (if compiling from source)**:
```
$ bazel version
Build label: 0.17.2
Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Sep 21 10:31:42 2018 (1537525902)
Build timestamp: 1537525902
Build timestamp as int: 1537525902
```

- **GCC/Compiler version (if compiling from source)**:
```
$ gcc -v
Using built-in specs.
COLLECT_GCC=gcc
COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/5/lto-wrapper
Target: x86_64-linux-gnu
Configured with: ../src/configure -v --with-pkgversion='Ubuntu 5.4.0-6ubuntu1~16.04.10' --with-bugurl=file:///usr/share/doc/gcc-5/README.Bugs --enable-languages=c,ada,c++,java,go,d,fortran,objc,obj-c++ --prefix=/usr --program-suffix=-5 --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-libmpx --enable-plugin --with-system-zlib --disable-browser-plugin --enable-java-awt=gtk --enable-gtk-cairo --with-java-home=/usr/lib/jvm/java-1.5.0-gcj-5-amd64/jre --enable-java-home --with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-5-amd64 --with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-5-amd64 --with-arch-directory=amd64 --with-ecj-jar=/usr/share/java/eclipse-ecj.jar --enable-objc-gc --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu
Thread model: posix
gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.10)
```

- **CUDA/cuDNN version**:
N/A

- **GPU model and memory**:
```
# Quadro M1200, 4GB (3D acceleration)
# Intel HD Graphics 630 (primary vga)
lspci | grep -iP '(vga|3d)'
00:02.0 VGA compatible controller: Intel Corporation Device 591d (rev 04)
01:00.0 3D controller: NVIDIA Corporation Device 13b6 (rev a2)
```

- **Exact command to reproduce**:

`bazel test -c opt --action_env PATH=""$PATH"" -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/contrib/lite/...`

or

`bazel test --test_verbose_timeout_warnings -c opt --action_env PATH=""$PATH"" -- //tensorflow/python/keras:data_utils_test`

(I added --action_env because I was having some issues getting bazel to find my installed python3)

### Describe the problem

This is the only test that fails for me when building from source.  It seems timeouts in this test have been a problem before, per a commit by @jlebar: https://github.com/tensorflow/tensorflow/commit/7a60167ba7718c23b0ed70d079bbb446f63a4fd9

### Source code / logs

```
$ bazel test --test_verbose_timeout_warnings -c opt --action_env PATH=""$PATH"" -- //tensorflow/python/keras:data_utils_test

...

TIMEOUT: //tensorflow/python/keras:data_utils_test (Summary)
      /home/calid/.cache/bazel/_bazel_calid/d95f42fa008125d605be7949a2399f3e/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/keras/data_utils_test/test.log
[1 / 2] 1 / 1 tests, 1 failed;  1 action; last test: .../python/keras:data_utils_test
Target //tensorflow/python/keras:data_utils_test up-to-date:
[2 / 2] 1 / 1 tests, 1 failed; no action; last  bazel-bin/tensorflow/python/keras/data_utils_test
[2 / 2] 1 / 1 tests, 1 failed; no action; lastINFO: Elapsed time: 915.423s, Critical Path: 915.01s
[2 / 2] 1 / 1 tests, 1 failed; no action; lastINFO: 1 process: 1 local.
[2 / 2] 1 / 1 tests, 1 failed; no action; lastINFO: Build completed, 1 test FAILED, 2 total actions
//tensorflow/python/keras:data_utils_test                               TIMEOUT in 915.0s
  /home/calid/.cache/bazel/_bazel_calid/d95f42fa008125d605be7949a2399f3e/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/keras/data_utils_test/test.log

INFO: Build completed, 1 test FAILED, 2 total actions

$ cat /home/calid/.cache/bazel/_bazel_calid/d95f42fa008125d605be7949a2399f3e/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/keras/data_utils_test/test.log
exec ${PAGER:-/usr/bin/less} ""$0"" || exit 1
Executing tests from //tensorflow/python/keras:data_utils_test
-----------------------------------------------------------------------------
Terminated
```
"
22803,Model converted to TFLite always returns NaN as output.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS 10.13.6
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: --
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:  v1.10.0-12-g4dcfddc5d1 1.10.1
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: --
- **GCC/Compiler version (if compiling from source)**: --
- **CUDA/cuDNN version**: --
- **GPU model and memory**: Intel Iris Plus Graphics 650 1536 MB
- **Exact command to reproduce**: `python3 test.py`

### Describe the problem
I have been trying to convert a frozen graph trained using [this repo](https://github.com/GeorgeSeif/Semantic-Segmentation-Suite) for using on android with TFLite. Trained model uses MobileNetV2 as frontend and [Mobile UNet for Semantic Segmentation](https://arxiv.org/abs/1704.04861) as the model. The problem I am facing is: the frozen pb graph segments the image correctly but TFLite converted model returns all `nan` for the output. To try the problem I wrote the following script. The model is converted without any errors or warnings, but the output is not correct. Do you have any idea what might be causing this?

Note: converted model is also returning NaNs on android device.

**Frozen graph**: [output_graph.pb](https://drive.google.com/file/d/1qGwD8h5ub0HjtO-Cc8Zd-HU6Uv-t9apF/view?usp=sharing)

### Source code / logs

**test.py**
```python
import tensorflow as tf
import numpy as np
import cv2
from tensorflow.python.platform import gfile
from tensorflow.contrib.lite.python.convert_saved_model import set_tensor_shapes

sess = tf.Session()

# load graph
with gfile.FastGFile('output_graph.pb', 'rb') as f:
    graph_def = tf.GraphDef()
graph_def.ParseFromString(f.read())
sess.graph.as_default()
tf.import_graph_def(graph_def, name='')

# get tensors
input_tensor = sess.graph.get_tensor_by_name('Placeholder:0')
output_tensor = sess.graph.get_tensor_by_name('logits/Conv2D:0')

# generate random image
input_image = np.array(np.random.random_sample(
    [1, 128, 128, 3]), dtype=np.float32)

# run the model with tf
output_image = sess.run(output_tensor, feed_dict={input_tensor: input_image})

# print tf output
print('--- Tensorflow output ---')
print(output_image)
print('-------------------------')

# set shapes
input_tensor.set_shape([1, 128, 128, 3])
output_tensor.set_shape([1, 128, 128, 32])

# convert model
converter = tf.contrib.lite.TocoConverter.from_session(
    sess, [input_tensor], [output_tensor])
tflite_model = converter.convert()

# Prepare interpreter
interpreter = tf.contrib.lite.Interpreter(model_content=tflite_model)
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# set input data
interpreter.set_tensor(input_details[0]['index'], input_image)

# run model on interpreter
interpreter.invoke()

# retrive output
output_data = interpreter.get_tensor(output_details[0]['index'])

# print tflite output
print('--- TFLite output ---')
print(output_data)
print('---------------------')
```

**output**
```shell
--- Tensorflow output ---
[[[[-14.484754   -14.454916    -3.9344878  ... -10.294399
     -2.837898    -8.190185  ]
   [-14.120294   -10.590508    -4.032942   ...  -6.7745924
     -0.4497184   -9.78646   ]
   [-14.561665   -10.49988     -8.065053   ...  -7.422716
     -0.7991432  -10.160792  ]
   ...
   [-13.12197     -7.3976164   -7.1669674  ...  -9.533363
     -2.0361094  -10.951963  ]
   [-15.041047    -7.3879066   -6.724542   ... -11.897878
     -2.1202648  -13.670592  ]
   [-14.483544   -10.037312    -6.356632   ... -12.075281
     -2.2860763  -10.284541  ]]

  [[-10.372202   -13.09114     -3.6517806  ...  -7.623592
     -1.8009435   -6.817739  ]
   [-10.72727    -10.886565    -5.621975   ...  -7.8185344
     -1.4768337  -10.389865  ]
   [-11.611484   -10.158413    -7.931344   ...  -4.938987
     -0.23626254  -8.830031  ]
   ...
   [-12.590868    -6.102834   -10.619679   ...  -9.990441
     -1.0927511  -10.764243  ]
   [-12.30341     -4.7649236   -6.600345   ...  -9.458132
     -0.8608778  -12.198781  ]
   [-11.649162    -6.2056537   -5.922945   ... -10.207803
     -1.5887291   -9.819743  ]]

  [[-11.40545    -13.755798    -6.9160714  ... -11.7735195
     -3.3357754  -11.139454  ]
   [-11.398698   -11.785369    -6.5561953  ...  -9.794318
     -2.8272014  -11.654141  ]
   [ -9.548821    -7.3276024   -8.640192   ...  -4.349879
      0.14261375  -7.0007625 ]
   ...
   [-12.497658    -5.8748426   -9.083981   ...  -9.841493
     -1.4732579  -11.357761  ]
   [-14.517144    -5.2391934   -8.496638   ... -10.834668
     -2.6033173  -13.944796  ]
   [-14.292226    -7.0837607   -6.3621516  ... -10.551426
     -3.6190045  -12.224428  ]]

  ...

  [[ -6.1242228  -14.730902    -6.034355   ...  -5.2220926
     -1.1160429   -2.2097938 ]
   [ -5.003286   -16.216772    -5.28262    ...  -5.2270694
     -1.7447093   -4.245701  ]
   [ -5.595118   -15.978978    -4.214302   ...  -5.4203877
     -1.8398296   -4.396698  ]
   ...
   [-13.178917   -13.012176   -10.450902   ... -15.064126
     -1.9914117   -9.5184765 ]
   [-10.992667    -8.671063    -6.456934   ... -14.054223
     -1.4051182   -9.887496  ]
   [ -9.728466   -10.335494    -7.3331285  ... -10.754501
     -1.7173084   -4.671226  ]]

  [[ -5.4983754  -15.449182    -5.7204423  ...  -4.4113154
     -1.0589103   -2.6990566 ]
   [ -5.384841   -16.741693    -5.5674496  ...  -5.684756
     -1.8891927   -4.65452   ]
   [ -5.7909193  -16.244637    -4.5293765  ...  -6.4048567
     -2.3706574   -4.982708  ]
   ...
   [-10.004818   -11.296059    -7.158481   ... -10.9329
     -2.0753372   -8.129092  ]
   [ -7.942011    -8.787835    -2.8869028  ... -10.7461605
     -1.7351687   -7.8243003 ]
   [ -9.368582   -11.195904    -5.3443894  ...  -8.967132
     -1.5083878   -5.205722  ]]

  [[ -7.6940765  -15.492795    -4.6488175  ...  -5.7006836
     -1.3711176   -3.7699785 ]
   [ -5.243174   -15.9268875   -5.07713    ...  -3.642994
     -1.4748344   -4.1258245 ]
   [ -4.8627806  -13.911514    -4.372596   ...  -2.4015875
     -1.4164882   -3.6560988 ]
   ...
   [ -9.049875   -12.410313    -5.53057    ...  -8.292001
     -2.442209    -4.6609883 ]
   [ -7.18582    -11.061987    -3.3339026  ...  -7.413499
     -2.0413182   -5.4470387 ]
   [ -9.58725    -13.576278    -5.9882216  ...  -8.204617
     -2.0788593   -5.216848  ]]]]
-------------------------
--- TFLite output ---
[[[[nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]
   ...
   [nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]]

  [[nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]
   ...
   [nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]]

  [[nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]
   ...
   [nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]]

  ...

  [[nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]
   ...
   [nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]]

  [[nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]
   ...
   [nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]]

  [[nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]
   ...
   [nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]]]]
---------------------
```
"
22802,Unable to download Android prebuilt binaries,"Hi all!

On the README.md file at https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/android#android-tensorflow-support it is said that we can download prebuilt binaries for Android via https://ci.tensorflow.org/view/Nightly/job/nightly-android, but unfortunately this link is broken. 

Is there another way to get prebuilt binaries for Android?

Kindest regards."
22800,Variance Inflation Factor estimate for each layer as guidance for dropout,"Dear All,

The [arXiv paper](https://arxiv.org/pdf/1806.06850.pdf) makes the following useful suggestion:

""We thus believe it would be helpful for NN software to include layer-by-layer checks for multicollinearity. If a layer is found to output a higher degree of multicollinearity, one might consider reducing the number of units in it, or even eliminating it entirely. Applying dropout to such layers is another possible action. One related implication is that later NN layers possibly should have fewer units than the earlier ones""

(Please see Table 2 of the paper.)

Concrete guidance on how to implement dropout remains a bit of a dark art. I don't think it needs to be. If we can produce VIF estimates for each layer, we can very possibly implement dropout with greater confidence. 
"
22798,"Could TensorFlow read data from MySQL? If not, what's the right way to add the feature?","I wanted to write a training program that read data from MySQL.

I found [tf.contrib.data.SqlDataset](https://www.tensorflow.org/api_docs/python/tf/contrib/data/SqlDataset), which seems reads from SQLite, but not other SQL DBMSes.

I tried to see how if I can extend it to support MySQL. It seems that (please correct me if I am wrong): [`tf.contrib.data.SqlDataset`](https://github.com/tensorflow/tensorflow/blob/b72265dc002e712fc3d0f33434f13c7a36a484b2/tensorflow/contrib/data/python/ops/readers.py#L363) uses `class SqlDatasetOp`, which uses [SQL drivers in the directory](https://github.com/tensorflow/tensorflow/tree/ad5c0c4d091c93ef65e91c55cb4df065d0c7a989/tensorflow/core/kernels/data/sql), and it seems that I can extend `SqlDatasetOp` by adding the MySQL connection driver as `mysql_query_connection.{h,cc}` in the directory.  However, this would require me to add MySQL libraries as TensorFlow dependencies in [`tensorflow/core/lib/db/BUILD`](https://github.com/tensorflow/tensorflow/blob/ad5c0c4d091c93ef65e91c55cb4df065d0c7a989/tensorflow/core/lib/db/BUILD); thus makes the TensorFlow binary depends on too many external staff. Is such kind of contributions welcome by the community?

I also noticed that the [Hadoop sequence file dataset](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/hadoop) is in the `contrib` directory. Is this the recommended way to adding datasets?

Thanks!

"
22795,System freezes after decreasing batch size when using CuDNNLSTM,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: Yes (conda)
- **TensorFlow version (use command below)**: 1.10.0
- **Python version**: 3.6.6
- **Bazel version (if compiling from source)**: 
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 10.0, V10.0.130
- **GPU model and memory**: GeForce GTX 1070 
- **Exact command to reproduce**:

### Describe the problem
While executing rather simple model in Keras/TensorFlow with three `keras.layers.CuDNNLSTM` layers, some skip connections, `Embedding` and `Dense` layers with <10 GB data read from numpy arrays, my system has frozen after using smaller batch size. When using batches of size 512 or larger everything was fine, when decreasing it to 256 system freezes (still screen, mouse and keyboard non responsive) after just a few dozen of iterations (not epochs). This happened several times in a row, with no single success, so I stopped experimenting further. Once I waited for 10 minutes to see if anything changes, but with no success. Everything went fine when using smaller batches and regular `keras.layers.LSTM` layers. Additionally, I used `nvidia-smi` and `top` monitoring tools and didn't notice anything strange before the crashes, but I might have missed something because of screen freezes. RAM and GPU usage in seconds before crushes was fine, far from limits."
22794,Win10: ImportError: DLL load failed: The specified module could not be found,"### System information:
Have I written custom code: No
OS Platform and Distribution: Windows 10 Pro updated
Mobile device: None
TensorFlow installed from: pip install
TensorFlow version:  1.11.0
Python Version: 3.6.6
Bazel version: not installed
CUDA/cuDNN version: CUDA 9.0, cuDNN 8.0
GPU model and memory: GF-GTX970 STRIX
Exact command to reproduce:
pip install tensorflow
pip install tensorflow-gpu
python
import tensorflow as tf

### Problem
I have had this error consistently even after trying to downgrade to older versions of CUDA tool, cuDNN, python, tensorflow and tensorflow-gpu. I have updated my enviornment variables. I have installed Visual C++ Redistributable Update.
I have read and tried to follow the solutions from other similar issues (such as #10033 and #17101), but have not succeeded in fixing the problem. 

### Log
C:\Users\user>python
Python 3.6.6 (v3.6.6:4cf1f54eb7, Jun 27 2018, 03:37:03) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
 <> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found."
22793,"tf.nn.softmax can give a result, when input shape is [2,3] and axis=2, Is it a bug?","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.10.0
- **Python version**:
3.5.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
no CUDA
- **GPU model and memory**:
no GPU
- **Exact command to reproduce**:


### Describe the problem

for  `tf.nn.softmax` , the `axis=2` but the input tensor shape is `[2,3]`.
And the result is 

```
tf.Tensor(
[[0.5 0.5 0.5]
 [0.5 0.5 0.5]], shape=(2, 3), dtype=float32)
```

I think the axis should `>=0` and `<=1`

if the `axis>=2`, it should be error.

### Source code / logs

```
import tensorflow as tf
tf.enable_eager_execution()
ones = tf.ones(shape=[2,3])
temp = tf.nn.softmax(ones,axis=2)
print(temp)
```
"
22780,tf.keras doesn't work with tensorflow optimizer when using TFOptimizer,"I'm trying to use tf.keras with the new AdamW optimizer in tensorflow and am running into issues. A toy version of the code is as follows:

```python
from tensorflow.contrib.opt import AdamWOptimizer
from tensorflow.python.keras.optimizers import TFOptimizer
from tensorflow.python.keras.layers import Dense
from tensorflow.python.keras.models import Sequential

model = Sequential()
model.add(Dense(2, activation=""tanh"", input_shape=(3,)))

tfopt = AdamWOptimizer(weight_decay=0.1, learning_rate=.004)
optimizer = TFOptimizer(tfopt)

model.compile(optimizer=optimizer, loss='mean_squared_error')
model.fit(np.random.random((5, 3)),
          np.random.random((5, 2)),
          epochs=5, batch_size=5)
```

Error is as follows:

```python
../python3.6/site-packages/tensorflow/python/keras/engine/training.py:1605: in fit
    validation_steps=validation_steps)
../python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py:153: in fit_loop
    outs = f(ins)
../python3.6/site-packages/tensorflow/python/keras/backend.py:2978: in __call__
    run_metadata=self.run_metadata)
../python3.6/site-packages/tensorflow/python/client/session.py:1399: in __call__
    run_metadata_ptr)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tensorflow.python.framework.errors_impl.raise_exception_on_not_ok_status object at 0x11ecde550>
type_arg = None, value_arg = None, traceback_arg = None

    def __exit__(self, type_arg, value_arg, traceback_arg):
      try:
        if c_api.TF_GetCode(self.status.status) != 0:
          raise _make_specific_exception(
              None, None,
              compat.as_text(c_api.TF_Message(self.status.status)),
>             c_api.TF_GetCode(self.status.status))
E             tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value training/TFOptimizer/beta2_power
E                [[{{node training/TFOptimizer/beta2_power/read}} = Identity[T=DT_FLOAT, _class=[""loc:@training/TFOptimizer/AdamW/Assign""], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](training/TFOptimizer/beta2_power)]]

../python3.6/site-packages/tensorflow/python/framework/errors_impl.py:526: FailedPre
```

### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS Mojave
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10.0
- **Python version**: 3.6
- **Exact command to reproduce**: See above
- **Have I written custom code**: N/A
- **Bazel version**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Mobile device**: N/A
"
22775,Keras cannot get gradient for using only one output from multi-outputs in another model,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Y
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**:  binary
- **TensorFlow version (use command below)**: 1.11.0-dev20180823
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0 / 7
- **GPU model and memory**: Quadro M4000
- **Exact command to reproduce**: N/A

### Describe the problem
I build one keras model with two outputs. I want to build another model that is exactly just a part of the existing model by using only one output. However, it shows the error message as

    ValueError: An operation has `None` for gradient. Please make sure that all of your ops have a gradient defined (i.e. are differentiable). Common ops without gradient: K.argmax, K.round, K.eval.

### Source code / logs
Example codes are shown below

	def get_test0_net():
	  data_input = keras.Input(shape=(4, 4, 3))
	  x0 = Conv2D(3, 3, padding='same')(data_input)
	  x1 = Conv2D(3, 3, padding='same')(data_input)
	  return keras.Model(inputs=data_input, outputs=[x0, x1])
	  
	def test0():
	  model = get_test0_net()

	  batch_in = np.ones((1,4,4,3))
	  batch_out = [np.zeros((1,4,4,3)), np.zeros((1,4,4,3))]

        # the original model can train without problem
	#  model.compile(loss='mse', optimizer='adam')
	#  model.train_on_batch(batch_in, batch_out)
	  
	  data_input = keras.Input(shape=(4, 4, 3))
	  out = model(data_input)[0]
	  new_model = keras.Model(data_input, out)
	  new_model.compile(loss='mse', optimizer='adam')
	  new_model.train_on_batch(batch_in, batch_out[0])"
22774,Eager Execution basics lacks definition of eager execution ,"Does the page [Eager Execution basics](https://www.tensorflow.org/tutorials/eager/eager_basics) have the right title? It hardly touches on eager execution, might be better named Tensor Basics."
22773,Tensorboard slow with S3,"TensorBoard version - 1.5.1
TensorFlow version if different from TensorBoard - No
OS Platform and version (e.g., Linux Ubuntu 16.04) - Tensorflow installed on EC2 instance of AWS

Description of issue:
I have placed all event files in S3 bucket of AWS cloud. And its content size will be around 6 GB. The problem which I am facing is that the tensorboard is very slow in scanning all the event files. It is taking more than an hour. Is there a way to reduce the processing time."
22772,tf.keras.layers.SeparableConv2D fails on Cloud TPU,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes: Convnet for MNIST, hand-written
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Google Colaboratory, default versions of Python & all libraries
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a
- **TensorFlow installed from (source or binary)**: n/a
- **TensorFlow version (use command below)**: n/a
- **Python version**: n/a
- **Bazel version (if compiling from source)**: n/a 
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: TPU
- **Exact command to reproduce**: n/a, see below

### Description
tf.keras.layers.SeparableConv2D fails on Cloud TPU. Colaboratory notebook with small example linked, works with GPU runtime but not TPU runtime.

https://colab.research.google.com/drive/1TyYSeA6bq2YBT7Ngo7thHfrcrAR30aXa

Expected result is successful training to >99% accuracy, this works on GPU but on TPU model.fit throws (full stack trace in notebook):

```
/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in __init__(self, fetches, contraction_fn)
    289       except ValueError as e:
    290         raise ValueError('Fetch argument %r cannot be interpreted as a '
--> 291                          'Tensor. (%s)' % (fetch, str(e)))
    292       except KeyError as e:
    293         raise ValueError('Fetch argument %r cannot be interpreted as a '

ValueError: Fetch argument <tf.Variable 'separable_conv2d_6/depthwise_kernel:0' shape=(5, 5, 1, 1) dtype=float32> cannot be interpreted as a Tensor. (Tensor Tensor(""separable_conv2d_6/depthwise_kernel/Read/ReadVariableOp:0"", shape=(5, 5, 1, 1), dtype=float32) is not an element of this graph.)
```"
22771,Use of bias term in tensorflow.keras conv3Dtranspose layer breaks if layer size is not defined,"### System information

    Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
    OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows, Colab
    Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
    TensorFlow installed from (source or binary): binary
    TensorFlow version (use command below): 1.10 / 1.11
    Python version: 3.6.x
    Bazel version (if compiling from source): NA
    GCC/Compiler version (if compiling from source): NA
    CUDA/cuDNN version: n/a
    GPU model and memory: n/a
    Exact command to reproduce: see code below

### Describe the problem
If the input layer size is None for a conv3Dtranspose layer the calculation of the bias term emits an error because it reshapes the tensor to be multiplied by the bias, even when just initializing with no input.  Note that this is supported for the 2D layer.

### Source code / logs
https://colab.research.google.com/gist/str4w/d60705ac54c5574f67ff2ec15cf89b42/conv_transpose_problem.ipynb

```
# use these to show tensorflow.keras issue
import tensorflow as tf
import tensorflow.keras as keras
import tensorflow.keras.layers as kl
import tensorflow.keras.models as km
import tensorflow.keras.backend as K
print(tf.__version__)
print(keras.__version__)

import numpy as np


# In 2d, the conv2dtranspose layer works fine with bias.

inp=kl.Input(shape=(None,None,1))
x=kl.Conv2D(1,kernel_size=5,padding='same')(inp)
x=kl.MaxPooling2D(pool_size=2)(x)
x=tf.layers.Conv2DTranspose(1,kernel_size=1,strides=2,use_bias=True)(x)
#x=kl.Conv2DTranspose(1,kernel_size=1,strides=2)(x)
model=km.Model(inputs=inp,outputs=x)
model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adadelta(),
              metrics=['accuracy'])
model.summary()
data2D=np.random.random_sample((1,42,74,1))
Z2=model.predict(data2D)
assert(Z2.shape == data2D.shape)


# in 3d, the bias term has issues
# Setting use_bias to false will allow this to pass
inp=kl.Input(shape=(None,None,None,1))
x=kl.Conv3D(1,kernel_size=5,padding='same')(inp)
x=kl.MaxPooling3D(pool_size=2)(x)
x=kl.Conv3DTranspose(1,kernel_size=1,strides=2,use_bias=True)(x)
#x=kl.Conv3DTranspose(1,kernel_size=1,strides=2)(x)
model=km.Model(inputs=inp,outputs=x)
model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adadelta(),
              metrics=['accuracy'])
model.summary()

data3D=np.random.random_sample((1,42,74,34,1))
Z3=model.predict(data3D)
assert(Z3.shape == data3D.shape)
```
Output:
```
1.11.0
2.1.6-tf
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, None, None, 1)     0         
_________________________________________________________________
conv2d (Conv2D)              (None, None, None, 1)     26        
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, None, None, 1)     0         
_________________________________________________________________
conv2d_transpose_1 (Conv2DTr (None, None, None, 1)     2         
=================================================================
Total params: 28
Trainable params: 28
Non-trainable params: 0
_________________________________________________________________

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-1-ff267c7f58e9> in <module>()
     32 x=kl.Conv3D(1,kernel_size=5,padding='same')(inp)
     33 x=kl.MaxPooling3D(pool_size=2)(x)
---> 34 x=kl.Conv3DTranspose(1,kernel_size=1,strides=2,use_bias=True)(x)
     35 #x=kl.Conv3DTranspose(1,kernel_size=1,strides=2)(x)
     36 model=km.Model(inputs=inp,outputs=x)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    767 
    768       if not in_deferred_mode:
--> 769         outputs = self.call(inputs, *args, **kwargs)
    770         if outputs is None:
    771           raise ValueError('A layer\'s `call` method should return a Tensor '

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/convolutional.py in call(self, inputs)
   1064       else:
   1065         outputs_4d = array_ops.reshape(outputs, [
-> 1066             outputs_shape[0], outputs_shape[1] * outputs_shape[2],
   1067             outputs_shape[3], outputs_shape[4]
   1068         ])

TypeError: unsupported operand type(s) for *: 'NoneType' and 'NoneType'
```

Pinging: @nuance-research"
22770,crash via tf_should_use format_stack,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: -
- **TensorFlow installed from (source or binary)**: binary (pip)
- **TensorFlow version (use command below)**: v1.11.0-0-gc19e29306c 1.11.0
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**: -
- **GCC/Compiler version (if compiling from source)**: -
- **CUDA/cuDNN version**: 8.0
- **GPU model and memory**: GTX 680 (will not be used)
- **Exact command to reproduce**: -


### Describe the problem
When `__repr__` is called on some TF objects at the wrong time, this can lead to a crash (seg fault; see below). There can be various reasons why this can happen, e.g. when a debugger shows the locals of all threads. My case was this, but I think this doesn't matter:

- Via [better_exchook](https://github.com/albertz/py_better_exchook), I extended the output of `sys.excepthook` and some `traceback` functions to print out some local vars and their `__repr__` output. There is something similar for IPython.
- I created some `tf.TensorArray` and called `unstack` and I did not use the result value. That `unstack` method is wrapped via `should_use_result`.
- The Python GC called the `_TFShouldUseHelper.__del__` function at some random point, and this triggered the stack formating and then the call some some `__repr__` of some TF objects.

Originally, this happened at exit, and I thought that probably it's just not safe at exit to touch any existing TF objects. So I fixed that case in better_exchook: It will not print any vars at exit. A test case to reproduce exactly that case is [here](https://github.com/albertz/playground/blob/master/test-tf111-tfshoulduse-crash.py).

However, now I get the same crash also not at exit but at another random point (see stack below). It will be hard to come up with a test case for this, as it is very non-deterministic when exactly the GC runs and calls the `__del__` function.



### Source code / logs

```
Current thread 0x00007f14209e8700 (most recent call first):
  File ""/u/zeyer/py-envs/py36-tf111/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1897 in name
  File ""/u/zeyer/py-envs/py36-tf111/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 352 in name
  File ""/u/zeyer/py-envs/py36-tf111/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 614 in __repr__
  File ""/u/zeyer/setups/librispeech/2018-02-26--att/returnn/tests/../better_exchook.py"", line 250 in pretty_print
  File ""/u/zeyer/setups/librispeech/2018-02-26--att/returnn/tests/../better_exchook.py"", line 487 in format_py_obj
  File ""/u/zeyer/setups/librispeech/2018-02-26--att/returnn/tests/../better_exchook.py"", line 571 in <lambda>
  File ""/u/zeyer/setups/librispeech/2018-02-26--att/returnn/tests/../better_exchook.py"", line 522 in _trySet
  File ""/u/zeyer/setups/librispeech/2018-02-26--att/returnn/tests/../better_exchook.py"", line 571 in format_tb
  File ""/u/zeyer/.linuxbrew/opt/python3/lib/python3.6/traceback.py"", line 37 in format_list
  File ""/u/zeyer/.linuxbrew/opt/python3/lib/python3.6/traceback.py"", line 193 in format_stack
  File ""/u/zeyer/py-envs/py36-tf111/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py"", line 60 in __del__
  File ""/u/zeyer/py-envs/py36-tf111/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 81 in __init__
  File ""/u/zeyer/py-envs/py36-tf111/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 4181 in _add_device_to_stack
  File ""/u/zeyer/py-envs/py36-tf111/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 4243 in device
  File ""/u/zeyer/.linuxbrew/opt/python3/lib/python3.6/contextlib.py"", line 81 in __enter__
  File ""/u/zeyer/py-envs/py36-tf111/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 3366 in _GroupControlDeps
  File ""/u/zeyer/py-envs/py36-tf111/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 3415 in group
  File ""/u/zeyer/py-envs/py36-tf111/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 3486 in tuple
  File ""/u/zeyer/py-envs/py36-tf111/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 791 in _GradientsHelper
  File ""/u/zeyer/py-envs/py36-tf111/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 596 in gradients
  File ""/u/zeyer/py-envs/py36-tf111/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 517 in compute_gradients
  File ""/u/zeyer/py-envs/py36-tf111/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 401 in minimize
  File ""tests/test_TFNetworkRecLayer.py"", line 219 in test_rhn_nan
  File ""tests/test_TFNetworkRecLayer.py"", line 2175 in <module>
```

""ops.py"", line 1897 in name, that is this code:

```
  @property
  def name(self):
    """"""The full name of this operation.""""""
    return c_api.TF_OperationName(self._c_op)
```

I often also see this just before the crash:

    pure virtual method called

A Travis log with this crash can also be seen [here](https://travis-ci.org/rwth-i6/returnn/jobs/437693971), or [here](https://travis-ci.org/rwth-i6/returnn/jobs/437716636).

The C backtrace is this:

```
/lib/x86_64-linux-gnu/libpthread.so.0(raise+0x29)[0x7f7e8df1a269]
/lib/x86_64-linux-gnu/libpthread.so.0(+0x11390)[0x7f7e8df1a390]
/u/zeyer/py-envs/py36-tf111/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so(TF_OperationName+0xa)[0x7f7e5ccc0eca]
/u/zeyer/py-envs/py36-tf111/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so(+0x1982264)[0x7f7e5ca78264]
/u/zeyer/.linuxbrew/Cellar/python3/3.6.3/lib/libpython3.6m.so.1.0(_PyCFunction_FastCallDict+0x209)[0x7f7e8e1f61c9]
...
```

"
22769,Different weights initializations for different values of `alignment_history` in `tf.contrib.seq2seq.AttentionWrapper` (fixed graph random seed),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.13.6
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: -
- **TensorFlow installed from (source or binary)**: binary (wheel)
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 2.7.15
- **Bazel version (if compiling from source)**: -
- **GCC/Compiler version (if compiling from source)**: -
- **CUDA/cuDNN version**: tested on CPU
- **GPU model and memory**: -
- **Exact command to reproduce**: code below

### Describe the problem
The initialization of the weights of the tensors in the decoder change when
changing the `alignment_history` parameter in `tf.contrib.seq2seq.AttentionWrapper`
(`True` or `False`). This occurs even if the graph's random seed is fixed
(`tf.set_random_seed(1)`).

### Source code / logs
Below, `create_graph` creates the graph consisting of a decoder with Bahdanau
attention and `test_attention_wrapper` creates a session in order to check the
values of the weight matrices at initialization.
```python
#! /usr/bin/env
# encoding: utf-8

import tensorflow as tf


def create_graph(memory,
                 memory_sequence_length,
                 batch_size=1,
                 vocab_size=30,
                 num_units=16,
                 attention_layer_size=16,
                 attention_alignment_history=False,
                 name=""decoder""):

    with tf.variable_scope(name) as scope:
        decoder_cell = tf.contrib.rnn.LSTMCell(num_units, name=""lstm"")
        attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(
            attention_layer_size,
            memory,
            memory_sequence_length)
        decoder_cell = tf.contrib.seq2seq.AttentionWrapper(
            decoder_cell,
            attention_mechanism,
            attention_layer_size=attention_layer_size,
            alignment_history=attention_alignment_history,
            name=""attention"")

        decoder_initial_state = decoder_cell.zero_state(batch_size, tf.float32)

        sos_id = tf.cast(vocab_size-2, tf.int32)
        # End token id
        eos_id = tf.cast(vocab_size-1, tf.int32)

        # Start tokens for the batch
        start_tokens = tf.fill([batch_size], sos_id)
        end_token = eos_id

        embedding = lambda ids: tf.one_hot(ids, vocab_size)
        # Helper for the decoder (performs sampling)
        helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(
            embedding,
            start_tokens,
            end_token
            )

        # Decoder
        decoder = tf.contrib.seq2seq.BasicDecoder(
            cell=decoder_cell,
            helper=helper,
            initial_state=decoder_initial_state,
            output_layer=None
            )

        final_outputs, final_state, _ = tf.contrib.seq2seq.dynamic_decode(
            decoder,
            scope=scope
            )


def test_attention_wrapper(batch, max_time, num_units, memory_arr,
                           memory_sequence_length_arr,
                           attention_alignment_history=False,
                           name=""decoder""):
    # Set the random graph seed
    tf.set_random_seed(1)
    with tf.Session() as sess:
        memory = tf.placeholder(tf.float32, [batch, max_time, num_units],
                                name=""memory"")
        memory_sequence_length = tf.placeholder(tf.int32, [batch],
                                                name=""memory_sequence_length"")
        create_graph(memory, memory_sequence_length,
                     attention_alignment_history=attention_alignment_history)
        # Initialize the variables
        variables = tf.VariableScope(None, name=""decoder"").global_variables()
        sess.run([v.initializer for v in variables])

        variables_out = sess.run(
            variables,
            feed_dict={memory: memory_arr,
                       memory_sequence_length: memory_sequence_length_arr}
            )

    return variables_out, variables
```

The full code used for reproducing the issue can be found below:

```python
#! /usr/bin/env
# encoding: utf-8

import numpy as np
import tensorflow as tf

# Parameters of the graph
batch = 1
max_time = 3
num_units = 1
memory_arr = np.ones((batch, max_time, num_units))
memory_sequence_length_arr = max_time * np.ones(batch)

# Create two graphs (with and without attention history)
tf.reset_default_graph()
variables_no, variables = test_attention_wrapper(
    batch, max_time, num_units, memory_arr,
    memory_sequence_length_arr,
    attention_alignment_history=False)
tf.reset_default_graph()
variables_yes, variables = test_attention_wrapper(
    batch, max_time, num_units, memory_arr,
    memory_sequence_length_arr,
    attention_alignment_history=True)

for i, (var_out_no, var_out_yes) in enumerate(zip(variables_no, variables_yes)):
    var_name = variables[i].name
    print(var_name)
    print(""-"" * len(var_name))
#     print ""NO:"", var_out_no
#     print ""YES:"", var_out_yes

    print ""--> The arrays are equal:"", np.array_equal(var_out_no, var_out_yes)
    print(""\n"")
```

The output obtained is the following:

```
decoder/memory_layer/kernel:0
-----------------------------
--> The arrays are equal: True


decoder/attention/lstm/kernel:0
-------------------------------
--> The arrays are equal: False


decoder/attention/lstm/bias:0
-----------------------------
--> The arrays are equal: True


decoder/attention/bahdanau_attention/query_layer/kernel:0
---------------------------------------------------------
--> The arrays are equal: False


decoder/attention/bahdanau_attention/attention_v:0
--------------------------------------------------
--> The arrays are equal: False


decoder/attention/attention_layer/kernel:0
------------------------------------------
--> The arrays are equal: False

```

More precisely, here are the two different weights for the LSTM cell (`LAS/decoder/attention/multi_rnn_cell/cell_0/lstm_cell/kernel:0`) obtained in each of the two cases for `alignment_history`:

- `alignment_history = False`:

```
[[  1.93134502e-01   5.19581586e-02  -2.18084604e-01 ...,   1.71998128e-01
    2.81669050e-02  -1.70363069e-01]
 [  1.95658550e-01   7.72098750e-02   2.17303529e-01 ...,   2.07259521e-01
    5.78315109e-02  -1.17801599e-01]
 [ -6.88283443e-02  -1.35597080e-01   1.65620014e-01 ...,  -1.32969454e-01
   -2.11523965e-01  -1.86820269e-01]
 ..., 
 [ -1.15171000e-01  -8.01331848e-02   1.30112335e-01 ...,   2.73928046e-04
    1.80437252e-01   1.90824643e-01]
 [ -7.53998458e-02   1.86289057e-01   1.80155411e-01 ...,  -1.64348409e-01
    2.10424170e-01  -1.46689758e-01]
 [ -2.08476633e-02   7.97681957e-02  -2.03553244e-01 ...,  -1.91280752e-01
    8.57728869e-02   1.46612525e-04]]
```

- `alignment_history = True`:

```
[[-0.18967885 -0.20777287  0.03707646 ...,  0.17251725  0.20710425
  -0.03941825]
 [-0.13857554 -0.05786179  0.17680736 ...,  0.01303713  0.05177127
  -0.12967519]
 [-0.18074478 -0.09467114  0.09963275 ..., -0.11447592  0.19544493
  -0.19714527]
 ..., 
 [ 0.16114177  0.14009587  0.11265792 ..., -0.11863185 -0.08480376
  -0.19559079]
 [-0.17472327  0.11717187  0.21214487 ...,  0.17373656 -0.15397248
   0.04700263]
 [-0.05792974  0.18947266  0.06573398 ..., -0.0308952   0.18018191
  -0.20467089]]
```"
22767,Feature request: Support tf.slice in contrib.receptive_field,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: n/a
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.11
- **Python version**: n/a
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: n/a

### Describe the problem
The `contrib.receptive_field` does not support `tf.slice` and this would be useful. Furthermore, it seems asymmetric to support `tf.pad` but not `tf.slice`.

My use case is: in ResNet, if I use 'VALID' padding in the blocks, I need to crop some edge pixels at the residual connection to maintain the alignment of the receptive fields. This voids the ability to compute the receptive field."
22766,nvcc error: string_view.h: constexpr function return is non-constant,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: This is what the bug is about, see below.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: no
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.11.0-0-gc19e29306c 1.11.0
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**: none
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 8.0, 9.0, 9.1
- **GPU model and memory**: doesn't matter
- **Exact command to reproduce**: See below.

### Describe the problem

Compiling some custom native op with `nvcc` fails, with basically this error:

    absl/strings/string_view.h(501): error: constexpr function return is non-constant

Compiling the same code with `g++` does not have this issue. This seems specifically related to the CUDA frontent `cudafe`.

### Source code / logs

Example code `test.cpp`:

```
// For Eigen::GpuDevice.
#define EIGEN_USE_GPU 1

// For Eigen::ThreadPoolDevice.
#define EIGEN_USE_THREADS 1

#include ""tensorflow/core/framework/op.h""
#include ""tensorflow/core/framework/shape_inference.h""
#include ""tensorflow/core/framework/op_kernel.h""
#include ""tensorflow/core/common_runtime/device.h""

#include <cuda.h>
#include <cuda_runtime.h>
#include <cublas_v2.h>
#include <math_constants.h>

#include ""tensorflow/core/platform/stream_executor.h""
```

Compile command:
`/usr/local/cuda-8.0/bin/nvcc -shared -O2 -std=c++11 -I /u/zeyer/py-envs/py36-tf111/lib/python3.6/site-packages/tensorflow/include -I /u/zeyer/py-envs/py36-tf111/lib/python3.6/site-packages/tensorflow/include/external/nsync/public -I /usr/local/cuda-8.0/include -L /usr/local/cuda-8.0/lib64 -x cu -DGOOGLE_CUDA=1 -Xcompiler -fPIC -D_GLIBCXX_USE_CXX11_ABI=0 -g test.cpp -o test.so -lblas -lf77blas -L/u/zeyer/py-envs/py36-tf111/lib/python3.6/site-packages/numpy/.libs -lopenblasp-r0-8dca6697.3.0.dev -L/u/zeyer/py-envs/py36-tf111/lib/python3.6/site-packages/tensorflow -ltensorflow_framework -v -Xcompiler -v`

The full compile output (including some more warnings) can be seen [here in the StackOverflow question](https://stackoverflow.com/questions/52665441/nvcc-error-string-view-h-constexpr-function-return-is-non-constant).
"
22763,AdamWOptimizer and learning rate decay,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04 
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**1.11:
- **Python version**:3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
I think I have found that the contrib.opt.AdamWOptimizer and associated decoupled weight decay optimizers do not function correctly when using learning rate decay without explicitly applying the decay to the weight_decay parameter. 

In the original paper, as seen in algorithm 2, the schedule multiplier is factored out and applied to the whole expression, the current interface means you have to do `AdamWOptimizer(weight_decay=wd*decay, learning_rate=lr*decay)` to achieve parity with the paper, this is not in its self an issue, but I think the documentation should reflect this difference. Alternatively the API could give a schedule multiplier parameter and then fixed lr and wd parameters used. 

Happy to submit a PR if someone can advise whether a documentation or interface update is the desired approach.

"
22762,New configure script alters .bazelrc in the source tree,"### System information
- Issue is when building from source 
- Linux Ubuntu 18
- HEAD of master
- python 3
- bazel 0.16.1

### Describe the problem
The issue is that the configure script now updates the contents of the .bazelrc file in place - thus changing the source code of the repo.  This is very unhelpful for change management.  There is a comment in the .bazelrc file which seems to suggest that the changes which are made should not be checked in. 

"
22761,"Error building on Windows, patch command failing","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.10
- **Python version**: 3.6.6
- **Bazel version (if compiling from source)**: 0.17.2
- **GCC/Compiler version (if compiling from source)**: Visual C++ Build Tools 2015
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: Follow build instructions on https://www.tensorflow.org/install/source_windows

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

When following the build instructions on https://www.tensorflow.org/install/source_windows I am unable to build tensorflow from source. I'm running on a clean Windows 10 install (nothing else installed but the requirements listed on the homepage). 

When running the bazel build step (CPU) with the command: $ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package I get two different errors, both when the build script tries to patch some files. The commands that fail are:

`C:\msys64\usr\bin\bash.exe -l -c patch -p1 -d C:/users/cm/_bazel_cm/xv6zejqw/external/kafka -i C:/tensorflow/third_party/kafka/config.patch
C:\msys64\usr\bin\bash.exe -l -c patch -p1 -d C:/users/cm/_bazel_cm/xv6zejqw/external/png_archive -i C:/tensorflow/third_party/png_fix_rpi.patch`

Copying the patch commands into the cmd.exe prompt makes the patch command ""hang"" until I manually terminate it. If I add ' ' around the command part of the bash invocation (e.g. bash -c 'patch ...') it works just fine. 

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

`$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
Starting local Bazel server and connecting to it...
DEBUG: C:/users/cm/_bazel_cm/xv6zejqw/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:
Auto-Configuration Warning: 'BAZEL_VC' is not set, start looking for the latest Visual C++ installed.
DEBUG: C:/users/cm/_bazel_cm/xv6zejqw/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:
Auto-Configuration Warning: Looking for VS%VERSION%COMNTOOLS environment variables, eg. VS140COMNTOOLS
DEBUG: C:/users/cm/_bazel_cm/xv6zejqw/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:
Auto-Configuration Warning: Visual C++ build tools found at C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@kafka//': Traceback (most recent call last):
        File ""C:/tensorflow/third_party/repo.bzl"", line 99
                _apply_patch(ctx, ctx.attr.patch_file)
        File ""C:/tensorflow/third_party/repo.bzl"", line 67, in _apply_patch
                _execute_and_check_ret_code(ctx, cmd)
        File ""C:/tensorflow/third_party/repo.bzl"", line 52, in _execute_and_check_ret_code
                fail(""Non-zero return code({1}) when ...))
Non-zero return code(256) when executing 'C:\msys64\usr\bin\bash.exe -l -c patch -p1 -d C:/users/cm/_bazel_cm/xv6zejqw/external/kafka -i C:/tensorflow/third_party/kafka/config.patch':
Stdout:
Stderr: Timed out
INFO: Elapsed time: 93,541s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (242 packages loaded)


cm@tbuilder  /c/tensorflow
$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
DEBUG: C:/users/cm/_bazel_cm/xv6zejqw/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:
Auto-Configuration Warning: 'BAZEL_VC' is not set, start looking for the latest Visual C++ installed.
DEBUG: C:/users/cm/_bazel_cm/xv6zejqw/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:
Auto-Configuration Warning: Looking for VS%VERSION%COMNTOOLS environment variables, eg. VS140COMNTOOLS
DEBUG: C:/users/cm/_bazel_cm/xv6zejqw/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:
Auto-Configuration Warning: Visual C++ build tools found at C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@png_archive//': Traceback (most recent call last):
        File ""C:/tensorflow/third_party/repo.bzl"", line 99
                _apply_patch(ctx, ctx.attr.patch_file)
        File ""C:/tensorflow/third_party/repo.bzl"", line 67, in _apply_patch
                _execute_and_check_ret_code(ctx, cmd)
        File ""C:/tensorflow/third_party/repo.bzl"", line 52, in _execute_and_check_ret_code
                fail(""Non-zero return code({1}) when ...))
Non-zero return code(256) when executing 'C:\msys64\usr\bin\bash.exe -l -c patch -p1 -d C:/users/cm/_bazel_cm/xv6zejqw/external/png_archive -i C:/tensorflow/third_party/png_fix_rpi.patch':
Stdout:
Stderr: Timed out
INFO: Elapsed time: 33,880s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)`
"
22760,Feature request for tf.contrib.image.translate,"### System information

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.4
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Not applicable
- **TensorFlow installed from (source or binary)**: Don't know
- **TensorFlow version (use command below)**: 1.18.0
- **Python version**: 2.7.15
- **Bazel version (if compiling from source)**: Don't know
- **GCC/Compiler version (if compiling from source)**: Don't know
- **CUDA/cuDNN version**: CUDA 7.5.17
- **GPU model and memory**: GeForce GTX 1080 Ti, 11GB
- **Exact command to reproduce**: tf.contrib.image.translate(img, output_of_network)

Hi, is there a way that the feature for computing gradients through the translation in `tf.contrib.image.translate` could be added? It does not involve any interpolation like in `tf.contrib.image.transform` or `tf.contrib.image.rotate` as in issue  #18649 
"
22759,Building custom OP with bazel in MacOs fails when upgrading TF from 1.8 to 1.11,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOs
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: no
- **TensorFlow installed from (source or binary)**: source by bazel ""http_archive""
- **TensorFlow version (use command below)**: 1.11.0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.15.2
- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 10.0.0 (clang-1000.11.45.2)
- **CUDA/cuDNN version**: no
- **GPU model and memory**: no
- **Exact command to reproduce**:  
clone branch https://github.com/shkarupa-alex/tfunicode/tree/feature/tf-1.11.0
export PYTHON_BIN_PATH=`which python2.7`
bazel clean --expunge
bazel build //tfunicode

### Describe the problem
I've got custom OPs library. I build it from source with bazel (for generating python wrappers).
It successfully builds with TF 1.8.0.
But when i try to upgrade TF to 1.11.0, building fails with such error:
> ld: can't open -exported_symbols_list file: -filelist
> clang: error: linker command failed with exit code 1 (use -v to see invocation)
File provided to clang with failed option is pywrap_tensorflow_internal_versionscript.lds (exists).

### Source code / logs
Full error log [err.txt](https://github.com/tensorflow/tensorflow/files/2449241/err.txt)
"
22758,Tensorflow CPU 1.5 VS 1.6 on Windows 7 - 64 bits - Bug on 1.6 version,"### System information
-  **script provided in TensorFlow :  import tensorflow as tf**.
- **OS Platform and Distribution Windows 7 64 bits**:
- **TensorFlow installed from binary**:
- **TensorFlow version 1.6**:
- **Python 3.6**:
- **Bazel version : no bazel**:
- **GCC/Compiler version : no compiler**:
- **CUDA/cuDNN version : CPU, not GPU**:
- **GPU model and memory: no GPU**:
- **compiling a python program with Pycharm**:


### Describe the problem
I got a bug when using Version 1.6.
Version 1.5 works fine on the same environment. But I need version Tensorflow 1.6 to run Mozilla's deepspeech tool.



### Source code / logs
Traceback (most recent call last):
  File ""C:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 658, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 571, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: DLL load failed with error code -1073741795

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 566, in run_nodebug
  File ""<azegrir1>"", line 1, in <module>
  File ""C:\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 658, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 571, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: DLL load failed with error code -1073741795

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'
"
22757,"keras.model.predict also needs two inputs (one for sample, and one for label) when used with tf.data together","I use tf.data.TFRecordDataset as the input of the tf.keras.model. The training and evaluation are OK. When I used the model for inference, I call the function tf.keras.model.prediction. However, it shows me an error 

> Please provide model inputs as a list or tuple of 2 elements: input and target pair.

I check the source code, and found that the function ""_standardize_user_data"" (line 988) in ""./tensorflow/python/keras/engine/training.py"" check the input number as the training and evaluation. I think it may be a bug, because when I does prediction, I don't know the true label.

tensorflow version: 1.11.0 gpu.

Thanks for your notice. I am looking forward your reply."
22756,C++ compilation error in rdma_mgr.cc,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:1.11 and master
- **Python version**:2.7,3.5,3.6
- **Bazel version (if compiling from source)**:0.16
- **GCC/Compiler version (if compiling from source)**:4.8
- **CUDA/cuDNN version**:9.0/9.2,7.2
- **GPU model and memory**:various
- **Exact command to reproduce**: compile with verbs

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

**It seems this static member functions (static void tensorflow::RdmaMgr::RegMemVisitors()) try to directly access non-static member (RdmaAdapter* rdma_adapter_;)**

ERROR: /tensorflow/tensorflow/contrib/verbs/BUILD:105:1: C++ compilation of rule '//tensorflow/contrib/verbs:rdma_mgr' failed (Exit 1)
In file included from tensorflow/contrib/verbs/rdma_mgr.cc:18:0:
./tensorflow/contrib/verbs/rdma_mgr.h: In static member function 'static void tensorflow::RdmaMgr::RegMemVisitors()':
./tensorflow/contrib/verbs/rdma_mgr.h:50:16: error: invalid use of member 'tensorflow::RdmaMgr::rdma_adapter_' in static member function
RdmaAdapter* rdma_adapter_;
^
tensorflow/contrib/verbs/rdma_mgr.cc:282:40: error: from this location
int32_t bus_id = TryToReadNumaNode(rdma_adapter_->context_->device) + 1;

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem."
22755,Will Tensorflow 2.0 support synchronizing batchnorm statistics across multiple GPUs?,"Have I written custom code: N/A
OS Platform and Distribution: cpe:/o:centos:centos:7
TensorFlow installed from: pip install 
TensorFlow version: 1.10
Bazel version: N/A
CUDA/cuDNN version: Cuda 9.0
GPU model and memory: 1080 Ti, 11G
Exact command to reproduce: N/A
Mobile device: N/A

I checked the [roadmap](https://www.tensorflow.org/community/roadmap) says the new feature:

```
Eager execution

    Use DistributionStrategy to utilize multiple GPUs and multiple TPU cores.
    Distributed training support (multi-machine).
    Performance improvements.
    Simpler export to a GraphDef/SavedModel.
```

I am wondering whether tf 2.0 will consider synchronizing batchnorm statistics across multiple GPUs which is extremely helpful for segmentation and other task.

Similar issue as [Synchronized BatchNorm statistics across GPUs](https://github.com/tensorflow/tensorflow/issues/18222)"
22750,Segmentation fault with small repro,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: archlinux
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:n/a
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:b'v1.9.0-rc2-5276-ge57874169f' 1.12.0-dev20181004
- **Python version**:3.6
- **Bazel version (if compiling from source)**:n/a
- **GCC/Compiler version (if compiling from source)**:n/a
- **CUDA/cuDNN version**:9.0
- **GPU model and memory**:1080Ti
- **Exact command to reproduce**:below

This code:
```python
import tensorflow as tf
import numpy as np

def f(boxes, scores):
    def f(X):
        prob, box = X
        output_shape = tf.shape(prob)
        ids = tf.reshape(tf.where(prob > 0.05), [-1])
        prob = tf.gather(prob, ids)
        box = tf.gather(box, ids)
        # prob = tf.Print(prob, [box, prob], summarize=100, message='boxandprob')
        selection = tf.image.non_max_suppression(box, prob, 100, 0.5)
        selection = tf.to_int32(tf.gather(ids, selection))
        selection = tf.Print(selection, [ids, selection], summarize=100, message='ids_selection_2')
        sorted_selection = -tf.nn.top_k(-selection, k=tf.size(selection))[0]
        mask = tf.sparse_to_dense(
            sparse_indices=sorted_selection,
            output_shape=output_shape,
            sparse_values=True,
            default_value=False)
        return mask

    masks = tf.map_fn(f, (scores, boxes), dtype=tf.bool, parallel_iterations=10)     # #cat x N
    return masks

with tf.device('/gpu:0'):
    boxes = tf.placeholder(tf.float32, (80, None, 4), name='boxes')
    scores = tf.placeholder(tf.float32, (80, None), name='scores')
    outs = f(boxes, scores)

config = tf.ConfigProto()
config.allow_soft_placement = True
sess = tf.Session(config=config)
data = dict(np.load('debug.npz'))
for k in range(1000):
    sess.run(outs, feed_dict={boxes: data['boxes'].transpose(1, 0, 2)[1:, :, :], scores: data['scores'][:, 1:].T})
    print(k)
```
causes segmentation fault on tf-nightly-gpu, as well as tensorflow-gpu==1.11.0. It works on 1.10.
It needs the data file `debug.npz` here: 
[debug.zip](https://github.com/tensorflow/tensorflow/files/2448247/debug.zip)

Note:
1. I tested on two machines, an error happen in >90% runs.
2. The code was distilled from the bug report about MaskRCNN evaluation [here](https://github.com/tensorpack/tensorpack/issues/919). The original bug report does not always segfault, but occasionally crash with other __different__ unreasonable TF internal errors, such as:
```
InvalidArgumentError (see above for traceback): scores has incompatible shape
         [[node map/while/non_max_suppression/NonMaxSuppressionV3 (defined at bug.py:15)  = NonMaxSuppressionV3[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](map/while/Gather
V2_1/_29, map/while/GatherV2/_31, map/while/non_max_suppression/NonMaxSuppressionV3/max_output_size/_33, map/while/non_max_suppression/iou_threshold/_35, map/while/non_max_suppression/score_thresh
old/_37)]]
``` 
```
2018-10-04 14:59:14.736180: F tensorflow/core/common_runtime/bfc_allocator.cc:458] Check failed: c->in_use() && (c->bin_num == kInvalidBinNum)                                                     
```
```
2018-10-04 14:59:49.523436: F tensorflow/core/common_runtime/bfc_allocator.cc:380] Check failed: h != kInvalidChunkHandle 
```
```
2018-10-05 00:12:03.720295: F ./tensorflow/core/framework/tensor.h:643] Check failed: new_num_elements == NumElements() (39 vs. 0)
```
```

InvalidArgumentError (see above for traceback): indices[1] = [0] is repeated
         [[{{node map/while/SparseToDense}} = SparseToDense[T=DT_BOOL, Tindices=DT_INT32, _class=[""loc:@map/while/TensorArrayWrite/TensorArrayWriteV3""], validate_indices=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](map/while/Neg_1/_51, map/while/Shape/_53, map/while/SparseToDense/sparse_values/_55, map/while/SparseToDense/default_value/_57)]]
         [[{{node map/while/SparseToDense/sparse_values/_54}} = _Send[T=DT_BOOL, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_111_map/while/SparseToDense/sparse_values"", _device=""/job:localhost/replica:0/task:0/device:GPU:0""](map/while/SparseToDense/sparse_values)]]
```
After distilled to this small repro, it seems to mostly do segfault. But the above error messages might help. Seems like a memory corruption."
22746,Cannot compile with C++17 because std::random_shuffle is removed,"std::random_shuffle has been deprecated since C++14 and [removed](https://meetingcpp.com/blog/items/stdrandom_shuffle-is-deprecated.html) from C++17. Unfortunately this means it's not possible to compile tensorflow with C++17.

Fortunately, there are not many [usages](https://github.com/tensorflow/tensorflow/search?l=C%2B%2B&q=random_shuffle
) present, and the [suggested replacement](https://meetingcpp.com/blog/items/stdrandom_shuffle-is-deprecated.html) is not very complicated.

"
22745,Trying to install tensorflow 1.11.0 on python 3.4.2,"Have I written custom code: No
OS Platform and Distribution: Mac Sierra
TensorFlow installed from: running on pyenv, currently on local pyenv of python 3.4.2, installed via `pip`
TensorFlow version: 1.11.0
Bazel version: N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: `pip install tensorflow`
Mobile device: N/A

This is the error I get,

> tensorflow 1.11.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.4.3 which is incompatible.

uninstllling and reinstalling setuptools to required version fixes it but ideally it should work with the default verision that comes with python 3.4.2"
22744,[Feature request] tfcompile AOT gonna support tensorflow_transform?,"Transform is out side of the graph. It could be great to have this support so that people do not have to re-implement this part in c++ while serve in c++ environment. 

The other way I can think of to solve this is: is it possible to make arbitrary feature transform a part of the tensorflow graph by modifying it afterwards using graph transform tools?"
22743,CUDA 10 tensorflow installation,"Hi,
I have NVDIA gtx 1080 and succesfully installed CUDA 10. I am having the following error.
Tensorflow installation does n ot warn me with anything but trying to import tensorflow I am getting the folllowing error.


`Python 3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56) 
[GCC 7.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""/home/ayshine/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/ayshine/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/ayshine/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/ayshine/anaconda3/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/ayshine/anaconda3/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/ayshine/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/home/ayshine/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/ayshine/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/ayshine/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/ayshine/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/ayshine/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/ayshine/anaconda3/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/ayshine/anaconda3/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
>>> 
`"
22741,TensorFlow build error possibly caused by renamed zlib license file,"I get a build failure building tensorflow using bazel, due to the fact that the following file apparently no longer exists: https://docs.python.org/2.7/_sources/license.txt

This file is specified in workspace.bzl (master):

  filegroup_external(
      name = ""org_python_license"",
      licenses = [""notice""],  # Python 2.0
      sha256_urls = {
          ""b5556e921715ddb9242c076cae3963f483aa47266c5e37ea4c187f77cc79501c"": [
              ""https://mirror.bazel.build/docs.python.org/2.7/_sources/license.txt"",
              ""https://docs.python.org/2.7/_sources/license.txt"",
          ],
      },
  )

It appears that the file has been replaced by one named license.rst.txt.

There is a second file which bazel can't download, for no obvious reason: zlib-1.2.11.tar.gz. I am able to manually download the file from the file's URLs in workspace.bzl.

Bazel successfully downloaded all other files.

As a workaround, I have acquired the two problematical files, and manually placed them in the bazel cache, verifying that they have the expected file hashes. However, when I run ""bazel fetch //tensorflow/tools/pip_package:build_pip_packages"", I get the following errors:

   ERROR: /home/cmesse02/.cache/bazel/_bazel_cmesse02/f4cce9ac06677e7da5b9b027126f0db6/external/org_python_pypi_backports_weakref/BUILD:17:1: no such package '@org_python_license//': java.io.IOException: Error downloading [https://mirror.bazel.build/docs.python.org/2.7/_sources/license.txt, https://docs.python.org/2.7/_sources/license.txt] to /home/cmesse02/.cache/bazel/_bazel_cmesse02/f4cce9ac06677e7da5b9b027126f0db6/external/org_python_license/license.txt: All mirrors are down: [GET returned 404 Not Found, sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target] and referenced by '@org_python_pypi_backports_weakref//:license'
   ERROR: /home/cmesse02/.cache/bazel/_bazel_cmesse02/f4cce9ac06677e7da5b9b027126f0db6/external/org_python_pypi_backports_weakref/BUILD:17:1: no such package '@org_python_license//': java.io.IOException: Error downloading [https://mirror.bazel.build/docs.python.org/2.7/_sources/license.txt, https://docs.python.org/2.7/_sources/license.txt] to /home/cmesse02/.cache/bazel/_bazel_cmesse02/f4cce9ac06677e7da5b9b027126f0db6/external/org_python_license/license.txt: All mirrors are down: [GET returned 404 Not Found, sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target] and referenced by '@org_python_pypi_backports_weakref//:license'
   ERROR: /home/cmesse02/local/tensorflow/tensorflow/tools/pip_package/BUILD:106:1: no such package '@zlib_archive//': java.io.IOException: Error downloading [https://mirror.bazel.build/zlib.net/zlib-1.2.11.tar.gz, https://zlib.net/zlib-1.2.11.tar.gz] to /home/cmesse02/.cache/bazel/_bazel_cmesse02/f4cce9ac06677e7da5b9b027126f0db6/external/zlib_archive/zlib-1.2.11.tar.gz: All mirrors are down: [sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target] and referenced by '//tensorflow/tools/pip_package:licenses'
   ERROR: Evaluation of query ""deps(//tensorflow/tools/pip_package:build_pip_package)"" failed: errors were encountered while computing transitive closure
   Building: no action

During the command, bazel removes the two files from its cache.

The argument could be made that this is a bazel issue. However, I think it might also be a tensorflow issue, at least for the license.txt file, because that file name is apparently wrong (it should be license.rst.txt)."
22740,why does tf.norm return a complex tensor?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
MacOS 10.14
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
N/A
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
v1.11.0-rc2-4-gc19e29306c 1.11.0
- **Python version**:
Python 3.6.5 :: Anaconda custom (64-bit)
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
`tf.norm(tf.Variable(0.0, dtype=tf.complex64)) `

### Describe the problem
The norm is always positive, why does it return a tensor of type tf.complex64?
"
22739,AttributeError: module 'tensorflow.python.keras.engine.base_layer' has no attribute 'Layer',"# I just update my keras so my tensorflow update from 1.7 update 1.9
but I found that tensorflow can't work,I just run `from tensorflow as tf`,the error show as title.My system just following with:
- ubuntu16.0.4 64bit
- gtx980m 8G
- intel i7 6820hk
- cuDNN7
- CUDA 9.0
I just tried that conda create a new environment and reinstall tensorflow with conda or pip,but the result is same.Use conda install from tsinghua's mirror,tensorflow's version is tensorflow1.10,pip install's tensorflow is tensorflow1.11,Had sameone happened the same question? 
```
~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in <module>()
     38 from tensorflow.python.keras.engine import training_generator
     39 from tensorflow.python.keras.engine import training_utils
---> 40 from tensorflow.python.keras.engine.network import Network
     41 from tensorflow.python.keras.utils.generic_utils import slice_arrays
     42 from tensorflow.python.ops import array_ops

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py in <module>()
     64 
     65 
---> 66 class Network(base_layer.Layer):
     67   """"""A `Network` is a composition of layers.
     68 

AttributeError: module 'tensorflow.python.keras.engine.base_layer' has no attribute 'Layer'

```"
22738,TensorFlow High level API with Low Level API,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22737,Batch Jacobian seems to have troubles to pass through `tf.nn.elu` and `tf.nn softplus`,"### System information
- Linux Ubuntu 16.04
- tensorflow 1.11.0
- python3.5.2

### The problem
Batch Jacobian seems to have troubles to pass through `tf.nn.elu` and `tf.nn softplus`

### Source code
```
import tensorflow as tf
from tensorflow.python.ops.parallel_for.gradients import batch_jacobian
tf.set_random_seed(1000)
import os
os.environ[""CUDA_DEVICE_ORDER""]=""PCI_BUS_ID""
os.environ[""CUDA_VISIBLE_DEVICES""]='1'
import argparse


parser = argparse.ArgumentParser(description='test jacobian', formatter_class=argparse.ArgumentDefaultsHelpFormatter)

parser.add_argument('--activation', '-a', default=None)

args = parser.parse_args()

print(""using tensorflow""+tf.__version__)
act1=None
if args.activation:
    act1 = getattr(tf.nn, args.activation)

print(""activation = ""+str(act1))

x = tf.placeholder(tf.float32, shape=[None, 784], name='input')
my_layer = tf.layers.dense(inputs=x,
                           units=10,
                           activation=act1
                           )

jac = batch_jacobian(my_layer,x)
```

 ### Error
```
$ python test-jacobian-simple.py -a softplus
using tensorflow1.11.0
activation = <function softplus at 0x7f345a279840>
Traceback (most recent call last):
  File ""test-jacobian-simple.py"", line 29, in <module>
    jac = batch_jacobian(my_layer,x)
  File ""/data1/env/tf1.11.0/lib/python3.5/site-packages/tensorflow/python/ops/parallel_for/gradients.py"", line 119, in batch_jacobian
    pfor_output = control_flow_ops.pfor(loop_fn, output_row_size)
  File ""/data1/env/tf1.11.0/lib/python3.5/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py"", line 129, in pfor
    outputs.append(converter.convert(loop_fn_output))
  File ""/data1/env/tf1.11.0/lib/python3.5/site-packages/tensorflow/python/ops/parallel_for/pfor.py"", line 1077, in convert
    output = self._convert_helper(y)
  File ""/data1/env/tf1.11.0/lib/python3.5/site-packages/tensorflow/python/ops/parallel_for/pfor.py"", line 1216, in _convert_helper
    if flags.FLAGS.op_conversion_fallback_to_while_loop:
  File ""/data1/env/tf1.11.0/lib/python3.5/site-packages/absl/flags/_flagvalues.py"", line 490, in __getattr__
    raise _exceptions.UnparsedFlagAccessError(error_message)
absl.flags._exceptions.UnparsedFlagAccessError: Trying to access flag --op_conversion_fallback_to_while_loop before flags were parsed.
```

Same error with:
`python test-jacobian-simple.py -a elu`

If I run with rel;u or leaky_relu instead it works fine:
`python test-jacobian-simple.py -a relu`
`python test-jacobian-simple.py -a leaky_relu`

Any ideas on how to fix this or how to workaround?
"
22736,s,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22732,CNN.Model pruning: no gain in speeding up of inference,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Windows 7
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:N/A
- **TensorFlow installed from (source or binary)**:pip install
- **TensorFlow version (use command below)**:1.10.1
- **Python version**:3.6
- **Bazel version (if compiling from source)**:N/A
- **GCC/Compiler version (if compiling from source)**:N/A
- **CUDA/cuDNN version**:use CPU
- **GPU model and memory**:N/A
- **Exact command to reproduce**:N/A


I implemented iterative pruning of a convolutional neural network using the example tensorflow **/tensorflow/contrib/model_pruning/** in order to increase the speed of inference .
The result of the procedure is an increase in the number of zero weight values(increase sparcity), which are still stored in dense tensors. I do not see a gain in speed.
How can I eliminate null weights used in convolutional layers from calculations? Can I remove  conv  filter knowing its sparseness?
Please give some recomendation on these issues.
"
22731,Layer Normalization : Choice of the correct axes where to apply mean and variance in case of LSTM Input,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: VERSION=""16.04.4 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
   No

- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.9.0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.11.1
- **GCC/Compiler version (if compiling from source)**: c++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609


- **CUDA/cuDNN version**: 9.1
- **GPU model and memory**:  Tesla K80

- **Exact command to reproduce**: no command to reproduce

### Describe the problem
I'm implementing layer_normalization in an LSTM cell. My input has the following shape                           
**[batch,time,features]**.
In tensorflow, we have the [LayerNormBasicLsmtCell](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LayerNormBasicLSTMCell).
Reading the code of this class, and in line [1417](https://github.com/tensorflow/tensorflow/blob/c19e29306ce1777456b2dbb3a14f511edf7883a8/tensorflow/contrib/rnn/python/ops/rnn_cell.py#L1417), the authors used the layer_norm function to apply the shift/scale of the inputs. 
Going to the implementation of the layer_norm and in line [2311](https://github.com/tensorflow/tensorflow/blob/c19e29306ce1777456b2dbb3a14f511edf7883a8/tensorflow/contrib/layers/python/layers/layers.py#L2311), tensorflow specified the axes where to compute the mean/variance. Knowing that `begin_param_axis=1` and the `inputs_rank=3` in my case.
The `norm_axes = list(range(1,3)) = [1,2]` so the mean/variance will be calculated on both axis time and features.
**My question is** : 
why do we need to compute the mean and variance of both axis=time and axis=Features?
Isn't okay to just do the nn.moments of the feature axis only? 
How can we set up the axes correctly in case of LSTM cells?

"
22730,"tf.test.is_gpu_available() blocking indefinitely, using 100% CPU","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Barely. See below.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10, Version 10.0.17134.285.
- **TensorFlow installed from (source or binary)**: Binary.
- **TensorFlow version (use command below)**: b'v1.11.0-rc2-4-gc19e29306c' 1.11.0.
- **Python version**: Python 3.6.6 (v3.6.6:4cf1f54eb7, Jun 27 2018, 03:37:03) [MSC v.1900 64 bit (AMD64)] on win32.
- **CUDA/cuDNN version**: CUDA V9.0.176, cuDNN v7.3.1.
- **GPU model and memory**: Nvidia GTX 970 4 GB.

### Reproduction
```
import tensorflow as tf
print(tf.test.is_gpu_available())
```

### Expected behaviour
Test prints `True`.

### Actual behaviour
The test script prints the following text and then hangs indefinitely, using 100% of one CPU core:
```
>>> tf.test.is_gpu_available()

2018-10-03 23:08:26.805592: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2

2018-10-03 23:08:27.171992: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties:

name: GeForce GTX 970 major: 5 minor: 2 memoryClockRate(GHz): 1.253

pciBusID: 0000:01:00.0

totalMemory: 4.00GiB freeMemory: 3.31GiB

2018-10-03 23:08:27.184134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
```"
22729,TF 1.11 failed to build,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.5 LTS (Xenial Xerus)
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: no
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: we are building TF 1.11, command below is not usable.

Here is git commit info:
```sh
$ git log -1 
commit c19e29306ce1777456b2dbb3a14f511edf7883a8
Author: Austin Anderson <angerson@google.com>
Date:   Tue Sep 25 14:50:52 2018 -0700

    Final version strings for 1.11.0 (#22513)
```
We are compiling code from this commit without any custom modification.
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.16.1
- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
- **CUDA/cuDNN version**:CUDA 8.0 / cuDNN 7
- **GPU model and memory**: (GeForce GTX 1080 Ti, 11178MiB) x 4
- **Exact command to reproduce**: `bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures`

### Describe the problem
We have successfully built TF 1.10 under the same machine and configuration, we can even build TF 1.10 as for now. However, it turns out that it's not working when building TF 1.11. Here are logs generated by bazel:

### Source code / logs
```
INFO: From Compiling tensorflow/core/kernels/l2loss_op_gpu.cu.cc [for host]:
external/com_google_absl/absl/strings/string_view.h(492): error: calling a __host__ function(""__builtin_strlen"") from a __device__ function(""absl::string_view::StrLenInternal"") i
s not allowed

1 error detected in the compilation of ""/tmp/tmpxft_00000f73_00000000-7_l2loss_op_gpu.cu.cpp1.ii"".
ERROR: /home/shyo/packages/tensorflow/tensorflow/core/kernels/BUILD:3602:1: output 'tensorflow/core/kernels/_objs/l2loss_op_gpu/l2loss_op_gpu.cu.pic.o' was not created
ERROR: /home/shyo/packages/tensorflow/tensorflow/core/kernels/BUILD:3602:1: not all outputs were created or valid

Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 285.352s, Critical Path: 52.48s
INFO: 2673 processes: 2673 local.
FAILED: Build did NOT complete successfully
```"
22728,while i was running the code          from keras.applications.vgg16 import VGG16 model = VGG16() i got the error Could not allocate ndarray. how to solve this?,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22727,Error with model.fit when input is a dataset,"EDIT: Probably solved, this is because no label information is provided. However, the error message is not very clear

### System information
- Tensorflow 1.10 and tf-nightly-gpu-1.12.0.dev20181004
- Ubuntu 16.04

model.fit() gives an error when the input data is a tensor that is concatenated from different datasets.

With this code example the error can be reproduced:
```
import tensorflow as tf
import numpy as np

if __name__ == '__main__':
    
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Dense(32, input_shape=(48,48,3)))
    model.add(tf.keras.layers.Dense(32))

    model.summary()

    # Compile with optimizer, loss and metrics
    optimizer = tf.train.AdamOptimizer(learning_rate=0.01)
    model.compile(loss=tf.keras.losses.mean_squared_error,
                  optimizer=optimizer)

    # make a dataset from a numpy array
    x = np.random.sample((1000,48,48,3))
    match = tf.data.Dataset.from_tensor_slices(x)

    classes = []
    for c in range(5):
        # In a real application here a dataset.filter() is used
        class_data = match
        class_data = class_data.batch(4)
        classes.append(class_data)
    
    data = list(map(lambda x: x.make_one_shot_iterator().get_next(), classes))
    batch = tf.concat(axis=0, values=data)

    model.fit(
        batch,
        steps_per_epoch=100,
        epochs=100)
```
The error is

> myproject/nightly-env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py"", line 137, in fit_loop
>     if issparse is not None and issparse(ins[i]) and not K.is_sparse(feed[i]):
> IndexError: list index out of range

The code can be even simplified further:
```
import tensorflow as tf
import numpy as np

if __name__ == '__main__':
    
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Dense(32, input_shape=(48,48,3)))
    model.add(tf.keras.layers.Dense(32))

    model.summary()

    # Compile with optimizer, loss and metrics
    optimizer = tf.train.AdamOptimizer(learning_rate=0.01)
    model.compile(loss=tf.keras.losses.mean_squared_error,
                  optimizer=optimizer)

    # make a dataset from a numpy array
    x = np.random.sample((1000,48,48,3))
    match = tf.data.Dataset.from_tensor_slices(x)
    match = match.batch(4)

    model.fit(
        match,
        steps_per_epoch=100,
        epochs=100)
```
This gives the same error"
22726,Object detection app built using TFlite is crashing,"Hi using tflite android app for object detection.
Followed the below link for building app:
https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/examples/android/app

APK is generated and installed on pixel 2XL devices.
Speech recognization works but object detection fails.
Attaching the logs for reference
[object_detection_logcat.txt](https://github.com/tensorflow/tensorflow/files/2446135/object_detection_logcat.txt)
"
22725,AttributeError: module 'tensorflow.python.training.checkpointable' has no attribute 'CheckpointableBase',"This problem has been posted before but the response does not help - there is no tf.nightly in my environment.

This same code worked before.

In the base Anaconda environment it said that tf did not have the following attributes: keras and __version__.

I created a new environment (COS801) in Anaconda.

This is the code:

import tensorflow as tf

This is the response:

AttributeError                            Traceback (most recent call last)
<ipython-input-1-3e38fd553753> in <module>()
----> 1 import tensorflow as tf
      2 for x in dir(tf): print(x)
      3 mnist = tf.keras.datasets.mnist
      4 
      5 (x_train, y_train),(x_test, y_test) = mnist.load_data()

~\Anaconda3\envs\COS801\lib\site-packages\tensorflow\__init__.py in <module>()
     22 
     23 # pylint: disable=g-bad-import-order
---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
     25 # pylint: disable=wildcard-import
     26 from tensorflow.tools.api.generator.api import *  # pylint: disable=redefined-builtin

~\Anaconda3\envs\COS801\lib\site-packages\tensorflow\python\__init__.py in <module>()
     61 
     62 # Framework
---> 63 from tensorflow.python.framework.framework_lib import *  # pylint: disable=redefined-builtin
     64 from tensorflow.python.framework.versions import *
     65 from tensorflow.python.framework import errors

~\Anaconda3\envs\COS801\lib\site-packages\tensorflow\python\framework\framework_lib.py in <module>()
    102 from tensorflow.python.framework.random_seed import set_random_seed
    103 from tensorflow.python.framework.sparse_tensor import convert_to_tensor_or_sparse_tensor
--> 104 from tensorflow.python.framework.importer import import_graph_def
    105 
    106 # Utilities for working with Tensors

~\Anaconda3\envs\COS801\lib\site-packages\tensorflow\python\framework\importer.py in <module>()
     30 from tensorflow.python.framework import dtypes
     31 from tensorflow.python.framework import errors
---> 32 from tensorflow.python.framework import function
     33 from tensorflow.python.framework import op_def_registry
     34 from tensorflow.python.framework import ops

~\Anaconda3\envs\COS801\lib\site-packages\tensorflow\python\framework\function.py in <module>()
     34 from tensorflow.python.framework import ops
     35 from tensorflow.python.ops import array_ops
---> 36 from tensorflow.python.ops import resource_variable_ops
     37 from tensorflow.python.ops import variable_scope as vs
     38 from tensorflow.python.util import compat

~\Anaconda3\envs\COS801\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py in <module>()
     33 from tensorflow.python.ops import gen_state_ops
     34 from tensorflow.python.ops import math_ops
---> 35 from tensorflow.python.ops import variables
     36 # go/tf-wildcard-import
     37 # pylint: disable=wildcard-import

~\Anaconda3\envs\COS801\lib\site-packages\tensorflow\python\ops\variables.py in <module>()
     38 
     39 @tf_export(""Variable"")
---> 40 class Variable(checkpointable.CheckpointableBase):
     41   """"""See the @{$variables$Variables How To} for a high level overview.
     42 

AttributeError: module 'tensorflow.python.training.checkpointable' has no attribute 'CheckpointableBase'
"
22724,How should we use the script tf_upgrade?,"I would like to use the [tf_upgrade](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/compatibility/tf_upgrade.py) script. However, of course, I cannot simply copy it to the directory which contains the file whose code I want to upgrade, because that script contains an import `from tensorflow.tools.compatibility import ast_edits` and, if I execute that script using `python tf_upgrade.py --infile main.py --outfile main-upgraded.py`, I get the error

```
Traceback (most recent call last):
  File ""tf_upgrade.py"", line 23, in <module>
    from tensorflow.tools.compatibility import ast_edits
ImportError: No module named compatibility
````

In fact, `compatibility` is not a module of `tools`. So, how exactly are we supposed to use the `tf_upgrade` script? The documentation or README me of https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/compatibility should be updated so that this info is present.

In a few posts on the web, it's written that we are simply supposed to copy that script, but this is not true (at least, not anymore).

I'm using Python 2.7 and TensorFlow 1.11.0 on a Mac OS X 10.13.6 (High Sierra)."
22723,bug: tf.train.Saver.save() fails if 2 tf.contrib.cudnn_rnn.cudnnLSTM instances passed to saver are built with build() method,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.5 LTS

- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy)**: N/A

- **TensorFlow installed from (source or binary)**: binary

- **TensorFlow version (use command below)**: v1.10.1-0-g4dcfddc5d1 1.10.1

- **Python version**: 3.6.0

- **Bazel version (if compiling from source)**: N/A

- **CUDA/cuDNN version**:
CUDA Version 9.0.176
cudnn: 7.1.4

- **GPU model and memory**:
NVIDIA GeForce GTX 1080 Ti
11GB

- **Exact command to reproduce**:
`python test.py`


### Describe the problem
**Description** 
If 2 instances of `tensorflow.contrib.cudnn_rnn.CudnnLSTM` are built via `build()` method they can not be saved together in a checkpoint. 

**Bug borders**
If instances are built implicitly in `__call__()`, no exceptions are thrown. Next code executes OK.

```python
import os
import tensorflow as tf
from tensorflow.contrib.cudnn_rnn import CudnnLSTM as CudnnLSTM
inp = tf.zeros([10, 32, 100])
lstm1 = CudnnLSTM(1, 128)
lstm2 = CudnnLSTM(2, 256)
lstm1(inp)
lstm2(inp)
saver = tf.train.Saver()
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    save_path = os.path.join('test_cudnn_lstm_save', '1')
    if not os.path.exists(save_path):
        os.makedirs(os.path.join(save_path))
    saver.save(sess, save_path)
```

 Also if only 1 instance is built using `build()` or if only 1 instance is passed to saver in `var_list` (see exсerpt below), save op is executed without exceptions.

```python
saver = tf.train.Saver([lstm1.saveable])
```

**Relevance**
I ran into this problem when tried to train my net on several GPUs. Shared CudnnLSTM params are built on CPU, while computations are made on GPUs.

Thank you in advance for any help!

### Source code / logs
Code to reproduce the bug

```python
import os
import tensorflow as tf
from tensorflow.contrib.cudnn_rnn import CudnnLSTM as CudnnLSTM
inp = tf.zeros([10, 32, 100])
lstm1 = CudnnLSTM(1, 128)
lstm2 = CudnnLSTM(2, 256)
lstm1.build(inp.shape)
lstm2.build(inp.shape)
saver = tf.train.Saver()
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    save_path = os.path.join('test_cudnn_lstm_save', '1')
    if not os.path.exists(save_path):
        os.makedirs(os.path.join(save_path))
    saver.save(sess, save_path)
```

Here is a traceback
```
2018-10-04 12:59:46.556727: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-10-04 12:59:46.635329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-10-04 12:59:46.635669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:01:00.0
totalMemory: 10.92GiB freeMemory: 10.01GiB
2018-10-04 12:59:46.635682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2018-10-04 12:59:46.841898: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-10-04 12:59:46.841930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 
2018-10-04 12:59:46.841937: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N 
2018-10-04 12:59:46.842099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9673 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-10-04 12:59:47.465382: W tensorflow/core/framework/op_kernel.cc:1275] OP_REQUIRES failed at save_restore_v2_ops.cc:134 : Invalid argument: Adding duplicate key: rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel
Traceback (most recent call last):
  File ""/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1278, in _do_call
    return fn(*args)
  File ""/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1263, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1350, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Adding duplicate key: rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel
         [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, cudnn_lstm/opaque_kernel/_1, transpose/_3, concat_5/_5, cudnn_lstm_1/opaque_kernel/_7, transpose_1/_9, transpose_2/_11, concat_11/_13, concat_17/_15)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""test.py"", line 17, in <module>
    saver.save(sess, save_path)
  File ""/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1620, in save
    {self.saver_def.filename_tensor_name: checkpoint_file})
  File ""/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 877, in run
    run_metadata_ptr)
  File ""/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1272, in _do_run
    run_metadata)
  File ""/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Adding duplicate key: rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel
         [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, cudnn_lstm/opaque_kernel/_1, transpose/_3, concat_5/_5, cudnn_lstm_1/opaque_kernel/_7, transpose_1/_9, transpose_2/_11, concat_11/_13, concat_17/_15)]]

Caused by op 'save/SaveV2', defined at:
  File ""test.py"", line 11, in <module>
    saver = tf.train.Saver()
  File ""/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1281, in __init__
    self.build()
  File ""/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1293, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1330, in _build
    build_save=build_save, build_restore=build_restore)
  File ""/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 775, in _build_internal
    save_tensor = self._AddSaveOps(filename_tensor, saveables)
  File ""/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 275, in _AddSaveOps
    save = self.save_op(filename_tensor, saveables)
  File ""/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 193, in save_op
    tensors)
  File ""/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1687, in save_v2
    shape_and_slices=shape_and_slices, tensors=tensors, name=name)
  File ""/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 454, in new_func
    return func(*args, **kwargs)
  File ""/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3155, in create_op
    op_def=op_def)
  File ""/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): Adding duplicate key: rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel
         [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, cudnn_lstm/opaque_kernel/_1, transpose/_3, concat_5/_5, cudnn_lstm_1/opaque_kernel/_7, transpose_1/_9, transpose_2/_11, concat_11/_13, concat_17/_15)]]
```"
22722,"Will it cause performance loss, when the grads are IndexedSlices?",https://github.com/tensorflow/tensorflow/blob/6f5d7a97cd2c0741ddfa756853ce5321377b5d53/tensorflow/contrib/distribute/python/cross_tower_ops.py#L313
22721,Enable USE_CBLAS_GEMM for Conv ops in C++ API,"### System information
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **TensorFlow installed from**: using bazel
- **TensorFlow version**: 1.10
- **Bazel version**: 0.17.2
- **Have I written custom code**: No
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: bazel run -c opt //tensorflow/example/example:example --define USE_CBLAS_GEMM=true
- **Mobile device**: N/A

### Describe the problem:
TF is not built from source, I am just configuring a specific example. So I am doing inference on TF C++ convolution layers, using bazel. I added 'USE_CBLAS_GEMM' and 'USE_GEMM_FOR_CONV' into the config setting of the build file.

```
config_setting(
    name = ""cblas"",
    values = {
        ""define"": ""USE_CBLAS_GEMM=1"",
    },
)
```
For running, 
```
bazel run -c opt //tensorflow/example/example:conv2d --define USE_CBLAS_GEMM=true
```

But the flag is not enabled and I cannot find a way to make sure whether CBLAS is running or not.
"
22718,tensorflow-gpu-1.10: failed call to cuInit: CUDA_ERROR_UNKNOWN,"### System information

- **OS Platform and Distribution** : Ubuntu 14.04

- **TensorFlow version** : 1.10 gpu 

- **TensorFlow installed from (source or binary)**: binary

- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: no

- **Python version**: 2.7

- **CUDA/cuDNN version**: 9.0 / 7

- **GPU model and memory**: Nvidia GeForce GTX TITAN X 

- **`nvidia-smi`**:

+------------------------------------------------------+                       
| NVIDIA-SMI 352.63     Driver Version: 384.130        |                       
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX TIT...  Off  | 0000:02:00.0     Off |                  N/A |
| 22%   61C    P0    75W / 250W |      0MiB / 12206MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GTX TIT...  Off  | 0000:03:00.0     Off |                  N/A |
| 23%   63C    P0    78W / 250W |      0MiB / 12207MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  GeForce GTX TIT...  Off  | 0000:83:00.0     Off |                  N/A |
| 22%   60C    P0    77W / 250W |      0MiB / 12207MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  GeForce GTX TIT...  Off  | 0000:84:00.0     Off |                  N/A |
| 22%   61C    P0    72W / 250W |      0MiB / 12207MiB |      0%      Default |
+-------------------------------+----------------------+-----------------

- **`find /lib/modules/ | grep -i nvidia `**

/lib/modules/4.2.0-27-generic/kernel/drivers/net/ethernet/nvidia
/lib/modules/4.2.0-27-generic/kernel/drivers/net/ethernet/nvidia/forcedeth.ko
/lib/modules/4.2.0-27-generic/kernel/drivers/video/fbdev/nvidia
/lib/modules/4.2.0-27-generic/kernel/drivers/video/fbdev/nvidia/nvidiafb.ko
/lib/modules/4.2.0-27-generic/updates/dkms/nvidia_384_uvm.ko
/lib/modules/4.2.0-27-generic/updates/dkms/nvidia_384.ko
/lib/modules/4.2.0-27-generic/updates/dkms/nvidia_384_modeset.ko
/lib/modules/4.2.0-27-generic/updates/dkms/nvidia_384_drm.ko

### Describe the problem

I upgrade the nvidia driver througth command: **`sudo apt-get install nvidia-384`**.  Then i found there are serveal nvidia driver installed througth command: **`sudo dpkg --list | grep nvidia-*`**，so i uninstalled these driver except nvidia-384 use commadn: **`sudo apt-get remove xxx`**.  After that, the info as follows:

ii  nvidia-384                                            384.130-0ubuntu0.14.04.1                            amd64        NVIDIA binary driver - version 384.130
ii  nvidia-opencl-icd-384                                 384.130-0ubuntu0.14.04.1                            amd64        NVIDIA OpenCL ICD
ii  nvidia-prime                                          0.6.2.1                                             amd64        Tools to enable NVIDIA's Prime
ii  nvidia-settings                                       352.39-0ubuntu1                                     amd64        Tool for configuring the NVIDIA graphics driver

**Error occured when i run tensorflow code as follows:**
**`import tensorflow as tf`
`tf.Session()`**

`2018-10-03 23:13:51.656015: E tensorflow/stream_executor/cuda/cuda_driver.cc:397   ] failed call to cuInit: CUDA_ERROR_UNKNOWN
2018-10-03 23:13:51.656131: I tensorflow/stream_executor/cuda/cuda_diagnostics.c   c:163] retrieving CUDA diagnostic information for host: root0-SCW4350-16
2018-10-03 23:13:51.656166: I tensorflow/stream_executor/cuda/cuda_diagnostics.c   c:170] hostname: root0-SCW4350-16
2018-10-03 23:13:51.656299: I tensorflow/stream_executor/cuda/cuda_diagnostics.c   c:194] libcuda reported version is: 384.130.0
2018-10-03 23:13:51.656428: I tensorflow/stream_executor/cuda/cuda_diagnostics.c   c:198] kernel reported version is: 384.130.0
2018-10-03 23:13:51.656465: I tensorflow/stream_executor/cuda/cuda_diagnostics.c   c:305] kernel version seems to match DSO: 384.130.0
<tensorflow.python.client.session.Session object at 0x7f30a08ae5d0>`

i have tried many solutions such as install `nvidia-modprobe` , `nvidia-cuda-mps-server` , `export LD_LIBRARY_PATH=""$LD_LIBRARY_PATH:/usr/local/cuda-9.0/lib64:/usr/local/cuda-9.0/extras/CUPTI/lib64:/usr/local/cuda-9.0/targets/x86_64-linux/lib/""` all of these don't work. 

Maybe  It is noteworthy that **`nvidia-smi`** shows that `NVIDIA-SMI 352.63     Driver Version: 384.130` and **`sudo dpkg --list | grep nvidia-*`** shows that **`nvidia-settings   352.39-0ubuntu1 `**. It seems that some moulde of nvidia-352 are not uninstalled. And i tried to installed nvidia driver througth **`nvidia_xxxx.run`** file，but the error remain while running tensorflow code.

Hopefully you can help me with this issue.
"
22715,"Trouble building from source Windows 10 Pro, MSVC, Cuda 10, Cudnn 7.3.1, Compute 7.5, Python 3.6 or 3.7","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10 Pro October Release 1809
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
No.
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:
1.11 (current)
- **Python version**:
3.7, Anaconda 5.3
- **Bazel version (if compiling from source)**:
0.17.2
- **GCC/Compiler version (if compiling from source)**:
MSVC 14.0 (but I want to switch to 15.0)
- **CUDA/cuDNN version**:
CUDA 10.0, CuDNN 7.3, Compute 7.5
- **GPU model and memory**:
RTX 2080 8GB GDDR6
- **Exact command to reproduce**:
```
python configure.py
bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
```

### Describe the problem

I can't find this information anywhere: How can you tell bazel to compile with Visual Studio 2017 (MSVC 15)?

I tried setting the `BAZEL_VS` and `BAZEL_VC` environment variables to `C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\` and `C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC`, restarting the shell, and running `python configure.py`, and `bazel clean`, and `bazel build ...`, but it still uses MSVC 14 (2015) rather than MSVC 15 (2017).

Trying to investigate if switching the compiler to VS2017 from VS2015 will solve my compilation issues.

### Source code / logs

I'm trying to investigate whether the following compile failures can be fixed by switching to VS 2017:

```

[12 / 21] [-----] BazelWorkspaceStatusAction stable-status.txt
ERROR: C:/users/joey/_bazel_joey/juz2ghmw/external/protobuf_archive/BUILD:659:1: C++ compilation of rule '@protobuf_archive//:python/google/protobuf/pyext/_message.so' failed (Exit 2): msvc_wrapper_for_nvcc.bat failed: error executing command
  cd C:/users/joey/_bazel_joey/juz2ghmw/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0
    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\INCLUDE;C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\include\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\winrt;
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\LIB\amd64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\LIB\amd64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17134.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17134.0\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\amd64;C:\WINDOWS\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\Tools;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Team Tools\Performance Tools;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Windows Kits\10\bin\x86;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.6.1 Tools\x64\;;C:\WINDOWS\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/Joey/Anaconda3/python.exe
    SET PYTHON_LIB_PATH=C:/Users/Joey/Anaconda3/lib/site-packages
    SET TEMP=C:\Users\Joey\AppData\Local\Temp
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=7.5
    SET TF_CUDA_VERSION=10.0
    SET TF_CUDNN_VERSION=7
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
    SET TMP=C:\Users\Joey\AppData\Local\Temp
  external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.bat /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/genfiles/external/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/bazel_tools /Iexternal/local_config_python /Ibazel-out/x64_windows-opt/genfiles/external/local_config_python /Ibazel-out/x64_windows-opt/bin/external/local_config_python /Iexternal/protobuf_archive/python /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/python /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/python /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /Iexternal/local_config_python/python_include /Ibazel-out/x64_windows-opt/genfiles/external/local_config_python/python_include /Ibazel-out/x64_windows-opt/bin/external/local_config_python/python_include /showIncludes /MD /O2 /DNDEBUG -w /arch:AVX /DHAVE_PTHREAD /wd4018 /wd4514 -DGOOGLE_PROTOBUF_HAS_ONEOF=1 -DPROTOBUF_PYTHON_ALLOW_OVERSIZE_PROTOS=1 /Fobazel-out/x64_windows-opt/bin/external/protobuf_archive/_objs/python/google/protobuf/pyext/_message.so/descriptor_containers.o /c external/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc
external/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc(172): error C2440: '=': cannot convert from 'const char *' to 'char *'
external/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc(172): note: Conversion loses qualifiers
external/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc(189): error C2440: '=': cannot convert from 'const char *' to 'char *'
external/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc(189): note: Conversion loses qualifiers
[2,999 / 6,086] Compiling external/protobuf_archive/src/google/protobuf/generated_message_util.cc; 1s local ... (39 actions running)
Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: C:/users/joey/downloads/tensorflow/tensorflow/contrib/lite/python/BUILD:42:1 C++ compilation of rule '@protobuf_archive//:python/google/protobuf/pyext/_message.so' failed (Exit 2): msvc_wrapper_for_nvcc.bat failed: error executing command
  cd C:/users/joey/_bazel_joey/juz2ghmw/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0
    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\INCLUDE;C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\include\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\winrt;
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\LIB\amd64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\LIB\amd64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17134.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17134.0\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\amd64;C:\WINDOWS\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\Tools;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Team Tools\Performance Tools;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Windows Kits\10\bin\x86;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.6.1 Tools\x64\;;C:\WINDOWS\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/Joey/Anaconda3/python.exe
    SET PYTHON_LIB_PATH=C:/Users/Joey/Anaconda3/lib/site-packages
    SET TEMP=C:\Users\Joey\AppData\Local\Temp
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=7.5
    SET TF_CUDA_VERSION=10.0
    SET TF_CUDNN_VERSION=7
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
    SET TMP=C:\Users\Joey\AppData\Local\Temp
  external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.bat /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/genfiles/external/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/bazel_tools /Iexternal/local_config_python /Ibazel-out/x64_windows-opt/genfiles/external/local_config_python /Ibazel-out/x64_windows-opt/bin/external/local_config_python /Iexternal/protobuf_archive/python /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/python /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/python /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /Iexternal/local_config_python/python_include /Ibazel-out/x64_windows-opt/genfiles/external/local_config_python/python_include /Ibazel-out/x64_windows-opt/bin/external/local_config_python/python_include /showIncludes /MD /O2 /DNDEBUG -w /arch:AVX /DHAVE_PTHREAD /wd4018 /wd4514 -DGOOGLE_PROTOBUF_HAS_ONEOF=1 -DPROTOBUF_PYTHON_ALLOW_OVERSIZE_PROTOS=1 /Fobazel-out/x64_windows-opt/bin/external/protobuf_archive/_objs/python/google/protobuf/pyext/_message.so/descriptor_containers.o /c external/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc
INFO: Elapsed time: 6.080s, Critical Path: 1.93s
INFO: 0 processes.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully

```
My goal is to build with cuda 10 and cudnn 7.3 for windows."
22710,[Cloud TPU] Intermittent freezes requiring reset of the TPU,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: r1.11
- **Python version**: N/A
- **Bazel version (if compiling from source)**: 0.16.1
- **GCC/Compiler version (if compiling from source)**: 7.2.0
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem

We are seeing intermittent freezes of the Cloud TPU when running VGG19 inference from the Julia frontend via xrt (do note that we're also seeing incorrect answers, which is filed as #22709 and may or may not be related). If the Cloud TPU freezes, the `TF_SessionRun` call never returns. An aggravating, but non-essential factor is the concurrent execution of the TPU profiler via `capture_tpu_profile`. In that situation both the main program and the `capture_tpu_profile` invocation never exit. If the profiler is running, we usually see only 2-3 successful runs until things start hanging. If I manually disconnect and forcefully kill the session (by severing the socket) and then reconnect, I see either continued freezes or errors of the form

```
ERROR: Tensorflow error: Status: Unable to enqueue when not opened, queue: [0000:00:04.0 PE0 C0 MC0 TN0 Queue HBM_WRITE]. State is: CLOSED
	 [[{{node XRTAllocate}} = XRTAllocate[_device=""/job:tpu_worker/replica:0/task:0/device:TPU:0""](XRTAllocate/Const_G1)]]
```

my standard protocol to recover from this has been the following:
- Reconnect the session
- Run the `ShutdownDistributedTPU` op (Remote session closes)
- Reconnect the session
- Run the `ShutdownDistributedTPU` op (Causes a non-fatal error, but the next step hangs if not done)
- Run the `ConfigureDistributedTPU` op (works)

afterwards I can usually use the TPU again.

### Source code / logs
XLA dump (batch size N=1, but the same happens for at least N=10 and N=100): https://gist.github.com/Keno/b54e4be146096daf4d464c1319639404
"
22709,[Cloud TPU] `dot` with non-standard layout operands produces incorrect output,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: r1.11
- **Python version**: N/A
- **Bazel version (if compiling from source)**: 0.16.1
- **GCC/Compiler version (if compiling from source)**: 7.2.0
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem

We are seeing incorrect answers from VGG19 from Cloud TPUs using the Julia frontend (VGG19 source here: https://github.com/FluxML/Metalhead.jl/blob/master/src/vgg19.jl). The Julia frontend generates XLA and submits it to the Cloud TPU using xrt. Reducing the test case, we find that there seems to be a problem with the `dot` operation when operating on arrays of non-standard data layout (julia is column major by default so tends to encounter this more, although the Julia->XLA compiler pass heuristically changes the layout of inputs/outputs when targeting TPUs to avoid padding if profitable). Example output showing the problem:

```
julia> W′′ = XRTArray(sess, W′)
10×10 XRTArray{Float32,(10, 10),2}:
  0.0117621   -0.0111235    0.0105805   -0.00403011   -0.00355716  -0.00718089  -0.0134884    -0.00459831    0.0197616    -0.00417151
 -0.00695795  -0.00393374   0.0090458   -0.00566363    0.0121302    0.00420088  -0.00750095    0.000967382   0.0220296     0.00428639
 -0.00687687  -0.00229628  -0.007712    -0.00997357   -0.00680831   0.00262956   0.000473469  -0.00876678   -0.011446     -0.00720634
  0.00530032   0.00782367  -0.00427682   0.00691867   -0.00343559   0.00912634  -0.0111816    -0.00729117    0.00522226   -0.00822838
 -0.00600947  -0.00683288  -0.0122932    0.000471149  -0.00142788   0.0201943    0.000141306   0.0052818     0.00387177   -0.00254829
  0.00345886   0.0106005   -3.07181e-5   0.0217212     0.00135223  -0.00411474   0.00345068   -0.00515104    0.00140949   -0.0132586
 -0.00233608  -0.0117845   -0.00938103   0.0190475    -0.00731929  -0.00104086  -0.0144478     0.00959422    0.00774611   -0.00810476
 -0.00620138  -0.00673539  -0.00782188   0.00821768    0.00439068   0.00334791  -0.00190924   -0.00682392   -0.00805664    0.0150204
 -0.00116647  -0.00305227   0.00475127   9.85245e-7    0.0024828    0.013036     0.00340338   -0.00387893    0.000677002   0.00680005
 -0.00499851   0.0115887   -0.00762315   0.0148181     0.0112692    0.00543651   0.0137934    -0.00666574    0.00736139    0.00202298

julia> x′′ = XRTArray(sess, x′)
10×1 XRTArray{Float32,(10, 1),2}:
 0.0
 6.706906
 0.0
 0.0
 2.0117874
 0.0
 0.0
 0.0
 0.0
 0.0

julia> W′′ * x′′
10×1 XRTArray{Float32,(10, 1),2}:
 -0.081760176
  0.0
  0.0
  0.0
  0.0
  0.0
  0.0
  0.0
  0.0
  0.0
```

The first value is correct, the rest of them are not (however I do believe I've seen cases where the first value was incorrect but non-zero also) - for python folks remember that `*` is matrix multiply in Julia. Dumping the XLA generated during the above session, we get:

```
ENTRY comp {
  comp0_parameter0 = f32[10,10]{0,1} parameter(0)
  comp0_parameter1 = f32[10,1]{0,1} parameter(1)
  ROOT comp0_dot3 = f32[10,1]{0,1} dot(comp0_parameter0, comp0_parameter1), lhs_contracting_dims={1}, rhs_contracting_dims={0}
}
```

Notice in particular the layout being `{0,1}` rather than the standard `{1,0}`. If I change the above mentioned layout heuristic to prefer the XLA standard layout, the issue disappears. The issue also disappears if the RHS is a plain vector, i.e. `f32[10]{0}` rather than `f32[10,1]{0,1}`. Further note that this issue was reduced from a case where the `dot` was not the final operation, but the zeros it generated nonetheless propagated. To me that would indicate that the output values are being written to the wrong place by the `dot` operation (as opposed to being a problem with the way that xrt retrieves the data from the device for example).

@michaelisard remarked that this sounded like a padding issue.
cc @eliben "
22706,CUDA 10,"**Edited:** 17-DEC-2018 Nightly builds are CUDA 10
**Edited:** 27-NOV-2018 Added 1.12 FINAL builds.
**Edited:** 29-OCT-2018 Added 1.12 RC2 builds.
**Edited:** 17-OCT-2018 Added 1.12 RC1 builds.


## **Nightly Builds are now CUDA 10 as of 16-DEC-2018**



- [**DONE**] CUDA 10 in the nightly builds mid-DEC 2018 if testing goes well
- CUDA 10 in official in mid-JAN-2019 as TF 1.13.

[Install instructions for CUDA 10](https://github.com/tensorflow/docs/pull/249) for a fresh system that should also work on an existing system if using `apt-get`.

TensorFlow will be upgrading to `CUDA 10` as soon as possible.  As of TF 1.11 building TensorFlow from source works just fine with `CUDA 10` and possibly even before.  There is nothing special needed other than all of the `CUDA`, `cuDNN`, `NCCL` (optional), and `TensorRT` (optional) libraries.  If people have some builds feel free to link them here (Keep in mind if you download them to decided what risk you want to take based on the source.) as well as any issues.  Also` NCCL` is now open source again and soon will be back to being automatically downloaded by `bazel` and included in the binary.

`CUDA 10` would likely go into TF 1.13, which is not scheduled.  I will update this post as I have more info to share.  I hope to flip nightly builds to CUDA 10 in November but the TF 1.13 release will likely push to early Jan.

I and some really cool people below made some binaries (even windows, see comments below) to help people out along with my really [bad ""instructions""](https://github.com/tfboyd/tf-tools/blob/master/install/cuda_10.md).  I suspect the instructions linked in the comments below are better.

Libraries used (rough list, similar to what I listed above)
  * Ubuntu 16.04 LTS on GCE from base Google Cloud Ubuntu image.
  * Python 2.7 because that is how I roll and someday I will change.
  * Install 410+ driver using apt-get
  * CUDA 10.0.130 from .tgz install (do not install the driver)
  * cuDNN-10.0 v7.3.0.29 rom tgz install
  * nccl 2.3.4 for CUDA 10 (I am not sure it matters I have mixed them up before)
  * TensorRT-5.0.0 for CUDA 10 cudnn 7.3 (TensorRT 5 was not stated as supported by Tensorflow when 
  * Compute Capability: 3.7,5.2,6.0,6.1,7.0 

_**TF 1.12.0 FINAL**_

**Python 2.7 (Ubuntu 16.04)**
   * [AVX2 + CUDA 10](https://storage.googleapis.com/tf-performance/tf_binary/tensorflow-1.12.0.a6d8ffa.AVX2.CUDA10-cp27-cp27mu-linux_x86_64.whl)

**Python 3.5 (Ubuntu 16.04)**
   * [AVX2 + CUDA 10](https://storage.googleapis.com/tf-performance/tf_binary/tensorflow-1.12.0.a6d8ffa.AVX2.CUDA10-cp35-cp35m-linux_x86_64.whl)

**Dockerfiles**
These are partial files and the apt-get commands should work on non-Docker systems as well assuming you have the NVIDIA apt-get repositories which should be the same as listed on [tf.org](https://www.tensorflow.org/install/gpu).
   * [Devel](https://github.com/tfboyd/tensorflow/blob/cuda_10_dockerfile/tensorflow/tools/dockerfiles/partials/nvidia-devel.partial.Dockerfile)  Confirmed by building TensorFlow at 1.12RC0.
  * [Runtime](https://github.com/tfboyd/tensorflow/blob/cuda_10_dockerfile/tensorflow/tools/dockerfiles/partials/nvidia.partial.Dockerfile)
"
22703,tensorflow-gpu being ruined by matplotlib,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: gpu-1.10.0-hf154084_0
- **Python version**: 3.6.6-hc3d631a_0
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: 1070
- **Exact command to reproduce**:
conda create --name test python=3.6.6 tensorflow-gpu spyder pandas
source activate test
-- problem starts here
conda install matplotlib
spyder
import tensorflow.contrib.training as training
- **Versions**:spyder=3.3.1, pandas=0.23.4, matplotlib=3.0.0

### Describe the problem
Tensorflow-gpu works fine (import tensorflow.contrib.training as training executes without errors) until the installation of matplotlib. After that ""import tensorflow.contrib.training as training"" gives this error:

    import tensorflow.contrib.training as training
    Traceback (most recent call last):
    
      File ""/home/nikolay/anaconda3/envs/test/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2961, in run_code
        exec(code_obj, self.user_global_ns, self.user_ns)
    
      File ""<ipython-input-1-f7e526cc11b2>"", line 1, in <module>
        import tensorflow.contrib.training as training
    
      File ""/home/nikolay/anaconda3/envs/test/lib/python3.6/site-packages/tensorflow/contrib/__init__.py"", line 48, in <module>
        from tensorflow.contrib import image
    
      File ""/home/nikolay/anaconda3/envs/test/lib/python3.6/site-packages/tensorflow/contrib/image/__init__.py"", line 70, in <module>
        from tensorflow.contrib.image.python.ops.single_image_random_dot_stereograms import single_image_random_dot_stereograms
    
      File ""/home/nikolay/anaconda3/envs/test/lib/python3.6/site-packages/tensorflow/contrib/image/python/ops/single_image_random_dot_stereograms.py"", line 27, in <module>
        ""_single_image_random_dot_stereograms.so""))
    
      File ""/home/nikolay/anaconda3/envs/test/lib/python3.6/site-packages/tensorflow/contrib/util/loader.py"", line 56, in load_op_library
        ret = load_library.load_op_library(path)
    
      File ""/home/nikolay/anaconda3/envs/test/lib/python3.6/site-packages/tensorflow/python/framework/load_library.py"", line 73, in load_op_library
        exec(wrappers, module.__dict__)
    
      File ""<string>"", line 27
        def single_image_random_dot_stereograms(depth_values, hidden_surface_removal=True, convergence_dots_size=8, dots_per_inch=72, eye_separation=2,5, mu=0,333299994, normalize=True, normalize_max=-100, normalize_min=100, border_level=0, number_colors=256, output_image_shape=[1024, 768, 1], output_data_window=[1022, 757], name=None):
                                                                                                                                                       ^
    SyntaxError: invalid syntax

if it matters, here is what matplotlib installation looks like:

    (test) xxx@xxx:~$ conda install matplotlib
    Solving environment: done
    
    ## Package Plan ##
    
      environment location: /home/xxx/anaconda3/envs/test
    
      added / updated specs: 
        - matplotlib
    
    
    The following NEW packages will be INSTALLED:
    
        cycler:     0.10.0-py36_0       
        kiwisolver: 1.0.1-py36hf484d3e_0
        matplotlib: 3.0.0-py36h5429711_0
    
    Proceed ([y]/n)? y
    
    Preparing transaction: done
    Verifying transaction: done
    Executing transaction: done
"
22701,tf.while_loop ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Colab
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Colab
- **TensorFlow installed from (source or binary)**: Colab
- **TensorFlow version (use command below)**: 1.11.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: Colab
- **GCC/Compiler version (if compiling from source)**: Colab
- **CUDA/cuDNN version**: Colab
- **GPU model and memory**: Colab
- **Exact command to reproduce**: Colab

### Describe the problem
The `tf.while_loop` no longer works. I have also checked the code from the official [documentation](https://www.tensorflow.org/api_docs/python/tf/while_loop).
I ran away from the eager mode issues and I found myself writing nonsense code:
`tf.cond`
`tf.while_loop`
`tf.Print`
`tf.gather`
`tf.scatter_update`

At the end of the day, nothing works properly neither the graph mode nor the eager one.


### Source code
```
import tensorflow as tf

n = 10000
x = tf.constant(list(range(n)))
c = lambda i, x: i < n
b = lambda i, x: (tf.Print(i + 1, [i]), tf.Print(x + 1, [i], ""x:""))
i, out = tf.while_loop(c, b, (0, x))
with tf.Session() as sess:
    print(sess.run(i))  # prints [0] ... [9999]

    # The following line may increment the counter and x in parallel.
    # The counter thread may get ahead of the other thread, but not the
    # other way around. So you may see things like
    # [9996] x:[9987]
    # meaning that the counter thread is on iteration 9996,
    # while the other thread is on iteration 9987
    print(sess.run(out).shape)
```

### logs
InvalidArgumentErrorTraceback (most recent call last)
<ipython-input-21-a546eb31055a> in <module>()
      7 i, out = tf.while_loop(c, b, (0, x))
      8 with tf.Session() as sess:
----> 9     print(sess.run(i))  # prints [0] ... [9999]
     10 
     11     # The following line may increment the counter and x in parallel.

/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)
    885     try:
    886       result = self._run(None, fetches, feed_dict, options_ptr,
--> 887                          run_metadata_ptr)
    888       if run_metadata:
    889         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1108     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1109       results = self._do_run(handle, final_targets, final_fetches,
-> 1110                              feed_dict_tensor, options, run_metadata)
   1111     else:
   1112       results = []

/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1284     if handle is None:
   1285       return self._do_call(_run_fn, feeds, fetches, targets, options,
-> 1286                            run_metadata)
   1287     else:
   1288       return self._do_call(_prun_fn, handle, feeds, fetches)

/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)
   1306           self._config.experimental.client_handles_error_formatting):
   1307         message = error_interpolation.interpolate(message, self._graph)
-> 1308       raise type(e)(node_def, op, message)
   1309 
   1310   def _extend_graph(self):

InvalidArgumentError: Input 1 of node while/Merge_3 was passed float from while/NextIteration_3:0 incompatible with expected int32.
"
22699,tensorflow 1.11.0 on Windows: No OpKernel was registered to support Op 'SparseMatMul' ,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 64bit
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.11.0
- **Python version**: 3.6.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: Run the test: //py_test_dir/tensorflow/python/keras:local_test

You can obtain the TensorFlow version with
```
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
b'v1.11.0-rc2-4-gc19e29306c' 1.11.0
```
### Describe the problem
```
python tensorflow\python\keras\layers\local_test.py
C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\util\tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  return _inspect.getargspec(target)
ERROR:tensorflow:No OpKernel was registered to support Op 'SparseMatMul' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

         [[{{node locally_connected1d_1_1/SparseMatMul}} = SparseMatMul[Ta=DT_FLOAT, Tb=DT_FLOAT, a_is_sparse=false, b_is_sparse=true, transpose_a=false, transpose_b=false](locally_connected1d_1_1/Reshape, locally_connected1d_1_1/Reshape_1)]]

Caused by op 'locally_connected1d_1_1/SparseMatMul', defined at:
  File ""tensorflow\python\keras\layers\local_test.py"", line 451, in <module>
    test.main()
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\test.py"", line 64, in main
    return _googletest.main(argv)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\googletest.py"", line 100, in main
    benchmark.benchmarks_main(true_main=main_wrapper)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\benchmark.py"", line 344, in benchmarks_main
    true_main()
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\googletest.py"", line 99, in main_wrapper
    return app.run(main=g_main, argv=args)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\googletest.py"", line 70, in g_main
    return unittest_main(argv=argv)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\main.py"", line 95, in __init__
    self.runTests()
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\main.py"", line 256, in runTests
    self.result = testRunner.run(self.test)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\runner.py"", line 176, in run
    test(result)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\suite.py"", line 84, in __call__
    return self.run(*args, **kwds)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\suite.py"", line 122, in run
    test(result)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\suite.py"", line 84, in __call__
    return self.run(*args, **kwds)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\suite.py"", line 122, in run
    test(result)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\case.py"", line 653, in __call__
    return self.run(*args, **kwds)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\case.py"", line 605, in run
    testMethod()
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\test_util.py"", line 776, in decorated
    f(self, **kwargs)
  File ""tensorflow\python\keras\layers\local_test.py"", line 63, in test_locallyconnected_1d
    input_shape=(num_samples, num_steps, input_dim))
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\testing_utils.py"", line 107, in layer_test
    y = layer(x)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 769, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\layers\local.py"", line 247, in call
    self.compute_output_shape(inputs.shape))
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\layers\local.py"", line 701, in local_conv_matmul
    output_flat = K.math_ops.sparse_matmul(inputs_flat, kernel, b_is_sparse=True)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\ops\gen_math_ops.py"", line 7887, in sparse_mat_mul
    b_is_sparse=b_is_sparse, name=name)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\util\deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\ops.py"", line 3272, in create_op
    op_def=op_def)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\ops.py"", line 1768, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'SparseMatMul' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

         [[{{node locally_connected1d_1_1/SparseMatMul}} = SparseMatMul[Ta=DT_FLOAT, Tb=DT_FLOAT, a_is_sparse=false, b_is_sparse=true, transpose_a=false, transpose_b=false](locally_connected1d_1_1/Reshape, locally_connected1d_1_1/Reshape_1)]]

E.ERROR:tensorflow:No OpKernel was registered to support Op 'SparseMatMul' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

         [[{{node locally_connected2d_1_1/SparseMatMul}} = SparseMatMul[Ta=DT_FLOAT, Tb=DT_FLOAT, a_is_sparse=false, b_is_sparse=true, transpose_a=false, transpose_b=false](locally_connected2d_1_1/Reshape, locally_connected2d_1_1/Reshape_1)]]

Caused by op 'locally_connected2d_1_1/SparseMatMul', defined at:
  File ""tensorflow\python\keras\layers\local_test.py"", line 451, in <module>
    test.main()
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\test.py"", line 64, in main
    return _googletest.main(argv)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\googletest.py"", line 100, in main
    benchmark.benchmarks_main(true_main=main_wrapper)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\benchmark.py"", line 344, in benchmarks_main
    true_main()
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\googletest.py"", line 99, in main_wrapper
    return app.run(main=g_main, argv=args)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\googletest.py"", line 70, in g_main
    return unittest_main(argv=argv)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\main.py"", line 95, in __init__
    self.runTests()
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\main.py"", line 256, in runTests
    self.result = testRunner.run(self.test)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\runner.py"", line 176, in run
    test(result)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\suite.py"", line 84, in __call__
    return self.run(*args, **kwds)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\suite.py"", line 122, in run
    test(result)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\suite.py"", line 84, in __call__
    return self.run(*args, **kwds)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\suite.py"", line 122, in run
    test(result)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\case.py"", line 653, in __call__
    return self.run(*args, **kwds)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\case.py"", line 605, in run
    testMethod()
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\test_util.py"", line 776, in decorated
    f(self, **kwargs)
  File ""tensorflow\python\keras\layers\local_test.py"", line 147, in test_locallyconnected_2d
    input_shape=(num_samples, num_row, num_col, stack_size))
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\testing_utils.py"", line 107, in layer_test
    y = layer(x)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 769, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\layers\local.py"", line 535, in call
    self.compute_output_shape(inputs.shape))
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\layers\local.py"", line 701, in local_conv_matmul
    output_flat = K.math_ops.sparse_matmul(inputs_flat, kernel, b_is_sparse=True)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\ops\gen_math_ops.py"", line 7887, in sparse_mat_mul
    b_is_sparse=b_is_sparse, name=name)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\util\deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\ops.py"", line 3272, in create_op
    op_def=op_def)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\ops.py"", line 1768, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'SparseMatMul' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

         [[{{node locally_connected2d_1_1/SparseMatMul}} = SparseMatMul[Ta=DT_FLOAT, Tb=DT_FLOAT, a_is_sparse=false, b_is_sparse=true, transpose_a=false, transpose_b=false](locally_connected2d_1_1/Reshape, locally_connected2d_1_1/Reshape_1)]]

EERROR:tensorflow:No OpKernel was registered to support Op 'SparseMatMul' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

         [[{{node locally_connected2d_2_1/SparseMatMul}} = SparseMatMul[Ta=DT_FLOAT, Tb=DT_FLOAT, a_is_sparse=false, b_is_sparse=true, transpose_a=false, transpose_b=false](locally_connected2d_2_1/Reshape, locally_connected2d_2_1/Reshape_1)]]

Caused by op 'locally_connected2d_2_1/SparseMatMul', defined at:
  File ""tensorflow\python\keras\layers\local_test.py"", line 451, in <module>
    test.main()
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\test.py"", line 64, in main
    return _googletest.main(argv)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\googletest.py"", line 100, in main
    benchmark.benchmarks_main(true_main=main_wrapper)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\benchmark.py"", line 344, in benchmarks_main
    true_main()
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\googletest.py"", line 99, in main_wrapper
    return app.run(main=g_main, argv=args)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\googletest.py"", line 70, in g_main
    return unittest_main(argv=argv)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\main.py"", line 95, in __init__
    self.runTests()
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\main.py"", line 256, in runTests
    self.result = testRunner.run(self.test)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\runner.py"", line 176, in run
    test(result)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\suite.py"", line 84, in __call__
    return self.run(*args, **kwds)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\suite.py"", line 122, in run
    test(result)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\suite.py"", line 84, in __call__
    return self.run(*args, **kwds)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\suite.py"", line 122, in run
    test(result)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\case.py"", line 653, in __call__
    return self.run(*args, **kwds)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\case.py"", line 605, in run
    testMethod()
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\test_util.py"", line 776, in decorated
    f(self, **kwargs)
  File ""tensorflow\python\keras\layers\local_test.py"", line 175, in test_locallyconnected_2d_channels_first
    input_shape=(num_samples, num_row, num_col, stack_size))
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\testing_utils.py"", line 107, in layer_test
    y = layer(x)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 769, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\layers\local.py"", line 535, in call
    self.compute_output_shape(inputs.shape))
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\layers\local.py"", line 701, in local_conv_matmul
    output_flat = K.math_ops.sparse_matmul(inputs_flat, kernel, b_is_sparse=True)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\ops\gen_math_ops.py"", line 7887, in sparse_mat_mul
    b_is_sparse=b_is_sparse, name=name)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\util\deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\ops.py"", line 3272, in create_op
    op_def=op_def)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\ops.py"", line 1768, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'SparseMatMul' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

         [[{{node locally_connected2d_2_1/SparseMatMul}} = SparseMatMul[Ta=DT_FLOAT, Tb=DT_FLOAT, a_is_sparse=false, b_is_sparse=true, transpose_a=false, transpose_b=false](locally_connected2d_2_1/Reshape, locally_connected2d_2_1/Reshape_1)]]

E.C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\backend.py:4435: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.
  xs.append(reshape(inputs[slices], (1, -1, feature_dim)))
ERROR:tensorflow:No OpKernel was registered to support Op 'SparseMatMul' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

         [[{{node SparseMatMul}} = SparseMatMul[Ta=DT_FLOAT, Tb=DT_FLOAT, a_is_sparse=false, b_is_sparse=true, transpose_a=false, transpose_b=false](Reshape_33, Reshape_34)]]

Caused by op 'SparseMatMul', defined at:
  File ""tensorflow\python\keras\layers\local_test.py"", line 451, in <module>
    test.main()
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\test.py"", line 64, in main
    return _googletest.main(argv)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\googletest.py"", line 100, in main
    benchmark.benchmarks_main(true_main=main_wrapper)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\benchmark.py"", line 344, in benchmarks_main
    true_main()
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\googletest.py"", line 99, in main_wrapper
    return app.run(main=g_main, argv=args)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\googletest.py"", line 70, in g_main
    return unittest_main(argv=argv)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\main.py"", line 95, in __init__
    self.runTests()
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\main.py"", line 256, in runTests
    self.result = testRunner.run(self.test)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\runner.py"", line 176, in run
    test(result)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\suite.py"", line 84, in __call__
    return self.run(*args, **kwds)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\suite.py"", line 122, in run
    test(result)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\suite.py"", line 84, in __call__
    return self.run(*args, **kwds)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\suite.py"", line 122, in run
    test(result)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\case.py"", line 653, in __call__
    return self.run(*args, **kwds)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\case.py"", line 605, in run
    testMethod()
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\test_util.py"", line 776, in decorated
    f(self, **kwargs)
  File ""tensorflow\python\keras\layers\local_test.py"", line 260, in test_locallyconnected_implementation
    out_2 = model_2.call(inputs)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\engine\sequential.py"", line 232, in call
    inputs, training=training, mask=mask)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\engine\sequential.py"", line 250, in _call_and_compute_mask
    x = layer.call(x, **kwargs)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\layers\local.py"", line 247, in call
    self.compute_output_shape(inputs.shape))
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\layers\local.py"", line 701, in local_conv_matmul
    output_flat = K.math_ops.sparse_matmul(inputs_flat, kernel, b_is_sparse=True)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\ops\gen_math_ops.py"", line 7887, in sparse_mat_mul
    b_is_sparse=b_is_sparse, name=name)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\util\deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\ops.py"", line 3272, in create_op
    op_def=op_def)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\ops.py"", line 1768, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'SparseMatMul' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

         [[{{node SparseMatMul}} = SparseMatMul[Ta=DT_FLOAT, Tb=DT_FLOAT, a_is_sparse=false, b_is_sparse=true, transpose_a=false, transpose_b=false](Reshape_33, Reshape_34)]]

E..
======================================================================
ERROR: test_locallyconnected_1d (__main__.LocallyConnectedLayersTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\client\session.py"", line 1292, in _do_call
    return fn(*args)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\client\session.py"", line 1275, in _run_fn
    self._extend_graph()
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\client\session.py"", line 1312, in _extend_graph
    tf_session.ExtendSession(self._session)
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'SparseMatMul' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

         [[{{node locally_connected1d_1_1/SparseMatMul}} = SparseMatMul[Ta=DT_FLOAT, Tb=DT_FLOAT, a_is_sparse=false, b_is_sparse=true, transpose_a=false, transpose_b=false](locally_connected1d_1_1/Reshape, locally_connected1d_1_1/Reshape_1)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\test_util.py"", line 776, in decorated
    f(self, **kwargs)
  File ""tensorflow\python\keras\layers\local_test.py"", line 63, in test_locallyconnected_1d
    input_shape=(num_samples, num_steps, input_dim))
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\testing_utils.py"", line 121, in layer_test
    actual_output = model.predict(input_data)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 1766, in predict
    self, x, batch_size=batch_size, verbose=verbose, steps=steps)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\engine\training_arrays.py"", line 324, in predict_loop
    batch_outs = f(ins_batch)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\backend.py"", line 2939, in __call__
    session = get_session()
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\backend.py"", line 465, in get_session
    _initialize_variables(session)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\backend.py"", line 719, in _initialize_variables
    [variables_module.is_variable_initialized(v) for v in candidate_vars])
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\test_util.py"", line 877, in run
    return super(ErrorLoggingSession, self).run(*args, **kwargs)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\client\session.py"", line 887, in run
    run_metadata_ptr)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\client\session.py"", line 1110, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\client\session.py"", line 1286, in _do_run
    run_metadata)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\client\session.py"", line 1308, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'SparseMatMul' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

         [[{{node locally_connected1d_1_1/SparseMatMul}} = SparseMatMul[Ta=DT_FLOAT, Tb=DT_FLOAT, a_is_sparse=false, b_is_sparse=true, transpose_a=false, transpose_b=false](locally_connected1d_1_1/Reshape, locally_connected1d_1_1/Reshape_1)]]

Caused by op 'locally_connected1d_1_1/SparseMatMul', defined at:
  File ""tensorflow\python\keras\layers\local_test.py"", line 451, in <module>
    test.main()
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\test.py"", line 64, in main
    return _googletest.main(argv)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\googletest.py"", line 100, in main
    benchmark.benchmarks_main(true_main=main_wrapper)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\benchmark.py"", line 344, in benchmarks_main
    true_main()
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\googletest.py"", line 99, in main_wrapper
    return app.run(main=g_main, argv=args)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\googletest.py"", line 70, in g_main
    return unittest_main(argv=argv)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\main.py"", line 95, in __init__
    self.runTests()
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\main.py"", line 256, in runTests
    self.result = testRunner.run(self.test)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\runner.py"", line 176, in run
    test(result)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\suite.py"", line 84, in __call__
    return self.run(*args, **kwds)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\suite.py"", line 122, in run
    test(result)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\suite.py"", line 84, in __call__
    return self.run(*args, **kwds)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\suite.py"", line 122, in run
    test(result)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\case.py"", line 653, in __call__
    return self.run(*args, **kwds)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\case.py"", line 605, in run
    testMethod()
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\test_util.py"", line 776, in decorated
    f(self, **kwargs)
  File ""tensorflow\python\keras\layers\local_test.py"", line 63, in test_locallyconnected_1d
    input_shape=(num_samples, num_steps, input_dim))
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\testing_utils.py"", line 107, in layer_test
    y = layer(x)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 769, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\layers\local.py"", line 247, in call
    self.compute_output_shape(inputs.shape))
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\layers\local.py"", line 701, in local_conv_matmul
    output_flat = K.math_ops.sparse_matmul(inputs_flat, kernel, b_is_sparse=True)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\ops\gen_math_ops.py"", line 7887, in sparse_mat_mul
    b_is_sparse=b_is_sparse, name=name)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\util\deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\ops.py"", line 3272, in create_op
    op_def=op_def)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\ops.py"", line 1768, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'SparseMatMul' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

         [[{{node locally_connected1d_1_1/SparseMatMul}} = SparseMatMul[Ta=DT_FLOAT, Tb=DT_FLOAT, a_is_sparse=false, b_is_sparse=true, transpose_a=false, transpose_b=false](locally_connected1d_1_1/Reshape, locally_connected1d_1_1/Reshape_1)]]


======================================================================
ERROR: test_locallyconnected_2d (__main__.LocallyConnectedLayersTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\client\session.py"", line 1292, in _do_call
    return fn(*args)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\client\session.py"", line 1275, in _run_fn
    self._extend_graph()
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\client\session.py"", line 1312, in _extend_graph
    tf_session.ExtendSession(self._session)
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'SparseMatMul' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

         [[{{node locally_connected2d_1_1/SparseMatMul}} = SparseMatMul[Ta=DT_FLOAT, Tb=DT_FLOAT, a_is_sparse=false, b_is_sparse=true, transpose_a=false, transpose_b=false](locally_connected2d_1_1/Reshape, locally_connected2d_1_1/Reshape_1)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\test_util.py"", line 776, in decorated
    f(self, **kwargs)
  File ""tensorflow\python\keras\layers\local_test.py"", line 147, in test_locallyconnected_2d
    input_shape=(num_samples, num_row, num_col, stack_size))
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\testing_utils.py"", line 121, in layer_test
    actual_output = model.predict(input_data)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 1766, in predict
    self, x, batch_size=batch_size, verbose=verbose, steps=steps)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\engine\training_arrays.py"", line 324, in predict_loop
    batch_outs = f(ins_batch)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\backend.py"", line 2939, in __call__
    session = get_session()
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\backend.py"", line 465, in get_session
    _initialize_variables(session)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\backend.py"", line 719, in _initialize_variables
    [variables_module.is_variable_initialized(v) for v in candidate_vars])
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\test_util.py"", line 877, in run
    return super(ErrorLoggingSession, self).run(*args, **kwargs)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\client\session.py"", line 887, in run
    run_metadata_ptr)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\client\session.py"", line 1110, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\client\session.py"", line 1286, in _do_run
    run_metadata)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\client\session.py"", line 1308, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'SparseMatMul' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

         [[{{node locally_connected2d_1_1/SparseMatMul}} = SparseMatMul[Ta=DT_FLOAT, Tb=DT_FLOAT, a_is_sparse=false, b_is_sparse=true, transpose_a=false, transpose_b=false](locally_connected2d_1_1/Reshape, locally_connected2d_1_1/Reshape_1)]]

Caused by op 'locally_connected2d_1_1/SparseMatMul', defined at:
  File ""tensorflow\python\keras\layers\local_test.py"", line 451, in <module>
    test.main()
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\test.py"", line 64, in main
    return _googletest.main(argv)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\googletest.py"", line 100, in main
    benchmark.benchmarks_main(true_main=main_wrapper)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\benchmark.py"", line 344, in benchmarks_main
    true_main()
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\googletest.py"", line 99, in main_wrapper
    return app.run(main=g_main, argv=args)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\googletest.py"", line 70, in g_main
    return unittest_main(argv=argv)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\main.py"", line 95, in __init__
    self.runTests()
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\main.py"", line 256, in runTests
    self.result = testRunner.run(self.test)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\runner.py"", line 176, in run
    test(result)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\suite.py"", line 84, in __call__
    return self.run(*args, **kwds)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\suite.py"", line 122, in run
    test(result)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\suite.py"", line 84, in __call__
    return self.run(*args, **kwds)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\suite.py"", line 122, in run
    test(result)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\case.py"", line 653, in __call__
    return self.run(*args, **kwds)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\case.py"", line 605, in run
    testMethod()
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\test_util.py"", line 776, in decorated
    f(self, **kwargs)
  File ""tensorflow\python\keras\layers\local_test.py"", line 147, in test_locallyconnected_2d
    input_shape=(num_samples, num_row, num_col, stack_size))
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\testing_utils.py"", line 107, in layer_test
    y = layer(x)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 769, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\layers\local.py"", line 535, in call
    self.compute_output_shape(inputs.shape))
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\layers\local.py"", line 701, in local_conv_matmul
    output_flat = K.math_ops.sparse_matmul(inputs_flat, kernel, b_is_sparse=True)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\ops\gen_math_ops.py"", line 7887, in sparse_mat_mul
    b_is_sparse=b_is_sparse, name=name)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\util\deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\ops.py"", line 3272, in create_op
    op_def=op_def)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\ops.py"", line 1768, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'SparseMatMul' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

         [[{{node locally_connected2d_1_1/SparseMatMul}} = SparseMatMul[Ta=DT_FLOAT, Tb=DT_FLOAT, a_is_sparse=false, b_is_sparse=true, transpose_a=false, transpose_b=false](locally_connected2d_1_1/Reshape, locally_connected2d_1_1/Reshape_1)]]


======================================================================
ERROR: test_locallyconnected_2d_channels_first (__main__.LocallyConnectedLayersTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\client\session.py"", line 1292, in _do_call
    return fn(*args)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\client\session.py"", line 1275, in _run_fn
    self._extend_graph()
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\client\session.py"", line 1312, in _extend_graph
    tf_session.ExtendSession(self._session)
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'SparseMatMul' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

         [[{{node locally_connected2d_2_1/SparseMatMul}} = SparseMatMul[Ta=DT_FLOAT, Tb=DT_FLOAT, a_is_sparse=false, b_is_sparse=true, transpose_a=false, transpose_b=false](locally_connected2d_2_1/Reshape, locally_connected2d_2_1/Reshape_1)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\test_util.py"", line 776, in decorated
    f(self, **kwargs)
  File ""tensorflow\python\keras\layers\local_test.py"", line 175, in test_locallyconnected_2d_channels_first
    input_shape=(num_samples, num_row, num_col, stack_size))
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\testing_utils.py"", line 121, in layer_test
    actual_output = model.predict(input_data)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 1766, in predict
    self, x, batch_size=batch_size, verbose=verbose, steps=steps)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\engine\training_arrays.py"", line 324, in predict_loop
    batch_outs = f(ins_batch)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\backend.py"", line 2939, in __call__
    session = get_session()
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\backend.py"", line 465, in get_session
    _initialize_variables(session)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\backend.py"", line 719, in _initialize_variables
    [variables_module.is_variable_initialized(v) for v in candidate_vars])
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\test_util.py"", line 877, in run
    return super(ErrorLoggingSession, self).run(*args, **kwargs)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\client\session.py"", line 887, in run
    run_metadata_ptr)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\client\session.py"", line 1110, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\client\session.py"", line 1286, in _do_run
    run_metadata)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\client\session.py"", line 1308, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'SparseMatMul' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

         [[{{node locally_connected2d_2_1/SparseMatMul}} = SparseMatMul[Ta=DT_FLOAT, Tb=DT_FLOAT, a_is_sparse=false, b_is_sparse=true, transpose_a=false, transpose_b=false](locally_connected2d_2_1/Reshape, locally_connected2d_2_1/Reshape_1)]]

Caused by op 'locally_connected2d_2_1/SparseMatMul', defined at:
  File ""tensorflow\python\keras\layers\local_test.py"", line 451, in <module>
    test.main()
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\test.py"", line 64, in main
    return _googletest.main(argv)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\googletest.py"", line 100, in main
    benchmark.benchmarks_main(true_main=main_wrapper)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\benchmark.py"", line 344, in benchmarks_main
    true_main()
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\googletest.py"", line 99, in main_wrapper
    return app.run(main=g_main, argv=args)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\googletest.py"", line 70, in g_main
    return unittest_main(argv=argv)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\main.py"", line 95, in __init__
    self.runTests()
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\main.py"", line 256, in runTests
    self.result = testRunner.run(self.test)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\runner.py"", line 176, in run
    test(result)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\suite.py"", line 84, in __call__
    return self.run(*args, **kwds)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\suite.py"", line 122, in run
    test(result)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\suite.py"", line 84, in __call__
    return self.run(*args, **kwds)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\suite.py"", line 122, in run
    test(result)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\case.py"", line 653, in __call__
    return self.run(*args, **kwds)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\case.py"", line 605, in run
    testMethod()
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\test_util.py"", line 776, in decorated
    f(self, **kwargs)
  File ""tensorflow\python\keras\layers\local_test.py"", line 175, in test_locallyconnected_2d_channels_first
    input_shape=(num_samples, num_row, num_col, stack_size))
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\testing_utils.py"", line 107, in layer_test
    y = layer(x)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 769, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\layers\local.py"", line 535, in call
    self.compute_output_shape(inputs.shape))
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\layers\local.py"", line 701, in local_conv_matmul
    output_flat = K.math_ops.sparse_matmul(inputs_flat, kernel, b_is_sparse=True)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\ops\gen_math_ops.py"", line 7887, in sparse_mat_mul
    b_is_sparse=b_is_sparse, name=name)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\util\deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\ops.py"", line 3272, in create_op
    op_def=op_def)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\ops.py"", line 1768, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'SparseMatMul' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

         [[{{node locally_connected2d_2_1/SparseMatMul}} = SparseMatMul[Ta=DT_FLOAT, Tb=DT_FLOAT, a_is_sparse=false, b_is_sparse=true, transpose_a=false, transpose_b=false](locally_connected2d_2_1/Reshape, locally_connected2d_2_1/Reshape_1)]]


======================================================================
ERROR: test_locallyconnected_implementation (__main__.LocallyConnectedLayersTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\client\session.py"", line 1292, in _do_call
    return fn(*args)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\client\session.py"", line 1275, in _run_fn
    self._extend_graph()
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\client\session.py"", line 1312, in _extend_graph
    tf_session.ExtendSession(self._session)
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'SparseMatMul' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

         [[{{node SparseMatMul}} = SparseMatMul[Ta=DT_FLOAT, Tb=DT_FLOAT, a_is_sparse=false, b_is_sparse=true, transpose_a=false, transpose_b=false](Reshape_33, Reshape_34)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\test_util.py"", line 776, in decorated
    f(self, **kwargs)
  File ""tensorflow\python\keras\layers\local_test.py"", line 262, in test_locallyconnected_implementation
    rtol=1e-5, atol=1e-5)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\test_util.py"", line 1489, in assertAllCloseAccordingToType
    a = self._GetNdArray(a)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\test_util.py"", line 1323, in _GetNdArray
    a = self.evaluate(a)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\test_util.py"", line 1035, in evaluate
    return sess.run(tensors)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\test_util.py"", line 877, in run
    return super(ErrorLoggingSession, self).run(*args, **kwargs)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\client\session.py"", line 887, in run
    run_metadata_ptr)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\client\session.py"", line 1110, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\client\session.py"", line 1286, in _do_run
    run_metadata)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\client\session.py"", line 1308, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'SparseMatMul' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

         [[{{node SparseMatMul}} = SparseMatMul[Ta=DT_FLOAT, Tb=DT_FLOAT, a_is_sparse=false, b_is_sparse=true, transpose_a=false, transpose_b=false](Reshape_33, Reshape_34)]]

Caused by op 'SparseMatMul', defined at:
  File ""tensorflow\python\keras\layers\local_test.py"", line 451, in <module>
    test.main()
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\test.py"", line 64, in main
    return _googletest.main(argv)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\googletest.py"", line 100, in main
    benchmark.benchmarks_main(true_main=main_wrapper)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\benchmark.py"", line 344, in benchmarks_main
    true_main()
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\googletest.py"", line 99, in main_wrapper
    return app.run(main=g_main, argv=args)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\platform\googletest.py"", line 70, in g_main
    return unittest_main(argv=argv)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\main.py"", line 95, in __init__
    self.runTests()
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\main.py"", line 256, in runTests
    self.result = testRunner.run(self.test)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\runner.py"", line 176, in run
    test(result)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\suite.py"", line 84, in __call__
    return self.run(*args, **kwds)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\suite.py"", line 122, in run
    test(result)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\suite.py"", line 84, in __call__
    return self.run(*args, **kwds)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\suite.py"", line 122, in run
    test(result)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\case.py"", line 653, in __call__
    return self.run(*args, **kwds)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\unittest\case.py"", line 605, in run
    testMethod()
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\test_util.py"", line 776, in decorated
    f(self, **kwargs)
  File ""tensorflow\python\keras\layers\local_test.py"", line 260, in test_locallyconnected_implementation
    out_2 = model_2.call(inputs)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\engine\sequential.py"", line 232, in call
    inputs, training=training, mask=mask)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\engine\sequential.py"", line 250, in _call_and_compute_mask
    x = layer.call(x, **kwargs)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\layers\local.py"", line 247, in call
    self.compute_output_shape(inputs.shape))
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\keras\layers\local.py"", line 701, in local_conv_matmul
    output_flat = K.math_ops.sparse_matmul(inputs_flat, kernel, b_is_sparse=True)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\ops\gen_math_ops.py"", line 7887, in sparse_mat_mul
    b_is_sparse=b_is_sparse, name=name)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\util\deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\ops.py"", line 3272, in create_op
    op_def=op_def)
  File ""C:\ci\tensorflow-base_1538579912447\work\test\lib\site-packages\tensorflow\python\framework\ops.py"", line 1768, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'SparseMatMul' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

         [[{{node SparseMatMul}} = SparseMatMul[Ta=DT_FLOAT, Tb=DT_FLOAT, a_is_sparse=false, b_is_sparse=true, transpose_a=false, transpose_b=false](Reshape_33, Reshape_34)]]


----------------------------------------------------------------------
Ran 8 tests in 7.794s

FAILED (errors=4)
```
"
22698,missing tf.keras.utils.OrderedEnqueuer in the builded tensorflow,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.11.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: no cuda
- **GPU model and memory**: no GPU
- **Exact command to reproduce**: N/A

### Describe the problem
I have recently added a PR https://github.com/tensorflow/tensorflow/pull/19183 that was supposed to add a possibility to use OrderedEnqueuer directly as `tf.keras.utils.OrderedEnqueuer`, but I have noticed that this actually did not help and that the `tensorflow.python.keras.utils.__init__.py` is actually not used and there is insted generated some other file created by `tensorflow/python/tools/api/generator/create_python_api.py`.

```
# This file is MACHINE GENERATED! Do not edit.
# Generated by: tensorflow/python/tools/api/generator/create_python_api.py script.
""""""Keras utilities.
""""""

from __future__ import print_function

from tensorflow.python.keras.activations import deserialize_keras_object
from tensorflow.python.keras.callbacks import Progbar
from tensorflow.python.keras.callbacks import Sequence
from tensorflow.python.keras.constraints import serialize_keras_object
from tensorflow.python.keras.datasets.boston_housing import get_file
from tensorflow.python.keras.engine.training_generator import GeneratorEnqueuer
from tensorflow.python.keras.models import CustomObjectScope
from tensorflow.python.keras.utils import HDF5Matrix
from tensorflow.python.keras.utils import SequenceEnqueuer
from tensorflow.python.keras.utils import convert_all_kernels_in_model
from tensorflow.python.keras.utils import custom_object_scope
from tensorflow.python.keras.utils import get_custom_objects
from tensorflow.python.keras.utils import multi_gpu_model
from tensorflow.python.keras.utils import normalize
from tensorflow.python.keras.utils import plot_model
from tensorflow.python.keras.utils import to_categorical

del print_function
``` 

### My question:
Could someone point me to some resource where I can find some guide of how is `tensorflow.keras.utils.__init__.py` being generated?
What might possibly be the problem that it isn't propagated from `tensorflow.python.keras.utils.__init__.py` to `tensorflow.keras.utils.__init__.py` in the pre-build tensorlow-package?

I'm not sure how much is this related, but I went through `tensorflow/tensorflow/tools/api/golden/v1` and `tensorflow/tensorflow/tools/api/golden/v2` and it looks like there is `OrderedEnqueuer` in both folders.

Is there any other place where I have to set that `OrderedEnqueuer` should be generated in `tensorflow.keras.utils.__init__.py`. I was thinking also about https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/BUILD file, but I'm not sure how it works.

Thank you in advance for pointing me to some relevant resources."
22697,Keras Mobilenet not loading after save,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
    - Yes, though very basic
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
    - Mac OSX 10.13.6
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
    - N/A
- **TensorFlow installed from (source or binary)**:
    - pip install tf-nightly
- **TensorFlow version (use command below)**:
    - v1.9.0-rc2-4942-g19b2383cc0 1.12.0-dev20180929
- **Python version**:
    - 3.6.5
- **Bazel version (if compiling from source)**:
    - N/A
- **GCC/Compiler version (if compiling from source)**:
    - N/A
- **CUDA/cuDNN version**:
    - N/A
- **GPU model and memory**:
    - N/A
- **Exact command to reproduce**:

```python
from tensorflow import keras
model = keras.applications.MobileNet()
model.compile(optimizer='adam', loss='categorical_crossentropy')
model.save('./model.h5')
loaded_model = keras.models.load_model('./model.h5')
```

### Describe the problem

Keras mobilenet_v1 model will not load after saving (see code sample above). An error is thrown in advanced_activations when loading a ReLU layer with a max_value parameter because the parameters are not of the expected type (float or None). They are a dict because that's how they were read from the config. The dict looks like this: {'type': 'ndarray', 'value': 6.0} but really should just be a float. This error occurs with the nightly build and also in 1.11.

### Source code / logs

```
Traceback (most recent call last):
  File ""error.py"", line 5, in <module>
    loaded_model = keras.models.load_model('./model.h5')
  File ""tmp_env/lib/python3.6/site-packages/tensorflow/python/keras/engine/saving.py"", line 230, in load_model
    model = model_from_config(model_config, custom_objects=custom_objects)
  File ""tmp_env/lib/python3.6/site-packages/tensorflow/python/keras/engine/saving.py"", line 310, in model_from_config
    return deserialize(config, custom_objects=custom_objects)
  File ""tmp_env/lib/python3.6/site-packages/tensorflow/python/keras/layers/serialization.py"", line 64, in deserialize
    printable_module_name='layer')
  File ""tmp_env/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py"", line 173, in deserialize_keras_object
    list(custom_objects.items())))
  File ""tmp_env/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 1292, in from_config
    process_layer(layer_data)
  File ""tmp_env/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 1278, in process_layer
    layer = deserialize_layer(layer_data, custom_objects=custom_objects)
  File ""tmp_env/lib/python3.6/site-packages/tensorflow/python/keras/layers/serialization.py"", line 64, in deserialize
    printable_module_name='layer')
  File ""tmp_env/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py"", line 175, in deserialize_keras_object
    return cls.from_config(config['config'])
  File ""tmp_env/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1617, in from_config
    return cls(**config)
  File ""tmp_env/lib/python3.6/site-packages/tensorflow/python/keras/layers/advanced_activations.py"", line 309, in __init__
    if max_value is not None and max_value < 0.:
TypeError: '<' not supported between instances of 'dict' and 'float'
```"
22695,Tensorflow C API: SessionRun() latency,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04 lts
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: no
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.10
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.16
- **GCC/Compiler version (if compiling from source)**: 7.3.0
- **CUDA/cuDNN version**: 9.2 / 7.3
- **GPU model and memory**: Nvidia GTX 1070 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683
- **Exact command to reproduce**: compile and execute my code

### Describe the problem
Scenario:
VGG16 from Tensorflow C API. We would like to inference a (potentially) different number of frames for each execution of 
SessionRun(), To do so we are allocating a tensor that holds data for the frames to execute (dimensions n_frames x 50 x 50 x 3).

Observed behavior and questions:
1. for a given n_frames, e.g. n_frames = 3, executing SessionRun() n times gives a time profiling like this:
~141ms
~3.8ms
~3.8ms
....
//n times

We are assuming the 1st  SessionRun() takes longer to upload the graph on GPU. Is that correct?

2. if we run a pattern like this:
n_frames = 3 --> call SessionRun() n times
n_frames = 3 --> call SessionRun() n times
n_frames = 1 --> call SessionRun() n times

the time profiling is like this:
~141ms
~3.8ms
~3.8ms
....
//n times
~3.8ms
~3.8ms
~3.8ms
....
//n times
**~70ms**
~3.8ms
~3.8ms
....
//n times

It looks like every time the input tensor size changes, something happens and the 1st execution takes longer.
Question:
Assuming that my application scenario is n_frames = 32 is the largest size for the input tensor is there a way 
to ""reserve"" GPU resources to get the same inference time (~3.8ms in the example) even though the actual tensor parameter passed to SessionRun() is smaller (i.e. n_frames = 1, n_frames= 2, etc...)



### Source code / logs
```
#include ""tensorflow/c/c_api.h""

#include <stdio.h>
#include <stdlib.h>
#include <memory.h>
#include <string.h>
#include <assert.h>
#include <vector>
#include <algorithm>
#include <iterator>
#include <iostream>


TF_Buffer* read_file(const char* file);

void free_buffer(void* data, size_t length) {
}

static void Deallocator(void* data, size_t length, void* arg) {
}

int main() {
  TF_Buffer* graph_def = read_file(""data/graph.pb"");
  TF_Graph* graph = TF_NewGraph();

  TF_Status* status = TF_NewStatus();
  TF_ImportGraphDefOptions* graph_opts = TF_NewImportGraphDefOptions();
  TF_GraphImportGraphDef(graph, graph_def, graph_opts, status);
  if (TF_GetCode(status) != TF_OK) {
          fprintf(stderr, ""ERROR: Unable to import graph %s"", TF_Message(status));
          return 1;
  }
  else {
          fprintf(stdout, ""Successfully imported graph\n"");
  }
  const int num_bytes_in = 1 * 50 * 50 * 3 * sizeof(float);
  const int num_bytes_out = 1 * 2 * sizeof(float);

  int64_t in_dims[] = {3, 50, 50, 3};
  int64_t out_dims[] = {3, 2};

  int64_t in_dims2[] = {2, 50, 50, 3};
  int64_t out_dims2[] = {2, 2};

  float values[3 * 50 * 50 * 3] = {0xff};
  float values2[2 * 50 * 50 * 3] = {0xff};

  std::vector<TF_Output> inputs;
  std::vector<TF_Tensor*> input_values;
  std::vector<TF_Tensor*> input_values2;

  inputs.push_back({TF_GraphOperationByName(graph, ""input_1""), 0});
  input_values.push_back(TF_NewTensor(TF_FLOAT, in_dims, 4, values, num_bytes_in, &Deallocator, 0));
  input_values2.push_back(TF_NewTensor(TF_FLOAT, in_dims, 4, values2, num_bytes_in, &Deallocator, 0));

  std::vector<TF_Output> outputs;
  outputs.push_back({TF_GraphOperationByName(graph, ""dense_3/Softmax""), 0});

  std::vector<TF_Tensor*> output_values;

  output_values.push_back(TF_AllocateTensor(TF_FLOAT, out_dims, 2, num_bytes_out));
  output_values.push_back(TF_AllocateTensor(TF_FLOAT, out_dims, 2, num_bytes_out));

  fprintf(stdout, ""Running session...\n"");
  TF_SessionOptions* sess_opts = TF_NewSessionOptions();
  TF_Session* session = TF_NewSession(graph, sess_opts, status);
  assert(TF_GetCode(status) == TF_OK);

  // about 140ms
  TF_SessionRun(session, nullptr,
                &inputs[0], &input_values[0], 1,
                &outputs[0], &output_values[0], 1,
                nullptr, 0, nullptr, status);
  // about 3ms
  TF_SessionRun(session, nullptr,
                &inputs[0], &input_values[0], 1,
                &outputs[0], &output_values[0], 1,
                nullptr, 0, nullptr, status);
  // about 3ms
  TF_SessionRun(session, nullptr,
                &inputs[0], &input_values[0], 1,
                &outputs[0], &output_values[0], 1,
                nullptr, 0, nullptr, status);

  // n times...
  // ...

  // NOW CHANGES TENSOR DIMENSIONS ---> about 70 ms
  TF_SessionRun(session, nullptr,
                &inputs[0], &input_values2[0], 1,
                &outputs[0], &output_values2[0], 1,
                nullptr, 0, nullptr, status);

  // about 3ms
  TF_SessionRun(session, nullptr,
                &inputs[0], &input_values2[0], 1,
                &outputs[0], &output_values2[0], 1,
                nullptr, 0, nullptr, status);

  // n times...
  // ...

  TF_Code c = TF_GetCode(status);

  std::cout << c << std::endl;

  for(size_t i = 0; i < output_values.size(); ++i)
  {
      if (output_values.at(i) == nullptr)
      {
          std::cout << ""bad parameters"" << std::endl;
      }
      else
      {
          const auto data = static_cast<float*>(TF_TensorData(output_values.at(i)));
          std::cout << ((data[1] > 0.5f) ? true : false) << std::endl;
      }
  }

  fprintf(stdout, ""Successfully run session\n"");

  TF_CloseSession(session, status);
  TF_DeleteSession(session, status);
  TF_DeleteSessionOptions(sess_opts);
  TF_DeleteImportGraphDefOptions(graph_opts);
  TF_DeleteGraph(graph);
  TF_DeleteStatus(status);
  return 0;
}

TF_Buffer* read_file(const char* file) {
  FILE *f = fopen(file, ""rb"");
  fseek(f, 0, SEEK_END);
  long fsize = ftell(f);
  fseek(f, 0, SEEK_SET);  //same as rewind(f);

  void* data = malloc(fsize);
  fread(data, fsize, 1, f);
  fclose(f);

  TF_Buffer* buf = TF_NewBuffer();
  buf->data = data;
  buf->length = fsize;
  buf->data_deallocator = free_buffer;
  return buf;
}
```
"
22694,r1.11 failed to build on debian,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:debian stretch
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:r1.11
- **Python version**:2.7.13
- **Bazel version (if compiling from source)**:0.17.2
- **GCC/Compiler version (if compiling from source)**:6.3.0 20170516 (Debian 6.3.0-18+deb9u1)
- **CUDA/cuDNN version**:N/A
- **GPU model and memory**: radeon RV370 256Mo
- CPU : Ryzen 5-1600 with 8Gb
- **Exact command to reproduce**: bazel build //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
compilation fails.
Is there a specific version of protobuf to use  ?

### Source code / logs
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded).
INFO: Found 1 target...
ERROR: /media/travail/tensorflow/tensorflow/contrib/tensorrt/BUILD:124:1: Executing genrule //tensorflow/contrib/tensorrt:trt_engine_op_pygenrule failed (Exit 127)
bazel-out/host/bin/tensorflow/contrib/tensorrt/gen_trt_engine_op_py_wrappers_cc: symbol lookup error: bazel-out/host/bin/tensorflow/contrib/tensorrt/gen_trt_engine_op_py_wrappers_cc: undefined symbol: _ZN6google8protobuf8internal26fixed_address_empty_stringB5cxx11E
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 231.846s, Critical Path: 52.14s
INFO: 38 processes: 38 local.
FAILED: Build did NOT complete successfully
"
22692,How to train multi-gpu on tensorflow using nccl library?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
Yes

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04

- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
Local custom build PC

- **TensorFlow installed from (source or binary)**:
binary

- **TensorFlow version (use command below)**:
1.10

- **Python version**:
3.5

- **Bazel version (if compiling from source)**:
No

- **GCC/Compiler version (if compiling from source)**:
gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609

- **CUDA/cuDNN version**:
9.0/7.2

- **GPU model and memory**:
Two GTX 1080 Ti each with 11G of memory 

- **Exact command to reproduce**:
IDK

### Describe the problem:
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I was wondering if you could provide a **tutorial** on how to train a simple CNN on multiple gpus on MNIST, or Cifar dataset that also explains how to use the **nccl library**.
The only tutorials available are the following: **https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py#L101**
**http://blog.s-schoener.com/2017-12-15-parallel-tensorflow-intro/**

**https://proteusmaster.urcf.drexel.edu/urcfwiki/index.php/Job_Script_Example_09_TensorFlow_MNIST_Multi-GPU-CNN**
None of which explains how to use nccl.


I also search for an answer on many websites but the only one i found is this from openai in Chinese:
http://openresearch.ai/t/nccl-efficient-tensorflow-multigpu-training/159

Although 1/6 of the population of earth does speak Chinese, the other 5/6 does not! So i was wondering if someone in tensorflow team that knows English and perhaps Chinese could help with this issue. Thank you. I would also appreciate if you do me a favor and do not refer or pass me to stackoverflow. Thanks again.
 "
22687,Bug in tf.keras.layers.ReLU.__init__,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
r1.11
- **Python version**:
3.6
- **Bazel version (if compiling from source)**:
0.15.0
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
There is a bug in tf.keras.layers.ReLU.\_\_init__. When passing keyward argument max_value with value None, which is also the default value, no gradient will be backword through it. I think is caused by the following code in the \_\_init__ function:
``` python
self.max_value = K.cast_to_floatx(max_value)
```
When max_value is None, the self.max_value will be nan of type float32. The self.max_value will be used with tf.clip_by_value as the upper bound and cause no gradient, since x < nan is false for any x.  I think the self.max_value should be None when max_value is None. 
``` python
self.max_value = K.cast_to_floatx(max_value) if max_value is not None else None
```
"
22682,TensorRT INT8 Calibration Bug,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary (pip install tensorflow-gpu)
- **TensorFlow version (use command below)**: 1.11
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0 / 7.3.0
- **GPU model and memory**: Titan Xp 12G
- **Exact command to reproduce**: python tf_to_trt.py

### Describe the problem
I trained an object detection network called AVOD (https://github.com/kujason/avod) and tried to use TensorRT to speed up inference. The model conversion seems to be successful but when I try to do the inference, it gave me this error:
```
Input node not found, at TensorRTInputPH_0
```
I believe this might be a bug. I would love to provide more info if needed. Any help would be greatly appreciated. Thanks a ton!

### Source code / logs
source code (modified from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tensorrt/test/test_tftrt.py)
```
import numpy as np
import time
import tensorflow as tf
from tensorflow.python.platform import gfile
from tensorflow.contrib import tensorrt as trt
from tensorflow.core.protobuf import config_pb2 as cpb2
from tensorflow.core.protobuf import rewriter_config_pb2 as rwpb2
from tensorflow.python.client import session as csess
from tensorflow.python.framework import constant_op as cop
from tensorflow.python.framework import dtypes as dtypes
from tensorflow.python.framework import importer as importer
from tensorflow.python.framework import ops as ops
from tensorflow.python.ops import array_ops as aops
from tensorflow.python.ops import math_ops as mops
from tensorflow.python.ops import nn as nn
from tensorflow.python.ops import nn_ops as nn_ops

output_node_name = ['proposals/nms/Gather',
                    'proposals/nms/Gather_1',
                    'avod_nms/Gather_1',
                    'avod_nms/Gather_2',
                    'avod_nms/Gather_3',
                    'avod_nms/Gather_4',
                    'avod_nms/Gather_5']
batch_size = 1
num_anchors = 1000
input_node = {
    'bev_input/bev_input_pl_batch': np.random.rand(batch_size, 700, 800, 6),
    'img_input/img_input_pl_batch': np.random.rand(batch_size, 360, 1200, 3),
    'pl_anchors/anchors_pl_batch': np.random.rand(batch_size, num_anchors, 6),
    'pl_anchors/bev_anchor_projections/bev_anchors_norm_pl_batch': np.random.rand(batch_size, num_anchors, 4),
    'pl_anchors/img_anchor_projections/img_anchors_norm_pl_batch': np.random.rand(batch_size, num_anchors, 4),
    'pl_anchors/sample_info/frame_calib_p2_batch': np.random.rand(batch_size, 3, 4),
    'pl_anchors/sample_info/ground_plane_batch': np.random.rand(batch_size, 4),
}
input_node_name = input_node.keys()

def execute_graph(gdef):
    """"""Run given graphdef once.""""""
    print(""executing"")
    gpu_options = None
    if trt.trt_convert.get_linked_tensorrt_version()[0] == 3:
        gpu_options = cpb2.GPUOptions(per_process_gpu_memory_fraction=0.50)
    sessconfig = cpb2.ConfigProto(gpu_options=gpu_options)
    ops.reset_default_graph()
    g = ops.Graph()
    with g.as_default():
        input_output = importer.import_graph_def(graph_def=gdef, return_elements=input_node_name + output_node_name)
        inputs = [elem.outputs[0] for elem in input_output[:len(input_node_name)]]
        outputs = [elem.outputs[0] for elem in input_output[len(input_node_name):]]
        inputs = {ph: input_node[name] for ph, name in zip(inputs, input_node_name)}
    with csess.Session(graph=g) as sess:
        for i in range(100):
            start = time.time()
            val = sess.run(outputs, inputs)
            print(time.time() - start)
    return val

def get_graph_def():
    #with tf.Session() as sess:
    model_filename ='/home/ecli/avod/avod/data/outputs/pyramid_people_with_aug_flexible_batchsize_example/checkpoints/frozen_model.pb'
    with gfile.FastGFile(model_filename, 'rb') as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())
    return graph_def

def get_optimized_graph(graph_def, precision='FP32'):
    return trt.create_inference_graph(
        input_graph_def=graph_def,
        outputs=output_node_name,
        max_batch_size=num_anchors,
        max_workspace_size_bytes=1 << 25,
        precision_mode=precision,
        minimum_segment_size=2,
        is_dynamic_op=False,
        maximum_cached_engines=1,
        cached_engine_batches=[])

orig_graph = get_graph_def()
trt_graph = get_optimized_graph(orig_graph, 'FP32')
execute_graph(trt_graph)                                                                                                                                                                                                                                                                                   
```
Here is the error log:
```
2018-10-02 17:55:49.128166: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 1
2018-10-02 17:55:49.128371: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session
2018-10-02 17:55:49.128693: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-10-02 17:55:49.130773: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:05:00.0
totalMemory: 11.90GiB freeMemory: 11.74GiB
2018-10-02 17:55:49.130795: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2018-10-02 17:55:49.797924: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-10-02 17:55:49.797993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2018-10-02 17:55:49.798002: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2018-10-02 17:55:49.798568: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11334 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:05:00.0, compute capability: 6.1)
2018-10-02 17:55:51.736457: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:853] MULTIPLE tensorrt candidate conversion: 110
2018-10-02 17:55:51.738030: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope '', converted to graph
2018-10-02 17:55:51.755022: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope '', converted to graph
2018-10-02 17:55:51.757490: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope '', converted to graph
2018-10-02 17:55:51.759075: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope '', converted to graph
2018-10-02 17:55:51.760315: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope '', converted to graph
2018-10-02 17:55:51.762196: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope '', converted to graph
2018-10-02 17:55:51.764472: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope '', converted to graph
2018-10-02 17:55:51.767273: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope '', converted to graph
2018-10-02 17:55:51.770472: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope '', converted to graph
2018-10-02 17:55:51.773202: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope '', converted to graph
2018-10-02 17:55:51.776177: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope '', converted to graph
2018-10-02 17:55:51.779351: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope '', converted to graph
2018-10-02 17:55:51.782439: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope '', converted to graph
2018-10-02 17:55:51.785647: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope '', converted to graph
2018-10-02 17:55:51.788626: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'anchor_predictor/cls_fc6_drop/dropout/', converted to graph
2018-10-02 17:55:51.791576: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'anchor_predictor/cls_fc7_drop/dropout/', converted to graph
2018-10-02 17:55:51.794625: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'anchor_predictor/reg_fc6_drop/dropout/', converted to graph
2018-10-02 17:55:51.797658: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'anchor_predictor/reg_fc7_drop/dropout/', converted to graph
2018-10-02 17:55:51.800434: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'avod_nms/', converted to graph
2018-10-02 17:55:51.803259: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'avod_nms/bev_projection/', converted to graph
2018-10-02 17:55:51.806150: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'avod_projection/img/', converted to graph
2018-10-02 17:55:51.809014: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'avod_projection/img/', converted to graph
2018-10-02 17:55:51.811796: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'avod_projection/img/', converted to graph
2018-10-02 17:55:51.814618: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'avod_projection/img/', converted to graph
2018-10-02 17:55:51.817641: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'avod_regression/', converted to graph
2018-10-02 17:55:51.820808: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'avod_regression/', converted to graph
2018-10-02 17:55:51.826062: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'avod_regression/', converted to graph
2018-10-02 17:55:51.830058: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'avod_regression/', converted to graph
2018-10-02 17:55:51.833936: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'avod_regression/', converted to graph
2018-10-02 17:55:51.837790: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'avod_regression/', converted to graph
2018-10-02 17:55:51.841412: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'avod_regression/', converted to graph
2018-10-02 17:55:51.844914: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'avod_regression/', converted to graph
2018-10-02 17:55:51.848273: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'avod_regression/', converted to graph
2018-10-02 17:55:51.851709: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'avod_regression/', converted to graph
2018-10-02 17:55:51.855383: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'avod_regression/', converted to graph
2018-10-02 17:55:51.858941: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'avod_regression/', converted to graph
2018-10-02 17:55:51.862423: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'avod_regression/', converted to graph
2018-10-02 17:55:51.866003: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_bottleneck/bottleneck/', converted to graph
2018-10-02 17:55:51.869569: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_bottleneck/bottleneck/BatchNorm/moments/', converted to graph
2018-10-02 17:55:51.873191: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/', converted to graph
2018-10-02 17:55:51.877027: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/', converted to graph
2018-10-02 17:55:51.881102: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/', converted to graph
2018-10-02 17:55:51.885375: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/', converted to graph
2018-10-02 17:55:51.890163: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/conv1/', converted to graph
2018-10-02 17:55:51.894660: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/conv1/conv1_1/BatchNorm/moments/', converted to graph
2018-10-02 17:55:51.899418: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/', converted to graph
2018-10-02 17:55:51.904140: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/conv1/conv1_2/BatchNorm/moments/', converted to graph
2018-10-02 17:55:51.909035: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/conv2/', converted to graph
2018-10-02 17:55:51.913803: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/conv2/conv2_1/BatchNorm/moments/', converted to graph
2018-10-02 17:55:51.918825: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/', converted to graph
2018-10-02 17:55:51.923751: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/conv2/conv2_2/BatchNorm/moments/', converted to graph
2018-10-02 17:55:51.929236: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/conv3/', converted to graph
2018-10-02 17:55:51.934581: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/conv3/conv3_1/BatchNorm/moments/', converted to graph
2018-10-02 17:55:51.940460: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/conv3/', converted to graph
2018-10-02 17:55:51.946734: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/conv3/conv3_2/BatchNorm/moments/', converted to graph
2018-10-02 17:55:51.953317: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/', converted to graph
2018-10-02 17:55:51.960702: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/conv3/conv3_3/BatchNorm/moments/', converted to graph
2018-10-02 17:55:51.968942: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/conv4/', converted to graph
2018-10-02 17:55:51.982940: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/conv4/conv4_1/BatchNorm/moments/', converted to graph
2018-10-02 17:55:51.994039: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/conv4/', converted to graph
2018-10-02 17:55:52.005718: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/conv4/conv4_2/BatchNorm/moments/', converted to graph
2018-10-02 17:55:52.017832: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/conv4/conv4_3/BatchNorm/moments/', converted to graph
2018-10-02 17:55:52.033887: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/pyramid_fusion1/BatchNorm/moments/', converted to graph
2018-10-02 17:55:52.061730: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/pyramid_fusion2/BatchNorm/moments/', converted to graph
2018-10-02 17:55:52.080736: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/pyramid_fusion3/BatchNorm/moments/', converted to graph
2018-10-02 17:55:52.098714: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/upconv1/BatchNorm/moments/', converted to graph
2018-10-02 17:55:52.116157: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/upconv1/BatchNorm/moments/', converted to graph
2018-10-02 17:55:52.133693: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/upconv2/BatchNorm/moments/', converted to graph
2018-10-02 17:55:52.150575: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/upconv2/BatchNorm/moments/', converted to graph
2018-10-02 17:55:52.166229: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/upconv3/BatchNorm/moments/', converted to graph
2018-10-02 17:55:52.182135: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/upconv3/BatchNorm/moments/', converted to graph
2018-10-02 17:55:52.197698: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'box_predictor/fc6/', converted to graph
2018-10-02 17:55:52.215250: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'box_predictor/fc6_drop/dropout/', converted to graph
2018-10-02 17:55:52.238710: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'box_predictor/fc7_drop/dropout/', converted to graph
2018-10-02 17:55:52.255985: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'box_predictor/fc8_drop/dropout/', converted to graph
2018-10-02 17:55:52.271427: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope '', converted to graph
2018-10-02 17:55:52.285149: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_bottleneck/bottleneck/BatchNorm/moments/', converted to graph
2018-10-02 17:55:52.298792: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_input/', converted to graph
2018-10-02 17:55:52.311283: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/', converted to graph
2018-10-02 17:55:52.325299: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/', converted to graph
2018-10-02 17:55:52.338964: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/', converted to graph
2018-10-02 17:55:52.352397: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/conv1/', converted to graph
2018-10-02 17:55:52.366803: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/conv1/conv1_1/', converted to graph
2018-10-02 17:55:52.380294: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/conv1/conv1_1/BatchNorm/moments/', converted to graph
2018-10-02 17:55:52.396706: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/', converted to graph
2018-10-02 17:55:52.411343: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/conv1/conv1_2/BatchNorm/moments/', converted to graph
2018-10-02 17:55:52.425555: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/conv2/', converted to graph
2018-10-02 17:55:52.440216: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/conv2/conv2_1/BatchNorm/moments/', converted to graph
2018-10-02 17:55:52.456120: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/', converted to graph
2018-10-02 17:55:52.474117: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/conv2/conv2_2/BatchNorm/moments/', converted to graph
2018-10-02 17:55:52.490652: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/conv3/', converted to graph
2018-10-02 17:55:52.509602: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/conv3/conv3_1/BatchNorm/moments/', converted to graph
2018-10-02 17:55:52.528796: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/conv3/', converted to graph
2018-10-02 17:55:52.548078: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/conv3/conv3_2/BatchNorm/moments/', converted to graph
2018-10-02 17:55:52.575318: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/', converted to graph
2018-10-02 17:55:52.602272: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/conv3/conv3_3/BatchNorm/moments/', converted to graph
2018-10-02 17:55:52.631036: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/conv4/', converted to graph
2018-10-02 17:55:52.661422: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/conv4/conv4_1/BatchNorm/moments/', converted to graph
2018-10-02 17:55:52.699411: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/conv4/', converted to graph
2018-10-02 17:55:52.770932: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/conv4/conv4_2/BatchNorm/moments/', converted to graph
2018-10-02 17:55:52.822570: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/conv4/conv4_3/BatchNorm/moments/', converted to graph
2018-10-02 17:55:52.865223: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/pyramid_fusion1/BatchNorm/moments/', converted to graph
2018-10-02 17:55:52.955710: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/pyramid_fusion2/BatchNorm/moments/', converted to graph
2018-10-02 17:55:53.005943: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/pyramid_fusion3/BatchNorm/moments/', converted to graph
2018-10-02 17:55:53.047781: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/upconv1/BatchNorm/moments/', converted to graph
2018-10-02 17:55:53.092236: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/upconv1/BatchNorm/moments/', converted to graph
2018-10-02 17:55:53.205154: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/upconv2/BatchNorm/moments/', converted to graph
2018-10-02 17:55:53.241229: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/upconv2/BatchNorm/moments/', converted to graph
2018-10-02 17:55:53.275667: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/upconv3/BatchNorm/moments/', converted to graph
2018-10-02 17:55:53.329565: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/upconv3/BatchNorm/moments/', converted to graph
2018-10-02 17:55:54.978083: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine my_trt_op_0 creation for segment 0, composed of 7 nodes succeeded.
2018-10-02 17:55:55.126408: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine my_trt_op_1 creation for segment 1, composed of 6 nodes succeeded.
2018-10-02 17:55:55.314276: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine my_trt_op_2 creation for segment 2, composed of 7 nodes succeeded.
2018-10-02 17:55:55.471285: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine my_trt_op_3 creation for segment 3, composed of 6 nodes succeeded.
2018-10-02 17:55:55.523551: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine my_trt_op_4 creation for segment 4, composed of 4 nodes succeeded.
2018-10-02 17:55:55.563377: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine my_trt_op_5 creation for segment 5, composed of 11 nodes succeeded.
2018-10-02 17:55:55.599293: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine my_trt_op_6 creation for segment 6, composed of 11 nodes succeeded.
2018-10-02 17:55:55.645190: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine my_trt_op_7 creation for segment 7, composed of 30 nodes succeeded.
2018-10-02 17:55:56.167550: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine my_trt_op_8 creation for segment 8, composed of 8 nodes succeeded.
2018-10-02 17:55:56.655547: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine my_trt_op_9 creation for segment 9, composed of 8 nodes succeeded.
2018-10-02 17:55:56.781781: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine my_trt_op_10 creation for segment 10, composed of 16 nodes succeeded.
2018-10-02 17:55:56.846884: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine my_trt_op_11 creation for segment 11, composed of 2 nodes succeeded.
2018-10-02 17:55:57.245480: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine my_trt_op_12 creation for segment 12, composed of 11 nodes succeeded.
2018-10-02 17:55:57.281911: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine my_trt_op_13 creation for segment 13, composed of 4 nodes succeeded.
2018-10-02 17:55:57.314901: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine anchor_predictor/cls_fc6_drop/dropout/my_trt_op_14 creation for segment 14, composed of 4 nodes succeeded.
2018-10-02 17:55:57.346135: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine anchor_predictor/cls_fc7_drop/dropout/my_trt_op_15 creation for segment 15, composed of 4 nodes succeeded.
2018-10-02 17:55:57.376888: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine anchor_predictor/reg_fc6_drop/dropout/my_trt_op_16 creation for segment 16, composed of 4 nodes succeeded.
2018-10-02 17:55:57.407602: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine anchor_predictor/reg_fc7_drop/dropout/my_trt_op_17 creation for segment 17, composed of 4 nodes succeeded.
2018-10-02 17:55:57.439721: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine avod_nms/my_trt_op_18 creation for segment 18, composed of 2 nodes succeeded.
2018-10-02 17:55:57.475973: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine avod_nms/bev_projection/my_trt_op_19 creation for segment 19, composed of 2 nodes succeeded.
2018-10-02 17:55:57.512484: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine avod_projection/img/my_trt_op_20 creation for segment 20, composed of 2 nodes succeeded.
2018-10-02 17:55:57.551874: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine avod_projection/img/my_trt_op_21 creation for segment 21, composed of 2 nodes succeeded.
2018-10-02 17:55:57.588385: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine avod_projection/img/my_trt_op_22 creation for segment 22, composed of 2 nodes succeeded.
2018-10-02 17:55:57.635975: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine avod_projection/img/my_trt_op_23 creation for segment 23, composed of 2 nodes succeeded.
2018-10-02 17:55:57.705177: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine avod_regression/my_trt_op_24 creation for segment 24, composed of 6 nodes succeeded.
2018-10-02 17:55:57.753337: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine avod_regression/my_trt_op_25 creation for segment 25, composed of 6 nodes succeeded.
2018-10-02 17:55:57.801667: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine avod_regression/my_trt_op_26 creation for segment 26, composed of 7 nodes succeeded.
2018-10-02 17:55:57.872030: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine avod_regression/my_trt_op_27 creation for segment 27, composed of 7 nodes succeeded.
2018-10-02 17:55:57.935326: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine avod_regression/my_trt_op_28 creation for segment 28, composed of 13 nodes succeeded.
2018-10-02 17:55:57.984397: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine avod_regression/my_trt_op_29 creation for segment 29, composed of 7 nodes succeeded.
2018-10-02 17:55:58.028066: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine avod_regression/my_trt_op_30 creation for segment 30, composed of 3 nodes succeeded.
2018-10-02 17:55:58.069669: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine avod_regression/my_trt_op_31 creation for segment 31, composed of 3 nodes succeeded.
2018-10-02 17:55:58.106058: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine avod_regression/my_trt_op_32 creation for segment 32, composed of 3 nodes succeeded.
2018-10-02 17:55:58.139464: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine avod_regression/my_trt_op_33 creation for segment 33, composed of 7 nodes succeeded.
2018-10-02 17:55:58.171112: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine avod_regression/my_trt_op_34 creation for segment 34, composed of 7 nodes succeeded.
2018-10-02 17:55:58.202794: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine avod_regression/my_trt_op_35 creation for segment 35, composed of 3 nodes succeeded.
2018-10-02 17:55:58.234781: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine avod_regression/my_trt_op_36 creation for segment 36, composed of 3 nodes succeeded.
2018-10-02 17:55:58.240744: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_bottleneck/bottleneck/my_trt_op_37 creation for segment 37, composed of 4 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_bottleneck/bottleneck/BatchNorm/moments/mean. Skipping...
2018-10-02 17:55:58.246462: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_bottleneck/bottleneck/BatchNorm/moments/my_trt_op_38 creation for segment 38, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_bottleneck/bottleneck/BatchNorm/moments/variance. Skipping...
2018-10-02 17:55:58.252287: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/my_trt_op_39 creation for segment 39, composed of 6 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/conv1/conv1_1/BatchNorm/moments/mean. Skipping...
2018-10-02 17:55:58.258291: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/my_trt_op_40 creation for segment 40, composed of 7 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/pyramid_fusion1/BatchNorm/moments/mean. Skipping...
2018-10-02 17:55:58.264489: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/my_trt_op_41 creation for segment 41, composed of 7 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/pyramid_fusion2/BatchNorm/moments/mean. Skipping...
2018-10-02 17:55:58.271844: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/my_trt_op_42 creation for segment 42, composed of 7 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/pyramid_fusion3/BatchNorm/moments/mean. Skipping...
2018-10-02 17:55:58.277734: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/conv1/my_trt_op_43 creation for segment 43, composed of 5 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/conv1/conv1_2/BatchNorm/moments/mean. Skipping...
2018-10-02 17:55:58.283385: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/conv1/conv1_1/BatchNorm/moments/my_trt_op_44 creation for segment 44, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/conv1/conv1_1/BatchNorm/moments/variance. Skipping...
2018-10-02 17:55:58.289392: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/my_trt_op_45 creation for segment 45, composed of 6 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/conv2/conv2_1/BatchNorm/moments/mean. Skipping...
2018-10-02 17:55:58.295056: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/conv1/conv1_2/BatchNorm/moments/my_trt_op_46 creation for segment 46, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/conv1/conv1_2/BatchNorm/moments/variance. Skipping...
2018-10-02 17:55:58.301151: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/conv2/my_trt_op_47 creation for segment 47, composed of 5 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/conv2/conv2_2/BatchNorm/moments/mean. Skipping...
2018-10-02 17:55:58.306853: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/conv2/conv2_1/BatchNorm/moments/my_trt_op_48 creation for segment 48, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/conv2/conv2_1/BatchNorm/moments/variance. Skipping...
2018-10-02 17:55:58.313223: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/my_trt_op_49 creation for segment 49, composed of 6 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/conv3/conv3_1/BatchNorm/moments/mean. Skipping...
2018-10-02 17:55:58.318923: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/conv2/conv2_2/BatchNorm/moments/my_trt_op_50 creation for segment 50, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/conv2/conv2_2/BatchNorm/moments/variance. Skipping...
2018-10-02 17:55:58.329949: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/conv3/my_trt_op_51 creation for segment 51, composed of 5 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/conv3/conv3_2/BatchNorm/moments/mean. Skipping...
2018-10-02 17:55:58.335704: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/conv3/conv3_1/BatchNorm/moments/my_trt_op_52 creation for segment 52, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/conv3/conv3_1/BatchNorm/moments/variance. Skipping...
2018-10-02 17:55:58.342597: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/conv3/my_trt_op_53 creation for segment 53, composed of 5 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/conv3/conv3_3/BatchNorm/moments/mean. Skipping...
2018-10-02 17:55:58.348323: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/conv3/conv3_2/BatchNorm/moments/my_trt_op_54 creation for segment 54, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/conv3/conv3_2/BatchNorm/moments/variance. Skipping...
2018-10-02 17:55:58.356835: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/my_trt_op_55 creation for segment 55, composed of 6 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/conv4/conv4_1/BatchNorm/moments/mean. Skipping...
2018-10-02 17:55:58.362568: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/conv3/conv3_3/BatchNorm/moments/my_trt_op_56 creation for segment 56, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/conv3/conv3_3/BatchNorm/moments/variance. Skipping...
2018-10-02 17:55:58.373950: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/conv4/my_trt_op_57 creation for segment 57, composed of 5 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/conv4/conv4_2/BatchNorm/moments/mean. Skipping...
2018-10-02 17:55:58.379702: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/conv4/conv4_1/BatchNorm/moments/my_trt_op_58 creation for segment 58, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/conv4/conv4_1/BatchNorm/moments/variance. Skipping...
2018-10-02 17:55:58.390926: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/conv4/my_trt_op_59 creation for segment 59, composed of 5 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/conv4/conv4_3/BatchNorm/moments/mean. Skipping...
2018-10-02 17:55:58.396673: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/conv4/conv4_2/BatchNorm/moments/my_trt_op_60 creation for segment 60, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/conv4/conv4_2/BatchNorm/moments/variance. Skipping...
2018-10-02 17:55:58.402288: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/conv4/conv4_3/BatchNorm/moments/my_trt_op_61 creation for segment 61, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/conv4/conv4_3/BatchNorm/moments/variance. Skipping...
2018-10-02 17:55:58.407929: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/pyramid_fusion1/BatchNorm/moments/my_trt_op_62 creation for segment 62, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/pyramid_fusion1/BatchNorm/moments/variance. Skipping...
2018-10-02 17:55:58.413521: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/pyramid_fusion2/BatchNorm/moments/my_trt_op_63 creation for segment 63, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/pyramid_fusion2/BatchNorm/moments/variance. Skipping...
2018-10-02 17:55:58.419108: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/pyramid_fusion3/BatchNorm/moments/my_trt_op_64 creation for segment 64, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/pyramid_fusion3/BatchNorm/moments/variance. Skipping...
2018-10-02 17:55:58.424731: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/upconv1/BatchNorm/moments/my_trt_op_65 creation for segment 65, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/upconv1/BatchNorm/moments/mean. Skipping...
2018-10-02 17:55:58.430337: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/upconv1/BatchNorm/moments/my_trt_op_66 creation for segment 66, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/upconv1/BatchNorm/moments/variance. Skipping...
2018-10-02 17:55:58.438284: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/upconv2/BatchNorm/moments/my_trt_op_67 creation for segment 67, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/upconv2/BatchNorm/moments/mean. Skipping...
2018-10-02 17:55:58.443884: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/upconv2/BatchNorm/moments/my_trt_op_68 creation for segment 68, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/upconv2/BatchNorm/moments/variance. Skipping...
2018-10-02 17:55:58.449507: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/upconv3/BatchNorm/moments/my_trt_op_69 creation for segment 69, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/upconv3/BatchNorm/moments/mean. Skipping...
2018-10-02 17:55:58.455066: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/upconv3/BatchNorm/moments/my_trt_op_70 creation for segment 70, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/upconv3/BatchNorm/moments/variance. Skipping...
2018-10-02 17:55:58.822526: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine box_predictor/fc6/my_trt_op_71 creation for segment 71, composed of 5 nodes succeeded.
2018-10-02 17:55:58.871051: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine box_predictor/fc6_drop/dropout/my_trt_op_72 creation for segment 72, composed of 4 nodes succeeded.
2018-10-02 17:55:58.920620: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine box_predictor/fc7_drop/dropout/my_trt_op_73 creation for segment 73, composed of 4 nodes succeeded.
2018-10-02 17:55:58.964147: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine box_predictor/fc8_drop/dropout/my_trt_op_74 creation for segment 74, composed of 4 nodes succeeded.
2018-10-02 17:55:58.974897: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine my_trt_op_75 creation for segment 75, composed of 5 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_bottleneck/bottleneck/BatchNorm/moments/mean. Skipping...
2018-10-02 17:55:58.985725: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_bottleneck/bottleneck/BatchNorm/moments/my_trt_op_76 creation for segment 76, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_bottleneck/bottleneck/BatchNorm/moments/variance. Skipping...
2018-10-02 17:55:59.071557: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine img_input/my_trt_op_77 creation for segment 77, composed of 8 nodes succeeded.
2018-10-02 17:55:59.085895: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/my_trt_op_78 creation for segment 78, composed of 7 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/pyramid_fusion1/BatchNorm/moments/mean. Skipping...
2018-10-02 17:55:59.092811: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/my_trt_op_79 creation for segment 79, composed of 7 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/pyramid_fusion2/BatchNorm/moments/mean. Skipping...
2018-10-02 17:55:59.101052: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/my_trt_op_80 creation for segment 80, composed of 7 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/pyramid_fusion3/BatchNorm/moments/mean. Skipping...
2018-10-02 17:55:59.108438: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/conv1/my_trt_op_81 creation for segment 81, composed of 5 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/conv1/conv1_2/BatchNorm/moments/mean. Skipping...
2018-10-02 17:55:59.119578: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/conv1/conv1_1/my_trt_op_82 creation for segment 82, composed of 4 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/conv1/conv1_1/BatchNorm/moments/mean. Skipping...
2018-10-02 17:55:59.129550: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/conv1/conv1_1/BatchNorm/moments/my_trt_op_83 creation for segment 83, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/conv1/conv1_1/BatchNorm/moments/variance. Skipping...
2018-10-02 17:55:59.139482: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/my_trt_op_84 creation for segment 84, composed of 6 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/conv2/conv2_1/BatchNorm/moments/mean. Skipping...
2018-10-02 17:55:59.147043: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/conv1/conv1_2/BatchNorm/moments/my_trt_op_85 creation for segment 85, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/conv1/conv1_2/BatchNorm/moments/variance. Skipping...
2018-10-02 17:55:59.164181: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/conv2/my_trt_op_86 creation for segment 86, composed of 5 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/conv2/conv2_2/BatchNorm/moments/mean. Skipping...
2018-10-02 17:55:59.174128: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/conv2/conv2_1/BatchNorm/moments/my_trt_op_87 creation for segment 87, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/conv2/conv2_1/BatchNorm/moments/variance. Skipping...
2018-10-02 17:55:59.186279: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/my_trt_op_88 creation for segment 88, composed of 6 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/conv3/conv3_1/BatchNorm/moments/mean. Skipping...
2018-10-02 17:55:59.195722: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/conv2/conv2_2/BatchNorm/moments/my_trt_op_89 creation for segment 89, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/conv2/conv2_2/BatchNorm/moments/variance. Skipping...
2018-10-02 17:55:59.225677: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/conv3/my_trt_op_90 creation for segment 90, composed of 5 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/conv3/conv3_2/BatchNorm/moments/mean. Skipping...
2018-10-02 17:55:59.235903: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/conv3/conv3_1/BatchNorm/moments/my_trt_op_91 creation for segment 91, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/conv3/conv3_1/BatchNorm/moments/variance. Skipping...
2018-10-02 17:55:59.255517: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/conv3/my_trt_op_92 creation for segment 92, composed of 5 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/conv3/conv3_3/BatchNorm/moments/mean. Skipping...
2018-10-02 17:55:59.266444: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/conv3/conv3_2/BatchNorm/moments/my_trt_op_93 creation for segment 93, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/conv3/conv3_2/BatchNorm/moments/variance. Skipping...
2018-10-02 17:55:59.279985: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/my_trt_op_94 creation for segment 94, composed of 6 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/conv4/conv4_1/BatchNorm/moments/mean. Skipping...
2018-10-02 17:55:59.291482: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/conv3/conv3_3/BatchNorm/moments/my_trt_op_95 creation for segment 95, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/conv3/conv3_3/BatchNorm/moments/variance. Skipping...
2018-10-02 17:55:59.310105: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/conv4/my_trt_op_96 creation for segment 96, composed of 5 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/conv4/conv4_2/BatchNorm/moments/mean. Skipping...
2018-10-02 17:55:59.316996: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/conv4/conv4_1/BatchNorm/moments/my_trt_op_97 creation for segment 97, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/conv4/conv4_1/BatchNorm/moments/variance. Skipping...
2018-10-02 17:55:59.364412: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/conv4/my_trt_op_98 creation for segment 98, composed of 5 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/conv4/conv4_3/BatchNorm/moments/mean. Skipping...
2018-10-02 17:55:59.373789: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/conv4/conv4_2/BatchNorm/moments/my_trt_op_99 creation for segment 99, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/conv4/conv4_2/BatchNorm/moments/variance. Skipping...
2018-10-02 17:55:59.380203: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/conv4/conv4_3/BatchNorm/moments/my_trt_op_100 creation for segment 100, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/conv4/conv4_3/BatchNorm/moments/variance. Skipping...
2018-10-02 17:55:59.389040: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/pyramid_fusion1/BatchNorm/moments/my_trt_op_101 creation for segment 101, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/pyramid_fusion1/BatchNorm/moments/variance. Skipping...
2018-10-02 17:55:59.411862: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/pyramid_fusion2/BatchNorm/moments/my_trt_op_102 creation for segment 102, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/pyramid_fusion2/BatchNorm/moments/variance. Skipping...
2018-10-02 17:55:59.421726: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/pyramid_fusion3/BatchNorm/moments/my_trt_op_103 creation for segment 103, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/pyramid_fusion3/BatchNorm/moments/variance. Skipping...
2018-10-02 17:55:59.432249: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/upconv1/BatchNorm/moments/my_trt_op_104 creation for segment 104, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/upconv1/BatchNorm/moments/mean. Skipping...
2018-10-02 17:55:59.443949: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/upconv1/BatchNorm/moments/my_trt_op_105 creation for segment 105, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/upconv1/BatchNorm/moments/variance. Skipping...
2018-10-02 17:55:59.449435: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/upconv2/BatchNorm/moments/my_trt_op_106 creation for segment 106, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/upconv2/BatchNorm/moments/mean. Skipping...
2018-10-02 17:55:59.460761: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/upconv2/BatchNorm/moments/my_trt_op_107 creation for segment 107, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/upconv2/BatchNorm/moments/variance. Skipping...
2018-10-02 17:55:59.471910: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/upconv3/BatchNorm/moments/my_trt_op_108 creation for segment 108, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/upconv3/BatchNorm/moments/mean. Skipping...
2018-10-02 17:55:59.477365: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/upconv3/BatchNorm/moments/my_trt_op_109 creation for segment 109, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/upconv3/BatchNorm/moments/variance. Skipping...
2018-10-02 17:56:00.141815: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:853] MULTIPLE tensorrt candidate conversion: 63
2018-10-02 17:56:00.142857: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_bottleneck/bottleneck/', converted to graph
2018-10-02 17:56:00.170427: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_bottleneck/bottleneck/BatchNorm/moments/', converted to graph
2018-10-02 17:56:00.195656: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/', converted to graph
2018-10-02 17:56:00.220905: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/', converted to graph
2018-10-02 17:56:00.247540: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/', converted to graph
2018-10-02 17:56:00.279776: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/', converted to graph
2018-10-02 17:56:00.310694: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/conv1/', converted to graph
2018-10-02 17:56:00.345061: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/conv1/conv1_1/BatchNorm/moments/', converted to graph
2018-10-02 17:56:00.377090: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/', converted to graph
2018-10-02 17:56:00.409263: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/conv1/conv1_2/BatchNorm/moments/', converted to graph
2018-10-02 17:56:00.442132: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/conv2/', converted to graph
2018-10-02 17:56:00.473882: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/conv2/conv2_1/BatchNorm/moments/', converted to graph
2018-10-02 17:56:00.505522: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/', converted to graph
2018-10-02 17:56:00.538089: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/conv2/conv2_2/BatchNorm/moments/', converted to graph
2018-10-02 17:56:00.573492: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/conv3/', converted to graph
2018-10-02 17:56:00.630581: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/conv3/conv3_1/BatchNorm/moments/', converted to graph
2018-10-02 17:56:00.674344: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/conv3/', converted to graph
2018-10-02 17:56:00.714034: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/conv3/conv3_2/BatchNorm/moments/', converted to graph
2018-10-02 17:56:00.759295: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/', converted to graph
2018-10-02 17:56:00.836794: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/conv3/conv3_3/BatchNorm/moments/', converted to graph
2018-10-02 17:56:00.889462: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/conv4/', converted to graph
2018-10-02 17:56:00.928113: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/conv4/conv4_1/BatchNorm/moments/', converted to graph
2018-10-02 17:56:00.967656: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/conv4/', converted to graph
2018-10-02 17:56:01.010712: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/conv4/conv4_2/BatchNorm/moments/', converted to graph
2018-10-02 17:56:01.078553: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/conv4/conv4_3/BatchNorm/moments/', converted to graph
2018-10-02 17:56:01.133705: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/pyramid_fusion1/BatchNorm/moments/', converted to graph
2018-10-02 17:56:01.176612: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/pyramid_fusion2/BatchNorm/moments/', converted to graph
2018-10-02 17:56:01.218213: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/pyramid_fusion3/BatchNorm/moments/', converted to graph
2018-10-02 17:56:01.265483: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/upconv1/BatchNorm/moments/', converted to graph
2018-10-02 17:56:01.311145: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/upconv1/BatchNorm/moments/', converted to graph
2018-10-02 17:56:01.356390: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/upconv2/BatchNorm/moments/', converted to graph
2018-10-02 17:56:01.433850: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/upconv2/BatchNorm/moments/', converted to graph
2018-10-02 17:56:01.487395: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/upconv3/BatchNorm/moments/', converted to graph
2018-10-02 17:56:01.531475: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'bev_vgg_pyr/upconv3/BatchNorm/moments/', converted to graph
2018-10-02 17:56:01.573956: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope '', converted to graph
2018-10-02 17:56:01.620681: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_bottleneck/bottleneck/BatchNorm/moments/', converted to graph
2018-10-02 17:56:01.665840: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/', converted to graph
2018-10-02 17:56:01.711575: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/', converted to graph
2018-10-02 17:56:01.778189: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/', converted to graph
2018-10-02 17:56:01.821042: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/conv1/', converted to graph
2018-10-02 17:56:01.866717: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/conv1/conv1_1/', converted to graph
2018-10-02 17:56:01.909214: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/conv1/conv1_1/BatchNorm/moments/', converted to graph
2018-10-02 17:56:01.946133: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/', converted to graph
2018-10-02 17:56:01.999098: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/conv1/conv1_2/BatchNorm/moments/', converted to graph
2018-10-02 17:56:02.060485: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/conv2/', converted to graph
2018-10-02 17:56:02.119455: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/conv2/conv2_1/BatchNorm/moments/', converted to graph
2018-10-02 17:56:02.168367: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/', converted to graph
2018-10-02 17:56:02.211278: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/conv2/conv2_2/BatchNorm/moments/', converted to graph
2018-10-02 17:56:02.251685: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/conv3/', converted to graph
2018-10-02 17:56:02.297304: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/conv3/conv3_1/BatchNorm/moments/', converted to graph
2018-10-02 17:56:02.340571: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/conv3/', converted to graph
2018-10-02 17:56:02.391434: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/conv3/conv3_2/BatchNorm/moments/', converted to graph
2018-10-02 17:56:02.445167: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/', converted to graph
2018-10-02 17:56:02.490863: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/conv3/conv3_3/BatchNorm/moments/', converted to graph
2018-10-02 17:56:02.535191: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/pyramid_fusion1/BatchNorm/moments/', converted to graph
2018-10-02 17:56:02.575691: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/pyramid_fusion2/BatchNorm/moments/', converted to graph
2018-10-02 17:56:02.628437: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/pyramid_fusion3/BatchNorm/moments/', converted to graph
2018-10-02 17:56:02.669795: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/upconv1/BatchNorm/moments/', converted to graph
2018-10-02 17:56:02.716209: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/upconv1/BatchNorm/moments/', converted to graph
2018-10-02 17:56:02.757055: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/upconv2/BatchNorm/moments/', converted to graph
2018-10-02 17:56:02.798237: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/upconv2/BatchNorm/moments/', converted to graph
2018-10-02 17:56:02.841372: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/upconv3/BatchNorm/moments/', converted to graph
2018-10-02 17:56:02.907552: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'img_vgg_pyr/upconv3/BatchNorm/moments/', converted to graph
2018-10-02 17:56:02.969336: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_bottleneck/bottleneck/my_trt_op_0 creation for segment 0, composed of 4 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_bottleneck/bottleneck/BatchNorm/moments/mean. Skipping...
2018-10-02 17:56:02.980822: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_bottleneck/bottleneck/BatchNorm/moments/my_trt_op_1 creation for segment 1, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_bottleneck/bottleneck/BatchNorm/moments/variance. Skipping...
2018-10-02 17:56:02.989601: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/my_trt_op_2 creation for segment 2, composed of 6 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/conv1/conv1_1/BatchNorm/moments/mean. Skipping...
2018-10-02 17:56:03.010604: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/my_trt_op_3 creation for segment 3, composed of 7 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/pyramid_fusion1/BatchNorm/moments/mean. Skipping...
2018-10-02 17:56:03.033675: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/my_trt_op_4 creation for segment 4, composed of 7 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/pyramid_fusion2/BatchNorm/moments/mean. Skipping...
2018-10-02 17:56:03.058413: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/my_trt_op_5 creation for segment 5, composed of 7 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/pyramid_fusion3/BatchNorm/moments/mean. Skipping...
2018-10-02 17:56:03.128939: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/conv1/my_trt_op_6 creation for segment 6, composed of 5 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/conv1/conv1_2/BatchNorm/moments/mean. Skipping...
2018-10-02 17:56:03.144605: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/conv1/conv1_1/BatchNorm/moments/my_trt_op_7 creation for segment 7, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/conv1/conv1_1/BatchNorm/moments/variance. Skipping...
2018-10-02 17:56:03.153443: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/my_trt_op_8 creation for segment 8, composed of 6 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/conv2/conv2_1/BatchNorm/moments/mean. Skipping...
2018-10-02 17:56:03.163759: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/conv1/conv1_2/BatchNorm/moments/my_trt_op_9 creation for segment 9, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/conv1/conv1_2/BatchNorm/moments/variance. Skipping...
2018-10-02 17:56:03.173146: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/conv2/my_trt_op_10 creation for segment 10, composed of 5 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/conv2/conv2_2/BatchNorm/moments/mean. Skipping...
2018-10-02 17:56:03.203674: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/conv2/conv2_1/BatchNorm/moments/my_trt_op_11 creation for segment 11, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/conv2/conv2_1/BatchNorm/moments/variance. Skipping...
2018-10-02 17:56:03.211912: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/my_trt_op_12 creation for segment 12, composed of 6 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/conv3/conv3_1/BatchNorm/moments/mean. Skipping...
2018-10-02 17:56:03.227933: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/conv2/conv2_2/BatchNorm/moments/my_trt_op_13 creation for segment 13, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/conv2/conv2_2/BatchNorm/moments/variance. Skipping...
2018-10-02 17:56:03.239869: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/conv3/my_trt_op_14 creation for segment 14, composed of 5 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/conv3/conv3_2/BatchNorm/moments/mean. Skipping...
2018-10-02 17:56:03.248090: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/conv3/conv3_1/BatchNorm/moments/my_trt_op_15 creation for segment 15, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/conv3/conv3_1/BatchNorm/moments/variance. Skipping...
2018-10-02 17:56:03.258185: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/conv3/my_trt_op_16 creation for segment 16, composed of 5 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/conv3/conv3_3/BatchNorm/moments/mean. Skipping...
2018-10-02 17:56:03.265557: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/conv3/conv3_2/BatchNorm/moments/my_trt_op_17 creation for segment 17, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/conv3/conv3_2/BatchNorm/moments/variance. Skipping...
2018-10-02 17:56:03.282523: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/my_trt_op_18 creation for segment 18, composed of 6 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/conv4/conv4_1/BatchNorm/moments/mean. Skipping...
2018-10-02 17:56:03.288078: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/conv3/conv3_3/BatchNorm/moments/my_trt_op_19 creation for segment 19, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/conv3/conv3_3/BatchNorm/moments/variance. Skipping...
2018-10-02 17:56:03.302207: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/conv4/my_trt_op_20 creation for segment 20, composed of 5 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/conv4/conv4_2/BatchNorm/moments/mean. Skipping...
2018-10-02 17:56:03.311663: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/conv4/conv4_1/BatchNorm/moments/my_trt_op_21 creation for segment 21, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/conv4/conv4_1/BatchNorm/moments/variance. Skipping...
2018-10-02 17:56:03.327409: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/conv4/my_trt_op_22 creation for segment 22, composed of 5 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/conv4/conv4_3/BatchNorm/moments/mean. Skipping...
2018-10-02 17:56:03.336531: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/conv4/conv4_2/BatchNorm/moments/my_trt_op_23 creation for segment 23, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/conv4/conv4_2/BatchNorm/moments/variance. Skipping...
2018-10-02 17:56:03.341961: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/conv4/conv4_3/BatchNorm/moments/my_trt_op_24 creation for segment 24, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/conv4/conv4_3/BatchNorm/moments/variance. Skipping...
2018-10-02 17:56:03.347354: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/pyramid_fusion1/BatchNorm/moments/my_trt_op_25 creation for segment 25, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/pyramid_fusion1/BatchNorm/moments/variance. Skipping...
2018-10-02 17:56:03.356055: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/pyramid_fusion2/BatchNorm/moments/my_trt_op_26 creation for segment 26, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/pyramid_fusion2/BatchNorm/moments/variance. Skipping...
2018-10-02 17:56:03.361473: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/pyramid_fusion3/BatchNorm/moments/my_trt_op_27 creation for segment 27, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/pyramid_fusion3/BatchNorm/moments/variance. Skipping...
2018-10-02 17:56:03.366898: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/upconv1/BatchNorm/moments/my_trt_op_28 creation for segment 28, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/upconv1/BatchNorm/moments/mean. Skipping...
2018-10-02 17:56:03.372328: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/upconv1/BatchNorm/moments/my_trt_op_29 creation for segment 29, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/upconv1/BatchNorm/moments/variance. Skipping...
2018-10-02 17:56:03.377764: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/upconv2/BatchNorm/moments/my_trt_op_30 creation for segment 30, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/upconv2/BatchNorm/moments/mean. Skipping...
2018-10-02 17:56:03.383151: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/upconv2/BatchNorm/moments/my_trt_op_31 creation for segment 31, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/upconv2/BatchNorm/moments/variance. Skipping...
2018-10-02 17:56:03.388535: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/upconv3/BatchNorm/moments/my_trt_op_32 creation for segment 32, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/upconv3/BatchNorm/moments/mean. Skipping...
2018-10-02 17:56:03.393864: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine bev_vgg_pyr/upconv3/BatchNorm/moments/my_trt_op_33 creation for segment 33, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atbev_vgg_pyr/upconv3/BatchNorm/moments/variance. Skipping...
2018-10-02 17:56:03.399323: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine my_trt_op_34 creation for segment 34, composed of 5 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_bottleneck/bottleneck/BatchNorm/moments/mean. Skipping...
2018-10-02 17:56:03.404688: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_bottleneck/bottleneck/BatchNorm/moments/my_trt_op_35 creation for segment 35, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_bottleneck/bottleneck/BatchNorm/moments/variance. Skipping...
2018-10-02 17:56:03.410415: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/my_trt_op_36 creation for segment 36, composed of 7 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/pyramid_fusion1/BatchNorm/moments/mean. Skipping...
2018-10-02 17:56:03.416480: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/my_trt_op_37 creation for segment 37, composed of 7 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/pyramid_fusion2/BatchNorm/moments/mean. Skipping...
2018-10-02 17:56:03.426178: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/my_trt_op_38 creation for segment 38, composed of 7 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/pyramid_fusion3/BatchNorm/moments/mean. Skipping...
2018-10-02 17:56:03.431928: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/conv1/my_trt_op_39 creation for segment 39, composed of 5 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/conv1/conv1_2/BatchNorm/moments/mean. Skipping...
2018-10-02 17:56:03.437507: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/conv1/conv1_1/my_trt_op_40 creation for segment 40, composed of 4 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/conv1/conv1_1/BatchNorm/moments/mean. Skipping...
2018-10-02 17:56:03.442960: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/conv1/conv1_1/BatchNorm/moments/my_trt_op_41 creation for segment 41, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/conv1/conv1_1/BatchNorm/moments/variance. Skipping...
2018-10-02 17:56:03.448674: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/my_trt_op_42 creation for segment 42, composed of 6 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/conv2/conv2_1/BatchNorm/moments/mean. Skipping...
2018-10-02 17:56:03.454163: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/conv1/conv1_2/BatchNorm/moments/my_trt_op_43 creation for segment 43, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/conv1/conv1_2/BatchNorm/moments/variance. Skipping...
2018-10-02 17:56:03.460002: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/conv2/my_trt_op_44 creation for segment 44, composed of 5 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/conv2/conv2_2/BatchNorm/moments/mean. Skipping...
2018-10-02 17:56:03.465472: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/conv2/conv2_1/BatchNorm/moments/my_trt_op_45 creation for segment 45, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/conv2/conv2_1/BatchNorm/moments/variance. Skipping...
2018-10-02 17:56:03.471639: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/my_trt_op_46 creation for segment 46, composed of 6 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/conv3/conv3_1/BatchNorm/moments/mean. Skipping...
2018-10-02 17:56:03.477128: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/conv2/conv2_2/BatchNorm/moments/my_trt_op_47 creation for segment 47, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/conv2/conv2_2/BatchNorm/moments/variance. Skipping...
2018-10-02 17:56:03.483727: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/conv3/my_trt_op_48 creation for segment 48, composed of 5 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/conv3/conv3_2/BatchNorm/moments/mean. Skipping...
2018-10-02 17:56:03.489220: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/conv3/conv3_1/BatchNorm/moments/my_trt_op_49 creation for segment 49, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/conv3/conv3_1/BatchNorm/moments/variance. Skipping...
2018-10-02 17:56:03.495857: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/conv3/my_trt_op_50 creation for segment 50, composed of 5 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/conv3/conv3_3/BatchNorm/moments/mean. Skipping...
2018-10-02 17:56:03.501322: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/conv3/conv3_2/BatchNorm/moments/my_trt_op_51 creation for segment 51, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/conv3/conv3_2/BatchNorm/moments/variance. Skipping...
2018-10-02 17:56:03.510326: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/my_trt_op_52 creation for segment 52, composed of 6 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/conv4/conv4_1/BatchNorm/moments/mean. Skipping...
2018-10-02 17:56:03.515900: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/conv3/conv3_3/BatchNorm/moments/my_trt_op_53 creation for segment 53, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/conv3/conv3_3/BatchNorm/moments/variance. Skipping...
2018-10-02 17:56:03.521290: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/pyramid_fusion1/BatchNorm/moments/my_trt_op_54 creation for segment 54, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/pyramid_fusion1/BatchNorm/moments/variance. Skipping...
2018-10-02 17:56:03.526716: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/pyramid_fusion2/BatchNorm/moments/my_trt_op_55 creation for segment 55, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/pyramid_fusion2/BatchNorm/moments/variance. Skipping...
2018-10-02 17:56:03.537755: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/pyramid_fusion3/BatchNorm/moments/my_trt_op_56 creation for segment 56, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/pyramid_fusion3/BatchNorm/moments/variance. Skipping...
2018-10-02 17:56:03.543147: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/upconv1/BatchNorm/moments/my_trt_op_57 creation for segment 57, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/upconv1/BatchNorm/moments/mean. Skipping...
2018-10-02 17:56:03.548554: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/upconv1/BatchNorm/moments/my_trt_op_58 creation for segment 58, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/upconv1/BatchNorm/moments/variance. Skipping...
2018-10-02 17:56:03.553972: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/upconv2/BatchNorm/moments/my_trt_op_59 creation for segment 59, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/upconv2/BatchNorm/moments/mean. Skipping...
2018-10-02 17:56:03.559438: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/upconv2/BatchNorm/moments/my_trt_op_60 creation for segment 60, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/upconv2/BatchNorm/moments/variance. Skipping...
2018-10-02 17:56:03.564861: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/upconv3/BatchNorm/moments/my_trt_op_61 creation for segment 61, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/upconv3/BatchNorm/moments/mean. Skipping...
2018-10-02 17:56:03.570331: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine img_vgg_pyr/upconv3/BatchNorm/moments/my_trt_op_62 creation for segment 62, composed of 2 nodes failed: Invalid argument: TRT cannot reduce at batch dimension, atimg_vgg_pyr/upconv3/BatchNorm/moments/variance. Skipping...
2018-10-02 17:56:03.962856: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:04.062406: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:04.263895: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:04.351102: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:04.810160: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:04.903127: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:05.216729: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:05.377328: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:05.629837: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:05.741889: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:06.285153: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:06.426025: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:06.896986: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:07.005234: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:07.331087: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:07.429775: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:07.702808: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:07.843982: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:08.083679: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:08.211226: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:08.449366: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:08.540734: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:08.745183: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:08.847796: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:09.216510: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:09.316383: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:09.724700: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:09.814051: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:10.055081: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:10.137400: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:10.380309: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:10.468057: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:10.788696: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:10.915843: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:11.258064: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:11.475492: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:11.838890: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:11.967300: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:12.349391: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:12.470533: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:12.768804: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:12.892336: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:13.168478: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:13.255246: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:13.480106: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:13.605198: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:13.812729: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:13.895074: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:14.066856: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:14.142464: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:14.405436: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:14.503853: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:14.834100: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:14.971755: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:15.221326: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:15.347533: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:15.602816: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:15.716234: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:16.072528: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:16.195363: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:16.481762: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:16.616132: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:16.891134: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:16.997634: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:17.269403: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:17.402055: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:17.755220: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:17.849290: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:18.543444: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:18.673213: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:19.079337: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:19.204213: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:19.639048: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:19.743795: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:19.985325: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:20.112254: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:20.473620: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:20.611352: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:20.840764: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:20.928101: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:21.147636: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:21.249638: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:21.544871: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:21.651990: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:21.959924: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:22.066834: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:22.312655: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:22.393502: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:22.701783: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:22.803812: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:23.015327: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:23.107253: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:23.313834: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:23.425949: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:23.800021: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:23.924795: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:24.253029: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:24.412785: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:24.722232: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:24.829318: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:25.037879: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:25.113719: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:25.324188: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:25.416653: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:25.623164: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:25.731420: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:25.937949: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:26.019311: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:26.322064: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:26.416136: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:26.667155: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:26.796819: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:27.137403: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:27.263612: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:27.671781: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:27.778717: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:28.133562: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:28.258397: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:28.517705: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:28.674028: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:28.977364: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:29.139223: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:29.447015: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:29.547793: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:29.852750: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:29.964112: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:30.180069: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:30.268583: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:30.480015: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:30.575854: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:30.883435: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:30.999684: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:31.218258: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:31.310087: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:31.597708: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:31.695835: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:31.948859: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:32.076745: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:32.326095: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:32.435555: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:32.726703: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:32.869378: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:33.402936: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:33.544019: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:33.883843: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:33.990118: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:34.247859: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:34.327272: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:34.624772: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:34.712355: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:35.017296: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:35.156685: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:35.521514: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:35.625491: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:35.853263: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:35.957000: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:36.384367: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:36.485557: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:36.685878: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:36.769251: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:37.007802: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:37.112754: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:37.328329: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:37.412646: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:37.603709: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:37.682906: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:38.087271: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:38.256376: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:38.648774: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:38.762487: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:39.112597: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:39.228518: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:39.581079: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:39.750554: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:39.994712: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:40.132410: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:40.519194: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:40.649858: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-10-02 17:56:40.777227: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: tf_graph
2018-10-02 17:56:40.777314: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 1856 nodes (-210), 2288 edges (-237), time = 849.462ms.
2018-10-02 17:56:40.777331: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Invalid argument: Unsupported tensor size: 2
2018-10-02 17:56:40.777343: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 1639 nodes (-217), 2056 edges (-232), time = 8124.63281ms.
2018-10-02 17:56:40.777352: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 1639 nodes (0), 2056 edges (0), time = 167.208ms.
2018-10-02 17:56:40.777366: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 1639 nodes (0), 2056 edges (0), time = 3975.28198ms.
2018-10-02 17:56:40.777401: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: img_vgg_pyr/conv3/my_trt_op_50_native_segment
2018-10-02 17:56:40.777445: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 130.254ms.
2018-10-02 17:56:40.777472: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 6 nodes (0), 5 edges (0), time = 59.916ms.
2018-10-02 17:56:40.777482: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 18.611ms.
2018-10-02 17:56:40.777493: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 81.438ms.
2018-10-02 17:56:40.777498: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 17.863ms.
2018-10-02 17:56:40.777517: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: bev_vgg_pyr/my_trt_op_3_native_segment
2018-10-02 17:56:40.777523: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 9 nodes (0), 8 edges (0), time = 79.581ms.
2018-10-02 17:56:40.777528: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 9 nodes (0), 8 edges (0), time = 42.578ms.
2018-10-02 17:56:40.777532: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 9 nodes (0), 8 edges (0), time = 14.459ms.
2018-10-02 17:56:40.777537: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 9 nodes (0), 8 edges (0), time = 73.104ms.
2018-10-02 17:56:40.777542: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 9 nodes (0), 8 edges (0), time = 13.9ms.
2018-10-02 17:56:40.777546: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: bev_vgg_pyr/conv4/my_trt_op_59_native_segment
2018-10-02 17:56:40.777551: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 97.347ms.
2018-10-02 17:56:40.777556: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 6 nodes (0), 5 edges (0), time = 54.02ms.
2018-10-02 17:56:40.777561: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 20.284ms.
2018-10-02 17:56:40.777565: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 73.974ms.
2018-10-02 17:56:40.777570: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 14.996ms.
2018-10-02 17:56:40.777575: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: bev_vgg_pyr/conv4/my_trt_op_22_native_segment
2018-10-02 17:56:40.777580: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 114.152ms.
2018-10-02 17:56:40.777584: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 6 nodes (0), 5 edges (0), time = 66.639ms.
2018-10-02 17:56:40.777589: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 23.315ms.
2018-10-02 17:56:40.777594: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 137.84ms.
2018-10-02 17:56:40.777598: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 26.118ms.
2018-10-02 17:56:40.777603: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: avod_regression/my_trt_op_25_native_segment
2018-10-02 17:56:40.777608: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 7 nodes (0), 8 edges (0), time = 92.864ms.
2018-10-02 17:56:40.777612: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 7 nodes (0), 8 edges (0), time = 48.034ms.
2018-10-02 17:56:40.777617: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 7 nodes (0), 8 edges (0), time = 16.397ms.
2018-10-02 17:56:40.777622: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 7 nodes (0), 8 edges (0), time = 95.741ms.
2018-10-02 17:56:40.777626: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 7 nodes (0), 8 edges (0), time = 16.872ms.
2018-10-02 17:56:40.777634: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: img_vgg_pyr/my_trt_op_37_native_segment
2018-10-02 17:56:40.777639: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 9 nodes (0), 8 edges (0), time = 128.588ms.
2018-10-02 17:56:40.777644: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 9 nodes (0), 8 edges (0), time = 86.13ms.
2018-10-02 17:56:40.777649: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 9 nodes (0), 8 edges (0), time = 20.842ms.
2018-10-02 17:56:40.777653: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 9 nodes (0), 8 edges (0), time = 120.321ms.
2018-10-02 17:56:40.777658: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 9 nodes (0), 8 edges (0), time = 31.859ms.
2018-10-02 17:56:40.777663: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: img_vgg_pyr/my_trt_op_38_native_segment
2018-10-02 17:56:40.777667: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 9 nodes (0), 8 edges (0), time = 120.262ms.
2018-10-02 17:56:40.777673: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 9 nodes (0), 8 edges (0), time = 65.366ms.
2018-10-02 17:56:40.777677: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 9 nodes (0), 8 edges (0), time = 16.09ms.
2018-10-02 17:56:40.777682: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 9 nodes (0), 8 edges (0), time = 92.29ms.
2018-10-02 17:56:40.777687: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 9 nodes (0), 8 edges (0), time = 17.853ms.
2018-10-02 17:56:40.777691: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: my_trt_op_34_native_segment
2018-10-02 17:56:40.777696: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 86.285ms.
2018-10-02 17:56:40.777701: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 6 nodes (0), 5 edges (0), time = 48.044ms.
2018-10-02 17:56:40.777706: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 16.753ms.
2018-10-02 17:56:40.777711: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 82.209ms.
2018-10-02 17:56:40.777715: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 16.027ms.
2018-10-02 17:56:40.777720: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: img_vgg_pyr/my_trt_op_78_native_segment
2018-10-02 17:56:40.777725: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 9 nodes (0), 8 edges (0), time = 100.958ms.
2018-10-02 17:56:40.777729: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 9 nodes (0), 8 edges (0), time = 51.425ms.
2018-10-02 17:56:40.777734: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 9 nodes (0), 8 edges (0), time = 20.608ms.
2018-10-02 17:56:40.777739: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 9 nodes (0), 8 edges (0), time = 122.391ms.
2018-10-02 17:56:40.777743: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 9 nodes (0), 8 edges (0), time = 13.772ms.
2018-10-02 17:56:40.777748: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: bev_vgg_pyr/conv4/my_trt_op_20_native_segment
2018-10-02 17:56:40.777756: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 75.488ms.
2018-10-02 17:56:40.777761: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 6 nodes (0), 5 edges (0), time = 46.379ms.
2018-10-02 17:56:40.777765: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 16.234ms.
2018-10-02 17:56:40.777770: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 111.487ms.
2018-10-02 17:56:40.777775: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 15.929ms.
2018-10-02 17:56:40.777779: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: box_predictor/fc7_drop/dropout/my_trt_op_73_native_segment
2018-10-02 17:56:40.777785: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 5 nodes (0), 4 edges (0), time = 101.857ms.
2018-10-02 17:56:40.777789: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 5 nodes (0), 4 edges (0), time = 56.307ms.
2018-10-02 17:56:40.777794: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 5 nodes (0), 4 edges (0), time = 17.826ms.
2018-10-02 17:56:40.777798: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 5 nodes (0), 4 edges (0), time = 73.912ms.
2018-10-02 17:56:40.777803: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 5 nodes (0), 4 edges (0), time = 18.894ms.
2018-10-02 17:56:40.777808: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: box_predictor/fc6_drop/dropout/my_trt_op_72_native_segment
2018-10-02 17:56:40.777812: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 5 nodes (0), 4 edges (0), time = 79.862ms.
2018-10-02 17:56:40.777817: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 5 nodes (0), 4 edges (0), time = 43.957ms.
2018-10-02 17:56:40.777822: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 5 nodes (0), 4 edges (0), time = 18.656ms.
2018-10-02 17:56:40.777827: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 5 nodes (0), 4 edges (0), time = 84.13ms.
2018-10-02 17:56:40.777832: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 5 nodes (0), 4 edges (0), time = 15.831ms.
2018-10-02 17:56:40.777836: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: img_vgg_pyr/my_trt_op_88_native_segment
2018-10-02 17:56:40.777841: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 7 nodes (0), 6 edges (0), time = 112.088ms.
2018-10-02 17:56:40.777846: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 11 nodes (4), 10 edges (4), time = 48.561ms.
2018-10-02 17:56:40.777851: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 11 nodes (0), 10 edges (0), time = 15.32ms.
2018-10-02 17:56:40.777855: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 11 nodes (0), 10 edges (0), time = 84.949ms.
2018-10-02 17:56:40.777860: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 11 nodes (0), 10 edges (0), time = 15.206ms.
2018-10-02 17:56:40.777864: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: img_vgg_pyr/conv1/my_trt_op_39_native_segment
2018-10-02 17:56:40.777875: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 97.002ms.
2018-10-02 17:56:40.777880: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 6 nodes (0), 5 edges (0), time = 49.334ms.
2018-10-02 17:56:40.777884: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 14.348ms.
2018-10-02 17:56:40.777889: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 75.246ms.
2018-10-02 17:56:40.777894: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 14.323ms.
2018-10-02 17:56:40.777898: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: bev_vgg_pyr/my_trt_op_2_native_segment
2018-10-02 17:56:40.777903: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 7 nodes (0), 6 edges (0), time = 100.693ms.
2018-10-02 17:56:40.777908: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 7 nodes (0), 6 edges (0), time = 46.782ms.
2018-10-02 17:56:40.777912: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 7 nodes (0), 6 edges (0), time = 21.77ms.
2018-10-02 17:56:40.777917: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 7 nodes (0), 6 edges (0), time = 67.921ms.
2018-10-02 17:56:40.777922: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 7 nodes (0), 6 edges (0), time = 13.852ms.
2018-10-02 17:56:40.777926: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: bev_vgg_pyr/my_trt_op_8_native_segment
2018-10-02 17:56:40.777931: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 7 nodes (0), 6 edges (0), time = 72.786ms.
2018-10-02 17:56:40.777936: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 11 nodes (4), 10 edges (4), time = 46.438ms.
2018-10-02 17:56:40.777940: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 11 nodes (0), 10 edges (0), time = 14.778ms.
2018-10-02 17:56:40.777945: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 11 nodes (0), 10 edges (0), time = 73.322ms.
2018-10-02 17:56:40.777950: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 11 nodes (0), 10 edges (0), time = 14.568ms.
2018-10-02 17:56:40.777954: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: img_vgg_pyr/my_trt_op_79_native_segment
2018-10-02 17:56:40.777959: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 9 nodes (0), 8 edges (0), time = 93.484ms.
2018-10-02 17:56:40.777964: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 9 nodes (0), 8 edges (0), time = 57.016ms.
2018-10-02 17:56:40.777968: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 9 nodes (0), 8 edges (0), time = 16.567ms.
2018-10-02 17:56:40.777973: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 9 nodes (0), 8 edges (0), time = 110.085ms.
2018-10-02 17:56:40.777978: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 9 nodes (0), 8 edges (0), time = 19.795ms.
2018-10-02 17:56:40.777982: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: my_trt_op_8_native_segment
2018-10-02 17:56:40.777990: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 10 nodes (0), 9 edges (0), time = 171.247ms.
2018-10-02 17:56:40.777995: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 10 nodes (0), 9 edges (0), time = 60.652ms.
2018-10-02 17:56:40.778000: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 10 nodes (0), 9 edges (0), time = 15.295ms.
2018-10-02 17:56:40.778005: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 10 nodes (0), 9 edges (0), time = 202.229ms.
2018-10-02 17:56:40.778009: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 10 nodes (0), 9 edges (0), time = 19.85ms.
2018-10-02 17:56:40.778014: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: anchor_predictor/cls_fc6_drop/dropout/my_trt_op_14_native_segment
2018-10-02 17:56:40.778019: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 5 nodes (0), 4 edges (0), time = 125.611ms.
2018-10-02 17:56:40.778024: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 5 nodes (0), 4 edges (0), time = 71.643ms.
2018-10-02 17:56:40.778028: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 5 nodes (0), 4 edges (0), time = 23.51ms.
2018-10-02 17:56:40.778033: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 5 nodes (0), 4 edges (0), time = 105.809ms.
2018-10-02 17:56:40.778038: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 5 nodes (0), 4 edges (0), time = 20.109ms.
2018-10-02 17:56:40.778042: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: box_predictor/fc8_drop/dropout/my_trt_op_74_native_segment
2018-10-02 17:56:40.778047: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 5 nodes (0), 4 edges (0), time = 96.14ms.
2018-10-02 17:56:40.778052: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 5 nodes (0), 4 edges (0), time = 62.772ms.
2018-10-02 17:56:40.778057: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 5 nodes (0), 4 edges (0), time = 20.635ms.
2018-10-02 17:56:40.778061: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 5 nodes (0), 4 edges (0), time = 100.912ms.
2018-10-02 17:56:40.778066: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 5 nodes (0), 4 edges (0), time = 19.505ms.
2018-10-02 17:56:40.778071: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: my_trt_op_7_native_segment
2018-10-02 17:56:40.778076: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 42 nodes (0), 48 edges (0), time = 90.341ms.
2018-10-02 17:56:40.778080: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 42 nodes (0), 48 edges (0), time = 58.895ms.
2018-10-02 17:56:40.778085: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 42 nodes (0), 48 edges (0), time = 23.053ms.
2018-10-02 17:56:40.778090: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 42 nodes (0), 48 edges (0), time = 101.459ms.
2018-10-02 17:56:40.778094: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 42 nodes (0), 48 edges (0), time = 15.277ms.
2018-10-02 17:56:40.778099: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: avod_regression/my_trt_op_32_native_segment
2018-10-02 17:56:40.778107: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 5 nodes (0), 4 edges (0), time = 68.565ms.
2018-10-02 17:56:40.778112: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 5 nodes (0), 4 edges (0), time = 43.078ms.
2018-10-02 17:56:40.778116: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 5 nodes (0), 4 edges (0), time = 17.118ms.
2018-10-02 17:56:40.778121: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 5 nodes (0), 4 edges (0), time = 69.783ms.
2018-10-02 17:56:40.778125: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 5 nodes (0), 4 edges (0), time = 15.171ms.
2018-10-02 17:56:40.778130: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: my_trt_op_5_native_segment
2018-10-02 17:56:40.778135: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 15 nodes (0), 15 edges (0), time = 81.789ms.
2018-10-02 17:56:40.778139: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 15 nodes (0), 15 edges (0), time = 58.871ms.
2018-10-02 17:56:40.778144: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 15 nodes (0), 15 edges (0), time = 21.822ms.
2018-10-02 17:56:40.778149: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 15 nodes (0), 15 edges (0), time = 103.572ms.
2018-10-02 17:56:40.778153: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 15 nodes (0), 15 edges (0), time = 21.173ms.
2018-10-02 17:56:40.778158: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: avod_regression/my_trt_op_24_native_segment
2018-10-02 17:56:40.778163: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 7 nodes (0), 8 edges (0), time = 76.028ms.
2018-10-02 17:56:40.778167: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 7 nodes (0), 8 edges (0), time = 47.977ms.
2018-10-02 17:56:40.778172: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 7 nodes (0), 8 edges (0), time = 15.578ms.
2018-10-02 17:56:40.778177: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 7 nodes (0), 8 edges (0), time = 67.261ms.
2018-10-02 17:56:40.778181: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 7 nodes (0), 8 edges (0), time = 12.204ms.
2018-10-02 17:56:40.778186: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: bev_vgg_pyr/conv2/my_trt_op_10_native_segment
2018-10-02 17:56:40.778191: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 66.655ms.
2018-10-02 17:56:40.778196: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 6 nodes (0), 5 edges (0), time = 41.333ms.
2018-10-02 17:56:40.778201: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 12.683ms.
2018-10-02 17:56:40.778205: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 63.096ms.
2018-10-02 17:56:40.778210: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 14.835ms.
2018-10-02 17:56:40.778215: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: my_trt_op_4_native_segment
2018-10-02 17:56:40.778222: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 5 nodes (0), 4 edges (0), time = 94.363ms.
2018-10-02 17:56:40.778227: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 5 nodes (0), 4 edges (0), time = 49.062ms.
2018-10-02 17:56:40.778232: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 5 nodes (0), 4 edges (0), time = 15.634ms.
2018-10-02 17:56:40.778237: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 5 nodes (0), 4 edges (0), time = 83.174ms.
2018-10-02 17:56:40.778242: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 5 nodes (0), 4 edges (0), time = 16.232ms.
2018-10-02 17:56:40.778246: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: bev_vgg_pyr/conv2/my_trt_op_47_native_segment
2018-10-02 17:56:40.778251: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 114.132ms.
2018-10-02 17:56:40.778256: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 6 nodes (0), 5 edges (0), time = 69.724ms.
2018-10-02 17:56:40.778261: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 22.896ms.
2018-10-02 17:56:40.778265: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 115.544ms.
2018-10-02 17:56:40.778270: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 18.883ms.
2018-10-02 17:56:40.778275: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: my_trt_op_12_native_segment
2018-10-02 17:56:40.778279: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 13 nodes (0), 12 edges (0), time = 99.906ms.
2018-10-02 17:56:40.778284: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 19 nodes (6), 20 edges (8), time = 65.87ms.
2018-10-02 17:56:40.778289: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 19 nodes (0), 20 edges (0), time = 20.68ms.
2018-10-02 17:56:40.778294: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 19 nodes (0), 20 edges (0), time = 105.972ms.
2018-10-02 17:56:40.778299: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 19 nodes (0), 20 edges (0), time = 20.176ms.
2018-10-02 17:56:40.778303: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: avod_regression/my_trt_op_27_native_segment
2018-10-02 17:56:40.778308: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 11 nodes (0), 10 edges (0), time = 96.599ms.
2018-10-02 17:56:40.778313: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 11 nodes (0), 10 edges (0), time = 58.041ms.
2018-10-02 17:56:40.778317: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 11 nodes (0), 10 edges (0), time = 19.126ms.
2018-10-02 17:56:40.778322: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 11 nodes (0), 10 edges (0), time = 94.906ms.
2018-10-02 17:56:40.778327: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 11 nodes (0), 10 edges (0), time = 19.079ms.
2018-10-02 17:56:40.778331: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: bev_vgg_pyr/my_trt_op_55_native_segment
2018-10-02 17:56:40.778339: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 7 nodes (0), 6 edges (0), time = 108.69ms.
2018-10-02 17:56:40.778344: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 11 nodes (4), 10 edges (4), time = 65.153ms.
2018-10-02 17:56:40.778349: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 11 nodes (0), 10 edges (0), time = 20.794ms.
2018-10-02 17:56:40.778354: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 11 nodes (0), 10 edges (0), time = 103.203ms.
2018-10-02 17:56:40.778359: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 11 nodes (0), 10 edges (0), time = 21.007ms.
2018-10-02 17:56:40.778363: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: avod_regression/my_trt_op_28_native_segment
2018-10-02 17:56:40.778368: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 19 nodes (0), 20 edges (0), time = 114.885ms.
2018-10-02 17:56:40.778373: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 19 nodes (0), 20 edges (0), time = 77.866ms.
2018-10-02 17:56:40.778378: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 19 nodes (0), 20 edges (0), time = 23.805ms.
2018-10-02 17:56:40.778383: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 19 nodes (0), 20 edges (0), time = 111.436ms.
2018-10-02 17:56:40.778387: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 19 nodes (0), 20 edges (0), time = 19.458ms.
2018-10-02 17:56:40.778400: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: img_vgg_pyr/my_trt_op_94_native_segment
2018-10-02 17:56:40.778405: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 7 nodes (0), 6 edges (0), time = 80.543ms.
2018-10-02 17:56:40.778410: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 11 nodes (4), 10 edges (4), time = 58.339ms.
2018-10-02 17:56:40.778415: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 11 nodes (0), 10 edges (0), time = 19.708ms.
2018-10-02 17:56:40.778419: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 11 nodes (0), 10 edges (0), time = 87.838ms.
2018-10-02 17:56:40.778424: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 11 nodes (0), 10 edges (0), time = 16.698ms.
2018-10-02 17:56:40.778429: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: avod_regression/my_trt_op_30_native_segment
2018-10-02 17:56:40.778433: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 5 nodes (0), 4 edges (0), time = 121.923ms.
2018-10-02 17:56:40.778438: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 5 nodes (0), 4 edges (0), time = 47.098ms.
2018-10-02 17:56:40.778443: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 5 nodes (0), 4 edges (0), time = 24.22ms.
2018-10-02 17:56:40.778448: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 5 nodes (0), 4 edges (0), time = 108.566ms.
2018-10-02 17:56:40.778452: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 5 nodes (0), 4 edges (0), time = 23.011ms.
2018-10-02 17:56:40.778457: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: bev_vgg_pyr/conv3/my_trt_op_16_native_segment
2018-10-02 17:56:40.778465: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 101.374ms.
2018-10-02 17:56:40.778469: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 6 nodes (0), 5 edges (0), time = 42.587ms.
2018-10-02 17:56:40.778474: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 14.76ms.
2018-10-02 17:56:40.778479: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 79.239ms.
2018-10-02 17:56:40.778483: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 18.402ms.
2018-10-02 17:56:40.778488: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: img_vgg_pyr/my_trt_op_52_native_segment
2018-10-02 17:56:40.778493: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 7 nodes (0), 6 edges (0), time = 159.819ms.
2018-10-02 17:56:40.778497: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 11 nodes (4), 10 edges (4), time = 59.04ms.
2018-10-02 17:56:40.778502: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 11 nodes (0), 10 edges (0), time = 24.367ms.
2018-10-02 17:56:40.778507: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 11 nodes (0), 10 edges (0), time = 105.897ms.
2018-10-02 17:56:40.778512: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 11 nodes (0), 10 edges (0), time = 20.263ms.
2018-10-02 17:56:40.778516: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: bev_vgg_pyr/my_trt_op_41_native_segment
2018-10-02 17:56:40.778521: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 9 nodes (0), 8 edges (0), time = 119.099ms.
2018-10-02 17:56:40.778526: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 9 nodes (0), 8 edges (0), time = 64.511ms.
2018-10-02 17:56:40.778531: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 9 nodes (0), 8 edges (0), time = 19.904ms.
2018-10-02 17:56:40.778536: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 9 nodes (0), 8 edges (0), time = 105.488ms.
2018-10-02 17:56:40.778540: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 9 nodes (0), 8 edges (0), time = 16.375ms.
2018-10-02 17:56:40.778545: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: img_input/my_trt_op_77_native_segment
2018-10-02 17:56:40.778550: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 11 nodes (0), 10 edges (0), time = 121.065ms.
2018-10-02 17:56:40.778555: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 11 nodes (0), 10 edges (0), time = 49.449ms.
2018-10-02 17:56:40.778559: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 11 nodes (0), 10 edges (0), time = 17.159ms.
2018-10-02 17:56:40.778564: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 11 nodes (0), 10 edges (0), time = 87.813ms.
2018-10-02 17:56:40.778569: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 11 nodes (0), 10 edges (0), time = 17.637ms.
2018-10-02 17:56:40.778574: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: avod_regression/my_trt_op_36_native_segment
2018-10-02 17:56:40.778579: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 7 nodes (0), 6 edges (0), time = 97.444ms.
2018-10-02 17:56:40.778587: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 7 nodes (0), 6 edges (0), time = 56.214ms.
2018-10-02 17:56:40.778591: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 7 nodes (0), 6 edges (0), time = 19.357ms.
2018-10-02 17:56:40.778596: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 7 nodes (0), 6 edges (0), time = 108.06ms.
2018-10-02 17:56:40.778601: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 7 nodes (0), 6 edges (0), time = 16.506ms.
2018-10-02 17:56:40.778605: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: my_trt_op_6_native_segment
2018-10-02 17:56:40.778610: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 15 nodes (0), 15 edges (0), time = 144.376ms.
2018-10-02 17:56:40.778615: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 15 nodes (0), 15 edges (0), time = 64.375ms.
2018-10-02 17:56:40.778620: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 15 nodes (0), 15 edges (0), time = 23.864ms.
2018-10-02 17:56:40.778624: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 15 nodes (0), 15 edges (0), time = 114.696ms.
2018-10-02 17:56:40.778629: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 15 nodes (0), 15 edges (0), time = 16.925ms.
2018-10-02 17:56:40.778633: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: my_trt_op_10_native_segment
2018-10-02 17:56:40.778638: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 21 nodes (0), 20 edges (0), time = 88.207ms.
2018-10-02 17:56:40.778643: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 21 nodes (0), 20 edges (0), time = 48.169ms.
2018-10-02 17:56:40.778648: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 21 nodes (0), 20 edges (0), time = 14.026ms.
2018-10-02 17:56:40.778653: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 21 nodes (0), 20 edges (0), time = 73.447ms.
2018-10-02 17:56:40.778657: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 21 nodes (0), 20 edges (0), time = 14.421ms.
2018-10-02 17:56:40.778662: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: avod_regression/my_trt_op_34_native_segment
2018-10-02 17:56:40.778666: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 11 nodes (0), 10 edges (0), time = 97.452ms.
2018-10-02 17:56:40.778671: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 11 nodes (0), 10 edges (0), time = 51.176ms.
2018-10-02 17:56:40.778676: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 11 nodes (0), 10 edges (0), time = 17.601ms.
2018-10-02 17:56:40.778680: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 11 nodes (0), 10 edges (0), time = 84.946ms.
2018-10-02 17:56:40.778685: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 11 nodes (0), 10 edges (0), time = 16.067ms.
2018-10-02 17:56:40.778690: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: bev_vgg_pyr/conv3/my_trt_op_53_native_segment
2018-10-02 17:56:40.778695: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 129.842ms.
2018-10-02 17:56:40.778713: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 6 nodes (0), 5 edges (0), time = 65.908ms.
2018-10-02 17:56:40.778718: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 20.821ms.
2018-10-02 17:56:40.778723: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 86.942ms.
2018-10-02 17:56:40.778728: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 16.451ms.
2018-10-02 17:56:40.778732: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: bev_bottleneck/bottleneck/my_trt_op_0_native_segment
2018-10-02 17:56:40.778737: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 5 nodes (0), 4 edges (0), time = 94.827ms.
2018-10-02 17:56:40.778742: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 5 nodes (0), 4 edges (0), time = 53.676ms.
2018-10-02 17:56:40.778747: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 5 nodes (0), 4 edges (0), time = 18.177ms.
2018-10-02 17:56:40.778752: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 5 nodes (0), 4 edges (0), time = 89.32ms.
2018-10-02 17:56:40.778756: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 5 nodes (0), 4 edges (0), time = 16.613ms.
2018-10-02 17:56:40.778761: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: img_vgg_pyr/my_trt_op_36_native_segment
2018-10-02 17:56:40.778765: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 9 nodes (0), 8 edges (0), time = 74.077ms.
2018-10-02 17:56:40.778770: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 9 nodes (0), 8 edges (0), time = 43.634ms.
2018-10-02 17:56:40.778775: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 9 nodes (0), 8 edges (0), time = 14.627ms.
2018-10-02 17:56:40.778780: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 9 nodes (0), 8 edges (0), time = 66.59ms.
2018-10-02 17:56:40.778784: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 9 nodes (0), 8 edges (0), time = 12.386ms.
2018-10-02 17:56:40.778789: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: anchor_predictor/reg_fc7_drop/dropout/my_trt_op_17_native_segment
2018-10-02 17:56:40.778793: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 5 nodes (0), 4 edges (0), time = 91.361ms.
2018-10-02 17:56:40.778798: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 5 nodes (0), 4 edges (0), time = 62.223ms.
2018-10-02 17:56:40.778803: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 5 nodes (0), 4 edges (0), time = 19.545ms.
2018-10-02 17:56:40.778808: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 5 nodes (0), 4 edges (0), time = 83.007ms.
2018-10-02 17:56:40.778813: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 5 nodes (0), 4 edges (0), time = 16.396ms.
2018-10-02 17:56:40.778817: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: img_vgg_pyr/my_trt_op_84_native_segment
2018-10-02 17:56:40.778822: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 7 nodes (0), 6 edges (0), time = 88.167ms.
2018-10-02 17:56:40.778830: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 11 nodes (4), 10 edges (4), time = 45.84ms.
2018-10-02 17:56:40.778835: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 11 nodes (0), 10 edges (0), time = 13.792ms.
2018-10-02 17:56:40.778840: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 11 nodes (0), 10 edges (0), time = 78.441ms.
2018-10-02 17:56:40.778845: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 11 nodes (0), 10 edges (0), time = 13.941ms.
2018-10-02 17:56:40.778849: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: img_vgg_pyr/conv2/my_trt_op_86_native_segment
2018-10-02 17:56:40.778854: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 82.338ms.
2018-10-02 17:56:40.778859: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 6 nodes (0), 5 edges (0), time = 57.127ms.
2018-10-02 17:56:40.778864: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 18.181ms.
2018-10-02 17:56:40.778869: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 94.345ms.
2018-10-02 17:56:40.778874: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 18.682ms.
2018-10-02 17:56:40.778878: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: img_vgg_pyr/conv3/my_trt_op_90_native_segment
2018-10-02 17:56:40.778883: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 101.856ms.
2018-10-02 17:56:40.778888: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 6 nodes (0), 5 edges (0), time = 61.665ms.
2018-10-02 17:56:40.778893: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 17.499ms.
2018-10-02 17:56:40.778898: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 107.398ms.
2018-10-02 17:56:40.778902: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 21.293ms.
2018-10-02 17:56:40.778907: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: my_trt_op_0_native_segment
2018-10-02 17:56:40.778911: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 9 nodes (0), 8 edges (0), time = 103.244ms.
2018-10-02 17:56:40.778916: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 13 nodes (4), 12 edges (4), time = 68.954ms.
2018-10-02 17:56:40.778921: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 13 nodes (0), 12 edges (0), time = 22.664ms.
2018-10-02 17:56:40.778926: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 13 nodes (0), 12 edges (0), time = 137.616ms.
2018-10-02 17:56:40.778931: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 13 nodes (0), 12 edges (0), time = 28.718ms.
2018-10-02 17:56:40.778935: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: avod_regression/my_trt_op_35_native_segment
2018-10-02 17:56:40.778940: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 7 nodes (0), 6 edges (0), time = 126.762ms.
2018-10-02 17:56:40.778947: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 7 nodes (0), 6 edges (0), time = 50.357ms.
2018-10-02 17:56:40.778952: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 7 nodes (0), 6 edges (0), time = 18.147ms.
2018-10-02 17:56:40.778957: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 7 nodes (0), 6 edges (0), time = 80.564ms.
2018-10-02 17:56:40.778962: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 7 nodes (0), 6 edges (0), time = 25.198ms.
2018-10-02 17:56:40.778966: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: anchor_predictor/cls_fc7_drop/dropout/my_trt_op_15_native_segment
2018-10-02 17:56:40.778971: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 5 nodes (0), 4 edges (0), time = 83.305ms.
2018-10-02 17:56:40.778976: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 5 nodes (0), 4 edges (0), time = 40.139ms.
2018-10-02 17:56:40.778981: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 5 nodes (0), 4 edges (0), time = 12.973ms.
2018-10-02 17:56:40.778985: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 5 nodes (0), 4 edges (0), time = 63.052ms.
2018-10-02 17:56:40.778990: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 5 nodes (0), 4 edges (0), time = 13.269ms.
2018-10-02 17:56:40.778995: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: img_vgg_pyr/conv4/my_trt_op_98_native_segment
2018-10-02 17:56:40.778999: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 81.518ms.
2018-10-02 17:56:40.779004: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 6 nodes (0), 5 edges (0), time = 47.497ms.
2018-10-02 17:56:40.779009: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 16.047ms.
2018-10-02 17:56:40.779014: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 76.779ms.
2018-10-02 17:56:40.779019: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 15.221ms.
2018-10-02 17:56:40.779023: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: bev_vgg_pyr/my_trt_op_40_native_segment
2018-10-02 17:56:40.779028: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 9 nodes (0), 8 edges (0), time = 92.461ms.
2018-10-02 17:56:40.779033: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 9 nodes (0), 8 edges (0), time = 47.376ms.
2018-10-02 17:56:40.779038: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 9 nodes (0), 8 edges (0), time = 17.816ms.
2018-10-02 17:56:40.779043: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 9 nodes (0), 8 edges (0), time = 90.656ms.
2018-10-02 17:56:40.779047: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 9 nodes (0), 8 edges (0), time = 14.102ms.
2018-10-02 17:56:40.779052: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: bev_bottleneck/bottleneck/my_trt_op_37_native_segment
2018-10-02 17:56:40.779057: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 5 nodes (0), 4 edges (0), time = 63.86ms.
2018-10-02 17:56:40.779065: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 5 nodes (0), 4 edges (0), time = 38.907ms.
2018-10-02 17:56:40.779069: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 5 nodes (0), 4 edges (0), time = 11.652ms.
2018-10-02 17:56:40.779074: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 5 nodes (0), 4 edges (0), time = 69.803ms.
2018-10-02 17:56:40.779079: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 5 nodes (0), 4 edges (0), time = 12.246ms.
2018-10-02 17:56:40.779084: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: bev_vgg_pyr/my_trt_op_18_native_segment
2018-10-02 17:56:40.779089: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 7 nodes (0), 6 edges (0), time = 69.677ms.
2018-10-02 17:56:40.779093: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 11 nodes (4), 10 edges (4), time = 48.913ms.
2018-10-02 17:56:40.779098: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 11 nodes (0), 10 edges (0), time = 15.775ms.
2018-10-02 17:56:40.779103: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 11 nodes (0), 10 edges (0), time = 78.473ms.
2018-10-02 17:56:40.779107: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 11 nodes (0), 10 edges (0), time = 15.874ms.
2018-10-02 17:56:40.779112: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: avod_regression/my_trt_op_33_native_segment
2018-10-02 17:56:40.779117: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 11 nodes (0), 10 edges (0), time = 104.174ms.
2018-10-02 17:56:40.779121: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 11 nodes (0), 10 edges (0), time = 59.715ms.
2018-10-02 17:56:40.779126: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 11 nodes (0), 10 edges (0), time = 19.682ms.
2018-10-02 17:56:40.779131: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 11 nodes (0), 10 edges (0), time = 110.365ms.
2018-10-02 17:56:40.779136: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 11 nodes (0), 10 edges (0), time = 21.466ms.
2018-10-02 17:56:40.779140: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: bev_vgg_pyr/my_trt_op_4_native_segment
2018-10-02 17:56:40.779145: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 9 nodes (0), 8 edges (0), time = 112.393ms.
2018-10-02 17:56:40.779149: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 9 nodes (0), 8 edges (0), time = 68.502ms.
2018-10-02 17:56:40.779154: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 9 nodes (0), 8 edges (0), time = 20.575ms.
2018-10-02 17:56:40.779159: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 9 nodes (0), 8 edges (0), time = 107.055ms.
2018-10-02 17:56:40.779163: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 9 nodes (0), 8 edges (0), time = 20.598ms.
2018-10-02 17:56:40.779168: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: bev_vgg_pyr/my_trt_op_39_native_segment
2018-10-02 17:56:40.779173: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 7 nodes (0), 6 edges (0), time = 98.208ms.
2018-10-02 17:56:40.779181: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 7 nodes (0), 6 edges (0), time = 71.593ms.
2018-10-02 17:56:40.779195: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 7 nodes (0), 6 edges (0), time = 16.447ms.
2018-10-02 17:56:40.779200: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 7 nodes (0), 6 edges (0), time = 90.597ms.
2018-10-02 17:56:40.779204: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 7 nodes (0), 6 edges (0), time = 19.726ms.
2018-10-02 17:56:40.779209: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: img_vgg_pyr/my_trt_op_80_native_segment
2018-10-02 17:56:40.779214: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 9 nodes (0), 8 edges (0), time = 103.706ms.
2018-10-02 17:56:40.779218: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 9 nodes (0), 8 edges (0), time = 66.372ms.
2018-10-02 17:56:40.779223: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 9 nodes (0), 8 edges (0), time = 25.863ms.
2018-10-02 17:56:40.779228: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 9 nodes (0), 8 edges (0), time = 99.486ms.
2018-10-02 17:56:40.779233: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 9 nodes (0), 8 edges (0), time = 16.206ms.
2018-10-02 17:56:40.779237: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: avod_regression/my_trt_op_26_native_segment
2018-10-02 17:56:40.779242: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 11 nodes (0), 10 edges (0), time = 98.167ms.
2018-10-02 17:56:40.779247: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 11 nodes (0), 10 edges (0), time = 59.385ms.
2018-10-02 17:56:40.779251: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 11 nodes (0), 10 edges (0), time = 16.568ms.
2018-10-02 17:56:40.779256: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 11 nodes (0), 10 edges (0), time = 139.949ms.
2018-10-02 17:56:40.779261: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 11 nodes (0), 10 edges (0), time = 16.402ms.
2018-10-02 17:56:40.779265: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: box_predictor/fc6/my_trt_op_71_native_segment
2018-10-02 17:56:40.779270: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 146.346ms.
2018-10-02 17:56:40.779275: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 6 nodes (0), 5 edges (0), time = 64.568ms.
2018-10-02 17:56:40.779279: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 15.238ms.
2018-10-02 17:56:40.779284: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 146.288ms.
2018-10-02 17:56:40.779289: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 20.676ms.
2018-10-02 17:56:40.779294: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: img_vgg_pyr/conv3/my_trt_op_48_native_segment
2018-10-02 17:56:40.779298: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 107.618ms.
2018-10-02 17:56:40.779306: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 6 nodes (0), 5 edges (0), time = 65.234ms.
2018-10-02 17:56:40.779311: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 19.001ms.
2018-10-02 17:56:40.779315: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 82.546ms.
2018-10-02 17:56:40.779320: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 16.445ms.
2018-10-02 17:56:40.779325: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: img_vgg_pyr/conv1/conv1_1/my_trt_op_40_native_segment
2018-10-02 17:56:40.779329: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 5 nodes (0), 4 edges (0), time = 85.819ms.
2018-10-02 17:56:40.779334: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 5 nodes (0), 4 edges (0), time = 50.473ms.
2018-10-02 17:56:40.779339: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 5 nodes (0), 4 edges (0), time = 16.447ms.
2018-10-02 17:56:40.779343: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 5 nodes (0), 4 edges (0), time = 94.917ms.
2018-10-02 17:56:40.779348: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 5 nodes (0), 4 edges (0), time = 20.329ms.
2018-10-02 17:56:40.779353: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: img_vgg_pyr/my_trt_op_46_native_segment
2018-10-02 17:56:40.779358: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 7 nodes (0), 6 edges (0), time = 83.529ms.
2018-10-02 17:56:40.779362: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 11 nodes (4), 10 edges (4), time = 50.784ms.
2018-10-02 17:56:40.779367: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 11 nodes (0), 10 edges (0), time = 14.719ms.
2018-10-02 17:56:40.779372: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 11 nodes (0), 10 edges (0), time = 74.205ms.
2018-10-02 17:56:40.779377: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 11 nodes (0), 10 edges (0), time = 14.43ms.
2018-10-02 17:56:40.779381: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: bev_vgg_pyr/my_trt_op_12_native_segment
2018-10-02 17:56:40.779387: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 7 nodes (0), 6 edges (0), time = 83.226ms.
2018-10-02 17:56:40.779391: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 11 nodes (4), 10 edges (4), time = 50.931ms.
2018-10-02 17:56:40.779396: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 11 nodes (0), 10 edges (0), time = 17.027ms.
2018-10-02 17:56:40.779401: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 11 nodes (0), 10 edges (0), time = 79.41ms.
2018-10-02 17:56:40.779405: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 11 nodes (0), 10 edges (0), time = 16.315ms.
2018-10-02 17:56:40.779410: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: my_trt_op_2_native_segment
2018-10-02 17:56:40.779415: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 9 nodes (0), 8 edges (0), time = 120.436ms.
2018-10-02 17:56:40.779420: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 13 nodes (4), 12 edges (4), time = 70.529ms.
2018-10-02 17:56:40.779427: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 13 nodes (0), 12 edges (0), time = 21.516ms.
2018-10-02 17:56:40.779432: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 13 nodes (0), 12 edges (0), time = 95.753ms.
2018-10-02 17:56:40.779437: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 13 nodes (0), 12 edges (0), time = 16.356ms.
2018-10-02 17:56:40.779441: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: my_trt_op_75_native_segment
2018-10-02 17:56:40.779446: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 88.041ms.
2018-10-02 17:56:40.779451: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 6 nodes (0), 5 edges (0), time = 56.761ms.
2018-10-02 17:56:40.779455: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 17.758ms.
2018-10-02 17:56:40.779460: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 74.758ms.
2018-10-02 17:56:40.779465: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 13.811ms.
2018-10-02 17:56:40.779469: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: bev_vgg_pyr/conv1/my_trt_op_43_native_segment
2018-10-02 17:56:40.779474: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 79.718ms.
2018-10-02 17:56:40.779479: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 6 nodes (0), 5 edges (0), time = 53.626ms.
2018-10-02 17:56:40.779484: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 17.251ms.
2018-10-02 17:56:40.779489: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 81.104ms.
2018-10-02 17:56:40.779493: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 16.558ms.
2018-10-02 17:56:40.779498: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: img_vgg_pyr/conv1/conv1_1/my_trt_op_82_native_segment
2018-10-02 17:56:40.779503: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 5 nodes (0), 4 edges (0), time = 115.258ms.
2018-10-02 17:56:40.779507: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 5 nodes (0), 4 edges (0), time = 57.456ms.
2018-10-02 17:56:40.779512: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 5 nodes (0), 4 edges (0), time = 17.428ms.
2018-10-02 17:56:40.779517: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 5 nodes (0), 4 edges (0), time = 110.812ms.
2018-10-02 17:56:40.779521: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 5 nodes (0), 4 edges (0), time = 15.478ms.
2018-10-02 17:56:40.779526: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: img_vgg_pyr/conv1/my_trt_op_81_native_segment
2018-10-02 17:56:40.779530: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 92.83ms.
2018-10-02 17:56:40.779535: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 6 nodes (0), 5 edges (0), time = 50.043ms.
2018-10-02 17:56:40.779545: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 16.106ms.
2018-10-02 17:56:40.779550: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 93.328ms.
2018-10-02 17:56:40.779555: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 22.856ms.
2018-10-02 17:56:40.779559: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: my_trt_op_3_native_segment
2018-10-02 17:56:40.779564: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 8 nodes (0), 7 edges (0), time = 85.217ms.
2018-10-02 17:56:40.779569: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 12 nodes (4), 11 edges (4), time = 54.623ms.
2018-10-02 17:56:40.779574: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 12 nodes (0), 11 edges (0), time = 19.887ms.
2018-10-02 17:56:40.779579: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 12 nodes (0), 11 edges (0), time = 123.271ms.
2018-10-02 17:56:40.779583: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 12 nodes (0), 11 edges (0), time = 24.725ms.
2018-10-02 17:56:40.779588: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: avod_regression/my_trt_op_29_native_segment
2018-10-02 17:56:40.779593: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 11 nodes (0), 10 edges (0), time = 141.313ms.
2018-10-02 17:56:40.779597: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 11 nodes (0), 10 edges (0), time = 71.323ms.
2018-10-02 17:56:40.779602: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 11 nodes (0), 10 edges (0), time = 23.05ms.
2018-10-02 17:56:40.779607: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 11 nodes (0), 10 edges (0), time = 118.683ms.
2018-10-02 17:56:40.779612: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 11 nodes (0), 10 edges (0), time = 23.024ms.
2018-10-02 17:56:40.779616: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: avod_regression/my_trt_op_31_native_segment
2018-10-02 17:56:40.779621: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 5 nodes (0), 4 edges (0), time = 136.501ms.
2018-10-02 17:56:40.779626: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 5 nodes (0), 4 edges (0), time = 74.682ms.
2018-10-02 17:56:40.779631: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 5 nodes (0), 4 edges (0), time = 20.389ms.
2018-10-02 17:56:40.779636: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 5 nodes (0), 4 edges (0), time = 86.831ms.
2018-10-02 17:56:40.779640: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 5 nodes (0), 4 edges (0), time = 15.856ms.
2018-10-02 17:56:40.779645: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: bev_vgg_pyr/my_trt_op_45_native_segment
2018-10-02 17:56:40.779650: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 7 nodes (0), 6 edges (0), time = 87.749ms.
2018-10-02 17:56:40.779654: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 11 nodes (4), 10 edges (4), time = 45.413ms.
2018-10-02 17:56:40.779662: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 11 nodes (0), 10 edges (0), time = 13.229ms.
2018-10-02 17:56:40.779667: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 11 nodes (0), 10 edges (0), time = 66.453ms.
2018-10-02 17:56:40.779671: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 11 nodes (0), 10 edges (0), time = 16.001ms.
2018-10-02 17:56:40.779676: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: img_vgg_pyr/conv2/my_trt_op_44_native_segment
2018-10-02 17:56:40.779681: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 79.012ms.
2018-10-02 17:56:40.779685: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 6 nodes (0), 5 edges (0), time = 46.284ms.
2018-10-02 17:56:40.779690: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 14.666ms.
2018-10-02 17:56:40.779695: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 73.283ms.
2018-10-02 17:56:40.779700: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 13.655ms.
2018-10-02 17:56:40.779704: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: my_trt_op_1_native_segment
2018-10-02 17:56:40.779709: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 8 nodes (0), 7 edges (0), time = 90.175ms.
2018-10-02 17:56:40.779714: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 12 nodes (4), 11 edges (4), time = 52.019ms.
2018-10-02 17:56:40.779719: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 12 nodes (0), 11 edges (0), time = 20.334ms.
2018-10-02 17:56:40.779723: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 12 nodes (0), 11 edges (0), time = 119.619ms.
2018-10-02 17:56:40.779728: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 12 nodes (0), 11 edges (0), time = 16.267ms.
2018-10-02 17:56:40.779733: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: img_vgg_pyr/conv3/my_trt_op_92_native_segment
2018-10-02 17:56:40.779738: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 99.147ms.
2018-10-02 17:56:40.779743: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 6 nodes (0), 5 edges (0), time = 56.728ms.
2018-10-02 17:56:40.779747: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 17.775ms.
2018-10-02 17:56:40.779752: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 86.698ms.
2018-10-02 17:56:40.779757: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 17.552ms.
2018-10-02 17:56:40.779767: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: bev_vgg_pyr/conv1/my_trt_op_6_native_segment
2018-10-02 17:56:40.779773: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 90.597ms.
2018-10-02 17:56:40.779778: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 6 nodes (0), 5 edges (0), time = 57.158ms.
2018-10-02 17:56:40.779786: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 19.502ms.
2018-10-02 17:56:40.779791: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 84.636ms.
2018-10-02 17:56:40.779796: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 16.075ms.
2018-10-02 17:56:40.779800: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: bev_vgg_pyr/conv4/my_trt_op_57_native_segment
2018-10-02 17:56:40.779805: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 106.961ms.
2018-10-02 17:56:40.779810: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 6 nodes (0), 5 edges (0), time = 65.187ms.
2018-10-02 17:56:40.779815: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 18.778ms.
2018-10-02 17:56:40.779819: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 83.257ms.
2018-10-02 17:56:40.779824: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 16.312ms.
2018-10-02 17:56:40.779829: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: img_vgg_pyr/conv4/my_trt_op_96_native_segment
2018-10-02 17:56:40.779833: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 76.479ms.
2018-10-02 17:56:40.779839: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 6 nodes (0), 5 edges (0), time = 46.597ms.
2018-10-02 17:56:40.779843: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 14.137ms.
2018-10-02 17:56:40.779848: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 69.406ms.
2018-10-02 17:56:40.779853: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 13.234ms.
2018-10-02 17:56:40.779857: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: bev_vgg_pyr/my_trt_op_42_native_segment
2018-10-02 17:56:40.779862: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 9 nodes (0), 8 edges (0), time = 68.474ms.
2018-10-02 17:56:40.779867: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 9 nodes (0), 8 edges (0), time = 43.52ms.
2018-10-02 17:56:40.779872: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 9 nodes (0), 8 edges (0), time = 26.47ms.
2018-10-02 17:56:40.779877: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 9 nodes (0), 8 edges (0), time = 78.794ms.
2018-10-02 17:56:40.779881: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 9 nodes (0), 8 edges (0), time = 14.8ms.
2018-10-02 17:56:40.779886: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: img_vgg_pyr/my_trt_op_42_native_segment
2018-10-02 17:56:40.779891: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 7 nodes (0), 6 edges (0), time = 85.927ms.
2018-10-02 17:56:40.779895: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 11 nodes (4), 10 edges (4), time = 43.878ms.
2018-10-02 17:56:40.779903: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 11 nodes (0), 10 edges (0), time = 13.236ms.
2018-10-02 17:56:40.779908: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 11 nodes (0), 10 edges (0), time = 71.235ms.
2018-10-02 17:56:40.779912: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 11 nodes (0), 10 edges (0), time = 12.904ms.
2018-10-02 17:56:40.779917: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: bev_vgg_pyr/my_trt_op_5_native_segment
2018-10-02 17:56:40.779922: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 9 nodes (0), 8 edges (0), time = 70.093ms.
2018-10-02 17:56:40.779927: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 9 nodes (0), 8 edges (0), time = 43.777ms.
2018-10-02 17:56:40.779932: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 9 nodes (0), 8 edges (0), time = 15.261ms.
2018-10-02 17:56:40.779936: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 9 nodes (0), 8 edges (0), time = 64.352ms.
2018-10-02 17:56:40.779941: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 9 nodes (0), 8 edges (0), time = 11.675ms.
2018-10-02 17:56:40.779946: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: my_trt_op_9_native_segment
2018-10-02 17:56:40.779950: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 10 nodes (0), 9 edges (0), time = 138.978ms.
2018-10-02 17:56:40.779955: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 10 nodes (0), 9 edges (0), time = 61.699ms.
2018-10-02 17:56:40.779960: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 10 nodes (0), 9 edges (0), time = 15.373ms.
2018-10-02 17:56:40.779965: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 10 nodes (0), 9 edges (0), time = 153.454ms.
2018-10-02 17:56:40.779970: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 10 nodes (0), 9 edges (0), time = 24.219ms.
2018-10-02 17:56:40.779974: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: bev_vgg_pyr/conv3/my_trt_op_51_native_segment
2018-10-02 17:56:40.779979: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 124.851ms.
2018-10-02 17:56:40.779984: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 6 nodes (0), 5 edges (0), time = 76.039ms.
2018-10-02 17:56:40.779989: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 23.789ms.
2018-10-02 17:56:40.779993: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 91.301ms.
2018-10-02 17:56:40.779998: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 18.15ms.
2018-10-02 17:56:40.780003: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: anchor_predictor/reg_fc6_drop/dropout/my_trt_op_16_native_segment
2018-10-02 17:56:40.780008: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 5 nodes (0), 4 edges (0), time = 103.999ms.
2018-10-02 17:56:40.780013: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 5 nodes (0), 4 edges (0), time = 58.96ms.
2018-10-02 17:56:40.780018: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 5 nodes (0), 4 edges (0), time = 20.132ms.
2018-10-02 17:56:40.780026: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 5 nodes (0), 4 edges (0), time = 96.461ms.
2018-10-02 17:56:40.780031: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 5 nodes (0), 4 edges (0), time = 18.506ms.
2018-10-02 17:56:40.780035: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: my_trt_op_13_native_segment
2018-10-02 17:56:40.780040: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 5 nodes (0), 4 edges (0), time = 159.507ms.
2018-10-02 17:56:40.780045: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 5 nodes (0), 4 edges (0), time = 78.108ms.
2018-10-02 17:56:40.780050: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 5 nodes (0), 4 edges (0), time = 27.84ms.
2018-10-02 17:56:40.780054: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 5 nodes (0), 4 edges (0), time = 142.559ms.
2018-10-02 17:56:40.780059: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 5 nodes (0), 4 edges (0), time = 20.611ms.
2018-10-02 17:56:40.780064: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: bev_vgg_pyr/my_trt_op_49_native_segment
2018-10-02 17:56:40.780068: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 7 nodes (0), 6 edges (0), time = 92.86ms.
2018-10-02 17:56:40.780073: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 11 nodes (4), 10 edges (4), time = 58.017ms.
2018-10-02 17:56:40.780078: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 11 nodes (0), 10 edges (0), time = 21.799ms.
2018-10-02 17:56:40.780082: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 11 nodes (0), 10 edges (0), time = 115.933ms.
2018-10-02 17:56:40.780087: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 11 nodes (0), 10 edges (0), time = 23.586ms.
2018-10-02 17:56:40.780092: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: bev_vgg_pyr/conv3/my_trt_op_14_native_segment
2018-10-02 17:56:40.780097: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 100.614ms.
2018-10-02 17:56:40.780102: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 6 nodes (0), 5 edges (0), time = 71.394ms.
2018-10-02 17:56:40.780106: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 22.289ms.
2018-10-02 17:56:40.780111: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 107.775ms.
2018-10-02 17:56:40.780116: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 6 nodes (0), 5 edges (0), time = 21.687ms.
executing
2018-10-02 17:56:43.449862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2018-10-02 17:56:43.450066: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-10-02 17:56:43.450077: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2018-10-02 17:56:43.450085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2018-10-02 17:56:43.450426: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11334 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:05:00.0, compute capability: 6.1)
2018-10-02 17:56:47.919658: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:314] Input node not found, at TensorRTInputPH_0
2018-10-02 17:56:47.919799: W tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:295] Failed to execute engine, retrying with native segment for import/anchor_predictor/cls_fc6_drop/dropout/my_trt_op_14
2018-10-02 17:56:47.919973: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:314] Input node not found, at TensorRTInputPH_0
2018-10-02 17:56:47.920014: W tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:295] Failed to execute engine, retrying with native segment for import/anchor_predictor/reg_fc6_drop/dropout/my_trt_op_16
2018-10-02 17:56:47.996399: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:314] Input node not found, at TensorRTInputPH_0
2018-10-02 17:56:47.996398: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:314] Input node not found, at TensorRTInputPH_0
2018-10-02 17:56:47.996490: W tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:295] Failed to execute engine, retrying with native segment for import/anchor_predictor/reg_fc7_drop/dropout/my_trt_op_17
2018-10-02 17:56:47.996497: W tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:295] Failed to execute engine, retrying with native segment for import/anchor_predictor/cls_fc7_drop/dropout/my_trt_op_15
2018-10-02 17:56:48.276289: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:314] Input node not found, at TensorRTInputPH_0
2018-10-02 17:56:48.276379: W tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:295] Failed to execute engine, retrying with native segment for import/box_predictor/fc6_drop/dropout/my_trt_op_72
2018-10-02 17:56:48.321104: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:314] Input node not found, at TensorRTInputPH_0
2018-10-02 17:56:48.321176: W tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:295] Failed to execute engine, retrying with native segment for import/box_predictor/fc7_drop/dropout/my_trt_op_73
2018-10-02 17:56:48.364818: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:314] Input node not found, at TensorRTInputPH_0
2018-10-02 17:56:48.364933: W tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:295] Failed to execute engine, retrying with native segment for import/box_predictor/fc8_drop/dropout/my_trt_op_74
5.28882312775
2018-10-02 17:56:48.856168: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:314] Input node not found, at TensorRTInputPH_0
2018-10-02 17:56:48.856186: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:314] Input node not found, at TensorRTInputPH_0
2018-10-02 17:56:48.856236: W tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:295] Failed to execute engine, retrying with native segment for import/anchor_predictor/cls_fc6_drop/dropout/my_trt_op_14
2018-10-02 17:56:48.856259: W tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:295] Failed to execute engine, retrying with native segment for import/anchor_predictor/reg_fc6_drop/dropout/my_trt_op_16
2018-10-02 17:56:48.856704: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:314] Input node not found, at TensorRTInputPH_0
2018-10-02 17:56:48.856732: W tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:295] Failed to execute engine, retrying with native segment for import/anchor_predictor/cls_fc7_drop/dropout/my_trt_op_15
2018-10-02 17:56:48.856766: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:314] Input node not found, at TensorRTInputPH_0
2018-10-02 17:56:48.856798: W tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:295] Failed to execute engine, retrying with native segment for import/anchor_predictor/reg_fc7_drop/dropout/my_trt_op_17
2018-10-02 17:56:48.879536: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:314] Input node not found, at TensorRTInputPH_0
2018-10-02 17:56:48.879587: W tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:295] Failed to execute engine, retrying with native segment for import/box_predictor/fc6_drop/dropout/my_trt_op_72
2018-10-02 17:56:48.879916: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:314] Input node not found, at TensorRTInputPH_0
2018-10-02 17:56:48.879950: W tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:295] Failed to execute engine, retrying with native segment for import/box_predictor/fc7_drop/dropout/my_trt_op_73
2018-10-02 17:56:48.880169: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:314] Input node not found, at TensorRTInputPH_0
2018-10-02 17:56:48.880187: W tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:295] Failed to execute engine, retrying with native segment for import/box_predictor/fc8_drop/dropout/my_trt_op_74
0.145184993744
2018-10-02 17:56:48.977994: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:314] Input node not found, at TensorRTInputPH_0
2018-10-02 17:56:48.978025: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:314] Input node not found, at TensorRTInputPH_0
2018-10-02 17:56:48.978059: W tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:295] Failed to execute engine, retrying with native segment for import/anchor_predictor/cls_fc6_drop/dropout/my_trt_op_14
2018-10-02 17:56:48.978075: W tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:295] Failed to execute engine, retrying with native segment for import/anchor_predictor/reg_fc6_drop/dropout/my_trt_op_16
2018-10-02 17:56:48.978390: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:314] Input node not found, at TensorRTInputPH_0
2018-10-02 17:56:48.978406: W tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:295] Failed to execute engine, retrying with native segment for import/anchor_predictor/cls_fc7_drop/dropout/my_trt_op_15
2018-10-02 17:56:48.978410: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:314] Input node not found, at TensorRTInputPH_0
2018-10-02 17:56:48.978427: W tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:295] Failed to execute engine, retrying with native segment for import/anchor_predictor/reg_fc7_drop/dropout/my_trt_op_17
2018-10-02 17:56:48.993653: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:314] Input node not found, at TensorRTInputPH_0
2018-10-02 17:56:48.993714: W tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:295] Failed to execute engine, retrying with native segment for import/box_predictor/fc6_drop/dropout/my_trt_op_72
2018-10-02 17:56:48.993915: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:314] Input node not found, at TensorRTInputPH_0
2018-10-02 17:56:48.993939: W tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:295] Failed to execute engine, retrying with native segment for import/box_predictor/fc7_drop/dropout/my_trt_op_73
2018-10-02 17:56:48.994105: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:314] Input node not found, at TensorRTInputPH_0
2018-10-02 17:56:48.994121: W tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:295] Failed to execute engine, retrying with native segment for import/box_predictor/fc8_drop/dropout/my_trt_op_74
0.11088013649
2018-10-02 17:56:49.091495: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:314] Input node not found, at TensorRTInputPH_0
2018-10-02 17:56:49.091508: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:314] Input node not found, at TensorRTInputPH_0
2018-10-02 17:56:49.091550: W tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:295] Failed to execute engine, retrying with native segment for import/anchor_predictor/cls_fc6_drop/dropout/my_trt_op_14
2018-10-02 17:56:49.091563: W tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:295] Failed to execute engine, retrying with native segment for import/anchor_predictor/reg_fc6_drop/dropout/my_trt_op_16
2018-10-02 17:56:49.091917: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:314] Input node not found, at TensorRTInputPH_0
2018-10-02 17:56:49.091935: W tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:295] Failed to execute engine, retrying with native segment for import/anchor_predictor/cls_fc7_drop/dropout/my_trt_op_15
2018-10-02 17:56:49.091945: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:314] Input node not found, at TensorRTInputPH_0
2018-10-02 17:56:49.091968: W tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:295] Failed to execute engine, retrying with native segment for import/anchor_predictor/reg_fc7_drop/dropout/my_trt_op_17
2018-10-02 17:56:49.105637: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:314] Input node not found, at TensorRTInputPH_0
2018-10-02 17:56:49.105693: W tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:295] Failed to execute engine, retrying with native segment for import/box_predictor/fc6_drop/dropout/my_trt_op_72
2018-10-02 17:56:49.105894: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:314] Input node not found, at TensorRTInputPH_0
2018-10-02 17:56:49.105917: W tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:295] Failed to execute engine, retrying with native segment for import/box_predictor/fc7_drop/dropout/my_trt_op_73
2018-10-02 17:56:49.106113: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:314] Input node not found, at TensorRTInputPH_0
2018-10-02 17:56:49.106136: W tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:295] Failed to execute engine, retrying with native segment for import/box_predictor/fc8_drop/dropout/my_trt_op_74
0.112968921661
2018-10-02 17:56:49.217640: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:314] Input node not found, at TensorRTInputPH_0
2018-10-02 17:56:49.217641: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:314] Input node not found, at TensorRTInputPH_0
2018-10-02 17:56:49.217740: W tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:295] Failed to execute engine, retrying with native segment for import/anchor_predictor/cls_fc6_drop/dropout/my_trt_op_14
2018-10-02 17:56:49.217756: W tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:295] Failed to execute engine, retrying with native segment for import/anchor_predictor/reg_fc6_drop/dropout/my_trt_op_16
2018-10-02 17:56:49.218166: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:314] Input node not found, at TensorRTInputPH_0
2018-10-02 17:56:49.218185: W tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:295] Failed to execute engine, retrying with native segment for import/anchor_predictor/cls_fc7_drop/dropout/my_trt_op_15
2018-10-02 17:56:49.218238: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:314] Input node not found, at TensorRTInputPH_0
2018-10-02 17:56:49.218265: W tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:295] Failed to execute engine, retrying with native segment for import/anchor_predictor/reg_fc7_drop/dropout/my_trt_op_17
2018-10-02 17:56:49.233776: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:314] Input node not found, at TensorRTInputPH_0
2018-10-02 17:56:49.233867: W tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:295] Failed to execute engine, retrying with native segment for import/box_predictor/fc6_drop/dropout/my_trt_op_72
2018-10-02 17:56:49.234110: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:314] Input node not found, at TensorRTInputPH_0
2018-10-02 17:56:49.234136: W tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:295] Failed to execute engine, retrying with native segment for import/box_predictor/fc7_drop/dropout/my_trt_op_73
2018-10-02 17:56:49.234388: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:314] Input node not found, at TensorRTInputPH_0
2018-10-02 17:56:49.234412: W tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:295] Failed to execute engine, retrying with native segment for import/box_predictor/fc8_drop/dropout/my_trt_op_74
0.12806391716
```
"
22681,tensorflow.python.data.ops.dataset_ops.DatasetSource,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source 
- **TensorFlow version (use command below)**: 1.11
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.16
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
The abstract class [DatasetSource](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/ops/dataset_ops.py#L1496) is defined for representing a dataset with no inputs. The datasets (e.g. [CsvDataset](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/experimental/ops/readers.py#L513)) defined under `tensorflow/python/data/experimental/ops/readers.py` are inherited from `DatasetSource`, but these three [FixedLengthRecordDataset](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/ops/readers.py#L250), [TextLineDataset](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/ops/readers.py#L35), and [TFRecordDataset](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/ops/readers.py#L80) in `tensorflow/python/data/ops/readers.py` are herited from [Dataset](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/ops/dataset_ops.py#L53). In my understanding, they also need to be inherited from `DatasetSource`. If my understanding is right, I would like to submit a PR to update them."
22677,Tensorflow serving does not work for TF 1.11.0 ,"
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:No
- **TensorFlow installed from (source or binary)**:Binary with gpu supported
- **TensorFlow version (use command below)**:1.11.0
- **Python version**:python 27
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:9.0
- **GPU model and memory**: ec2 p2.16xlarge 16 NVIDIA K80 GPUs
- **Exact command to reproduce**:


### Describe the problem
Hi all, I already opened this issue in tensorflow-serving but I think this might be a better place to open this issue. Please let know if I need to close the previous one.
I used the code below to test whether tensor-serving works with tensorflow. Here are some combinations and results:
TF 1.10.0 TF-Serving 1.10.0 Success
TF 1.11.0 TF-Serving 1.10.0 Failed
TF 1.11.0 TF-Serving 1.11.0rc1 Failed

### Source code / logs
```
python tensorflow_serving/example/mnist_saved_model.py /tmp/mnist_model || exit 1

tensorflow_model_server --port=9000 --model_name=mnist --model_base_path=/tmp/mnist_model/ &
SERVER_PID=$!

python mnist_client.py --num_tests=1000 --server=localhost:9000 || exit 1

kill -9 $SERVER_PID
```

Log
```
Training model...
Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.
Extracting /tmp/train-images-idx3-ubyte.gz
Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.
Extracting /tmp/train-labels-idx1-ubyte.gz
Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.
Extracting /tmp/t10k-images-idx3-ubyte.gz
Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.
Extracting /tmp/t10k-labels-idx1-ubyte.gz
training accuracy 0.9092
Done training!
Exporting trained model to /tmp/mnist_model/1
WARNING:tensorflow:From mnist_saved_model.py:139: calling add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.
Instructions for updating:
Pass your op to the equivalent parameter main_op instead.
Done exporting!
Extracting /tmp/train-images-idx3-ubyte.gz
Extracting /tmp/train-labels-idx1-ubyte.gz
Extracting /tmp/t10k-images-idx3-ubyte.gz
Extracting /tmp/t10k-labels-idx1-ubyte.gz
<_Rendezvous of RPC that terminated with (StatusCode.UNAVAILABLE, Connect Failed)>
<_Rendezvous of RPC that terminated with (StatusCode.UNAVAILABLE, Connect Failed)>
<_Rendezvous of RPC that terminated with (StatusCode.UNAVAILABLE, Connect Failed)>
2018-10-02 21:18:32.604303: E external/org_tensorflow/tensorflow/core/common_runtime/executor.cc:697] Executor failed to create kernel. Invalid argument: NodeDef mentions attr 'Truncate' not in Op<name=Cast; signature=x:SrcT -> y:DstT; attr=SrcT:type; attr=DstT:type>; NodeDef: index_to_string/ToInt64 = Cast[DstT=DT_INT64, SrcT=DT_INT32, Truncate=false, _output_shapes=[[10]], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](index_to_string/range). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).
	 [[Node: index_to_string/ToInt64 = Cast[DstT=DT_INT64, SrcT=DT_INT32, Truncate=false, _output_shapes=[[10]], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](index_to_string/range)]]
2018-10-02 21:18:32.604833: E external/org_tensorflow/tensorflow/core/common_runtime/executor.cc:697] Executor failed to create kernel. Invalid argument: NodeDef mentions attr 'Truncate' not in Op<name=Cast; signature=x:SrcT -> y:DstT; attr=SrcT:type; attr=DstT:type>; NodeDef: index_to_string/ToInt64 = Cast[DstT=DT_INT64, SrcT=DT_INT32, Truncate=false, _output_shapes=[[10]], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](index_to_string/range). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).
	 [[Node: index_to_string/ToInt64 = Cast[DstT=DT_INT64, SrcT=DT_INT32, Truncate=false, _output_shapes=[[10]], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](index_to_string/range)]]
2018-10-02 21:18:32.604980: E tensorflow_serving/util/retrier.cc:37] Loading servable: {name: mnist version: 1} failed: Invalid argument: NodeDef mentions attr 'Truncate' not in Op<name=Cast; signature=x:SrcT -> y:DstT; attr=SrcT:type; attr=DstT:type>; NodeDef: index_to_string/ToInt64 = Cast[DstT=DT_INT64, SrcT=DT_INT32, Truncate=false, _output_shapes=[[10]], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](index_to_string/range). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).
	 [[Node: index_to_string/ToInt64 = Cast[DstT=DT_INT64, SrcT=DT_INT32, Truncate=false, _output_shapes=[[10]], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](index_to_string/range)]]
<_Rendezvous of RPC that terminated with (StatusCode.UNAVAILABLE, Connect Failed)>
<_Rendezvous of RPC that terminated with (StatusCode.UNAVAILABLE, Connect Failed)>
<_Rendezvous of RPC that terminated with (StatusCode.DEADLINE_EXCEEDED, Deadline Exceeded)>
<_Rendezvous of RPC that terminated with (StatusCode.UNAVAILABLE, Connect Failed)>
<_Rendezvous of RPC that terminated with (StatusCode.DEADLINE_EXCEEDED, Deadline Exceeded)>
<_Rendezvous of RPC that terminated with (StatusCode.UNAVAILABLE, Connect Failed)>
<_Rendezvous of RPC that terminated with (StatusCode.DEADLINE_EXCEEDED, Deadline Exceeded)>
<_Rendezvous of RPC that terminated with (StatusCode.DEADLINE_EXCEEDED, Deadline Exceeded)>
<_Rendezvous of RPC that terminated with (StatusCode.DEADLINE_EXCEEDED, Deadline Exceeded)>
<_Rendezvous of RPC that terminated with (StatusCode.UNAVAILABLE, Connect Failed)>
```"
22674,tf.parse_single_example parses the record incorrectly,"### System information
- **I have written custom code**:
- **OS  -  Linux Ubuntu 16.04**:
- **TensorFlow installed from binary**:
- **TensorFlow version - 1.10.1 (GPU)**:
- **Python version - 3.6.5**:
- **CUDA/cuDNN version - 9.0/7.0**:

### Issue:
I trying to create tfrecords for my sematic segmentation dataset (rgb_image_in -> binary_raycast_out). 

#### Below is my code to write the list of images to a train.tfrecord.


```
        def _process_image_files(image_names, raycast_names):
        
            writer = tf.python_io.TFRecordWriter('train')
    
            #My implementation of decoding jpeg/png image
            coder = ImageCoder()
    
            for i in range(len(image_names)):
                print('{}\n{}\n\n'.format(image_names[i], raycast_names[i]))

                image_buffer, im_height, im_width, im_channels = _process_image(image_names[i], coder)

                raycast_buffer, rc_height, rc_width, rc_channels = _process_image(raycast_names[i], coder)

                example = _convert_to_example(image_names[i], raycast_names[i], image_buffer, raycast_buffer, \
                                              im_height, im_width, im_channels)

                writer.write(example.SerializeToString())
            writer.close()
            sys.stdout.flush() 
    
    def _process_image(filename, coder):
        with tf.gfile.FastGFile(filename, 'rb') as f:
            image_data = f.read()

        # Decode the RGB JPEG.
        image = coder.decode_jpeg(image_data)

        return image_data, height, width, channels
    
    
    def _convert_to_example(image_name, raycast_name, image_buffer, raycast_buffer, sample_height, sample_width, sample_channels):
    
        example = tf.train.Example(features=tf.train.Features(feature={
            'height': _int64_feature(sample_height),
            'width': _int64_feature(sample_width),
            'channels': _int64_feature(sample_channels),
            'image/filename': _bytes_feature(tf.compat.as_bytes(image_name)),
            'image/encoded': _bytes_feature(tf.compat.as_bytes(image_buffer)),
            'raycast/filename': _bytes_feature(tf.compat.as_bytes(raycast_name)),
            'raycast/encoded': _bytes_feature(tf.compat.as_bytes(raycast_buffer))}))
    
        return example
```


The above code works fine in creating the tfrecord file. I put some print statements inside the `_convert_to_example` method to make sure the corresponding filenames (image_file & raycast_file) are getting written in one example.

However, when I read the examples from tfrecord and print the image names, it looks like the image_file & raycast_file names do not correspond. The pair of images read by the tfRecordReader() is wrong.

#### Below is my code to read the record:


```
    def parse_example_proto(example_serialized):
    
        feature_map = {
                        'image/encoded': tf.FixedLenFeature([], dtype=tf.string, default_value=''),
                        'raycast/encoded': tf.FixedLenFeature([], dtype=tf.string, default_value=''),
                        'height': tf.FixedLenFeature([1], dtype=tf.int64, default_value=-1),
                        'width': tf.FixedLenFeature([1], dtype=tf.int64, default_value=-1),
                        'channels': tf.FixedLenFeature([1], dtype=tf.int64, default_value=-1),
                        'image/filename': tf.FixedLenFeature([], dtype=tf.string, default_value=''),
                        'raycast/filename': tf.FixedLenFeature([], dtype=tf.string, default_value='')
                        }
    
        features = tf.parse_single_example(example_serialized, feature_map)
    
        return features['image/encoded'], features['raycast/encoded'], \
               features['height'], features['width'], features['channels'],\
               features['image/filename'], features['raycast/filename']
    
                
    
    def retirieve_samples():
        
        with tf.name_scope('batch_processing'):
            data_files = ['train']
    
            filename_queue = tf.train.string_input_producer(data_files, shuffle=False)
    
            reader = tf.TFRecordReader()
    
            _, example_serialized = reader.read(filename_queue)
    
            image_buffer, raycast_buffer, height, width, channels, image_name, raycast_name = parse_example_proto(example_serialized)            
            
            return image_name, raycast_name
```

#### Below is my code to print a pair of filenames



```
    image_name, raycast_name = retirieve_samples()
    with tf.Session() as sess:    
        for i in range(1):
            coord = tf.train.Coordinator()
            threads = tf.train.start_queue_runners(sess=sess, coord=coord)
            print(sess.run(image_name))
            print(sess.run(raycast_name))
            coord.request_stop()
            coord.join(threads)
```
I have spent few days on this. I am not able to identify why I am not able to retrieve the correct pair. An example being retrieved should have the same data as the example being created right ? Why am I seeing different name pairs when I read and write ?

Any help would be appriciated
"
22673,Toco outdated in docker build,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: docker
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a
- **TensorFlow installed from (source or binary)**: docker
- **TensorFlow version (use command below)**: latest-gpu-py3
- **Python version**: python3
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: 
- **GPU model and memory**:
- **Exact command to reproduce**: toco

### Describe the problem
Attempting to convert TF graph to TF-lite using the latest stable GPU Python3 docker image. Commands (shown below) differ from those shown in documentation [here](https://www.tensorflow.org/lite/devguide). Additionally, toco conversion of graph throws errors about unsupported operations Fill, Unpack, and MeanSquaredDifference. 

Downloading the latest bazel in the docker container and rebuilding toco from source gives a clearly different version.

### Source code / logs

```
usage: toco [-h] --output_file OUTPUT_FILE
            (--graph_def_file GRAPH_DEF_FILE | --saved_model_dir SAVED_MODEL_DIR | --keras_model_file KERAS_MODEL_FILE)
            [--output_format {TFLITE,GRAPHVIZ_DOT}] [--inference_type {FLOAT,QUANTIZED_UINT8}]
            [--inference_input_type {FLOAT,QUANTIZED_UINT8}] [--input_arrays INPUT_ARRAYS] [--input_shapes INPUT_SHAPES]
            [--output_arrays OUTPUT_ARRAYS] [--saved_model_tag_set SAVED_MODEL_TAG_SET]
            [--saved_model_signature_key SAVED_MODEL_SIGNATURE_KEY] [--std_dev_values STD_DEV_VALUES]
            [--mean_values MEAN_VALUES] [--default_ranges_min DEFAULT_RANGES_MIN]
            [--default_ranges_max DEFAULT_RANGES_MAX] [--quantize_weights QUANTIZE_WEIGHTS] [--drop_control_dependency]
            [--reorder_across_fake_quant] [--change_concat_input_ranges] [--allow_custom_ops]
            [--dump_graphviz_dir DUMP_GRAPHVIZ_DIR] [--dump_graphviz_video]
```"
22670,C API: TF_SessionRun with multiple inputs gives Segmentation Fault ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: no
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.10
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.16.1
- **GCC/Compiler version (if compiling from source)**: 7.3.0
- **CUDA/cuDNN version**: 9.2 / 7.3
- **GPU model and memory**: NVidia GTX 1070ti
- **Exact command to reproduce**: compile and execute my program

### Describe the problem
In C API, I'm using TF_SessionRun to execute inference. I don't understand why I get Segmentation Fault when I pass to TF_SessionRun multiple input tensor, but when I pass only 1 input tensor I get no errors. I see that the signature of the function is:
```
TF_CAPI_EXPORT extern void TF_SessionRun(
    TF_Session* session,
    // RunOptions
    const TF_Buffer* run_options,
    // Input tensors
    const TF_Output* inputs, TF_Tensor* const* input_values, int ninputs,
    // Output tensors
    const TF_Output* outputs, TF_Tensor** output_values, int noutputs,
    // Target operations
    const TF_Operation* const* target_opers, int ntargets,
    // RunMetadata
    TF_Buffer* run_metadata,
    // Output status
    TF_Status*);
```
It seems that, with TF_Tensor* const* input_values, I can send more that once input tensor. For passing more that once input tensor, I've created a std::vector of Tensor *, in fact I haven't any errors in compilation. My code below. My graph have this shape: ? x 50 x 50 x 3 for input, ? x 2 for output (like VGG16, you can use VGG16 for tests) 

### Source code / logs
```
#include ""tensorflow/c/c_api.h""

#include <stdio.h>
#include <stdlib.h>
#include <memory.h>
#include <string.h>
#include <assert.h>
#include <vector>
#include <algorithm>
#include <iterator>
#include <iostream>


TF_Buffer* read_file(const char* file);

void free_buffer(void* data, size_t length) {
}

static void Deallocator(void* data, size_t length, void* arg) {
}

int main() {
  TF_Buffer* graph_def = read_file(""data/graph.pb"");
  TF_Graph* graph = TF_NewGraph();

  TF_Status* status = TF_NewStatus();
  TF_ImportGraphDefOptions* graph_opts = TF_NewImportGraphDefOptions();
  TF_GraphImportGraphDef(graph, graph_def, graph_opts, status);
  if (TF_GetCode(status) != TF_OK) {
          fprintf(stderr, ""ERROR: Unable to import graph %s"", TF_Message(status));
          return 1;
  }
  else {
          fprintf(stdout, ""Successfully imported graph\n"");
  }
  const int num_bytes_in = 1 * 50 * 50 * 3 * sizeof(float);
  const int num_bytes_out = 1 * 2 * sizeof(float);

  int64_t in_dims[] = {1, 50, 50, 3};
  int64_t out_dims[] = {1, 2};

  float values[1 * 50 * 50 * 3] = {0xff};

  std::vector<TF_Output> inputs;
  std::vector<TF_Tensor*> input_values;

  inputs.push_back({TF_GraphOperationByName(graph, ""input_1""), 0});
  input_values.push_back(TF_NewTensor(TF_FLOAT, in_dims, 4, values, num_bytes_in, &Deallocator, 0));
  input_values.push_back(TF_NewTensor(TF_FLOAT, in_dims, 4, values, num_bytes_in, &Deallocator, 0));

  std::vector<TF_Output> outputs;
  outputs.push_back({TF_GraphOperationByName(graph, ""dense_3/Softmax""), 0});

  std::vector<TF_Tensor*> output_values;

  output_values.push_back(TF_AllocateTensor(TF_FLOAT, out_dims, 2, num_bytes_out));
  output_values.push_back(TF_AllocateTensor(TF_FLOAT, out_dims, 2, num_bytes_out));

  fprintf(stdout, ""Running session...\n"");
  TF_SessionOptions* sess_opts = TF_NewSessionOptions();
  TF_Session* session = TF_NewSession(graph, sess_opts, status);
  assert(TF_GetCode(status) == TF_OK);

  TF_SessionRun(session, nullptr,
                &inputs[0], &input_values[0], input_values.size(),
                &outputs[0], &output_values[0], output_values.size(),
                nullptr, 0, nullptr, status);

  TF_Code c = TF_GetCode(status);

  std::cout << c << std::endl;

  for(size_t i = 0; i < output_values.size(); ++i)
  {
      if (output_values.at(i) == nullptr)
      {
          std::cout << ""bad parameters"" << std::endl;
      }
      else
      {
          const auto data = static_cast<float*>(TF_TensorData(output_values.at(i)));
          std::cout << ((data[1] > 0.5f) ? true : false) << std::endl;
      }
  }

  fprintf(stdout, ""Successfully run session\n"");

  TF_CloseSession(session, status);
  TF_DeleteSession(session, status);
  TF_DeleteSessionOptions(sess_opts);
  TF_DeleteImportGraphDefOptions(graph_opts);
  TF_DeleteGraph(graph);
  TF_DeleteStatus(status);
  return 0;
}

TF_Buffer* read_file(const char* file) {
  FILE *f = fopen(file, ""rb"");
  fseek(f, 0, SEEK_END);
  long fsize = ftell(f);
  fseek(f, 0, SEEK_SET);  //same as rewind(f);

  void* data = malloc(fsize);
  fread(data, fsize, 1, f);
  fclose(f);

  TF_Buffer* buf = TF_NewBuffer();
  buf->data = data;
  buf->length = fsize;
  buf->data_deallocator = free_buffer;
  return buf;
}
```
"
22668,Android Tensorflow Lite: Efficient YUV -> RGB conversion are handled by libtensorflow_demo.so.,Solved: yes seems RenderScript faster than YUV -> RGB methods from libtensorflow_demo.so
22667,Build Fail : Build using MSbuild to create whl file (pip package),"I am trying to build Tenserflow following these steps:

https://www.python36.com/install-tensorflow-gpu-windows/

I am on Step 10

Build using MSbuild to create whl file (pip package) using the following command:

**MSBuild /p:Configuration=Release /verbosity:detailed tf_python_build_pip_package.vcxproj**

I am getting following error and build is failing can anyone help?

----------------------------------------------------------------------------------
Done executing task ""Touch"".
Done building target ""FinalizeBuildStatus"" in project ""tf_python_copy_scripts_to_destination.vcxproj"".
Target ""Build"" in file ""C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\v140\Microsoft.BuildSteps.Targets"" from project ""C:\tensorflow\tensorflow\c
ontrib\cmake\build\tf_python_copy_scripts_to_destination.vcxproj"" (entry point):
Done building target ""Build"" in project ""tf_python_copy_scripts_to_destination.vcxproj"".
Done Building Project ""C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_copy_scripts_to_destination.vcxproj"" (default targets).

Done executing task ""MSBuild"" -- FAILED.
Done building target ""ResolveProjectReferences"" in project ""tf_python_build_pip_package.vcxproj"" -- FAILED.
Done Building Project ""c:\tensorflow\tensorflow\contrib\cmake\build\tf_python_build_pip_package.vcxproj"" (default targets) -- FAILED.


Build FAILED.

""c:\tensorflow\tensorflow\contrib\cmake\build\tf_python_build_pip_package.vcxproj"" (default target) (1) ->
""C:\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow_internal.vcxproj"" (default target) (3) ->
""C:\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow_internal_static.vcxproj"" (default target) (4) ->
""C:\tensorflow\tensorflow\contrib\cmake\build\tf_c.vcxproj"" (default target) (5) ->
""C:\tensorflow\tensorflow\contrib\cmake\build\tf_cc_framework.vcxproj"" (default target) (6) ->
""C:\tensorflow\tensorflow\contrib\cmake\build\tf_core_framework.vcxproj"" (default target) (7) ->
""C:\tensorflow\tensorflow\contrib\cmake\build\proto_text.vcxproj"" (default target) (8) ->
""C:\tensorflow\tensorflow\contrib\cmake\build\grpc.vcxproj"" (default target) (9) ->
(CustomBuild target) ->
  C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\v140\Microsoft.CppCommon.targets(171,5): error MSB6006: ""cmd.exe"" exited with code 1. [C:\tensorf
low\tensorflow\contrib\cmake\build\grpc.vcxproj]

    0 Warning(s)
    1 Error(s)
----------------------------------------------------------------------------------

### System information
- **OS Platform and Distribution**: **Windows 8**
- **TensorFlow installed from (source or binary)**: **Source**
- **TensorFlow version (use command below)**: **1.5**
- **Python version**: **3.6**
- **Bazel version (if compiling from source)**: **N/A**
- **GCC/Compiler version (if compiling from source)**:**N/A**
- **CUDA/cuDNN version**: **9.1/7**
- **GPU model and memory**:**N/A**
- **Mobile Device**:**N/A**
- **Exact command to reproduce**: **MSBuild /p:Configuration=Release /verbosity:detailed tf_python_build_pip_package.vcxproj**
"
22665,no such package 'util/hash': BUILD file not found,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: FreeBSD 12.0-ALPHA3
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.11.0
- **Python version**: 2.7.15
- **Bazel version (if compiling from source)**: 0.17.1
- **GCC/Compiler version (if compiling from source)**: clang version 6.0.1
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**: `bazel fetch --repository_cache=/path/to/empty/bazel_cache //tensorflow:libtensorflow.so`

### Describe the problem
I'm doing the port for tensorflow and trying to fetch everything that's required for build first. After extracting tensorflow tarball I run `bazel fetch --repository_cache=/path/to/empty/bazel_cache //tensorflow:libtensorflow.so` and get the following error:

```
ERROR: /wrkdirs/usr/ports/science/tensorflow-core/work/tensorflow-1.11.0/tensorflow/core/common_runtime/eager/BUILD:215:1: no such package 'util/hash': BUILD file not found on package path and referenced by '//tensorflow/core/common_runtime/eager:attr_builder'
ERROR: Evaluation of query ""deps(//tensorflow:libtensorflow.so)"" failed: errors were encountered while computing transitive closure
```"
22664,Cannot compile tensorflow with CUDA 10.0 and cuDNN 7.2,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10
- **TensorFlow installed from (source or binary)**:
Trying to build from source.
- **TensorFlow version (use command below)**:
1.11.0
- **Python version**:
3.6.7rc1
- **Bazel version (if compiling from source)**:
0.16.1 (I tried with latest 1.17.2 but that also didn't build).
- **GCC/Compiler version (if compiling from source)**:
I don't know
- **CUDA/cuDNN version**:
CUDA 9.0 and cuDNN 7.3.1
- **GPU model and memory**:
GTX 1080 8gb
- **Exact command to reproduce**:
bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
I'm trying to build tensorflow from source. This is the error:
```
ERROR: D:/temp/tensorflow/tensorflow/contrib/rnn/BUILD:217:1: C++ compilation of rule '//tensorflow/contrib/rnn:python/ops/_lstm_ops_gpu' failed (Exit 1): msvc_wrapper_for_nvcc.bat failed: error executing command
  cd C:/users/tim/_bazel_tim/lj4hacgi/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0
    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\INCLUDE;C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\winrt;
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\LIB\amd64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\LIB\amd64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17134.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17134.0\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\Tools;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Windows Kits\10\bin\x86;;C:\Windows\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/Tim/AppData/Local/Programs/Python/Python37/python.exe
    SET PYTHON_LIB_PATH=C:/Users/Tim/AppData/Local/Programs/Python/Python37/lib/site-packages
    SET TEMP=C:\Users\Tim\AppData\Local\Temp
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1
    SET TF_CUDA_VERSION=9.0
    SET TF_CUDNN_VERSION=7
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
    SET TMP=C:\Users\Tim\AppData\Local\Temp
```"
22663,Add demo gif to README,"__Disclaimer: This is a bot__

 It looks like your repo is trending. The [github_trending_videos](https://www.instagram.com/github_trending_videos/) Instgram account automatically shows the demo gifs of trending repos in Github.

Your README doesn't seem to have any demo gifs. Add one and the next time the parser runs it will pick it up and post it on its Instagram feed. If you don't want to just close this issue we won't bother you again."
22655,C++ low level API documentation,"I am unable to find the C++ low level API documentation, please advise."
22654,"[ubuntu] Auto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\Windows\Temp' as default","If you open a GitHub issue, here is our policy:

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
N/A
- **TensorFlow installed from (source or binary)**:
release tag v1.110
- **TensorFlow version (use command below)**:
- **Python version**:
3.6
- **Bazel version (if compiling from source)**:
0.17.2
- **GCC/Compiler version (if compiling from source)**:
4.9
- **CUDA/cuDNN version**:
9.1
- **GPU model and memory**:
GeForce 940M/PCIe/SSE2 2GB
- **Exact command to reproduce**:
Commands below. ``./configure`` then;
```
bazel build --config=opt --config=cuda //tensorflow/tools
```

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh
[tf_env.txt](https://github.com/tensorflow/tensorflow/files/2435301/tf_env.txt)

You can obtain the TensorFlow version with
None, 

### Describe the problem
Getting on compliation error:
Auto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\Windows\Temp' as default

### Source code / logs

```
~/tmp/tensorflow$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
Starting local Bazel server and connecting to it...
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
Loading: 0 packages loaded
Loading: 0 packages loaded
    currently loading: tensorflow/tools/pip_package
DEBUG: /home/guy/.cache/bazel/_bazel_guy/0d29558304d115cf217e115548450924/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5: 
Auto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\Windows\Temp' as default
```

my configure:
```
$ ./configure                  
Extracting Bazel installation...
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by com.google.protobuf.UnsafeUtil (file:/home/guy/.cache/bazel/_bazel_guy/install/792a28b07894763eaa2bd870f8776b23/_embedded_binaries/A-server.jar) to field java.lang.String.value
WARNING: Please consider reporting this to the maintainers of com.google.protobuf.UnsafeUtil
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.17.2 installed.
Please specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3


Found possible Python library paths:
  /usr/lib/python3/dist-packages
  /usr/local/lib/python3.6/dist-packages
Please input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]

Do you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: 
jemalloc as malloc support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n
No Google Cloud Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Hadoop File System support? [Y/n]: n
No Hadoop File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Amazon AWS Platform support? [Y/n]: n
No Amazon AWS Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Apache Kafka Platform support? [Y/n]: n
No Apache Kafka Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with XLA JIT support? [y/N]: y
XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with GDR support? [y/N]: n
No GDR support will be enabled for TensorFlow.

Do you wish to build TensorFlow with VERBS support? [y/N]: 
No VERBS support will be enabled for TensorFlow.

Do you wish to build TensorFlow with nGraph support? [y/N]: 
No nGraph support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: 
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]: 9.1


Please specify the location where CUDA 9.1 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /usr


Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: 7.1


Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr]: /usr


Do you wish to build TensorFlow with TensorRT support? [y/N]: 
No TensorRT support will be enabled for TensorFlow.

Please specify the NCCL version you want to use. If NCCL 2.2 is not installed, then you can use version 1.3 that can be fetched automatically but it may have worse performance with multiple GPUs. [Default is 2.2]: 1.3


Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,7.0]: 5.0


Do you want to use clang as CUDA compiler? [y/N]: 
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/x86_64-linux-gnu-gcc-7]: /usr/bin/x86_64-linux-gnu-gcc-4.9


Do you wish to build TensorFlow with MPI support? [y/N]: 
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: 
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See tools/bazel.rc for more details.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
Configuration finished
```
"
22652,tensorflow gpu problem,"hi all,
below is my issue, has tackle it for days and not success
C:\Users\chanw>python
Python 3.6.6 (v3.6.6:4cf1f54eb7, Jun 27 2018, 03:37:03) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\chanw\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\chanw\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\chanw\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\chanw\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\chanw\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: 找不到指定的模組。
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\chanw\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\chanw\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\chanw\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\chanw\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\chanw\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\chanw\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\chanw\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\chanw\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: 找不到指定的模組。

Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/install_sources#common_installation_problems
for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
win10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
pip install command
- **TensorFlow version (use command below)**:
- **Python version**:
3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
9.0/7.3
- **GPU model and memory**:
1070ti
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22651,doc bug on gpu installation page,"Your install website (https://www.tensorflow.org/install/gpu) has the wrong cuda version number referenced. It says that tensorflow wants cuda 9.0 and cuda 7.2, but cuda 7.2.1 seems to work only with cuda 9.2. If you don't upgrade cuda you get this obscure error: 
 
""Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED""

I have written custom code, but not relevant for this problem.
os: ubuntu 16.04
tensorflow-gpu installed with pip
tensorflow version: v1.11.0-0-gc19e29306c 1.11.0
python 3.5.2


Summary: This combination works for me:

cuda 9.2, (download .deb from nvidia website, installed with dpkg)
cudnn 7.2.1 (download .deb from nvidia website, installed with dpg
pip install tensorflow-gpu  (tensorflow 1.11)
python 3.5.2
ubuntu 16.04 
uname - a:
`4.15.0-34-generic #37~16.04.1-Ubuntu SMP Tue Aug 28 10:44:06 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux`

"
22649,request for more general einsum,"### Describe the problem
Tensors have sometimes a lot of dimensions and explicitly writing them helps a lot for writing concise and readable code without bugs. For this, notations like in tf.einsum is invaluable. Unfortunately, the current implementation of tf.einsum is only a subset of what could be done with it : 
* As documented, ""Subscripts that are summed across multiple inputs (e.g., ij,ij,jk->ik)."" are not supported.
* Sums are not supported (eg. ij+ik->ijk, or even both products and sums ij*jk+i*k->ik)
* Expanding dimensions could also be useful (eg. ij,jk->ikl). 
"
22648,v1.11 breaking tf.Variable changes: no inheritance possible,"Cannot inherit from `tf.Variable` anymore. `__init__` only raises `NotImplementedError`.

I've noticed the MetaClass structure that is new (and the VariableV1/V2 WIP on Master)

So should anything different be subclassed? I found no documentation nor notes in the changelog for this (breaking) change.

Request: any kind of documentation or changelog notes on this. Hack: use only < 1.11 version


### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: irrelevant
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: binary, pip
- **TensorFlow version (use command below)**: v1.11.0-0-gc19e29306c 1.11.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: irrelevant
- **GPU model and memory**: irrelevant
- **Exact command to reproduce**:

```
class TestClass(tf.Variable):
    def __init__(self, *args, **kwargs):
        super(TestClass, self).__init__(*args, **kwargs)

instance1 = TestClass()
```

"
22647,Unable to build Tensorflow on Windows 8,"I am not able to build Tensorflow on windows 8 using the following command.

bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

Tensorflow 1.7 
Cuda 9.0

1 error detected in the compilation of ""C:/Users/imrans/AppData/Local/Temp/nvcc_inter_files_tmp_dir/gru_ops_gpu.cu.compute_70.cpp1.ii"".
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 985.757s, Critical Path: 177.12s
INFO: 466 processes: 466 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully

"
22646,Tensorflow 1.10 C++ project errors : logging.h error in line 229,"I have generated ` libtensorflow_cc.so`(V1.9, V1.10 and V1.11) file successully with bazel in Win10 enviroment,  but when I use this TensorFlow API in VS2017, there are errors as bellow:


![image](https://user-images.githubusercontent.com/39480728/46284482-96ce1300-c5aa-11e8-90f1-48f6384eb254.png)

![image](https://user-images.githubusercontent.com/39480728/46296842-dc500780-c5cd-11e8-8ba6-718c2535f5ea.png)

I also build TensorFlow1.8 C++ version API, and it used  OK without these errors in VS2017.

C:\tf\tensorflow1.10\bazel-tensorflow1.10\tensorflow\core\platform\default\logging.h
I don not know why it occur these errors. Tensorflow1.8 did not have these errors.
I build tensorflow1.10 by bazel with Windows10-gpu version."
22645,Problem with CUDA 10 and Tensorflow 1.11,"Hello,

I have Windows 10 , a Geforce 1060 6GB and I have installed CUDA 10. What I would like is to use tensorflow gpu for deep learning purposes. I have installed tensorflow-gpu 1.11

I installed some C++ compilers in Microsoft Visual basic 2017 prior to CUDA 10 installation.
I tried CUDA which was compatible with the graphic card but not CUDA 9. 
Then , I copied the cudnn 7.3.1 (latest) .dll into the bin located in CUDA/bin.


Problem is..tensorflow doesn't still recognize my GPU (recognize only the CPU but printed ""CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2"").

Version of keras 2.2 are still not compatible with tensorflow 1.11.

I checked te version of CUDA and it worked ,it is the 10 version printed out.

However I tried the C++ compiling tests (I tried to compile the bandwidth project in Microsoft Visual Studio loacted in CUDA Samples ) but it seemed to fail.


Can someone help me please , I am a bit desperate !
"
22644,Is there some way to decode bytes string back to unicode which is decoded using string.encode('utf-8') in tensorflow pipline.,"I am using tfRecord for storing my data. I have a Hindi text which is basically Unicode text and to convert it into bytes I have encoded into using string.encode('utf-8').
At the time of taking out data using tensorflow dataset API, I am facing trouble in getting my data in original form, Since I did not find any working method that could convert my byte string back to Unicode string.
I have Tried 
`tf.compat.as_text(
    byte_tensor,
    encoding='utf-8'
)`
also, raw_decode which should not work and it definitely not worked. Though I can convert my text back to original using string.decode('utf-8') by taking data out of the graph. But definitely, I don't want to do that since it will defeat the whole purpose and make my operation slow.
Can anyone suggest any functionality that could help in this matter?"
22643,r1.11: Build instructions for tensorflow lite as static library for android,Is there any? I followed https://github.com/yesmung/tensorflow approach to build 1.9 and it worked. But now I see structure was changed a bit and I would like to avoid porting original solution to 1.11 (especially if there is better way)
22641,[BUG] Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1),"ERROR: /home/tensor/tensorflow/tensorflow/BUILD:566:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1)
Traceback (most recent call last):
  File ""/home/tensor/.cache/bazel/_bazel_tensor/6576bf81f142261c3a81cca43642eb08/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 27, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""/home/tensor/.cache/bazel/_bazel_tensor/6576bf81f142261c3a81cca43642eb08/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 127, in <module>
    from tensorflow.python.platform import test
  File ""/home/tensor/.cache/bazel/_bazel_tensor/6576bf81f142261c3a81cca43642eb08/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/platform/test.py"", line 47, in <module>
    import mock                # pylint: disable=g-import-not-at-top,unused-import
ImportError: No module named mock
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 3283.850s, Critical Path: 90.89s
INFO: 3418 processes: 3418 local.
FAILED: Build did NOT complete successfully
"
22638,[BUG] tf.batch_gather error when feeding int64 indices,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.11.0
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: using CPU
- **GPU model and memory**: using CPU
- **Exact command to reproduce**:
```
import tensorflow as tf
x = [[0, 1, 2], [3, 4, 5]]
y = tf.convert_to_tensor([[1, 2, 0], [2, 0, 1]], dtype=tf.int64)
tf.batch_gather(x, y)
```
### Describe the problem
When feeding an int64 Tensor as `indices` to `tf.batch_gather`, an error occurs.

### Source code / logs
```
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py"", line 510, in _apply_op_helper
    preferred_dtype=default_dtype)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 1144, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 981, in _TensorTensorConversionFunction
    (dtype.name, t.dtype.name, str(t)))
ValueError: Tensor conversion requested dtype int64 for Tensor with dtype int32: 'Tensor(""Reshape:0"", shape=(2, 1), dtype=int32)'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""a.py"", line 4, in <module>
    tf.batch_gather(x, y)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py"", line 2726, in batch_gather
    batch_indices += reshape(dim_indices, dim_shape)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py"", line 862, in binary_op_wrapper
    return func(x, y, name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py"", line 301, in add
    ""Add"", x=x, y=y, name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py"", line 546, in _apply_op_helper
    inferred_from[input_arg.type_attr]))
TypeError: Input 'y' of 'Add' Op has type int32 that does not match type int64 of argument 'x'.
```

The error occurs [here](https://github.com/tensorflow/tensorflow/blob/5fa4e1ac928b0512b28e955c588c5a7eab2ea046/tensorflow/python/ops/array_ops.py#L2726) at the first `for` iteration. I found `batch_indices` is an int64 Tensor, while `dim_indices` is an int32 Tensor.
Obviously, a simple fix is to cast `dim_indices` to the dtype of the input `indices`. I wonder if I can directly send a pull request."
22637,[Feature request] Refactor gFile out into separate library?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS 
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072 1.8.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
I really love gfile's API/ abstraction layer for wrapping GCS, and it's my primary method of scripting file manipulations (as opposed to, say, using a subprocessed gsutil and scraping stdout). However, TensorFlow's long import time imposes a high overhead on these scripts because `from tensorflow import gfile` will import all of TensorFlow. It'd be nice if gfile were refactored into a separate python module.

"
22636,Query regarding 1.11.0 release,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: Binary
- **Python version**: N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
Two questions:
- Are the wheels uploaded on PyPI for Windows made using bazel or cmake?
- The wheels uploaded on PyPI for macOS claim that they target 10.11, however, the function at https://github.com/tensorflow/tensorflow/blob/v1.11.0/tensorflow/core/platform/posix/env_time.cc#L31 uses `clock_gettime`, which wasn't available till 10.12 was released. So, how was it built? 🤔 "
22635,Tensorflow image classification,"I've been trying to build a neural network model that will be able to classify images. But there is this one consistent error that keeps on popping up. Can anyone please help me with this?Here the error below

TypeError: Expected bool for argument 'transpose_a' not <tf.Variable 'Variable_6:0' shape=(1024,) dtype=float32_ref>."
22634,Cmake can't handle deprecated files (assert_next_dataset_op.cc...),"Have I written custom code - No

OS Platform and Distribution - Windows 10


Cmake cannot find deprecated files 

CMake Error at tf_core_kernels.cmake:221 (add_library):
  Cannot find source file:

    C:/tensorflow/tensorflow/contrib/data/kernels/assert_next_dataset_op.cc

  Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm
  .hpp .hxx .in .txx
Call Stack (most recent call first):
  CMakeLists.txt:525 (include)


CMake Error at tf_core_ops.cmake:73 (add_library):
  Cannot find source file:

    C:/tensorflow/tensorflow/contrib/data/ops/dataset_ops.cc

  Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm
  .hpp .hxx .in .txx
Call Stack (most recent call first):
  tf_core_ops.cmake:92 (GENERATE_CONTRIB_OP_LIBRARY)
  CMakeLists.txt:523 (include)


CMake Error at tf_core_ops.cmake:73 (add_library):
  No SOURCES given to target: tf_contrib_data_dataset_ops
Call Stack (most recent call first):
  tf_core_ops.cmake:92 (GENERATE_CONTRIB_OP_LIBRARY)
  CMakeLists.txt:523 (include)


CMake Error at tf_core_kernels.cmake:221 (add_library):
  No SOURCES given to target: tf_core_kernels
Call Stack (most recent call first):
  CMakeLists.txt:525 (include)"
22633,calibration for tensorRT INT8 in tensorflow failed?,"when I use the resnetV150_frozen.pb model， I can calibrate the INT8 graph.     However, I try to use another pb model I download from web, I find the INT8 calibration does not work.   I just don't know why?  I doubt the problem may lie in the memory allocation. And when I use data to calibrate the INT8 graph, it always reports the following error.  “ERROR:tensorflow:Not a calib graph. Doesn't seem to contain any calibration nodes.”  please help me!

if f.INT8:
    calibGraph=getINT8CalibGraph(f.batch_size,wsize)
    
    timings,comp,_,mdstats=timeGraph(calibGraph,1,1,dummy_input)
    int8Graph=getINT8InferenceGraph(calibGraph)
    del calibGraph
    timings,comp,valint8,mdstats=timeGraph(int8Graph,f.batch_size,
                                   f.num_loops,dummy_input)
    
  vals=[valnative,valfp32,valfp16,valint8]


def timeGraph(gdef,batch_size=128,num_loops=100,dummy_input=None,timelineName=None):
  tf.logging.info(""Starting execution"")
  gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)
  tf.reset_default_graph()
  g = tf.Graph()
  if dummy_input is None:
    dummy_input = np.random.random_sample((batch_size,3,224,224))
  outlist=[]
  with g.as_default():

    next_element=tf.constant(dummy_input, dtype= tf.float32)
    out = tf.import_graph_def(
      graph_def=gdef,
      input_map={f.input_node:next_element},
      return_elements=[f.output_node]
    )
    out = out[0].outputs[0]
    outlist.append(out)
    
  timings=[]
  
  with tf.Session(graph=g,config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
    run_metadata = tf.RunMetadata()
    tf.logging.info(""Starting Warmup cycle"")
    
    



    num_iters=1
    for i in range(num_loops):
      tstart=time.time()
      for k in range(num_iters):
        val = sess.run(outlist)
      timings.append((time.time()-tstart)/float(num_iters))
      print(""iter "",i,"" "",timings[-1])
    comp=sess.run(tf.reduce_all(tf.equal(val[0],val[0])))
    print(""Comparison="",comp)
    sess.close()
    tf.logging.info(""Timing loop done!"")
    return timings,comp,val[0],None


def getINT8InferenceGraph(calibGraph):
  trt_graph=trt.calib_graph_to_infer_graph(calibGraph)
  return trt_graph

def getINT8CalibGraph(batch_size=128,workspace_size=1<<25):
  trt_graph = trt.create_inference_graph(getResnet50(f.frozen_graph), [ f.output_node],
                                         max_batch_size=batch_size,
                                         max_workspace_size_bytes=workspace_size,
                                         precision_mode=""INT8""
                                       )  









[model.zip](https://github.com/tensorflow/tensorflow/files/2537426/model.zip)
"
22632,Sometimes Deadlock from TF Dataset using Generator,"Seems to be a normal generator code, but 50% possibility to hang on exit / or print deconstruction exceptions, and it is also possible to exit successfully.
The Tensorflow version I use v1.10

```sh
import tensorflow as tf
import numpy as np

data_dir, batch_size, depth, height, width = '/tmp/dataset/flowers_jpegs', 64, 3, 224, 224

def generator():
  from keras.preprocessing.image import ImageDataGenerator
  from keras.utils import OrderedEnqueuer
  gen = ImageDataGenerator(data_format='channels_first', rescale=1./255, fill_mode='nearest').flow_from_directory(
                           data_dir + '/train', target_size=(height, width), batch_size=batch_size)
  enqueuer = OrderedEnqueuer(gen, use_multiprocessing=False)
  enqueuer.start(workers=16)
  n_classes = gen.num_classes

  while True:
    batch_xs, batch_ys = next(enqueuer.get())
    yield batch_xs, batch_ys

ds = tf.data.Dataset.from_generator(generator, (tf.float32, tf.float32))
ds = ds.prefetch(buffer_size=batch_size)
ds_iter = ds.make_one_shot_iterator()


with tf.Session() as sess:
  images, labels = ds_iter.get_next()
  images = tf.reshape(images, (-1, 3, height, width))
  labels = tf.reshape(labels, (-1, 2))
  print(sess.run(images).reshape([-1])[0:8])

  images, labels = ds_iter.get_next()
  images = tf.reshape(images, (-1, 3, height, width))
  labels = tf.reshape(labels, (-1, 2))
  print(sess.run(images).reshape([-1])[0:8])
```

**Environment:**
**Have I written custom code:** The code above the the complete single python3 file, and no other libraries include customized changes.
**OS Platform and Distribution: Ubuntu 16.04 x86-64 LTS;
**TensorFlow installed from:** clone `tensorflow:v1.10.1` from github and build for CUDA 1.10 and CUDNN 7.3;
**Bazel version:** 0.15.0
**CUDA/cuDNN version:** CUDA = 10.0; CUDNN = 7.3;
**GPU model and memory:** Tesla V100 (each GPU 16 GB memory)
**Exact command to reproduce:** `python3 above-code.py` (note that ""/tmp/dataset/flowers_jpegs"" is a standard directory including original jpeg images inside serveral subfolders (as different classes) which satisfies Keras ImageData directory input format.)
**Mobile device:** N/A
"
22631,"""Not found: Resource does not exist"" exception thrown in runtime","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: .
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.9.0
- **Python version**: 3.5.0
- **Bazel version (if compiling from source)**: /
- **GCC/Compiler version (if compiling from source)**: /
- **CUDA/cuDNN version**: 9.1
- **GPU model and memory**: Tesla M40 24GB, Tesla P100 16GB
- **Exact command to reproduce**: /

### Describe the problem
An exception is produced when I try to run a slightly modified version of a implementation of Tacotron-2(https://github.com/Rayhane-mamah/Tacotron-2.git). The problem can be reproduced with batch size=32 or 48, but not with batch size=64, either on single or multiple GPUs.
Exact the same exception may occur on another model of my own, when the batch size is large enough.
In my implementation variables (VariableV2 ops) are placed on CPU0, while other ops are placed on GPU towers.
The problem looks similar with https://github.com/tensorflow/tensorflow/issues/22094

**Update**
The problems only occurs when colocate_gradients_with_ops=True in gradients computation.
### Source code / logs
2018-09-29 05:10:28.440837: W tensorflow/core/framework/op_kernel.cc:1318] OP_REQUIRES failed at tensor_array_ops.cc:121 : Not found: Resource __per_step_11/_tensor_arraysmodel/tower_2/tacotron/decoder/TensorArray_3/N10tensorflow11TensorArrayE does not exist.
Exiting due to exception: Resource __per_step_11/_tensor_arraysmodel/tower_2/tacotron/decoder/TensorArray_3/N10tensorflow11TensorArrayE does not exist.
	 [[Node: model/tower_2/gradients/model/tower_2/tacotron/decoder/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3 = TensorArrayGradV3[_class=[""loc:@model...yScatterV3""], source=""model/tower_2/gradients"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](model/tower_2/tacotron/decoder/TensorArray/_231, model/tower_2/tacotron/decoder/while/Exit_1/_1161)]]
	 [[Node: model/tower_1/gradients/model/tower_1/tacotron/postnet_conv/conv1d_3/batch_normalization/moments/mean_grad/DynamicStitch/_1982 = _Send[T=DT_INT32, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:1"", send_device=""/job:localhost/replica:0/task:0/device:GPU:1"", send_device_incarnation=1, tensor_name=""edge_14621...amicStitch"", _device=""/job:localhost/replica:0/task:0/device:GPU:1""](model/tower_1/gradients/model/tower_1/tacotron/postnet_conv/conv1d_3/batch_normalization/moments/mean_grad/DynamicStitch)]]

Caused by op 'model/tower_2/gradients/model/tower_2/tacotron/decoder/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3', defined at:
  File ""/tmp/apprunner/.working/runtime/app/train.py"", line 369, in <module>
    main()
  File ""/tmp/apprunner/.working/runtime/app/train.py"", line 365, in main
    train(log_dir, args)
  File ""/tmp/apprunner/.working/runtime/app/train.py"", line 231, in train
    model.add_gradients()
  File ""/tmp/apprunner/.working/runtime/app/models/tacotron.py"", line 256, in add_gradients
    grad_vars = optimizer.compute_gradients(self.loss, colocate_gradients_with_ops=True)
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py"", line 511, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py"", line 532, in gradients
    gate_gradients, aggregation_method, stop_gradients)
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py"", line 701, in _GradientsHelper
    lambda: grad_fn(op, *out_grads))
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py"", line 396, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py"", line 701, in <lambda>
    lambda: grad_fn(op, *out_grads))
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/tensor_array_grad.py"", line 161, in _TensorArrayGatherGrad
    .grad(source=grad_source, flow=flow))
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/tensor_array_ops.py"", line 812, in grad
    return self._implementation.grad(source, flow=flow, name=name)
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/tensor_array_ops.py"", line 239, in grad
    handle=self._handle, source=source, flow_in=flow, name=name)
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/gen_data_flow_ops.py"", line 6229, in tensor_array_grad_v3
    name=name)
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3414, in create_op
    op_def=op_def)
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1740, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

...which was originally created as op 'model/tower_2/tacotron/decoder/TensorArrayStack/TensorArrayGatherV3', defined at:
  File ""/tmp/apprunner/.working/runtime/app/train.py"", line 369, in <module>
    main()
[elided 0 identical lines from previous traceback]
  File ""/tmp/apprunner/.working/runtime/app/train.py"", line 365, in main
    train(log_dir, args)
  File ""/tmp/apprunner/.working/runtime/app/train.py"", line 229, in train
    feeder.target_lengths, global_step=global_step)
  File ""/tmp/apprunner/.working/runtime/app/models/tacotron.py"", line 115, in initialize
    maximum_iterations=max_iters)
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py"", line 329, in dynamic_decode
    final_outputs = nest.map_structure(lambda ta: ta.stack(), final_outputs_ta)
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/util/nest.py"", line 374, in map_structure
    structure[0], [func(*x) for x in entries])
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/util/nest.py"", line 374, in <listcomp>
    structure[0], [func(*x) for x in entries])
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py"", line 329, in <lambda>
    final_outputs = nest.map_structure(lambda ta: ta.stack(), final_outputs_ta)
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/tensor_array_ops.py"", line 856, in stack
    return self._implementation.stack(name=name)
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/tensor_array_ops.py"", line 289, in stack
    return self.gather(math_ops.range(0, self.size()), name=name)
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/tensor_array_ops.py"", line 303, in gather
    element_shape=element_shape)
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/gen_data_flow_ops.py"", line 6018, in tensor_array_gather_v3
    flow_in=flow_in, dtype=dtype, element_shape=element_shape, name=name)
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3414, in create_op
    op_def=op_def)
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1740, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

NotFoundError (see above for traceback): Resource __per_step_11/_tensor_arraysmodel/tower_2/tacotron/decoder/TensorArray_3/N10tensorflow11TensorArrayE does not exist.
	 [[Node: model/tower_2/gradients/model/tower_2/tacotron/decoder/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3 = TensorArrayGradV3[_class=[""loc:@model...yScatterV3""], source=""model/tower_2/gradients"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](model/tower_2/tacotron/decoder/TensorArray/_231, model/tower_2/tacotron/decoder/while/Exit_1/_1161)]]
	 [[Node: model/tower_1/gradients/model/tower_1/tacotron/postnet_conv/conv1d_3/batch_normalization/moments/mean_grad/DynamicStitch/_1982 = _Send[T=DT_INT32, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:1"", send_device=""/job:localhost/replica:0/task:0/device:GPU:1"", send_device_incarnation=1, tensor_name=""edge_14621...amicStitch"", _device=""/job:localhost/replica:0/task:0/device:GPU:1""](model/tower_1/gradients/model/tower_1/tacotron/postnet_conv/conv1d_3/batch_normalization/moments/mean_grad/DynamicStitch)]]

Traceback (most recent call last):
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1322, in _do_call
    return fn(*args)
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1307, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1409, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.NotFoundError: Resource __per_step_11/_tensor_arraysmodel/tower_2/tacotron/decoder/TensorArray_3/N10tensorflow11TensorArrayE does not exist.
	 [[Node: model/tower_2/gradients/model/tower_2/tacotron/decoder/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3 = TensorArrayGradV3[_class=[""loc:@model...yScatterV3""], source=""model/tower_2/gradients"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](model/tower_2/tacotron/decoder/TensorArray/_231, model/tower_2/tacotron/decoder/while/Exit_1/_1161)]]
	 [[Node: model/tower_1/gradients/model/tower_1/tacotron/postnet_conv/conv1d_3/batch_normalization/moments/mean_grad/DynamicStitch/_1982 = _Send[T=DT_INT32, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:1"", send_device=""/job:localhost/replica:0/task:0/device:GPU:1"", send_device_incarnation=1, tensor_name=""edge_14621...amicStitch"", _device=""/job:localhost/replica:0/task:0/device:GPU:1""](model/tower_1/gradients/model/tower_1/tacotron/postnet_conv/conv1d_3/batch_normalization/moments/mean_grad/DynamicStitch)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/tmp/apprunner/.working/runtime/app/train.py"", line 283, in train
    step, loss, _, target_lengths = sess.run([global_step, avg_loss, train_op, feeder.target_lengths])
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 900, in run
    run_metadata_ptr)
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1316, in _do_run
    run_metadata)
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: Resource __per_step_11/_tensor_arraysmodel/tower_2/tacotron/decoder/TensorArray_3/N10tensorflow11TensorArrayE does not exist.
	 [[Node: model/tower_2/gradients/model/tower_2/tacotron/decoder/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3 = TensorArrayGradV3[_class=[""loc:@model...yScatterV3""], source=""model/tower_2/gradients"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](model/tower_2/tacotron/decoder/TensorArray/_231, model/tower_2/tacotron/decoder/while/Exit_1/_1161)]]
	 [[Node: model/tower_1/gradients/model/tower_1/tacotron/postnet_conv/conv1d_3/batch_normalization/moments/mean_grad/DynamicStitch/_1982 = _Send[T=DT_INT32, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:1"", send_device=""/job:localhost/replica:0/task:0/device:GPU:1"", send_device_incarnation=1, tensor_name=""edge_14621...amicStitch"", _device=""/job:localhost/replica:0/task:0/device:GPU:1""](model/tower_1/gradients/model/tower_1/tacotron/postnet_conv/conv1d_3/batch_normalization/moments/mean_grad/DynamicStitch)]]

Caused by op 'model/tower_2/gradients/model/tower_2/tacotron/decoder/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3', defined at:
  File ""/tmp/apprunner/.working/runtime/app/train.py"", line 369, in <module>
    main()
  File ""/tmp/apprunner/.working/runtime/app/train.py"", line 365, in main
    train(log_dir, args)
  File ""/tmp/apprunner/.working/runtime/app/train.py"", line 231, in train
    model.add_gradients()
  File ""/tmp/apprunner/.working/runtime/app/models/tacotron.py"", line 256, in add_gradients
    grad_vars = optimizer.compute_gradients(self.loss, colocate_gradients_with_ops=True)
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py"", line 511, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py"", line 532, in gradients
    gate_gradients, aggregation_method, stop_gradients)
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py"", line 701, in _GradientsHelper
    lambda: grad_fn(op, *out_grads))
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py"", line 396, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py"", line 701, in <lambda>
    lambda: grad_fn(op, *out_grads))
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/tensor_array_grad.py"", line 161, in _TensorArrayGatherGrad
    .grad(source=grad_source, flow=flow))
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/tensor_array_ops.py"", line 812, in grad
    return self._implementation.grad(source, flow=flow, name=name)
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/tensor_array_ops.py"", line 239, in grad
    handle=self._handle, source=source, flow_in=flow, name=name)
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/gen_data_flow_ops.py"", line 6229, in tensor_array_grad_v3
    name=name)
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3414, in create_op
    op_def=op_def)
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1740, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

...which was originally created as op 'model/tower_2/tacotron/decoder/TensorArrayStack/TensorArrayGatherV3', defined at:
  File ""/tmp/apprunner/.working/runtime/app/train.py"", line 369, in <module>
    main()
[elided 0 identical lines from previous traceback]
  File ""/tmp/apprunner/.working/runtime/app/train.py"", line 365, in main
    train(log_dir, args)
  File ""/tmp/apprunner/.working/runtime/app/train.py"", line 229, in train
    feeder.target_lengths, global_step=global_step)
  File ""/tmp/apprunner/.working/runtime/app/models/tacotron.py"", line 115, in initialize
    maximum_iterations=max_iters)
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py"", line 329, in dynamic_decode
    final_outputs = nest.map_structure(lambda ta: ta.stack(), final_outputs_ta)
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/util/nest.py"", line 374, in map_structure
    structure[0], [func(*x) for x in entries])
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/util/nest.py"", line 374, in <listcomp>
    structure[0], [func(*x) for x in entries])
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py"", line 329, in <lambda>
    final_outputs = nest.map_structure(lambda ta: ta.stack(), final_outputs_ta)
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/tensor_array_ops.py"", line 856, in stack
    return self._implementation.stack(name=name)
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/tensor_array_ops.py"", line 289, in stack
    return self.gather(math_ops.range(0, self.size()), name=name)
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/tensor_array_ops.py"", line 303, in gather
    element_shape=element_shape)
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/gen_data_flow_ops.py"", line 6018, in tensor_array_gather_v3
    flow_in=flow_in, dtype=dtype, element_shape=element_shape, name=name)
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3414, in create_op
    op_def=op_def)
  File ""/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1740, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

NotFoundError (see above for traceback): Resource __per_step_11/_tensor_arraysmodel/tower_2/tacotron/decoder/TensorArray_3/N10tensorflow11TensorArrayE does not exist.
	 [[Node: model/tower_2/gradients/model/tower_2/tacotron/decoder/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3 = TensorArrayGradV3[_class=[""loc:@model...yScatterV3""], source=""model/tower_2/gradients"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](model/tower_2/tacotron/decoder/TensorArray/_231, model/tower_2/tacotron/decoder/while/Exit_1/_1161)]]
	 [[Node: model/tower_1/gradients/model/tower_1/tacotron/postnet_conv/conv1d_3/batch_normalization/moments/mean_grad/DynamicStitch/_1982 = _Send[T=DT_INT32, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:1"", send_device=""/job:localhost/replica:0/task:0/device:GPU:1"", send_device_incarnation=1, tensor_name=""edge_14621...amicStitch"", _device=""/job:localhost/replica:0/task:0/device:GPU:1""](model/tower_1/gradients/model/tower_1/tacotron/postnet_conv/conv1d_3/batch_normalization/moments/mean_grad/DynamicStitch)]]
"
22630,"Failure [INSTALL_FAILED_NO_MATCHING_ABIS: Failed to extract native libraries, res=-113]","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:1.10
- **Python version**:3.6
- **Bazel version (if compiling from source)**:0.17
- **GCC/Compiler version (if compiling from source)**:N/a
- **CUDA/cuDNN version**: N/a
- **GPU model and memory**: N/a
- **Exact command to reproduce**: adb install bazel-bin/tensorflow/contrib/lite/examples/android/tflite_demo.apk


You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I'm trying tensorflow demo app on my computer using the bazel build in the ubuntu terminal. The project has built successfully. When I run **adb install bazel-bin/tensorflow/contrib/lite/examples/android/tflite_demo.apk** I got this error
**Failure [INSTALL_FAILED_NO_MATCHING_ABIS: Failed to extract native libraries, res=-113]**

How can I solve this problem? Any help, please

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22629,Building tensorflow on Aarch64 (bazel newer than 0.16.0) has no default_toolchain provided.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 18.04 on NVIDIA Jetson AGX Xavier (Aarch64)
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
N/A
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
1.10.1
- **Python version**:
- **Bazel version (if compiling from source)**:
0.17.2
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
NVIDIA Jetson Xavier
- **Exact command to reproduce**:

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Compiling on a Jetson AGX Xavier board yields an error `ERROR: No default_toolchain found for cpu 'aarch64'. Valid cpus are`...

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

### Commentary

It looks like the problem is that recent (newer than 0.16.0) versions of bazel provide a named cpu for aarch64 rather than wrapping it in to ARM. The relevent bazel commit is https://github.com/bazelbuild/bazel/commit/886d01c89fca32e46f5841081eb3288e5b4f313b#diff-1ec7b345acf6aff694c52be93dc2f3d1. Using bazel 0.15.2 on aarch64 works (although needs the patch https://github.com/JasonAtNvidia/JetsonTFBuild/blob/master/jetson.patch)"
22628,tf-1.10.1 freeze_graph 'list index out of range',"See [here](https://github.com/tensorflow/tensorflow/issues/22029), I wonder why no one answer my question ???"
22627,tf-1.10.1 freeze_graph 'list index out of range',"See [here](https://github.com/tensorflow/tensorflow/issues/22029), I wonder why no one answer my question ???"
22626,tf-1.10.1 freeze_graph 'list index out of range',"See [here](https://github.com/tensorflow/tensorflow/issues/22029), I wonder why no one answer my question ???"
22625,The scope of application for  Post-training quantization,"If I want to optimize tensorflow model running on linux,  can I use Post-training quantization? or can a tensorflow lite model run on linux?"
22624,The problem of   quantifying model by tf.keras,"I can use the format of h5 to convert my model to tflite, but i can't quantize the model to int8"
22623,Almost 2GB GPU memory missing in Tensorflow as compared to what nvidia-smi reports,"I have a 11GB 1080Ti GPU, NVidia-smi reports 11264MiB memory, Tensorflow reports 9.1GiB memory only. 

I understand that stackoverflow may be a better option to raise this question, but I believe this issue could be a bug or incompatibility among Windows 10, NVidia driver and Tensorflow. 


### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Problem starts to happen when enabling the GPU in python:
    import tensorflow as tf
    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=1)
    config = tf.ConfigProto(gpu_options=gpu_options)
    session = tf.Session(config=config)

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10

- **TensorFlow installed from (source or binary)**:
Tensorflow installed from binary
- **TensorFlow version (use command below)**:
print(tf.__version__)
1.10.0

- **Python version**:
python --version
Python 3.6.6

- **CUDA/cuDNN version**:
nvcc: NVIDIA (R) Cuda compiler driver
Cuda compilation tools, release 9.0, V9.0.176
NVIDIA-SMI 388.13                 Driver Version: 388.13


- **GPU model and memory**:
EVGA 1080 Ti 11GB memory

- **Exact command to reproduce**:
    import tensorflow as tf
    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=1)
    config = tf.ConfigProto(gpu_options=gpu_options)
    session = tf.Session(config=config)


### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I couldn't find similar situation online. After a fresh boot, with the 1080Ti connecting to NO monitor (Intel 4600HD act as the primary display), NVIDIA-SMI reports no process occupying the memory, Tensorflow still reports much less memory than NVIDIA-SMI.

Below are the output after issuing the tensorflow-start script with GPU support, and the outputs of the nvidia-smi command. The free GPU memory is 9.1GiB only in tensorflow as compared to 11GB by nvidia-smi. Tensorflow does attempt to allocate 11GiB memory because of per_process_gpu_memory_fraction=1, but cuda reports out of memory error. Using the Allow_growth option won't break the 9.1GiB limits. After launching tensorflow with GPU support, nvidia-smi reports 9460MiB / 11264MiB of memory usage.

*************************************Output from tensorflow ***********************************
T:\src\github\tensorflow\tensorflow\core\platform\cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2018-09-30 03:14:36.523917: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1405] Found device 0 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:01:00.0
totalMemory: 11.00GiB freeMemory: 9.10GiB
2018-09-30 03:14:36.528039: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1484] Adding visible gpu devices: 0
2018-09-30 03:14:37.821700: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-09-30 03:14:37.824541: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:971]      0 
2018-09-30 03:14:37.826779: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:984] 0:   N 
2018-09-30 03:14:37.828695: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11264 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-09-30 03:14:37.835373: E T:\src\github\tensorflow\tensorflow\stream_executor\cuda\cuda_driver.cc:903] failed to allocate 11.00G (11811160064 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2018-09-30 03:14:37.838382: E T:\src\github\tensorflow\tensorflow\stream_executor\cuda\cuda_driver.cc:903] failed to allocate 9.90G (10630043648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY

**************************nvidia-smi output before launching tensorflow **************************
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 388.13                 Driver Version: 388.13                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 108... WDDM  | 00000000:01:00.0 Off |                  N/A |
|  9%   55C    P0    60W / 250W |    132MiB / 11264MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

**************************nvidia-smi output after launching tensorflow **************************
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 388.13                 Driver Version: 388.13                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 108... WDDM  | 00000000:01:00.0 Off |                  N/A |
| 10%   55C    P2    57W / 250W |   9460MiB / 11264MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0     18556      C   ...cal\Programs\Python\Python36\python.exe N/A      |
+-----------------------------------------------------------------------------+"
22621,FailedPreconditionError in of tf.keras TF1.11 but not TF1.10,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 RS4 x64
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.11
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 10/7.3
- **GPU model and memory**: GTX1060 6GB
- **Exact command to reproduce**:

```
import tensorflow as tf
get_session = tf.keras.backend.get_session
a = tf.Variable([[0.5, 0., 1.], [2., 0., -1.]])
tf.add(a, a).eval(session=get_session())
```

### Describe the problem
A code that run with TF1.10 but not TF1.11

### Source code / logs
```
FailedPreconditionError (see above for traceback): Attempting to use uninitialized value Variable_1
	 [[{{node Variable_1/read}} = Identity[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Variable_1)]]
```
"
22620,Why using Camera2 API for Android Tensorflow Lite Demo?,"Many manufactures didn't implemented Camera2 that well https://github.com/googlesamples/android-Camera2Basic/issues/123

So it's better to use legacy Camera API even for Android API >= 25"
22619,Unable to use multiple CPU cores in TensorFlow,"This issue was previously asked here on StackOverflow (with no answers at the time of this issue): https://stackoverflow.com/q/52507748/188046

------------------------

### System information
- **Have I written custom code**: custom Python code, but no custom ops (Python is linked below)
- **OS Platform and Distribution**: Ubuntu 16.04.5
- **Mobile device**: N/A
- **TensorFlow installed from**: binary, from `tensorflow` PyPI package via pip (also tried from conda with same result)
- **TensorFlow version**:  v1.11.0-0-gc19e29306c 1.11.0
- **Python version**: 3.6.6 from conda
- **Bazel version**: N/A
- **GCC/Compiler version**: N/A
- **CUDA/cuDNN version**: N/A, problem occurs on CPU
- **GPU model and memory**: N/A, problem occurs on CPU
- **CPU model**: Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz (2 sockets, 10 cores per socket)
- **Exact command to reproduce**: See below

### Describe the problem

I am unable to configure TensorFlow to use multiple CPU cores for inter-op parallelism on my machine. As described in my [StackOverflow question](https://stackoverflow.com/q/52507748/188046), I have read other answers extensively, and scrubbed the first page of Google search results for several keywords, and tried everything I've seen suggested, and I just can't get this to work.

I have included a program below that demonstrates the problem. The program calls `matmul` once per core (i.e. weak scaling). I would expect that as the number of cores increases, the running time would stay roughly constant. Instead the running time seems to increase linearly with the core count, indicating that the `matmul` ops are running sequentially, not in parallel.

I have also confirmed via `htop` that there is only one core on my CPU that is in use when the program is running. The system is otherwise idle. `htop` has the capability to show multiple threads within a process, but I do not even see these (or they are not using enough CPU to show up on the first page of results when sorted by CPU usage).

How can I get TensorFlow to execute different operations on different cores in parallel?

Note:

  * I am creating a session with multiple CPU devices. I have also tried only creating a single CPU device, and relying entirely on `inter_op_parallelism_threads`. Nothing I have tried has been able to use multiple cores.
  * I can comment out the line `with tf.device(d):`, and it makes no difference.
  * I have tried tracing (see the commented out lines), and the trace seems to reflect what I'd expect. Ops are being assigned to the CPUs like I want them to be. However, they still don't run in parallel.
  * I have also tried generating a Chrome trace (commented out lines at the very bottom). The Chrome trace doesn't seem to be working properly, or at least the reported running times are way off what they should be. So I'm not sure how much this information can be relied upon. Perhaps I'm doing something wrong.

### Source code / logs

Source for test_cores.py: https://gist.github.com/elliottslaughter/750a27c832782f4daec8686281027de8

Sample output:

```
$ python test_cores.py 
2018-09-29 09:40:34.489657: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary 
was not compiled to use: AVX2 FMA
Running on 1 cores
  Assigning matmul to /cpu:0
  Duration (via time.perf_counter()): 3.209691 (693823.014392 - 693819.804701)
  Clock (via time.clock()): 3.205479 (8.912035 - 5.706556)
Running on 2 cores
  Assigning matmul to /cpu:0
  Assigning matmul to /cpu:1
  Duration (via time.perf_counter()): 6.452124 (693829.493906 - 693823.041782)
  Clock (via time.clock()): 6.449224 (15.388567 - 8.939343)
```"
22618,cannot make learning rate as variable when using keras model to estimator,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: pip install
- **TensorFlow version (use command below)**: 1.11.0-cpu-py3
- **Python version**:3.6.5
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA 
- **GPU model and memory**: NA
- **Exact command to reproduce**: run the code provided below 

### Describe the problem
I was trying to make learning rate a variable and control it using hook during training, however, it gives me the following error <Tensor(""Variable/read:0"", shape=(), dtype=float32) must be from the same graph as Tensor(""dense/kernel:0"", shape=(), dtype=resource).>

PS: when I define the estimator through the generic ""model_fn"" instead of keras, variable learning rate can work.  Therefore, I believe the issue comes from tf.keras.estimator.model_to_estimator


### Source code / logs
```
import tensorflow as tf
import numpy as np

def estimator_fn():
    x_in = tf.keras.layers.Input(shape=[10])
    x = tf.keras.layers.Dense(16, activation='relu')(x_in)
    x_out = tf.keras.layers.Dense(1, activation='sigmoid')(x)
    model = tf.keras.models.Model(x_in, x_out)
    lr = tf.Variable(0.1, trainable=False, dtype=tf.float32)
    optimizer = tf.train.GradientDescentOptimizer(lr)
    model.compile(loss='binary_crossentropy', optimizer= optimizer)
    estimator = tf.keras.estimator.model_to_estimator(keras_model = model)
    return estimator

def input_fn():
    np.random.seed(100)
    x = np.random.random((1024, 10))
    y = np.random.randint(2, size=(1024, 1))
    x = tf.cast(x, tf.float32)
    dataset = tf.data.Dataset.from_tensor_slices((x, y))
    dataset = dataset.repeat()
    dataset = dataset.batch(1024)
    return dataset
  
model_estimator = estimator_fn()
model_estimator.train(input_fn=input_fn, steps=1000)
```"
22617,No module named '_tensorflow_wrap_toco',"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
binary (through pip)
- **TensorFlow version (use command below)**:
1.11.0
- **Python version**:
3.6.4
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
None (CPU only)
- **GPU model and memory**:
None
- **Exact command to reproduce**:

### Describe the problem
I saved my keras model to file and tried to use ""lite.TFLiteConverter.from_keras_model_file(...)""  and ""tflite_model = converter.convert()"" to get a lite model, but got error ""No module named '_tensorflow_wrap_toco'"". This is only a ""tensorflow_wrap_toco.py"" in ""\tensorflow\contrib\lite\toco\python"", and no ""_tensorflow_wrap_toco"" in that file.
I have updated my tensorflow through ""pip install tensorflow --upgrade""

### Source code / logs
```
model= get_testing_model(input_shape=(160,140)) 
model.load_weights(keras_weights_file)
model.save(""kerasModel.h5"")
converter = lite.TFLiteConverter.from_keras_model_file(""kerasModel.h5"")
tflite_model = converter.convert() # bug happens here
open(""converted_model.tflite"", ""wb"").write(tflite_model)
```
Logs:
FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2018-09-29 21:03:55.936260: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.
Traceback (most recent call last):
  File ""C:/Users/wang/Desktop/OpenPoseApp/camera-openpose-keras/demo_camera.py"", line 325, in <module>
    save_tf_lite_model()
  File ""C:/Users/wang/Desktop/OpenPoseApp/camera-openpose-keras/demo_camera.py"", line 311, in save_tf_lite_model
    tflite_model = converter.convert()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\contrib\lite\python\lite.py"", line 453, in convert
    **converter_kwargs)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\contrib\lite\python\convert.py"", line 342, in toco_convert_impl
    input_data.SerializeToString())
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\contrib\lite\python\convert.py"", line 135, in toco_convert_protos
    (stdout, stderr))
RuntimeError: TOCO failed see console for info.
b'c:\\programdata\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nTraceback (most recent call last):\r\n  File ""c:\\programdata\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\lite\\toco\\python\\tensorflow_wrap_toco.py"", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module(\'_tensorflow_wrap_toco\', [dirname(__file__)])\r\n  File ""c:\\programdata\\anaconda3\\lib\\imp.py"", line 297, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named \'_tensorflow_wrap_toco\'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File ""c:\\programdata\\anaconda3\\lib\\runpy.py"", line 193, in _run_module_as_main\r\n    ""__main__"", mod_spec)\r\n  File ""c:\\programdata\\anaconda3\\lib\\runpy.py"", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File ""C:\\ProgramData\\Anaconda3\\Scripts\\toco_from_protos.exe\\__main__.py"", line 5, in <module>\r\n  File ""c:\\programdata\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\lite\\toco\\python\\toco_from_protos.py"", line 22, in <module>\r\n    from tensorflow.contrib.lite.toco.python import tensorflow_wrap_toco\r\n  File ""c:\\programdata\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\lite\\toco\\python\\tensorflow_wrap_toco.py"", line 28, in <module>\r\n    _tensorflow_wrap_toco = swig_import_helper()\r\n  File ""c:\\programdata\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\lite\\toco\\python\\tensorflow_wrap_toco.py"", line 20, in swig_import_helper\r\n    import _tensorflow_wrap_toco\r\nModuleNotFoundError: No module named \'_tensorflow_wrap_toco\'\r\n'
None
"
22615,AudioSpectrogram not support batch_size mode and no fbank op ,"AudioSpectrogram not support audios with `batch_size` dim.

```
ValueError: Shape must be rank 2 but is rank 3 for 'AudioSpectrogram' (op: 'AudioSpectrogram') with input shapes: [32,?,1].
```


There have `Mfcc` and `Spectrogram` api but no `Fbank` api export ."
22614,what is operator name for python corresponding to kReducePrecision,I am wondering what is the operator in python corresponding to the opcode kReducePrecision. Thanks for any hint.
22613,Is official 1.11.0 pip package for windows build by bazel or cmake?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.11.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.15.0
- **GCC/Compiler version (if compiling from source)**: vs2017 15.8 / msvc 19.15.26726
- **CUDA/cuDNN version**: 10.0.130_411.31/7.3.0.29
- **GPU model and memory**: 1080ti 11GB
- **Exact command to reproduce**: pip install tensorflow-gpu

### Describe the problem

In 1.10 release notes, you say `Starting from TensorFlow 1.11, Windows builds will use Bazel. Therefore, we will drop official support for cmake.`, but table from https://www.tensorflow.org/install/source_windows indicate that 1.11 is build by cmake.

The cuDNN version described in the table from https://www.tensorflow.org/install/source_windows or https://www.tensorflow.org/install/source still 7, not 7.2 or 7.x.x. Which version of cudnn do you used to build pip package?

Nvidia's website didn't provide cudnn 7.2 for cuda 9.0 download, please provide more clarify documentation.

Thanks

### Source code / logs

```
pip install tensorflow-gpu==1.11.0
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
b'v1.11.0-rc2-4-gc19e29306c' 1.11.0
```"
22612,"A Error about ""from tensorflow.examples.tutorials.mnist import input_data""","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code:YES ,I have
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:windows10 64 bit
- **Mobile device  if the issue happens on mobile device**:no ,it's not work on Mobile device
- **TensorFlow installed from (source or binary)**:installed from command
- **TensorFlow version (use command below)**:tensorflow 1.10.0
- **Python version**:python3.6.5
- **Bazel version (if compiling from source)**:no,it's not compiling from source 
- **GCC/Compiler version (if compiling from source)**:no,it's not compiling from source 
- **CUDA/cuDNN version**:CUDA9.0/cuDNN7.1
- **GPU model and memory**:GTX1080/8192
- **Exact command to reproduce**:command


### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
In python, I type:
from tensorflow.examples.tutorials.mnist import input_data

it got error as follows:

NotFoundError                             Traceback (most recent call last)
<ipython-input-6-127a3aab68d9> in <module>()
----> 1 from tensorflow.examples.tutorials.mnist import input_data
      2 import tensorflow as tf
      3 mnist = input_data.read_data_sets('MNIST_data', one_hot=True)

D:\software\anaconda\envs\tensorflow-gpu\lib\site-packages\tensorflow\examples\tutorials\mnist\__init__.py in <module>()
     19 from __future__ import print_function
     20 
---> 21 from tensorflow.examples.tutorials.mnist import input_data
     22 from tensorflow.examples.tutorials.mnist import mnist

D:\software\anaconda\envs\tensorflow-gpu\lib\site-packages\tensorflow\examples\tutorials\mnist\input_data.py in <module>()
     28 from six.moves import xrange  # pylint: disable=redefined-builtin
     29 import tensorflow as tf
---> 30 from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets
     31 # pylint: enable=unused-import

D:\software\anaconda\envs\tensorflow-gpu\lib\site-packages\tensorflow\contrib\__init__.py in <module>()
     35 from tensorflow.contrib import crf
     36 from tensorflow.contrib import cudnn_rnn
---> 37 from tensorflow.contrib import data
     38 from tensorflow.contrib import deprecated
     39 from tensorflow.contrib import distribute

D:\software\anaconda\envs\tensorflow-gpu\lib\site-packages\tensorflow\contrib\data\__init__.py in <module>()
     76 from tensorflow.contrib.data.python.ops.counter import Counter
     77 from tensorflow.contrib.data.python.ops.enumerate_ops import enumerate_dataset
---> 78 from tensorflow.contrib.data.python.ops.error_ops import ignore_errors
     79 from tensorflow.contrib.data.python.ops.get_single_element import get_single_element
     80 from tensorflow.contrib.data.python.ops.grouping import bucket_by_sequence_length

D:\software\anaconda\envs\tensorflow-gpu\lib\site-packages\tensorflow\contrib\data\python\ops\error_ops.py in <module>()
     18 from __future__ import print_function
     19 
---> 20 from tensorflow.contrib.data.python.ops import contrib_op_loader  # pylint: disable=unused-import
     21 from tensorflow.contrib.data.python.ops import gen_dataset_ops
     22 from tensorflow.python.data.ops import dataset_ops

D:\software\anaconda\envs\tensorflow-gpu\lib\site-packages\tensorflow\contrib\data\python\ops\contrib_op_loader.py in <module>()
     22 
     23 _dataset_ops = loader.load_op_library(
---> 24     resource_loader.get_path_to_datafile(""../../_dataset_ops.so""))

D:\software\anaconda\envs\tensorflow-gpu\lib\site-packages\tensorflow\contrib\util\loader.py in load_op_library(path)
     54       return None
     55   path = resource_loader.get_path_to_datafile(path)
---> 56   ret = load_library.load_op_library(path)
     57   assert ret, 'Could not load %s' % path
     58   return ret

D:\software\anaconda\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\framework\load_library.py in load_op_library(library_filename)
     54     RuntimeError: when unable to load the library or get the python wrappers.
     55   """"""
---> 56   lib_handle = py_tf.TF_LoadLibrary(library_filename)
     57 
     58   op_list_str = py_tf.TF_GetOpList(lib_handle)

NotFoundError: D:\software\anaconda\envs\tensorflow-gpu\lib\site-packages\tensorflow\contrib\data\python\ops\..\..\_dataset_ops.so not found

and It also pops up a window saying:
The procedure entry point ?addcleanup@arenaimpl@internal@protobuf@google@@QEAAXPEAXP6AX0@Z@Z could not be located in the dynamic link library _pywarp_tensorflow_internal.pyd

what should I do to solve it?by the way ,I Use it OK in tensorflow 1.5.have it changed in tensorflow?where  can I find guid about it?
please help ! thank you very much in advance!



### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22611,compiler/tests/stateless_random_ops_test.py:testRandomNormalIsFinite doesn't,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: n/a
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: n/a
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a
- **TensorFlow installed from (source or binary)**: n/a
- **TensorFlow version (use command below)**: n/a
- **Python version**: n/a
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: n/a

### Describe the problem

@hawkinsp wrote a test called [`testRandomNormalIsFinite`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tests/stateless_random_ops_test.py#L94), but it uses `stateless_random_uniform` instead of `stateless_random_normal`.

### Source code / logs

N/A."
22610,Here is a list of operators for which  you will need custom implementations: ListDiff,"i use tf.keras to save my h5 model, and convert to tflite, this op can't be supported"
22609,tf.keras.BatchNormalization  Cannot be quantified,"Batch normalization resolution requires that mean, multiplier and offset arrays be constant.



"
22608,tensorflow new installation method,"about a month agp, there are two ways to install tensorflow one of which is through anaconda. But now when i visit the tensorflow website, anaconda is not an option already. and now that I have changed laptop , I have to reinstall it. Installing under windows, i have to install bazel, MSYS2, visual c++ build tools 2015 which I have never heard of when I initially installed in my old laptop. Do I have to install bazel etc.?
https://www.tensorflow.org/install/source_windows

And must i install the dependencies listed in the setup.py file under REQUIRED_PACKAGES ? if so, how do i go about it?"
22603,[BUG] possible memory corruption when multiplying large matices,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
binary (through pip)
- **TensorFlow version (use command below)**:
1.11.0
- **Python version**:
2.7.12
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
9.1.85
- **GPU model and memory**:
TITAN Xp 12GB
- **Exact command to reproduce**:

more system information in [tf_env.txt](https://github.com/tensorflow/tensorflow/files/2429099/tf_env.txt)


### Describe the problem
For large matrix multiplications, part of the result is not correctly computed or its memory is partially overwritten. Not sure what exactly the problem is. I created as small as possible reproducible code that can be run and visualise the problem. 

Here are also visual outputs that I run. The result should be array of constant value 3.
with tf.tile:
sampling = 6
![sampling = 6](https://user-images.githubusercontent.com/8818326/46223432-45741880-c34b-11e8-8d0e-88c4a4682073.png)
sampling = 8
![sampling = 8](https://user-images.githubusercontent.com/8818326/46223453-591f7f00-c34b-11e8-9fcc-6edf42434fc3.png)
with broadcasting:
sampling = 6
![sampling = 6](https://user-images.githubusercontent.com/8818326/46223487-73595d00-c34b-11e8-9314-179848bb2f96.png)
sampling = 8
![sampling = 8](https://user-images.githubusercontent.com/8818326/46223500-7b190180-c34b-11e8-9122-31fb2e0991c7.png)

### Source code / logs
by varying the sampling variable from 6 to 12 (OOM) you can observe the behaviour by plotting the result (as done in the code)

```python
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable


# On Nvidia Titan XP 12GB:
# 1) MEMORY BUG - for shape [960, 480] and sampling > 6 the output 
#                 variable is partially overwritten (see figure) 
# 2) OOM error when sampling >= 12
sampling = 8  
shape = [480, 960]

# ""random"" input vectors
i1 = np.ones(shape=(shape[0]*shape[1], 1, 3), dtype=np.float)
i2 = np.ones(shape=(1, sampling**3, 3), dtype=np.float)

input1 = tf.placeholder(shape=(i1.shape[0], i1.shape[1], i1.shape[2]), dtype=tf.float32)
input2 = tf.placeholder(shape=(i2.shape[0], i2.shape[1], i2.shape[2]), dtype=tf.float32)

# Operation v1:  
# tile before multiplying to have same dimensions
# vectors (batch size, shape[0]*shape[1], sampling^2 , 3) and
# to get (after summation) vector (barch size, shape[0]*shape[1], sampling^3)
i1_tile = tf.tile(input1, (1, sampling**3, 1))
i2_tile = tf.tile(input2, (shape[0]*shape[1], 1, 1))
res1 = tf.reduce_sum(i1_tile * i2_tile, axis=-1)

# Operation v2:  
# use broadcasting to multiply 
# vectors i1 = (batch size, shape[0]*shape[1],     1     , 3) and
#         i2 = (batch size,         1        , sampling^3, 3) 
# to get (after summation) vector (barch size, shape[0]*shape[1], sampling^3)
res2 = tf.reduce_sum(input1*input2, axis=-1)

res = res1 #res2

with tf.Session() as sess:
    pred_out = sess.run(res, feed_dict={input1: i1, input2: i2})

# result should be image of size shape with all values equal to 3
fig = plt.figure()
pred_out = np.reshape(np.mean(pred_out, axis=-1), (shape[0], shape[1]))
imgplot = plt.imshow(pred_out)
imgplot.set_cmap('jet')
divider = make_axes_locatable(plt.gca())
cax1 = divider.append_axes(""right"", size=""5%"", pad=0.05)
fig.colorbar(imgplot, cax=cax1)
plt.clim(0, 3.1)

plt.savefig(""result.png"", dpi=100)

```

**Console output of the run of the script:**
/home/mifs/tv278/.virtualenvs/tf-keras/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
2018-09-28 18:22:45.596062: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2018-09-28 18:22:45.865097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:17:00.0
totalMemory: 11.91GiB freeMemory: 11.74GiB
2018-09-28 18:22:45.865123: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2018-09-28 18:22:46.099514: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-09-28 18:22:46.099541: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2018-09-28 18:22:46.099546: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2018-09-28 18:22:46.099791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11359 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:17:00.0, compute capability: 6.1)



"
22602,[BUG] Tensorflow branch 1.11 test. ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 LTS
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.11 (git branch 1.11)
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.15.2
- **GCC/Compiler version (if compiling from source)**: 
- **CUDA/cuDNN version**: 9.0/7.3
- **GPU model and memory**: Titan Xp
- **Exact command to reproduce**: ```bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/contrib/lite/...```

### Describe the problem
Found a missing imports (pandas and dask) in file tensorflow/tensorflow/contrib/learn/python/learn/learn_io/data_feeder_test.py at line 295.

### Source code / logs
Error1 log:
```
//tensorflow/contrib/learn:data_feeder_test                              FAILED in 1.9s
======================================================================
ERROR: test_dask_data_feeder (__main__.SetupPredictDataFeederTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/nikita/.cache/bazel/_bazel_nikita/992bfa23a6d33fa546f39f500a08968c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/data_feeder_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/learn_io/data_feeder_test.py"", line 295, in test_dask_data_feeder
    x = pd.DataFrame(
NameError: global name 'pandas' is not defined
```

Solution1:
in file tensorflow/tensorflow/contrib/learn/python/learn/learn_io/data_feeder_test.py add:
```
import pandas as pd
import dask.dataframe as dd
```

Error2 log:
```
exec ${PAGER:-/usr/bin/less} ""$0"" || exit 1
Executing tests from //tensorflow/contrib/learn:data_feeder_test
-----------------------------------------------------------------------------
WARNING:tensorflow:From /home/nikita/.cache/bazel/_bazel_nikita/992bfa23a6d33fa546f39f500a08968c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/data_feeder_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/learn_io/data_feeder_test.py:307: __init__ (from tensorflow.contrib.learn.python.learn.learn_io.data_feeder) is deprecated and will be removed in a future version.
Instructions for updating:
Please feed input to tf.data to support dask.
WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.
EWARNING:tensorflow:From /home/nikita/.cache/bazel/_bazel_nikita/992bfa23a6d33fa546f39f500a08968c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/data_feeder_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/learn_io/data_feeder_test.py:246: __init__ (from tensorflow.contrib.learn.python.learn.learn_io.data_feeder) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
WARNING:tensorflow:From /home/nikita/.cache/bazel/_bazel_nikita/992bfa23a6d33fa546f39f500a08968c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/data_feeder_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py:340: check_array (from tensorflow.contrib.learn.python.learn.learn_io.data_feeder) is deprecated and will be removed in a future version.
Instructions for updating:
Please convert numpy dtypes explicitly.
...2018-09-28 16:50:14.407888: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX
....WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.
WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.
WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.
.............WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.
E.......WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.
WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.
WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.
..........WARNING:tensorflow:From /home/nikita/.cache/bazel/_bazel_nikita/992bfa23a6d33fa546f39f500a08968c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/data_feeder_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/learn_io/data_feeder_test.py:363: setup_predict_data_feeder (from tensorflow.contrib.learn.python.learn.learn_io.data_feeder) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
WARNING:tensorflow:From /home/nikita/.cache/bazel/_bazel_nikita/992bfa23a6d33fa546f39f500a08968c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/data_feeder_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py:215: extract_dask_data (from tensorflow.contrib.learn.python.learn.learn_io.dask_io) is deprecated and will be removed in a future version.
Instructions for updating:
Please feed input to tf.data to support dask.
WARNING:tensorflow:From /home/nikita/.cache/bazel/_bazel_nikita/992bfa23a6d33fa546f39f500a08968c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/data_feeder_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py:217: extract_pandas_data (from tensorflow.contrib.learn.python.learn.learn_io.pandas_io) is deprecated and will be removed in a future version.
Instructions for updating:
Please access pandas data directly.
....
======================================================================
ERROR: test_dask_data_feeder (__main__.DataFeederTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/nikita/.cache/bazel/_bazel_nikita/992bfa23a6d33fa546f39f500a08968c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/data_feeder_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/learn_io/data_feeder_test.py"", line 307, in test_dask_data_feeder
    df = data_feeder.DaskDataFeeder(x, y, n_classes=2, batch_size=2)
  File ""/home/nikita/.cache/bazel/_bazel_nikita/992bfa23a6d33fa546f39f500a08968c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/data_feeder_test.runfiles/org_tensorflow/tensorflow/python/util/deprecation.py"", line 306, in new_func
    return func(*args, **kwargs)
  File ""/home/nikita/.cache/bazel/_bazel_nikita/992bfa23a6d33fa546f39f500a08968c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/data_feeder_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py"", line 872, in __init__
    self._output_dtype = _check_dtype(self._y.dtypes[self._y_columns])
  File ""/home/nikita/.cache/bazel/_bazel_nikita/992bfa23a6d33fa546f39f500a08968c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/data_feeder_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py"", line 281, in _check_dtype
    if dtypes.as_dtype(dtype) == dtypes.float64:
  File ""/home/nikita/.cache/bazel/_bazel_nikita/992bfa23a6d33fa546f39f500a08968c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/data_feeder_test.runfiles/org_tensorflow/tensorflow/python/framework/dtypes.py"", line 681, in as_dtype
    return _INTERN_TABLE[type_value]
  File ""/usr/local/lib/python2.7/dist-packages/pandas/core/generic.py"", line 1045, in __hash__
    ' hashed'.format(self.__class__.__name__))
TypeError: 'Series' objects are mutable, thus they cannot be hashed

======================================================================
ERROR: test_dask_data_feeder (__main__.SetupPredictDataFeederTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/nikita/.cache/bazel/_bazel_nikita/992bfa23a6d33fa546f39f500a08968c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/data_feeder_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/learn_io/data_feeder_test.py"", line 307, in test_dask_data_feeder
    df = data_feeder.DaskDataFeeder(x, y, n_classes=2, batch_size=2)
  File ""/home/nikita/.cache/bazel/_bazel_nikita/992bfa23a6d33fa546f39f500a08968c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/data_feeder_test.runfiles/org_tensorflow/tensorflow/python/util/deprecation.py"", line 306, in new_func
    return func(*args, **kwargs)
  File ""/home/nikita/.cache/bazel/_bazel_nikita/992bfa23a6d33fa546f39f500a08968c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/data_feeder_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py"", line 872, in __init__
    self._output_dtype = _check_dtype(self._y.dtypes[self._y_columns])
  File ""/home/nikita/.cache/bazel/_bazel_nikita/992bfa23a6d33fa546f39f500a08968c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/data_feeder_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py"", line 281, in _check_dtype
    if dtypes.as_dtype(dtype) == dtypes.float64:
  File ""/home/nikita/.cache/bazel/_bazel_nikita/992bfa23a6d33fa546f39f500a08968c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/data_feeder_test.runfiles/org_tensorflow/tensorflow/python/framework/dtypes.py"", line 681, in as_dtype
    return _INTERN_TABLE[type_value]
  File ""/usr/local/lib/python2.7/dist-packages/pandas/core/generic.py"", line 1045, in __hash__
    ' hashed'.format(self.__class__.__name__))
TypeError: 'Series' objects are mutable, thus they cannot be hashed

Ran 43 tests in 0.184s

FAILED (errors=2)
tf.estimator package not installed.
```


With second error I don`t know what to do."
22601,Tensorflow 1.11.0 incompatible with keras2.2.2?,"- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
No
- **TensorFlow installed from (source or binary)**:
Source

- **TensorFlow version (use command below)**:
1.11.0

- **Python version**:
3.5

- **Bazel version (if compiling from source)**:
0.17.2

- **GCC/Compiler version (if compiling from source)**:
5.4.0

- **CUDA/cuDNN version**:
CUDA 10
cuDNN 7.3

- **GPU model and memory**:
GTX1070

- **Exact command to reproduce**:
```
pip3 --no-cache-dir install ./tensorflow-1.11.0-cp35-cp35m-linux_x86_64.whl --user
```

Then  pip complains that
```
keras 2.2.2 has requirement keras-applications==1.0.4, but you'll have keras-applications 1.0.5 which is incompatible.
keras 2.2.2 has requirement keras-preprocessing==1.0.2, but you'll have keras-preprocessing 1.0.3 which is incompatible
```
But keras 2.2.2 is already the newest and it requires  keras-applications 1.0.4 and  keras-preprocessing 1.0.2.

Why does tensorflow require incompatible versions of keras-applications and keras-preprocessing? 

I reinstalled keras and it removed keras-applications 1.0.5 and and keras-processing 1.0.3 and installed 
 keras-applications 1.0.4 and keras-preprocessing 1.0.2 instead . I did a few tests with tensorflow  and it worked without problem.

If In tensorflow/tools/pip_package/setup.py change line 54 and 55 to

```
'keras_applications >= 1.0.4',
'keras_preprocessing >= 1.0.2',
```
before bazel build pip package then I think pip3 install would happily go along without complaint.   Are these incompatible requirements really necessary??


"
22598,Empty tfrecord in gcs would fail the TFRecordDataset.read,"The behaviour of gcs(google cloud file storage) file system is different than local file system. 

as `tf.gfile.Open('local_empty.tfrecord').read(100000000) == ''`
while `tf.gfile.Open('gcs_empty_file.tfrecord').read(1000000)=='Out[11]: ""<?xml version='1.0' encoding='UTF-8'?><Error><Code>InvalidRange</Code><Message>The requested range cannot be satisfied.</Message><Details>bytes=some number</Details></Error>""'`

so when we use `tf.data.TFRecordDataset` to process the tfrecord in the gcs, there would be 
`DataLossError`

such as here https://github.com/tensorflow/tensorflow/issues/13463
---

"
22597,Feature request: Documentation tf.keras.applications missing for TF 1.11,"
### Describe the problem
The documentation for tf.keras.applications is missing for version 1.11 and I am getting a page not found error (see below image). Documentation is available for 1.10. While both have the same documentation, 1.11 comes with `mobilenet_V2` which was not available in 1.10.

### Source code / logs
![image](https://user-images.githubusercontent.com/25213730/46215172-50ef2180-c302-11e8-9cd4-8e12ba62c1e1.png)
"
22596,TFGAN support in tensorflow 2.0,"Hi everyone,

as written in the [roadmap](https://www.tensorflow.org/community/roadmap) `tf.contrib` will be deprecated/removed and some parts of it will be merged inside tensorflow core or moved to a separate repository and some removed.

Since in my team we're currently using tensorflow and TFGAN (and we like it a lot, we even hold a tutorial at EuroSciPy 2018 about GANs in Tensorflow [[1]]) we're looking closely at the Tensorflow 2.0 release and we would like to move our stack to this new version in the fastest and smoothest possible way.

In order to do this, a couple of questions (that I can't ask on StackOverflow, since you're the only one that knows what will happen on Tensorflow 2.0 and the community, outside of the maintainers, can't help a lot in this phase):

- Will TFGAN be moved to a separate project? If so, will the repo be available at the day-one?
- Since `tf.layer` will be removed in favor of `tf.keras.layers`: the implementation of our models into TFGAN, that are currently defined using `tf.layers` will work just switching to `tf.keras.layers` or have we to refactor all the existing codebase?


Thank you

[1]: https://github.com/zurutech/gans-from-theory-to-production"
22595,is it tensorflow example bug ?,"

### System information
Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): window10 and jupyter
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPhone X
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 1.11
Python version: 3.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 9.1 / 7.1
GPU model and memory:
Exact command to reproduce:


### Describe the problem
I used the tensorflow github example form [here](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/generative_examples/image_captioning_with_attention.ipynb#scrollTo=Cffg2i257iMS) change the CNN model 
`img = tf.keras.applications.inception_v3.preprocess_input(img)
--->img = tf.keras.applications.inception_resnet_v2.preprocess_input(img)`

`image_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')
---->image_model =tf.keras.applications.InceptionResNetV2(include_top=False, weights='imagenet')`

the output
```
Epoch 1 Batch 0 Loss nan
Epoch 1 Batch 100 Loss nan
Epoch 1 Batch 200 Loss nan
Epoch 1 Batch 300 Loss nan
Epoch 1 Loss nan
```
i have tried to change the learning rate and loss function,even others CNN model (vgg19,vgg16) but it was same 

### Source code / logs
the Source code is tensorflow example [from here](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/generative_examples/image_captioning_with_attention.ipynb#scrollTo=Cffg2i257iMS)
you just need to change the CNN model and wait a few minute  run it."
22594,TensorFlow 1.11.0 with GPU support now requires CUDA Compute Capability 3.7?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No, [cifar10_train.py](https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_train.py)
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Version 1803 OS Build 17134.320
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: b'v1.11.0-rc2-4-gc19e29306c' 1.11.0
- **Python version**: Python 3.6.6 :: Anaconda, Inc.
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: CUDA 9.0/cuDNN v7.3.0
- **GPU model and memory**: GeForce GTX 780 Ti 3GB
- **Exact command to reproduce**: `python cifar10_train.py`

### Describe the problem
TensorFlow 1.11.0 now requires CUDA Compute Capability 3.7. But this was not mentioned in [
GPU support](https://www.tensorflow.org/install/gpu) or [Release Notes](https://github.com/tensorflow/tensorflow/releases/tag/v1.11.0). I want to ask whether this is intentional.

### Source code / logs

```
2018-09-28 18:31:27.274915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties:
name: GeForce GTX 780 Ti major: 3 minor: 5 memoryClockRate(GHz): 1.0715
pciBusID: 0000:01:00.0
totalMemory: 3.00GiB freeMemory: 2.45GiB
2018-09-28 18:31:27.278399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1461] Ignoring visible gpu device (device: 0, name: GeForce GTX 780 Ti, pci bus id: 0000:01:00.0, compute capability: 3.5) with Cuda compute capability 3.5. The minimum required Cuda capability is 3.7.
```"
22592,Unable to convert ops to .tflite model," tf.VERSION = 1.9.0
 tf.GIT_VERSION = v1.9.0-0-g25c197e023
 tf.COMPILER_VERSION = v1.9.0-0-g25c197e023
c++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-28)
centos 7

```
Converting unsupported operation: RandomStandardNormal\n
2018-09-28 14:53:49.252475: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: SplitV\n
2018-09-28 14:53:49.252515: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Size\n
2018-09-28 14:53:49.252535: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Size\n
2018-09-28 14:53:49.252944: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Cos\n
2018-09-28 14:53:49.253000: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RFFT\n
2018-09-28 14:53:49.253019: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ComplexAbs\n
2018-09-28 14:53:49.253067: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: LinSpace\n
2018-09-28 14:53:49.253189: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: LinSpace\n
2018-09-28 14:53:49.253302: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: SplitV\n
2018-09-28 14:53:49.253618: I tensorflow/contrib/lite/toco/import_tensorflow.cc:147] Unsupported data type in placeholder op: 2\n
2018-09-28 14:53:49.253703: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ListDiff\n
2018-09-28 14:53:49.253763: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Prod\n
2018-09-28 14:53:49.253790: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Prod\n
2018-09-28 14:53:49.271001: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ListDiff\n
2018-09-28 14:53:49.271105: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Prod\n
2018-09-28 14:53:49.271137: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Prod\n
2018-09-28 14:53:49.271492: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ListDiff\n
2018-09-28 14:53:49.271561: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Prod\n
2018-09-28 14:53:49.271589: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Prod\n
2018-09-28 14:53:49.271777: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ListDiff\n
2018-09-28 14:53:49.271830: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Prod\n
2018-09-28 14:53:49.271866: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Prod\n
2018-09-28 14:53:49.274806: F tensorflow/contrib/lite/toco/tooling_util.cc:921] Check failed: array->has_shape() \n'
None
```

What can I do for these errors?

"
22590,"Fast with eager execution, but slow without it for the same code?","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.5 LTS (Xenial Xerus)
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.9.0
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0/7.0
- **GPU model and memory**: GTX GeForce 1080Ti 11MB (11177MiB)
- **Exact command to reproduce**: Source code below

### Describe the problem
The same code runs very slowly using the classic graph model (around 7 minutes) while it runs very quickly using the new eager execution mode (not profiled exactly, but around 10 seconds). The code just tries to compute the SSIM value of the two images loaded from a file.

### Source code / logs
The codes are pretty much the same, the only difference being whether `ssim_result` is run in a session or eagerly executed.

-- Code redacted by request --"
22589,Error when re-initializing the model,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: N/A
- **TensorFlow version (use command below)**: 1.11.0-rc2
- **Python version**: 3
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

Used Google Colab.

### Describe the problem
I defined a model in terms of CuDNNLSTM, then re-defined it again in terms of LSTM. The model crushed with error message suggesting that it was looking for the CUDA optimized function no matter that the second model used standard LSTM.

Below I post a reproducible code and a ~~[Colab link](https://colab.research.google.com/drive/1fhsUfxAdtG2lTTYndhcr8Y9924IPYkiJ)~~ [Colab link](https://colab.research.google.com/drive/1fhsUfxAdtG2lTTYndhcr8Y9924IPYkiJ):

```python
from __future__ import print_function

from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Embedding
from keras.layers import LSTM, CuDNNLSTM
from keras.datasets import imdb

max_features = 20000
maxlen = 80
batch_size = 32

print('Loading data...')
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)
x_train = sequence.pad_sequences(x_train, maxlen=maxlen)
x_test = sequence.pad_sequences(x_test, maxlen=maxlen)

### ========== IMPORTANT STUFF STARTS HERE ================

# define the model

model = Sequential()
model.add(Embedding(max_features, 128))
model.add(CuDNNLSTM(128))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

# it will crush because not using GPUs, so re-define it in terms of standard LSTM:

# ===============================================================
#          The below part starts in a DIFFERENT notebook cell
# ===============================================================

model = Sequential()
model.add(Embedding(max_features, 128))
model.add(LSTM(128))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

# it crushes again, with the same error

### ================ END HERE ======================

model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=15,
          validation_data=(x_test, y_test))
score, acc = model.evaluate(x_test, y_test,
                            batch_size=batch_size)
```

### Source code / logs

I get the following error on the very first iteration:

```
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1291     try:
-> 1292       return fn(*args)
   1293     except errors.OpError as e:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)
   1274       # Ensure any changes to the graph are reflected in the runtime.
-> 1275       self._extend_graph()
   1276       return self._call_tf_sessionrun(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _extend_graph(self)
   1311     with self._graph._session_run_lock():  # pylint: disable=protected-access
-> 1312       tf_session.ExtendSession(self._session)
   1313 

InvalidArgumentError: No OpKernel was registered to support Op 'CudnnRNN' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

	 [[{{node cu_dnnlstm_1/CudnnRNN}} = CudnnRNN[T=DT_FLOAT, direction=""unidirectional"", dropout=0, input_mode=""linear_input"", is_training=true, rnn_mode=""lstm"", seed=87654321, seed2=0](cu_dnnlstm_1/transpose, cu_dnnlstm_1/ExpandDims_1, cu_dnnlstm_1/ExpandDims_2, cu_dnnlstm_1/concat_1)]]

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-1-60cbd0c8ff19> in <module>()
     53           batch_size=batch_size,
     54           epochs=15,
---> 55           validation_data=(x_test, y_test))
     56 score, acc = model.evaluate(x_test, y_test,
     57                             batch_size=batch_size)

/usr/local/lib/python3.6/dist-packages/keras/models.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)
   1000                               initial_epoch=initial_epoch,
   1001                               steps_per_epoch=steps_per_epoch,
-> 1002                               validation_steps=validation_steps)
   1003 
   1004     def evaluate(self, x=None, y=None,

/usr/local/lib/python3.6/dist-packages/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)
   1703                               initial_epoch=initial_epoch,
   1704                               steps_per_epoch=steps_per_epoch,
-> 1705                               validation_steps=validation_steps)
   1706 
   1707     def evaluate(self, x=None, y=None,

/usr/local/lib/python3.6/dist-packages/keras/engine/training.py in _fit_loop(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)
   1234                         ins_batch[i] = ins_batch[i].toarray()
   1235 
-> 1236                     outs = f(ins_batch)
   1237                     if not isinstance(outs, list):
   1238                         outs = [outs]

/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py in __call__(self, inputs)
   2478             feed_dict[tensor] = value
   2479         fetches = self.outputs + [self.updates_op] + self.fetches
-> 2480         session = get_session()
   2481         updated = session.run(fetches=fetches, feed_dict=feed_dict,
   2482                               **self.session_kwargs)

/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py in get_session()
    191                 # not already marked as initialized.
    192                 is_initialized = session.run(
--> 193                     [tf.is_variable_initialized(v) for v in candidate_vars])
    194                 uninitialized_vars = []
    195                 for flag, v in zip(is_initialized, candidate_vars):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    885     try:
    886       result = self._run(None, fetches, feed_dict, options_ptr,
--> 887                          run_metadata_ptr)
    888       if run_metadata:
    889         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1108     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1109       results = self._do_run(handle, final_targets, final_fetches,
-> 1110                              feed_dict_tensor, options, run_metadata)
   1111     else:
   1112       results = []

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1284     if handle is None:
   1285       return self._do_call(_run_fn, feeds, fetches, targets, options,
-> 1286                            run_metadata)
   1287     else:
   1288       return self._do_call(_prun_fn, handle, feeds, fetches)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1306           self._config.experimental.client_handles_error_formatting):
   1307         message = error_interpolation.interpolate(message, self._graph)
-> 1308       raise type(e)(node_def, op, message)
   1309 
   1310   def _extend_graph(self):

InvalidArgumentError: No OpKernel was registered to support Op 'CudnnRNN' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

	 [[{{node cu_dnnlstm_1/CudnnRNN}} = CudnnRNN[T=DT_FLOAT, direction=""unidirectional"", dropout=0, input_mode=""linear_input"", is_training=true, rnn_mode=""lstm"", seed=87654321, seed2=0](cu_dnnlstm_1/transpose, cu_dnnlstm_1/ExpandDims_1, cu_dnnlstm_1/ExpandDims_2, cu_dnnlstm_1/concat_1)]]

Caused by op 'cu_dnnlstm_1/CudnnRNN', defined at:
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py"", line 16, in <module>
    app.launch_new_instance()
  File ""/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py"", line 477, in start
    ioloop.IOLoop.instance().start()
  File ""/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py"", line 177, in start
    super(ZMQIOLoop, self).start()
  File ""/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py"", line 888, in start
    handler_func(fd_obj, events)
  File ""/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py"", line 440, in _handle_events
    self._handle_recv()
  File ""/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py"", line 472, in _handle_recv
    self._run_callback(callback, msg)
  File ""/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py"", line 414, in _run_callback
    callback(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py"", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py"", line 235, in dispatch_shell
    handler(stream, idents, msg)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py"", line 399, in execute_request
    user_expressions, allow_stdin)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py"", line 196, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py"", line 533, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py"", line 2718, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py"", line 2822, in run_ast_nodes
    if self.run_code(code, result):
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py"", line 2882, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-1-60cbd0c8ff19>"", line 32, in <module>
    model.add(CuDNNLSTM(128))
  File ""/usr/local/lib/python3.6/dist-packages/keras/models.py"", line 522, in add
    output_tensor = layer(self.outputs[0])
  File ""/usr/local/lib/python3.6/dist-packages/keras/layers/recurrent.py"", line 500, in __call__
    return super(RNN, self).__call__(inputs, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/keras/engine/topology.py"", line 619, in __call__
    output = self.call(inputs, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/keras/layers/cudnn_recurrent.py"", line 90, in call
    output, states = self._process_batch(inputs, initial_state)
  File ""/usr/local/lib/python3.6/dist-packages/keras/layers/cudnn_recurrent.py"", line 510, in _process_batch
    is_training=True)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py"", line 1544, in __call__
    input_data, input_h, input_c, params, is_training=is_training)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py"", line 1435, in __call__
    seed=self._seed)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py"", line 922, in _cudnn_rnn
    outputs, output_h, output_c, _ = gen_cudnn_rnn_ops.cudnn_rnn(**args)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_cudnn_rnn_ops.py"", line 116, in cudnn_rnn
    is_training=is_training, name=name)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3272, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1768, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'CudnnRNN' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

	 [[{{node cu_dnnlstm_1/CudnnRNN}} = CudnnRNN[T=DT_FLOAT, direction=""unidirectional"", dropout=0, input_mode=""linear_input"", is_training=true, rnn_mode=""lstm"", seed=87654321, seed2=0](cu_dnnlstm_1/transpose, cu_dnnlstm_1/ExpandDims_1, cu_dnnlstm_1/ExpandDims_2, cu_dnnlstm_1/concat_1)]]
```
"
22586,GPU Memory Allocator OOM Before Limit Reached,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Sort of, the issue seems to be occurring with the ELMo tfhub model
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/a
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.11.0 (git version v1.11.0-0-gc19e29306c)
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/a
- **GCC/Compiler version (if compiling from source)**: N/a
- **CUDA/cuDNN version**: 9.2/7.2.1
- **GPU model and memory**: 1080 Ti (11177 MiB)
- **Exact command to reproduce**:  
```
estimator = tf.estimator.DNNClassifier(
                           hidden_units=[500, 100],
                           feature_columns=[hub.text_embedding_column(key=""cleaned"", module_spec=""https://tfhub.dev/google/elmo/2"")],
                           n_classes=2,
                           optimizer=tf.train.ProximalAdagradOptimizer(learning_rate=0.003))
estimator.train(input_fn=train_input_fn, steps=1000)
```
You'll need to provide the data, but this should reproduce the error.

### Describe the problem
According to the memory trace, only 4.47 GiB have been allocated before the OOM error. Tensorflow has 8.74 GiB available. Barely over half of the available memory is used when tensorflow throws the OOM.

### Source code / logs
```
2018-09-27 22:58:14.599824: W tensorflow/core/graph/graph_constructor.cc:1263] Importing a graph with a lower producer version 26 into an existing graph with producer version 27. Shape inference will have run different parts of the graph with different producer versions.
2018-09-27 22:58:16.160346: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-09-27 22:58:16.241355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-09-27 22:58:16.241872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683
pciBusID: 0000:01:00.0
totalMemory: 10.92GiB freeMemory: 8.74GiB
2018-09-27 22:58:16.241896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2018-09-27 22:58:16.465458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-09-27 22:58:16.465495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2018-09-27 22:58:16.465500: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2018-09-27 22:58:16.465757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8437 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-09-27 22:58:22.105125: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.97GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-09-27 22:58:22.167820: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.03GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-09-27 22:58:25.227506: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.04GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-09-27 22:58:25.293991: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.03GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-09-27 22:58:29.024543: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.21GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-09-27 22:58:31.129676: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.43GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
[I 22:58:31.786 NotebookApp] Saving file at /Tensorflow Spam Classification.ipynb
2018-09-27 22:58:41.991342: W tensorflow/core/common_runtime/bfc_allocator.cc:271] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.30GiB.  Current allocation summary follows.
2018-09-27 22:58:41.991374: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (256):   Total Chunks: 54, Chunks in use: 54. 13.5KiB allocated for chunks. 13.5KiB in use in bin. 720B client-requested in use in bin.
2018-09-27 22:58:41.991384: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (512):   Total Chunks: 8, Chunks in use: 7. 4.2KiB allocated for chunks. 3.8KiB in use in bin. 3.3KiB client-requested in use in bin.
2018-09-27 22:58:41.991391: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (1024):  Total Chunks: 3, Chunks in use: 3. 3.2KiB allocated for chunks. 3.2KiB in use in bin. 3.0KiB client-requested in use in bin.
2018-09-27 22:58:41.991399: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (2048):  Total Chunks: 7, Chunks in use: 4. 15.5KiB allocated for chunks. 8.2KiB in use in bin. 8.0KiB client-requested in use in bin.
2018-09-27 22:58:41.991405: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (4096):  Total Chunks: 6, Chunks in use: 2. 27.8KiB allocated for chunks. 8.0KiB in use in bin. 8.0KiB client-requested in use in bin.
2018-09-27 22:58:41.991412: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (8192):  Total Chunks: 7, Chunks in use: 5. 68.0KiB allocated for chunks. 48.0KiB in use in bin. 44.0KiB client-requested in use in bin.
2018-09-27 22:58:41.991419: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (16384):         Total Chunks: 2, Chunks in use: 0. 40.2KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-09-27 22:58:41.991424: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (32768):         Total Chunks: 2, Chunks in use: 1. 81.0KiB allocated for chunks. 32.0KiB in use in bin. 32.0KiB client-requested in use in bin.
2018-09-27 22:58:41.991431: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (65536):         Total Chunks: 7, Chunks in use: 4. 540.0KiB allocated for chunks. 298.0KiB in use in bin. 256.0KiB client-requested in use in bin.
2018-09-27 22:58:41.991439: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (131072):        Total Chunks: 3, Chunks in use: 1. 525.5KiB allocated for chunks. 195.5KiB in use in bin. 195.3KiB client-requested in use in bin.
2018-09-27 22:58:41.991445: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (262144):        Total Chunks: 4, Chunks in use: 4. 1.46MiB allocated for chunks. 1.46MiB in use in bin. 1.38MiB client-requested in use in bin.
2018-09-27 22:58:41.991451: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (524288):        Total Chunks: 2, Chunks in use: 0. 1.66MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-09-27 22:58:41.991457: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (1048576):       Total Chunks: 1, Chunks in use: 1. 1.95MiB allocated for chunks. 1.95MiB in use in bin. 1.95MiB client-requested in use in bin.
2018-09-27 22:58:41.991463: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (2097152):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-09-27 22:58:41.991468: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (4194304):       Total Chunks: 2, Chunks in use: 1. 8.00MiB allocated for chunks. 4.00MiB in use in bin. 4.00MiB client-requested in use in bin.
2018-09-27 22:58:41.991475: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (8388608):       Total Chunks: 4, Chunks in use: 4. 32.00MiB allocated for chunks. 32.00MiB in use in bin. 32.00MiB client-requested in use in bin.
2018-09-27 22:58:41.991481: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (16777216):      Total Chunks: 5, Chunks in use: 4. 84.00MiB allocated for chunks. 64.00MiB in use in bin. 64.00MiB client-requested in use in bin.
2018-09-27 22:58:41.991488: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (33554432):      Total Chunks: 3, Chunks in use: 1. 115.59MiB allocated for chunks. 41.80MiB in use in bin. 41.80MiB client-requested in use in bin.
2018-09-27 22:58:41.991494: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (67108864):      Total Chunks: 4, Chunks in use: 4. 256.00MiB allocated for chunks. 256.00MiB in use in bin. 256.00MiB client-requested in use in bin.
2018-09-27 22:58:41.991500: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (134217728):     Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-09-27 22:58:41.991506: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (268435456):     Total Chunks: 6, Chunks in use: 3. 7.75GiB allocated for chunks. 4.08GiB in use in bin. 4.08GiB client-requested in use in bin.
2018-09-27 22:58:41.991512: I tensorflow/core/common_runtime/bfc_allocator.cc:626] Bin for 2.30GiB was 256.00MiB, Chunk State: 
2018-09-27 22:58:41.991523: I tensorflow/core/common_runtime/bfc_allocator.cc:632]   Size: 615.25MiB | Requested Size: 225.70MiB | in_use: 0, prev:   Size: 41.80MiB | Requested Size: 41.80MiB | in_use: 1, next:   Size: 615.25MiB | Requested Size: 615.25MiB | in_use: 1
2018-09-27 22:58:41.991532: I tensorflow/core/common_runtime/bfc_allocator.cc:632]   Size: 1.17GiB | Requested Size: 1.17GiB | in_use: 0, prev:   Size: 615.25MiB | Requested Size: 615.25MiB | in_use: 1, next:   Size: 1.17GiB | Requested Size: 1.17GiB | in_use: 1
2018-09-27 22:58:41.991539: I tensorflow/core/common_runtime/bfc_allocator.cc:632]   Size: 1.90GiB | Requested Size: 2.0KiB | in_use: 0, prev:   Size: 2.30GiB | Requested Size: 2.30GiB | in_use: 1
2018-09-27 22:58:41.991545: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0000000 of size 1280
2018-09-27 22:58:41.991550: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0000500 of size 256
2018-09-27 22:58:41.991554: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0000600 of size 256
2018-09-27 22:58:41.991558: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0000700 of size 256
2018-09-27 22:58:41.991562: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0000800 of size 256
2018-09-27 22:58:41.991566: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0000900 of size 256
2018-09-27 22:58:41.991570: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0000a00 of size 256
2018-09-27 22:58:41.991575: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0000b00 of size 256
2018-09-27 22:58:41.991579: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0000c00 of size 256
2018-09-27 22:58:41.991583: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0000d00 of size 256
2018-09-27 22:58:41.991587: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0000e00 of size 256
2018-09-27 22:58:41.991591: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0000f00 of size 256
2018-09-27 22:58:41.991595: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0001000 of size 256
2018-09-27 22:58:41.991599: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0001100 of size 256
2018-09-27 22:58:41.991603: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0001200 of size 256
2018-09-27 22:58:41.991607: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0001300 of size 256
2018-09-27 22:58:41.991611: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0001400 of size 256
2018-09-27 22:58:41.991615: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0001500 of size 256
2018-09-27 22:58:41.991619: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0001600 of size 256
2018-09-27 22:58:41.991623: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0001700 of size 256
2018-09-27 22:58:41.991627: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0001800 of size 256
2018-09-27 22:58:41.991631: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0001900 of size 256
2018-09-27 22:58:41.991635: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0001a00 of size 256
2018-09-27 22:58:41.991639: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0001b00 of size 256
2018-09-27 22:58:41.991644: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0001c00 of size 512
2018-09-27 22:58:41.991648: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0001e00 of size 256
2018-09-27 22:58:41.991652: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0001f00 of size 256
2018-09-27 22:58:41.991656: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0002000 of size 256
2018-09-27 22:58:41.991660: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0002100 of size 256
2018-09-27 22:58:41.991664: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0002200 of size 256
2018-09-27 22:58:41.991668: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0002300 of size 256
2018-09-27 22:58:41.991672: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0002400 of size 256
2018-09-27 22:58:41.991676: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0002500 of size 256
2018-09-27 22:58:41.991680: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0002600 of size 256
2018-09-27 22:58:41.991684: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0002700 of size 256
2018-09-27 22:58:41.991688: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0002800 of size 256
2018-09-27 22:58:41.991692: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0002900 of size 256
2018-09-27 22:58:41.991696: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0002a00 of size 256
2018-09-27 22:58:41.991700: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0002b00 of size 256
2018-09-27 22:58:41.991704: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0002c00 of size 256
2018-09-27 22:58:41.991708: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0002d00 of size 256
2018-09-27 22:58:41.991712: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0002e00 of size 256
2018-09-27 22:58:41.991716: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0002f00 of size 256
2018-09-27 22:58:41.991720: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0003000 of size 256
2018-09-27 22:58:41.991724: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0003100 of size 256
2018-09-27 22:58:41.991727: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0003200 of size 256
2018-09-27 22:58:41.991731: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0003300 of size 512
2018-09-27 22:58:41.991735: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0003500 of size 256
2018-09-27 22:58:41.991739: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0003600 of size 256
2018-09-27 22:58:41.991743: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0003700 of size 256
2018-09-27 22:58:41.991747: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0003800 of size 768
2018-09-27 22:58:41.991752: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0003b00 of size 512
2018-09-27 22:58:41.991756: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0003d00 of size 256
2018-09-27 22:58:41.991760: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94a0003e00 of size 91136
2018-09-27 22:58:41.991764: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a001a200 of size 8192
2018-09-27 22:58:41.991768: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94a001c200 of size 16640
2018-09-27 22:58:41.991772: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0020300 of size 1024
2018-09-27 22:58:41.991779: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94a0020700 of size 24576
2018-09-27 22:58:41.991786: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0026700 of size 8192
2018-09-27 22:58:41.991793: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94a0028700 of size 50176
2018-09-27 22:58:41.991799: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0034b00 of size 256
2018-09-27 22:58:41.991805: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94a0034c00 of size 12288
2018-09-27 22:58:41.991812: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0037c00 of size 4096
2018-09-27 22:58:41.991818: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94a0038c00 of size 512
2018-09-27 22:58:41.991823: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0038e00 of size 512
2018-09-27 22:58:41.991829: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94a0039000 of size 4096
2018-09-27 22:58:41.991834: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a003a000 of size 4096
2018-09-27 22:58:41.991839: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a003b000 of size 256
2018-09-27 22:58:41.991845: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94a003b100 of size 74752
2018-09-27 22:58:41.991850: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a004d500 of size 512
2018-09-27 22:58:41.991855: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94a004d700 of size 8192
2018-09-27 22:58:41.991860: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a004f700 of size 8192
2018-09-27 22:58:41.991866: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0051700 of size 308736
2018-09-27 22:58:41.991871: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a009cd00 of size 256
2018-09-27 22:58:41.991876: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94a009ce00 of size 198656
2018-09-27 22:58:41.991882: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a00cd600 of size 2048
2018-09-27 22:58:41.991887: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94a00cde00 of size 2048
2018-09-27 22:58:41.991893: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a00ce600 of size 256
2018-09-27 22:58:41.991898: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a00ce700 of size 256
2018-09-27 22:58:41.991903: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94a00ce800 of size 4352
2018-09-27 22:58:41.991909: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a00cf900 of size 2304
2018-09-27 22:58:41.991914: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a00d0200 of size 67108864
2018-09-27 22:58:41.991920: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a40d0200 of size 12288
2018-09-27 22:58:41.991925: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a40d3200 of size 256
2018-09-27 22:58:41.991930: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a40d3300 of size 65536
2018-09-27 22:58:41.991936: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a40e3300 of size 8388608
2018-09-27 22:58:41.991941: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94a48e3300 of size 81920
2018-09-27 22:58:41.991946: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a48f7300 of size 67108864
2018-09-27 22:58:41.991952: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a88f7300 of size 65536
2018-09-27 22:58:41.991957: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a8907300 of size 8388608
2018-09-27 22:58:41.991963: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a9107300 of size 2048000
2018-09-27 22:58:41.991968: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a92fb300 of size 2048
2018-09-27 22:58:41.991973: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a92fbb00 of size 12288
2018-09-27 22:58:41.991979: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94a92feb00 of size 3328
2018-09-27 22:58:41.991985: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a92ff800 of size 32768
2018-09-27 22:58:41.991991: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a9307800 of size 301056
2018-09-27 22:58:41.991996: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a9351000 of size 92160
2018-09-27 22:58:41.992001: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a9367800 of size 512
2018-09-27 22:58:41.992007: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94a9367a00 of size 2048
2018-09-27 22:58:41.992012: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a9368200 of size 1024
2018-09-27 22:58:41.992017: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94a9368600 of size 7424
2018-09-27 22:58:41.992022: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a936a300 of size 2048
2018-09-27 22:58:41.992028: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94a936ab00 of size 4352
2018-09-27 22:58:41.992033: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a936bc00 of size 200192
2018-09-27 22:58:41.992038: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94a939ca00 of size 917504
2018-09-27 22:58:41.992044: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a947ca00 of size 81920
2018-09-27 22:58:41.992049: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94a9490a00 of size 139264
2018-09-27 22:58:41.992054: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a94b2a00 of size 458752
2018-09-27 22:58:41.992060: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a9522a00 of size 458752
2018-09-27 22:58:41.992065: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a9592a00 of size 67108864
2018-09-27 22:58:41.992070: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94ad592a00 of size 819200
2018-09-27 22:58:41.992075: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94ad65aa00 of size 8388608
2018-09-27 22:58:41.992081: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94ade5aa00 of size 67108864
2018-09-27 22:58:41.992086: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94b1e5aa00 of size 8388608
2018-09-27 22:58:41.992092: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94b265aa00 of size 4194304
2018-09-27 22:58:41.992097: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94b2a5aa00 of size 4194304
2018-09-27 22:58:41.992102: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94b2e5aa00 of size 16777216
2018-09-27 22:58:41.992108: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94b3e5aa00 of size 20971520
2018-09-27 22:58:41.992113: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94b525aa00 of size 16777216
2018-09-27 22:58:41.992118: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94b625aa00 of size 33554432
2018-09-27 22:58:41.992123: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94b825aa00 of size 16777216
2018-09-27 22:58:41.992129: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94b925aa00 of size 16777216
2018-09-27 22:58:41.992134: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94ba25aa00 of size 43827200
2018-09-27 22:58:41.992139: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94bcc26a00 of size 43827200
2018-09-27 22:58:41.992145: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94bf5f2a00 of size 645136384
2018-09-27 22:58:41.992150: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94e5d32a00 of size 645136384
2018-09-27 22:58:41.992155: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f950c472a00 of size 1262223360
2018-09-27 22:58:41.992161: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f9557832a00 of size 1262223360
2018-09-27 22:58:41.992166: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f95a2bf2a00 of size 2468347904
2018-09-27 22:58:41.992172: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f9635df2a00 of size 2038319360
2018-09-27 22:58:41.992177: I tensorflow/core/common_runtime/bfc_allocator.cc:651]      Summary of in-use Chunks by size: 
2018-09-27 22:58:41.992184: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 54 Chunks of size 256 totalling 13.5KiB
2018-09-27 22:58:41.992190: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 6 Chunks of size 512 totalling 3.0KiB
2018-09-27 22:58:41.992196: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 768 totalling 768B
2018-09-27 22:58:41.992202: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 2 Chunks of size 1024 totalling 2.0KiB
2018-09-27 22:58:41.992208: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 1280 totalling 1.2KiB
2018-09-27 22:58:41.992213: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 3 Chunks of size 2048 totalling 6.0KiB
2018-09-27 22:58:41.992219: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 2304 totalling 2.2KiB
2018-09-27 22:58:41.992225: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 2 Chunks of size 4096 totalling 8.0KiB
2018-09-27 22:58:41.992231: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 3 Chunks of size 8192 totalling 24.0KiB
2018-09-27 22:58:41.992238: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 2 Chunks of size 12288 totalling 24.0KiB
2018-09-27 22:58:41.992244: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 32768 totalling 32.0KiB
2018-09-27 22:58:41.992250: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 2 Chunks of size 65536 totalling 128.0KiB
2018-09-27 22:58:41.992256: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 81920 totalling 80.0KiB
2018-09-27 22:58:41.992262: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 92160 totalling 90.0KiB
2018-09-27 22:58:41.992268: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 200192 totalling 195.5KiB
2018-09-27 22:58:41.992274: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 301056 totalling 294.0KiB
2018-09-27 22:58:41.992280: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 308736 totalling 301.5KiB
2018-09-27 22:58:41.992286: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 2 Chunks of size 458752 totalling 896.0KiB
2018-09-27 22:58:41.992292: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 2048000 totalling 1.95MiB
2018-09-27 22:58:41.992298: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 4194304 totalling 4.00MiB
2018-09-27 22:58:41.992304: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 4 Chunks of size 8388608 totalling 32.00MiB
2018-09-27 22:58:41.992310: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 4 Chunks of size 16777216 totalling 64.00MiB
2018-09-27 22:58:41.992316: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 43827200 totalling 41.80MiB
2018-09-27 22:58:41.992322: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 4 Chunks of size 67108864 totalling 256.00MiB
2018-09-27 22:58:41.992328: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 645136384 totalling 615.25MiB
2018-09-27 22:58:41.992333: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 1262223360 totalling 1.17GiB
2018-09-27 22:58:41.992339: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 2468347904 totalling 2.30GiB
2018-09-27 22:58:41.992345: I tensorflow/core/common_runtime/bfc_allocator.cc:658] Sum Total of in-use chunks: 4.47GiB
2018-09-27 22:58:41.992352: I tensorflow/core/common_runtime/bfc_allocator.cc:660] Stats: 
Limit:                  8847717172
InUse:                  4797028096
MaxInUse:               7450003456
NumAllocs:                   54412
MaxAllocSize:           2468347904

2018-09-27 22:58:41.992366: W tensorflow/core/common_runtime/bfc_allocator.cc:275] ******_______********_____________*******************************************_______________________
2018-09-27 22:58:41.992384: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at conv_ops.cc:693 : Resource exhausted: OOM when allocating tensor with shape[128,1024,107,44] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
```"
22585,Tensorflow Class Diagram for android,"Hi,

I could not find any info on Tensorflow Class Diagram for android, which helps in understanding what are classes specific for TF android Project and Understanding the process flow in TF Android Project.

Can any one help me out with the Class Diagram of the TF  Android Project or any details about the working structure of TF in Android.

Thank You.

Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22584,MappedByteBuffer is not a valid flatbuffer model TFLITE Model_conversion_issue,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: 
- **TensorFlow version (use command below)**: 1.10
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.15.2
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: NVIDIA 12 GB
- **Exact command to reproduce**:

I have a model in which i have connected two mobilenet models . These are the following steps:

1. Train the model. - Success
2. Freeze the graph using eval graph - Success
///Defiine the graph then 
` 
  with tf.Session() as sess:

        saver.restore(sess, '/home/ubuntu/raj/research/trimap_mobnet/model.ckpt-205740')
        g = tf.get_default_graph()
        tf.contrib.quantize.create_eval_graph(input_graph=g)
        sess.run(tf.global_variables_initializer())
        saver.restore(sess, '/home/ubuntu/raj/research/trimap_mobnet/model.ckpt-205740')
        input_graph_def = graph.as_graph_def()
        output_graph_def = graph_util.convert_variables_to_constants(
            sess,
            input_graph_def,
           [_OUTPUT_NAME])        
        with tf.gfile.GFile(output_graph_name, ""wb"") as f:
            f.write(output_graph_def.SerializeToString())`
3. Toco convert -Success

tflite_convert  --output_file=./lite.tflite  --input=./frozen.pb --input_arrays=image_ph --output_arrays=SemanticPredictions --input_shapes=1,481,481,3

4 . Load the model on phone - Error

While loading the lite model on phone by running the command

`var localSource = FirebaseLocalModelSource.Builder(""my_local_model"")
               .setAssetFilePath(""lite.tflite"")
               .build()
       FirebaseModelManager.getInstance().registerLocalModelSource(localSource)
        var options = FirebaseModelOptions.Builder()
               .setLocalModelName(""my_local_model"")
               .build()
       firebaseInterpreter = FirebaseModelInterpreter.getInstance(options)
       inputOutputOptions = FirebaseModelInputOutputOptions.Builder()
               .setInputFormat(0, FirebaseModelDataType.FLOAT32, intArrayOf(1, 481, 481, 3))
               .setOutputFormat(0, FirebaseModelDataType.FLOAT32, intArrayOf(1, 481, 481, 3))
               .build()`

 i get the following error:
MappedByteBuffer is not a valid flatbuffer model

Any reason why am i getting this error or is it a tensorflow converter bug?`"
22583,Tensorflow 1.11.0 build failed with ngraph,"
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04.5

- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
No

- **TensorFlow installed from (source or binary)**:
Source

- **TensorFlow version (use command below)** :
1.11.0

- **Python version**:
3.5

- **Bazel version (if compiling from source)**:
0.17.2

- **GCC/Compiler version (if compiling from source)**:
5.4.0

- **CUDA/cuDNN version**:
cuda10

Cudnn 7.3

- **GPU model and memory**:
GTX1070

- **Exact command to reproduce**:
configure  TensorFlow with nGraph support

Then build
```
bazel build --config=opt --config=cuda --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 --copt=-mavx --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both //tensorflow/tools/pip_package:build_pip_package

```
```
ERROR: /home/bernard/.cache/bazel/_bazel_bernard/b48040ee72f8a3090b277569ef55e694/external/ngraph_tf/BUILD.bazel:19:1: C++ compilation of rule '@ngraph_tf//:ngraph_tf' failed (Exit 1)
In file included from external/org_tensorflow/tensorflow/core/framework/common_shape_fns.h:22:0,
                 from external/org_tensorflow/tensorflow/core/framework/resource_mgr.h:24,
                 from external/org_tensorflow/tensorflow/core/common_runtime/device.h:43,
                 from external/org_tensorflow/tensorflow/core/common_runtime/device_set.h:23,
                 from external/org_tensorflow/tensorflow/core/common_runtime/optimization_registry.h:25,
                 from external/ngraph_tf/src/ngraph_encapsulate_pass.cc:23:
external/org_tensorflow/tensorflow/core/util/tensor_format.h: In function 'tensorflow::TensorShape tensorflow::ShapeFromFormat(tensorflow::TensorFormat, tensorflow::int64, tensorflow::gtl::ArraySlice<long long int>, tensorflow::int64)':
external/org_tensorflow/tensorflow/core/util/tensor_format.h:501:45: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (format == FORMAT_NHWC_VECT_W && dim == spatial.size() - 1) {
                                             ^
external/ngraph_tf/src/ngraph_encapsulate_pass.cc: In member function 'tensorflow::Status ngraph_bridge::NGraphEncapsulatePass::EncapsulateFunctions(tensorflow::Graph*)':
external/ngraph_tf/src/ngraph_encapsulate_pass.cc:393:17: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
           if (i < node->requested_inputs().size()) {
                 ^
external/ngraph_tf/src/ngraph_encapsulate_pass.cc:414:42: error: 'class absl::string_view' has no member named 'ToString'
             cluster_idx, tensor_id.first.ToString(), tensor_id.second));
                                          ^
In file included from external/org_tensorflow/tensorflow/core/platform/default/logging.h:24:0,
                 from external/org_tensorflow/tensorflow/core/platform/logging.h:25,
                 from external/org_tensorflow/tensorflow/core/lib/core/refcount.h:22,
                 from external/org_tensorflow/tensorflow/core/platform/tensor_coding.h:21,
                 from external/org_tensorflow/tensorflow/core/framework/resource_handle.h:19,
                 from external/org_tensorflow/tensorflow/core/framework/allocator.h:24,
                 from external/org_tensorflow/tensorflow/core/common_runtime/device.h:35,
                 from external/org_tensorflow/tensorflow/core/common_runtime/device_set.h:23,
                 from external/org_tensorflow/tensorflow/core/common_runtime/optimization_registry.h:25,
                 from external/ngraph_tf/src/ngraph_encapsulate_pass.cc:23:
external/org_tensorflow/tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetTensorDim(tensorflow::gtl::ArraySlice<T>, tensorflow::TensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':
external/org_tensorflow/tensorflow/core/util/tensor_format.h:452:47:   required from here
external/org_tensorflow/tensorflow/core/util/tensor_format.h:420:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   CHECK(index >= 0 && index < dimension_attributes.size())
                             ^
external/org_tensorflow/tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
external/org_tensorflow/tensorflow/core/util/tensor_format.h:420:3: note: in expansion of macro 'CHECK'
   CHECK(index >= 0 && index < dimension_attributes.size())
   ^
external/org_tensorflow/tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetFilterDim(tensorflow::gtl::ArraySlice<T>, tensorflow::FilterTensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':
external/org_tensorflow/tensorflow/core/util/tensor_format.h:461:54:   required from here
external/org_tensorflow/tensorflow/core/util/tensor_format.h:435:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   CHECK(index >= 0 && index < dimension_attribute.size())
                             ^
external/org_tensorflow/tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
external/org_tensorflow/tensorflow/core/util/tensor_format.h:435:3: note: in expansion of macro 'CHECK'
   CHECK(index >= 0 && index < dimension_attribute.size())
   ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 3531.592s, Critical Path: 131.48s
INFO: 5128 processes: 5128 local.
FAILED: Build did NOT complete successfully
```"
22581,Calling `tf.image.non_max_suppression()` in parallel `tf.while_loop()` causes crash,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: YES
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 7.2 (and also Windows 10)
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary (via `pip install tensorflow-gpu`)
- **TensorFlow version (use command below)**: v1.11.0-0-gc19e29306c 1.11.0
- **Python version**: 3.6.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0/7.1.4
- **GPU model and memory**: NVIDIA GeForce GTX 1070 (8 GiB)
- **Exact command to reproduce**: `python nmsstest.py`; see below for the content of `nmstest.py`

### Describe the problem
The code in the following section, which calls `tf.image.non_max_suppression()` in `tf.while_loop()` many times, crashes abnormally.
Crash reason (and loop count) varies from time to time, for example:
* `F tensorflow/core/common_runtime/bfc_allocator.cc:384] Check failed: h != kInvalidChunkHandle` at loop `i == 140`
* `F tensorflow/core/common_runtime/bfc_allocator.cc:462] Check failed: c->in_use() && (c->bin_num == kInvalidBinNum)` at loop `i == 76`
* `Bus error` at loop `i == 34`
* heap corruption reported by libc (see the following section) at loop `i == 35`
* sometimes it crashes sliently without any logs (on Windows)

I notice that:
* it's since TensorFlow 1.11.0rc0; TF 1.10.1 was okay
* it also reproduces on Windows
* it also reproduces on CPU version (`pip install tensorflow`)
* it does **not** reproduce if `num_threads=1`; calling `tf.image.non_max_suppression()` in parallel seems the trigger
* even when I gave a fixed seed to `tf.random_uniform()`, crash cause varied

### Source code / logs

The code to reproduce the problem is as follows:
```
import tensorflow as tf

if __name__ == '__main__':
    # crashes iff num_threads > 1 on TensorFlow >= 1.11.rc0
    num_threads = 10

    top_k = 1
    batch_size = 32
    num_boxes = 10000
    boxes_op = tf.random_uniform((batch_size,num_boxes,4), 0, 1)
    scores_op = tf.random_uniform((batch_size,num_boxes), 0, 1)
    indices_op = tf.while_loop(
        (lambda b, ta: True),
        (lambda b, ta: (b+1, ta.write(b, tf.image.non_max_suppression(boxes_op[b], scores_op[b], top_k, iou_threshold=0.3)))),
        (tf.constant(0),
         tf.TensorArray(tf.int32, size=batch_size, infer_shape=False, element_shape=(top_k,))),
        back_prop=False,
        parallel_iterations=num_threads,
        maximum_iterations=batch_size)[1].stack()

    with tf.Session() as session:
        for i in range(1000):
            indices = session.run(indices_op)
            print(f'#{i}: {indices.shape}')
```

```
$ python nmstest.py
2018-09-28 13:18:42.760545: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-09-28 13:18:43.123328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-09-28 13:18:43.124208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.683
pciBusID: 0000:01:00.0
totalMemory: 7.93GiB freeMemory: 7.83GiB
2018-09-28 13:18:43.124232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2018-09-28 13:18:43.351813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-09-28 13:18:43.351847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2018-09-28 13:18:43.351859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2018-09-28 13:18:43.352078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7558 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)
#0: (32, 1)
#1: (32, 1)
#2: (32, 1)
#3: (32, 1)
#4: (32, 1)
#5: (32, 1)
#6: (32, 1)
#7: (32, 1)
#8: (32, 1)
#9: (32, 1)
#10: (32, 1)
#11: (32, 1)
#12: (32, 1)
#13: (32, 1)
#14: (32, 1)
#15: (32, 1)
#16: (32, 1)
#17: (32, 1)
#18: (32, 1)
#19: (32, 1)
#20: (32, 1)
#21: (32, 1)
#22: (32, 1)
#23: (32, 1)
#24: (32, 1)
#25: (32, 1)
#26: (32, 1)
#27: (32, 1)
#28: (32, 1)
#29: (32, 1)
#30: (32, 1)
#31: (32, 1)
#32: (32, 1)
#33: (32, 1)
#34: (32, 1)
#35: (32, 1)
*** Error in `python': malloc(): smallbin double linked list corrupted: 0x00007fb6c001c960 ***
======= Backtrace: =========
/lib64/libc.so.6(+0x7f5e4)[0x7fb9816b75e4]
/lib64/libc.so.6(+0x82d00)[0x7fb9816bad00]
/lib64/libc.so.6(__libc_malloc+0x4c)[0x7fb9816bd84c]
/home/hyabe/anaconda3/envs/work/bin/../lib/libstdc++.so.6(_Znwm+0x16)[0x7fb936087084]
/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow15OpKernelContext15allocate_outputEiRKNS_11TensorShapeEPPNS_6TensorENS_19AllocatorAttributesE+0x48)[0x7fb93f0c0608]
/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow15OpKernelContext15allocate_outputEiRKNS_11TensorShapeEPPNS_6TensorE+0xc5)[0x7fb93f0c0785]
/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow7MergeOp7ComputeEPNS_15OpKernelContextE+0xa4)[0x7fb9423854b4]
/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow13BaseGPUDevice13ComputeHelperEPNS_8OpKernelEPNS_15OpKernelContextE+0x37d)[0x7fb93f23ac9d]
/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow13BaseGPUDevice7ComputeEPNS_8OpKernelEPNS_15OpKernelContextE+0x8d)[0x7fb93f23b1dd]
/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(+0x63a4bc)[0x7fb93f2844bc]
/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(+0x63ae2a)[0x7fb93f284e2a]
/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN5Eigen26NonBlockingThreadPoolTemplIN10tensorflow6thread16EigenEnvironmentEE10WorkerLoopEi+0x21a)[0x7fb93f2f296a]
/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZNSt17_Function_handlerIFvvEZN10tensorflow6thread16EigenEnvironment12CreateThreadESt8functionIS0_EEUlvE_E9_M_invokeERKSt9_Any_data+0x32)[0x7fb93f2f1a12]
/home/hyabe/anaconda3/envs/work/bin/../lib/libstdc++.so.6(+0xb8678)[0x7fb9360a2678]
/lib64/libpthread.so.0(+0x7e25)[0x7fb981a0ce25]
/lib64/libc.so.6(clone+0x6d)[0x7fb981736bad]
======= Memory map: ========
200000000-200200000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
200200000-200400000 ---p 00000000 00:00 0 
200400000-200404000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
200404000-200600000 ---p 00000000 00:00 0 
200600000-200a00000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
200a00000-201200000 ---p 00000000 00:00 0 
201200000-201204000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
201204000-201400000 ---p 00000000 00:00 0 
201400000-201800000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
201800000-201804000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
201804000-201a00000 ---p 00000000 00:00 0 
201a00000-201e00000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
201e00000-201e04000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
201e04000-202000000 ---p 00000000 00:00 0 
202000000-202400000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
202400000-202404000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
202404000-202600000 ---p 00000000 00:00 0 
202600000-202a00000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
202a00000-202a04000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
202a04000-202c00000 ---p 00000000 00:00 0 
202c00000-203000000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
203000000-203004000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
203004000-203200000 ---p 00000000 00:00 0 
203200000-203600000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
203600000-203604000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
203604000-203800000 ---p 00000000 00:00 0 
203800000-203c00000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
203c00000-203c04000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
203c04000-203e00000 ---p 00000000 00:00 0 
203e00000-204200000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
204200000-204204000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
204204000-204400000 ---p 00000000 00:00 0 
204400000-204800000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
204800000-204804000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
204804000-204a00000 ---p 00000000 00:00 0 
204a00000-204e00000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
204e00000-204e04000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
204e04000-205000000 ---p 00000000 00:00 0 
205000000-205400000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
205400000-205404000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
205404000-205600000 ---p 00000000 00:00 0 
205600000-205a00000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
205a00000-205a04000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
205a04000-205c00000 ---p 00000000 00:00 0 
205c00000-206000000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
206000000-206004000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
206004000-206200000 ---p 00000000 00:00 0 
206200000-206600000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
206600000-206604000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
206604000-206800000 ---p 00000000 00:00 0 
206800000-206c00000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
206c00000-206e00000 ---p 00000000 00:00 0 
206e00000-207000000 rw-s 00000000 00:05 25687                            /dev/nvidiactl
207000000-300200000 ---p 00000000 00:00 0 
10000000000-10204000000 ---p 00000000 00:00 0 
5596eb5dd000-5596eb89c000 r-xp 00000000 fd:02 86155766                   /home/hyabe/anaconda3/envs/work/bin/python3.6
5596eba9c000-5596eba9f000 r--p 002bf000 fd:02 86155766                   /home/hyabe/anaconda3/envs/work/bin/python3.6
5596eba9f000-5596ebb02000 rw-p 002c2000 fd:02 86155766                   /home/hyabe/anaconda3/envs/work/bin/python3.6
5596ebb02000-5596ebb33000 rw-p 00000000 00:00 0 
5596eda67000-5597016ee000 rw-p 00000000 00:00 0                          [heap]
7fb698000000-7fb69803a000 rw-p 00000000 00:00 0 
7fb69803a000-7fb69c000000 ---p 00000000 00:00 0 
7fb69c000000-7fb69c021000 rw-p 00000000 00:00 0 
7fb69c021000-7fb6a0000000 ---p 00000000 00:00 0 
7fb6a0000000-7fb6a0036000 rw-p 00000000 00:00 0 
7fb6a0036000-7fb6a4000000 ---p 00000000 00:00 0 
7fb6a4000000-7fb6a4049000 rw-p 00000000 00:00 0 
7fb6a4049000-7fb6a8000000 ---p 00000000 00:00 0 
7fb6a8000000-7fb6a8039000 rw-p 00000000 00:00 0 
7fb6a8039000-7fb6ac000000 ---p 00000000 00:00 0 
7fb6b0000000-7fb6b0036000 rw-p 00000000 00:00 0 
7fb6b0036000-7fb6b4000000 ---p 00000000 00:00 0 
7fb6b4000000-7fb6b403b000 rw-p 00000000 00:00 0 
7fb6b403b000-7fb6b8000000 ---p 00000000 00:00 0 
7fb6b8000000-7fb6b8037000 rw-p 00000000 00:00 0 
7fb6b8037000-7fb6bc000000 ---p 00000000 00:00 0 
7fb6c0000000-7fb6c003c000 rw-p 00000000 00:00 0 
7fb6c003c000-7fb6c4000000 ---p 00000000 00:00 0 
7fb6c5400000-7fb6c5a00000 rw-p 00000000 00:00 0 
7fb6c5ad4000-7fb6c5ad5000 ---p 00000000 00:00 0 
7fb6c5ad5000-7fb6c6326000 rw-p 00000000 00:00 0 
7fb6c6326000-7fb6c6327000 ---p 00000000 00:00 0 
7fb6c6327000-7fb6c6b78000 rw-p 00000000 00:00 0 
7fb6c6b78000-7fb6c6b79000 ---p 00000000 00:00 0 
7fb6c6b79000-7fb6d6000000 rw-p 00000000 00:00 0 
7fb6d6000000-7fb8ae800000 ---p 00000000 00:00 0 
7fb8ae800000-7fb8aea00000 rw-s 00000000 00:04 31968                      /dev/zero (deleted)
7fb8aea00000-7fb8aec00000 rw-s 00000000 00:04 29039                      /dev/zero (deleted)
7fb8aec00000-7fb8b0000000 ---p 00000000 00:00 0 
7fb8b0000000-7fb8b0021000 rw-p 00000000 00:00 0 
7fb8b0021000-7fb8b4000000 ---p 00000000 00:00 0 
7fb8b4000000-7fb8b4021000 rw-p 00000000 00:00 0 
7fb8b4021000-7fb8b8000000 ---p 00000000 00:00 0 
7fb8b8000000-7fb8bc000000 ---p 00000000 00:00 0 
7fb8bc000000-7fb8bc021000 rw-p 00000000 00:00 0 
7fb8bc021000-7fb8c0000000 ---p 00000000 00:00 0 
7fb8c0000000-7fb8c0001000 rw-s 00000000 00:05 25688                      /dev/nvidia0
7fb8c0001000-7fb8c0002000 rw-s 00000000 00:05 25688                      /dev/nvidia0
7fb8c0002000-7fb8c0003000 rw-s 00000000 00:05 25688                      /dev/nvidia0
7fb8c0003000-7fb8c0004000 rw-s 00000000 00:05 25688                      /dev/nvidia0
7fb8c0004000-7fb8c0005000 rw-s 00000000 00:05 25688                      /dev/nvidia0
7fb8c0005000-7fb8c0006000 rw-s 00000000 00:05 25688                      /dev/nvidia0
7fb8c0006000-7fb8c0007000 rw-s 00000000 00:05 25688                      /dev/nvidia0
7fb8c0007000-7fb8c0008000 rw-s 00000000 00:05 25688                      /dev/nvidia0
7fb8c0008000-7fb8c0009000 rw-s 00000000 00:05 25688                      /dev/nvidia0
7fb8c0009000-7fb8c000a000 rw-s 00000000 00:05 25688                      /dev/nvidia0
7fb8c000a000-7fb8c000b000 rw-s 00000000 00:05 25688                      /dev/nvidia0
7fb8c000b000-7fb8c000c000 rw-s 00000000 00:05 25688                      /dev/nvidia0
7fb8c000c000-7fb8c000d000 rw-s 00000000 00:05 25688                      /dev/nvidia0
7fb8c000d000-7fb8c000e000 rw-s 00000000 00:05 25688                      /dev/nvidia0
7fb8c000e000-7fb8c000f000 rw-s 00000000 00:05 25688                      /dev/nvidia0
7fb8c000f000-7fb8c0010000 rw-s 00000000 00:05 25688                      /dev/nvidia0
7fb8c0010000-7fb8d0000000 ---p 00000000 00:00 0 
7fb8d0000000-7fb8d0021000 rw-p 00000000 00:00 0 
7fb8d0021000-7fb8d4000000 ---p 00000000 00:00 0 
7fb8d4200000-7fb8d4600000 rw-p 00000000 00:00 0 
7fb8d470a000-7fb8d470b000 ---p 00000000 00:00 0 
7fb8d470b000-7fb8d4f5c000 rw-p 00000000 00:00 0 
7fb8d4f5c000-7fb8d4f5d000 ---p 00000000 00:00 0 
7fb8d4f5d000-7fb8d57ae000 rw-p 00000000 00:00 0 
7fb8d57ae000-7fb8d57af000 ---p 00000000 00:00 0 
7fb8d57af000-7fb8d6000000 rw-p 00000000 00:00 0 
7fb8d6000000-7fb8d6200000 ---p 00000000 00:00 0 
7fb8d6200000-7fb8d6400000 rw-s 00000000 00:04 31965                      /dev/zero (deleted)
7fb8d6400000-7fb8d6600000 rw-s 00000000 00:05 25687                      /dev/nvidiactl
7fb8d6600000-7fb8d6800000 rw-s 00000000 00:04 31966                      /dev/zero (deleted)
7fb8d6800000-7fb8d6a00000 rw-s 00000000 00:05 25687                      /dev/nvidiactl
7fb8d6a00000-7fb8d6c00000 ---p 00000000 00:00 0 
7fb8d6c00000-7fb8d6ed6000 rw-s 00000000 00:05 25687                      /dev/nvidiactl
7fb8d6ed6000-7fb8d8000000 ---p 00000000 00:00 0 
7fb8d8000000-7fb8d8021000 rw-p 00000000 00:00 0 
7fb8d8021000-7fb8dc000000 ---p 00000000 00:00 0 
7fb8dc200000-7fb8dc600000 rw-p 00000000 00:00 0 
7fb8dc70a000-7fb8dc70b000 ---p 00000000 00:00 0 
7fb8dc70b000-7fb8dcf5c000 rw-p 00000000 00:00 0 
7fb8dcf5c000-7fb8dcf5d000 ---p 00000000 00:00 0 
7fb8dcf5d000-7fb8dd7ae000 rw-p 00000000 00:00 0 
7fb8dd7ae000-7fb8dd7af000 ---p 00000000 00:00 0 
7fb8dd7af000-7fb8de000000 rw-p 00000000 00:00 0 
7fb8de000000-7fb8e4000000 ---p 00000000 00:00 0 
7fb8e4000000-7fb8e4021000 rw-p 00000000 00:00 0 
7fb8e4021000-7fb8e8000000 ---p 00000000 00:00 0 
7fb8e8000000-7fb8e8021000 rw-p 00000000 00:00 0 
7fb8e8021000-7fb8ec000000 ---p 00000000 00:00 0 
7fb8ec000000-7fb8ec021000 rw-p 00000000 00:00 0 
7fb8ec021000-7fb8f0000000 ---p 00000000 00:00 0 
7fb8f0000000-7fb8f0021000 rw-p 00000000 00:00 0 
7fb8f0021000-7fb8f4000000 ---p 00000000 00:00 0 
7fb8f4200000-7fb8f4400000 rw-p 00000000 00:00 0 
7fb8f45c2000-7fb8f45c3000 ---p 00000000 00:00 0 
7fb8f45c3000-7fb8f4e14000 rw-p 00000000 00:00 0 
7fb8f4e14000-7fb8f4e15000 ---p 00000000 00:00 0 
7fb8f4e15000-7fb8f5666000 rw-p 00000000 00:00 0 
7fb8f5666000-7fb8f5667000 ---p 00000000 00:00 0 
7fb8f5667000-7fb8f5eb8000 rw-p 00000000 00:00 0 
7fb8f5eb8000-7fb8f5eb9000 ---p 00000000 00:00 0 
7fb8f5eb9000-7fb8f670a000 rw-p 00000000 00:00 0 
7fb8f670a000-7fb8f670b000 ---p 00000000 00:00 0 
7fb8f670b000-7fb8f6f5c000 rw-p 00000000 00:00 0 
7fb8f6f5c000-7fb8f6f5d000 ---p 00000000 00:00 0 
7fb8f6f5d000-7fb8f77ae000 rw-p 00000000 00:00 0 
7fb8f77ae000-7fb8f77af00Aborted
```"
22579,save tensorflow  model to .pb file ,"Hi I wonder , how do I save model to .pb file to retrain model  ? Below ,there is sample model for experiment. Thanks for helping

#######################################################################
x=tf.placeholder(tf.float32,shape=[None,2],name=""x"")
y=tf.placeholder(tf.float32,shape=[None,1],name=""y"")

with tf.name_scope(""dnn""):
      layer1=tf.layers.dense(x,15,activation=tf.nn.relu,
                           kernel_initializer= tf.initializers.truncated_normal(),name=""layer1"")
    layer2=tf.layers.dense(layer1,20,activation=tf.nn.relu,
                           kernel_initializer= tf.initializers.truncated_normal(),name=""layer2"")
    logits=tf.layers.dense(layer2,1,
                           kernel_initializer= tf.initializers.truncated_normal(),name=""logits"")
    out=tf.nn.sigmoid(logits,name=""out"")
    
with tf.name_scope(""train""):
    loss=tf.losses.sigmoid_cross_entropy(multi_class_labels=y,logits=logits)
    train_op=tf.train.AdamOptimizer().minimize(loss)

with tf.name_scope(""eval""):
    correct=tf.equal(tf.round(out),y)
    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))

saver=tf.train.Saver()

epochs=2000
with tf.Session( ) as sess:
    sess.run(tf.global_variables_initializer())
    for epoch in range(epochs+1):
        sess.run(train_op,feed_dict={x:trainx,y:trainy})
        if epoch%1000==0:
            l,a=sess.run([loss,accuracy],feed_dict={x:trainx,y:trainy})
            print(""Epoch {} | Accuracy : {:.2f} | Loss :{:.2f}"".format(epoch,a,l))
            saver.save(sess,""./moon_model"")
    tf.train.write_graph(sess.graph.as_graph_def(), '.', 'moon_model.pbtxt', as_text=True)

#########################################################################3

"
22578,Tensorflow top 1 prediction drop on the /example/label_image/label_image.py  with mobile net . ,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NO
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.9.0
- **Python version**:  2.7
- **Bazel version (if compiling from source)**: 0.17.2
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:  No
- **GPU model and memory**: No
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I compared official code for mobilenet v1 on both python and C++ and find some performance drop.  
model: https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet
I use mobilenet_v1_224

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.


The model and input data are **EXACTLY SAME** 

I doubt the problem is the mean and std in label_image.py but I couldn't make the prediction as same as the above inference. 

The label_image.py comes from tensorflow/example/label_image/label_image.py. Just change inception to mobilenet. 

```
import tensorflow as tf
tf_model_path = './frozen_graph.pb'
with open(tf_model_path, 'rb') as f:
    serialized = f.read()
tf.reset_default_graph()
original_gdef = tf.GraphDef()
original_gdef.ParseFromString(serialized)

import numpy as np
import PIL
import requests
from io import BytesIO
from matplotlib.pyplot import imshow
img_url = 'https://upload.wikimedia.org/wikipedia/commons/9/93/Golden_Retriever_Carlos_%2810581910556%29.jpg'
response = requests.get(img_url)
%matplotlib inline
img = PIL.Image.open(BytesIO(response.content))
imshow(np.asarray(img))
img = img.resize([224,224], PIL.Image.ANTIALIAS)

img_np = np.array(img).astype(np.float32)
print( 'image shape:', img_np.shape)
print( 'first few values: ', img_np.flatten()[0:4], 'max value: ', np.amax(img_np))
img_tf = np.expand_dims(img_np, axis = 0) #now shape is [1,224,224,3] as required by TF
img_tf = (2.0/255.0) * img_tf - 1


tf_input_name = 'input:0'
tf_output_name = 'MobilenetV1/Predictions/Reshape_1:0'
with tf.Session(graph = g) as sess:
    tf_out = sess.run(tf_output_name, 
                      feed_dict={tf_input_name: img_tf})
tf_out = tf_out.flatten()    
idx = np.argmax(tf_out)
label_file = 'labels.txt' 
with open(label_file) as f:
    labels = f.readlines()
    
print('\n')
print(""TF prediction class = {}, probability = {}"".format(labels[idx],
                                            str(tf_out[idx])))
```

it shows: 
image shape: (224, 224, 3)
first few values:  [39. 33. 18. 42.] max value:  255.0
TF prediction class = 208:golden retriever
, probability = 0.9581951

#while for when  I use tensorflow/example/label_image/label_image.py the result is different:

```

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import argparse

import numpy as np
import tensorflow as tf


def load_graph(model_file):
  graph = tf.Graph()
  graph_def = tf.GraphDef()

  with open(model_file, ""rb"") as f:
    graph_def.ParseFromString(f.read())
  with graph.as_default():
    tf.import_graph_def(graph_def)

  return graph


def read_tensor_from_image_file(file_name,
                                input_height=224,
                                input_width=224,
                                input_mean=0,
                                input_std=255):
  input_name = ""file_reader""
  output_name = ""normalized""
  file_reader = tf.read_file(file_name, input_name)
  if file_name.endswith("".png""):
    image_reader = tf.image.decode_png(
        file_reader, channels=3, name=""png_reader"")
  elif file_name.endswith("".gif""):
    image_reader = tf.squeeze(
        tf.image.decode_gif(file_reader, name=""gif_reader""))
  elif file_name.endswith("".bmp""):
    image_reader = tf.image.decode_bmp(file_reader, name=""bmp_reader"")
  else:
    image_reader = tf.image.decode_jpeg(
        file_reader, channels=3, name=""jpeg_reader"")
  float_caster = tf.cast(image_reader, tf.float32)
  dims_expander = tf.expand_dims(float_caster, 0)
  resized = tf.image.resize_bilinear(dims_expander, [input_height, input_width])
  normalized = tf.subtract(tf.divide(resized,[input_std]),[input_mean])
  sess = tf.Session()
  result = sess.run(normalized)

  return result


def load_labels(label_file):
  label = []
  proto_as_ascii_lines = tf.gfile.GFile(label_file).readlines()
  for l in proto_as_ascii_lines:
    label.append(l.rstrip())
  return label


if __name__ == ""__main__"":
  file_name = ""./data/test.jpg""
  model_file = \
    ""./data/frozen_graph.pb""
  label_file = ""./data/labels.txt""
  input_height = 224
  input_width = 224
  input_mean = 1
  input_std = 255/2
  input_layer = ""input""
  output_layer = ""MobilenetV1/Predictions/Reshape_1""

  parser = argparse.ArgumentParser()
  parser.add_argument(""--image"", help=""image to be processed"")
  parser.add_argument(""--graph"", help=""graph/model to be executed"")
  parser.add_argument(""--labels"", help=""name of file containing labels"")
  parser.add_argument(""--input_height"", type=int, help=""input height"")
  parser.add_argument(""--input_width"", type=int, help=""input width"")
  parser.add_argument(""--input_mean"", type=int, help=""input mean"")
  parser.add_argument(""--input_std"", type=int, help=""input std"")
  parser.add_argument(""--input_layer"", help=""name of input layer"")
  parser.add_argument(""--output_layer"", help=""name of output layer"")
  args = parser.parse_args()

  if args.graph:
    model_file = args.graph
  if args.image:
    file_name = args.image
  if args.labels:
    label_file = args.labels
  if args.input_height:
    input_height = args.input_height
  if args.input_width:
    input_width = args.input_width
  if args.input_mean:
    input_mean = args.input_mean
  if args.input_std:
    input_std = args.input_std
  if args.input_layer:
    input_layer = args.input_layer
  if args.output_layer:
    output_layer = args.output_layer

  graph = load_graph(model_file)
  t = read_tensor_from_image_file(
      file_name,
      input_height=input_height,
      input_width=input_width,
      input_mean=input_mean,
      input_std=input_std)

  input_name = ""import/"" + input_layer
  output_name = ""import/"" + output_layer
  input_operation = graph.get_operation_by_name(input_name)
  output_operation = graph.get_operation_by_name(output_name)

  with tf.Session(graph=graph) as sess:
    results = sess.run(output_operation.outputs[0], {
        input_operation.outputs[0]: t
    })
  results = np.squeeze(results)

  top_k = results.argsort()[-5:][::-1]
  labels = load_labels(label_file)
  for i in top_k:
    print(labels[i], results[i])
```
`208:golden retriever 0.83966106
209:Labrador retriever 0.033579364
213:English setter 0.024712995
274:dingo, warrigal, warragal, Canis dingo 0.01596725
217:clumber, clumber spaniel 0.010608253

if I change std =255 mean=0 it shows 208:golden retriever 0.7953789
213:English setter 0.03927898
217:clumber, clumber spaniel 0.028547956
209:Labrador retriever 0.025497263
221:Sussex spaniel 0.013308002"
22577,[Bug] TensorRT conversion error,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:No
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**: 1.11.0 dev build from dockerhub, last commit c19e29306ce1777456b2dbb3a14f511edf7883a8
- **Python version**:2.7
- **Bazel version (if compiling from source)**:0.15.0
- **GCC/Compiler version (if compiling from source)**:5.4.0
- **CUDA/cuDNN version**: 9.0/7.0.5
- **GPU model and memory**: GTX 1080 with 8G memory
- **Exact command to reproduce**: python minimal_graph.py 

### Describe the problem
Met with another conversion error when using latest tensorRT conversion pipeline. After localization of the problem, it seems that the problem is caused by NHWC -> NCHW conversion: If we have a constant of shape (C) (usually bias add term) added after the convolution, after the NHWC -> NCHW conversion, we should expect it to be of shape (C, 1, 1) in order to be broadcast-able, yet current pipeline failed to do so.
My guess may be incorrect, hopefully this would be helpful though.

@aaroey 

### Source code / logs
Test case:
```
 import numpy as np
 import tensorflow as tf
 from tensorflow.contrib import tensorrt as trt
 
 def build_graph_from_def(graph_def, input_nodes, output_nodes):
     """"""
     build the actual graph from definition
     """"""
     tf.reset_default_graph()
     graph = tf.Graph()
     with graph.as_default():
         return_tensors = [operation_name + "":0"" for operation_name in input_nodes + output_nodes]
         tensors = tf.import_graph_def(graph_def=graph_def, name="""",
                                       return_elements=return_tensors)
         input_tensor_list = tensors[:len(input_nodes)]
         output_tensor_list = tensors[len(input_nodes):]
 
     return graph, input_tensor_list, output_tensor_list
 
 
 def main():
 
     with tf.variable_scope(""Net""):
         inp = tf.placeholder(tf.float32, shape=(1, 32, 32, 3), name=""input_image"")
         weight = tf.zeros([3,3,3,8] ,tf.float32)
         conv = tf.nn.conv2d(inp, weight, strides=[1,1,1,1], padding=""SAME"")
         bn = conv + tf.ones((8,), tf.float32, name=""add"")
         bn = tf.nn.relu(bn, name=""output"")
 
     input_nodes = [""Net/input_image""]
     output_nodes = [""Net/output""]
 
     with tf.Session() as sess:
         sess.run(tf.global_variables_initializer())
         const_graph_def = tf.graph_util.convert_variables_to_constants(
             sess, sess.graph.as_graph_def(), output_nodes)
 
     optimized_graph_def = trt.create_inference_graph(
         input_graph_def=const_graph_def,
         outputs=output_nodes,
         max_batch_size=1,
         max_workspace_size_bytes=1 << 25)
     graph, input_tensors, output_tensors = build_graph_from_def(
         optimized_graph_def, input_nodes, output_nodes)
 
     with tf.Session(graph=graph) as sess:
         output_value = sess.run(output_tensors[0], feed_dict={input_tensors[0]: np.zeros((1, 32,    32, 3))})
     print(""output:{}"".format(output_value.shape))
 
 if __name__ == ""__main__"":
     main()
```
Sample Output:
```
$ python minimal_graph2.py
2018-09-27 17:44:41.357261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-09-27 17:44:41.357805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335
pciBusID: 0000:01:00.0
totalMemory: 7.92GiB freeMemory: 5.67GiB
2018-09-27 17:44:41.357817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2018-09-27 17:44:41.632033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-09-27 17:44:41.632053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2018-09-27 17:44:41.632058: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2018-09-27 17:44:41.632205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5419 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-09-27 17:44:41.689349: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 1
2018-09-27 17:44:41.689390: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session
2018-09-27 17:44:41.689539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2018-09-27 17:44:41.689552: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-09-27 17:44:41.689557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2018-09-27 17:44:41.689562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2018-09-27 17:44:41.689643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5419 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-09-27 17:44:41.693257: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'Net/', converted to graph
2018-09-27 17:44:41.693268: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:418] Can't find a device placement for the op!
python: customWinogradConvActLayer.cpp:48: nvinfer1::cudnn::WinogradConvActLayer::WinogradConvActLayer(const string&, const EngineTensors&, const EngineTensors&, const nvinfer1::ConvolutionParameters&, bool, const std::vector<float>&): Assertion `matchNbDims(inputs[0], outputs[0]) && (inputs.size() == 1 || inputs[1].extent == outputs[0].extent)' failed.
[1]    28090 abort (core dumped)  python minimal_graph2.py

```
"
22576,Confusing information in the tf.data Op kernel files,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.11.0
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: 0.16.0
- **GCC/Compiler version (if compiling from source)**: 4.2.1
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
A bunch of files for tf.data Ops under the directory `tensorflow/core/kernels/data/` has the following doc:

```
// See documentation in ../ops/dataset_ops.cc for a high-level
// description of the following op.
```
However, I could not find this file at the referred location (`../ops/dataset_ops.cc`). Does it mean the file `tensorflow/core/ops/dataset_ops.cc`? If yes, I would like to submit a PR to update them.
"
22575,Preferred way of installing TensorRT with Tensorflow 1.11 on Ubuntu?,"Back when TensorRT support was first announced back in version 1.7, for TensorRT to work it was necessary that you install TensorRT 3.0.4 for Ubuntu 14.04, regardless of your version. For 1.11, which version of TensorRT should be used when installing the tensorflow-gpu pip package?
"
22574,1.11.0 is missing the package for Python 3.6 on macOS,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra (10.13.6)
- **TensorFlow installed from (source or binary)**: binary (attempted)
- **TensorFlow version (use command below)**: 1.11.0
- **Python version**: 3.6.4 (Anaconda)
- **Exact command to reproduce**: `pip install tensorflow==1.11.0`

### Describe the problem

1.11.0 is missing the package for Python 3.6 on macOS. See https://pypi.org/project/tensorflow/1.11.0/#files -- it has many combinations of {Windows, Linux, macOS} and Python {2.7, 3.3, 3.4, 3.5, 3.6}, but it's missing macOS + Python 3.6. (It's also missing Windows + older Python versions, but that's been that way for a while.)

### Source code / log
Here's what https://pypi.org/project/tensorflow/1.11.0/#files currently lists:


Filename, size & hash SHA256 hash help | File type | Python version | Upload date
-- | -- | -- | --
tensorflow-1.11.0-cp27-cp27m-macosx_10_11_x86_64.whl (59.4 MB)  Copy SHA256 hashSHA256 | Wheel | cp27 | Sep 27, 2018
tensorflow-1.11.0-cp27-cp27mu-manylinux1_x86_64.whl (63.0 MB)  Copy SHA256 hashSHA256 | Wheel | cp27 | Sep 27, 2018
tensorflow-1.11.0-cp33-cp33m-macosx_10_11_x86_64.whl (59.9 MB)  Copy SHA256 hashSHA256 | Wheel | cp33 | Sep 27, 2018
tensorflow-1.11.0-cp33-cp33m-manylinux1_x86_64.whl (63.5 MB)  Copy SHA256 hashSHA256 | Wheel | cp33 | Sep 27, 2018
tensorflow-1.11.0-cp34-cp34m-macosx_10_11_x86_64.whl (59.9 MB)  Copy SHA256 hashSHA256 | Wheel | cp34 | Sep 27, 2018
tensorflow-1.11.0-cp34-cp34m-manylinux1_x86_64.whl (63.5 MB)  Copy SHA256 hashSHA256 | Wheel | cp34 | Sep 27, 2018
tensorflow-1.11.0-cp35-cp35m-macosx_10_11_x86_64.whl (59.3 MB)  Copy SHA256 hashSHA256 | Wheel | cp35 | Sep 27, 2018
tensorflow-1.11.0-cp35-cp35m-manylinux1_x86_64.whl (63.0 MB)  Copy SHA256 hashSHA256 | Wheel | cp35 | Sep 27, 2018
tensorflow-1.11.0-cp35-cp35m-win_amd64.whl (46.9 MB)  Copy SHA256 hashSHA256 | Wheel | cp35 | Sep 27, 2018
tensorflow-1.11.0-cp36-cp36m-manylinux1_x86_64.whl (63.0 MB)  Copy SHA256 hashSHA256 | Wheel | cp36 | Sep 27, 2018
tensorflow-1.11.0-cp36-cp36m-win_amd64.whl (46.9 MB)  Copy SHA256 hashSHA256 | Wheel | cp36 | Sep 27, 2018

"
22573,TensorFlow 1.11.0 requires scipy,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.11.0
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: `pip install tensorflow==1.11.0`

### Describe the problem

TensorFlow 1.11.0 requires `keras-preprocessing>=1.0.3`, which in turn requires `scipy>=0.14`.  So now scipy is a dependency of TensorFlow.  This seems like a rather large change, and I didn't see anything about it in the release notes, so I'm wondering whether it was intentional or not.

Note that this same change makes all of Keras a dependency of TensorFlow now, as noted in #22426
"
22570,Keras eager execution: Accessing the model DeferredTensors,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Colab
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: 
- **TensorFlow installed from (source or binary)**: Colab
- **TensorFlow version (use command below)**:  1.11.0-rc2
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: Colab
- **GCC/Compiler version (if compiling from source)**: Colab
- **CUDA/cuDNN version**: Colab
- **GPU model and memory**: Colab
- **Exact command to reproduce**:

### Describe the problem
I have a custom loss function which requires some specific tensors to be computed based on other tensors generated from the model itself.  Unfortunately, this can not be done in eager mode.

Please find below a toy example that illustrates the issue. The code runs without problems in the graph mode.
I really need a workaround. Could you please help.

Thanks.

### Source code 
```
import tensorflow as tf
import tensorflow.keras as keras
import tensorflow.keras.backend as K
import numpy as np
from tensorflow.keras.layers import Dense, Input, Layer
from tensorflow.keras.models import Model

tf.enable_eager_execution()

def custom_loss_wrapper(input_tensor):
    def custom_loss(y_true, y_pred):
        return K.binary_crossentropy(y_true, y_pred) + K.mean(input_tensor)
    return custom_loss
  
input_tensor = Input(shape=(10,))
hidden = Dense(100, activation='relu')(input_tensor)
out = Dense(1, activation='sigmoid')(hidden)
model = Model(input_tensor, out)
model.compile(loss=custom_loss_wrapper(input_tensor), optimizer=tf.train.AdamOptimizer(learning_rate=0.001))

np.random.seed(0)
X = np.random.random((3, 10)).astype(np.float32)
Y1 = np.random.random((3, 1)).astype(np.float32)
model.fit(x=X, y=Y1, batch_size=1, epochs=10)
```

### logs
Epoch 1/10

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-3-9dbf3db39e42> in <module>()
     22 X = np.random.random((3, 10)).astype(np.float32)
     23 Y1 = np.random.random((3, 1)).astype(np.float32)
---> 24 model.fit(x=X, y=Y1, batch_size=1, epochs=10)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)
   1577           initial_epoch=initial_epoch,
   1578           steps_per_epoch=steps_per_epoch,
-> 1579           validation_steps=validation_steps)
   1580     elif self._distribution_strategy:
   1581       return training_distributed.fit_loop(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py in fit_loop(model, inputs, targets, sample_weights, class_weight, val_inputs, val_targets, val_sample_weights, batch_size, epochs, verbose, callbacks, shuffle, initial_epoch, steps_per_epoch, validation_steps)
    696           validation_steps=validation_steps,
    697           do_validation=do_validation,
--> 698           batch_size=batch_size)
    699       callbacks.on_epoch_end(epoch, epoch_logs)
    700       if callbacks.model.stop_training:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py in iterator_fit_loop(model, inputs, class_weight, steps_per_epoch, epoch_logs, val_inputs, val_targets, val_sample_weights, epochs, verbose, callbacks, validation_steps, do_validation, batch_size)
    248     # Train model.
    249     outs, loss, loss_metrics, masks = _process_single_batch(
--> 250         model, x, y, sample_weights=sample_weights, training=True)
    251     outs = generic_utils.to_list(outs)
    252 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py in _process_single_batch(model, inputs, targets, sample_weights, training)
    502           targets,
    503           sample_weights=sample_weights,
--> 504           training=training)
    505       if loss is None:
    506         raise ValueError('The model cannot be run '

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py in _model_loss(model, inputs, targets, sample_weights, training)
    110       with backend.name_scope(model.output_names[i] + '_loss'):
    111         output_loss = weighted_masked_fn(
--> 112             targets[i], outs[i], weights, mask=mask)
    113       # If the number of outputs is 1 then we don't append the loss metric
    114       # associated with each model output. When there are multiple outputs

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_utils.py in weighted(y_true, y_pred, weights, mask)
    569     """"""
    570     # score_array has ndim >= 2
--> 571     score_array = fn(y_true, y_pred)
    572     if mask is not None:
    573       mask = math_ops.cast(mask, y_pred.dtype)

<ipython-input-3-9dbf3db39e42> in custom_loss(y_true, y_pred)
     10 def custom_loss_wrapper(input_tensor):
     11     def custom_loss(y_true, y_pred):
---> 12         return K.binary_crossentropy(y_true, y_pred) + K.mean(input_tensor)
     13     return custom_loss
     14 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py in mean(x, axis, keepdims)
   1727   if x.dtype.base_dtype == dtypes_module.bool:
   1728     x = math_ops.cast(x, floatx())
-> 1729   return math_ops.reduce_mean(x, axis, keepdims)
   1730 
   1731 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)
    486                 'in a future version' if date is None else ('after %s' % date),
    487                 instructions)
--> 488       return func(*args, **kwargs)
    489     return tf_decorator.make_decorator(func, new_func, 'deprecated',
    490                                        _add_deprecated_arg_notice_to_docstring(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py in reduce_mean(input_tensor, axis, keepdims, name, reduction_indices, keep_dims)
   1488                                    input_tensor,
   1489                                    _ReductionDims(input_tensor, axis,
-> 1490                                                   reduction_indices),
   1491                                    keepdims,
   1492                                    name=name))

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py in _ReductionDims(x, axis, reduction_indices)
   1270 
   1271     # Otherwise, we rely on Range and Rank to do the right thing at run-time.
-> 1272     return range(0, array_ops.rank(x))
   1273 
   1274 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py in rank(input, name)
    366   @end_compatibility
    367   """"""
--> 368   return rank_internal(input, name, optimize=True)
    369 
    370 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py in rank_internal(input, name, optimize)
    386       return gen_array_ops.size(input.dense_shape, name=name)
    387     else:
--> 388       input_tensor = ops.convert_to_tensor(input)
    389       input_shape = input_tensor.get_shape()
    390       if optimize and input_shape.ndims is not None:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype)
   1046       name=name,
   1047       preferred_dtype=preferred_dtype,
-> 1048       as_ref=False)
   1049 
   1050 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)
   1142 
   1143     if ret is None:
-> 1144       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1145 
   1146     if ret is NotImplemented:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)
    226                                          as_ref=False):
    227   _ = as_ref
--> 228   return constant(v, dtype=dtype, name=name)
    229 
    230 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name, verify_shape)
    176   ctx = context.context()
    177   if ctx.executing_eagerly():
--> 178     t = convert_to_eager_tensor(value, ctx, dtype)
    179     if shape is None:
    180       return t

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype)
    111     return t
    112   else:
--> 113     return ops.EagerTensor(value, context=handle, device=device, dtype=dtype)
    114 
    115 

ValueError: Attempt to convert a value (<DeferredTensor 'input_2' shape=(?, 10) dtype=float32>) with an unsupported type (<class 'tensorflow.python.keras.engine.base_layer.DeferredTensor'>) to a Tensor."
22569,How debug model during inference in mlEngine,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22568,Tensorflow CPU computation of NCHW data_format.,"I have trained my model on GPU
But I wanna launch it at my server, which has only CPU
What can i do?"
22567,Is upper constraint for setuptools still required?,"Currently ``tensorflow`` has an upper limit for ``setuptools`` in install requirements, as I understand it's related to https://github.com/tensorflow/tensorflow/pull/19820 and https://github.com/tensorflow/tensorflow/pull/19818. However recent setuptools releases have certain number of bug fixes which could solve that issues. May you please check if constraint for setuptools to be <= 39.1.0 is still required? Also removing this constraint may help with https://github.com/tensorflow/tensorflow/issues/22082 because at some of ``40.*`` releases setuptools include the fix for namespaces packages related to that problem.

I ask because I'd like to use newer setuptools and this constraint prevents me from doing this. Also I checked installation of tensorflow with latest setuptools and imports work."
22565,"Tensorflow example, Object Detection: Failed to find input Node 'image_tensor","I try to run tensorflow object Detection example on android with my own '.pb' file.
I change `TF_OD_API_MODEL_FILE` and `TF_OD_API_LABELS_FILE` to my own files.
When I run it I've got the error:

```
java.lang.RuntimeException: Failed to find input Node 'image_tensor'
        at org.tensorflow.demo.TensorFlowObjectDetectionAPIModel.create(TensorFlowObjectDetectionAPIModel.java:102)
```
But by graph HAS input node name 'image_tensor'!! To check it, I open it with tensorboard and check it.

I don't know what versions and libraries should i write here, maybe someone has same issue? 
 
PS - maybe some error got because I have only one class (and label) to detect? Or it doesn't mean?

_________

EDIT: answer on questions from @tensorflowbutler 

**Have I written custom code**
I'm using only code example from [`official page`](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android)
I only change name's of files to my own pb - `TF_OD_API_MODEL_FILE` and `TF_OD_API_LABELS_FILE`.

**OS Platform and Distribution**
MAC OS High Sierra 10.13.6

**TensorFlow installed from**
Tensorflow installed from pip

**TensorFlow version**
1.10.1

**Bazel version**
Build label: 0.16.1
Build target: bazel-out/darwin-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Mon Aug 13 13:42:50 2018 (1534167770)
Build timestamp: 1534167770
Build timestamp as int: 1534167770

**CUDA/cuDNN version**
no

**GPU model and memory**
no

**Exact command to reproduce**
Run app in android studio, start application 'TF Detect', crash on starting.

**Mobile device**
google Nexus 5, api23"
22564," ValueError: None is only supported in the 1st dimension. Tensor 'image_tensor:0' has invalid shape '[None, None, None, 3]'.","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu, **CPU**
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: binary (pip install through anaconda)
- **TensorFlow version (use command below)**: 1.8
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Hi, I want to covert a .pb model to .tflite one. The model is train with tensorflow object detection API. The input tensor shape is (None, None, None, 3) but it seems that tflite_convert doesn't support this kind of input.
Source code / logs

ValueError: None is only supported in the 1st dimension. Tensor 'image_tensor:0' has invalid shape '[None, None, None, 3]'.

@pkulzc Do you think this error is caused because we don't provide the input shape for in the []( models/research/object_detection/export_inference_graph.py ) ? If so, should we edit this file and set the input shape (dimension of our pictures) ?

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22563,RuntimeError: TOCO failed see console for info.,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22561,tflite inceptionv3      the output is greater than 1.,"HI，

demo :  tflite inceptionv3 

May I ask why the output is greater than 1.

Thanks."
22560,Problem while building tensorflow,"Sry, my mistake"
22558,@fft2d//:fft2d bazel ,"fisrt build 
ERROR: /private/var/tmp/_bazel_dsr/f1e96f4f92f1092791bdd5610d5595fa/external/com_google_absl/absl/strings/BUILD.bazel:84:1: C++ compilation of rule '@com_google_absl//absl/strings:internal' failed (Exit 1)



after build
ERROR: /private/var/tmp/_bazel_dsr/f1e96f4f92f1092791bdd5610d5595fa/external/fft2d/BUILD.bazel:26:1: C++ compilation of rule '@fft2d//:fft2d' failed (Exit 1)

macos 10.13.6
ndk 16.0.4442984 
 


"
22555,[Feature Request]Add support of model.eval() and model.train(),"For pytorch, you can use model.eval or mdoel.train to make sure different behavior like dropout.
For tf keras interface however if you write class using keras.Model, you still need to pass all training=training in call function, I think this is less elegant. 
So can we support model.train, model.eval ? when set model.train then all layer and model declared in __init__ is in training mode."
22554,Adding custom op instructions lacking the GPU building instructions,"Tensorflow [Add a new op tutorial](https://www.tensorflow.org/extend/adding_an_op) never mention the building instructions for GPU ops. 

I find one [here](https://gist.github.com/Sergio0694/fc94fb14388ee4b7b92be6e33704e5b9), but it met the similar issue with [Building custom op instructions out of date ](https://github.com/tensorflow/tensorflow/issues/13607).
`
tensorflow.python.framework.errors_impl.NotFoundError: ./libcuda_op.so: undefined symbol: _ZTIN10tensorflow8OpKernelE`


Tensorflow really needs to give some suggestions on how to build GPU ops."
22552,TFLite Model Optimizer: Doesn't work on Transformer Model,"
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.10.0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

```
tflite_convert \
  --output_file=foo.tflite \
  --saved_model_dir=./saved
```
### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I have a SavedModel of the Transformer from the Models repo (https://github.com/tensorflow/models/tree/master/official/transformer). I am trying to use the 'Model Optimization Toolkit for TensorFlow' available in TFLite. The documentation states that I should be able to convert a SavedModel into a TFLite Model using the above command. However, I get an error: ValueError: None is only supported in the 1st dimension. Tensor 'input_tensor' has invalid shape '[None, None]'. (see full log below).

This is a feature request to allow TFLite to quantize variable length sequence models. 

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
2018-09-26 16:06:20.690105: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-09-26 16:06:20.712622: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
2018-09-26 16:06:23.327033: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:344] Starting optimization for grappler item: tf_graph
2018-09-26 16:06:25.991930: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:344] Starting optimization for grappler item: tf_graph
2018-09-26 16:06:26.384859: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:344] Starting optimization for grappler item: tf_graph
Traceback (most recent call last):
  File ""/home/<>/int8/virt2.7/bin/tflite_convert"", line 11, in <module>
    sys.exit(main())
  File ""/home/<>/int8/virt2.7/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 412, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/home/<>/int8/virt2.7/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/home/<>/int8/virt2.7/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 408, in run_main
    _convert_model(tflite_flags)
  File ""/home/<>/int8/virt2.7/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 162, in _convert_model
    output_data = converter.convert()
  File ""/home/<>/int8/virt2.7/lib/python2.7/site-packages/tensorflow/contrib/lite/python/lite.py"", line 408, in convert
    ""invalid shape '{1}'."".format(_tensor_name(tensor), shape))
ValueError: None is only supported in the 1st dimension. Tensor 'input_tensor' has invalid shape '[None, None]'
```"
22551,tf.distributions.Uniform does not produce unique samples,"### System information

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: debian 9 (stretch)
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.9.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: see below (short version: `N=int(1e4); assert len(tf.unique(tf.distributions.Uniform().sample(N)).y.eval()) == N`)


### Describe the problem

Traced subtle bug in my code back to the fact that the TF Uniform distribution (maybe others?) does not produce unique samples.. This is bad.

```
N = int(1e4)

len(np.unique(np.random.rand(N))) # 10000

tf.InteractiveSession()
len(tf.unique(tf.distributions.Uniform().sample(N)).y.eval()) # 9993
```"
22550,MirroredStrategy and Optimizer Compatibility,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes, duplicated the minimal code from documentation snippet [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute#example-with-keras-api)

But the included [keras_mnist.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distribute/python/examples/keras_mnist.py) example is broken as well.

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Docker container `tensorflow/tensorflow:1.11.0-rc2-gpu-py3`
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
2 x 1080ti
- **Exact command to reproduce**:

### Describe the problem

[keras_mnist.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distribute/python/examples/keras_mnist.py) example seems to break. Investigating further... 

Optimizers other than `tf.train.GradientDescentOptimizer` are not working with `MirroredStrategy` for Keras models as shown in this official [example](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute#example-with-keras-api)


### Source code / logs
**Setup**, copied from the [example](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute#example-with-keras-api):

```python
inputs = tf.keras.layers.Input(shape=(1,))
predictions = tf.keras.layers.Dense(1)(inputs)
model = tf.keras.models.Model(inputs=inputs, outputs=predictions)

features = tf.data.Dataset.from_tensors([1.]).repeat(10000).batch(10)
labels = tf.data.Dataset.from_tensors([1.]).repeat(10000).batch(10)
train_dataset = tf.data.Dataset.zip((features, labels))

distribution = tf.contrib.distribute.MirroredStrategy()
```

**Then, this works**:
```python
model.compile(loss='categorical_crossentropy',
              optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.2),
              distribute=distribution)
model.fit(train_dataset, epochs=5, steps_per_epoch=10)
```


But, **Keras optimizers** don't seem to work:
```python
model.compile(loss='categorical_crossentropy',
              optimizer=tf.keras.optimizers.SGD(lr=0.2, momentum=0.9),
              distribute=distribution)
model.fit(train_dataset, epochs=5, steps_per_epoch=10)
```

and these **Tensorflow Optimizers** are throwing errors as well:

```python
model.compile(loss='categorical_crossentropy',
              optimizer=tf.train.AdamOptimizer(learning_rate=0.2),
              distribute=distribution)
model.fit(train_dataset, epochs=5, steps_per_epoch=10)
```

```python
model.compile(loss='categorical_crossentropy',
              optimizer=tf.train.MomentumOptimizer(learning_rate=0.2, momentum=0.9),
              distribute=distribution)
model.fit(train_dataset, epochs=5, steps_per_epoch=10)
```

```python
model.compile(loss='categorical_crossentropy',
              optimizer=tf.train.RMSPropOptimzier(learning_rate=0.2),
              distribute=distribution)
model.fit(train_dataset, epochs=5, steps_per_epoch=10)
```



Error looks like:
```bash
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py"", line 1590, in fit
    validation_steps=validation_steps)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_distributed.py"", line 125, in fit_loop
    orig_model_weights = model.get_weights()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/network.py"", line 483, in get_weights
    return backend.batch_get_value(weights)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py"", line 2717, in batch_get_value
    return get_session().run(tensors)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py"", line 465, in get_session
    _initialize_variables(session)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py"", line 710, in _initialize_variables
    variables = _get_variables(ops.get_default_graph())
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py"", line 704, in _get_variables
    variables.update(opt.optimizer.variables())
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py"", line 785, in variables
    optimizer_variables = [v for v in self._non_slot_variables()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py"", line 786, in <listcomp>
    if _from_current_graph(v)]
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py"", line 779, in _from_current_graph
    return variable.op.graph is current_graph
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/values.py"", line 305, in op
    return self.get().op
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/values.py"", line 73, in get
    (device, self._index.keys(), device_util.current())), e)
  File ""<string>"", line 3, in raise_from
ValueError: Device /replica:0/task:0/device:CPU:0 not found in dict_keys(['/replica:0/task:0/device:GPU:1', '/replica:0/task:0/device:GPU:0']) (current device )
```



"
22546,Number of intra_op_parallelism_threads Issue with TF 1.10,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra version 10.13.6
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10
- **Python version**: 2.7.10
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: None (It's a CPU build)
- **GPU model and memory**: N/A
- **Exact command to reproduce**: Please see sections below

### Describe the problem


In tensorflow 1.7.0, getting the number of threads in the intra_op pool was possible with the following line of code in C++:
`context->device()->tensorflow_cpu_worker_threads()->num_threads`
where context is an object of type OpKernelContext. (because of https://github.com/tensorflow/tensorflow/blob/7bccde15ce0dd29dce62092a5e9d48ffdc772963/tensorflow/core/common_runtime/local_device.cc#L43).

If the corresponding ConfigProto was set using the following python code:
```
import tensorflow as tf 
conf = tf.ConfigProto(inter_op_parallelism_threads=5, intra_op_parallelism_threads=4) 
sess = tf.Session(config=conf)
```

TF 1.7 would output the value of `context->device()->tensorflow_cpu_worker_threads()->num_threads` as `4`.

TF 1.10 outputs the value of `context->device()->tensorflow_cpu_worker_threads()->num_threads` as `1`, irrespective of the intra_op_parallelism_threads value specified in the ConfigProto.

I was wondering if something changed in the TF upgrade from 1.7.0 to 1.10.0 to cause this piece of code to give a different value or is the number of threads in the threadpool truly 1?


Other relevant information:

- macOS High Sierra version `10.13.6`

- Working TF version: `1.7.0`

- Breaking TF version: `1.10.0`

- Python information: Python 2.7.10 (default, Oct 6 2017, 22:29:07) [GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.31)] on darwin

- Detailed debugging information from tensorflow (by setting `TF_CPP_MIN_VLOG_LEVEL=3` before running the python program) says that TF 1.10 is indeed planning to create a thread pool of size 4 on this line of C++ code. The C++ piece of code at the start of this post however outputs value 1.

- This is the first and only session being created in the program.
"
22545,Confusion matrix and printing CNN kernel with TF,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22543,sparse_tensor_to_dense() does not seem to support back propagation,"**Have I written custom code: Yes
**OS Platform and Distribution: Windows 10 (primary), Ubunto 16.04.3 LTS (secondary)
**TensorFlow installed from (source or binary): Binary
**TensorFlow version (use command below): b'v1.10.0-rc1-19-g656e7a2b34' 1.10.0
**Python version: Python 3.6.6 :: Anaconda, Inc.
CUDA/cuDNN version: N/A (not relevant)
GPU model and memory: N/A (not relevant)

### Describe the problem
The function sparse_tensor_to_dense() does not seem to support backpropagation. I've written a test script below, which yielded 

> Fetch argument None has invalid type <class 'NoneType'>

If you change the line
`grad_A00 = tf.gradients(A_dense_transformed[0,0], par)`
to the next (currently commented-out)
`grad_A00 = tf.gradients(A_dense_manual[0,0], par)`
the script can be executed.

If the sparse tensor formulation is necessary, the workaround is to use the commented lines 
```
identity = tf.Variable(eye(2), dtype = float32)
A_dense_transformed = A = tf.sparse_tensor_dense_matmul(tf.sparse_transpose(A_sparse), identity)
```
to replace original `A_dense_transformed = A = tf.sparse_tensor_to_dense(A_sparse)`. 
Such an exercise allows the program to execute the original `grad_A00 = tf.gradients(A_dense_transformed[0,0], par)`.

### Source code / logs
```

import tensorflow as tf
from numpy import *
# showing backprop fails if using sparse tensor

tf.reset_default_graph()

par = tf.Variable(1.0, dtype = float32)
A_sparse = tf.SparseTensor(indices=[[0,0],], values=tf.stack([par,]), dense_shape = (2,1))

A_dense_manual = tf.stack([[par],[0]])
A_dense_transformed = A = tf.sparse_tensor_to_dense(A_sparse)
#identity = tf.Variable(eye(2), dtype = float32)
#A_dense_transformed = A = tf.sparse_tensor_dense_matmul(tf.sparse_transpose(A_sparse), identity)

grad_A00 = tf.gradients(A_dense_transformed[0,0], par)
#grad_A00 = tf.gradients(A_dense_manual[0,0], par)

init = tf.global_variables_initializer()

with tf.Session() as sess:
    
    init.run()
    vA_manual, vA_transformed, vGrad = sess.run([A_dense_manual, A_dense_transformed, grad_A00])    
    
print(vA_manual)
print(vA_transformed)
print(vGrad)
```
"
22542,Misleading full description on Tensorflow's Docker Hub public repository,"The instructions on https://hub.docker.com/r/tensorflow/tensorflow/ still refer to the old `nvidia-docker`, when new images support nvidia-docker2, which works a bit differently.. The full description from Docker Hub doesn't seem to be under version control, so I can't submit a diff, but the following changes are needed, under the ""Start GPU (CUDA) container"" section:

* In ""Install nvidia-docker and run"", change ""nvidia-docker"" for ""nvidia-docker2"" instead. (The link stays the same).
* Instead of  ""nvidia-docker run -it -p 8888:8888 tensorflow/tensorflow:latest-gpu"", now the command is `docker run --runtime=nvidia -it -p 8888:8888 tensorflow/tensorflow:latest-gpu`.

This was the best place I could find where to submit this bug report. If there's a better place, please let me know. Thanks!"
22541,Java process crashes during model loading,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: AWS c5.xlarge, Amazon Linux
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: java binary
- **TensorFlow version (use command below)**: java org.tensorflow:tensorflow:1.11.0-rc2
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:


### Describe the problem
I run the following program on `c5.xlarge` (CPU only, no GPU) with 8GB RAM and 12GB swap memory.
```java
public class TryOut {
    public static void main(String[] args) {
        SavedModelBundle[] loadedModels = new SavedModelBundle[15];
        for (int i = 0; i < 15; i++) {
            System.out.println(""loading model "" + i);
            loadedModels[i] = SavedModelBundle.load(""models/0"", ""serve"");
        }
    }
}
```

`models/0` contains the model [faster_rcnn_resnet101_coco](http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet101_coco_2018_01_28.tar.gz) from the [model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md).
Here is the directory structure:
```sh
models/0/
└── saved_model.pb
```

I run the program as `java -jar mem-test.jar`.


#### Problem
Process crashes during model loading when the free memory becomes low (see `output.txt`).
The process does not try to utilize available swap memory.

This behavior is reproducible with other models from the model zoo.

On the other hand, `tensorflow-serving` can download multiple models into memory, utilizes swap and runs inferences without crashing as long as there is swap memory available.

### Source code / logs

See also the attached files:
* [output.txt](https://github.com/tensorflow/tensorflow/files/2420367/output.txt)
* [hs_err_pid31150.log](https://github.com/tensorflow/tensorflow/files/2420368/hs_err_pid31150.log)

"
22540,conflicting shape of tflite conv2d weight tensor shape vs conv2d documentation," https://www.tensorflow.org/api_docs/python/tf/nn/conv2d
shows that conv2d expects kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]
The mobilenet_v1_1.0_224/mobilenet_v1_1.0_224.tflite from the following link contains shape for the first conv2d of [32,3,3,3]
while the mobilenet_v1_1.0_224/mobilenet_v1_1.0_223_frozen.pb has conv2d tensor shape of [3,3,3,32]
https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md

So, since these are conflicting shapes, both going into the first conv2d, then one of them does not match the documentation kernel tensor shape.  It appears to me the .pb file matches the documentation, since HWCN is 3,3,3,32 for the first conv2d

Is the .tflite format going to persist the weight tensors in a different shape than in the .pb?   If so, I don't see that documented.

You can see this by downloading the netron viewer from 
https://github.com/lutzroeder/netron
and clicking on the weight tab in the first conv2d, and look at the shape in the weights node properties.
I'm using tensorflow 1.10 on ubuntu 18.04, and using netron viewer 2.1.8  


"
22538,tfe.Saver only stores last 5 checkpoints,"
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10.1
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: CUDA 9.0
- **GPU model and memory**: 940 MX
- **Exact command to reproduce**:
```
import numpy as np
import os

import tensorflow as tf
import tensorflow.contrib.eager as tfe


 '''The following should be irrelevant, but nevertheless'''
class CNNClassifier(tf.keras.Model):

    def __init__(self):
        super(CNNClassifier, self).__init__()
        xav_init = tf.contrib.layers.xavier_initializer

        self.layer_1 = tf.layers.Conv2D(
            filters=32,
            kernel_size=[3, 3],
            padding=""same"",
            activation=tf.nn.relu,
            kernel_initializer=xav_init())
        self.layer_2 = tf.layers.Conv2D(
            filters=64,
            kernel_size=[3, 3],
            padding=""same"",
            activation=tf.nn.relu,
            kernel_initializer=xav_init())
        self.layer_3 = tf.layers.Conv2D(
            filters=128,
            kernel_size=[3, 3],
            padding=""same"",
            activation=tf.nn.relu,
            kernel_initializer=xav_init())
        self.layer_4 = tf.layers.Conv2D(
            filters=256,
            kernel_size=[3, 3],
            padding=""same"",
            activation=tf.nn.relu,
            kernel_initializer=xav_init())
        self.layer_5 = tf.layers.Conv2D(
            filters=512,
            kernel_size=[3, 3],
            padding=""same"",
            activation=tf.nn.relu,
            kernel_initializer=xav_init())
        self.dense = tf.layers.Dense(units=10)

    def call(self, inputs):
        inputs = tf.convert_to_tensor(inputs)
        x = tf.layers.max_pooling2d(
            self.layer_1(inputs), pool_size=[2, 2], strides=[2, 2])
        x = tf.layers.max_pooling2d(
            self.layer_2(x), pool_size=[2, 2], strides=[2, 2])
        x = tf.layers.max_pooling2d(
            self.layer_3(x), pool_size=[2, 2], strides=[2, 2])
        x = tf.layers.max_pooling2d(
            self.layer_4(x), pool_size=[2, 2], strides=[2, 2])
        x = tf.layers.max_pooling2d(
            self.layer_5(x), pool_size=[2, 2], strides=[2, 2])
        x = tf.layers.flatten(x)
        x = self.dense(x)
        return x

if __name__ == ""__main__"":
	os.mkdir(""saved_models"")
    tfe.enable_eager_execution()

    c = CNNClassifier()
    c(np.random.randn(1, 32, 32, 3))

    saver = tfe.Saver(c.variables)

    for i in range(100):
        print(i)
        saver.save(f""saved_models/step_{i}.ckpt"")

```

### Problem Description
I am using eager execution and the `tfe.Saver` class to save my model's weights. However, if I am saving many checkpoints, only last five checkpoints are saved. This is very similar to issues with `tf.train.Saver` where the `max_to_keep` argument defaults to `5`. Since `tfe.Saver` doesn't have a corresponding argument, it seems to me that it is impossible to store more than the 5 most recent checkpoints.

I am happy to submit PRs if there's any guidance on how to fix this issue.


"
22536,wrong version of protobuf for tensorflow1.10,"I'm trying to build the C++ shared library with the following steps. [ubuntu18.04]
1, git clone -b r1.10 https://github.com/tensorflow/tensorflow.git
2, run /tensorflow/contrib/makefile/build_all_linux.sh to get third-party dependencies
3, ./configure and babzel build //tensorflow:libtensorflow_cc.so
4, Get all the needed header files to INCLUDE and libraries to LIB

When testing the example ""label_image"", I got an error ""missing header google/protobuf/inlined_string_fieled.h"". 

Then I checked downloaded version of protobuf, it was version 3.5.0.
As I checked somewhere I didn't remember, tensorflow r1.10 requires protobuf version over 3.6.0. Then I replaced the /google directory with version 3.6.0. It works quite well.

Could you please fix it to get version 3.6.0 of protobuf installed rather than verson 3.5.0 by running build_linux_all.sh?

Thank you in advance!!"
22535,After post-training quantization -- Internal error: Cannot allocate memory for the interpreter,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: tf-nightly-1.12.0.dev20180926
- **TensorFlow version (use command below)**: tf-nightly-1.12.0.dev20180926
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: no
- **GCC/Compiler version (if compiling from source)**: no
- **CUDA/cuDNN version**: no
- **GPU model and memory**: no
- **Exact command to reproduce**: see follows

### Describe the problem
When I tried to use the quantized model for the example [TensorFlow for Poets 2: TFLite Android](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2-tflite/#0), I encountered the following problem.
beginning of crash
09-26 13:08:10.249 31824-31824/? E/AndroidRuntime: FATAL EXCEPTION: main
    Process: android.example.com.tflitecamerademo, PID: 31824
    java.lang.RuntimeException: Unable to start activity ComponentInfo{android.example.com.tflitecamerademo/com.example.android.tflitecamerademo.CameraActivity}: **java.lang.NullPointerException: Internal error: Cannot allocate memory for the interpreter: tensorflow/contrib/lite/kernels/conv.cc:201 filter->type != data_type (3 != 1)Node 4 failed to prepare.**
    
        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2665)
        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2726)
        at android.app.ActivityThread.-wrap12(ActivityThread.java)
        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1477)
        at android.os.Handler.dispatchMessage(Handler.java:102)
        at android.os.Looper.loop(Looper.java:154)
        at android.app.ActivityThread.main(ActivityThread.java:6119)
        at java.lang.reflect.Method.invoke(Native Method)
        at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:886)
        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:776)
     Caused by: java.lang.NullPointerException: Internal error: Cannot allocate memory for the interpreter: tensorflow/contrib/lite/kernels/conv.cc:201 filter->type != data_type (3 != 1)Node 4 failed to prepare.
    
        at org.tensorflow.lite.NativeInterpreterWrapper.createInterpreter(Native Method)
        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:75)
        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:54)
        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:114)
        at com.example.android.tflitecamerademo.ImageClassifier.<init>(ImageClassifier.java:97)
        at com.example.android.tflitecamerademo.Camera2BasicFragment.onActivityCreated(Camera2BasicFragment.java:299)
        at android.app.Fragment.performActivityCreated(Fragment.java:2362)
        at android.app.FragmentManagerImpl.moveToState(FragmentManager.java:1014)
        at android.app.FragmentManagerImpl.moveToState(FragmentManager.java:1171)
        at android.app.BackStackRecord.run(BackStackRecord.java:816)
        at android.app.FragmentManagerImpl.execPendingActions(FragmentManager.java:1578)
        at android.app.FragmentController.execPendingActions(FragmentController.java:371)
        at android.app.Activity.performStart(Activity.java:6695)
        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2628)
        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2726) 
        at android.app.ActivityThread.-wrap12(ActivityThread.java) 
        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1477) 
        at android.os.Handler.dispatchMessage(Handler.java:102) 
        at android.os.Looper.loop(Looper.java:154) 
        at android.app.ActivityThread.main(ActivityThread.java:6119) 
        at java.lang.reflect.Method.invoke(Native Method) 
        at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:886) 
        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:776)

The cmds were as follows:
```
import tensorflow as tf
graph_def_file='retrained_graph.pb'
input_arrays = [""input""]
output_arrays = [""final_result""]
converter = tf.contrib.lite.TocoConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays)
converter.post_training_quantize = True
tflite_quantized_model = converter.convert()
open(""quantized_graph.tflite"", ""wb"").write(tflite_quantized_model)
```
But if post-training quantization is not enabled, the app works.

The quantized model could work in python env (though the result is not accurate).
```
interpreter = tf.contrib.lite.Interpreter(model_path=""quantized_graph.tflite"")
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
from label_image import read_tensor_from_image_file
input_data = read_tensor_from_image_file('1.jpg',input_height=224,input_width=224, input_mean=128, input_std=128)
interpreter.set_tensor(input_details[0]['index'], input_data)
interpreter.invoke()
output_data = interpreter.get_tensor(output_details[0]['index'])
print(output_data)
```

### Source code / logs
Models are attached.
[tf_files.zip](https://github.com/tensorflow/tensorflow/files/2419845/tf_files.zip)

"
22532,Tensorflow quantization on Windows,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04, Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: -
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10.1 CPU
- **Python version**: 3.6

### Describe the problem
I've freezed my model and got .pb file. Then I've quantize my model using tocoConverter on Linux, as it's not supported on Windows. I've got quantized_model.tflite. I can load it and get predictions on Linux, but I have issues to make it on Windows, as my project requires. I've tried to load it using tf.contrib.lite.Interpreter using this code:
`import numpy as np
import tensorflow as tf

interpreter=tf.contrib.lite.Interpreter(model_path=""quantized_model.tflite"")
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

input_shape = input_details[0]['shape']

input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
interpreter.set_tensor(input_details[0]['index'],input_data)

interpreter.invoke()
output_data = interpreter.get_tensor(output_details[0]['index'])
print(output_data)

*//ImportError: No module named 'tensorflow.contrib.lite.python.Interpreter*`

But it failed with ""No module named 'tensorflow.contrib.lite.python.interpreter"" error. I always get this errors on Windows, when trying to use something from tf.contrib.lite. Maybe there is a way to load this on Windows? Or can you advice alternative options to quantize a model on Windows?
"
22531,Android tfLite Shared STL support ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Android
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
Android
- **TensorFlow installed from (source or binary)**:
N/A
- **TensorFlow version (use command below)**:
All version of TFlite
- **Python version**:
N/A
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
N/A

Currently the final .so files from the android build are produced linking against the libc++_static.a library, this has the effect of compiling a large amount of code into the tfLite shared library.

Android also supports linking against a libc++_shared.so version of the STL (as of NDK 18 libc++ is the ONLY supported version of the stl). 

The main reason for this is that we also have a few other native modules that currently link against libc++_shared.so so it would be handy to reduce the size of the tflite.so library we have to package into the final APK.

It would be VERY handy to have two versions of the AAR tfLite package produced, the current version with a static version of libc++ and another version linking against libc++_shared.so 

NDK Docs for libc++:
https://developer.android.com/ndk/guides/cpp-support#libc

Current tfLite AAR package: 
https://bintray.com/google/tensorflow/tensorflow-lite/1.10.0

maybe a new package could be something like: 
org.tensorflow:tensorflow-lite-shared:1.10.0"
22530,Inconsistent behaviour of tf.range,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: v1.10.0-0-g656e7a2b34 1.10.0
- **Python version**: 3.5.2
- **CUDA/cuDNN version**: V9.0.176
- **GPU model and memory**: GTX Titan X + 2xGTX Titan Black


### Describe the problem
the tensor produced by tf.range exceeds the limit with the configuration of parameters below.

### Source code / logs
```
import tensorflow as tf

with tf.Session() as sess:
	max_value=379.0
	delta=379.0/94.0

	range_tensor = tf.range(max_value,delta=delta,dtype=tf.float32)

	print(sess.run(range_tensor))
```
Result:

> [  0.          4.0319147   8.063829   12.095744   16.127659   20.159573
  24.191486   28.2234     32.255314   36.287228   40.31914    44.351055
  48.38297    52.414883   56.446796   60.47871    64.51063    68.54254
  72.574455   76.60637    80.63828    84.6702     88.70211    92.734024
  96.76594   100.79785   104.829765  108.86168   112.89359   116.92551
 120.95742   124.989334  129.02126   133.05318   137.0851    141.11702
 145.14894   149.18086   153.21278   157.2447    161.27663   165.30855
 169.34047   173.37239   177.40431   181.43623   185.46815   189.50008
 193.532     197.56392   201.59584   205.62776   209.65968   213.6916
 217.72353   221.75545   225.78737   229.81929   233.85121   237.88313
 241.91505   245.94698   249.9789    254.01082   258.04272   262.07465
 266.10657   270.1385    274.1704    278.20233   282.23425   286.26617
 290.2981    294.33002   298.36194   302.39386   306.42578   310.4577
 314.48962   318.52155   322.55347   326.5854    330.6173    334.64923
 338.68115   342.71307   346.745     350.77692   354.80884   358.84076
 362.87268   366.9046    370.93652   374.96844   379.00037  ]

379.00037 should not be there!

Equivalent numpy code
```
import numpy as np
max_value=379.0
delta=379.0/94.0
print(np.arange(0,max_value,delta))
```
Result:
> [  0.           4.03191489   8.06382979  12.09574468  16.12765957
  20.15957447  24.19148936  28.22340426  32.25531915  36.28723404
  40.31914894  44.35106383  48.38297872  52.41489362  56.44680851
  60.4787234   64.5106383   68.54255319  72.57446809  76.60638298
  80.63829787  84.67021277  88.70212766  92.73404255  96.76595745
 100.79787234 104.82978723 108.86170213 112.89361702 116.92553191
 120.95744681 124.9893617  129.0212766  133.05319149 137.08510638
 141.11702128 145.14893617 149.18085106 153.21276596 157.24468085
 161.27659574 165.30851064 169.34042553 173.37234043 177.40425532
 181.43617021 185.46808511 189.5        193.53191489 197.56382979
 201.59574468 205.62765957 209.65957447 213.69148936 217.72340426
 221.75531915 225.78723404 229.81914894 233.85106383 237.88297872
 241.91489362 245.94680851 249.9787234  254.0106383  258.04255319
 262.07446809 266.10638298 270.13829787 274.17021277 278.20212766
 282.23404255 286.26595745 290.29787234 294.32978723 298.36170213
 302.39361702 306.42553191 310.45744681 314.4893617  318.5212766
 322.55319149 326.58510638 330.61702128 334.64893617 338.68085106
 342.71276596 346.74468085 350.77659574 354.80851064 358.84042553
 362.87234043 366.90425532 370.93617021 374.96808511]
"
22529,Feature request: Make contrib.receptive_field extensible,"### System information
- **TensorFlow version (use command below)**: 1.8.0

### Describe the problem

For the problem I am working on, I need to compute a cross-correlation where the filters are a function of some inputs, i.e. `c_i = conv2d(f(a_i), g(b_i))` for the `i`-th example `(a_i, b_i)` in the batch. After doing this, the receptive field of `c` with respect to `a` is still well-defined.

I can think of several ways to implement this, but none of them are compatible with `receptive_field`!

For example, I usually use `reshape`-`depthwise_conv2d`-`reshape`-`sum` to transform a batch of `B` convolutions with `C` channels into a batch of `1` depthwise-convolution with `B*C` channels, then sum over the `C` channels at the end. Another way to do it would be using `tf.map_fn`. However, neither of these will work with `compute_receptive_field_from_graph_def`.

I understand that it would be very difficult to support edge cases like this. However, it breaks the workflow a bit to keep track of every place where the graph is incompatible with `receptive_field` and perform the composition manually here.

Perhaps it would be possible to add a manual override parameter to `compute_receptive_field_from_graph_def` that allows a user to manually specify the receptive field from one node to another, instead of using the graph?

Another way could be to support custom layers, either by checking if the custom layer has a receptive field method, or allowing users to register custom layers with the `receptive_field` module. However, this would require me to write my own layer for the ""dynamic"" convolution described above, which is not currently necessary.

I'm open to suggestions!"
