Issue Number,Issue Title,Issue Body
27181,"[TF 2.0 alpha] Tutorial ""Using TFRecords and tf.Example"" can't run on GPU","<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: Tensorflow 2.0 alpha
- Doc Link: [Using TFRecords and tf.Example](https://www.tensorflow.org/alpha/tutorials/load_data/tf_records)


**Describe the documentation issue**
I don't know why this tutorial only works for tensorflow CPU version.
If I run on GPU I will have an error in the cell:
`tf_serialize_example(f0,f1,f2,f3)`

```
---------------------------------------------------------------------------
UnknownError                              Traceback (most recent call last)
<ipython-input-14-406ee79a7f52> in <module>
----> 1 tf_serialize_example(f0,f1,f2,f3)

<ipython-input-13-3eb13f28c327> in tf_serialize_example(f0, f1, f2, f3)
      3     serialize_example,
      4     (f0,f1,f2,f3),  # pass these args to the above function.
----> 5     tf.string)      # the return type is `tf.string`.
      6   return tf.reshape(tf_string, ()) # The result is a scalar

D:\Anaconda\envs\tf2\lib\site-packages\tensorflow\python\ops\script_ops.py in eager_py_func(func, inp, Tout, name)
    387     if `func` returns None.
    388   """"""
--> 389   return _internal_py_func(func=func, inp=inp, Tout=Tout, eager=True, name=name)
    390 
    391 

D:\Anaconda\envs\tf2\lib\site-packages\tensorflow\python\ops\script_ops.py in _internal_py_func(func, inp, Tout, stateful, eager, is_grad_func, name)
    276   if eager:
    277     result = gen_script_ops.eager_py_func(
--> 278         input=inp, token=token, Tout=Tout, name=name)
    279   else:
    280     if stateful:

D:\Anaconda\envs\tf2\lib\site-packages\tensorflow\python\ops\gen_script_ops.py in eager_py_func(input, token, Tout, name)
     64       else:
     65         message = e.message
---> 66       _six.raise_from(_core._status_to_exception(e.code, message), None)
     67   # Add nodes to the TensorFlow graph.
     68   token = _execute.make_str(token, ""token"")

~\AppData\Roaming\Python\Python36\site-packages\six.py in raise_from(value, from_value)

UnknownError: RuntimeError: Error copying tensor to device: CPU:0. Can't copy 37 bytes of a tensor into another with 32 bytes buffer.
Traceback (most recent call last):

  File ""D:\Anaconda\envs\tf2\lib\site-packages\tensorflow\python\ops\script_ops.py"", line 205, in __call__
    return func(device, token, args)

  File ""D:\Anaconda\envs\tf2\lib\site-packages\tensorflow\python\ops\script_ops.py"", line 107, in __call__
    ret = self._func(*args)

  File ""<ipython-input-7-60a59d21d73e>"", line 10, in serialize_example
    'feature2': _bytes_feature(feature2),

  File ""<ipython-input-3-8047e5ac1b09>"", line 7, in _bytes_feature
    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.

  File ""D:\Anaconda\envs\tf2\lib\site-packages\tensorflow\python\framework\ops.py"", line 732, in numpy
    return self._cpu_nograd()._numpy()  # pylint: disable=protected-access

  File ""D:\Anaconda\envs\tf2\lib\site-packages\tensorflow\python\framework\ops.py"", line 899, in _cpu_nograd
    return self._copy_nograd(context.context(), ""CPU:0"")

  File ""D:\Anaconda\envs\tf2\lib\site-packages\tensorflow\python\framework\ops.py"", line 847, in _copy_nograd
    new_tensor = self._copy_to_device(context=ctx._handle, device=device_name)

RuntimeError: Error copying tensor to device: CPU:0. Can't copy 37 bytes of a tensor into another with 32 bytes buffer.

 [Op:EagerPyFunc]
```

Does it have a specific thing only for CPU? 


**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
27180,"[TF 2.0 alpha] Fix ""Load images with tf.data"" to run on window","<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: Tensorflow 2.0 alpha
- Doc Link: Tutorial [Load images with tf.data](https://www.tensorflow.org/alpha/tutorials/load_data/images)

**Describe the documentation issue**
Tutorial [Load images with tf.data](https://www.tensorflow.org/alpha/tutorials/load_data/images) will have an error if it runs on windows because of the backslash of the path in this cell.
```
import IPython.display as display

def caption_image(image_path):
    image_rel = pathlib.Path(image_path).relative_to(data_root)
    return ""Image (CC BY 2.0) "" + ' - '.join(attributions[str(image_rel)].split(' - ')[:-1])
```

So I fixed it by replacing the backslash with the slash.
import IPython.display as display

```
def caption_image(image_path):
    image_rel = pathlib.Path(image_path).relative_to(data_root)
    image_rel = str(image_rel)
    image_rel=image_rel.replace('\\','/')
    str_return = ""Image (CC BY 2.0) "" + ' - '.join(attributions[str(image_rel)].split(' - ')[:-1]) 
    return str_return

```



**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
27176,TensorflowLite Convert error,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 1.18


**Provide the text output from tflite_convert**

```
# Copy and paste here
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONV_2D, EXPAND_DIMS, FULLY_CONNECTED, MAX_POOL_2D, RESHAPE, SQUEEZE, TRANSPOSE. Here is a list of operators for which you will need custom implementations: Merge, Switch.
```

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27173,"InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'import/Placeholder' with dtype uint8 and shape [1,30,300,3]","I try to convert a tensorflow model to tf-trt model ,but when running has errors:
Traceback (most recent call last):
  File ""./tranTFtoTRT.py"", line 27, in <module>
    sess.run(output_node)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 929, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1328, in _do_run
    run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'import/Placeholder' with dtype uint8 and shape [1,30,300,3]
	 [[node import/Placeholder (defined at ./tranTFtoTRT.py:26) ]]

Caused by op u'import/Placeholder', defined at:
  File ""./tranTFtoTRT.py"", line 26, in <module>
    return_elements=['AttentionOcr_v1/predicted_chars:0'])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py"", line 442, in import_graph_def
    _ProcessNewOps(graph)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py"", line 235, in _ProcessNewOps
    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3433, in _add_new_tf_operations
    for c_op in c_api_util.new_tf_operations(self)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3325, in _create_op_from_tf_operation
    ret = Operation(c_op, self)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1801, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'import/Placeholder' with dtype uint8 and shape [1,30,300,3]
	 [[node import/Placeholder (defined at ./tranTFtoTRT.py:26) ]]

Here is my code:

  1 #!/usr/bin/env python
  2 # -*- coding: utf-8 -*-
  3 
  4 # Import TensorFlow and TensorRT
  5 import tensorflow as tf
  6 import tensorflow.contrib.tensorrt as trt
  7 
  8 # Inference with TF-TRT 'SavedModel' workflow :
  9 graph = tf.Graph()
 10 with graph.as_default():
 11     with tf.Session() as sess:
 12         # Create a TensorRT inference graph from a SavedModel:
 13         trt_graph = trt.create_inference_graph(
 14             input_graph_def=None,
 15             outputs=None,
 16             input_saved_model_dir=""/workspace/models/ocrmodel"",
 17             input_saved_model_tags=[""serve""],
 18             max_batch_size=1,
 19             #max_workspace_size_bytes=1 << 32,
 20             max_workspace_size_bytes=1 << 8,
 21             #precision_mode=""FP32"")
 22             precision_mode=""INT8"")
 23         # Import the TensorRT graph into a new graph and run:
 24         output_node = tf.import_graph_def(
 25             trt_graph,
 26             return_elements=['AttentionOcr_v1/predicted_chars:0'])
 27         sess.run(output_node)

Can anyone help me!"
27172,Bazel Build Error: Failed to build libtensorflow_inference.so,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):MacOS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No,it's during compiling
- TensorFlow installed from (source or binary):
- TensorFlow version:latest
- Python version:2.7
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):0.23.2
- GCC/Compiler version (if compiling from source):clang-1000.10.44.4


**Describe the problem**
I'm tring to build libtensorflow_inference.so using bazel following [this tutorial](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/android) and encounter this error when running command 
~~~
bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so \
   --crosstool_top=//external:android/crosstool \
   --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \
   --cxxopt=-std=c++11 \
   --cpu=armeabi-v7a
~~~
Error Message:
~~~
ERROR: /private/var/tmp/_bazel_vigor/d97331ab44cbad00aa5fa19b76a95775/external/com_google_absl/absl/numeric/BUILD.bazel:25:1: C++ compilation of rule '@com_google_absl//absl/numeric:int128' failed (Exit 1)
external/com_google_absl/absl/numeric/int128.cc:139:59: error: no member named 'trunc' in namespace 'std'; did you mean simply 'trunc'?
  uint64_t w0 = static_cast<uint64_t>(static_cast<double>(std::trunc(v)));
                                                          ^~~~~~~~~~
                                                          trunc
external/androidndk/ndk/platforms/android-14/arch-arm/usr/include/math.h:283:8: note: 'trunc' declared here
double  trunc(double);
        ^
external/com_google_absl/absl/numeric/int128.cc:141:59: error: no member named 'trunc' in namespace 'std'; did you mean simply 'trunc'?
  uint64_t w1 = static_cast<uint64_t>(static_cast<double>(std::trunc(v)));
                                                          ^~~~~~~~~~
                                                          trunc
external/androidndk/ndk/platforms/android-14/arch-arm/usr/include/math.h:283:8: note: 'trunc' declared here
double  trunc(double);
        ^
external/com_google_absl/absl/numeric/int128.cc:143:59: error: no member named 'trunc' in namespace 'std'; did you mean simply 'trunc'?
  uint64_t w2 = static_cast<uint64_t>(static_cast<double>(std::trunc(v)));
                                                          ^~~~~~~~~~
                                                          trunc
external/androidndk/ndk/platforms/android-14/arch-arm/usr/include/math.h:283:8: note: 'trunc' declared here
double  trunc(double);
        ^
3 errors generated.
Target //tensorflow/contrib/android:libtensorflow_inference.so failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 156.381s, Critical Path: 11.75s
INFO: 602 processes: 602 local.
FAILED: Build did NOT complete successfully
~~~
I'm using the WORKSPACE file as default, the android sdk and ndk version is like this:
~~~
android_sdk_repository(
    name = ""androidsdk"",
    api_level = 28,
    build_tools_version = ""28.0.3"",
    # Replace with path to Android SDK on your system
    path = ""/Users/vigor/Library/Android/sdk"",
)

android_ndk_repository(
    name=""androidndk"",
    path=""/Users/vigor/Library/Android/ndk14/"",
    api_level=14)

~~~"
27171,Compute F1 score for multilabel classifier,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.13
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**
I am working with tf.contrib.metrics.f1_score in a metric function and call it using an estimator. I want to compute the F1 score for **multi label classifier** but this contrib function can not compute it. Please add this capability to this F1 ( computing macro and micro f1). I need it to compare the dev set and based on that keep the best model. I don't like to compute it using the sklearn

**Will this change the current api? How?**
IDK

**Who will benefit with this feature?**
Every one who is trying to compute macro and micro f1 inside the Tensorflow function and not willing to use other python libraries. 

**Any Other info.**
If it is possible to compute macro f1 score in tensorflow using tf.contrib.metrics please let me know.

Thanks"
27170,Keras support for RaggedTensors,"# System Information
- TensorFlow version: 1.13.1 (issue present in 2.0 alpha
- Are you willing to contribute it: Yes

# Current State/Behaviour
`tf.RaggedTensor`s do a fantastic job of masking their internal representation from the user, allowing them to be used as regular tensors in most contexts. This does not extend to keras models however. As an example:

```python
import tensorflow as tf
from tensorflow.keras.layers import Input
from tensorflow.keras.layers import Lambda
from tensorflow.keras.models import Model
if not hasattr(tf, 'nest'):
    tf.nest = tf.contrib.framework.nest


values = Input(shape=(10,), dtype=tf.float32)

## non-ragged version
indices = Input(shape=(5,), dtype=tf.int64)
gathered = Lambda(lambda args: tf.gather(*args))([values, indices])
Model(inputs=(values, indices), outputs=gathered)
# works fine

## ragged version
index_values = Input(shape=(), dtype=tf.int64)
index_row_splits = Input(shape=(), dtype=tf.int64)

indices = tf.RaggedTensor.from_row_splits(index_values, index_row_splits)
gathered = Lambda(lambda args: tf.gather(*args))([values, indices])
Model(inputs=(values, indices), outputs=gathered)
# raises
# 1.13.1: ValueError: Input tensors to a Model must come from `tf.keras.Input`.
# Received: tf.RaggedTensor(values=Tensor(""input_3:0"", shape=(?,), dtype=int64),
# row_splits=Tensor(""input_4:0"", shape=(?,), dtype=int64))
# (missing previous layer metadata).
# 2.0: AttributeError: 'RaggedTensor' object has no attribute 'op'
```

# Workaround
The operations acting on the component tensors are fine. As a work around, one can do the following:

```python
def ragged_tensor_from_row_lengths(values, row_lengths):
    def components(args):
        values, row_lengths = args
        ragged = tf.RaggedTensor.from_row_lengths(values, row_lengths)
        return ragged.values, ragged.row_splits

    components = tf.keras.layers.Lambda(components)([values, row_lengths])
    return tf.RaggedTensor.from_row_splits(*components)


def as_ragged_components(tensor):
    if isinstance(tensor, tf.RaggedTensor):
        return dict(values=tensor.values, row_splits=tensor.row_splits)
    elif isinstance(tensor, (list, tuple)):
        return tuple(as_ragged_components(t) for t in tensor)
    elif isinstance(tensor, dict):
        return {k: as_ragged_components(v) for k, v in tensor.items()}
    else:
        # leave unchanged
        assert(isinstance(tensor, tf.Tensor))
        return tensor


def as_ragged(components):
    if isinstance(components, (list, tuple)):
        return tuple(as_ragged(c) for c in components)
    elif isinstance(components, dict):
        if all(k in components for k in ('values', 'row_splits')):
            return tf.RaggedTensor.from_row_splits(**components)
        else:
            return {k: as_ragged(v) for k, v in components.items()}
    else:
        assert(isinstance(components, tf.Tensor))
        return components


def ragged_lambda(fn, args):
    assert(isinstance(args, (list, tuple)))
    if not any(isinstance(a, tf.RaggedTensor) for a in args):
        out_components = tf.keras.layers.Lambda(fn)(args)
    else:
        components = as_ragged_components(args)
        flat_args = tf.nest.flatten(components)

        def actual_fn(flat_args):
            args = tf.nest.pack_sequence_as(components, flat_args)
            args = as_ragged(components)
            out = fn(args)
            return as_ragged_components(out)

        out_components = tf.keras.layers.Lambda(actual_fn)(flat_args)
    return as_ragged(out_components)


gathered = ragged_lambda(
    lambda args: tf.gather(*args), [values, indices])

gathered_components = tf.nest.flatten(as_ragged_components(gathered))
Model(
    inputs=(values, index_values, index_row_splits),
    outputs=gathered_components)
```

This is, however, very convoluted, and largely defeats the purpose of having compound tensors like `RaggedTensor`s that should be able to be used transparently in place of regular tensors.

# Change to API
Nothing, though will allow `RaggedTensor`s to be used in keras `Model`s.

# Who will benefit
People who appreciate clean code."
27168,"tf.dataset guide, TF 2.0 and one shot iterator","<

**System information**
- TensorFlow version: 2.0
- Doc Link: https://www.tensorflow.org/guide/datasets

Issue
 - The tf.dataset guide mentions using one shot iterator.  
 - The doc does not have a section to view by API release.  
"
27166,[TF 2.0 API Docs] tf.bitcast and tf.bitwise.*,"**System information**
- TensorFlow version: 2.0
- Doc Link: 
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/bitcast
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/bitwise/bitwise_and
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/bitwise/bitwise_or
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/bitwise/bitwise_xor
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/bitwise/bitwise_invert
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/bitwise/right_shift
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/bitwise/left_shift

**Describe the documentation issue**
* Correct links? No
* Clear Description?  Yes
* Usage Example? No
* Parameters Defined? Yes
* Returns defined? Yes
* Raises listed and defined? No
* Visuals, if applicable? No

same as #26492, #26532 documentation for tf.bitcast and tf.bitwise.* is created from a generated file, python/ops/gen_array_ops.py, and made incorrect link to source code.
The documentation can be modified by editing the appropriate .pbtxt files within the tensorflow/tensorflow/core/api_def/base_api directory of the source repository.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
yes"
27165,Error when compiling a keras model with top_k_categorical_accuracy metrics,"I am trying to compile a KERAS model for multi-class segmentation of 2D images with top_k_categorical_accuracy metrics, i.e.
`model.compile(optimizer='adadelta', 
              loss='categorical_crossentropy', 
              metrics=['accuracy', 'top_k_categorical_accuracy'])`
The error I get is:

> ValueError: Shape must be rank 2 but is rank 4 for 'metrics_2/top_k_categorical_accuracy/in_top_k/InTopKV2' (op: 'InTopKV2') with input shapes: [?,?,?,10], [?,?,?], [].

I am using one-hot encoding for the labels (10 is number of classes).
Interestingly, trying to use sparse_top_k_categorical_accuracy (which I am not supposed to use given my labels are not flattened) gives exactly the same error... 

I am using tensorflow 1.10.0

Does top_k_categorical_accuracy has any assumptions I am not aware of?  How can I make top_k_categorical_accuracy work for my model?"
27163,std::terminate may be called when environment is misconfigured while using C API,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 1.12
- Python version: N/A, C API is used
- Bazel version (if compiling from source): 0.16.1
- GCC/Compiler version (if compiling from source): 6.4.0/5.3.0
- CUDA/cuDNN version: libcuda comes from 418.43 drivers, cudnn 7.3.1
- GPU model and memory: nvidia, model/memory irrelevant

**Describe the current behavior**

When attempting to run a session via C API on a TensorRT-using graph, it is possible for exceptions to not get caught and abort the whole process via `std::terminate`. Stack-trace of one such exception, which happens when cudnn initialization fails, is provided below. Then the execution fails with:

```
terminate called after throwing an instance of 'nvinfer1::CudaError'
  what():  std::exception
fish: “./target/debug/foobar"" terminated by signal SIGABRT (Abort)
```

**Describe the expected behavior**

I would expect C API for `TF_SessionRun` and similar to return an error and caller to have an option to handle the error in some way that does not involve crashing.

**Code to reproduce the issue**

N/A, this is a failure of libtensorflow to return an error for system configuration issue.

**Other info / logs**

```
#0  0x00007fffca5d7b60 in __cxa_throw () from libstdc++.so.6
#1  0x00007fffc494fc23 in nvinfer1::throwCudaError(char const*, char const*, int, int, char const*) () from libnvinfer.so.5
#2  0x00007fffc495578e in nvinfer1::rt::initializeCommonContext(nvinfer1::rt::CommonContext&, nvinfer1::IGpuAllocator&, void*, void*, int) () from libnvinfer.so.5
#3  0x00007fffc495a4d4 in nvinfer1::rt::Engine::initialize() () from libnvinfer.so.5
#4  0x00007fffc495dc04 in nvinfer1::rt::Engine::deserialize(void const*, unsigned long, nvinfer1::IGpuAllocator&, nvinfer1::IPluginFactory*) () from libnvinfer.so.5
#5  0x00007fffc4946b33 in nvinfer1::Runtime::deserializeCudaEngine(void const*, unsigned long, nvinfer1::IPluginFactory*) () from libnvinfer.so.5
#6  0x00007fffd4d001e1 in tensorflow::tensorrt::TRTEngineOp::GetEngine(int, tensorflow::OpKernelContext*) () from libtftrt.so
#7  0x00007fffd4d03b8b in tensorflow::tensorrt::TRTEngineOp::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>) () from libtftrt.so
#8  0x00007fffd3bd9f11 in tensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>) () from libtensorflow_framework.so
#9  0x00007fffd3c20fbb in tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) () from libtensorflow_framework.so
#10 0x00007fffd3c212ef in std::_Function_handler<void (), tensorflow::(anonymous namespace)::ExecutorState::ScheduleReady(absl::InlinedVector<tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, 8ul, std::allocator<tensorflow::(anonymous namespace)::ExecutorState::TaggedNode> > const&, tensorflow::(anonymous namespace)::ExecutorState::TaggedNodeReadyQueue*)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from libtensorflow_framework.so
#11 0x00007fffd3c8a4c1 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) () from libtensorflow_framework.so
#12 0x00007fffd3c880e7 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from libtensorflow_framework.so
#13 0x00007fffca6026df in ?? () from libstdc++.so.6
#14 0x00007fffd5b11244 in start_thread () from libpthread.so.0
#15 0x00007fffd563bccf in clone () from libc.so.6
```"
27161,Documentation Request: WALSModel with shard,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: script built around docs
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a
- **TensorFlow installed from (source or binary)**: conda gpu version
- **TensorFlow version (use command below)**: 1.10
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: 10
- **GPU model and memory**: Quadro 16GiB
- **Exact command to reproduce**: `ex.run()` (see comments below)

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

This request comes with an associated [colab](https://colab.research.google.com/drive/1oJDfvIWnf7sY5uFcJ7XhTSEdA1n_3Jxv#scrollTo=8pqa0A7BUrG-)

Further, this documentation request stems from @walidk 's response on issue #26928 where 
it is suggested to shard the input matrix based on [wals_test.py](https://github.com/tensorflow/tensorflow/blob/f07558116ac7c90858cf0572a1bca1e50e208a37/tensorflow/contrib/factorization/python/ops/wals_test.py#L141)

While the code is relevant, it is not necessarily clear how to apply it to the [colab](https://colab.research.google.com/drive/1oJDfvIWnf7sY5uFcJ7XhTSEdA1n_3Jxv#scrollTo=8pqa0A7BUrG-)

Why?


1. there is `WALSModel` (used in the [colab](https://colab.research.google.com/drive/1oJDfvIWnf7sY5uFcJ7XhTSEdA1n_3Jxv#scrollTo=8pqa0A7BUrG-) ) and `WALSMatrixFactorization` (used in the linked test code). They seem the same, but what is the difference? Is that code compatible with `WALSModel`?

2. both `WALSModel` and `WALSMatrixFactorization` have `num_row_shards=1` / `num_col_shards=1` as arguments. It is unclear if these would automatically shard the input matrix.

3. as test code, it is not necessarily commented for pedagogical purposes

4. as test code, it is not exactly a MWE demonstrating just what is needed to handle shard an input matrix and train accordingly. Should both rows and cols be sharded at the same time? 

5. part of the test code may work against the goal of sharding (or it is also likely I do not full understand it):

e.g.

```python
....
 def _fn():
      ...
      sp_mat = self.np_array_to_sparse(np_matrix)
      sp_mat_t = sparse_ops.sparse_transpose(sp_mat) #<--- if goal is to shard input matrix because it is too large, why make 2?
      ...
...
```

I understand that this request it is a bit of an ""advanced""  documentation and that those contributing to TF are very busy - especially with v2. However I feel like this is a valid request as:

1. the current documentation for `WALSModel` and `WALSMatrixFactorization` is already more than MWE including session supervisors (although leaning more towards pseudo code in some places which may make it more confusing than need be)

2. the current documentation for the built in arguments `num_row_shards` / `col` state ""number of shards to use"", which gives no indication that the linked test code is even needed or where it should be introduced

3. the current documentation hints at sharding (e.g. `transposed_matrix_slices_from_queue_for_worker_shard`) but assumes the user knows how to set that up.



### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
27156,TFLite MicroInterpreter Crashes/Seg Faults,"**System information**
- Have I written custom code: Yes (custom model and custom code)
- OS Platform and Distribution: Mac OS X 10.13.6
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): v1.12.0-11117-gca1f0ce1b9 1.14.1-dev20190326
- Python version: 3.7.2
- GCC/Compiler version (if compiling from source): Apple LLVM version 10.0.0 (clang-1000.10.44.4)

**Describe the current behavior**
`./demo`

```
Model loaded!, Size:6052, Version:3
Model description: TOCO Converted.
Tensor arena allocated on stack
Allocator working (ish): Size=0
Segmentation fault: 11
```

The code fails when running this line: https://github.com/kmader/tflite_micro/blob/278c2899f05cc721133829d75f9e88ed87a3d33f/simple_app/demo.cc#L59-L60

**Describe the expected behavior**
`./demo`

```
Model loaded!, Size:6052, Version:3
Model description: TOCO Converted.
Tensor arena allocated on stack
Allocator working (ish): Size=0
Interpreter loaded!
Input #0: 1, 49, 40, 1, Type:UInt8, 3
Output #0: 1, 4, Type:UInt8, 3
Bad input tensor parameters in model
Model Results: [0]=0.411765, [1]=0.223529, [2]=0.160784, [3]=0.207843, 
```

**Code to reproduce the issue**
The repository has been simplified and binderized at https://github.com/kmader/tflite_micro/tree/master/simple_app. I have it setup so you can easily change the model from the example speech model `g_tiny_conv_micro_features_model_data` to a the model I am interested in `local_min_cnn_tflite`. The `demo.cc` is a modified and simplified version of https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/micro/examples/micro_speech/main.cc


**Other info / logs**
A brief look at the lldb output suggests the problem might be related to parsing the https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/micro/examples/micro_speech/main.cc g related to:
```
* thread #1, queue = 'com.apple.main-thread', stop reason = EXC_BAD_ACCESS (code=1, address=0x200015b10)
    frame #0: 0x000000010000463b demo`tflite::SimpleTensorAllocator::AllocateTensor(tflite::Tensor const&, int, int, flatbuffers::Vector<flatbuffers::Offset<tflite::Buffer> > const*, tflite::ErrorReporter*, TfLiteTensor*) [inlined] flatbuffers::IndirectHelper<long long>::Read(i=0) at flatbuffers.h:68 [opt]
   65  	  typedef T mutable_return_type;
   66  	  static const size_t element_stride = sizeof(T);
   67  	  static return_type Read(const uint8_t *p, uoffset_t i) {
-> 68  	    return EndianScalar((reinterpret_cast<const T *>(p))[i]);
   69  	  }
   70  	};
   71  	template<typename T> struct IndirectHelper<Offset<T>> {
Target 0: (demo) stopped.
```
- And the backtrace being
```
* thread #1, queue = 'com.apple.main-thread', stop reason = EXC_BAD_ACCESS (code=1, address=0x200015b10)
  * frame #0: 0x000000010000463b demo`tflite::SimpleTensorAllocator::AllocateTensor(tflite::Tensor const&, int, int, flatbuffers::Vector<flatbuffers::Offset<tflite::Buffer> > const*, tflite::ErrorReporter*, TfLiteTensor*) [inlined] flatbuffers::IndirectHelper<long long>::Read(i=0) at flatbuffers.h:68 [opt]
    frame #1: 0x000000010000463b demo`tflite::SimpleTensorAllocator::AllocateTensor(tflite::Tensor const&, int, int, flatbuffers::Vector<flatbuffers::Offset<tflite::Buffer> > const*, tflite::ErrorReporter*, TfLiteTensor*) [inlined] flatbuffers::Vector<long long>::Get(i=0) const at flatbuffers.h:199 [opt]
    frame #2: 0x000000010000463b demo`tflite::SimpleTensorAllocator::AllocateTensor(this=<unavailable>, flatbuffer_tensor=0x0000000100016280, create_before=<unavailable>, destroy_after=<unavailable>, buffers=0x000000010001585c, error_reporter=0x00007ffeefbf57a8, result=0x00007ffeefbf5840) at simple_tensor_allocator.cc:135 [opt]
    frame #3: 0x00000001000032e5 demo`tflite::MicroInterpreter::MicroInterpreter(this=0x00007ffeefbf36a8, model=<unavailable>, op_resolver=<unavailable>, tensor_allocator=<unavailable>, error_reporter=0x00007ffeefbf57a8) at micro_interpreter.cc:89 [opt]
    frame #4: 0x0000000100000eb8 demo`main(argc=1, argv=0x00007ffeefbff870) at demo.cc:57
    frame #5: 0x00007fff78ccf015 libdyld.dylib`start + 1
```

"
27155,Built TF 1.13.1 - libcublas.so.10.1: cannot open shared object file with classify_image.py,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Slackware Linux 64 -current (14.2+)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 1.13.1
- Python version: 3.7.2
- Bazel version (if compiling from source): 0.21.0- (@non-git)
- GCC/Compiler version (if compiling from source): gcc-8.3.0-x86_64
- CUDA/cuDNN version: CUDA Version 10.1.105
- GPU model and memory: TITAN X (Pascal) 12 GB


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
Running a tutorial sample `tensorflow/models-git/tutorials/image/imagenet]$ python3 classify_image.py` causes error:

```
2019-03-26 08:22:44.464647: I tensorflow/stream_executor/dso_loader.cc:142] Couldn't open CUDA library libcublas.so.10.1. LD_LIBRARY_PATH: 
2019-03-26 08:22:44.465972: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Failed precondition: could not dlopen DSO: libcublas.so.10.1; dlerror: libcublas.so.10.1: cannot open shared object file: No such file or directory
Aborted
```

**Describe the expected behavior**
Sample tutorial runs

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
`tensorflow/models-git/tutorials/image/imagenet]$ python3 classify_image.py`

**Other info / logs**
This issue seems different from the other hundreds (thousands?) of similar reports. Most other similar issues are due to a mismatch between a binary (pre-compiled) tensorflow and the CUDA version installed by the user.

In my case, I successfully compiled TF 1.13.1 from source with GPU support. The tensorflow module loads in python3.

I can resolve the issue with running this sample by setting `LD_LIBRARY_PATH=/opt/nvidia/cuda/lib64` but this should NOT be necessary because I already configured this path in `/etc/ld.so.conf.d/cuda.conf` and ran `$ ldconfig`.

```
$ python3
Python 3.7.2 (default, Feb 19 2019, 23:15:48) 
[GCC 8.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
>>> 
```

```
$ cat /etc/ld.so.conf.d/cuda.conf 
/opt/nvidia/cuda/lib64
/opt/nvidia/cuda/extras/CUPTI/lib64
/opt/nvidia/nccl/lib
```

```
$ ls -al /opt/nvidia/cuda/lib64/libcublas.so.10.1
lrwxrwxrwx 1 root root 23 Mar 26 08:01 /opt/nvidia/cuda/lib64/libcublas.so.10.1 -> libcublas.so.10.1.0.105
```

```
$ ldconfig -p | grep libcublas
	libcublasLt.so.10 (libc6,x86-64) => /opt/nvidia/cuda/lib64/libcublasLt.so.10
	libcublasLt.so (libc6,x86-64) => /opt/nvidia/cuda/lib64/libcublasLt.so
	libcublas.so.10 (libc6,x86-64) => /opt/nvidia/cuda/lib64/libcublas.so.10
	libcublas.so (libc6,x86-64) => /opt/nvidia/cuda/lib64/libcublas.so
```"
27154,SmartReply  Tensorflow Lite ,I'm visited in github repo on tensorflow but there are showing source code batch files not found when i want try to clone repo...
27152,Error importing tensorflow on windows 10 ( Tensorflow 1.13.1  python3.7.2 ),"System information

OS Platform and Distribution : Windows 10, 64-bit
TensorFlow installed from (source or binary): source
TensorFlow version: 1.13.1
Python version: 3.7.2
Installed using virtualenv? pip? conda?: pip on a conda env

I want to run a python file that uses tensorflow, but I keep getting this error.

Traceback (most recent call last):
  File ""C:\Users\popco\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
  File ""C:\Users\popco\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 297, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\popco\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 66, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\popco\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\popco\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow
ModuleNotFoundError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\popco\DeepLearning\Science Fair\progressive_growing_of_gans\dataset_tool.py"", line 16, in <module>
    import tensorflow as tf
  File ""C:\Users\popco\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\popco\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 72, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\popco\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
  File ""C:\Users\popco\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 297, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\popco\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 66, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\popco\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\popco\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow
ModuleNotFoundError: No module named '_pywrap_tensorflow'"
27150,keras.metrics.MeanIoU - AttributeError: 'list' object has no attribute 'shape',"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-alpha0
- Python version: 3.7.2
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: not relevant
- GPU model and memory: not relevant

**Describe the current behavior**
The example given for tf.keras.metrics.MeanIoU returns `AttributeError: 'list' object has no attribute 'shape'`. If I convert the lists to tf arrays as input, it runs without error.

**Describe the expected behavior**
The example given in the tf.keras.metrics.MeanIoU documentation should run without error.

**Code to reproduce the issue**
This is the example given in the docs with error message returned when ran:

```python
m = tf.keras.metrics.MeanIoU(num_classes=2)
m.update_state([0, 0, 1, 1], [0, 1, 0, 1])
# cm = [[1, 1],
#      [1, 1]]
# sum_row = [2, 2], sum_col = [2, 2], true_positives = [1, 1]
# iou = true_positives / (sum_row + sum_col - true_positives))
# result = (1 / (2 + 2 - 1) + 1 / (2 + 2 - 1)) / 2 = 0.33
print('Final result: ', m.result().numpy())  # Final result: 0.33
```

```python
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-29-75e080c0562a> in <module>
      1 m = tf.keras.metrics.MeanIoU(num_classes=2)
----> 2 m.update_state([0, 0, 1, 1], [0, 1, 0, 1])
      3 # cm = [[1, 1],
      4 #      [1, 1]]
      5 # sum_row = [2, 2], sum_col = [2, 2], true_positives = [1, 1]

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/utils/metrics_utils.py in decorated(metric_obj, *args, **kwargs)
     69     """"""Decorated function with `add_update()`.""""""
     70 
---> 71     update_op = update_state_fn(*args, **kwargs)
     72     if update_op is not None:  # update_op will be None in eager execution.
     73       metric_obj.add_update(update_op, inputs=True)

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/metrics.py in update_state(self, y_true, y_pred, sample_weight)
   2273     """"""
   2274     # Flatten the input if its rank > 1.
-> 2275     if y_pred.shape.ndims > 1:
   2276       y_pred = array_ops.reshape(y_pred, [-1])
   2277 

AttributeError: 'list' object has no attribute 'shape'
```
"
27149,how to generate batch pictures of myselt dataset," Based on MNIST recognition project,I want to train myself dataset,I want to generate more pictures on the basis of the existing captured pictures. thank you for your reply as soon as possible.
 "
27147,AttentionMechanism not be supported in Eager Mode,"When I want to add certain classical AttetionMechansim through Tensorflow API, I find that Tensorflow 's AttentionMechansim API dose not support Eager Mode for the reason that ""memory"" parameter is in the attention mechiansim s' __init__ method. Because in general, I construct a AttentionMechanism in static graph mode, it must be constructed transferring a parameter ""memory"" , a tensor whose value is decided by the model input. However, in Eager Mode,  when constructing a AttentionMechanism, ""memory"" is deterministically taking real values, which enables me not to change to a new memory values in a new training iteration.
"
27146,Should `fold_constants` be before or after `flatten_atrous_conv`,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: v1.13.1
- Doc Link: https://github.com/tensorflow/tensorflow/blob/v1.13.1/tensorflow/tools/graph_transforms/README.md#flatten_atrous_conv


**Describe the documentation issue**
The `flatten_atrous_conv` section lists `Prerequisites: fold_constants`, whereas in the main paragraph, it says `You will need to make sure you run fold_constants after this transform.`
So should `fold_constants` be before or after `flatten_atrous_conv`?

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
I assume it's before `flatten_atrous_conv`, so I submitted a PR.
https://github.com/tensorflow/tensorflow/pull/27145"
27144,CUDNN_STATUS_INTERNAL_ERROR on GTX 1660 TI,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, simple dummy code
```
import tensorflow as tf

def main():
    config = tf.ConfigProto()
    
    sess = tf.Session(config=config)

    x = tf.random.uniform([16, 64, 64, 3])
    net = x
    net = tf.layers.conv2d(net, filters=16, kernel_size=3, padding='same', strides=2)
    net = tf.layers.flatten(net)
    net = tf.layers.dense(net, 1)


    sess.run(tf.global_variables_initializer())
    
    out = sess.run(net)
    print(out)

if __name__ == '__main__':
    main()
```

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): Both source and compiled tested
- TensorFlow version (use command below): v1.13.1-0-g6612da8951
- Python version: Tested on both 3.5 and 3.6
- Bazel version (if compiling from source): 0.19.2
- GCC/Compiler version (if compiling from source): gcc-6 (Ubuntu 6.5.0-2ubuntu1~18.04) 6.5.0 20181026
- CUDA/cuDNN version: Tested with 10.0/7.5 and 10.1/7.5
- GPU model and memory: GTX 1660 TI 6GB


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
Using any convolution layers fails. Same error with all both versions of CUDA. `~/.nv` directory cleared between runs. Same error in docker images as well. CUDNN verified to be working correctly with simple CUDNN programs (e.g. [this](https://gist.github.com/odashi/1c20ba90388cf02330e1b95963d78039))

**Describe the expected behavior**
Code should print an array of 16 numbers

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
```
2019-03-26 11:14:43.230340: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-03-26 11:14:43.330837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-03-26 11:14:43.331452: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x3d4a090 executing computations on platform CUDA. Devices:
2019-03-26 11:14:43.331472: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce GTX 1660 Ti, Compute Capability 7.5
2019-03-26 11:14:43.356898: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3400255000 Hz
2019-03-26 11:14:43.357161: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x3db1d00 executing computations on platform Host. Devices:
2019-03-26 11:14:43.357190: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2019-03-26 11:14:43.357646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: 
name: GeForce GTX 1660 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.8
pciBusID: 0000:01:00.0
totalMemory: 5.77GiB freeMemory: 5.19GiB
2019-03-26 11:14:43.357674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2019-03-26 11:14:43.358609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-03-26 11:14:43.358630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2019-03-26 11:14:43.358640: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
2019-03-26 11:14:43.359045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5016 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)
WARNING:tensorflow:From simpletf_test/tf_convnet.py:11: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.conv2d instead.
WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From simpletf_test/tf_convnet.py:12: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:From simpletf_test/tf_convnet.py:13: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
2019-03-26 11:14:43.819308: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally
2019-03-26 11:14:44.479283: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2019-03-26 11:14:44.498430: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1334, in _do_call
    return fn(*args)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[{{node conv2d/Conv2D}}]]
	 [[{{node dense/BiasAdd}}]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""simpletf_test/tf_convnet.py"", line 22, in <module>
    main()
  File ""simpletf_test/tf_convnet.py"", line 18, in main
    out = sess.run(net)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 929, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1328, in _do_run
    run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[node conv2d/Conv2D (defined at simpletf_test/tf_convnet.py:11) ]]
	 [[node dense/BiasAdd (defined at simpletf_test/tf_convnet.py:13) ]]

Caused by op 'conv2d/Conv2D', defined at:
  File ""simpletf_test/tf_convnet.py"", line 22, in <module>
    main()
  File ""simpletf_test/tf_convnet.py"", line 11, in main
    net = tf.layers.conv2d(net, filters=16, kernel_size=3)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py"", line 324, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/convolutional.py"", line 424, in conv2d
    return layer.apply(inputs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 1227, in apply
    return self.__call__(inputs, *args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py"", line 530, in __call__
    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 554, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/convolutional.py"", line 194, in call
    outputs = self._convolution_op(inputs, self.kernel)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py"", line 966, in __call__
    return self.conv_op(inp, filter)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py"", line 591, in __call__
    return self.call(inp, filter)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py"", line 208, in __call__
    name=self.name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_nn_ops.py"", line 1026, in conv2d
    data_format=data_format, dilations=dilations, name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 3300, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 1801, in __init__
    self._traceback = tf_stack.extract_stack()

UnknownError (see above for traceback): Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[node conv2d/Conv2D (defined at simpletf_test/tf_convnet.py:11) ]]
	 [[node dense/BiasAdd (defined at simpletf_test/tf_convnet.py:13) ]]

```"
27143,[TF 2.0] tf 2 around 40 times slower than tf 1 for unrolled taylor series,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 1.13.1
- Python version: Python 3.6.7
- CUDA/cuDNN version: 10.0/7.4
- GPU model and memory: GeForce GTX 1060, Compute Capability 6.1, 6GB RAM

This issue follows on from https://github.com/tensorflow/tensorflow/issues/26807

**Describe the current behavior**

The code linked at the bottom calculates a matrix exponential (`expm`) using a [taylor series](https://en.wikipedia.org/wiki/Taylor_series#Exponential_function). It outputs the following when running with the latest TF1 and TF2 installations from pip:

```shell
(tf1) $ python expm.py 
BENCHMARKS:
tf took 0.1590 seconds for 25 iterations
taylor took 0.0158 seconds for 25 iterations

(tf2) $ python expm.py 
BENCHMARKS:
tf took 0.7530 seconds for 25 iterations
taylor took 1.0743 seconds for 25 iterations
taylor_v2 took 0.6025 seconds for 25 iterations
PROFILES:
profile for taylor_v2
         2251 function calls in 0.634 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
       25    0.000    0.000    0.634    0.025 expm.py:74(<lambda>)
       25    0.000    0.000    0.634    0.025 /home/jeff/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py:400(__call__)
       25    0.000    0.000    0.634    0.025 /home/jeff/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py:1267(__call__)
       25    0.000    0.000    0.632    0.025 /home/jeff/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py:542(_filtered_call)
       25    0.000    0.000    0.632    0.025 /home/jeff/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py:560(_call_flat)
       25    0.000    0.000    0.630    0.025 /home/jeff/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py:379(call)
       25    0.000    0.000    0.630    0.025 /home/jeff/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/execute.py:33(quick_execute)
       25    0.630    0.025    0.630    0.025 {built-in method _pywrap_tensorflow_internal.TFE_Py_Execute}
       25    0.000    0.000    0.001    0.000 /home/jeff/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py:1505(_maybe_define_function)
   ...
```

I've included some profiling information for the function of interest (`taylor_v2` in TF 2) which shows that it spends all the time in `_pywrap_tensorflow_internal.TFE_Py_Execute`.

**Describe the expected behavior**

I expect TF 1 `taylor` (which uses `tf.Session.run`) and TF 2 `taylor_v2` (which uses `tf.function`) to have similar performance.

**Code to reproduce the issue**

``` python
import tensorflow as tf
import numpy as np
import time as tm
import cProfile as pr
import io
import pstats as ps

MATRIX_DIM = 1000
TAYLOR_SUM = 30
EPS = 1e-2
WARMUP = 10
ITERATIONS = 25


def benchmark(name, fn):
    for _ in range(WARMUP):
        value = fn()
    start = tm.time()
    for _ in range(ITERATIONS):
        value = fn()
    end = tm.time()
    runtime = end - start
    print(f""{name} took {runtime:.4f} seconds for {ITERATIONS} iterations"")
    return value


def profile(name, fn):
    for _ in range(WARMUP):
        value = fn()

    profile = pr.Profile()
    profile.enable()

    for _ in range(ITERATIONS):
        value = fn()

    profile.disable()
    s = io.StringIO()
    sortby = ""cumulative""
    stats = ps.Stats(profile, stream=s).sort_stats(sortby)
    stats.print_stats()
    print(f""profile for {name}"")
    print(s.getvalue())

    return value


def taylor_expm(x, n):
    x_0 = tf.eye(tf.shape(x)[0], dtype=x.dtype)
    x_i = x
    y = x_0 + x_i
    for i in range(2, n + 1):
        x_i = (x_i @ x) / tf.cast(i, x.dtype)
        y = y + x_i
    return y


np.random.seed(42)

x = tf.constant(np.random.uniform(-0.5, 0.5, [MATRIX_DIM, MATRIX_DIM]), tf.float32)

if tf.__version__.startswith(""2""):

    @tf.function
    def taylor_expm_v2(x, n):
        return taylor_expm(x, n)

    print(""\nBENCHMARKS:"")
    tf_expm = benchmark(""tf"", lambda: tf.linalg.expm(x))
    my_expm = benchmark(""taylor"", lambda: taylor_expm(x, TAYLOR_SUM))
    my_expm_v2_b = benchmark(""taylor_v2"", lambda: taylor_expm_v2(x, TAYLOR_SUM))

    print(""\nPROFILES:"")
    my_expm_v2_p = profile(""taylor_v2"", lambda: taylor_expm_v2(x, TAYLOR_SUM))

    np.testing.assert_allclose(tf_expm.numpy(), my_expm.numpy(), atol=EPS)
    np.testing.assert_allclose(tf_expm.numpy(), my_expm_v2_b.numpy(), atol=EPS)
    np.testing.assert_allclose(tf_expm.numpy(), my_expm_v2_p.numpy(), atol=EPS)
else:
    tf_expm_ = tf.linalg.expm(x)
    my_expm_ = taylor_expm(x, TAYLOR_SUM)

    with tf.Session() as sess:
        print(""\nBENCHMARKS:"")
        tf_expm = benchmark(""tf"", lambda: sess.run(tf_expm_))
        my_expm = benchmark(""taylor"", lambda: sess.run(my_expm_))

    np.testing.assert_allclose(tf_expm, my_expm, atol=EPS)
```
"
27142,AttributeError: 'LeakyReLU' object has no attribute '__name__' when running the train model.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
Docker container from tensorflow:2.0.0a0-gpu-py3



**Describe the current behavior**
I use leaky relu activation from advanced_activations module like this.
` from tensorflow.python.keras.layers.advanced_activations import LeakyReLU `
` x1 = Conv2D(filters=32, kernel_size=(8, 8), strides=(2, 2), activation=learky_relu)(inputs) `

And error occurred as follow when run the train my model.
```
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/activations.py"", line 206, in serialize
    if activation.__name__ in _TF_ACTIVATIONS_V2:
AttributeError: 'LeakyReLU' object has no attribute '__name__'
```

So the leaky relu instance is sent to `activations.serialize` and called attribute '__name__' at https://github.com/tensorflow/tensorflow/blob/1e43727d9c4ef56c66af8c63ecca9d8465b786c7/tensorflow/python/keras/activations.py#L255.
But common python objects aren't have attribute '__name__' so this bug is occurred.

** expected behavior **
My model should get 100% accuracy and my self-driving RC move as like human driving. 



I don't know why advanced_activation classes are implement the Layer. 
Maybe I think it better to make abstract class for advanced activation or make them independent function.
I want it is fixed as soon as possible.
good luck.

More info.
My train code is as follow.
```
    def train(self, X, y, saved_model_path, batch_size=8, epochs=100,  train_split=0.8, verbose=1, min_delta=.0005, patience=5, use_early_stop=True):
        """"""
        Args:
            train: list of traininig data
            validate: list of validation data
            saved_model_path: saved previous model path
        """"""
        
        # checkpoint to save model after each epoch
        save_best = ModelCheckpoint(saved_model_path,
                                    monitor='val_loss',
                                    verbose=verbose,
                                    save_best_only=True,
                                    mode='min')

        # stop training if the validation error stops improving.
        early_stop = EarlyStopping(monitor='val_loss',
                                    min_delta=min_delta,
                                    patience=patience,
                                    verbose=verbose,
                                    mode='auto')

        callbacks_list = [save_best]
        if use_early_stop:
            callbacks_list.append(early_stop)

        hist = self.model.fit(
            X,
            y,
            batch_size=batch_size,
            epochs=epochs,
            verbose=1,
            validation_split=0.2,
            callbacks=callbacks_list,
        )
        return hist
```

See My model code as follow.

```
from tensorflow.python.keras import Input
from tensorflow.python.keras.layers import Conv2D, Flatten, Dense, MaxPool2D
from tensorflow.python.keras.layers.advanced_activations import LeakyReLU
from tensorflow.python.keras.models import Model
from tensorflow.python.keras.utils import plot_model
from tensorflow.python.keras.callbacks import ModelCheckpoint, EarlyStoppin

def linear():
    learky_relu = LeakyReLU()
    inputs = Input(shape=(391, 554, 3), name='inputs')
    x1 = Conv2D(filters=32, kernel_size=(8, 8), strides=(2, 2), activation=learky_relu)(inputs)
    x2 = MaxPool2D(pool_size=(4, 4), strides=2)(x1)
    x3 = Conv2D(filters=32, kernel_size=(4, 4), strides=(3, 3), activation=learky_relu)(x2)
    x4 = MaxPool2D(pool_size=(3, 3), strides=2)(x3)
    x5 = Conv2D(filters=48, kernel_size=(4, 4), strides=(2, 2), activation=learky_relu)(x4)
    x6 = MaxPool2D(pool_size=(2, 2), strides=2)(x5)
    x7 = Flatten(name='flattened')(x6)
    d1 = Dense(units=100, activation='linear')(x7)
    d2 = Dense(units=50, activation='linear')(d1)
    handling = Dense(units=1, activation='linear', name='handling')(d2)
    model = Model(inputs=[inputs], outputs=handling)
    model.compile(optimizer='adam', loss={'handling': 'mean_squared_error'} , loss_weights={'handling': 0.5 })
    return model

```

"
27141,Tensorflow 2.0 failed to get convolution algorithm,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Fedora 29
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.0.0-alpha
- Python version: 3.6.6
- Installed using virtualenv? pip? conda?: virtualenv and pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA-10.0, cuDNN 7.4.2 and 7.5.0
- GPU model and memory: RTX 2080, 8 GB



**Describe the problem**

Installed tensorflow 2.0 using pip in a virtualenv. Trying to train a model gives the following error message(tried both cuDNN 7.4 and 7.5)

2019-03-26 14:44:26.468142: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-03-26 14:44:26.471383: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-03-26 14:44:26.580198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1009] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-03-26 14:44:26.580761: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x1e8c600 executing computations on platform CUDA. Devices:
2019-03-26 14:44:26.580775: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): GeForce RTX 2080, Compute Capability 7.5
2019-03-26 14:44:26.593170: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3696000000 Hz
2019-03-26 14:44:26.593891: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x1ee6d40 executing computations on platform Host. Devices:
2019-03-26 14:44:26.593903: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): <undefined>, <undefined>
2019-03-26 14:44:26.594049: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1467] Found device 0 with properties: 
name: GeForce RTX 2080 major: 7 minor: 5 memoryClockRate(GHz): 1.8
pciBusID: 0000:01:00.0
totalMemory: 7.76GiB freeMemory: 7.21GiB
2019-03-26 14:44:26.594061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Adding visible gpu devices: 0
2019-03-26 14:44:26.594091: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-03-26 14:44:26.594521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1015] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-03-26 14:44:26.594530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021]      0 
2019-03-26 14:44:26.594534: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 0:   N 
2019-03-26 14:44:26.594620: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7014 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080, pci bus id: 0000:01:00.0, compute capability: 7.5)
2019-03-26 14:44:26.890272: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-03-26 14:44:27.510334: E tensorflow/stream_executor/cuda/cuda_dnn.cc:338] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2019-03-26 14:44:27.514389: E tensorflow/stream_executor/cuda/cuda_dnn.cc:338] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/atom/PythonEnvs/Tensorflow_2_alpha/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 660, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/home/atom/PythonEnvs/Tensorflow_2_alpha/lib/python3.6/site-packages/tensorflow/python/keras/layers/convolutional.py"", line 196, in call
    outputs = self._convolution_op(inputs, self.kernel)
  File ""/home/atom/PythonEnvs/Tensorflow_2_alpha/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 1078, in __call__
    return self.conv_op(inp, filter)
  File ""/home/atom/PythonEnvs/Tensorflow_2_alpha/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 634, in __call__
    return self.call(inp, filter)
  File ""/home/atom/PythonEnvs/Tensorflow_2_alpha/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 233, in __call__
    name=self.name)
  File ""/home/atom/PythonEnvs/Tensorflow_2_alpha/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 1951, in conv2d
    name=name)
  File ""/home/atom/PythonEnvs/Tensorflow_2_alpha/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 1031, in conv2d
    data_format=data_format, dilations=dilations, name=name, ctx=_ctx)
  File ""/home/atom/PythonEnvs/Tensorflow_2_alpha/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 1130, in conv2d_eager_fallback
    ctx=_ctx, name=name)
  File ""/home/atom/PythonEnvs/Tensorflow_2_alpha/lib/python3.6/site-packages/tensorflow/python/eager/execute.py"", line 66, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. [Op:Conv2D]  

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Minimal code snippet which raises the error:
    tf.keras.layers.Conv2D(3,3,1)(tf.ones((1,5,5,3)))

"
27139,"save_weights/load_weights with custom layer does not work in tensorflow 1.13.1, but work in tf 2.0.0a0","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, but very basic
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary (conda)
- TensorFlow version (use command below): 1.13.1 and 2.0.0a0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
In summary, I created a custom model in Model Subclassing style, which contained only 1 custom layer. After initial, I dumped its trainable weights to file and then restored it, by using save_weights and load_weights functions. The trainable weights before and after saving were different.

I also ran the same test on Tensorflow 2.0.0a0, and it turned out the latter version did not get this phenomenon. 
**Describe the expected behavior**
The trainable weights before and after saving should be the same, as they are in TF 2.0.0a0.

**Code to reproduce the issue**
My custom layer:
```
class EncodingLayer(tf.keras.layers.Layer):
    def __init__(self, out_size):
        super().__init__()
        self.rnn_layer = tf.keras.layers.GRU(out_size, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')

    def call(self, X, **kwargs):
        output, state = self.rnn_layer(X)
        return output, state
```
The main part:
```
class EncodingModel(tf.keras.Model):

    def __init__(self):
        super().__init__()
        self.encoder_layer = EncodingLayer(out_size=1)

    def infer(self, inputs):
        output, state = self.encoder_layer(inputs)
        return output


if __name__ == '__main__':
    # Comment line below for running in TF 2.0
    tf.enable_eager_execution()

    # shape == (2, 3, 2)
    inputs = tf.convert_to_tensor([
        [[1., 2.], [2., 3.], [4., 4.]],
        [[1., 2.], [2., 3.], [4., 4.]],
    ])

    model = EncodingModel()

    # Just for building the graph
    model.infer(inputs)

    print('Before saving model: ', model.trainable_weights[0].numpy().mean())
    model.save_weights('weight')

    new_model = EncodingModel()
    new_model.infer(inputs)
    new_model.load_weights('weight')
    print('Loaded model: ', new_model.trainable_weights[0].numpy().mean())
```

**Other info / logs**
The result when running in TF 1.13.1:
```
Before saving model:  0.28864467
Loaded model:  0.117300846
```
The result when running in TF 2.0.0a0:
```
Before saving model:  -0.06922924
Loaded model:  -0.06922924
```"
27138,"Add sigma, k1, k2, etc. as default parameters to tf.image.ssim_multiscale","I previously requested a feature on tf.image.ssim for window size and sigma on this link. https://github.com/tensorflow/tensorflow/issues/26929 
Please read it for additional information.

I would like to request for variables of multiscale ssim (such as sigma, k1, k2, etc.) to be listed as defaults (instead of being fixed inside the script) as well.


**System information**
- TensorFlow version (you are using): 1.13
- Are you willing to contribute it (Yes/No): No. Sorry, I am not qualified to alter the code.



**Describe the feature and the current behavior/state.**

Currently, key parameters such as the sigma value are fixed when Tensorflow runs tf.image.ssim_multiscale. 

It would be great if all the variables mentioned in the original paper proposing MS-SSIM could be altered by the user.
(See Z. Wang, E. P. Simoncelli and A. C. Bovik, “Multi-scale structural similarity for image quality assessment,” IEEE Asilomar Conference Signals, Systems and Computers, Nov. 2003.)

This is especially important for those who wish to use MS-SSIM as a loss function as those variables have a significant influence on the character of the outputs.

**Will this change the current api? How?**
There will be almost no change. If the parameters which are currently fixed are set as default values, most people will never notice the difference.

**Who will benefit with this feature?**
Those who wish to use MS-SSIM as a loss function or wish to fine tune it on a deep learning model.

**Any Other info.**
I do not understand all of the features of MS-SSIM perfectly. I understand that a minimum window size of 176x176 is fixed and that K1 and K2 parameters are usually not altered. Perhaps these values should be set as keyword arguments (**kwargs) to hide unnecessary complexity.
"
27135,TFLite got same result whatever feed any image,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: 1+6T
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10.0
- GPU model and memory: GTX 1050 Ti


**Describe the current behavior**
I used tf.keras built a FaceNet model and try to deploy it in Android TFLite, the processing of the transforming(from tf.keras to tflite) was smoothed, but when I try to deploy it in Android TFLite, whatever I feed any image, the Interpreter always send me the same result
**Describe the expected behavior**

**Code to reproduce the issue**
``` java
package org.blackwalnutlabs.angel.tensorflowmobile.model;

import android.app.Activity;
import android.content.res.AssetFileDescriptor;
import android.graphics.Bitmap;
import android.util.Log;

import org.opencv.android.Utils;
import org.opencv.core.Mat;
import org.tensorflow.lite.Delegate;
import org.tensorflow.lite.Interpreter;

import java.io.FileInputStream;
import java.io.IOException;
import java.nio.ByteBuffer;
import java.nio.ByteOrder;
import java.nio.MappedByteBuffer;
import java.nio.channels.FileChannel;
import java.util.Map;

import static org.blackwalnutlabs.angel.tensorflowmobile.setting.ModelSetting.DIM_BATCH_SIZE;
import static org.blackwalnutlabs.angel.tensorflowmobile.setting.ModelSetting.DIM_IMG_SIZE_X;
import static org.blackwalnutlabs.angel.tensorflowmobile.setting.ModelSetting.DIM_IMG_SIZE_Y;
import static org.blackwalnutlabs.angel.tensorflowmobile.setting.ModelSetting.DIM_PIXEL_SIZE;
import static org.blackwalnutlabs.angel.tensorflowmobile.setting.ModelSetting.IMAGE_STD;
import static org.blackwalnutlabs.angel.tensorflowmobile.setting.ModelSetting.MODELFILE;

public class TensorFlowLiteDetector {
    private static final String TAG = ""Detector"";

    // 存储图像 RGB 值
    private int[] intValues = new int[DIM_IMG_SIZE_X * DIM_IMG_SIZE_Y];
    // TFLite 执行类
    private Interpreter tfLite;
    // 存储图像位数据
    private ByteBuffer imgData;
    // 存储预测结果
    private float[][] embeddingArray;
    private final Interpreter.Options tfliteOptions = new Interpreter.Options();

    private Delegate gpuDelegate = null;
    private MappedByteBuffer modelFile = null;


    // 初始化 TFLite
    public TensorFlowLiteDetector(Map<String, Object> othersMap) {
        try {
            Activity activity = (Activity) othersMap.get(""activity"");
            modelFile = loadModelFile(activity);
            tfLite = new Interpreter(modelFile, tfliteOptions);
            useCPU();
            imgData = ByteBuffer.allocateDirect(
                    DIM_BATCH_SIZE * DIM_IMG_SIZE_X * DIM_IMG_SIZE_Y * DIM_PIXEL_SIZE * 4);
            imgData.order(ByteOrder.nativeOrder());
            embeddingArray = new float[1][128];
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    // 读取模型文件
    private MappedByteBuffer loadModelFile(Activity activity) throws IOException {
        AssetFileDescriptor fileDescriptor = activity.getAssets().openFd(MODELFILE);
        FileInputStream inputStream = new FileInputStream(
                fileDescriptor.getFileDescriptor()
        );
        FileChannel fileChannel = inputStream.getChannel();
        long startOffset = fileDescriptor.getStartOffset();
        long declaredLength = fileDescriptor.getDeclaredLength();
        return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);
    }

    // 格式化传入图像像素值
    private void convertBitmapToByteBuffer(Bitmap bitmap) {
        if (imgData == null) {
            return;
        }
        imgData.rewind();
        bitmap.getPixels(intValues, 0, bitmap.getWidth(),
                0, 0, bitmap.getWidth(), bitmap.getHeight());

        int pixel = 0;
        for (int i = 0; i < DIM_IMG_SIZE_X; ++i) {
            for (int j = 0; j < DIM_IMG_SIZE_Y; ++j) {
                final int val = intValues[pixel++];
                imgData.putFloat(((val >> 16) & 0xFF) / IMAGE_STD);
                imgData.putFloat(((val >> 8) & 0xFF) / IMAGE_STD);
                imgData.putFloat(((val) & 0xFF) / IMAGE_STD);
            }
        }
    }

    // 执行预测
    public float[][] detectImage(Mat src) {
        if (tfLite != null) {
            Bitmap bmp = Bitmap.createBitmap(src.width(), src.height(),
                    Bitmap.Config.ARGB_8888);
            Utils.matToBitmap(src, bmp);
            convertBitmapToByteBuffer(bmp);

            tfLite.resizeInput(0, new int[]{1, 96, 96, 3});
            tfLite.run(imgData, embeddingArray);

            return embeddingArray;
        }
        return null;
    }

    /**
     * Enables use of the GPU for inference, if available.
     */
    private void useGpu() {
        Log.e(TAG, """" + GpuDelegateHelper.isGpuDelegateAvailable());
        if (gpuDelegate == null && GpuDelegateHelper.isGpuDelegateAvailable()) {
            gpuDelegate = GpuDelegateHelper.createGpuDelegate();
            tfliteOptions.addDelegate(gpuDelegate);
            recreateInterpreter();
        }
    }

    /**
     * Enables use of the CPU for inference.
     */
    private void useCPU() {
        tfliteOptions.setUseNNAPI(false);
        recreateInterpreter();
    }

    /**
     * Enables use of NNAPI for inference, if available.
     */
    private void useNNAPI() {
        tfliteOptions.setUseNNAPI(true);
        recreateInterpreter();
    }

    /**
     * Adjusts the number of threads used in CPU inference.
     */
    private void setNumThreads(int numThreads) {
        tfliteOptions.setNumThreads(numThreads);
        recreateInterpreter();
    }

    private void recreateInterpreter() {
        if (tfLite != null) {
            tfLite.close();
            // TODO(b/120679982)
            // gpuDelegate.close();
            tfLite = new Interpreter(modelFile, tfliteOptions);
        }
    }

    /**
     * Closes the interpreter and model to release resources.
     */
    private void close() {
        if (tfLite != null) {
            tfLite.close();
            tfLite = null;
        }
        modelFile = null;
    }
}

```

[converted_model.zip](https://github.com/tensorflow/tensorflow/files/3006839/converted_model.zip)

```
2019-03-26 15:01:46.346 11635-11823/org.blackwalnutlabs.angel.facedetection E/MainActivity: --------------
2019-03-26 15:01:46.347 11635-11823/org.blackwalnutlabs.angel.facedetection E/MainActivity: [[0.13254969, 0.06506284, -0.12454985, -0.033946857, 0.13087335, 0.24849732, 0.13556245, -0.04702322, -0.1379856, -0.007572789, 0.019669764, 0.012697782, 0.13016781, -0.08282088, 0.07000842, -0.13634653, -0.033497285, 0.06713182, -0.056609895, 0.13066444, 0.031595007, 0.010904907, 0.06502004, 0.055475105, -0.042641174, -0.11620673, -0.13694304, -0.1973542, 0.023469506, 0.105563894, -0.018650757, 0.03307536, -0.098828435, 0.08058539, 0.08887855, 0.0022337863, 0.0055133225, 0.05411017, -0.044450507, -0.082797125, 0.07151169, -0.068259895, -0.09238541, 0.051023263, -0.26672673, 0.062153224, 0.089065045, 0.1272812, -0.15051614, 0.093154915, -0.04973236, -0.055004556, 0.028005369, 9.352369E-4, 0.021919679, -0.019822104, -0.02774606, 0.12849025, -0.051899977, -0.08593918, -0.08694194, 0.10073202, 0.20436272, -0.19222663, 0.084071614, -0.011063285, 0.037152126, 0.03323839, -0.13506632, -0.026216896, 0.012508951, 0.06862544, -0.023205, 0.013072543, -0.031873662, 0.025892422, -0.061958347, -0.06703131, 0.10233366, 0.06788932, -0.02056665, 0.0018195248, -0.07498662, -0.05542643, -0.01115404, 0.009607625, 0.029644933, -0.020729594, 0.0023015668, 0.09378674, 0.18983097, -0.11676965, -0.01661709, -0.019705301, -0.11590064, -0.13303259, 0.0070525194, 0.056587018, 0.02837071, -0.045306873, 0.105889864, 0.0897621, 0.0022198055, 0.043863844, -0.16235375, 0.06855683, -0.04233673, 0.10779853, -0.019658986, 0.096174344, 0.14861028, 0.09041121, -0.03691038, -0.040842745, -0.044187892, 0.06624289, -0.13923372, -0.038001977, -0.018037193, 0.114912674, -7.4458803E-4, -0.122649185, -0.010618781, 0.11189667, -0.03145326, 0.036915697, -0.0388509, -0.096018195]]
2019-03-26 15:01:46.347 11635-11823/org.blackwalnutlabs.angel.facedetection E/MainActivity: [[0.13254969, 0.06506284, -0.12454985, -0.033946857, 0.13087335, 0.24849732, 0.13556245, -0.04702322, -0.1379856, -0.007572789, 0.019669764, 0.012697782, 0.13016781, -0.08282088, 0.07000842, -0.13634653, -0.033497285, 0.06713182, -0.056609895, 0.13066444, 0.031595007, 0.010904907, 0.06502004, 0.055475105, -0.042641174, -0.11620673, -0.13694304, -0.1973542, 0.023469506, 0.105563894, -0.018650757, 0.03307536, -0.098828435, 0.08058539, 0.08887855, 0.0022337863, 0.0055133225, 0.05411017, -0.044450507, -0.082797125, 0.07151169, -0.068259895, -0.09238541, 0.051023263, -0.26672673, 0.062153224, 0.089065045, 0.1272812, -0.15051614, 0.093154915, -0.04973236, -0.055004556, 0.028005369, 9.352369E-4, 0.021919679, -0.019822104, -0.02774606, 0.12849025, -0.051899977, -0.08593918, -0.08694194, 0.10073202, 0.20436272, -0.19222663, 0.084071614, -0.011063285, 0.037152126, 0.03323839, -0.13506632, -0.026216896, 0.012508951, 0.06862544, -0.023205, 0.013072543, -0.031873662, 0.025892422, -0.061958347, -0.06703131, 0.10233366, 0.06788932, -0.02056665, 0.0018195248, -0.07498662, -0.05542643, -0.01115404, 0.009607625, 0.029644933, -0.020729594, 0.0023015668, 0.09378674, 0.18983097, -0.11676965, -0.01661709, -0.019705301, -0.11590064, -0.13303259, 0.0070525194, 0.056587018, 0.02837071, -0.045306873, 0.105889864, 0.0897621, 0.0022198055, 0.043863844, -0.16235375, 0.06855683, -0.04233673, 0.10779853, -0.019658986, 0.096174344, 0.14861028, 0.09041121, -0.03691038, -0.040842745, -0.044187892, 0.06624289, -0.13923372, -0.038001977, -0.018037193, 0.114912674, -7.4458803E-4, -0.122649185, -0.010618781, 0.11189667, -0.03145326, 0.036915697, -0.0388509, -0.096018195]]
2019-03-26 15:01:46.347 11635-11823/org.blackwalnutlabs.angel.facedetection E/MainActivity: --------------
2019-03-26 15:01:46.464 11635-11823/org.blackwalnutlabs.angel.facedetection E/MainActivity: --------------
2019-03-26 15:01:46.464 11635-11823/org.blackwalnutlabs.angel.facedetection E/MainActivity: [[0.13254969, 0.06506284, -0.12454985, -0.033946857, 0.13087335, 0.24849732, 0.13556245, -0.04702322, -0.1379856, -0.007572789, 0.019669764, 0.012697782, 0.13016781, -0.08282088, 0.07000842, -0.13634653, -0.033497285, 0.06713182, -0.056609895, 0.13066444, 0.031595007, 0.010904907, 0.06502004, 0.055475105, -0.042641174, -0.11620673, -0.13694304, -0.1973542, 0.023469506, 0.105563894, -0.018650757, 0.03307536, -0.098828435, 0.08058539, 0.08887855, 0.0022337863, 0.0055133225, 0.05411017, -0.044450507, -0.082797125, 0.07151169, -0.068259895, -0.09238541, 0.051023263, -0.26672673, 0.062153224, 0.089065045, 0.1272812, -0.15051614, 0.093154915, -0.04973236, -0.055004556, 0.028005369, 9.352369E-4, 0.021919679, -0.019822104, -0.02774606, 0.12849025, -0.051899977, -0.08593918, -0.08694194, 0.10073202, 0.20436272, -0.19222663, 0.084071614, -0.011063285, 0.037152126, 0.03323839, -0.13506632, -0.026216896, 0.012508951, 0.06862544, -0.023205, 0.013072543, -0.031873662, 0.025892422, -0.061958347, -0.06703131, 0.10233366, 0.06788932, -0.02056665, 0.0018195248, -0.07498662, -0.05542643, -0.01115404, 0.009607625, 0.029644933, -0.020729594, 0.0023015668, 0.09378674, 0.18983097, -0.11676965, -0.01661709, -0.019705301, -0.11590064, -0.13303259, 0.0070525194, 0.056587018, 0.02837071, -0.045306873, 0.105889864, 0.0897621, 0.0022198055, 0.043863844, -0.16235375, 0.06855683, -0.04233673, 0.10779853, -0.019658986, 0.096174344, 0.14861028, 0.09041121, -0.03691038, -0.040842745, -0.044187892, 0.06624289, -0.13923372, -0.038001977, -0.018037193, 0.114912674, -7.4458803E-4, -0.122649185, -0.010618781, 0.11189667, -0.03145326, 0.036915697, -0.0388509, -0.096018195]]
2019-03-26 15:01:46.465 11635-11823/org.blackwalnutlabs.angel.facedetection E/MainActivity: [[0.13254969, 0.06506284, -0.12454985, -0.033946857, 0.13087335, 0.24849732, 0.13556245, -0.04702322, -0.1379856, -0.007572789, 0.019669764, 0.012697782, 0.13016781, -0.08282088, 0.07000842, -0.13634653, -0.033497285, 0.06713182, -0.056609895, 0.13066444, 0.031595007, 0.010904907, 0.06502004, 0.055475105, -0.042641174, -0.11620673, -0.13694304, -0.1973542, 0.023469506, 0.105563894, -0.018650757, 0.03307536, -0.098828435, 0.08058539, 0.08887855, 0.0022337863, 0.0055133225, 0.05411017, -0.044450507, -0.082797125, 0.07151169, -0.068259895, -0.09238541, 0.051023263, -0.26672673, 0.062153224, 0.089065045, 0.1272812, -0.15051614, 0.093154915, -0.04973236, -0.055004556, 0.028005369, 9.352369E-4, 0.021919679, -0.019822104, -0.02774606, 0.12849025, -0.051899977, -0.08593918, -0.08694194, 0.10073202, 0.20436272, -0.19222663, 0.084071614, -0.011063285, 0.037152126, 0.03323839, -0.13506632, -0.026216896, 0.012508951, 0.06862544, -0.023205, 0.013072543, -0.031873662, 0.025892422, -0.061958347, -0.06703131, 0.10233366, 0.06788932, -0.02056665, 0.0018195248, -0.07498662, -0.05542643, -0.01115404, 0.009607625, 0.029644933, -0.020729594, 0.0023015668, 0.09378674, 0.18983097, -0.11676965, -0.01661709, -0.019705301, -0.11590064, -0.13303259, 0.0070525194, 0.056587018, 0.02837071, -0.045306873, 0.105889864, 0.0897621, 0.0022198055, 0.043863844, -0.16235375, 0.06855683, -0.04233673, 0.10779853, -0.019658986, 0.096174344, 0.14861028, 0.09041121, -0.03691038, -0.040842745, -0.044187892, 0.06624289, -0.13923372, -0.038001977, -0.018037193, 0.114912674, -7.4458803E-4, -0.122649185, -0.010618781, 0.11189667, -0.03145326, 0.036915697, -0.0388509, -0.096018195]]
2019-03-26 15:01:46.465 11635-11823/org.blackwalnutlabs.angel.facedetection E/MainActivity: --------------
```


**Other info / logs**
The FaceNet has it own loss function, so I set compile=False in the tflite conventor source code.

@wangtz "
27132,missing 2.0.0 alpha custom estimator guide,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: 2.0.0-alpha.0
- Doc Link: https://www.tensorflow.org/alpha/guide/keras/estimators


**Describe the documentation issue**

It's confusing since abundant guide stuff were uploaded, since no guide for custom estimator, just an example of using boostedClassifier

Is tf.keras.Model replacing tf.estimator?

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
27129,[Feature Request] tf.data.Dataset.batch to return RaggedTensor when batching different shape Tensors,"**System information**
- TensorFlow version (you are using): TF 1.13.1
- Are you willing to contribute it (Yes/No): No


**Describe the feature and the current behavior/state.**

Currently, when working with a tf.data.Dataset and batching together Tensors which have different shapes result in an error.
The new feature would return a RaggedTensor instead, which would also require tf.data.Dataset to handle RaggedTensor.


**Will this change the current api? How?**
API would be the same. Instead of an error, the Dataset might contain RaggedTensors.

**Who will benefit with this feature?**
Anyone working with tensors with different shapes.

**Any Other info.**
"
27128,dataset memory cache runs out of memory,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 
- GPU model and memory: 


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
I use tf.dataset.cache to cache data and it will run out of my computer's memory
**Describe the expected behavior**
cache won't run out of memory
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
``` python
dataset.cache()
```

If we can use some cache storages just like leveldb which will provide high performance and will avoid memory leak. "
27127,Issue while importing sugartensor --- tensorflow has no attribute core - SOLVED,"I am using versions:
Tensorflow 1.13.1
Python 3.6
Sugartensor (0.0.2.4)

```
AttributeError                            Traceback (most recent call last)
<ipython-input-1-5db6f289462a> in <module>
     17 '''
     18 from __future__ import print_function
---> 19 import sugartensor as tf
     20 import numpy as np
     21 from prepro import *

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Anaconda3_64\envs\myenv\lib\site-packages\sugartensor\__init__.py in <module>
      1 from __future__ import absolute_import
----> 2 from tensorflow import *
      3 
      4 from .sg_util import sg_opt
      5 from .sg_main import *

AttributeError: module 'tensorflow' has no attribute 'core'

```

"
27126,TF 2.0 TPUStrategy error 'explicit_paddings' not in Op<name=Conv2D; ...>,"**System information**
- TensorFlow version: 2.0.0-alpha0
- Python version: 3.6
- Environment: Google Collaboratory with TPU

**Describe the current behavior**
I'm trying to do TPUStrategy for tensorflow 2.0 with disabled eager execution. I'm using Keras for the CNN model. When I try to run the .fit function on the model, I get an error:

NodeDef mentions attr 'explicit_paddings' not in Op<name=Conv2D; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]; attr=strides:list(int); attr=use_cudnn_on_gpu:bool,default=true; attr=padding:string,allowed=[""SAME"", ""VALID""]; attr=data_format:string,default=""NHWC"",allowed=[""NHWC"", ""NCHW""]; attr=dilations:list(int),default=[1, 1, 1, 1]>; NodeDef: {{node conv2d/Conv2D}}. (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).

**Code to reproduce the issue**
Google Collaboratory notebook: https://gist.github.com/thatsmesasha/0b7cca49517987166f479382158d2d42
"
27122,Error selecting GPU programmatically from jupyter,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

I have a multi-GPU machine and am trying to train different models on different GPUs. I'm trying to set the GPU programmatically rather than use env vars, but am running into issues. Here's my code:

```python
def choose_gpu(gpu_id):
    os.environ[""CUDA_DEVICE_ORDER""] = ""PCI_BUS_ID""
    with K.tf.device(f'/gpu:{gpu_id}'):
        config = tf.ConfigProto(intra_op_parallelism_threads=10,
                                inter_op_parallelism_threads=10,
                                allow_soft_placement=True,
                                device_count={'CPU': 1, 'GPU': 1})
        session = tf.Session(config=config)
        K.set_session(session)
```

However, when I try to run the code in two separate jupyter notebooks, using ids 0 and 1, I get the following error:

```
2019-03-04 10:36:27.210489: W tensorflow/compiler/xla/service/platform_util.cc:240] unable to create StreamExecutor for CUDA:0: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 11523260416
2019-03-04 10:36:27.210803: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x56311c4bccf0 executing computations on platform CUDA. Devices:
2019-03-04 10:36:27.211230: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value
instead of handling error Invalid argument: device CUDA:0 not supported by XLA service
```

It works fine if I set `CUDA_VISIBLE_DEVICES` at the top of the script, but I'd rather not have to manage this. Any sense of what I may be doing wrong? I'm using
- keras 2.2.4
- tensorflow 1.13.1
- ubuntu 18.04
- jupyter 5.2.4
- jupyter lab 0.35.4
"
27121,Build from source error on knl,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.13
- Python version: 3.6
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 0.20
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version: none
- GPU model and memory: none



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Build failed with following command

bazel build --config=mkl --copt=""-DEIGEN_USE_VML"" --copt=""-mfma"" --copt=""-mavx2""  --copt=""-O3"" -s -c opt //tensorflow/tools/pip_package:build_pip_package

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
ERROR: /global/cscratch1/sd/jw447/tensorflow/tensorflow/core/kernels/BUILD:3206:1: C++ compilation of rule '//tensorflow/core/kernels:reduction_ops' failed (Exit 1)
In file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:124:0,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/core/kernels/reduction_ops_common.h:27,
                 from tensorflow/core/kernels/reduction_ops_sum.cc:16:
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h: In static member function 'static void std::_Function_handler<void(_ArgTypes ...), _Functor>::_M_invoke(const std::_Any_data&, _ArgTypes&& ...) [with _Functor = Eigen::internal::TensorExecutor<Expression, Eigen::ThreadPoolDevice, Vectorizable, Tileable>::run(const Expression&, const Eigen::ThreadPoolDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<std::complex<float>, 0, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorReductionOp<Eigen::internal::SumReducer<std::complex<float> >, const Eigen::IndexList<Eigen::type2index<0> >, const Eigen::TensorMap<Eigen::Tensor<const std::complex<float>, 1, 1, long int>, 16, Eigen::MakePointer>, Eigen::MakePointer> >; bool Vectorizable = true; bool Tileable = false]::<lambda(Eigen::internal::TensorExecutor<const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<std::complex<float>, 0, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorReductionOp<Eigen::internal::SumReducer<std::complex<float> >, const Eigen::IndexList<Eigen::type2index<0> >, const Eigen::TensorMap<Eigen::Tensor<const std::complex<float>, 1, 1, long int>, 16, Eigen::MakePointer>, Eigen::MakePointer> >, Eigen::ThreadPoolDevice, true, false>::StorageIndex, Eigen::internal::TensorExecutor<const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<std::complex<float>, 0, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorReductionOp<Eigen::internal::SumReducer<std::complex<float> >, const Eigen::IndexList<Eigen::type2index<0> >, const Eigen::TensorMap<Eigen::Tensor<const std::complex<float>, 1, 1, long int>, 16, Eigen::MakePointer>, Eigen::MakePointer> >, Eigen::ThreadPoolDevice, true, false>::StorageIndex)>; _ArgTypes = {long int, long int}]':
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h:801:9: internal compiler error: in emit_move_insn, at expr.c:3698
         values[i] = internal::InnerMostDimReducer<Self, Op>::reduce(*this, firstIndex + i * num_values_to_reduce,
         ^~~~~~
0x8c3a4a emit_move_insn(rtx_def*, rtx_def*)
	../../cray-gcc-7.3.0-201801270210.d61239fc6000b/gcc/expr.c:3697
0x8b3ddd store_bit_field_1
	../../cray-gcc-7.3.0-201801270210.d61239fc6000b/gcc/expmed.c:814
0x8b4448 store_bit_field(rtx_def*, unsigned long, unsigned long, unsigned long, unsigned long, machine_mode, rtx_def*, bool)
	../../cray-gcc-7.3.0-201801270210.d61239fc6000b/gcc/expmed.c:1122
0x8ce4ae store_field
	../../cray-gcc-7.3.0-201801270210.d61239fc6000b/gcc/expr.c:6974
0x8cbadb expand_assignment(tree_node*, tree_node*, bool)
	../../cray-gcc-7.3.0-201801270210.d61239fc6000b/gcc/expr.c:5209
0x7dd681 expand_call_stmt
	../../cray-gcc-7.3.0-201801270210.d61239fc6000b/gcc/cfgexpand.c:2656
0x7dd681 expand_gimple_stmt_1
	../../cray-gcc-7.3.0-201801270210.d61239fc6000b/gcc/cfgexpand.c:3571
0x7dd681 expand_gimple_stmt
	../../cray-gcc-7.3.0-201801270210.d61239fc6000b/gcc/cfgexpand.c:3737
0x7de85f expand_gimple_basic_block
	../../cray-gcc-7.3.0-201801270210.d61239fc6000b/gcc/cfgexpand.c:5744
0x7e39c6 execute
	../../cray-gcc-7.3.0-201801270210.d61239fc6000b/gcc/cfgexpand.c:6357
Please submit a full bug report,
with preprocessed source if appropriate.
Please include the complete backtrace with any bug report.
See <https://gcc.gnu.org/bugs/> for instructions.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 143.389s, Critical Path: 110.81s
INFO: 537 processes: 537 local.
FAILED: Build did NOT complete successfully
```
Could anyone please help? Thanks!"
27120,tf.function-decorated function tried to create variables on non-first call,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian Testing
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.0.0.dev20190227
- Python version: 3.7


**Describe the current behavior**

A function which correctly works when in eager mode does not work anymore when annotated with `tf.function`.

In particular, it complains about `ValueError: tf.function-decorated function tried to create variables on non-first call.`, even though the function is always called with different parameters.

This is a continuation of https://github.com/tensorflow/tensorflow/issues/26812#issuecomment-475600836.

**Describe the expected behavior**

The `apply_gradients_once()` function should work even when annotated with `tf.function`.

**Code to reproduce the issue**

```python3
import tensorflow as tf
import numpy as np


fast_optimizer = tf.keras.optimizers.Adam(
        learning_rate=1e-3)

slow_optimizer = tf.keras.optimizers.Adam(
        learning_rate=1e-3 * 1e-9)


@tf.function
def apply_gradients_once(optimizer, grads, vars):
    grads = [grads]
    optimizer.apply_gradients(zip(grads, vars))


def apply_grads(use_fast, grads_per_model, vars):
    for i in range(2):
        if use_fast[i]:
            apply_gradients_once(fast_optimizer, grads_per_model[i], vars[i])
        else:
            apply_gradients_once(slow_optimizer, grads_per_model[i], vars[i])


def compute_loss(w, x, y):
    r = (w * x - y)**2
    r = tf.math.reduce_mean(r)
    return r

def compute_gradients(model):
    with tf.GradientTape() as tape:
        tape.watch(model)
        loss = compute_loss(model, x, y)
    grads = tape.gradient(loss, model)
    return grads


w = [
    tf.Variable(0.0),
    tf.Variable(1.0)]

x = np.array([1, 2, 3])
y = np.array([1, 2, 3])

vars = []
grads = []
for i in range(2):
    vars.append([w[i]])
    grads.append(compute_gradients(w[i]))

apply_grads([True, False], grads, vars)
```


**Other info / logs**

Error log:

```
Traceback (most recent call last):
  File ""main.py"", line 52, in <module>
    apply_grads([True, False], grads, vars)
  File ""main.py"", line 23, in apply_grads
    apply_gradients_once(slow_optimizer, grads_per_model[i], vars[i])
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py"", line 414, in __call__
    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py"", line 1254, in __call__
    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py"", line 1577, in _maybe_define_function
    args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py"", line 1479, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py"", line 685, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py"", line 317, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py"", line 677, in wrapper
    ), args, kwargs)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py"", line 392, in converted_call
    result = converted_f(*effective_args, **kwargs)
  File ""/tmp/tmpr2ti5o1e.py"", line 4, in tf__apply_gradients_once
    ag__.converted_call('apply_gradients', optimizer, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (zip(grads, vars),), {})
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py"", line 267, in converted_call
    return _call_unconverted(f, args, kwargs)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py"", line 188, in _call_unconverted
    return f(*args, **kwargs)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py"", line 399, in apply_gradients
    self._create_hypers()
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py"", line 558, in _create_hypers
    aggregation=tf_variables.VariableAggregation.ONLY_FIRST_REPLICA)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py"", line 727, in add_weight
    aggregation=aggregation)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py"", line 622, in _add_variable_with_custom_getter
    **kwargs_for_getter)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py"", line 152, in make_variable
    aggregation=aggregation)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/variables.py"", line 212, in __call__
    return cls._variable_v1_call(*args, **kwargs)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/variables.py"", line 175, in _variable_v1_call
    aggregation=aggregation)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/variables.py"", line 58, in getter
    return captured_getter(captured_previous, **kwargs)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py"", line 375, in invalid_creator_scope
    ""tf.function-decorated function tried to create ""
ValueError: tf.function-decorated function tried to create variables on non-first call.
```"
27119,window + flat_map fails on dataset of tuples,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home edition
- TensorFlow installed from (source or binary): pip install tensorflow-gpu
- TensorFlow version (use command below): b'v1.12.0-rc2-3-ga6d8ffae09' 1.12.0
- Python version: 3.6.7
- CUDA/cuDNN version: Cuda compilation tools, release 9.0, V9.0.176, CuDNN 7.4.1, I believe
- GPU model and memory: NVIDIA GTX 1070, 8 GiB memory (maybe 6, as TF is a bit unclear on that point)

**Describe the current behavior**
On a range dataset, window followed by flat_map works as expected, but on a dataset of tuples (originally encountered using a CSV dataset), flat_map fails, complaining that the input argument to the mapped function has multiple components.

**Describe the expected behavior**
I expected flat_map to treat the windows as datasets, allowing me to perform dataset operations such as `batch` on them.

**Code to reproduce the issue**

    #Simple window batch test
    import tensorflow as tf
    data = tf.data.Dataset.range(0,10)
    data = data.map(lambda *x: (x[0],x[0]+1))
    #Each entry is now a tuple of (n,n+1)
    #Without the mapping to tuple, window+flat_map works.
    data = data.window(3)
    #Each entry is now a window dataset
    #map fails, because the dataset is nested
    #data.map(lambda x: print(x))
    #flat_map fails because x is a tuple, not a dataset
    data = data.flat_map(lambda x: x.batch(3))

**Other info / logs**

> Traceback (most recent call last):
>   File ""window_batch_test.py"", line 14, in <module>
>     data = data.flat_map(lambda x: x.batch(3))
>   File ""C:\Users\elias\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 1070, in flat_map
>     return FlatMapDataset(self, map_func)
>   File ""C:\Users\elias\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 2677, in __init__
>     experimental_nested_dataset_support=True)
>   File ""C:\Users\elias\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 1860, in __init__
>     self._function.add_to_graph(ops.get_default_graph())
>   File ""C:\Users\elias\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\function.py"", line 479, in add_to_graph
>     self._create_definition_if_needed()
>   File ""C:\Users\elias\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\function.py"", line 335, in _create_definition_if_needed
>     self._create_definition_if_needed_impl()
>   File ""C:\Users\elias\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\function.py"", line 344, in _create_definition_if_needed_impl
>     self._capture_by_value, self._caller_device)
>   File ""C:\Users\elias\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\function.py"", line 864, in func_graph_from_py_func
>     outputs = func(*func_graph.inputs)
>   File ""C:\Users\elias\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 1794, in tf_data_structured_function_wrapper
>     ret = func(*nested_args)
> TypeError: <lambda>() takes 1 positional argument but 2 were given

The (regretably) deprecated `sliding_window_batch` seems to work, though. It is much more convenient for my purposes to just get the windows as batches."
27118,Inference vs training in tensorflow ,"Hello,
Could you please tell me how does Tensoflow differentiate the training step from the inference? ( in most examples I found, they use the same call for both steps )"
27116,Failed to load the native TensorFlow runtime,"Exception has occurred: ImportError
Traceback (most recent call last):   File ""C:\Users\moham\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>     from tensorflow.python.pywrap_tensorflow_internal import *   File ""C:\Users\moham\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>     _pywrap_tensorflow_internal = swig_import_helper()   File ""C:\Users\moham\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)   File ""C:\Users\moham\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 242, in load_module     return load_dynamic(name, filename, file)   File ""C:\Users\moham\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 342, in load_dynamic     return _load(spec) ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.   Failed to load the native TensorFlow runtime.  See https://www.tensorflow.org/install/errors  for some common reasons and solutions.  Include the entire stack trace above this error message when asking for help.
  File ""D:\Dropbox\Ph.D\AMIR Ph.D\Python\Projects\Test_project\Machine_learning\Deep_learning\TensorFlowDL\First_example_TF.py"", line 1, in <module>
    import tensorflow as ts
"
27114,Unable to assign GPU using c_api,"**System information**
Windows 10
Source (c_api)
TensorFlow version: 1.13
CUDA/cuDNN version:10, 7.4.2
GPU model and memory: RTX Titan 24 G

Using the c_api I am unable to assign a graph to a specific GPU. Overall the objective is to have the same graph running inference individually on each GPU. I have moved to the c_api from building in c++ because SetDefaultDevice is broken in c++ since version 1.5.

One way to set the device is getting the hex string in python:

gpu_options = tf.GPUOptions(allow_growth=True,visible_device_list='1')
config = tf.ConfigProto(gpu_options=gpu_options)
serialized = config.SerializeToString()
print(list(map(hex, serialized)))

I can get the GPU:1 to work, or GPU:0, but only individually. If I try to construct both, I get this error: 

TensorFlow device (GPU:0) is being mapped to multiple CUDA devices (1 now, and 0 previously), which is not supported. This may be the result of providing different GPU configurations (ConfigProto.gpu_options, for example different visible_device_list) when creating multiple Sessions in the same process. This is not  currently supported, see https://github.com/tensorflow/tensorflow/issues/19083

Is it possible to use multiple GPUs running the same graph using the c_api? If not, is there any possibility to get SetDefaultDevice working in c++?



"
27112,"Cannot export Keras model TypeError: ('Not JSON Serializable:', b'\n...')","Versions:
Mac OS 10.14.3
TF 2.0.0-dev20190319


```python
import tensorflow as tf
from tensorflow.python import tf2
from tensorflow.keras.layers import Input
from tensorflow.keras.models import Model
print(tf.__version__)

import numpy as np
import pandas as pd
import os
# Goal: predict liquidlinePressure as a function of ambientTemp and current
df = pd.DataFrame([[250, 23.2, 8],
                    [255, 22.5, 7.5],
                    [256, 24.0, 8.5],
                    [258, 24.0, 9],
                    [260, 25.1, 8.5]], columns=['liquidLinePressure', 'ambientTemp', 'current'])
#-----standardizing constants
xmu = [26.0,6.0]
xsigma = [5.0, 3.0]
ymu = [255.0]
ysigma = [41.0]
#------------ prepare inputs and outputs
df = df.values
input_features=['ambientTemp','current']
output_features=['liquidLinePressure']
X=df[:,1:]
X = {input_features[i]:x.reshape(-1,1) for i, x in enumerate(X.T)}
y=(df[:,0:1]-ymu)/ysigma #standardize y's for training, but standardize x's within the model

#--------------using Kera's functional api for standardization of inputs
ambientTemp = Input(shape=(1,), dtype='float', name='ambientTemp')
ambientTemp_enc = ((ambientTemp-xmu[0])/xsigma[0])
current=Input(shape=(1,), dtype='float', name='current')
current_enc = ((current-xmu[1])/xsigma[1])

inputs=[ambientTemp, current]
standardizedIns = [ambientTemp_enc, current_enc]
standardized_inputs = tf.keras.layers.concatenate(inputs)
#--model layers
dense1 = tf.keras.layers.Dense(16, activation='relu')(standardized_inputs)
dense2 = tf.keras.layers.Dense(64, activation='relu')(dense1)
dense3 = tf.keras.layers.Dense(64, activation='relu')(dense2)
standardized_outputs = tf.keras.layers.Dense(1, activation='linear')(dense3)

train_model=Model(inputs=inputs, outputs=standardized_outputs)
train_model.compile(optimizer='adam', loss='mse')
train_model.fit(x=X, y=y, epochs=1, verbose=1, batch_size=64,
 shuffle=True);
train_model.save_weights('trainweights.h5')
train_model.save('train_model.h5')  #this works
train_model.summary()

#--- for tf serving, I would like to return unstandardized outputs
liquidLinePressureMean = ((standardized_outputs*ysigma[0])+ymu[0])
outputs=[liquidLinePressureMean]

export_model=Model(inputs=inputs, outputs=outputs, name='model_that_takes_care_of_standardization.h5')
export_model.build(input_shape=[(None,),(None,)]) #is this line necessary/correct?
export_model.summary()
#traceback occurs below
export_model.save('my_model.h5') #this doesn't work
```

It is curious that the preprocessing steps can serialize fine, but something goes wrong either with the definition/building of the `export_model`, or with the postprocessing steps...
(I apologize in advance if this is user error and not a bug-- it seems related to issues [here] (https://github.com/tensorflow/tensorflow/issues/19303) and [here](https://github.com/keras-team/keras/issues/9342) both of which have seemingly hacky solutions.

```python output

2.0.0-dev20190319
2019-03-25 10:43:07.165064: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
5/5 [==============================] - 0s 26ms/sample - loss: 10.0754
Model: ""model""
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
ambientTemp (InputLayer)        [(None, 1)]          0                                            
__________________________________________________________________________________________________
current (InputLayer)            [(None, 1)]          0                                            
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 2)            0           ambientTemp[0][0]                
                                                                 current[0][0]                    
__________________________________________________________________________________________________
dense (Dense)                   (None, 16)           48          concatenate[0][0]                
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 64)           1088        dense[0][0]                      
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 64)           4160        dense_1[0][0]                    
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 1)            65          dense_2[0][0]                    
==================================================================================================
Total params: 5,361
Trainable params: 5,361
Non-trainable params: 0
__________________________________________________________________________________________________
Model: ""model_that_takes_care_of_standardization.h5""
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
ambientTemp (InputLayer)        [(None, 1)]          0                                            
__________________________________________________________________________________________________
current (InputLayer)            [(None, 1)]          0                                            
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 2)            0           ambientTemp[0][0]                
                                                                 current[0][0]                    
__________________________________________________________________________________________________
dense (Dense)                   (None, 16)           48          concatenate[0][0]                
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 64)           1088        dense[0][0]                      
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 64)           4160        dense_1[0][0]                    
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 1)            65          dense_2[0][0]                    
__________________________________________________________________________________________________
strided_slice (TensorFlowOpLaye [None]               0           dense_3[0][0]                    
__________________________________________________________________________________________________
mul (TensorFlowOpLayer)         [None]               0           strided_slice[0][0]              
__________________________________________________________________________________________________
add (TensorFlowOpLayer)         [None]               0           mul[0][0]                        
==================================================================================================
Total params: 5,361
Trainable params: 5,361
Non-trainable params: 0
__________________________________________________________________________________________________
Traceback (most recent call last):
  File ""minimal_example.py"", line 65, in <module>
    export_model.save('my_model.h5')
  File ""/Users/jamesmckeown/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 1319, in save
    save_model(self, filepath, overwrite, include_optimizer)
  File ""/Users/jamesmckeown/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py"", line 103, in save_model
    default=serialization.get_json_type).encode('utf8')
  File ""/Users/jamesmckeown/anaconda2/envs/py36/lib/python3.6/json/__init__.py"", line 238, in dumps
    **kw).encode(obj)
  File ""/Users/jamesmckeown/anaconda2/envs/py36/lib/python3.6/json/encoder.py"", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/jamesmckeown/anaconda2/envs/py36/lib/python3.6/json/encoder.py"", line 257, in iterencode
    return _iterencode(o, 0)
  File ""/Users/jamesmckeown/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/util/serialization.py"", line 69, in get_json_type
    raise TypeError('Not JSON Serializable:', obj)
TypeError: ('Not JSON Serializable:', b'\n\rstrided_slice\x12\x0cStridedSlice\x1a\x0fdense_3/BiasAdd\x1a\x13strided_slice/begin\x1a\x11strided_slice/end\x1a\x15strided_slice/strides*\x16\n\x10shrink_axis_mask\x12\x02\x18\x01*\x10\n\nbegin_mask\x12\x02\x18\x00*\x13\n\rellipsis_mask\x12\x02\x18\x00*\x13\n\rnew_axis_mask\x12\x02\x18\x00*\x0e\n\x08end_mask\x12\x02\x18\x00*\x07\n\x01T\x12\x020\x01*\x0b\n\x05Index\x12\x020\x03')
```

It is also curious to me that the constants used for unstandardizing the output do not show up as `Non-trainable params` of the second model.  Is this expected behavior?  "
27111,CPU version of Tensorflow training is very slow with PlotLossesCallback,"I have the CPU version of tensorflow and have been training a 3 layered Simple Recurrent keras model. However, the model takes takes too long to complete the an epoch (in excess of 30 minutes). 
This started when I added the `plot_losses = PlotLossesCallback()` so that I can get live progress plots of the progress on the losses and accuracies for each epoch. I'm not sure how to remediate the issue. Would installing cuDNN be helpful to fixing the issue or does it not work with the CPU version of tensorflow?
Or is there another when this can be fixed?"
27109,"Micro speech tutorial - wav_to_features script fails with ""ImportError: No module named enum""","I'm following the instructions in the [micro_speech tutorial](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro/examples/micro_speech#creating-your-own-model)

I'm on OSX 10.14.2

When I get to the step to generate the test data files, and run the command:

```bash
bazel run tensorflow/examples/speech_commands:wav_to_features -- \
--input_wav=${HOME}/speech_commands_test_set_v0.02/yes/f2e59fea_nohash_1.wav \
--output_c_file=yes_features_data.cc \
--window_stride=20 --preprocess=average --quantize=1
```

I eventually get to an error ""ImportError: No module named enum"":

> ERROR: /Users/danoved/Source/thesis/speech-rec/tensorflow/tensorflow/BUILD:713:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1)
> Traceback (most recent call last):
>   File ""/private/var/tmp/_bazel_danoved/be5220cf895eff30f56ec50496f58755/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 27, in <module>
>     from tensorflow.python.tools.api.generator import doc_srcs
>   File ""/private/var/tmp/_bazel_danoved/be5220cf895eff30f56ec50496f58755/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 63, in <module>
>     from tensorflow.python.framework.framework_lib import *  # pylint: disable=redefined-builtin
>   File ""/private/var/tmp/_bazel_danoved/be5220cf895eff30f56ec50496f58755/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/framework/framework_lib.py"", line 52, in <module>
>     from tensorflow.python.framework.importer import import_graph_def
>   File ""/private/var/tmp/_bazel_danoved/be5220cf895eff30f56ec50496f58755/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/framework/importer.py"", line 28, in <module>
>     from tensorflow.python.framework import function
>   File ""/private/var/tmp/_bazel_danoved/be5220cf895eff30f56ec50496f58755/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/framework/function.py"", line 37, in <module>
>     from tensorflow.python.ops import resource_variable_ops
>   File ""/private/var/tmp/_bazel_danoved/be5220cf895eff30f56ec50496f58755/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/ops/resource_variable_ops.py"", line 41, in <module>
>     from tensorflow.python.ops import variables
>   File ""/private/var/tmp/_bazel_danoved/be5220cf895eff30f56ec50496f58755/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/ops/variables.py"", line 20, in <module>
>     import enum  # pylint: disable=g-bad-import-order
> ImportError: No module named enum
> Target //tensorflow/examples/speech_commands:wav_to_features failed to build
> Use --verbose_failures to see the command lines of failed build steps.
> INFO: Elapsed time: 3774.946s, Critical Path: 286.02s
> INFO: 10376 processes: 10376 local.
> FAILED: Build did NOT complete successfully
> FAILED: Build did NOT complete successfully

"
27107,Implement WALS matrix factorization in Tensorflow 2.0,"**System information**
- TensorFlow version (you are using): 2.0-master
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
Implement tf.contrib.factorization.WALSMatrixFactorization and tf.contrib.factorization.WALSModel in Tensorflow 2.0 (ideally in a distributed manner)

**Will this change the current api? How?**
Most likely.
The current WALSMatrixFactorization and WALSModel lie under tf.contrib.factorization, which contains some other methods, in particular clustering ones, such as KMeans or GMM.
The factorization itself makes sense under tf.linalg instead (the other methods need further assesment) while the estimator may be somewhere.
In other words, the factorization would probably benefit from being decoupled from the model implementation.

**Who will benefit with this feature?**
Anyone using the current factorization who wants (or needs) to upgrade to Tensorflow 2.0.


**Any Other info.**
There is an open issue related to WALS that could be of interest to take into account if this feature is considered: https://github.com/tensorflow/tensorflow/issues/21991"
27106,TF Lite model parsing in Python,"Could you please make it a little bit clear for me 

I am using pyhon generated code to read and get node attributes from the model graph.

I generated it this way (output is `tflite/` folder with autogenerated *.py files):
`flatc -python tensorflow/tensorflow/lite/schema/schema.fbs`

Than I read the model: 

```
    from tflite.Model import Model
    def read_tflite_model(file):
        buf = open(file, ""rb"").read()
        buf = bytearray(buf)
        model = Model.GetRootAsModel(buf, 0)
        return model

```
Getting model parameters:

```
    def print_model_info(model):
        version = model.Version()
        print(""Model version:"", version)
        description = model.Description().decode('utf-8')
        print(""Description:"", description)
        subgraph_len = model.SubgraphsLength()
        print(""Subgraph length:"", subgraph_len)

```
Than I realized that graph nodes could be interpreted as `Tensor` object or `Operator` object.

`Tensor` object stores quantization params, shape, tensor type (I don't understand the meaning of it yet)

`Operator` object has `BuiltinOptions` and `CustomOptions` methods that seems to me should give me access to node parameters (such as paddings, dilations and all layer specific info)

I tried to iterate over them since there are Inputs and Outputs methods. But failed. I don't understand w

This is my scratch:
```
def print_nodes_info(model):
    # what does this 0 mean? should it always be zero?
    subgraph = model.Subgraphs(0)
    operators_len = subgraph.OperatorsLength()
    print('Operators length:', operators_len)

    from collections import deque
    nodes = deque(subgraph.InputsAsNumpy())

    STEP_N = 0
    MAX_STEPS = operators_len
    print(""Nodes info:"")
    while len(nodes) != 0 and STEP_N <= MAX_STEPS:
        print(""MAX_STEPS={} STEP_N={}"".format(MAX_STEPS, STEP_N))
        print(""-"" * 60)

        node_id = nodes.pop()
        print(""Node id:"", node_id)

        tensor = subgraph.Tensors(node_id)
        print(""Node name:"", tensor.Name().decode('utf-8'))
        print(""Node shape:"", tensor.ShapeAsNumpy())

        # which type is it? what does it mean?
        type_of_tensor = tensor.Type()
        print(""Tensor type:"", type_of_tensor)

        quantization = tensor.Quantization()
        min = quantization.MinAsNumpy()
        max = quantization.MaxAsNumpy()
        scale = quantization.ScaleAsNumpy()
        zero_point = quantization.ZeroPointAsNumpy()
        print(""Quantization: ({}, {}), s={}, z={}"".format(min, max, scale, zero_point))

        # I do not understand it again. what is j, that I set to 0 here?
        operator = subgraph.Operators(0)
        for i in operator.OutputsAsNumpy():
            nodes.appendleft(i)

        STEP_N += 1

    print(""-""*60)

```

Please help me to get access the node attributes.
Thank you in advance for your help.

"
27105,TF Lite model parsing in Python,"Could you please make it a little bit clear for me 

I am using pyhon generated code to read and get node attributes from the model graph.

I generated it this way (output is `tflite/` folder with autogenerated *.py files):
`flatc -python tensorflow/tensorflow/lite/schema/schema.fbs`

Than I read the model: 

```
    from tflite.Model import Model
    def read_tflite_model(file):
        buf = open(file, ""rb"").read()
        buf = bytearray(buf)
        model = Model.GetRootAsModel(buf, 0)
        return model

```
Getting model parameters:

```
    def print_model_info(model):
        version = model.Version()
        print(""Model version:"", version)
        description = model.Description().decode('utf-8')
        print(""Description:"", description)
        subgraph_len = model.SubgraphsLength()
        print(""Subgraph length:"", subgraph_len)

```
Than I realized that graph nodes could be interpreted as `Tensor` object or `Operator` object.

`Tensor` object stores quantization params, shape, tensor type (I don't understand the meaning of it yet)

`Operator` object has `BuiltinOptions` and `CustomOptions` methods that seems to me should give me access to node parameters (such as paddings, dilations and all layer specific info)

I tried to iterate over them since there are Inputs and Outputs methods. But failed. I don't understand w

This is my scratch:
```
def print_nodes_info(model):
    # what does this 0 mean? should it always be zero?
    subgraph = model.Subgraphs(0)
    operators_len = subgraph.OperatorsLength()
    print('Operators length:', operators_len)

    from collections import deque
    nodes = deque(subgraph.InputsAsNumpy())

    STEP_N = 0
    MAX_STEPS = operators_len
    print(""Nodes info:"")
    while len(nodes) != 0 and STEP_N <= MAX_STEPS:
        print(""MAX_STEPS={} STEP_N={}"".format(MAX_STEPS, STEP_N))
        print(""-"" * 60)

        node_id = nodes.pop()
        print(""Node id:"", node_id)

        tensor = subgraph.Tensors(node_id)
        print(""Node name:"", tensor.Name().decode('utf-8'))
        print(""Node shape:"", tensor.ShapeAsNumpy())

        # which type is it? what does it mean?
        type_of_tensor = tensor.Type()
        print(""Tensor type:"", type_of_tensor)

        quantization = tensor.Quantization()
        min = quantization.MinAsNumpy()
        max = quantization.MaxAsNumpy()
        scale = quantization.ScaleAsNumpy()
        zero_point = quantization.ZeroPointAsNumpy()
        print(""Quantization: ({}, {}), s={}, z={}"".format(min, max, scale, zero_point))

        # I do not understand it again. what is j, that I set to 0 here?
        operator = subgraph.Operators(0)
        for i in operator.OutputsAsNumpy():
            nodes.appendleft(i)

        STEP_N += 1

    print(""-""*60)

```

Please help me to get access the node attributes.
Thank you in advance for your help.

"
27102,tf.py_function InternalError in distributed mode,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 16.04.6 LTS
- TensorFlow installed from (source or binary):
pip
- TensorFlow version (use command below):
1.13.1
- Python version:
3.6.7

**Describe the current behavior**
I'm testing tensorflow distributed on the same machine. I opened two separate shells and created a cluster using:

```python
import tensorflow as tf  
cluster = tf.train.ClusterSpec({""local"": [""localhost:2222"", ""localhost:2223""]})
server = tf.train.Server(cluster, job_name=""local"", task_index=0)
```

on shell 0 and

```python
import tensorflow as tf
cluster = tf.train.ClusterSpec({""local"": [""localhost:2222"", ""localhost:2223""]})
server = tf.train.Server(cluster, job_name=""local"", task_index=1)
```

on shell 1.

On shell 1 I ran:

```python
import tensorflow as tf
import numpy as np

def add(a, b):
    return a + b

NUM = 1000
ones = np.ones((NUM))

graph = tf.Graph()
with graph.as_default():
    va = tf.Variable(ones)
    vb = tf.Variable(ones)
    with tf.device(""job:local/task:1""):
        inputs = [va, vb]
        out = tf.py_function(add, inputs, tf.float64)

    with tf.Session(""grpc://localhost:2223"", graph=graph) as sess:
        sess.run(tf.global_variables_initializer())
        print(sess.run([va, vb]))
        print(sess.run([out]))
```

which returns the error:

> InternalError (see above for traceback): expected the py_func to return a Tensor backed by memory in /job:local/replica:0/task:1/device:CPU:0, but is actually in /job:localhost/replica:0/task:0/device:CPU:0. This is a bug.

while this does not happen if I replace tf.py_function in the code above with tf.py_func.

**Describe the expected behavior**

Code should run successfully like when using tf.py_func and should print the arrays:

> [array([1., 1., 1., 1., ..., 1., 1., 1., 1.]), array([1., 1., 1., 1., ..., 1., 1., 1., 1.])]
> [array([2., 2., 2., 2., ..., 2., 2., 2., 2.])]
"
27101,Unable to convert custom trained model ( .pb file ) into tflite format,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution : Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below):1.12
- Python version: 3.5
- Bazel version (if compiling from source): NA
- CUDA/cuDNN version: No gpu
- GPU model and memory: No gpu


**Describe the current behavior**
I have the following script using which I was able to successfully convert [deeplabv3_mnv2_pascal_train.pb model ](https://drive.google.com/file/d/1xKI0SIrXB6Wl8SBuX-D1otI_8nuHjza1/view) into tflite format 

script is as follows: 

> tflite_convert \
>   --output_file=test.lite \
>   --graph_def_file=deeplabv3_mnv2_pascal_tain.pb \
>   --input_arrays=ImageTensor \
>   --output_arrays=SemanticPredictions \
>   --input_shapes=1,513,513,3 \
>   --inference_input_type=QUANTIZED_UINT8 \
>   --inference_type=FLOAT \
>   --mean_values=128 \
>   --std_dev_values=128

I obtained input_arrays, and output_arrays for deeplabv3_mnv2_pascal_train.pb using the following python script.

> import tensorflow as tf
> gf = tf.GraphDef()   
> m_file = open('deeplabv3_mnv2_pascal_tain.pb','rb')
> gf.ParseFromString(m_file.read())
> 
> #We get the names of the nodes
> for n in gf.node:
>     print( n.name )
> 
> #To get the tensor
> tensor = n.op

I am planning to apply the same steps above towards my custom trained model, and convert it into tflite format. My model is [here](https://drive.google.com/file/d/1YUoayPHOqnkd7PR0QVBS9Vzk15b6r4p3/view)
I used the above python script to get the input_arrays, and output_arrays and then ran the following:

> tflite_convert \
>   --output_file=test.lite \
>   --graph_def_file=my_graph.pb \
>   --input_arrays=Const \
>   --output_arrays=detection_masks \
>   --input_shapes=1,513,513,3 \
>   --inference_input_type=QUANTIZED_UINT8 \
>   --inference_type=FLOAT \
>   --mean_values=128 \
>   --std_dev_values=128
> 

I am getting the following error :

> 2019-03-25 12:54:10.156375: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
> Traceback (most recent call last):
>   File ""/home/ajinkya/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 558, in set_shape
>     unknown_shape)
> tensorflow.python.framework.errors_impl.InvalidArgumentError: Shapes must be equal rank, but are 1 and 4
> 
> During handling of the above exception, another exception occurred:
> 
> Traceback (most recent call last):
>   File ""/home/ajinkya/.local/bin/tflite_convert"", line 11, in <module>
>     sys.exit(main())
>   File ""/home/ajinkya/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 412, in main
>     app.run(main=run_main, argv=sys.argv[:1])
>   File ""/home/ajinkya/.local/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 125, in run
>     _sys.exit(main(argv))
>   File ""/home/ajinkya/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 408, in run_main
>     _convert_model(tflite_flags)
>   File ""/home/ajinkya/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 100, in _convert_model
>     converter = _get_toco_converter(flags)
>   File ""/home/ajinkya/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 87, in _get_toco_converter
>     return converter_fn(**converter_kwargs)
>   File ""/home/ajinkya/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/lite.py"", line 286, in from_frozen_graph
>     _set_tensor_shapes(input_tensors, input_shapes)
>   File ""/home/ajinkya/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/convert_saved_model.py"", line 205, in set_tensor_shapes
>     tensor.set_shape(shape)
>   File ""/home/ajinkya/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 561, in set_shape
>     raise ValueError(str(e))
> ValueError: Shapes must be equal rank, but are 1 and 4

How do I resolve this error?
"
27100,Segmentation Fault with TensorRT create interference graph ,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): bynary (tensorflow-gpu)
- TensorFlow version (use command below): b'v1.13.1-0-g6612da8951' 1.13.1

- Python version: Python 3.6.7
- CUDA/cuDNN version: CUDA 10
- GPU model and memory: NVIDIA 1060 GTX


**Describe the current behavior**
i'm trying to optimize a tensorflow model to tensort optimization. i'm using the example of object detection given by https://github.com/tensorflow/tensorrt/tree/master/tftrt/examples/object_detection. So the tensorflow model loads perfect but when I try to optimize it a segmentation fault raise.

**Describe the expected behavior**

**Code to reproduce the issue**
`with tf.Graph().as_default() as tf_graph:
            with tf.Session(config=tf_config) as tf_sess:
                frozen_graph = trt.create_inference_graph(
                    input_graph_def=frozen_graph,
                    outputs=output_names,
                    max_batch_size=max_batch_size,
                    max_workspace_size_bytes=max_workspace_size_bytes,
                    precision_mode=precision_mode,
                    minimum_segment_size=minimum_segment_size,
                    is_dynamic_op=True,
                    maximum_cached_engines=maximum_cached_engines)`

So the segmentation fault occurs in trt create_create_interference_graph.
**Other info / logs**

This is the log from python output

> 2019-03-25 09:08:39.360172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
> 2019-03-25 09:08:39.360201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
> 2019-03-25 09:08:39.360207: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
> 2019-03-25 09:08:39.360210: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
> 2019-03-25 09:08:39.360303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5171 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)
> INFO:tensorflow:Running against TensorRT version 5.0.2
> INFO:tensorflow:Running against TensorRT version 5.0.2
> 2019-03-25 09:08:40.787773: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 1
> 2019-03-25 09:08:40.788522: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session
> 2019-03-25 09:08:40.790765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
> 2019-03-25 09:08:40.790785: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
> 2019-03-25 09:08:40.790790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
> 2019-03-25 09:08:40.790793: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
> 2019-03-25 09:08:40.790903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5171 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)
> 2019-03-25 09:08:42.079562: I tensorflow/contrib/tensorrt/segment/segment.cc:443] There are 2316 ops of 32 different types in the graph that are not converted to TensorRT: Fill, Switch, Range, TopKV2, ConcatV2, Identity, Squeeze, Transpose, Const, Unpack, ResizeBilinear, Reshape, Mul, Slice, Merge, Split, Where, ExpandDims, NonMaxSuppressionV3, GatherV2, Cast, Greater, Minimum, Sub, ZerosLike, Pack, Exp, Placeholder, Add, Shape, NoOp, StridedSlice, (For more information see https://docs.nvidia.com/deeplearning/dgx/integrate-tf-trt/index.html#support-ops).
> 2019-03-25 09:08:42.206925: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:913] Number of TensorRT candidate segments: 185
> 2019-03-25 09:08:47.654116: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:1015] TensorRT node TRTEngineOp_0 added for segment 0 consisting of 486 nodes succeeded.
> Segmentation fault (core dumped)

And this is the callstack from gdb .

> > 2019-03-25 09:12:23.651268: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:1015] TensorRT node TRTEngineOp_0 added for segment 0 consisting of 486 nodes succeeded.
> 
> Thread 1 ""python3"" received signal SIGSEGV, Segmentation fault.
> 0x00007fff68d60261 in tensorflow::tensorrt::convert::GetDeviceAndAllocator(tensorflow::tensorrt::convert::ConversionParams const&, tensorflow::tensorrt::convert::EngineInfo const&) ()
>    from /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tensorrt/_wrap_conversion.so
> (gdb) bt
> #0  0x00007fff68d60261 in tensorflow::tensorrt::convert::GetDeviceAndAllocator(tensorflow::tensorrt::convert::ConversionParams const&, tensorflow::tensorrt::convert::EngineInfo const&) ()
>    from /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tensorrt/_wrap_conversion.so
> #1  0x00007fff68d651aa in tensorflow::tensorrt::convert::ConvertAfterShapes(tensorflow::tensorrt::convert::ConversionParams&) ()
>    from /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tensorrt/_wrap_conversion.so
> #2  0x00007fff68d90f56 in tensorflow::tensorrt::convert::TRTOptimizationPass::Optimize(tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem const&, tensorflow::GraphDef*) ()
>    from /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tensorrt/_wrap_conversion.so
> #3  0x00007fffb549a8ee in tensorflow::grappler::MetaOptimizer::RunOptimizer(tensorflow::grappler::GraphOptimizer*, tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem*, tensorflow::GraphDef*, tensorflow::grappler::MetaOptimizer::GraphOptimizationResult*) () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
> #4  0x00007fffb549b552 in tensorflow::grappler::MetaOptimizer::OptimizeGraph(tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem const&, tensorflow::GraphDef*) ()
>    from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
> #5  0x00007fffb549c8a7 in tensorflow::grappler::MetaOptimizer::Optimize(tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem const&, tensorflow::GraphDef*) ()
>    from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
> #6  0x00007fffb028ab9c in TF_OptimizeGraph(GCluster, tensorflow::ConfigProto const&, tensorflow::MetaGraphDef const&, bool, std::string const&, TF_Status*) ()
>    from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
> #7  0x00007fffb0293157 in _wrap_TF_OptimizeGraph () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
> #8  0x0000000000502d6f in ?? ()
> #9  0x0000000000506859 in _PyEval_EvalFrameDefault ()
> #10 0x0000000000504c28 in ?? ()
> #11 0x0000000000502540 in ?? ()
> #12 0x0000000000502f3d in ?? ()
> #13 0x0000000000507641 in _PyEval_EvalFrameDefault ()
> #14 0x0000000000504c28 in ?? ()
> #15 0x0000000000502540 in ?? ()
> #16 0x0000000000502f3d in ?? ()
> #17 0x0000000000507641 in _PyEval_EvalFrameDefault ()
> #18 0x0000000000504c28 in ?? ()
> #19 0x0000000000502540 in ?? ()
> #20 0x0000000000502f3d in ?? ()
> #21 0x0000000000507641 in _PyEval_EvalFrameDefault ()
> #22 0x0000000000504c28 in ?? ()
> #23 0x0000000000506393 in PyEval_EvalCode ()
> #24 0x0000000000634d52 in ?? ()
> #25 0x00000000004a38c5 in ?? ()
> #26 0x00000000004a5cd5 in PyRun_InteractiveLoopFlags ()
> #27 0x00000000006387b3 in PyRun_AnyFileExFlags ()
> #28 0x000000000063915a in Py_Main ()
> #29 0x00000000004a6f10 in main ()

"
27099,tf.feature_column.sequence_categorical,"
What is the difference between tf.feature_column.sequence_categorical... and tf.feature_column.categorical...？"
27095,Build Tensorflow r1.13 with verbs and gdr report rdma error,"cmd : ` bazel build --config=opt --config=cuda --config=gdr --config=verbs --distdir=/home/bazel_package //tensorflow/tools/pip_package:build_pip_package`
Due to network problems, I downloaded the dependencies in advance.
And I have install  the SW stack (libibverbs, etc..)
```
	sudo yum install jemalloc
	sudo yum install jemalloc-devel
	sudo yum install libibverbs
	sudo yum install libibverbs-devel
```

Error info:
```
bazel-out/k8-opt/genfiles/tensorflow/core/protobuf/config.pb_text.cc:882:9: note: 'map_value' was declared here
   int32 map_value;
         ^
ERROR: /home/zgz/submit_code/tensorflow/tensorflow/contrib/verbs/BUILD:105:1: C++ compilation of rule '//tensorflow/contrib/verbs:rdma_mgr' failed (Exit 1)
In file included from tensorflow/contrib/verbs/rdma_mgr.cc:18:0:
./tensorflow/contrib/verbs/rdma_mgr.h: In static member function 'static void tensorflow::RdmaMgr::RegMemVisitors()':
./tensorflow/contrib/verbs/rdma_mgr.h:50:16: error: invalid use of member 'tensorflow::RdmaMgr::rdma_adapter_' in static member function
   RdmaAdapter* rdma_adapter_;
                ^
tensorflow/contrib/verbs/rdma_mgr.cc:282:40: error: from this location
     int32_t bus_id = TryToReadNumaNode(rdma_adapter_->context_->device) + 1;
                                        ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 80.943s, Critical Path: 51.17s
INFO: 2025 processes: 2025 local.
FAILED: Build did NOT complete successfully
```

please tell me how to fix this."
27093,Export user_data from TfLiteContext,"**System information**
- TensorFlow version (you are using): 13fe6ef76e9f882584fe63a2b7f292266dedf847
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**

Thinking binding of Python or Java, no way to map TfLiteContext to Python/Java objcet. Current implementation of TfLiteRegistration does not have user_data to store object that can be used for mapping Python or Java. So we can't implement custom op with Python or Java.

```cpp
  static TfLiteRegistration registration = {
      .init = nullptr,
      .free = nullptr,
      .prepare =
          [](TfLiteContext* context, TfLiteNode* node) {
              // call Python/Java
             //PyObject* obj = context->user_data;
          }
   };
```
If TfLiteContext have user_data (typed void*), we can call Python or Java in the function (init/free/prepare/...).

**Will this change the current api? How?**

So I suggest to add field user_data into TfLiteContext and TfLiteRegistration.

```cpp
registration.user_data = GetMyUserData();
```
```cpp
MyUserData* my_user_data = (MyUserData*)context->user_data;
```

**Who will benefit with this feature?**

The authors of binding for Python/Java or [Me](https://github.com/mattn/go-tflite)

**Any Other info.**

However, we need more APIs to implement custom op which is implemened with C.

ex: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/models/smartreply/ops"
27090,"InvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'RoiPoolGrad' with these attrs.  Registered devices: [CPU], Registered kernels:","/home/chen/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""
Traceback (most recent call last):
  File ""/home/chen/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1322, in _do_call
    return fn(*args)
  File ""/home/chen/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1305, in _run_fn
    self._extend_graph()
  File ""/home/chen/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1340, in _extend_graph
    tf_session.ExtendSession(self._session)
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'RoiPoolGrad' with these attrs.  Registered devices: [CPU], Registered kernels:
  device='GPU'; T in [DT_FLOAT]

	 [[Node: gradients/pool_5_grad/RoiPoolGrad = RoiPoolGrad[T=DT_FLOAT, pooled_height=7, pooled_width=7, spatial_scale=0.0625](conv5_3/Relu, roi-data/rois, pool_5:1, gradients/fc6/transpose_grad/transpose)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""./faster_rcnn/train_net.py"", line 109, in <module>
    restore=bool(int(args.restore)))
  File ""./faster_rcnn/../lib/fast_rcnn/train.py"", line 401, in train_net
    sw.train_model(sess, max_iters, restore=restore)
  File ""./faster_rcnn/../lib/fast_rcnn/train.py"", line 149, in train_model
    sess.run(tf.global_variables_initializer())
  File ""/home/chen/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 900, in run
    run_metadata_ptr)
  File ""/home/chen/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/chen/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1316, in _do_run
    run_metadata)
  File ""/home/chen/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'RoiPoolGrad' with these attrs.  Registered devices: [CPU], Registered kernels:
  device='GPU'; T in [DT_FLOAT]

	 [[Node: gradients/pool_5_grad/RoiPoolGrad = RoiPoolGrad[T=DT_FLOAT, pooled_height=7, pooled_width=7, spatial_scale=0.0625](conv5_3/Relu, roi-data/rois, pool_5:1, gradients/fc6/transpose_grad/transpose)]]

Caused by op 'gradients/pool_5_grad/RoiPoolGrad', defined at:
  File ""./faster_rcnn/train_net.py"", line 109, in <module>
    restore=bool(int(args.restore)))
  File ""./faster_rcnn/../lib/fast_rcnn/train.py"", line 401, in train_net
    sw.train_model(sess, max_iters, restore=restore)
  File ""./faster_rcnn/../lib/fast_rcnn/train.py"", line 143, in train_model
    grads, norm = tf.clip_by_global_norm(tf.gradients(loss, tvars), 10.0)
  File ""/home/chen/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py"", line 532, in gradients
    gate_gradients, aggregation_method, stop_gradients)
  File ""/home/chen/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py"", line 701, in _GradientsHelper
    lambda: grad_fn(op, *out_grads))
  File ""/home/chen/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py"", line 396, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""/home/chen/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py"", line 701, in <lambda>
    lambda: grad_fn(op, *out_grads))
  File ""./faster_rcnn/../lib/roi_pooling_layer/roi_pooling_op_grad.py"", line 23, in _roi_pool_grad
    data_grad = roi_pooling_op.roi_pool_grad(data, rois, argmax, grad, pooled_height, pooled_width, spatial_scale)
  File ""<string>"", line 139, in roi_pool_grad
  File ""/home/chen/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/chen/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3414, in create_op
    op_def=op_def)
  File ""/home/chen/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1740, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

...which was originally created as op 'pool_5', defined at:
  File ""./faster_rcnn/train_net.py"", line 101, in <module>
    network = get_network(args.network_name)
  File ""./faster_rcnn/../lib/networks/factory.py"", line 29, in get_network
    return VGGnet_train()
  File ""./faster_rcnn/../lib/networks/VGGnet_train.py"", line 17, in __init__
    self.setup()
  File ""./faster_rcnn/../lib/networks/VGGnet_train.py"", line 84, in setup
    .roi_pool(7, 7, 1.0/16, name='pool_5')
  File ""./faster_rcnn/../lib/networks/network.py"", line 36, in layer_decorated
    layer_output = op(self, layer_input, *args, **kwargs)
  File ""./faster_rcnn/../lib/networks/network.py"", line 235, in roi_pool
    name=name)[0]
  File ""<string>"", line 57, in roi_pool
  File ""/home/chen/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/chen/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3414, in create_op
    op_def=op_def)
  File ""/home/chen/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1740, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'RoiPoolGrad' with these attrs.  Registered devices: [CPU], Registered kernels:
  device='GPU'; T in [DT_FLOAT]

	 [[Node: gradients/pool_5_grad/RoiPoolGrad = RoiPoolGrad[T=DT_FLOAT, pooled_height=7, pooled_width=7, spatial_scale=0.0625](conv5_3/Relu, roi-data/rois, pool_5:1, gradients/fc6/transpose_grad/transpose)]]

when i train the TFFRCNN ,there have a bug.
what should i do now?
thanks!
"
27088,failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED,"Hi, 

I am a on a Ubuntu system, installed tensorflow using conda install tensorflow-gpu, have cuda 9.0.176. I am getting the error: 

> failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED

I tried the solutions:

`rm -rf .nv/`

```
config = tf.ConfigProto()
config.gpu_options.allow_growth = True
session = tf.Session(config=config....)

```
Both are not helping resolve the issue. What could be the error source? Any more possible solutions to try?

Thanks!

```
2019-03-24 20:18:47.549553: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX FMA
2019-03-24 20:18:47.718642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties: 
name: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:01:00.0
totalMemory: 11.91GiB freeMemory: 10.28GiB
2019-03-24 20:18:47.718677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0
2019-03-24 20:18:48.009540: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9950 MB memory) -> physical GPU (device: 0, name: TITAN X (Pascal), pci bus id: 0000:01:00.0, compute capability: 6.1)
2019-03-24 20:19:03.125607: E tensorflow/stream_executor/cuda/cuda_blas.cc:443] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-03-24 20:19:15.413499: E tensorflow/stream_executor/cuda/cuda_blas.cc:443] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-03-24 20:19:27.701539: E tensorflow/stream_executor/cuda/cuda_blas.cc:443] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-03-24 20:19:39.989549: E tensorflow/stream_executor/cuda/cuda_blas.cc:443] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-03-24 20:19:52.277482: E tensorflow/stream_executor/cuda/cuda_blas.cc:443] failed to create cublas handle: CUBLAS_STATUS_NOT_IThis template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
NITIALIZED
2019-03-24 20:20:04.565579: E tensorflow/stream_executor/cuda/cuda_blas.cc:443] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-03-24 20:20:16.853469: E tensorflow/stream_executor/cuda/cuda_blas.cc:443] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-03-24 20:20:29.141579: E tensorflow/stream_executor/cuda/cuda_blas.cc:443] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED

```"
27087,`tk.layers.CuDNNLSTM` is not compatible with `tk.layers.StackedRNNCells`,"I follow the official document and write the following code to create a stack of RNN cells using Tensorflow 1.13 on GPU

```python
import tensorflow.keras as tk

lstm = [tk.layers.CuDNNLSTM(128) for _ in range(2)]
cells = tk.layers.StackedRNNCells(lstm)
```
However, I receive the error message:

> ---------------------------------------------------------------------------
> ValueError                                Traceback (most recent call last)
> <ipython-input-30-1b47a86eb8ab> in <module>
>       2 # x = tf.placeholder(tf.float32, (4, 100, 128))
>       3 lstm = [tk.layers.CuDNNLSTM(128) for _ in range(2)]
> ----> 4 lstm = tk.layers.StackedRNNCells(lstm)
> 
> ~/anaconda3/envs/gym/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py in __init__(self, cells, **kwargs)
>      74         raise ValueError('All cells must have a '
>      75                          '`state_size` attribute. '
> ---> 76                          'received cells:', cells)
>      77     self.cells = cells
>      78     # reverse_state_order determines whether the state size will be in a reverse
> 
> ValueError: ('All cells must have a `state_size` attribute. received cells:', [<tensorflow.python.keras.layers.cudnn_recurrent.CuDNNLSTM object at 0x13aa1c940>])

Is this a bug, or do I do it wrong?"
27086,Tensorflow 2.0: Adding @tf.function decorator with categorical feature_column raises FailedPreconditionError: Table already initialized,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS X 10.14
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-alpha0
- Python version: Python 3.6.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
Just want to preface this by saying while strictly speaking I am using 'custom' code, all I did was piece together code from the 'classify structured data' and the 'introduction for experts'.

I am trying to add @tf.function for performance enhancement reasons to my custom training code in Tensorflow 2.0. However, running the code raises a FailedPreconditionError: Table already initialized. when using categorical feature columns.

What I suspect is happening is that the lookup tables for the categorical encoding are being initialized more than once (in the model definition and in @tf.function).  How should I go about using feature_columns and @tf.function?

**Code to reproduce the issue**
```
from __future__ import absolute_import, division, print_function

import numpy as np
import pandas as pd

!pip install sklearn
!pip install tensorflow==2.0.0-alpha0
import tensorflow as tf

from tensorflow import feature_column
from tensorflow.keras import layers
from tensorflow.keras.models import Model
from sklearn.model_selection import train_test_split

URL = 'https://storage.googleapis.com/applied-dl/heart.csv'
dataframe = pd.read_csv(URL)

train, test = train_test_split(dataframe, test_size=0.2)
train, val = train_test_split(train, test_size=0.2)

# A utility method to create a tf.data dataset from a Pandas Dataframe
def df_to_dataset(dataframe, shuffle=True, batch_size=32):
  dataframe = dataframe.copy()
  labels = dataframe.pop('target').values.reshape(-1,1)
  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))
  if shuffle:
    ds = ds.shuffle(buffer_size=len(dataframe))
  ds = ds.batch(batch_size)
  return ds

batch_size = 32
train_ds = df_to_dataset(train, batch_size=batch_size)
val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)
test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)

feature_columns = []

# numeric cols
for header in ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope', 'ca']:
  feature_columns.append(feature_column.numeric_column(header))

# bucketized cols
age = feature_column.numeric_column(""age"")
age_buckets = feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])
feature_columns.append(age_buckets)

# indicator cols
thal = feature_column.categorical_column_with_vocabulary_list(
      'thal', ['fixed', 'normal', 'reversible'])
thal_one_hot = feature_column.indicator_column(thal)
feature_columns.append(thal_one_hot)

# embedding cols
thal_embedding = feature_column.embedding_column(thal, dimension=8)
feature_columns.append(thal_embedding)

#crossed cols
crossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=1000)
crossed_feature = feature_column.indicator_column(crossed_feature)
feature_columns.append(crossed_feature)

feature_layer = tf.keras.layers.DenseFeatures(feature_columns, trainable=False)


class MyModel(Model):
  def __init__(self):
    super(MyModel, self).__init__()
    self.features = feature_layer
    self.dense = layers.Dense(128, activation = 'relu')
    self.dense2 = layers.Dense(128, activation = 'relu')
    self.sigmoid = layers.Dense(1, activation = 'sigmoid')

  def call(self, x):
    x = self.features(x)
    x = self.dense(x)
    x = self.dense2(x)
    return self.sigmoid(x)

model = MyModel()
loss_object = tf.keras.losses.BinaryCrossentropy()
optimizer = tf.keras.optimizers.Adam()

train_loss = tf.keras.metrics.Mean(name='train_loss')
train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')

test_loss = tf.keras.metrics.Mean(name='test_loss')
test_accuracy = tf.keras.metrics.BinaryAccuracy(name='test_accuracy')

@tf.function
def train_step(features, label, counter):
  with tf.GradientTape() as tape:
    predictions = model(features)
    loss = loss_object(label, predictions)
  gradients = tape.gradient(loss, model.trainable_variables)
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))
  train_loss(loss)
  train_accuracy(label, predictions)

@tf.function
def test_step(image, label):
  predictions = model(image)
  t_loss = loss_object(label, predictions)

  test_loss(t_loss)
  test_accuracy(label, predictions)

EPOCHS = 5
for epoch in range(EPOCHS):
  counter = 0
  for features, labels in train_ds:
    train_step(features, labels, counter)
    counter +=1

  for features, labels in val_ds:
      test_step(features, labels)

  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'
  print (template.format(epoch+1,
                           train_loss.result(), 
                           train_accuracy.result()*100,
                           test_loss.result(), 
                           test_accuracy.result()*100))
```

**Actual Output**
```
Epoch 1, Loss: 1.2603175640106201, Accuracy: 61.65803146362305, Test Loss: 1.3003877401351929, Test Accuracy: 67.34693908691406
---------------------------------------------------------------------------
FailedPreconditionError                   Traceback (most recent call last)
<ipython-input-3-165a6f89be48> in <module>()
    110   counter = 0
    111   for features, labels in train_ds:
--> 112     train_step(features, labels, counter)
    113     counter +=1
    114 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    412       # In this case we have created variables on the first call, so we run the
    413       # defunned version which is guaranteed to never create variables.
--> 414       return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
    415     elif self._stateful_fn is not None:
    416       # In this case we have not created variables on the first call. So we can

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)
   1286     """"""Calls a graph function specialized to the inputs.""""""
   1287     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
-> 1288     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
   1289 
   1290   @property

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs)
    572     """"""
    573     return self._call_flat(
--> 574         (t for t in nest.flatten((args, kwargs))
    575          if isinstance(t, (ops.Tensor,
    576                            resource_variable_ops.ResourceVariable))))

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _call_flat(self, args)
    625     # Only need to override the gradient in graph mode and when we have outputs.
    626     if context.executing_eagerly() or not self.outputs:
--> 627       outputs = self._inference_function.call(ctx, args)
    628     else:
    629       self._register_gradient()

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in call(self, ctx, args)
    413             attrs=(""executor_type"", executor_type,
    414                    ""config_proto"", config),
--> 415             ctx=ctx)
    416       # Replace empty list with None
    417       outputs = outputs or None

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     64     else:
     65       message = e.message
---> 66     six.raise_from(core._status_to_exception(e.code, message), None)
     67   except TypeError as e:
     68     if any(ops._is_keras_symbolic_tensor(x) for x in inputs):

/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

FailedPreconditionError: Table already initialized.
	 [[{{node my_model_2/dense_features_2/age_bucketized_X_thal_indicator/thal_lookup/hash_table/table_init/LookupTableImportV2}}]] [Op:__inference_train_step_34833]
```

**Expected Output**
```
Epoch 1, Loss: 0.8113343119621277, Accuracy: 63.21243667602539, Test Loss: 0.5340840816497803, Test Accuracy: 71.42857360839844
Epoch 2, Loss: 0.6469629406929016, Accuracy: 69.4300537109375, Test Loss: 0.5265070199966431, Test Accuracy: 72.44898223876953
Epoch 3, Loss: 0.5749971270561218, Accuracy: 71.84800720214844, Test Loss: 0.5283268094062805, Test Accuracy: 72.10884094238281
Epoch 4, Loss: 0.5360371470451355, Accuracy: 72.79792785644531, Test Loss: 0.5270806550979614, Test Accuracy: 72.44898223876953
Epoch 5, Loss: 0.5122867226600647, Accuracy: 73.57512664794922, Test Loss: 0.5229357481002808, Test Accuracy: 72.65306091308594
```"
27085,initialize_or_restore() executed on InitializationOnlyStatus returned by tf.train.Checkpoint.restore fails to initialize custom tf.keras.layers.Layer trainables,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, provided below
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): various
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: None
- GPU model and memory: None

**Describe the current behavior**

In the self-contained test below, I am expecting `status.initialize_or_restore(session)` to initialize `my_dense/dense/kernel` and `my_dense/dense/bias`, which should be checkpointable, but it does not, as can be seen from the error message.

**Describe the expected behavior**

The test should execute normally.

**Code to reproduce the issue**

```
import tensorflow as tf
import numpy as np

class MyDense(tf.keras.layers.Layer):
    def __init__(self, units):
        super(MyDense, self).__init__()
        self.units = units

    def build(self, input_shape):
        self.dense = tf.keras.layers.Dense(units=self.units)

    def call(self, inputs, mask=None):
        return self.dense(inputs)

inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.float32)
model = tf.keras.models.Model(inputs=inputs, outputs=MyDense(units=1)(inputs))

checkpoint = tf.train.Checkpoint(model=model)
manager = tf.train.CheckpointManager(checkpoint, directory='./', max_to_keep=None)  # no checkpoint present
status = checkpoint.restore(manager.latest_checkpoint)

inputs_placeholder = tf.placeholder(dtype=tf.float32, shape=[None, 1])
called_model = model(inputs_placeholder)

with tf.Session() as session:
    status.initialize_or_restore(session)
    session.run(called_model, feed_dict={inputs_placeholder: np.array([[1.]])})
```

**Other info / logs**

WARNING:tensorflow:From /venv/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
2019-03-24 16:09:11.964524: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Traceback (most recent call last):
  File ""/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1334, in _do_call
    return fn(*args)
  File ""/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.FailedPreconditionError: Error while reading resource variable my_dense/dense/kernel from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/my_dense/dense/kernel/N10tensorflow3VarE does not exist.
	 [[{{node model/my_dense/dense/MatMul/ReadVariableOp}}]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/initialize_or_restore.py"", line 27, in <module>
    session.run(called_model, feed_dict={inputs_placeholder: np.array([[1.]])})
  File ""/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 929, in run
    run_metadata_ptr)
  File ""/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1328, in _do_run
    run_metadata)
  File ""/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.FailedPreconditionError: Error while reading resource variable my_dense/dense/kernel from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/my_dense/dense/kernel/N10tensorflow3VarE does not exist.
	 [[node model/my_dense/dense/MatMul/ReadVariableOp (defined at /initialize_or_restore.py:13) ]]

Caused by op 'model/my_dense/dense/MatMul/ReadVariableOp', defined at:
  File ""/initialize_or_restore.py"", line 23, in <module>
    called_model = model(inputs_placeholder)
  File ""/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 554, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 815, in call
    mask=masks)
  File ""/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 1002, in _run_internal_graph
    output_tensors = layer.call(computed_tensor, **kwargs)
  File ""/initialize_or_restore.py"", line 13, in call
    return self.dense(inputs)
  File ""/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 554, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/venv/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py"", line 975, in call
    outputs = gen_math_ops.mat_mul(inputs, self.kernel)
  File ""/venv/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 5333, in mat_mul
    name=name)
  File ""/venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 511, in _apply_op_helper
    preferred_dtype=default_dtype)
  File ""/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1175, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/venv/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 1222, in _dense_var_to_tensor
    return var._dense_var_to_tensor(dtype=dtype, name=name, as_ref=as_ref)  # pylint: disable=protected-access
  File ""/venv/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 1177, in _dense_var_to_tensor
    return self.value()
  File ""/venv/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 644, in value
    return self._read_variable_op()
  File ""/venv/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 728, in _read_variable_op
    self._dtype)
  File ""/venv/lib/python3.6/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py"", line 550, in read_variable_op
    ""ReadVariableOp"", resource=resource, dtype=dtype, name=name)
  File ""/venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""/venv/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3300, in create_op
    op_def=op_def)
  File ""/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1801, in __init__
    self._traceback = tf_stack.extract_stack()

FailedPreconditionError (see above for traceback): Error while reading resource variable my_dense/dense/kernel from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/my_dense/dense/kernel/N10tensorflow3VarE does not exist.
	 [[node model/my_dense/dense/MatMul/ReadVariableOp (defined at /initialize_or_restore.py:13) ]]


Process finished with exit code 1
"
27084,ModuleNotFoundError: No module named 'tensorflow.contrib',"Just cannot import from tensorflow.contrib in 1.13.xx, where do the packages be moved to? Thanks
"
27083,Build Failure: Cannot link XLA allocator with CUDA 10.1 and GCC 7.2.1,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ArchLInux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.13.1
- Python version: 3.7.2
- Installed using virtualenv? pip? conda?: system install to `/usr`
- Bazel version (if compiling from source): 0.22.0
- GCC/Compiler version (if compiling from source): GCC 7.2.1
- CUDA/cuDNN version: 10.1
- GPU model and memory: n/a



**Describe the problem**
Compiling Tensorflow results in failing the final linking step when linking the XLA GPU fusion instruction allocators.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```bash
export PYTHON_BIN_PATH=/usr/bin/python
export USE_DEFAULT_PYTHON_LIB_PATH=1
export TF_NEED_JEMALLOC=1
export TF_NEED_KAFKA=0
export TF_NEED_OPENCL_SYCL=0
export TF_NEED_AWS=0
export TF_NEED_GCP=0
export TF_NEED_HDFS=0
export TF_NEED_S3=0
export TF_ENABLE_XLA=1
export TF_NEED_GDR=0
export TF_NEED_VERBS=0
export TF_NEED_OPENCL=0
export TF_NEED_MPI=0
export TF_NEED_TENSORRT=0
export TF_NEED_NGRAPH=0
export TF_NEED_IGNITE=0
export TF_NEED_ROCM=0
export TF_SET_ANDROID_WORKSPACE=0
export TF_DOWNLOAD_CLANG=0
export TF_NCCL_VERSION=2.3
export NCCL_INSTALL_PATH=/usr
export CC_OPT_FLAGS=""-march=x86-64""
export TF_NEED_CUDA=1
export GCC_HOST_COMPILER_PATH=/usr/bin/gcc-7
export TF_CUDA_CLANG=0
export CUDA_TOOLKIT_PATH=/opt/cuda
export TF_CUDA_VERSION=$($CUDA_TOOLKIT_PATH/bin/nvcc --version | sed -n 's/^.*release \(.*\),.*/\1/p')
export CUDNN_INSTALL_PATH=/usr/lib
export TF_CUDNN_VERSION=$(sed -n 's/^#define CUDNN_MAJOR\s*\(.*\).*/\1/p' /usr/include/cudnn.h)
export TF_CUDA_COMPUTE_CAPABILITIES=3.5,3.7,5.0,5.2,5.3,6.0,6.1,6.2,7.0,7.2,7.5
./configure
bazel \
    build --config=opt \
      //tensorflow:libtensorflow.so \
      //tensorflow:libtensorflow_cc.so \
      //tensorflow:install_headers \
      //tensorflow/tools/pip_package:build_pip_package
bazel-bin/tensorflow/tools/pip_package/build_pip_package ${srcdir}/tmp
```


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```bash
ERROR: /build/tensorflow/src/tensorflow-1.13.1-cuda/tensorflow/BUILD:463:1: Linking of rule '//tensorflow:libtensorflow.so' failed (Exit 1)
/usr/bin/ld: bazel-out/k8-opt/bin/tensorflow/compiler/xla/service/gpu/libinstruction_fusion.pic.a(instruction_fusion.pic.o): in function `xla::gpu::GpuInstructionFusion::FusionWouldBeTooLarge(xla::HloInstruction const*, xla::HloInstruction const*)':
instruction_fusion.cc:(.text._ZN3xla3gpu20GpuInstructionFusion21FusionWouldBeTooLargeEPKNS_14HloInstructionES4_+0x7c): undefined reference to `std::allocator<xla::HloInstruction const*>::allocator()'
collect2: error: ld returned 1 exit status
INFO: Elapsed time: 4415.531s, Critical Path: 2250.52s
INFO: 15052 processes: 15052 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
```
"
27082,Win10: ImportError: DLL load failed: The specified module could not be found,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution: Windows 10 pro
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.12 GPU
- Python version: 3.6.6
- Installed using virtualenv? pip? conda?: pip
- CUDA/cuDNN version: CUDA 9.0 and cuDNN is  cudnn-9.0-windows10-x64-v7.1 (7.1.4)
- GPU model and memory: NVIDIA GeForce 940MX /  2GB



**Describe the problem**

I am facing following issue when I try to import tensorflow on python console 

import tensorflow as tf

>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\Ajay\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Ajay\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Ajay\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Ajay\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Ajay\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Ajay\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Ajay\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Ajay\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Ajay\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Ajay\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Ajay\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Ajay\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Ajay\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help



**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27081,Can't create non-trainable keras variables,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

`tf.GIT_VERSION, tf.VERSION == v1.13.1-0-g6612da8951, 1.13.1`

**Describe the current behavior**

You should be able to specify `K.variable(..., trainable=False)` to create a non-trainable variable. Currently, I'm having to do:

```python
import tensorflow.keras.backend as K

variable = K.variable(5)
variable._trainable = False
...
```

But this doesn't seem like a proper/safe way to do this.

**Describe the expected behavior**
```python
variable = K.variable(5, trainable=False)
```

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27079,Why this error on tensorflow 1.13.1 with python 2.7 : ImportError: No module named model_utils,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 1.13.1
- Python version: 2.7
- GCC/Compiler version (if compiling from source): 7.0
- CUDA/cuDNN version: 10.1
- GPU model and memory: RTX 2080Ti 11G


You can collect some of this information using our environment capture [script]
**Describe the current behavior**
Just got the import error
**Describe the expected behavior**
Should be able to run the code smoothly
**Other info / logs**
Traceback (most recent call last):
  File ""official_tensorflow_phased_lstm.py"", line 6, in <module>
    import tensorflow.contrib.slim as slim
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/__init__.py"", line 40, in <module>
    from tensorflow.contrib import distribute
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/__init__.py"", line 33, in <module>
    from tensorflow.contrib.distribute.python.tpu_strategy import TPUStrategy
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/tpu_strategy.py"", line 27, in <module>
    from tensorflow.contrib.tpu.python.ops import tpu_ops
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/__init__.py"", line 73, in <module>
    from tensorflow.contrib.tpu.python.tpu.keras_support import tpu_model as keras_to_tpu_model
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py"", line 62, in <module>
    from tensorflow.contrib.tpu.python.tpu import tpu
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu.py"", line 24, in <module>
    from tensorflow.contrib.compiler import xla
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/compiler/xla.py"", line 28, in <module>
    from tensorflow.python.estimator import model_fn as model_fn_lib
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/__init__.py"", line 26, in <module>
    from tensorflow_estimator.python import estimator
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/__init__.py"", line 8, in <module>
    from tensorflow_estimator._api.v1 import estimator
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>
    from tensorflow_estimator._api.v1.estimator import experimental
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>
    from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/__init__.py"", line 25, in <module>
    import tensorflow_estimator.python.estimator.estimator_lib
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator_lib.py"", line 22, in <module>
    from tensorflow_estimator.python.estimator.canned.baseline import BaselineClassifier
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/canned/baseline.py"", line 64, in <module>
    from tensorflow_estimator.python.estimator import estimator
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 66, in <module>
    from tensorflow_estimator.python.estimator import model_fn as model_fn_lib
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/model_fn.py"", line 36, in <module>
    from tensorflow_estimator.python.estimator.export import export_lib
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/export/export_lib.py"", line 25, in <module>
    from tensorflow.python.saved_model.model_utils import build_all_signature_defs
ImportError: No module named model_utils

"
27078,tensorflow 1.13.1 on linux on python 3.7 (not osx) uses -D_GLIBCXX_USE_CXX11_ABI=1 -- this behavior is undocumented and/or unspecified,"### System information
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Linux Ubuntu 18.04.2`
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: *N/A*
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version: `1.13.1`
- Python version: `3.7`
- Installed using virtualenv? pip? conda?: **pip** -- see excerpt in pantsbuild/pants#7424
- Bazel version (if compiling from source): *N/A*
- GCC/Compiler version (if compiling from source): `7.3.0`
- CUDA/cuDNN version: *N/A*
- GPU model and memory: *N/A*

### Problem
- `tensorflow==1.13.1`, specifically only on `Linux` for python `3.7`, uses `-D_GLIBCXX_USE_CXX11_ABI=1`, which contradicts the documentation at https://www.tensorflow.org/guide/extend/op#build_the_op_library, which specifically says that `-D_GLIBCXX_USE_CXX11_ABI=0` is used.
  - This caused an opaque compile error when building a custom operator in C++ in pants (see pantsbuild/pants#7417).
  - **Is this ABI change intentional?**
- **This does not affect any user who is building from source nor any user/tool which relies on `tensorflow.sysconfig.get_compile_flags()` to get compile flags.**
- *(tangential)* `self.test_session()` is deprecated, but is also used in the documentation at https://www.tensorflow.org/guide/extend/op.

### Exact commands / steps
pantsbuild/pants#7424 has command line logs from linux and osx to show that `tensorflow==1.13.1` on python 3.7 uses `-D_GLIBCXX_USE_CXX11_ABI=1` on linux, but not osx.

### Background
In pants, we are imitating the instructions in the tutorial, but we are not using the `tf.sysconfig` API right now (see pantsbuild/pants#7046). We had set `-D_GLIBCXX_USE_CXX11_ABI=0` for all tensorflow compiles, and found a failure when trying to run testing for our tensorflow example custom op project when trying to set up a python 3.7 Linux CI shard. This is not blocking us -- we have a well-functioning workaround in pantsbuild/pants#7424, but we would like to know if this change is intentional and whether we should expect new tensorflow releases to also use the newer C++ ABI."
27074,Celery hangs with Tensorflow shared model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 on Docker (18.03)
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6
- CUDA/cuDNN version: No gpu
- GPU model and memory: No gpu
- Celery 4.2.2, Keras 2.2.4

**Scenario**
I'm running a Celery worker which handless ml requests for prediction using LSTM NN. I'm loading the model in a main worker which passes it to the worker process as reference so that the model is not loaded all the time in a concurrent set-up. 

**Describe the current behavior**
When the request comes the execution hangs indefinitely when it hits the access to the passed model reference.  I've searched for a solution and found that the sessions cannot be shared across process which is the cause of the hang. I've tried several solutions which are mentioned lower. However only changing back end to Theano solved the issue. Is there any other solution for this issue or I have to keep loading model in worker who uses it for prediction? Or I am doing something wrong?  

**Describe the expected behavior**
I would like the set-up to work with Tensorflow as is with Theano that is to share a reference with workers to save memory. 

**Code to reproduce the issue**
```
main.py

from keras.models import load_model
from celery import Celery, states
from celery import Task

class DomainAnalyzer(Task):
    ignore_result = True
    def __init__(self):
        self.model = load_model(""lstm_model.h5"")

    def run(self, data):
        try:
            preprocessor = Preprocessor(data, self.model)
            data = preprocessor.predict_data()
            return data 
        except Exception as e:
            print(e)

app = Celery('worker', broker=""broker"")
app.register_task(DomainAnalyzer())

if __name__ == '__main__':
    app.start()
```
```
worker.py

class Preprocessor:

    def __init__(self, data: str, model):
        self.data= data
        self.model = model

    def data_analysis(self):
        return self.model.predict(self.data)
```

### Tried solution 1.
I've specified the multiprocessing start method. 
`multiprocessing.set_start_method('spawn', force=True)`

### Tried solution 2.
Passing graph object in worker
```
import tensorflow as tf
self.model = load_model(""lstm_model.h5"")
self.model._make_predict_function()
self.graph = tf.get_default_graph()
preprocessor = Preprocessor(data, self.model, self.graph)
```

In worker:
```
def data_analysis(self):
     with self.graph.as_default():
         return self.model.predict(self.data)

```
### Tried solution 3. (currently only one which works)
Use Theano :(

"
27070,Typo in doc: `tf.keras.losses.CategoricalCrossentropy`,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: 1.13
- Doc Link: [tf.keras.losses.CategoricalCrossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy#class_categoricalcrossentropy)


**Describe the documentation issue**
In the example code 

```
loss = cce(
    [[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]],
    [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]])
```

I think the second array in the second line should be `[.05, .89, .06]` instead of `[.5, .89, .6]`, as they're supposed to be summed to one.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
27067,TensorFlow stopped working with custom ops built with GCC5,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): tf-nightly >= 20190321
- Python version: Python 2, Python 3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): 5.4.0-6ubuntu1~16.04.11
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
TensorFlow fails with segmentation fault when using custom ops built with gcc5.  The segmentation fault originates from usage of `std::function` on the interface boundary, introduced in https://github.com/tensorflow/tensorflow/commit/41e7b3ca0abfd7e40a82ae38f96a9fbfeecb0ee5.

**Describe the expected behavior**
Custom ops built with gcc5 should continue working with TF built with gcc4.

**Code to reproduce the issue**
```
$ docker run -it tensorflow/tensorflow:nightly
# apt install -y mpich
# pip install horovod
# cat > test.py
import tensorflow as tf
import horovod.tensorflow as hvd
hvd.init()
sess = tf.Session()
sess.run(hvd.allreduce(tf.constant(1.0)))
^D
# python test.py
```

Outputs:
```
root@011ee61f092e:/# python test.py
2019-03-24 00:25:51.898835: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-03-24 00:25:51.910390: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz
2019-03-24 00:25:51.914924: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4269380 executing computations on platform Host. Devices:
2019-03-24 00:25:51.914981: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
Segmentation fault (core dumped)
root@011ee61f092e:/#
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

The reason for this issue is the fact that definition of `std::function` has changed between gcc4 and gcc5.

gcc4:` _M_invoke(const _Any_data& __functor, _ArgTypes... __args)`
gcc5: `_M_invoke(const _Any_data& __functor, _ArgTypes&&... __args)`

While this change is ABI-compatible, it produced segfault in situation where gcc4-compiled code is calling function defined in gcc5-compiled plugin.

Short repro:

Prepare files:
```
$ cat > std_function_fw.h
#include <functional>
int call_me(std::function<int(int)> f);
^D
$ cat > std_function_fw.cc
#include ""std_function_fw.h""
int call_me(std::function<int(int)> f) {
	return f(42);
}
^D
$ cat > std_function_client.cc
#include <iostream>
#include ""std_function_fw.h""
int main(int argc, char **argv) {
	std::cout << call_me([](int val) { return val + 201808; }) << std::endl;
}
^D
```

Mount to gcc4 docker (e.g. `debian:jessie`):
```
# g++ --std=c++11 -fPIC std_function_fw.cc -shared -o libstd_function_fw.so
# g++ --std=c++11 -fPIC std_function_client.cc -o std_function_client -lstd_function_fw -L.
# LD_LIBRARY_PATH=. ./std_function_client
<will work>
```

Mount to gcc5 docker (e.g. `ubuntu:16.04`), keep `libstd_function_fw.so`:
```
# g++ --std=c++11 -fPIC std_function_client.cc -o std_function_client -lstd_function_fw -L.
# LD_LIBRARY_PATH=. ./std_function_client
<will crash>
```

Proposed solution is to revert https://github.com/tensorflow/tensorflow/commit/41e7b3ca0abfd7e40a82ae38f96a9fbfeecb0ee5 and keep using function pointers on the plugin interface boundary w/o using `std::function`."
27065,"In tensorflow, why does a 6B GPU show with memory limit than 5GB of VRAM?","I have a 6GB 1060 GPU. Running:

```
from tensorflow.python.client import device_lib
device_lib.list_local_devices()
```
gives

```
name: ""/device:GPU:0""
 device_type: ""GPU""
 memory_limit: 4951913267
 locality {
   bus_id: 1
   links {
   }
 }
 incarnation: 15506209764385210283
 physical_device_desc: ""device: 0, name: GeForce GTX 1060 6GB, 
pci bus id: 0000:02:00.0, compute capability: 6.1""
```
Notice it is showing memory_limit of 4951913267, which is 1.1GB short of 6GB.  Where did my 1.1GB go?"
27064,Support for Kotlin Native?,As I see there is a full support for swift language now and I believe it will surely replace python as the primary language for building deep learning models by software developers. Will I see the same for Kotlin Native? 
27063,How Can I set input shape of Subclass Model ?,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): tensorflow 1.13
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**
in subclass model,

 def call(self,x):

When I call my mode(subclass) at first time, I can't check batch_size(None).



If I want to concatenate multiple layers, the model check input shape.

but it can't

How can I set input shape of subclass model?

    def call(self,x):
        """""" Forward """"""
        # [x,y]=Lambda(lambda x:[x[0],x[1]], output_shape=[-1,self.opt.ncond,self.opt.nc,self.opt.height,self.opt.width])(x)
        inputs = Reshape((self.opt.ncond * self.opt.nc, self.opt.height, self.opt.width))(x)  # batch_size, shape
        target = Reshape((self.opt.npred * self.opt.nc, self.opt.height, self.opt.width))(self.y)
        g_pred_v = K.variable(self.deterministic(inputs), name=""g_pred_v"")
        r=K.abs(g_pred_v-target) # residual
        z=self.phi_network_conv(r)
        s = self.deterministic.get_layer()[0](inputs)
        h=concatenate([s,z], axis=1) # concatenate
        # shape=K.int_shape(s)
        # h=Reshape((shape[1]*2, shape[2],shape[3]))(h)
        pred_f = self.f_network_decoder(h)
        return pred_f

    def get_target(self,y):
        # input layer
        self.y=y


 

"
27062,Relationship between batch size and CUDA load in deep learning task,"I am learning about machine translation by running this TensorFlow example on my GPU in my home PC: https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb

My GPU has 6GB of VRAM. I have to reduce the batch size to get the example code not to run out of memory on the GPU. With trial and error, I find a batch size (minimum is 1) where the code runs and uses the most memory. I notice that, as I reduce the batch size, the CUDA core load as reported by Windows Task Manager GPU view goes down.

The application described in the link above creates a complex TensorFlow network. I don't know whether Tensorflow creates one copy of the network or multiple copies to load the GPU.

If it can create multiple copies, is there a TensorFlow switch for that? I don't think memory speed should be a bottleneck in feeding the GPU. That is, I should be able to optimize between batch size or number of jobs resident in the GPU, and number of compute networks in the GPU.

Is there an easy way in TensorFlow to assess the size in CUDA cores of a compute network?

Question: What are the factors I can use to optimize CUDA load in a small GPU with a large deep learning task?"
27060,Where is the api of PhasedLSTM in tensorflow 2.0.0 alpha? ,"Just like the title suggests, where can I find the PhasedLSTM api, or there is still lacking of that? Thanks
"
27059,"~\Anaconda3\lib\imp.py......ImportError: DLL load failed: The specified module could not be found - Win10 Pro, Core I3 with AMD Radeon HD7500M Accelerator!","Hi All,

Facing error -  _ImportError: DLL load failed: The specified module could not be found!_

Can anyone provide a 'working' env info for **AMD Radeon HD7500M** GPU alongside the following:

Win10 Pro, FU 1803
Core I3 3rd Gen (2370M), 4GB
CUDA ver.10.0.130
CUDNN ver 7.3.1 for 10.0_0
Anaconda 2018.12
Py 3.7.1
Tensorflow-gpu 1.13.1 - Installed using 'Conda' in an 'Anaconda prompt'!

Thanks!"
27058,"Is the task type ""master"" for distributed training deprecated?","Should task type ""master"" be immediately replaced with ""chief""?
For example, `CollectiveAllReduceStrategy` only use ""chief"" (not ""master"") and ""worker"":
https://github.com/tensorflow/tensorflow/blob/36f817a9f3e7d2339cb53b91ddc508b3e25ab761/tensorflow/python/distribute/multi_worker_util.py#L159-L161

However, in some environment such as Google Cloud ML Engine, environment variable `TF_CONFIG` with ""master"" (not ""chief""), ""worker"", and ""ps"".

Do you have any plan to keep compatibility?
"
27056,ImportError: cannot import name trt_engine_op when using `tensorrt.create_inference_graph`,"hello, I use `tensorflow-gpu-2.0.0a0`. Here I want to use `tensorrt` to convert a tensorflow `saved_model` to a `output_saved_model` optimized by tensorrt. But when I run code:

```
import os
import sys
from tensorflow.python.compiler import tensorrt
tensorrt.create_inference_graph(
  None,
  None,
  max_batch_size=1,
  max_workspace_size_bytes=1<<32,
  precision_mode='FP32',
  minimum_segment_size=3,
  is_dynamic_op=True,
  input_saved_model_dir='/tmp/imagenet_model/resnet/saved_model/1553141893',
  input_saved_model_tags=['serve'],
  output_saved_model_dir='/tmp/imagenet_model/resnet/trt_saved_model/1553141893')
```

it shows error:
```
**** Failed to import TF-TRT ops. This is because the binary was not built with CUDA or TensorRT enabled. ****
Traceback (most recent call last):
  File ""convert.py"", line 20, in <module>
    output_saved_model_dir='/tmp/imagenet_model/resnet/trt_saved_model/1553141893')
  File ""/xxx/.virtualenvs/tf/lib/python2.7/site-packages/tensorflow/python/compiler/tensorrt/trt_convert.py"", line 880, in create_inference_graph
    use_calibration=use_calibration)
  File ""/xxx/.virtualenvs/tf/lib/python2.7/site-packages/tensorflow/python/compiler/tensorrt/trt_convert.py"", line 637, in __init__
    trt_ops.load_trt_ops()
  File ""/xxx/.virtualenvs/tf/lib/python2.7/site-packages/tensorflow/compiler/tf2tensorrt/python/ops/trt_ops.py"", line 49, in load_trt_ops
    raise e
ImportError: cannot import name trt_engine_op
```

how to fix it? anyone can give some advises?"
27055,Deeplab TypeError: MonitoredTrainingSession() got an unexpected keyword argument 'summary_dir',"  When i am running train.py file for pascal voc dataset then fallowing error occures
file ""train.py"", line 500, in <module>
    tf.app.run()
  File ""/home/suraj/anaconda2/envs/tensor/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""train.py"", line 492, in main
    hooks=[stop_hook]) as sess:
TypeError: MonitoredTrainingSession() got an unexpected keyword argument 'summary_dir'
"
27054,tensorflow 2.0 transfer_learning tutorial: tensorflow_datasets error on local jupyter notebook Anaconda Win10,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Anaconda on Windows 10
- TensorFlow installed from (source or binary): install by pip in anaconda environment
- TensorFlow version (use command below): tensorflow-gpu 2.0 alpha
- Python version: 3.6.8
- CUDA/cuDNN version: Cuda toolkit 10.0; cuDNN 7.5
- GPU model and memory: GTX 980 Ti 6Gb


**Describe the current behavior**
I run the [transfer_learning.ipynb](https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/images/transfer_learning.ipynb) tutorial on/r2 for Tensroflow 2.0 on Jupyter Notebook in Anaconda Windows 10 and I get the error on step:
```
SPLIT_WEIGHTS = (8, 1, 1)
splits = tfds.Split.TRAIN.subsplit(weighted=SPLIT_WEIGHTS)

(raw_train, raw_validation, raw_test), metadata = tfds.load(
    'cats_vs_dogs', split=list(splits),
    with_info=True, as_supervised=True)
```

**Error**: 
```
`Downloading / extracting dataset cats_vs_dogs (786.68 MiB) to C:\Users\Khoa\tensorflow_datasets\cats_vs_dogs\2.0.0...

Dl Completed...: 0 url [00:00, ? url/s]

Dl Size...: 0 MiB [00:00, ? MiB/s]



0 examples [00:00, ? examples/s]
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-7-2bc776459ab0> in <module>
      4 (raw_train, raw_validation, raw_test), metadata = tfds.load(
      5     'cats_vs_dogs', split=list(splits),
----> 6     with_info=True, as_supervised=True)

D:\Anaconda\envs\tf2\lib\site-packages\tensorflow_datasets\core\api_utils.py in disallow_positional_args_dec(fn, instance, args, kwargs)
     50     _check_no_positional(fn, args, ismethod, allowed=allowed)
     51     _check_required(fn, kwargs)
---> 52     return fn(*args, **kwargs)
     53 
     54   return disallow_positional_args_dec(wrapped)  # pylint: disable=no-value-for-parameter

D:\Anaconda\envs\tf2\lib\site-packages\tensorflow_datasets\core\registered.py in load(name, split, data_dir, batch_size, download, as_supervised, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs)
    251   if download:
    252     download_and_prepare_kwargs = download_and_prepare_kwargs or {}
--> 253     dbuilder.download_and_prepare(**download_and_prepare_kwargs)
    254 
    255   if as_dataset_kwargs is None:

D:\Anaconda\envs\tf2\lib\site-packages\tensorflow_datasets\core\api_utils.py in disallow_positional_args_dec(fn, instance, args, kwargs)
     50     _check_no_positional(fn, args, ismethod, allowed=allowed)
     51     _check_required(fn, kwargs)
---> 52     return fn(*args, **kwargs)
     53 
     54   return disallow_positional_args_dec(wrapped)  # pylint: disable=no-value-for-parameter

D:\Anaconda\envs\tf2\lib\site-packages\tensorflow_datasets\core\dataset_builder.py in download_and_prepare(self, download_dir, download_config)
    217         self._download_and_prepare(
    218             dl_manager=dl_manager,
--> 219             max_examples_per_split=download_config.max_examples_per_split)
    220 
    221         # NOTE: If modifying the lines below to put additional information in

D:\Anaconda\envs\tf2\lib\site-packages\tensorflow_datasets\core\dataset_builder.py in _download_and_prepare(self, dl_manager, max_examples_per_split)
    666       self._file_format_adapter.write_from_generator(
    667           make_generator_fn(**split_generator.gen_kwargs),
--> 668           output_files,
    669       )
    670 

D:\Anaconda\envs\tf2\lib\site-packages\tensorflow_datasets\core\file_format_adapter.py in write_from_generator(self, generator_fn, output_files)
    105     wrapped = (
    106         _dict_to_tf_example(d).SerializeToString() for d in generator_fn())
--> 107     _write_tfrecords_from_generator(wrapped, output_files, shuffle=True)
    108 
    109   def dataset_from_filename(self, filename):

D:\Anaconda\envs\tf2\lib\site-packages\tensorflow_datasets\core\file_format_adapter.py in _write_tfrecords_from_generator(generator, output_files, shuffle)
    270     with _close_on_exit(writers) as writers:
    271       logging.info(""Writing TFRecords"")
--> 272       _round_robin_write(writers, generator)
    273     # Shuffle each shard
    274     if shuffle:

D:\Anaconda\envs\tf2\lib\site-packages\tensorflow_datasets\core\file_format_adapter.py in _round_robin_write(writers, generator)
    283 def _round_robin_write(writers, generator):
    284   """"""Write records from generator round-robin across writers.""""""
--> 285   for i, example in enumerate(tqdm.tqdm(generator, unit="" examples"")):
    286     writers[i % len(writers)].write(example)
    287 

D:\Anaconda\envs\tf2\lib\site-packages\tqdm\_tqdm.py in __iter__(self)
   1020                 """"""), fp_write=getattr(self.fp, 'write', sys.stderr.write))
   1021 
-> 1022             for obj in iterable:
   1023                 yield obj
   1024                 # Update and possibly print the progressbar.

D:\Anaconda\envs\tf2\lib\site-packages\tensorflow_datasets\core\file_format_adapter.py in <genexpr>(.0)
    104   def write_from_generator(self, generator_fn, output_files):
    105     wrapped = (
--> 106         _dict_to_tf_example(d).SerializeToString() for d in generator_fn())
    107     _write_tfrecords_from_generator(wrapped, output_files, shuffle=True)
    108 

D:\Anaconda\envs\tf2\lib\site-packages\tensorflow_datasets\core\dataset_builder.py in generator_fn()
    636 
    637       def generator_fn():
--> 638         for i, ex in enumerate(self._generate_examples(**kwargs)):
    639           # Use the DatasetInfo FeaturesDict to encode the example. This allows
    640           # the user's function to simply yield raw examples from the source

D:\Anaconda\envs\tf2\lib\site-packages\tensorflow_datasets\image\cats_vs_dogs.py in _generate_examples(self, archive)
    104     if num_skipped != _NUM_CORRUPT_IMAGES:
    105       raise ValueError(""Expected % corrupt images, but found %d"" % (
--> 106           _NUM_CORRUPT_IMAGES, num_skipped))
    107     logging.warning(""%d images were corrupted and were skipped"", num_skipped)

ValueError: Expected ۊorrupt images, but found 0`
```


**Describe the expected behavior**
The code run perfectly on my Jupyter Notebook - Anaconda server Ubuntu 16.04 and also on Colab.
My Jupyter notebook is 5.7.4 on both windows and ubuntu.

"
27053,tf.Session and tf.ConfigProto do not work in the new TF2.0,"tf.Session and tf.ConfigProto do not work in the new TF2.0; what other options do I have?

with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as session:
        result = session.run(sum_operation)
        print(result)"
27052,Unable to import tensorflow in Gpu after successful  installation,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Anaonda 
- TensorFlow version: 1.13.1
- Python version: 3.7
- Installed using virtualenv? pip? conda?: conda 
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 9.0
- GPU model and memory: nvidia getforce Gtx- 4GB 



**Describe the problem**

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-4-fc42d8cf7cab> in <module>
----> 1 import tensorflow as tf
      2 from tensorflow.keras.callbacks import TensorBoard
      3 from tensorflow.keras import models,layers,optimizers
      4 from tensorflow.keras.models import Sequential
      5 from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten,BatchNormalization

~\Anaconda3\lib\site-packages\tensorflow\__init__.py in <module>
     22 
     23 # pylint: disable=g-bad-import-order
---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
     25 
     26 from tensorflow._api.v1 import app

~\Anaconda3\lib\site-packages\tensorflow\python\__init__.py in <module>
     61 
     62 # Framework
---> 63 from tensorflow.python.framework.framework_lib import *  # pylint: disable=redefined-builtin
     64 from tensorflow.python.framework.versions import *
     65 from tensorflow.python.framework import config

~\Anaconda3\lib\site-packages\tensorflow\python\framework\framework_lib.py in <module>
     23 # Classes used when building a Graph.
     24 from tensorflow.python.framework.device import DeviceSpec
---> 25 from tensorflow.python.framework.ops import Graph
     26 from tensorflow.python.framework.ops import Operation
     27 from tensorflow.python.framework.ops import Tensor

~\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in <module>
     42 from tensorflow.python.eager import tape
     43 from tensorflow.python.framework import c_api_util
---> 44 from tensorflow.python.framework import composite_tensor
     45 from tensorflow.python.framework import device as pydev
     46 from tensorflow.python.framework import dtypes

~\Anaconda3\lib\site-packages\tensorflow\python\framework\composite_tensor.py in <module>
     24 
     25 from tensorflow.python import pywrap_tensorflow
---> 26 from tensorflow.python.util import nest
     27 
     28 

~\Anaconda3\lib\site-packages\tensorflow\python\util\nest.py in <module>
    104 _is_mapping = _pywrap_tensorflow.IsMapping
    105 _is_attrs = _pywrap_tensorflow.IsAttrs
--> 106 _is_composite_tensor = _pywrap_tensorflow.IsCompositeTensor
    107 
    108 

AttributeError: module 'tensorflow.python.pywrap_tensorflow' has no attribute 'IsCompositeTensor'


**Any other info / logs**
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-4-fc42d8cf7cab> in <module>
----> 1 import tensorflow as tf
      2 from tensorflow.keras.callbacks import TensorBoard
      3 from tensorflow.keras import models,layers,optimizers
      4 from tensorflow.keras.models import Sequential
      5 from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten,BatchNormalization

~\Anaconda3\lib\site-packages\tensorflow\__init__.py in <module>
     22 
     23 # pylint: disable=g-bad-import-order
---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
     25 
     26 from tensorflow._api.v1 import app

~\Anaconda3\lib\site-packages\tensorflow\python\__init__.py in <module>
     61 
     62 # Framework
---> 63 from tensorflow.python.framework.framework_lib import *  # pylint: disable=redefined-builtin
     64 from tensorflow.python.framework.versions import *
     65 from tensorflow.python.framework import config

~\Anaconda3\lib\site-packages\tensorflow\python\framework\framework_lib.py in <module>
     23 # Classes used when building a Graph.
     24 from tensorflow.python.framework.device import DeviceSpec
---> 25 from tensorflow.python.framework.ops import Graph
     26 from tensorflow.python.framework.ops import Operation
     27 from tensorflow.python.framework.ops import Tensor

~\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in <module>
     42 from tensorflow.python.eager import tape
     43 from tensorflow.python.framework import c_api_util
---> 44 from tensorflow.python.framework import composite_tensor
     45 from tensorflow.python.framework import device as pydev
     46 from tensorflow.python.framework import dtypes

~\Anaconda3\lib\site-packages\tensorflow\python\framework\composite_tensor.py in <module>
     24 
     25 from tensorflow.python import pywrap_tensorflow
---> 26 from tensorflow.python.util import nest
     27 
     28 

~\Anaconda3\lib\site-packages\tensorflow\python\util\nest.py in <module>
    104 _is_mapping = _pywrap_tensorflow.IsMapping
    105 _is_attrs = _pywrap_tensorflow.IsAttrs
--> 106 _is_composite_tensor = _pywrap_tensorflow.IsCompositeTensor
    107 
    108 

AttributeError: module 'tensorflow.python.pywrap_tensorflow' has no attribute 'IsCompositeTensor'
"
27050,Immediately build `tf.keras.layers.Layer` if `input_dim` or `input_shape` are specified,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.0alpha0
- Are you willing to contribute it (Yes/No): Y



**Describe the feature and the current behavior/state.**
Currently `tf.keras.layers.Layer`(and its subclasses like `tf.keras.layers.Dense`) is lazily built when the first time it sees input tensors since it needs to know the input shape or input dim, but after `input_dim` or `input_shape`  are specified in the constructor, there is no need for that, it should get immediately built because users expect to see variables attached to this layer right after they provide `input_dim` or `input_shape` 

**Will this change the current api? How?**
No, no need to change the current api.

**Who will benefit with this feature?**
I believe lots of users benefit with this feature

**Any Other info.**
```python
import tensorflow as tf
from tensorflow.python import keras

d = keras.layers.Dense(2, input_dim=3)
# this line should raise errors since the input_dim is 3 not 4, but it works in TF 2.0alpha0
# if `tf.keras.layers.Dense` will be immediately built if `input_dim` is specified, then
# errors that (1, 4) can't do matmul with (2, 3) will be rasied, much more clear right?
o = d(tf.random.normal(shape=(1, 4)))
```"
27049,Resuming training from saved checkpoint produces different result than uninterrupted training,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes ([Google Collab](https://colab.research.google.com/drive/1ZRhX4VBEWo4fh4zbXL_p-HDCUGTRelnH))
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Collab
- TensorFlow installed from (source or binary): Google Collab
- TensorFlow version (use command below): 1.13.1
- Python version: Python 3

**Describe the current behavior**
Loading a model with `tf.keras.models.load_model`, produced with `tf.keras.callbacks.ModelCheckpoint` , and resuming training produces different results from running the training without save model + restore model in the middle.

**Describe the expected behavior**
Saving and restoring the model should allow to resume training as if there was no interruption in the first place.

**Code to reproduce the issue**
[Google Collab](https://colab.research.google.com/drive/1ZRhX4VBEWo4fh4zbXL_p-HDCUGTRelnH)

**Other info / logs**
No interruption:

> Epoch 49/100
>  - 0s - loss: 3.5190 - val_loss: 3.3597
> Epoch 50/100
>  - 0s - loss: 3.4090 - val_loss: 3.2668
> Epoch 51/100
>  - 0s - loss: 3.2637 - val_loss: 3.1623
> Epoch 52/100
>  - 0s - loss: 3.0962 - val_loss: 2.9975

With interruption:

> Epoch 49/50
>  - 0s - loss: 3.5190 - val_loss: 3.3597
> Epoch 50/50
> 
> Epoch 00050: saving model to weights.50.ckpt
>  - 0s - loss: 3.4090 - val_loss: 3.2668
> ... `load_model('weights.50.ckpt')` ...
> Epoch 51/100
>  - 0s - loss: 3.2637 - val_loss: 3.3816
> Epoch 52/100
>  - 0s - loss: 3.3175 - val_loss: 3.1457

The model does not have any random elements, so it looks like optimizer state is lost.
"
27047,tf.train.BytesList should accept bytearray as an input type,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.13
- Are you willing to contribute it (Yes/No): I don't know how to write a *.proto file, so no



**Describe the feature and the current behavior/state.**

If something is called a `BytesList`, then I should be allowed to pass in a `bytearray` to it
Currently I cannot.
For example:
```
import tensorflow as tf
img = bytearray([1, 2, 3, 4])
print(img)
feat = tf.train.Feature(bytes_list=tf.train.BytesList(value=[img]))
```


**Will this change the current api? How?**

It will make the tf.train.BytesList class more intuitive to use

**Who will benefit with this feature?**

Everyone trying to do image recognition

**Any Other info.**
"
27046,tf.Module doesn't see variables from keras layers,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-alpha0
- Python version: 3.6.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
User-defined subclasses of `tf.Module` do not recognize and include variables from keras layer object members.

**Describe the expected behavior**
Subclassing `tf.Module` should expose the variables (not to mention training_variables) from all sub-computations, including keras layers, so long as these computations are set as members of the class.

**Code to reproduce the issue**
This came from a TF2.0 port of the pytorch dynamic graph example [here](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-control-flow-weight-sharing).
```
import tensorflow as tf
import numpy as np
import random

class DynamicNet(tf.Module):
    def __init__(self, D_in, H, D_out):
        self.input_linear = tf.keras.layers.Dense(H, input_dim=D_in)
        self.middle_linear = tf.keras.layers.Dense(H, input_dim=H)
        self.output_linear = tf.keras.layers.Dense(D_out, input_dim=H)

    def __call__(self, x):
        h_relu = tf.maximum(self.input_linear(x), 0)
        for _ in range(random.randint(0, 3)):
            h_relu = tf.maximum(self.middle_linear(h_relu), 0)
        y_pred = self.output_linear(h_relu)
        return y_pred

N, D_in, H, D_out = 64, 1000, 100, 10

x = np.random.randn(N, D_in)

model = DynamicNet(D_in, H, D_out)
model(x)
assert len(model.variables) > 0
```
(This particular example can be fixed by manually defining a `@property` variables which combines the variables members from each of the Dense layers into one list, but I think this partially defeats the point of using `tf.Module`).

**Other info / logs**
I'm not completely sure this is a bug/oversight, or if `tf.Module` was never intended to be compatible with keras layers. If that's the case, perhaps this is more appropriately categorized as a feature request. It seems like a natural thing to be able to do based on the the [tf.Module RFC](https://github.com/tensorflow/community/blob/master/rfcs/20190117-tf-module.md)."
27045,Cannot remove all warnings,"Run the following code:

```
import os, logging

os.environ[""TF_CPP_MIN_LOG_LEVEL""] = ""3""
logging.getLogger(""tensorflow"").setLevel(logging.CRITICAL)
logging.getLogger(""tensorflow_hub"").setLevel(logging.CRITICAL)

import tensorflow as tf
import tensorflow_hub as hub

print(tf.contrib.util.constant_value(tf.ones([1])))
```
Then you will get the following warnings:
```

WARNING: Logging before flag parsing goes to stderr.
W0322 16:52:19.971889 139848381953792 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14

WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
If you depend on functionality not listed there, please file an issue.

```

How can I remove these disgusting messages?"
27044,math_ops.div_no_nan support for float16,"**System information**
- OS Platform and Distribution: Ubuntu 18.04
- TensorFlow installed from: binary
- TensorFlow version: 1.14.1-dev20190306
- Python version: 3.6
- CUDA/cuDNN version: 1.01
- GPU model and memory: M60

**Describe the current behavior**
math_ops.div_no_nan only allows `float32` or `float64`, this limits the ability to use `MirroredStrategy` with `float16`"
27043,"In Eager mode,  batch norm doesn't support Second order derivative","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.4.1708 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):  1.12.0  and  1.10.1
- Python version:  3.6 
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 9.0, cuDNN - 7
- GPU model and memory:  TITAN Xp

**Describe the current behavior**

I wanted to implement an eager mode version of the large margin code from google-research [code](https://github.com/google-research/google-research/tree/master/large_margin).

In other words, I computed a second order derivative. If my model/network didn't contain/enable the batch norm layers, it was fine. 

But if I enabled the batch norm layer, it said that 
""tf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead."" however I didn't use tf.gradients and other layers didn't raise such an error.

The static/graph code work fine on both cases(with or without batch normalization)

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
import os
import numpy as np
import tensorflow as tf

os.environ[""TF_CPP_MIN_LOG_LEVEL""] = '2'
os.environ[""CUDA_DEVICE_ORDER""] = ""PCI_BUS_ID""
os.environ['CUDA_VISIBLE_DEVICES'] = """"

tf_config = tf.ConfigProto()
tf_config.gpu_options.allow_growth = True
tf.enable_eager_execution(tf_config)

EPS = 1e-5
MOMENTUM = 0.9

class ConfigDict(object):
    def __init__(self):
        self.num_classes = 10

        # List of tuples specify (kernel_size, number of filters) for each layer.
        self.filter_sizes_conv_layers = [(5, 32), (5, 64)]
        # Dictionary of pooling type (""max""/""average"", size and stride).
        self.pool_params = {""type"": ""max"", ""size"": 2, ""stride"": 2}
        self.num_units_fc_layers = [512]
        self.dropout_rate = 0
        self.batch_norm = True
        self.activation = tf.nn.relu
    
def pool2d_layer(inputs, pool_type, pool_size=2, pool_stride=2):
    if pool_type == ""max"":
        # Max pooling layer
        return tf.layers.max_pooling2d(
            inputs, pool_size=[pool_size] * 2, strides=pool_stride)
    
    
class MNISTNetwork(tf.keras.Model):
    """"""MNIST model. """"""

    def __init__(self, config):
        super(MNISTNetwork, self).__init__()
        self.num_classes = config.num_classes
        self.var_list = []
        self.init_ops = None
        self.activation = config.activation
        self.filter_sizes_conv_layers = config.filter_sizes_conv_layers
        self.num_units_fc_layers = config.num_units_fc_layers
        self.pool_params = config.pool_params
        self.dropout_rate = config.dropout_rate
        self.batch_norm = config.batch_norm
        self.conv_layers = []
        self.bn_layers = []
        in_channel = 1
        for i, filter_size in enumerate(self.filter_sizes_conv_layers):
            f_size = filter_size[0]
            conv_layer = tf.layers.Conv2D(kernel_size=filter_size[0], filters=filter_size[1], 
                                          strides=(1, 1), padding=""same"",
                                          activation=self.activation, 
                                          use_bias=not self.batch_norm)
            self.conv_layers.append(conv_layer)
            batch_norm_layer = tf.keras.layers.BatchNormalization(momentum=MOMENTUM, epsilon=EPS)
            self.bn_layers.append(batch_norm_layer)
            in_channel = filter_size[1]
            
        self.fc_layers = []
        in_shape = 64 * 7 * 7
        for i, num_units in enumerate(self.num_units_fc_layers):
            fc_layer = tf.layers.Dense(num_units, activation=self.activation)
            self.fc_layers.append(fc_layer)
            in_shape = num_units
        self.output_layer = tf.layers.Dense(self.num_classes, activation=None)

    def __call__(self, images, is_training=False):
        """"""Builds model.""""""
        net = images
        for i in range(len(self.filter_sizes_conv_layers)):
            net = self.conv_layers[i](net)

            if self.pool_params:
                net = pool2d_layer(net, pool_type=self.pool_params[""type""], pool_size=self.pool_params[""size""]
                                   , pool_stride=self.pool_params[""stride""])
            if self.dropout_rate > 0:
                net = tf.layers.dropout(net, rate=self.dropout_rate, training=is_training)
                
            if self.batch_norm:
                # net = tf.layers.batch_normalization(net, training=is_training, epsilon=EPS, momentum=MOMENTUM)
                net = self.bn_layers[i](net, training=is_training)
            
        net = tf.layers.flatten(net)

        for i in range(len(self.num_units_fc_layers)):
            net = self.fc_layers[i](net)
        logits = self.output_layer(net)
        return logits
    

config = ConfigDict()

# enable/disable batch norm
config.batch_norm = True

model = MNISTNetwork(config)

images = np.random.uniform(0, 1, (3, 28, 28, 1))
images = tf.convert_to_tensor(images, dtype=np.float32)
# images = tf.Variable(images)
print(""data %.5f"" % images.numpy().sum())

with tf.GradientTape(persistent=True) as t:
    with tf.GradientTape(persistent=True) as t2:
        logits = model(images, is_training=True)
        m = tf.reduce_sum(logits)
        print(logits.numpy().sum())
        dp_dx = t2.gradient(m, model.variables)
    print(""first"", dp_dx[0].numpy().sum())
    d2y_dx2 = t.gradient(dp_dx[0], model.variables)
    print(""second order"", d2y_dx2[0].numpy().sum())
```


**Other info / logs**

```
RuntimeError                              Traceback (most recent call last)
<ipython-input-86-44f63511f093> in <module>
     20         dp_dx = t2.gradient(m, model.variables)
     21     print(""first"", dp_dx[0].numpy().sum())
---> 22     d2y_dx2 = t.gradient(dp_dx[0], model.variables)
     23     print(""second order"", d2y_dx2[0].numpy().sum())

~/software/miniconda3/envs/dllib3/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in gradient(self, target, sources, output_gradients)
    899         nest.flatten(target),
    900         flat_sources,
--> 901         output_gradients=output_gradients)
    902 
    903     if not self._persistent:

~/software/miniconda3/envs/dllib3/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py in imperative_grad(tape, target, sources, output_gradients)
     62       target,
     63       sources,
---> 64       output_gradients)

~/software/miniconda3/envs/dllib3/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in _gradient_function(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads)
    115     return [None] * num_inputs
    116 
--> 117   return grad_fn(mock_op, *out_grads)
    118 
    119 

~/software/miniconda3/envs/dllib3/lib/python3.6/site-packages/tensorflow/python/ops/nn_grad.py in _FusedBatchNormGradGrad(op, *grad)
    938   grad_initial = [grad_grad_x, grad_grad_scale, grad_grad_offset]
    939   grad_grad_y, grad_x, grad_scale = gradients_impl.gradients(
--> 940       [grad_x, grad_scale, grad_offset], [grad_y, x, scale], grad_initial)
    941   return grad_grad_y, grad_x, grad_scale, None, None
    942 

~/software/miniconda3/envs/dllib3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients)
    628   with ops.get_default_graph()._mutation_lock():  # pylint: disable=protected-access
    629     return _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops,
--> 630                             gate_gradients, aggregation_method, stop_gradients)
    631 
    632 

~/software/miniconda3/envs/dllib3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, src_graph)
    642   """"""Implementation of gradients().""""""
    643   if context.executing_eagerly():
--> 644     raise RuntimeError(""tf.gradients is not supported when eager execution ""
    645                        ""is enabled. Use tf.GradientTape instead."")
    646   if src_graph is None:

RuntimeError: tf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead.
```"
27042,tf.sparse: limited functionality,"1. `tf.sparse.sparse_dense_matmul` only support matrix of rank 2, why cannot it support higher ranks?
2. `tf.sparse.to_dense` cannot support back-propagation at least in and before TF 1.13
3. there is no element-wise multiplication for sparse tensors similar to tf.multiply
4. there is no matmul between two sparse tensors"
27035,Tensorflow 1.13.1 import Error on Python 3.7.1,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution : Windows 10 Pro, 64-bit
- TensorFlow installed from (source or binary): Binary
- TensorFlow version: 1.13.1
- Python version: 3.7.1
- Installed using virtualenv? pip? conda?: pip on a conda env

Firstly, I reinstalled in my old environment with python 3.5.6, and have faced the same error, then created a new one with 3.7.1 after seeing @gunan  on the #22300 issue .
I checked that i have the : _pywrap_tensorflow_internal.lib on the directory ""..\Lib\site-packages\tensorflow\python""
i don't know what causes the error since tf 1.13.1 supports python 3.7

`Traceback (most recent call last):
  File ""C:\Users\EMP\Anaconda3\envs\MachineLearning\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\EMP\Anaconda3\envs\MachineLearning\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\EMP\Anaconda3\envs\MachineLearning\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\EMP\Anaconda3\envs\MachineLearning\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\EMP\Anaconda3\envs\MachineLearning\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\EMP\Anaconda3\envs\MachineLearning\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\EMP\Anaconda3\envs\MachineLearning\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\EMP\Anaconda3\envs\MachineLearning\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\EMP\Anaconda3\envs\MachineLearning\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\EMP\Anaconda3\envs\MachineLearning\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\EMP\Anaconda3\envs\MachineLearning\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\EMP\Anaconda3\envs\MachineLearning\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\EMP\Anaconda3\envs\MachineLearning\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.` 
"
27034,Unexpected log output in Tensorboard,"Env:
Tensorflow-GPU 1.13.1
Tensorboard 1.13.1
TensorboardX 1.6
Installed time: 2019/03/22
Installed from PIP
OS: Windows 10
Python version: 3.7
CUDA: 10

Problem:
I use command ""tensorboard --logdir ... --host 127.0.0.1"" to run the program, and there are many log output in the command. And not add ""-v"" parameter..

TensorBoard 1.13.1 at http://127.0.0.1:6006 (Press CTRL+C to quit)
I0322 17:56:22.683215 10068 _internal.py:97] 127.0.0.1 - - [22/Mar/2019 17:56:22] ""GET / HTTP/1.1"" 200 -
I0322 17:56:23.341972 12492 _internal.py:97] 127.0.0.1 - - [22/Mar/2019 17:56:23] ""GET /font-roboto/oMMgfZMQthOryQo9n22dcuvvDin1pK8aKteLpeZ5c0A.woff2 HTTP/1.1"" 200 -
I0322 17:56:24.188966 10068 _internal.py:97] 127.0.0.1 - - [22/Mar/2019 17:56:24] ""GET /tf-interactive-inference-dashboard/editedexample.png HTTP/1.1"" 200 -
...

This does not appare in the previous version. I not sure whether it is a bug or a new feature."
27033,"gpu doesn't report error when id is actually out of range in tf.nn.embedding_lookup(embed, ids) ","- TensorFlow version (use command below):1.13
- Python version:3.6


```
import tensorflow as tf
x = tf.constant(999)
#with tf.device('/cpu:0'):
#    voc_emb = tf.get_variable(name=""voc_emb"", dtype=tf.float32, shape=[3, 12])
#    embedded_x = tf.nn.embedding_lookup(voc_emb, x)
voc_emb = tf.get_variable(name=""voc_emb"", dtype=tf.float32, shape=[3, 12])
embedded_x = tf.nn.embedding_lookup(voc_emb, x)
sess = tf.Session()
sess.run(tf.global_variables_initializer())
print(sess.run(embedded_x))
```

the following above I run on cpu and gpu separately, 
while cpu reports:
InvalidArgumentError (see above for traceback): indices = 999 is not in [0, 3)
	 [[node embedding_lookup (defined at test.py:24) ]]

but gpu just give an output like this:
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
"
27031,Tensorflow-lite segmentation model does not work with NNAPI on Android 9.0,"

**System information**
- OS Platform and Distribution : Android, Tensorflow-Lite-Experimental
- Mobile device : Samsung SM-A730F, Android 9 , API 28
- Tensorflow: 1.13

**Describe the current behavior**

The official tensorflow lite segmentation model '[deeplabv3_257_mv_gpu.tflite](https://storage.googleapis.com/download.tensorflow.org/models/tflite/gpu/deeplabv3_257_mv_gpu.tflite)' cannot be run on  Andorid 9.0, using NNAPI. It seems some operators are not supported currently by NNAPI.

**Describe the expected behavior**

The model should run on phone using NNAPI, without any problems.

**Other info / logs**
The model gives error when we run it using tensorflow lite interpreter on android.Also it gives similar error on android benchmark tool, when we run it with 'use_nnapi=1' option.

TF-Lite Benchmark:-

adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/deeplabv3_257_mv_gpu.tflite --num_threads=1 --use_nnapi=1
adb: /opt/intel/intelpython27/lib/libcrypto.so.1.0.0: no version information available (required by adb)
STARTING!
Min num runs: [50]
Min runs duration (seconds): [1]
Inter-run delay (seconds): [-1]
Num threads: [1]
Benchmark name: []
Output prefix: []
Min warmup runs: [1]
Min warmup runs duration (seconds): [0.5]
Graph: [/data/local/tmp/deeplabv3_257_mv_gpu.tflite]
Input layers: []
Input shapes: []
Use nnapi : [1]
Allow fp16 : [0]
Loaded model /data/local/tmp/deeplabv3_257_mv_gpu.tflite
resolved reporter
Initialized session in 325.573ms
Running benchmark for at least 1 iterations and at least 0.5 seconds
Op code 23 is currently not delegated to NNAPI
Returning error since TFLite returned failure nnapi_delegate.cc:736.
Failed to build graph for NNAPI
Failed to invoke!
Aborted 

Android Log:

android.example.com.tflitecamerademo E/tflite: Op code 23 is currently not delegated to NNAPI
    Returning error since TFLite returned failure nnapi_delegate.cc:751.
    Failed to build graph for NNAPI
"
27030,'tensorflow/core/framework/numeric_types.h' file not found,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- OS Platform and Distribution : cenos7 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): source code 
- TensorFlow version: 1.93
- Python version: 3.6
- Installed using virtualenv? pip? conda?: un installed virtualenv
- Bazel version (if compiling from source): 0.22.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: no
- GPU model and memory: no


**Describe the documentation issue**
In centos, I downloaded the tensorflow project. I open '/root/tensorflow/tensorflow/core/framework/allocator.h' file,  Tips can not find thoese included files, such as 'tensorflow/core/framework/numeric_types.h' file not found.but those files are already exists. thanks 
**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
27029,Make documentation link to C++ code,"**System information**
- TensorFlow version: All
- Doc Link: All the Python API documentation, for example: https://www.tensorflow.org/api_docs/python/tf/nn/conv2d_transpose

**Describe the documentation issue**
The Python API documentation often points to the Python code (on github) where the operation is defined. For the example, for `tf.nn.conv2d_transpose()`, it links to [this code](https://www.tensorflow.org/code/stable/tensorflow/python/ops/nn_ops.py).

Unfortunately, most operations are fairly thin wrappers around C++ operations, and since the link from Python to C++ is automatically generated (in this example, it's `gen_nn_ops.conv2d_backprop_input()`), it is not trivial to find the corresponding C++ code (the mapping is in Bazel code, really hard to find). Many people have been bothered by this problem, as you can see by searching for gen_nn_ops on StackOverflow, for example this question: https://stackoverflow.com/questions/41147734/looking-for-source-code-of-from-gen-nn-ops-in-tensorflow

It would be great if the documentation could point to both the Python function and the C++ operation. In this case, it would be https://www.tensorflow.org/versions/r2.0/api_docs/cc/class/tensorflow/ops/conv2-d-backprop-input and the source code is in [tensorflow/core/kernels/conv_grad_input_ops.cc](https://github.com/tensorflow/tensorflow/blob/94be8f012aa59730570bf71e6ba7cd2aa432a589/tensorflow/core/kernels/conv_grad_input_ops.cc#L265).

To find it, I had to search locally on my computer to find the `gen_nn_ops.py` file, and I found that `gen_nn_ops.conv2d_backprop_input()` just called the `Conv2DBackpropInput` operation. But then I had to go back to github to search for its C++ source code (since the TensorFlow binary does not include it), and it was tricky to find, since the C++ operation is also dynamically registered, so the actual name of the function is `Conv2DCustomBackpropInputOp`. Searching for `""REGISTER_KERNEL_BUILDER  Conv2DBackpropInput""` helps.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
I'm not sure where I could contribute this fix."
27027,tf.dataset failed to correctly load image data,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Debian GNU/Linux 8 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
None
- TensorFlow installed from (source or binary):
conda install 
- TensorFlow version (use command below):
tensorflow-gpu1.12.0rc0
- Python version:
python 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
cuda 9.2 with cdunn7.3.1
- GPU model and memory:
nvidia1080ti, 11gb


**Describe the current behavior**

i tried to use tf.dataset api to build a custom dataloader, all the images are in .jpg format and rgb color space with exact 3 channels (i double checked this using opencv to load images and numpy to print image shape). then i got this error:

InvalidArgumentError (see above for traceback): Cannot batch tensors with different shapes in component 0. First element had shape [270,270,3] and element 92 had shape [270,270,1].


**Code to reproduce the issue**
```
def load_function(image_path, label_path, crop_size=256):
        image = tf.read_file(image_path)
        image = tf.image.decode_jpeg(image)
        image = tf.image.resize_images(image, [270, 270])
        image = tf.image.random_crop(image, [crop_size, crop_size, 3])
        label = tf.read_file(label_path)
        label = tf.image.decode_png(label)
        label = tf.image.resize_images(label, [270, 270])
        label = tf.image.random_crop(label, [crop_size, crop_size, 1])
        return image, label
```

"
27026,dilated convolution does not convergence with data format NHWC,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.4.1708
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): conda installation
- TensorFlow version (use command below): 1.9.0
- Python version: 3.6.5
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: CUDA Version 9.0.176， 
- GPU model and memory: V100, 16G


**Describe the current behavior**
I implemented WaveGlow model, in the model it contains dilated convolution, so in early time I used tf.nn.conv2d to implement the dilated convolution model- -- WaveNet, I used the default data format NHWC, but after many experiments, I found the model does not convergence. Even after 700K steps with a small learning rate, it did not convergence. I have tried many hyper-parameters settings.

So I changed the dilated convolution to a native implementation from [tensorflow-wavenet](https://github.com/ibab/tensorflow-wavenet). Then the WaveGlow model convergence quickly after 50k-80k steps.

Then, I want to understand what's wrong with my usage of tf.nn.conv2d, but no clue. Then I tried data format NCHW, the model convergences quickly after 70K steps. 

So I doubt there may be a bug in Tensorflow's implementation of dilated convolution with data format NHWC.

**Describe the expected behavior**
tf.nn.conv2d with data format NHWC should convergence as expected, it should behave the same as data format NCHW or naive dilated convolution implementation.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

my code: https://github.com/weixsong/WaveGlow
master branch is the naive implementation of dilated conv1d

model samples are also in the repo.

branch **tf_dilated_conv** is the implementation by tf.nn.conv2d with data format NHWC : https://github.com/weixsong/WaveGlow/tree/tf_dilated_conv

branch **tf_dilated_conv_channel_first** is the implementation by tf.nn.conv2d with data format NCHW  , https://github.com/weixsong/WaveGlow/tree/tf_dilated_conv_channel_first

**Other info / logs**
N/A
"
27023,"TF 1.x: remove the ""deprecated"" warning messages","I know the functional APIs, such as tf.layers.dense, will disappear in TF 2.0. However, their alternatives, tf.keras.layers, are not compatible with other components of TF 1.x, for example, they even do not support variable scope (#27016). So I will stick on the deprecated APIs in TF 1.x. 
Would you please remove the disgusting ""deprecated"" warning messages like this: `xxx (from tensorflow.python.layers.core) is deprecated and will be removed in a future version. Use keras.layers.xxx instead.`"
27022,tf.data.experimental.ignore_errors,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**May have been addressed by:** [https://github.com/tensorflow/tensorflow/issues/25700](url)

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes. Stock code with additional operations
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): tensorflow-gpu 1.12.0
- Python version: 3.6.5
- CUDA/cuDNN version: 10.0/7.4.1
- GPU model and memory: TitanXP 12196MiB


**Describe the current behavior**
tf.data.experimental.ignore_errors() causes tensorflow to hang when applied after a ""dataset.batch(N)"" operation.
**Describe the expected behavior**
Throw an error, suggest that the user ignore errors prior to batching to prevent uneven batches.
**Code to reproduce the issue**
`import tensorflow as tf

dataset = tf.data.Dataset.from_tensor_slices([1., 2., 0., 4.])

dataset = dataset.map(lambda x: tf.check_numerics(1. / x, ""error""))

dataset = dataset.repeat().batch(6)

dataset = dataset.apply(tf.data.experimental.ignore_errors()) 

it = dataset.make_one_shot_iterator()
nex = it.get_next()
sess = tf.Session()
print(sess.run(nex))`

Provide a reproducible test case that is the bare minimum necessary to generate the problem.
See above.
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27020,tensorflow-gpu 2.0.0-alpha0 on win10 ImportError: DLL load failed:,"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  win10 Pro x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): 
- TensorFlow version: 2.0.0
- Python version: 3.6.6
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA: v9.0, cuDnn: v7.5
- GPU model and memory: 1060 6GB



**Describe the problem**
errors occur when `import tensorflow as tf`
 
I have read some similar issues and [this](https://github.com/jvishnuvardhan/Installing-TensorFlow-GPU-on-Windows-10-TF1.12) but those do not work on my case.

And I reinstall tensorflow 1.13.1, it works well.

**Any other info / logs**

I'm sure you've seen many similar logs like below:

>Traceback (most recent call last):
  File ""C:\Users\JoTang507\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\JoTang507\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\JoTang507\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\JoTang507\Anaconda3\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\JoTang507\Anaconda3\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found

>During handling of the above exception, another exception occurred:

>Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\JoTang507\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 27, in <module>
    from tensorflow._api.v2 import audio
  File ""C:\Users\JoTang507\Anaconda3\lib\site-packages\tensorflow\_api\v2\audio\__init__.py"", line 8, in <module>
    from tensorflow.python.ops.gen_audio_ops import decode_wav
  File ""C:\Users\JoTang507\Anaconda3\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\JoTang507\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\JoTang507\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\JoTang507\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\JoTang507\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\JoTang507\Anaconda3\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\JoTang507\Anaconda3\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
>ImportError: DLL load failed: The specified module could not be found"
27016,tf.keras.layers (in TF 1.13.1): variable_scope does not work,"Run the following code:
```
import tensorflow as tf

for index in range(2):
    with tf.variable_scope(name_or_scope=tf.get_variable_scope(), reuse=True if index > 0 else None):
        outputs = tf.keras.layers.Dense(10).apply(tf.ones([10, 10]))

print(tf.trainable_variables())
```
There will be 4 variables:
```
[<tf.Variable 'dense/kernel:0' shape=(10, 10) dtype=float32>,
 <tf.Variable 'dense/bias:0' shape=(10,) dtype=float32>,
 <tf.Variable 'dense_1/kernel:0' shape=(10, 10) dtype=float32>,
 <tf.Variable 'dense_1/bias:0' shape=(10,) dtype=float32>]
```

But if you change tf.keras.layers to tf.layers:
```
import tensorflow as tf

for index in range(2):
    with tf.variable_scope(name_or_scope=tf.get_variable_scope(), reuse=True if index > 0 else None):
        outputs = tf.layers.Dense(10).apply(tf.ones([10, 10]))

print(tf.trainable_variables())
```
There will be only 2 variables (the correct result):
```
[<tf.Variable 'dense/kernel:0' shape=(10, 10) dtype=float32_ref>,
 <tf.Variable 'dense/bias:0' shape=(10,) dtype=float32_ref>]
```"
27012,tflite depthwise_conv2d produces different(wrong) results ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): `pip install tensorflow`
- TensorFlow version (use command below): 1.12.0
- Python version: 2.7.12
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): 
- CUDA/cuDNN version: CUDA 9 cuDNN 7.1
- GPU model and memory:GTX 1080

So I was trying to use tflite converter to convert a pretrained model. When I use the same input to do inference by both tflite interpreter and native tensorflow I observed vastly different outputs . The difference is caused by the tfLite depthwise2d operator converted from the original  `tf.nn.conv2d` operator.  So I constructed a mini example to explain the problem:

```python
import numpy as np
import tensorflow as tf

#construct a fake weight coeffecients
np.random.seed(0)
weight_array= np.random.uniform(-1,1, size=(1,3,1,9)).astype(""float32"")

#construct a simple graph involving conv2d
tf.reset_default_graph()
input_1 = tf.placeholder(dtype=tf.float32, shape=[1,1,600,1])
weights = tf.constant(weight_array)
weights = tf.quantization.fake_quant_with_min_max_args(weights, np.min(weight_array), np.max(weight_array))
out = tf.nn.conv2d(input_1, weights, [1,2,2,1], ""SAME"")
out_1 = tf.nn.relu6(out)
q_out = tf.quantization.fake_quant_with_min_max_args(out_1, 0, 6)

#convert the graph to tflite in unit8 quantized inference mode
with tf.Session() as sess:
    converter = tf.contrib.lite.TFLiteConverter.from_session(sess, [input_1], [q_out])
    converter.inference_type = tf.contrib.lite.constants.QUANTIZED_UINT8
    converter.inference_input_type = tf.contrib.lite.constants.QUANTIZED_UINT8
    input_arrays = converter.get_input_arrays()
    converter.quantized_input_stats = {input_arrays[0] : (128, 1)} 
    tflite_model = converter.convert()

#define a dummy input
input_sig = np.ones([1,1,600,1])
input_data = input_sig.astype(""uint8"")

#inference using tflite interpreter
interpreter = tf.contrib.lite.Interpreter(model_content=tflite_model)
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

input_shape = input_details[0]['shape']
input_index = input_details[0]['index']
output_index = output_details[0]['index']

interpreter.allocate_tensors()
interpreter.set_tensor(input_index, input_data)
interpreter.invoke()
#get tflite int8 output
tf_lite_output = interpreter.get_tensor(output_index)

scale = output_details[0]['quantization'][0] #scale factor for quantized output
weight_scale =  interpreter._get_tensor_details(0)['quantization'][0] #scale factor for quantized weights
zero_point = output_details[0]['quantization'][1] #the point for quantized output maps to float 0 

#get tensorflow float32 output from fake quantized node
with tf.Session() as sess:
    tf_output = sess.run(q_out,  {input_1: input_sig})

#compare, they should be at least close, but nope
print tf_lite_output[0][0][0] 
print tf_output[0][0][0]/ scale + zero_point #converted to uint8 output using the quantization information in the output_details
```
**Describe the current behavior**
tflite output gives:
`[  0,   0,   0,   0,   0,   0, 255,   0,   0]`
after converting to uint8 by sc, tensorflow model gives:
`[18. 74. 52. 35. 27.  0.  0.  4. 37.]`
**Describe the expected behavior**
I believe those two results should give a very similar result when fed with identical inputs. I also checked that the quantized weight matches with the output from fake quantization node after scaling using the `weight_scale`. I also manually calculated the operation defined in the graph and produced a result close to what tensorflow model gives. Is there a reason that tflite computation behaves completely different than the native tensorflow ?
"
27010,tf.keras.layers.Softmax does not support masking?,"```
import tensorflow as tf
outputs = tf.keras.layers.Softmax().apply(
  tf.keras.layers.Masking().apply(
    tf.zeros([3,5,7])
  )
)
```
Since the default mask value of Masking is zero, Softmax should skip all values in the above case, and its behavior should be like sparse softmax. Therefore, I suppose the output should be all zeros, but that is not the case."
27009,How horovod integrate itself into tf distribute strategy?,"Hi,
I read from tensorflow TFC that horovod team is collobrating with tf team upon the HorovodDistributionStrategy.
RFC: https://github.com/tensorflow/community/blob/master/rfcs/20181016-replicator.md#use-case-4-users-who-will-create-new-distributionstrategy-implementations

Can anyone share with us more details about how to integrate horovod into d.s.? Because there's a lot of difference between them: e.g. launcher mode

Thanks!
"
27008,Cannot use load_model for a model with a DenseFeatures layer,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no (https://www.tensorflow.org/alpha/tutorials/keras/feature_columns + model.save + load_model)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-alpha0
- Python version: 3.6.8
- CUDA/cuDNN version: 10.0.130/7.3.1
- GPU model and memory: GTX 1060 6GB

**Describe the current behavior**
loading a saved .h5 model which includes a DenseFeatures Layer fails:
`ValueError: Unknown layer: DenseFeatures`

**Describe the expected behavior**
model is loading

**Code to reproduce the issue**
```
import pandas as pd

# pip install -q tensorflow==2.0.0-alpha0
import tensorflow as tf
from tensorflow import feature_column
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split

URL = 'https://storage.googleapis.com/applied-dl/heart.csv'
dataframe = pd.read_csv(URL)
train, test = train_test_split(dataframe, test_size=0.2)
train, val = train_test_split(train, test_size=0.2)

# A utility method to create a tf.data dataset from a Pandas Dataframe
def df_to_dataset(dataframe, shuffle=True, batch_size=32):
  dataframe = dataframe.copy()
  labels = dataframe.pop('target')
  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))
  if shuffle:
    ds = ds.shuffle(buffer_size=len(dataframe))
  ds = ds.batch(batch_size)
  return ds

feature_columns = []

# numeric cols
for header in ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope', 'ca']:
  feature_columns.append(feature_column.numeric_column(header))

feature_layer = tf.keras.layers.DenseFeatures(feature_columns)

batch_size = 32
train_ds = df_to_dataset(train, batch_size=batch_size)
val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)
test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)

model = tf.keras.Sequential([
  feature_layer,
  layers.Dense(128, activation='relu'),
  layers.Dense(128, activation='relu'),
  layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam',
              loss='binary_crossentropy')

model.fit(train_ds, 
          validation_data=val_ds, 
          epochs=5)


model.save('my_model.h5')
from tensorflow import keras
new_model = keras.models.load_model('my_model.h5')
```

**Other info / logs**
```
Traceback (most recent call last):
  File ""error.py"", line 67, in <module>
    new_model = keras.models.load_model('my_model.h5')
  File ""C:\Anaconda3\envs\tf_gpu\lib\site-packages\tensorflow\python\keras\saving\hdf5_format.py"", line 215, in load_model
    custom_objects=custom_objects)
  File ""C:\Anaconda3\envs\tf_gpu\lib\site-packages\tensorflow\python\keras\saving\model_config.py"", line 55, in model_from_config
    return deserialize(config, custom_objects=custom_objects)
  File ""C:\Anaconda3\envs\tf_gpu\lib\site-packages\tensorflow\python\keras\layers\serialization.py"", line 95, in deserialize
    printable_module_name='layer')
  File ""C:\Anaconda3\envs\tf_gpu\lib\site-packages\tensorflow\python\keras\utils\generic_utils.py"", line 192, in deserialize_keras_object
    list(custom_objects.items())))
  File ""C:\Anaconda3\envs\tf_gpu\lib\site-packages\tensorflow\python\keras\engine\sequential.py"", line 351, in from_config
    custom_objects=custom_objects)
  File ""C:\Anaconda3\envs\tf_gpu\lib\site-packages\tensorflow\python\keras\layers\serialization.py"", line 95, in deserialize
    printable_module_name='layer')
  File ""C:\Anaconda3\envs\tf_gpu\lib\site-packages\tensorflow\python\keras\utils\generic_utils.py"", line 181, in deserialize_keras_object
    config, module_objects, custom_objects, printable_module_name)
  File ""C:\Anaconda3\envs\tf_gpu\lib\site-packages\tensorflow\python\keras\utils\generic_utils.py"", line 166, in class_and_config_for_serialized_keras_object
    raise ValueError('Unknown ' + printable_module_name + ': ' + class_name)
ValueError: Unknown layer: DenseFeatures
```
"
27005,[TF 2.0] Documentation still mentions _ref types,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version:2.0.0-alpha
- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/dtypes/DType


**Describe the documentation issue**
The documentation of TF 2.0 still mentions ```_ref``` types even though it is no longer a thing.
Related issue #26941.


 
**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
Yes"
27004,WARNING: Entity <method-wrapper '__call__' of weakref object...,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): one line modification to official [colab checkpoints.ipynb](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/guide/checkpoints.ipynb)
- Tensorflow version: 2.0.0-alpha0

**Describe the current behavior**
After the model has been loaded from a checkpoint, the following messages appear:
```
W0321 20:29:37.423420 139748586436480 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x7f197ec58278> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.
W0321 20:29:37.434415 139748586436480 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x7f197ebfad68> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.
WARNING: Entity <method-wrapper '__call__' of weakref object at 0x7f197ec58278> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.
WARNING: Entity <method-wrapper '__call__' of weakref object at 0x7f197ebfad68> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.
```

**Describe the expected behavior**
No warning messages.

**Code to reproduce the issue**
Decorate `def call` with `@tf.function` in the second cell of the notebook:
```
class Net(tf.keras.Model):
  """"""A simple linear model.""""""

  def __init__(self):
    super(Net, self).__init__()
    self.l1 = tf.keras.layers.Dense(5)

  @tf.function
  def call(self, x):
    return self.l1(x)
```
"
27003,TF2 has tf.floor but not tf.ceil,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):n/a
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):archlinux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:n/a
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):tf-nightly-gpu-2.0-preview-2.0.0.dev20190319
- Python version:3.7
- Bazel version (if compiling from source):n/a
- GCC/Compiler version (if compiling from source):n/a
- CUDA/cuDNN version:n/a
- GPU model and memory:n/a

```python
import tensorflow as tf
print(tf.floor)
print(tf.ceil)
```
prints:
```
<function floor at 0x7fad8ad08950>
Traceback (most recent call last):
  File ""a.py"", line 6, in <module>
    print(tf.ceil)
AttributeError: module 'tensorflow' has no attribute 'ceil'
```
`tf.ceil` is only accessible through `tf.math.ceil` now. But `tf.floor` still exists.

I expect the two functions to either both exist or both not exist."
27002,sigdev error when training model with image data generator,"Hi 

see also [SO question](https://stackoverflow.com/questions/55283194/tensorflow-gpu-sigdev-error-when-training-on-image-data-generator) regarding this issue 

I am using ubuntu with conda, python 3.6 and tf gpu 1.12, and using image data generator to load images, and then fit generator. 

I am using this code 

    i = gen.flow_from_dataframe(dataframe=df, x_col=""new_filename"",
                                y_col=""label"", class_mode=""sparse"",
                                directory=links_dir, target_size=(224, 224), batch_size=32)
    
    model.fit_generator(i, epochs=1, workers=1,
                        use_multiprocessing=False, max_queue_size=1,
                        verbose=1)

and getting this error, without any batches even running 

> Process finished with exit code 139 (interrupted by signal 11:
> SIGSEGV)

I validated that the generator is loading the images using 

I used nvidia smi and made sure the model is created on GPU and the gpu memory is used.

The model is a simple, small conv net, nothing this GPU can't handle. 

print(gen[0]) and got data and everything went fine.
So I assume that the image data is fine.

What can be the issue here? could this be happening because I have two GPU's in the machine and I am only using one?



"
27000,estimator built with keras.estimator.model_to_estimator fails with CollectiveAllReduceStrategy,"

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
CentOS 7 on a HPC system

- TensorFlow installed from (source or binary):
binary (from anaconda)
- TensorFlow version (use command below):
1.12.0
- Python version:
3.6.8
- CUDA/cuDNN version: None
- GPU model and memory: None



**Describe the current behavior**
When converting a keras model to an estimator, it can be trained in a non-distributed environment,
but it fails with CollectiveAllReduceStrategy on multiple machines. However, when building an estimator from scratch (without keras), CollectiveAllReduceStrategy works.
**Describe the expected behavior**
estimator created from keras models should be trainable with CollectiveAllReduceStrategy.

**Code to reproduce the issue**

the following script fails when run in a distributed way in a slurm-cluster:
```

import numpy as np
import tensorflow as tf

from tensorflow.keras.datasets import mnist
import os
import sys
import json



train_number = ""test""
worker_ips = sys.argv[1]
port = sys.argv[2]
worker_index = int(sys.argv[3])
modeldir = sys.argv[4]
workers = [ip+':'+port for ip in worker_ips.split(',')]

os.environ[""TF_CONFIG""] = json.dumps({
    ""cluster"": {
        ""worker"": workers
    },
   ""task"": {""type"": ""worker"", ""index"": worker_index}
})

distribution = tf.contrib.distribute.CollectiveAllReduceStrategy(
    num_gpus_per_worker=0)
config = tf.estimator.RunConfig(train_distribute=distribution, model_dir=modeldir)


batch_size = 128
num_classes = 10
epochs = 12

# input image dimensions
img_rows, img_cols = 28, 28

# the data, split between train and test sets
(x_train, y_train), (x_test, y_test) = mnist.load_data()



x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)
input_shape = (img_rows, img_cols, 1)

x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255
print('x_train shape:', x_train.shape)
print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')

# convert class vectors to binary class matrices
y_train = tf.keras.utils.to_categorical(y_train, num_classes)
y_test = tf.keras.utils.to_categorical(y_test, num_classes)

model = tf.keras.Sequential()
model.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3),
                 activation='relu',
                 input_shape=input_shape))
model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))
model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))
model.add(tf.keras.layers.Dropout(0.25))
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(128, activation='relu'))
model.add(tf.keras.layers.Dropout(0.5))
model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))

model.compile(loss=tf.keras.losses.categorical_crossentropy,
              optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001),
metrics=['accuracy'])

estimator = tf.keras.estimator.model_to_estimator(keras_model=model, config=config)

input_name = model.input_names[0]


def gen():
    for x,y in zip(x_train,y_train):
        yield x, y


def train_input_fn():
    dataset = tf.data.Dataset.from_generator(
        gen, (tf.float32, tf.float32), output_shapes=(x_train.shape[1:],y_train.shape[1:]))

    return dataset.shuffle(1).repeat(epochs).batch(batch_size)

train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn)
eval_spec = tf.estimator.EvalSpec(input_fn=train_input_fn)
tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
```

the multi-machine job is started with the following script
```
#! /bin/bash

# N must be >=2

#SBATCH -N2
#SBATCH -A snic2018-1-13
#SBATCH --time=00:30:00

nodelists=$(scontrol show hostname $SLURM_JOB_NODELIST | paste -d, -s)
IFS=',' read -r -a nodearray <<< ""$nodelists""


source activate tf-dist-env

port=2222

modeldir=$(mktemp -d --tmpdir='.')
rm -r $modeldir
mkdir $modeldir

for i in $(seq 1 $SLURM_JOB_NUM_NODES); do
# convert to 0-based index
    i=$((i-1))
    echo starting job $i
    srun --nodelist=""${nodearray[i]}""  -N1 -n1 python test-tf-estimators-from-keras_distributed.py ${nodelists} ${port} ${i} ${modeldir} &
done
wait
```


which produces one of the following error
```
Traceback (most recent call last):
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1334, in _do_call
    return fn(*args)
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Upper bound check fail for input 1 from node training/TFOptimizer/gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter to node scoped_allocator_concat_1 input bounds = [0x2b70cc944a40, 0x2b70cc944ec0] backing_tensor bounds = [0x2b70cc01e000, 0x2b70cc4b1c28]
	 [[{{node scoped_allocator_concat_1}} = _ScopedAllocatorConcat[N=8, T=DT_FLOAT, id=1, reshape=false, sa_name=""scoped_allocator_1"", shape=[1199882], _device=""/job:worker/replica:0/task:1/device:CPU:0""](scoped_allocator_1, training/TFOptimizer/gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter, training/TFOptimizer/gradients/conv2d/BiasAdd_grad/BiasAddGrad, training/TFOptimizer/gradients/conv2d_1/Conv2D_grad/Conv2DBackpropFilter, training/TFOptimizer/gradients/conv2d_1/BiasAdd_grad/BiasAddGrad, training/TFOptimizer/gradients/dense/MatMul_grad/MatMul_1, training/TFOptimizer/gradients/dense/BiasAdd_grad/BiasAddGrad, training/TFOptimizer/gradients/dense_1/MatMul_grad/MatMul_1, training/TFOptimizer/gradients/dense_1/BiasAdd_grad/BiasAddGrad, ^training/TFOptimizer/gradients/conv2d_1/Conv2D_grad/Conv2DBackpropInput, ^training/TFOptimizer/gradients/dense/MatMul_grad/MatMul, ^training/TFOptimizer/gradients/dense_1/MatMul_grad/MatMul)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""test-tf-estimators-from-keras_distributed.py"", line 100, in <module>
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 462, in train_and_evaluate
    estimator, train_spec, eval_spec, _TrainingExecutor)
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/distribute/estimator_training.py"", line 279, in train_and_evaluate
    session_config=run_config.session_config)
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_coordinator.py"", line 792, in run_distribute_coordinator
    task_id, session_config, rpc_layer)
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_coordinator.py"", line 344, in _run_single_worker
    worker_fn(strategy)
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/distribute/estimator_training.py"", line 246, in _worker_fn
    hooks=hooks)
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 354, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1205, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1352, in _train_model_distributed
    saving_listeners)
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1471, in _train_with_estimator_spec
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 671, in run
    run_metadata=run_metadata)
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1156, in run
    run_metadata=run_metadata)
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1255, in run
    raise six.reraise(*original_exc_info)
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/six.py"", line 693, in reraise
    raise value
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1240, in run
    return self._sess.run(*args, **kwargs)
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1312, in run
    run_metadata=run_metadata)
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1076, in run
    return self._sess.run(*args, **kwargs)
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 929, in run
    run_metadata_ptr)
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1328, in _do_run
    run_metadata)
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Upper bound check fail for input 1 from node training/TFOptimizer/gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter to node scoped_allocator_concat_1 input bounds = [0x2b70cc944a40, 0x2b70cc944ec0] backing_tensor bounds = [0x2b70cc01e000, 0x2b70cc4b1c28]
	 [[{{node scoped_allocator_concat_1}} = _ScopedAllocatorConcat[N=8, T=DT_FLOAT, id=1, reshape=false, sa_name=""scoped_allocator_1"", shape=[1199882], _device=""/job:worker/replica:0/task:1/device:CPU:0""](scoped_allocator_1, training/TFOptimizer/gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter, training/TFOptimizer/gradients/conv2d/BiasAdd_grad/BiasAddGrad, training/TFOptimizer/gradients/conv2d_1/Conv2D_grad/Conv2DBackpropFilter, training/TFOptimizer/gradients/conv2d_1/BiasAdd_grad/BiasAddGrad, training/TFOptimizer/gradients/dense/MatMul_grad/MatMul_1, training/TFOptimizer/gradients/dense/BiasAdd_grad/BiasAddGrad, training/TFOptimizer/gradients/dense_1/MatMul_grad/MatMul_1, training/TFOptimizer/gradients/dense_1/BiasAdd_grad/BiasAddGrad, ^training/TFOptimizer/gradients/conv2d_1/Conv2D_grad/Conv2DBackpropInput, ^training/TFOptimizer/gradients/dense/MatMul_grad/MatMul, ^training/TFOptimizer/gradients/dense_1/MatMul_grad/MatMul)]]
2019-03-21 17:18:47.019340: W tensorflow/core/common_runtime/base_collective_executor.cc:203] BaseCollectiveExecutor::StartAbort Invalid argument: Upper bound check fail for input 1 from node training/TFOptimizer/gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter to node scoped_allocator_concat_1 input bounds = [0x2b7b2409eb00, 0x2b7b2409ef80] backing_tensor bounds = [0x2b7a76ae2040, 0x2b7a76f75c68]
	 [[{{node scoped_allocator_concat_1}} = _ScopedAllocatorConcat[N=8, T=DT_FLOAT, id=1, reshape=false, sa_name=""scoped_allocator_1"", shape=[1199882], _device=""/job:worker/replica:0/task:0/device:CPU:0""](scoped_allocator_1, training/TFOptimizer/gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter, training/TFOptimizer/gradients/conv2d/BiasAdd_grad/BiasAddGrad, training/TFOptimizer/gradients/conv2d_1/Conv2D_grad/Conv2DBackpropFilter, training/TFOptimizer/gradients/conv2d_1/BiasAdd_grad/BiasAddGrad, training/TFOptimizer/gradients/dense/MatMul_grad/MatMul_1, training/TFOptimizer/gradients/dense/BiasAdd_grad/BiasAddGrad, training/TFOptimizer/gradients/dense_1/MatMul_grad/MatMul_1, training/TFOptimizer/gradients/dense_1/BiasAdd_grad/BiasAddGrad, ^training/TFOptimizer/gradients/conv2d_1/Conv2D_grad/Conv2DBackpropInput, ^training/TFOptimizer/gradients/dense/MatMul_grad/MatMul, ^training/TFOptimizer/gradients/dense_1/MatMul_grad/MatMul)]]
x_train shape: (60000, 28, 28, 1)
60000 train samples
10000 test samples
Traceback (most recent call last):
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1334, in _do_call
    return fn(*args)
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Upper bound check fail for input 1 from node training/TFOptimizer/gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter to node scoped_allocator_concat_1 input bounds = [0x2b7b2409eb00, 0x2b7b2409ef80] backing_tensor bounds = [0x2b7a76ae2040, 0x2b7a76f75c68]
	 [[{{node scoped_allocator_concat_1}} = _ScopedAllocatorConcat[N=8, T=DT_FLOAT, id=1, reshape=false, sa_name=""scoped_allocator_1"", shape=[1199882], _device=""/job:worker/replica:0/task:0/device:CPU:0""](scoped_allocator_1, training/TFOptimizer/gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter, training/TFOptimizer/gradients/conv2d/BiasAdd_grad/BiasAddGrad, training/TFOptimizer/gradients/conv2d_1/Conv2D_grad/Conv2DBackpropFilter, training/TFOptimizer/gradients/conv2d_1/BiasAdd_grad/BiasAddGrad, training/TFOptimizer/gradients/dense/MatMul_grad/MatMul_1, training/TFOptimizer/gradients/dense/BiasAdd_grad/BiasAddGrad, training/TFOptimizer/gradients/dense_1/MatMul_grad/MatMul_1, training/TFOptimizer/gradients/dense_1/BiasAdd_grad/BiasAddGrad, ^training/TFOptimizer/gradients/conv2d_1/Conv2D_grad/Conv2DBackpropInput, ^training/TFOptimizer/gradients/dense/MatMul_grad/MatMul, ^training/TFOptimizer/gradients/dense_1/MatMul_grad/MatMul)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""test-tf-estimators-from-keras_distributed.py"", line 100, in <module>
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 462, in train_and_evaluate
    estimator, train_spec, eval_spec, _TrainingExecutor)
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/distribute/estimator_training.py"", line 279, in train_and_evaluate
    session_config=run_config.session_config)
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_coordinator.py"", line 792, in run_distribute_coordinator
    task_id, session_config, rpc_layer)
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_coordinator.py"", line 344, in _run_single_worker
    worker_fn(strategy)
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/distribute/estimator_training.py"", line 246, in _worker_fn
    hooks=hooks)
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 354, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1205, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1352, in _train_model_distributed
    saving_listeners)
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1471, in _train_with_estimator_spec
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 671, in run
    run_metadata=run_metadata)
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1156, in run
    run_metadata=run_metadata)
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1255, in run
    raise six.reraise(*original_exc_info)
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/six.py"", line 693, in reraise
    raise value
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1240, in run
    return self._sess.run(*args, **kwargs)
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1312, in run
    run_metadata=run_metadata)
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1076, in run
    return self._sess.run(*args, **kwargs)
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 929, in run
    run_metadata_ptr)
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1328, in _do_run
    run_metadata)
  File ""/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Upper bound check fail for input 1 from node training/TFOptimizer/gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter to node scoped_allocator_concat_1 input bounds = [0x2b7b2409eb00, 0x2b7b2409ef80] backing_tensor bounds = [0x2b7a76ae2040, 0x2b7a76f75c68]
	 [[{{node scoped_allocator_concat_1}} = _ScopedAllocatorConcat[N=8, T=DT_FLOAT, id=1, reshape=false, sa_name=""scoped_allocator_1"", shape=[1199882], _device=""/job:worker/replica:0/task:0/device:CPU:0""](scoped_allocator_1, training/TFOptimizer/gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter, training/TFOptimizer/gradients/conv2d/BiasAdd_grad/BiasAddGrad, training/TFOptimizer/gradients/conv2d_1/Conv2D_grad/Conv2DBackpropFilter, training/TFOptimizer/gradients/conv2d_1/BiasAdd_grad/BiasAddGrad, training/TFOptimizer/gradients/dense/MatMul_grad/MatMul_1, training/TFOptimizer/gradients/dense/BiasAdd_grad/BiasAddGrad, training/TFOptimizer/gradients/dense_1/MatMul_grad/MatMul_1, training/TFOptimizer/gradients/dense_1/BiasAdd_grad/BiasAddGrad, ^training/TFOptimizer/gradients/conv2d_1/Conv2D_grad/Conv2DBackpropInput, ^training/TFOptimizer/gradients/dense/MatMul_grad/MatMul, ^training/TFOptimizer/gradients/dense_1/MatMul_grad/MatMul)]]
```



When instead running the following script on a single machine, it works (same as the python script for the distributed job, but without the configuration for distribution):

```

import numpy as np
import tensorflow as tf

from tensorflow.keras.datasets import mnist
import os
import sys
import json





batch_size = 128
num_classes = 10
epochs = 12

# input image dimensions
img_rows, img_cols = 28, 28

# the data, split between train and test sets
(x_train, y_train), (x_test, y_test) = mnist.load_data()



x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)
input_shape = (img_rows, img_cols, 1)

x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255
print('x_train shape:', x_train.shape)
print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')

# convert class vectors to binary class matrices
y_train = tf.keras.utils.to_categorical(y_train, num_classes)
y_test = tf.keras.utils.to_categorical(y_test, num_classes)

model = tf.keras.Sequential()
model.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3),
                 activation='relu',
                 input_shape=input_shape))
model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))
model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))
model.add(tf.keras.layers.Dropout(0.25))
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(128, activation='relu'))
model.add(tf.keras.layers.Dropout(0.5))
model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))

model.compile(loss=tf.keras.losses.categorical_crossentropy,
              optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001),
metrics=['accuracy'])

estimator = tf.keras.estimator.model_to_estimator(keras_model=model)

input_name = model.input_names[0]


def gen():
    for x,y in zip(x_train,y_train):
        yield x, y


def train_input_fn():
    dataset = tf.data.Dataset.from_generator(
        gen, (tf.float32, tf.float32), output_shapes=(x_train.shape[1:],y_train.shape[1:]))

    return dataset.shuffle(1).repeat(epochs).batch(batch_size)

train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn)
eval_spec = tf.estimator.EvalSpec(input_fn=train_input_fn)
tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
```

The following minimum example using an estimator not created with keras works with CollectiveAllReduceStrategy



```

import tensorflow as tf
import os
import json

import sys
worker_ips = sys.argv[1]
port = sys.argv[2]
worker_index = int(sys.argv[3])
modeldir = sys.argv[4]
workers = [ip+':'+port for ip in worker_ips.split(',')]

os.environ[""TF_CONFIG""] = json.dumps({
    ""cluster"": {
        ""worker"": workers
    },
   ""task"": {""type"": ""worker"", ""index"": worker_index}
})

distribution = tf.contrib.distribute.CollectiveAllReduceStrategy(
    num_gpus_per_worker=0)
config = tf.estimator.RunConfig(train_distribute=distribution, model_dir=modeldir)


def model_fn(features, labels, mode):
    layer = tf.layers.Dense(1)
    logits = layer(features)

    if mode == tf.estimator.ModeKeys.PREDICT:
        predictions = {""logits"": logits}
        return tf.estimator.EstimatorSpec(mode, predictions=predictions)

    loss = tf.losses.mean_squared_error(
        labels=labels, predictions=tf.reshape(logits, []))

    if mode == tf.estimator.ModeKeys.EVAL:
        return tf.estimator.EstimatorSpec(mode, loss=loss)

    if mode == tf.estimator.ModeKeys.TRAIN:
        train_op = tf.train.GradientDescentOptimizer(0.2).minimize(loss)
        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)


def input_fn():
    print('debug: calling input_fn')
    # check whether input_fn can see info about the worker is bein
    # executed it
    # print(os.environ[""TF_CONFIG""]) # this does not work...
    print(os.environ)
    print('dir():', dir())
    print('globals():', globals())
    print('locals():', locals())
    features = tf.data.Dataset.from_tensors([[1.]]).repeat(100000)
    labels = tf.data.Dataset.from_tensors(1.).repeat(100000)
    return tf.data.Dataset.zip((features, labels))


estimator = tf.estimator.Estimator(model_fn=model_fn, config=config)
train_spec = tf.estimator.TrainSpec(input_fn=input_fn, max_steps=10000)
eval_spec = tf.estimator.EvalSpec(input_fn=input_fn)
tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)

```"
26998,"2.0 Reference Models: MobileNetV2 (1 GPU, 8 GPU with dist strat and Keras)","**MobileNetV2** is a significant improvement over [MobileNetV1](https://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html) and pushes the state of the art for mobile visual recognition including classification, object detection and semantic segmentation. It builds upon the ideas from MobileNetV1, using depthwise separable convolution as efficient building blocks. However, V2 introduces two new features to the architecture: 

1. Linear bottlenecks between the layers, and 
2. Shortcut connections between the bottlenecks1.

The research team's academic paper describes MobileNetV2 in detail: https://arxiv.org/abs/1801.04381.

An example of MobileNetV2 implemented with Slim can be found [here](https://colab.research.google.com/github/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet_example.ipynb).

The purpose of this issue is to migrate models into the TF 2.0 Keras API. Each migrated model must be eager and distribution compatible, with tests, and all associated engineering artifacts."
26997,2.0 Reference Models: MobileNetv2 (TPU with dist strat and Keras),"**MobileNetV2** is a significant improvement over [MobileNetV1](https://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html) and pushes the state of the art for mobile visual recognition including classification, object detection and semantic segmentation. It builds upon the ideas from MobileNetV1, using depthwise separable convolution as efficient building blocks. However, V2 introduces two new features to the architecture: 

1. Linear bottlenecks between the layers, and 
2. Shortcut connections between the bottlenecks1.

The research team's academic paper describes MobileNetV2 in detail: https://arxiv.org/abs/1801.04381.

An example of MobileNetV2 implemented with Slim can be found [here](https://colab.research.google.com/github/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet_example.ipynb).

The purpose of this issue is to migrate models into the TF 2.0 Keras API. Each migrated model must be eager and distribution compatible, with tests, and all associated engineering artifacts."
26996,Keras: Bug when creating a custom layer without inputs,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: 
- TensorFlow installed from (source or binary): Colab
- TensorFlow version (use command below): 1.13
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the expected behavior**
I want to implement a Keras custom layer without any input, just trainable weights. This kind of layers is very important for visualization (e.g., [Understanding Deep Image Representations by Inverting Them](https://arxiv.org/abs/1412.0035))

**Code to reproduce the issue**
 
    class Simple(Layer):

        def __init__(self, output_dim, **kwargs):
           self.output_dim = output_dim
           super(Simple, self).__init__(**kwargs)

        def build(self):
           self.kernel = self.add_weight(name='kernel', shape=self.output_dim, initializer='uniform', trainable=True)
           super(Simple, self).build()  

        def call(self):
           return self.kernel

        def compute_output_shape(self):
           return self.output_dim

    X = Simple((1, 784))()

**Other info / logs**

I am getting an error message:

`__call__() missing 1 required positional argument: 'inputs'`"
26995,"2.0 Reference Models: BERT (1 GPU, 8 GPU with dist strat and Keras)","[**BERT**, or Bidirectional Encoder Representations from Transformers](https://github.com/google-research/bert), is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.

The research team's academic paper describes BERT in detail and provides full results on a number of tasks: https://arxiv.org/abs/1810.04805.

An example of using BERT can be found [here](https://colab.sandbox.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb).

The purpose of this issue is to migrate models into the TF 2.0 Keras API. Each migrated model must be eager and distribution compatible, with tests, and all associated engineering artifacts."
26994,2.0 Reference Models: BERT (TPU with dist strat and Keras),"[**BERT**, or Bidirectional Encoder Representations from Transformers](https://github.com/google-research/bert), is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.

The research team's academic paper describes BERT in detail and provides full results on a number of tasks: https://arxiv.org/abs/1810.04805.

An example of using BERT can be found [here](https://colab.sandbox.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb).

The purpose of this issue is to migrate models into the TF 2.0 Keras API. Each migrated model must be eager and distribution compatible, with tests, and all associated engineering artifacts."
26991,[TF2.0][FR] Skip TFRecord files with 'DataLossError: corrupted record at',"
**System information**
- TensorFlow version (you are using): 2.0.0-dev20190319

**Describe the feature and the current behavior/state.**

In this version of TF and prior versions, TFRecord files get corrupted or are detected as corrupt spontaneously during training. This is a problem already heavily discussed in e.g https://github.com/tensorflow/tensorflow/issues/13463 . Unfortunately, no one came up with an solution or a script triggering the problem, yet. This makes debugging what causes the files to change(?) during training quite hard.

Thus, it would be nice to be able to indicate to Tensorflow that such files should be ignored and only a warning is printed instead of canceling the whole training process. 

An exemplary error message:
```cmd
Traceback (most recent call last):
[...]
    obj = distributed_train(summary_writer)
  File ""/home/meyerjo/code/kaggle/nuclei-detection-tf2/nucleidetection/train.py"", line 297, in distributed_train
    lambda x: train_step(x, summary_writer), train_iterator)
  File ""/home/meyerjo/code/testfield-tf-nightly/tf-nightly/lib/python3.5/site-packages/tensorflow/python/distribute/distribute_lib.py"", line 470, in experimental_run
    args = (input_iterator.get_next(),) if input_iterator is not None else ()
  File ""/home/meyerjo/code/testfield-tf-nightly/tf-nightly/lib/python3.5/site-packages/tensorflow/python/distribute/input_lib.py"", line 145, in get_next
    self._iterators[i].get_next_as_list(new_name))
  File ""/home/meyerjo/code/testfield-tf-nightly/tf-nightly/lib/python3.5/site-packages/tensorflow/python/distribute/input_lib.py"", line 415, in get_next_as_list
    data_list = self._iterator.get_next_as_optional()
  File ""/home/meyerjo/code/testfield-tf-nightly/tf-nightly/lib/python3.5/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py"", line 311, in get_next_as_optional
    self._device_iterators[i]))
  File ""/home/meyerjo/code/testfield-tf-nightly/tf-nightly/lib/python3.5/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 689, in get_next_as_optional
    output_shapes=iterator._element_structure._flat_shapes),
  File ""/home/meyerjo/code/testfield-tf-nightly/tf-nightly/lib/python3.5/site-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 1858, in iterator_get_next_as_optional
    _six.raise_from(_core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 852968336
	 [[{{node MultiDeviceIteratorGetNextFromShard}}]]
	 [[RemoteCall]] [Op:IteratorGetNextAsOptional]

```
"
26990,"in nmt_with_attention,the gru layer in decoder confuse me","[nmt_with_attention file link](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb)
In the Decoder class, the gru layer is defined, and it is used in `call()` function, the code is:
```python
output, state = self.gru(x)
```

My question is, the gru cell state may not be updated? In the same batch, every timestamp use the same cell state? Is my understand correct?
```python
output, state = self.gru(x,initial_state=context_vector)
```
I then use this code and retrained the model, there is no obvious difference. Maybe someone can help me...

Forgive my lame English."
26989,"Add fftshift, ifftshift, etc","**System information**
- TensorFlow version (you are using): 1.13
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
Tensorflow is currently lacking a [fftshift](https://docs.scipy.org/doc/numpy/reference/generated/numpy.fft.fftshift.html) operation. This has been repeatedly requested, see e.g. #23587 and [fftshift in tensorflow???????????!!!!!!!!!!!](https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/tfCCukPtwI4).

**Will this change the current api? How?**
No changes to current API, this is a pure addition.

**Who will benefit with this feature?**
* The FFT in tensorflow is currently different from e.g. numpy, see #10749, anyone in need of the numpy convention will benefit.
* Anyone who wants to visualize the result of a fourier transform inside tensorflow

**Any Other info.**
No"
26987,Installing tensorflow-gpu,"Folks,
In many of the forums you see countless people strugging to install the tensorflow for GPU.

If NVIDIA is really a partner you need to create files that say libcublas.latest with a sym link to the version.

It's pretty annoying that i have to deinstall cuda 10.1, because tensorflow-gpu only works with cuda 10.0, and that others struggle because the file name is 10 vs 10.0, etc..

this part makes installing a few hour/day process. 

It won't kill NVIDIA to have a few prior versions installed with the latest package.

Start typing: libcublas.so.9.0... all you see is hits on file not found for libcublas.so.8, libcublas.so.8.0, libcublas.so.9, libcublas.so.9.0, libcublas.so.10.0, libcublas.so.10, etc...

how much longer are you going to let this problem last?


Also, stop being so conda focused. Look what oracle did to java once they had the world on it.
stay with the open source standards. Conda is cr..p.


"
26984,"[TF 2.0 API Docs] tf.math modules, new endpoints, gen_math_ops.py","**System information**
- TensorFlow version: 2.0 alpha
- Doc links: 
`tf.math` module-related, including:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math/log
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math/less
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math/less_equal
... and **many more** - a lot of endpoints mentioned below affected

**Describe the documentation issue**

Similar to [26530](https://github.com/tensorflow/tensorflow/issues/26530), [25802](https://github.com/tensorflow/tensorflow/issues/25802), [25846](https://github.com/tensorflow/tensorflow/issues/25846)

(May be similar to [26532](https://github.com/tensorflow/tensorflow/issues/26532) but has to do with `gen_math_ops.py` not `[math_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_ops.py)`.)

- **Incorrect links:**
Missing links to ?non-existent `...python/ops/gen_math_ops.py`

Since [1.10](https://github.com/tensorflow/tensorflow/blob/67a4cbbc7cacdbc33b2adb44f1b08b5a0dbc3186/RELEASE.md):
```
New symbols have been added to the following modules: tf.debugging, tf.dtypes, tf.image, tf.io, tf.linalg, tf.manip, **tf.math**, tf.quantization, tf.strings
```

`...python/ops/gen_math_ops.py` has been referenced a bunch of times e.g. [here](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math/log
) but does not appear to exist or cannot be easily found. There are similar issues with examples, raises, etc for some if not all of the [following](https://github.com/tensorflow/tensorflow/blob/67a4cbbc7cacdbc33b2adb44f1b08b5a0dbc3186/RELEASE.md): 
```
New endpoints in tf.math module namespace: tf.math.acos, tf.math.acosh, tf.math.add, tf.math.asin, tf.math.asinh, tf.math.atan, tf.math.atan2, tf.math.atanh, tf.math.betainc, tf.math.ceil, tf.math.cos, tf.math.cosh, tf.math.digamma, tf.math.equal, tf.math.erfc, tf.math.exp, tf.math.expm1, tf.math.floor, tf.math.greater, tf.math.greater_equal, tf.math.igamma, tf.math.igammac, tf.math.invert_permutation, tf.math.less, tf.math.less_equal, tf.math.lgamma, tf.math.log, tf.math.log1p, tf.math.logical_and, tf.math.logical_not, tf.math.logical_or, tf.math.maximum, tf.math.minimum, tf.math.not_equal, tf.math.polygamma, tf.math.reciprocal, tf.math.rint, tf.math.rsqrt, tf.math.segment_max, tf.math.segment_mean, tf.math.segment_min, tf.math.segment_prod, tf.math.segment_sum, tf.math.sin, tf.math.sinh, tf.math.softplus, tf.math.softsign, tf.math.squared_difference, tf.math.tan, tf.math.unsorted_segment_max, tf.math.unsorted_segment_min, tf.math.unsorted_segment_prod, tf.math.unsorted_segment_sum, tf.math.zeta
```

- **Usage examples**
Needs examples for each of tf.math calculations e.g.
`tf.math.log(42.)` returns
```
<tf.Tensor: id=5, shape=(), dtype=float32, numpy=3.7376697>
```

- **Raises**
Not listed/defined. Obfuscated error from unexpected args (e.g. strings)

E.g. `tf.math.log('hello log')` gives this long error message for a simple natural log calculation. As @dynamicwebpaige said [here](https://github.com/tensorflow/tensorflow/issues/25802) all tf.math.* operations and the operations they influence ... would experience these same obfuscated XLA errors.
```
---------------------------------------------------------------------------
InternalError                             Traceback (most recent call last)
<ipython-input-7-208ff95958ff> in <module>()
----> 1 tf.math.log('hello log')

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py in log(x, name)
   5327       try:
   5328         return log_eager_fallback(
-> 5329             x, name=name, ctx=_ctx)
   5330       except _core._SymbolicException:
   5331         pass  # Add nodes to the TensorFlow graph.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py in log_eager_fallback(x, name, ctx)
   5376   _attrs = (""T"", _attr_T)
   5377   _result = _execute.execute(b""Log"", 1, inputs=_inputs_flat, attrs=_attrs,
-> 5378                              ctx=_ctx, name=name)
   5379   _execute.record_gradient(
   5380       ""Log"", _inputs_flat, _attrs, _result, name)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     64     else:
     65       message = e.message
---> 66     six.raise_from(core._status_to_exception(e.code, message), None)
     67   except TypeError as e:
     68     if any(ops._is_keras_symbolic_tensor(x) for x in inputs):

/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

InternalError: Could not find valid device for node.
Node: {{node Log}}
All kernels registered for op Log :
  device='XLA_CPU'; T in [DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BFLOAT16, DT_COMPLEX128, DT_HALF]
  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BFLOAT16, DT_COMPLEX128, DT_HALF]
  device='CPU'; T in [DT_COMPLEX128]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_BFLOAT16]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_FLOAT]
 [Op:Log]
```

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
Ok
"
26981,"Error file not found, when running protoc command. (use tensorflow  Objection detection api tutorial )","<

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10 64 bit
- 
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.13.1
- Python version: 3.5.3
- Installed using virtualenv? pip? conda?: pip




**Problem Description*
I am trying to run ""introduction use tensorflow objection detection api tutorial"" 
https://pythonprogramming.net/introduction-use-tensorflow-object-detection-api-tutorial/  

. I have installed protoc version 3.7.0 for my 64bit windows 10. I extracted the protoc file into the models-master/research directory. 

When i run the following command 
 **protoc object_detection/protos/*.proto --python_out=** 

i get the following error 

 **_### no such file or directory_** 
  

How do i solve this error ?

"
26980,[TF2.0a0] Distribute Strategies and Summary Writer don't work together,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-alpha0
- Python version: python 3.6
- CUDA/cuDNN version: 10.1
- GPU model and memory: TITAN X


**Describe the current behavior**
I am trying to use tf.summary at the same time as using distribute strategies. However, the call to `strategy.experimental_run` seems to end the scope of the `summary_writer`. I have initialized it properly, but it is not available to me. I have tried passing the SummaryWriter object directly as another parameter of the train_step but it does not work and fails with the error message. Somehow, the scope seems to be lost through the experimental_run call.

```[ops/summary_ops_v2.py:608] ValueError: No step set via 'step' argument or tf.summary.experimental.set_step()```

For the most part, I followed this guide on how to work with DuplicateStrategies: 
https://www.tensorflow.org/alpha/tutorials/distribute/training_loops

**Describe the expected behavior**
Being able to write to write to the summary from within the training loop.


**Code to reproduce the issue**
```python
import tensorflow as tf

def train_step(input, s_writer):
  with s_writer.as_default():
    tf.summary.scalar('test', 0)

def distributed_train(s_writer):
  return strategy.experimental_run(
    lambda x: train_step(x, s_writer), train_iterator)

if __name__ == '__main__':
  dataset = tf.data.Dataset.from_tensor_slices(
    (tf.random.uniform([5, 100])))

  strategy = tf.distribute.MirroredStrategy()
  summary_writer = tf.summary.create_file_writer('/tmp/test')

  with summary_writer.as_default() as summary_scope:
    with strategy.scope():

      train_iterator = strategy.make_dataset_iterator(dataset)
      train_iterator.initialize()

      tf.summary.experimental.set_step(0)
      # This works
      train_step(None, summary_writer)

      # This fails
      distributed_train(summary_writer)
```
"
26978,All images were broken in Chinese document (404),"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version:
- Doc Link: https://www.tensorflow.org/guide/premade_estimators


**Describe the documentation issue**

All images were broken in guide pages.
For example:
https://www.tensorflow.org/images/tensorflow_programming_environment.png
in 
https://www.tensorflow.org/guide/premade_estimators

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
26976,[TF 2.0 API Docs] tf.transpose,"**System information**
- TensorFlow version: 2.0 alpha
- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/transpose

**Describe the documentation issue**

_Params:_ Minor thing - `tf.conj(tf.transpose(input))` should be formatted appropriately imho

_Visuals:_ One would be useful

_Raises:_ No raises listed /defined but would be useful for some folks
e.g. Running this `tf.transpose([1, 2, 3], 262)` gives a long error because `perm` must be a vector
```
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-19-357e79a92891> in <module>()
----> 1 tf.transpose([1, 2, 3], 262)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py in transpose_v2(a, perm, conjugate, name)
   1605     A transposed `Tensor`.
   1606   """"""
-> 1607   return transpose(a=a, perm=perm, name=name, conjugate=conjugate)
   1608 
   1609 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py in transpose(a, perm, name, conjugate)
   1693           ret.set_shape(input_shape[::-1])
   1694     else:
-> 1695       ret = transpose_fn(a, perm, name=name)
   1696     return ret
   1697 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py in transpose(x, perm, name)
  10743       try:
  10744         return transpose_eager_fallback(
> 10745             x, perm, name=name, ctx=_ctx)
  10746       except _core._SymbolicException:
  10747         pass  # Add nodes to the TensorFlow graph.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py in transpose_eager_fallback(x, perm, name, ctx)
  10780   _attrs = (""T"", _attr_T, ""Tperm"", _attr_Tperm)
  10781   _result = _execute.execute(b""Transpose"", 1, inputs=_inputs_flat,
> 10782                              attrs=_attrs, ctx=_ctx, name=name)
  10783   _execute.record_gradient(
  10784       ""Transpose"", _inputs_flat, _attrs, _result, name)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     64     else:
     65       message = e.message
---> 66     six.raise_from(core._status_to_exception(e.code, message), None)
     67   except TypeError as e:
     68     if any(ops._is_keras_symbolic_tensor(x) for x in inputs):

/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

InvalidArgumentError: perm must be a vector, not [] [Op:Transpose] name: transpose/
```

**We welcome contributions by users. Will you be able to update submit a PR (use the doc style guide) to fix the doc Issue?**
Will try
"
26975,"get ""virtual memory exhausted: Cannot allocate memory"" in Makefile","when installing Tensorflow models with:
sudo make install     I get ""Error 1"" when making argmax_op_0

re ran the make file several times after rebooting Raspberry Pi 3 B+
and get same error each time."
26974,tf.keras.layers.Bidirectional is not equivalent to tf.nn.bidirectional_dynamic_rnn,"In tf.nn.bidirectional_dynamic_rnn, we can set ""sequence_length"", which contains the actual lengths of the sequences in a batch. But there is no such setting in tf.keras.layers...
So, if each sample in a batch has a different actual length, how can I do bidirectional_dynamic_rnn in tf.keras.layers?"
26973,Tensorboard crashed when keras finish train in eager execution,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.13.1
- Python version:3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.0
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
tensorboard crashed when keras finish training the model and do a record in tensorboard
**Describe the expected behavior**
tensorboard do the record normally
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
``` python
  if True:
        model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-3), loss={
            # use custom yolo_loss Lambda layer.
            'yolo_loss': lambda y_true, y_pred: y_pred})

        model.fit_generator(data_generator(files,batch_size, input_shape, anchors, num_classes),
                    epochs=0, initial_epoch=0,
                    steps_per_epoch=max(1, sum // batch_size),
                    callbacks=[logging, checkpoint],
                    validation_data=data_generator(val_files, batch_size, input_shape, anchors,num_classes,train=False),
                    validation_steps=max(1, val_sum // batch_size))
        model.save_weights(log_dir + 'trained_weights_stage_1.h5')

    # Unfreeze and continue training, to fine-tune.
    # Train longer if the result is not good.
    if True:
        for i in range(len(model.layers)):
            model.layers[i].trainable = True
        model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-4),
                      loss={'yolo_loss': lambda y_true, y_pred: y_pred})  # recompile to apply the change
        print('Unfreeze all of the layers.')

        model.fit_generator(data_generator(files,batch_size, input_shape, anchors, num_classes),
                    epochs=1, initial_epoch=0, steps_per_epoch=max(1, sum // batch_size),
                    callbacks=[logging,checkpoint, reduce_lr, early_stopping],
                    validation_data=data_generator(val_files, batch_size, input_shape, anchors, num_classes,train=False),
                    validation_steps=max(1, val_sum // batch_size))
        model.save_weights(log_dir + 'trained_weights_final.h5')
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Traceback (most recent call last):
  File ""/media/fangsixie/data/pycharm-2018.2.4/helpers/pydev/pydevd.py"", line 1664, in <module>
    main()
  File ""/media/fangsixie/data/pycharm-2018.2.4/helpers/pydev/pydevd.py"", line 1658, in main
    globals = debugger.run(setup['file'], None, None, is_module)
  File ""/media/fangsixie/data/pycharm-2018.2.4/helpers/pydev/pydevd.py"", line 1068, in run
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""/media/fangsixie/data/pycharm-2018.2.4/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""/media/fangsixie/data/keras-yolo3/train.py"", line 192, in <module>
    _main()
  File ""/media/fangsixie/data/keras-yolo3/train.py"", line 78, in _main
    validation_steps=max(1, val_sum // batch_size))
  File ""/home/fangsixie/.conda/envs/tensorflow1.12/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1426, in fit_generator
    initial_epoch=initial_epoch)
  File ""/home/fangsixie/.conda/envs/tensorflow1.12/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py"", line 232, in model_iteration
    callbacks.on_epoch_end(epoch, epoch_logs, mode=mode)
  File ""/home/fangsixie/.conda/envs/tensorflow1.12/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py"", line 251, in on_epoch_end
    callback.on_epoch_end(epoch, logs)
  File ""/home/fangsixie/.conda/envs/tensorflow1.12/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py"", line 1159, in on_epoch_end
    self._write_custom_summaries(step, logs)
  File ""/home/fangsixie/.conda/envs/tensorflow1.12/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py"", line 1106, in _write_custom_summaries
    summary_ops_v2.scalar(name, value, step=step)
  File ""/home/fangsixie/.conda/envs/tensorflow1.12/lib/python3.6/site-packages/tensorflow/python/ops/summary_ops_v2.py"", line 564, in scalar
    return summary_writer_function(name, tensor, function, family=family)
  File ""/home/fangsixie/.conda/envs/tensorflow1.12/lib/python3.6/site-packages/tensorflow/python/ops/summary_ops_v2.py"", line 508, in summary_writer_function
    should_record_summaries(), record, _nothing, name="""")
  File ""/home/fangsixie/.conda/envs/tensorflow1.12/lib/python3.6/site-packages/tensorflow/python/framework/smart_cond.py"", line 54, in smart_cond
    return true_fn()
  File ""/home/fangsixie/.conda/envs/tensorflow1.12/lib/python3.6/site-packages/tensorflow/python/ops/summary_ops_v2.py"", line 501, in record
    with ops.control_dependencies([function(tag, scope)]):
  File ""/home/fangsixie/.conda/envs/tensorflow1.12/lib/python3.6/site-packages/tensorflow/python/ops/summary_ops_v2.py"", line 562, in function
    name=scope)
  File ""/home/fangsixie/.conda/envs/tensorflow1.12/lib/python3.6/site-packages/tensorflow/python/ops/gen_summary_ops.py"", line 675, in write_scalar_summary
    writer, step, tag, value, name=name, ctx=_ctx)
  File ""/home/fangsixie/.conda/envs/tensorflow1.12/lib/python3.6/site-packages/tensorflow/python/ops/gen_summary_ops.py"", line 706, in write_scalar_summary_eager_fallback
    attrs=_attrs, ctx=_ctx, name=name)
  File ""/home/fangsixie/.conda/envs/tensorflow1.12/lib/python3.6/site-packages/tensorflow/python/eager/execute.py"", line 66, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.NotFoundError: Resource localhost/logdir:logs/000//N10tensorflow22SummaryWriterInterfaceE does not exist. [Op:WriteScalarSummary] name: epoch_loss/"
26969,Batched matmul gives incorrect result on GPU with 32-bit precision and batch size >= 2 ** 16,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): not sure
- TensorFlow version (use command below): v1.12.0-0-ga6d8ffa
- Python version: 3.6.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10.0
- GPU model and memory: GeForce GTX 1080 Ti, 11178MiB

**Describe the current behavior**
When computing a batched matmul with 32-bit precision and batch size >= 2 ** 16, the first 2 ** 16 - 1 batch elements of the result are correct, and the remaining elements are arbitrary - often zero, but not always. When the `sess.run` call is made multiple times, the result is usually the same or similar. The bug only seems to occur when using a variable or placeholder.

**Describe the expected behavior**
Every batch element of the result should be correct.

**Code to reproduce the issue**
```
import numpy as np
import tensorflow as tf

s = (100000, 1, 1)
p = tf.placeholder(shape=s, dtype=tf.float32)
x = tf.matmul(tf.ones(s), p)

with tf.Session() as sess:
    r = sess.run(x, feed_dict={p: np.ones(s, dtype=np.float32)})
    print(r[2**16 - 5:2**16 + 5, 0, 0])
```

Typical output:
```
[1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]
```

**Other info / logs**
"
26967,InvalidArgumentError (see above for traceback): ConcatOp : Dimensions of inputs should match:,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
When I am trianging my model with tf.estimator and tf.data, this issue occurs:
`InvalidArgumentError (see above for traceback): ConcatOp : Dimensions of inputs should match: shape[0] = [128,64] vs. shape[1] = [127,16]`
the batch of data is 128, so the first feature' s is correct, the second wrong. 
Does anyone else have the same problem? Thanks in advance.
**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26966,Add a `length` or size attribute to the `tf.data.Dataset`,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):
- Are you willing to contribute it (Yes/No):
Tensorflow version 1.13.1, running on Ubuntu linux 18.04 LTS x64. Yes, I would be willing to contribute. 


**Describe the feature and the current behavior/state.**
The feature request is to add a length or size method on the `tf.data.Dataset` class. This would allow users to check the number of records in a dataset without having to iterate through the entire dataset. 

Here is a simple use case. I have a bunch of images and labels that I encode to TFRecords format for Tensorflow. The data wrote to the TFRecords format with no trouble, and I can open the dataset with `tf.data.TFRecordDataset()`. I was even able to decode the images and make sure that the data was properly encoded. So far so good. 

However, one challenge is that there does not seem to be an easy way to find the number of records that were encoded into the TFRecords file. One thing I would like to check is whether the number of records encoded to the TFRecords file is equal to the number of images in the source directory. Right now the only way to count the records is to setup the `tf.data.Dataset()`, setup the parse function, the batches, and the iterator. And then I would need to iterate through the dataset in order to count the number of records. This seems to be a lot of work to do a simple check on the number of examples encoded to the TFRecords file. 

Another caveat is that a user could just look at the size of a TFRecords file to get a sense if all of the data is encoded. But this approach has problems because the data encoding to string makes the size of the TFRecords vary substantially (plus or minus) from the sum of the sizes of the individual image and label files. So that metric does not really clarify whether all the data has been encoded. 

**Will this change the current api? How?**
This would change the `tf.data.Dataset` api. Seems like this would just add an additional method to the `Dataset` class to check for length, just as is done in `numpy` or `pandas`, etc. Even tensors have a `shape` attribute.

I understand that the `Dataset` API is an iterator and is deliberately setup to not load all data into memory. So that is fine. But users still need to check that the data they are putting into the TFRecords files still need to check/test that the data is encoded properly. 


**Who will benefit with this feature?**
Any user who uses the `tf.data.Dataset` API to pass data into Tensorflow. This feature will simplify checks/tests to ensure data credibility.


**Any Other info.**
Here are some Stack Exchange posts that show the cumbersome way to do this kind of check today. And even then I am not sure that these approaches are entirely successful. 

https://stackoverflow.com/questions/51871573/is-there-a-way-to-get-the-size-of-tfrecord-file-and-the-size-of-one-example-in-i

https://stackoverflow.com/questions/42799007/number-of-examples-in-each-tfrecord

https://stackoverflow.com/questions/39196955/how-to-get-the-total-number-of-entries-contained-in-a-tfrecord-file

"
26965,freeze_graph successful from command line but fails from Python file,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): tf-nightly-gpu `v1.12.0-10390-ge9a2281040 1.14.1-dev20190319`
- Python version: 3.5.2
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: CUDA 10.0, cuDNN 7.5.0 for CUDA 10.0
- GPU model and memory: GeForce GTX 1070 8GB


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

I have a GraphDef and checkpoint I am trying to freeze.  I am able to do so from the command line, but not from a Python file.

**Describe the expected behavior**

I expect the behavior from the Python file to match the CLI behavior. I am referencing [the args on freeze_graph.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py#L77) and failing to find an issue with my usage.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

This is my CLI command that works:
```
python3 -m tensorflow.python.tools.freeze_graph
    --input_graph=mygraph.pb
    --input_checkpoint=mygraph.ckpt
    --input_binary=true
    --output_graph=frozen_graph.pb
    --output_node_names=mygraph/convolutional10/BiasAdd,mygraph/convolutional13/BiasAdd
```

And this is my attempted Python file.
```
from tensorflow.python.tools import freeze_graph

freeze_graph.freeze_graph(
    'mygraph.pb', # input_graph_def
    None, # input_saver_def
    'mygraph.ckpt', # input_checkpoint
    'mygraph/convolutional10/BiasAdd,mygraph/convolutional13/BiasAdd', # output_node_names
    None, # restore_op_name
    None, # filename_tensor_name
    'frozen_graph.pb', # output_graph
    False, # clear_devices
    """", # initializer_nodes
    """" # variable_names_whitelist
    )
```

If I don't include the `variable_names_whitelist` arg at the end, then I get this error: `TypeError: freeze_graph() missing 1 required positional argument: 'initializer_nodes'`. If I do include it, then I get past that weird error and encounter this one instead: `tensorflow.python.framework.errors_impl.NotFoundError: mygraph/convolutional10/BiasAdd,mygraph/convolutional13; No such file or directory`. Not sure what to make of either of these errors. Why is it using the `output_node_names` to find a file? Also, why is it truncating the string? `mygraph/convolutional10/BiasAdd,mygraph/convolutional13` ≠ `mygraph/convolutional10/BiasAdd,mygraph/convolutional13/BiasAdd`

Any ideas? Thanks.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

See above.
"
26964,Restoring model doesn't see variables,"I saved model using:
```
def serving_input_receiver_fn():
    features = {'comment': tf.placeholder(dtype=tf.int64, shape=[None, 3])}
    return tf.estimator.export.ServingInputReceiver(features, features)

classifier.export_saved_model('saved_model', serving_input_receiver_fn)
```
When I'm trying to restore and use it:
```
predict_fn = predictor.from_saved_model(model_path)
pred = predict_fn({'comment': [1, 2, 3]})['output']
```
I'm getting the following error:

> ValueError: Got unexpected keys in input_dict: {'comment'}
> expected: set()

But when I use cli everything looks ok:
```
saved_model_cli show --dir saved_model/1553134286/ --all

MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:

signature_def['predict']:
  The given SavedModel SignatureDef contains the following input(s):
    inputs['comment'] tensor_info:
        dtype: DT_INT64
        shape: (-1, 3)
        name: Placeholder:0
  The given SavedModel SignatureDef contains the following output(s):
    outputs['class_ids'] tensor_info:
        dtype: DT_INT64
        shape: (-1, 1)
        name: head/predictions/ExpandDims:0
```"
26961,Dynamic ksize and strides with AvgPool,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):
1.11.0

- Are you willing to contribute it (Yes/No):
Yes - but unfamiliar with contribution process and I think it would be very similar to MaxPoolV2

**Describe the feature and the current behavior/state.**
Adding an AvgPoolV2 with dynamic ksize, much like MaxPoolV2. This is very related to issue #11875

**Will this change the current api? How?**
It would add a new function gen_nn_ops.avg_pool_v2

**Who will benefit with this feature?**
Anybody looking to use an average pooling layer with dynamic ksize. For example, when running a graph with an input of an unspecified size when the ksize is dependent on the size of the input

**Any Other info.**
"
26960,tf.compat.v2.summary.scalar has no attribute 'summary_scope',"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, I have a minimal test file
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below):
tf.__version__
'2.0.0-alpha0'
- Python version:
$ python3
Python 3.6.7 (default, Oct 22 2018, 11:32:17)
- Bazel version (if compiling from source): binary installer version 0.23
- GCC/Compiler version (if compiling from source):
$ gcc --version
gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
- CUDA/cuDNN version: Not supported
- GPU model and memory: Not supported


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

tf.compat.v2.summary.scalar(""stddev"",_st,step=count,description='stddev')
is being processed as an _api.v1.summary

**Describe the expected behavior**

tf.compat.v2.summary.scalar(""stddev"",_st,step=count,description='stddev')
is processed as an _api.v2.summary

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

    tf_sdata = tf.convert_to_tensor(sdata,dtype=tf.float32,name=""tf_sdata"")
    tfdata_stddev = tf.data.Dataset.from_tensor_slices(tf_sdata[:,0])

    tfstddev_iter = tfdata_stddev.__iter__()

    stats_writer = tf.compat.v2.summary.create_file_writer(logs_path,max_queue=10,flush_millis=120000,
            filename_suffix='.stats',name='stats_writer')

    count = 1
    for _st in tfstddev_iter:
        tf.compat.v2.summary.scalar(""stddev"",_st,step=count,description='stddev')
        count += 1

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Traceback (most recent call last):
  File ""./tf2summary_bug.py"", line 71, in <module>
    tf.compat.v2.summary.scalar(""stddev"",_st,step=count,description='stddev')
  File ""/usr/lib/python3/dist-packages/tensorboard/plugins/scalar/summary_v2.py"", line 55, in scalar
    with tf.summary.summary_scope(
AttributeError: module 'tensorflow._api.v1.summary' has no attribute 'summary_scope'
"
26959,Component function execution failed: Unknown: Fail to find the dnn implementation.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): should be 2.0.0 alpha; command below throws AttributeError
- Python version: 3.6.7 (Anaconda)
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1/7.5
- GPU model and memory: GTX 1050 2gb


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

Using an LSTM with the Keras package works great until I try to fit my data. This error appears:

```
UnknownError: Fail to find the dnn implementation.
	 [[{{node unified_lstm/CudnnRNN}}]] [Op:__inference_keras_scratch_graph_1590]
```

A similar error appears when I try to use CuDNNLSTM

**Describe the expected behavior**

The call to `model.fit(X, Y)` should fit the model

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
model = tf.keras.Sequential()

model.add(tf.keras.layers.LSTM(256, return_sequences=True))
model.add(tf.keras.layers.Dropout(0.2))
model.add(tf.keras.layers.LSTM(256))
model.add(tf.keras.layers.Dropout(0.2))
model.add(tf.keras.layers.Dense(1))
model.compile(loss='mse', optimizer='sgd')
```
```
model.fit(X, Y)
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Don't know what would be helpful, but `tf.test.is_gpu_available()` returns true. I followed the instructions for installing Tensorflow 2.0 with GPU support here: https://www.tensorflow.org/install/gpu

Another error I get along the way is 
```
tensorflow/stream_executor/cuda/cuda_dnn.cc:338] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
```

I've tried one of the solutions in #6698, but `tf.ConfigProto()` was not found. "
26958,Error C2338 in tensorflow/compiler/tf2xla/cpu_function_runtime.h(71),"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64 bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: branch r1.13
- Python version: Python 3.6.8 :: Anaconda, Inc.
- Installed using virtualenv? pip? conda?: conda 4.6.8
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source): cl.exe C/C++ Optimizing Compiler Version 19.16.27026.1 for x64, Visual Studio Build Tools 2017
- CUDA/cuDNN version: CUDA Toolkit V10.0.130, cuDNN 7.5.0
- GPU model and memory: GTX 1060 6GB



**Describe the problem**

Got error C2338 while building tensorflow from `r13.1` branch in this section of code

```
28    class BufferInfo {
29     public:

[...]

66      // Encodes this BufferInfo into two 64 bit integers that can be used to
67      // reconstruct the BufferInfo later using the constructor.  We need this
68      // because we use BufferInfo in places where using protocol buffers would
69      // negatively impact binary size.
70      std::pair<uint64, uint64> Encode() const {
71        static_assert(sizeof(*this) == 16, """");    <====
72        uint64 upper = Pack(kind(), size_);
73        uint64 lower = entry_param_number_;
74        return {upper, lower};
75      }
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
(tf_gpu) D:\Neural\tensorflow>python ./configure.py
ERROR: Failed to query DisplayName of HKCU\Software\Microsoft\Windows\CurrentVersion\Uninstall\{4A656C6C-D24A-473F-9747-3A8D00907A04}
WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:
nul
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
INFO: Invocation ID: 083eb68f-20f4-4c71-8c8d-9b76a9b0d5cf
You have bazel 0.21.0- (@non-git) installed.
Please specify the location of python. [Default is D:\Programs\Anaconda3\envs\tf_gpu\python.exe]:


Found possible Python library paths:
  D:\Programs\Anaconda3\envs\tf_gpu\lib\site-packages
Please input the desired Python library path to use.  Default is [D:\Programs\Anaconda3\envs\tf_gpu\lib\site-packages]

Do you wish to build TensorFlow with XLA JIT support? [y/N]: y
XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]:
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10.0]:


Please specify the location where CUDA 10.0 toolkit is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0]:


Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]:


Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0]:


Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,7.0]: 6.1


Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]: /arch:AVX2


Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:
Eigen strong inline overridden.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=gdr            # Build with GDR support.
        --config=verbs          # Build with libverbs support.
        --config=ngraph         # Build with Intel nGraph support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=noignite       # Disable Apacha Ignite support.
        --config=nokafka        # Disable Apache Kafka support.
        --config=nonccl         # Disable NVIDIA NCCL support.

(tf_gpu) D:\Neural\tensorflow>bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

[...]

ERROR: D:/neural/tensorflow/tensorflow/compiler/tf2xla/BUILD:98:1: C++ compilation of rule '//tensorflow/compiler/tf2xla:cpu_function_runtime' failed (Exit 2): python.exe failed: error executing command
  cd C:/users/stepii/_bazel_stepii/5mniti2w/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0
    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\ATLMFC\include;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\ATLMFC\lib\x64;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\lib\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\MSBuild\15.0\bin\Roslyn;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\\x64;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\;C:\Program Files (x86)\Windows Kits\10\bin\10.0.17763.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\\MSBuild\15.0\bin;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\Tools\;;C:\Windows\system32;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=D:/Programs/Anaconda3/envs/tf_gpu/python.exe
    SET PYTHON_LIB_PATH=D:/Programs/Anaconda3/envs/tf_gpu/lib/site-packages
    SET TEMP=C:\Users\Stepii\AppData\Local\Temp
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1
    SET TF_CUDA_VERSION=10.0
    SET TF_CUDNN_VERSION=7
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
    SET TMP=C:\Users\Stepii\AppData\Local\Temp
  D:/Programs/Anaconda3/envs/tf_gpu/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Ibazel-out/x64_windows-opt/bin /Iexternal/nsync /Ibazel-out/x64_windows-opt/genfiles/external/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/genfiles/external/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /showIncludes /MD /O2 /DNDEBUG -w /arch:AVX2 /Fobazel-out/x64_windows-opt/bin/tensorflow/compiler/tf2xla/_objs/cpu_function_runtime/cpu_function_runtime.o /c tensorflow/compiler/tf2xla/cpu_function_runtime.cc
Execution platform: @bazel_tools//platforms:host_platform
.\tensorflow/compiler/tf2xla/cpu_function_runtime.h(71): error C2338:
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 1368.629s, Critical Path: 221.82s
INFO: 2899 processes: 2899 local.
FAILED: Build did NOT complete successfully
```


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Full log [here](https://github.com/tensorflow/tensorflow/files/2989879/FullLog.txt)
"
26957,why this error have happen? 'NoneType' object has no attribute '_inbound_nodes',"Hi,
 I want to fill a tensor with the value of the specific index of another tensor, this is my full code
```
from keras.layers import Input, Concatenate, GaussianNoise,Dropout,BatchNormalization
from keras.layers import Conv2D, AtrousConv2D
from keras.models import Model
from keras.datasets import mnist
from keras.callbacks import TensorBoard
from keras import backend as K
from keras import layers
import matplotlib.pyplot as plt
import tensorflow as tf
import keras as Kr
from keras.optimizers import SGD,RMSprop,Adam
from keras.callbacks import ReduceLROnPlateau
from keras.callbacks import EarlyStopping
from keras.callbacks import ModelCheckpoint
import numpy as np
import pylab as pl
import matplotlib.cm as cm
import keract
from matplotlib import pyplot
from keras import optimizers
from keras import regularizers

from tensorflow.python.keras.layers import Lambda;
#-----------------building w train---------------------------------------------
w_expand=np.zeros((49999,28,28),dtype='float32')
wv_expand=np.zeros((9999,28,28),dtype='float32')
wt_random=np.random.randint(2, size=(49999,4,4))
wt_random=wt_random.astype(np.float32)
wv_random=np.random.randint(2, size=(9999,4,4))
wv_random=wv_random.astype(np.float32)
w_expand[:,:4,:4]=wt_random
wv_expand[:,:4,:4]=wv_random
x,y,z=w_expand.shape
w_expand=w_expand.reshape((x,y,z,1))
x,y,z=wv_expand.shape
wv_expand=wv_expand.reshape((x,y,z,1))

#-----------------building w test---------------------------------------------
w_test = np.random.randint(2,size=(1,4,4))
w_test=w_test.astype(np.float32)
wt_expand=np.zeros((1,28,28),dtype='float32')
wt_expand[:,0:4,0:4]=w_test
wt_expand=wt_expand.reshape((1,28,28,1))

#-----------------------encoder------------------------------------------------
#------------------------------------------------------------------------------
wtm=Input((28,28,1))
image = Input((28, 28, 1))
conv1 = Conv2D(64, (5, 5), activation='relu', padding='same', name='convl1e',dilation_rate=(2,2))(image)
conv2 = Conv2D(64, (5, 5), activation='relu', padding='same', name='convl2e',dilation_rate=(2,2))(conv1)
conv3 = Conv2D(64, (5, 5), activation='relu', padding='same', name='convl3e',dilation_rate=(2,2))(conv2)
BN=BatchNormalization()(conv3)
encoded =  Conv2D(1, (5, 5), activation='relu', padding='same',name='encoded_I',dilation_rate=(2,2))(BN)


temp=tf.reshape(wtm,(28,28))
wfill=Kr.layers.Lambda(lambda x:tf.fill([28,28],x))
wtm_Fill=wfill(temp[0,0])
add_const = Kr.layers.Lambda(lambda x: x[0] + x[1])
encoded_merged = add_const([encoded,wtm_Fill])

#wfill=Kr.layers.Lambda(lambda x:tf.fill([28,28],x))
#value=wtm[0][0][0]
#x=tf.fill((28,28,1),value)
#add_const = Kr.layers.Lambda(lambda x: x[0] + x[1])
#encoded_merged = add_const([encoded,x])
#encoder=Model(inputs=[image,wtm], outputs= encoded_merged ,name='encoder')
#encoder.summary()

#-----------------------decoder------------------------------------------------
#------------------------------------------------------------------------------
deconv1 = Conv2D(64, (5, 5), activation='relu', padding='same', name='convl1d',dilation_rate=(2,2))(encoded_merged)
deconv2 = Conv2D(64, (5, 5), activation='relu', padding='same', name='convl2d',dilation_rate=(2,2))(deconv1)
deconv3 = Conv2D(64, (5, 5), activation='relu',padding='same', name='convl3d',dilation_rate=(2,2))(deconv2)
deconv4 = Conv2D(64, (5, 5), activation='relu',padding='same', name='convl4d',dilation_rate=(2,2))(deconv3)
BNd=BatchNormalization()(deconv3)

decoded = Conv2D(1, (5, 5), activation='sigmoid', padding='same', name='decoder_output',dilation_rate=(2,2))(BNd) 

model=Model(inputs=[image,wtm],outputs=decoded)

decoded_noise = GaussianNoise(0.5)(decoded)

#----------------------w extraction------------------------------------
convw1 = Conv2D(64, (3,3), activation='relu', padding='same', name='conl1w',dilation_rate=(2,2))(decoded_noise)
convw2 = Conv2D(64, (3, 3), activation='relu', padding='same', name='convl2w',dilation_rate=(2,2))(convw1)
convw3 = Conv2D(64, (3, 3), activation='relu', padding='same', name='conl3w',dilation_rate=(2,2))(convw2)
convw4 = Conv2D(64, (3, 3), activation='relu', padding='same', name='conl4w',dilation_rate=(2,2))(convw3)
convw5 = Conv2D(64, (3, 3), activation='relu', padding='same', name='conl5w',dilation_rate=(2,2))(convw4)
convw6 = Conv2D(64, (3, 3), activation='relu', padding='same', name='conl6w',dilation_rate=(2,2))(convw5)
pred_w = Conv2D(1, (1, 1), activation='sigmoid', padding='same', name='reconstructed_W',dilation_rate=(2,2))(convw6)  
w_extraction=Model(inputs=[image,wtm],outputs=[decoded,pred_w])

w_extraction.summary()
```
I need sth like this. 
```
wtm=
0 1 1
1 1 0
0 0 1
```
wtm(0,0)=0 so I want to produce this new tensor with shape(28,28,1)

```
0 0 0 ... 0
.
. 0 0 ... 0
0 0 0 ... 0
```
but when I implement the code it produces this error:

> Traceback (most recent call last):
> 
> File """", line 106, in model=Model(inputs=[image,wtm],outputs=decoded)
> 
> File ""D:\software\Anaconda3\envs\py36\lib\site-packages\keras\legacy\interfaces.py"", line 91, in wrapper return func(*args, **kwargs)
> 
> File ""D:\software\Anaconda3\envs\py36\lib\site-packages\keras\engine\network.py"", line 93, in init self._init_graph_network(*args, **kwargs)
> 
> File ""D:\software\Anaconda3\envs\py36\lib\site-packages\keras\engine\network.py"", line 231, in _init_graph_network self.inputs, self.outputs)
> 
> File ""D:\software\Anaconda3\envs\py36\lib\site-packages\keras\engine\network.py"", line 1366, in _map_graph_network tensor_index=tensor_index)
> 
> File ""D:\software\Anaconda3\envs\py36\lib\site-packages\keras\engine\network.py"", line 1353, in build_map node_index, tensor_index)
> 
> File ""D:\software\Anaconda3\envs\py36\lib\site-packages\keras\engine\network.py"", line 1353, in build_map node_index, tensor_index)
> 
> File ""D:\software\Anaconda3\envs\py36\lib\site-packages\keras\engine\network.py"", line 1353, in build_map node_index, tensor_index)
> 
> File ""D:\software\Anaconda3\envs\py36\lib\site-packages\keras\engine\network.py"", line 1353, in build_map node_index, tensor_index)
> 
> File ""D:\software\Anaconda3\envs\py36\lib\site-packages\keras\engine\network.py"", line 1353, in build_map node_index, tensor_index)
> 
> File ""D:\software\Anaconda3\envs\py36\lib\site-packages\keras\engine\network.py"", line 1353, in build_map node_index, tensor_index)
> 
> File ""D:\software\Anaconda3\envs\py36\lib\site-packages\keras\engine\network.py"", line 1353, in build_map node_index, tensor_index)
> 
> File ""D:\software\Anaconda3\envs\py36\lib\site-packages\keras\engine\network.py"", line 1325, in build_map node = layer._inbound_nodes[node_index]
> 
> AttributeError: 'NoneType' object has no attribute '_inbound_nodes'


what should I do? how can I access to the specific value in tensor and do sth like the thing I mentioed before? I really need your explanation. Thanks"
26956,tensorflow/core/kernels/slice_op_gpu.cu.cc is never referenced in any BUILD file,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.13
- Python version: 3.6
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): 0.19.2
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10
- GPU model and memory: N/A

**Describe the problem**

This file: https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/core/kernels/slice_op_gpu.cu.cc

Is never referenced anywhere in the tree. When building with CUDA and a dependency on [//tensorflow/core/kernels:strided_slice_op](https://github.com/tensorflow/tensorflow/blob/6612da89516247503f03ef76e974b51a434fb52e/tensorflow/core/kernels/BUILD#L119), it fails at link time with errors like this one:

undefined reference to `tensorflow::functor::Slice<Eigen::GpuDevice, signed char, 5>::operator()(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<signed char, 5, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<signed char const, 5, 1, long>, 16, Eigen::MakePointer>, Eigen::DSizes<long, 5> const&, Eigen::DSizes<long, 5> const&)'

The problem seems to be that [tensorflow::HandleStridedSliceCase](https://github.com/tensorflow/tensorflow/blob/6612da89516247503f03ef76e974b51a434fb52e/tensorflow/core/kernels/strided_slice_op_impl.h#L95) will call `functor::Slice` if the slice is a ""simple slice"", so when the template is instantiated for GPU [here](https://github.com/tensorflow/tensorflow/blob/6612da89516247503f03ef76e974b51a434fb52e/tensorflow/core/kernels/strided_slice_op_impl.h#L284-L290) it references the undefined GPU version of `functor::Slice`.

Seems to me like somewhere in tensorflow/core/kernels/BUILD should be referencing slice_op_gpu.cu.cc, maybe the strided_slice_op target?"
26954,Documentation Request: Transfer Learning in TF 2 with same and/or different final layer,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: 2 alpha
- Doc Link: N/A


**Describe the documentation issue**
Hello, I'm in the process of porting some of our existing TF 1.x based code to be ready for TF 2.0. This means we are trying to be focused around tf.keras, so I was hoping for an example of the best way to do transfer learning in tf.keras. Specifically, we have the following (I assume common) use case.

1. Train a model (say it's classification) with on dataset a, with 100 classes in the outputs.
2. Finetune the model on dataset b, using the same architecture, also with 100 classes.
3. Finetune the model on dataset c, which is the same except it has 50 classes, and thus the final layer has a different shape. So, we want to load all of the weights except the final layer,  and then train.

In TF 1.x, we used tf.estimator. For task 3, we would label the final layer with the name ""final_layer"", and then when warm-starting the models for finetuning, we would use the WarmstartSettings object to exclude weights that have final_layer in them. What is the equivalent for tf.keras in TF 2.x? Probably the cleanest way we could have the old functionality is if tf.keras.Model.load_weights had a regex field in the same form as WarmStartSettings, which only loaded a subset of variables.

Thanks!

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
I'm happy to write the docs if someone could give a high-level summary of how this is done. All the ways I've come up with are pretty jank, and I assume there is a better way to do this given how elegant it was in TF 1.X.
"
26952,OSError: libcudart.so.9.0: cannot open shared object file: No such file or directory,"**I know that my question isn't nothing to do with tensorflow but i'll appreciate any help**
when i was trying to prepare voc dataset for training pre trained model i got this error

> OSError: libcudart.so.9.0: cannot open shared object file: No such file or directory

`from gluoncv.data import VOCDetection
class VOCLike(VOCDetection):
    CLASSES = ['iris']
    def __init__(self, root, splits, transform=None, index_map=None, preload_label=True):
        super(VOCLike, self).__init__(root, splits, transform, index_map, preload_label)
dataset = VOCLike(root='/home/dell/Bureau/myDataset', splits=((2019, 'train'),))
print(dataset[0])`
**Knowing that I can't use cuda or gpu because i haven't a graphic card nvidia**
"
26951,EagerTensor.numpy() fails with string dtype when called from py_function on GPU,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.13.1
- Python version: 3.7.1
- Bazel version (if compiling from source): 0.20.0 
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version: 10.0
- GPU model and memory: Tesla K80

**Describe the current behavior**
When a `py_function` op is defined for GPU (the default if one is available), and it calls a function that takes a string argument, a `RuntimeError` is raised if one attempts to access the value of the argument by calling `EagerTensor.numpy()`.

If the `py_function` op is defined for CPU no error is raised.

**Describe the expected behavior**
One of the following:
  - String arguments are disallowed for a `py_function` running on GPU, and the TF documentation explains this behavior.
  - The value of a string argument is accessible in the scenario described above.

**Code to reproduce the issue**
This does not work:
```python
def func_with_string_arg(arg):
    return arg.numpy()

with tf.Graph().as_default():
    tf_arg = tf.convert_to_tensor('abcdef')
    ret = tf.py_function(
        func_with_string_arg,
        [tf_arg],
        tf.dtypes.string
    )
    with tf.Session() as sess:
        print(sess.run(ret))
```

Snippet of the error:
```
UnknownError: RuntimeError: Error copying tensor to device: CPU:0. Can't copy 38 bytes of a tensor into another with 32 bytes buffer.
Traceback (most recent call last):

  File ""/home/asarroff/anaconda3/envs/tf-1.13.1/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py"", line 205, in __call__
    return func(device, token, args)

  File ""/home/asarroff/anaconda3/envs/tf-1.13.1/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py"", line 107, in __call__
    ret = self._func(*args)

  File ""<ipython-input-5-590852b9a5d8>"", line 2, in func_with_string_arg
    return arg.numpy()

  File ""/home/asarroff/anaconda3/envs/tf-1.13.1/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 725, in numpy
    return self._cpu_nograd()._numpy()  # pylint: disable=protected-access

  File ""/home/asarroff/anaconda3/envs/tf-1.13.1/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 892, in _cpu_nograd
    return self._copy_nograd(context.context(), ""CPU:0"")

  File ""/home/asarroff/anaconda3/envs/tf-1.13.1/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 840, in _copy_nograd
    new_tensor = self._copy_to_device(context=ctx._handle, device=device_name)

RuntimeError: Error copying tensor to device: CPU:0. Can't copy 38 bytes of a tensor into another with 32 bytes buffer.


	 [[{{node EagerPyFunc}}]]
	 [[{{node EagerPyFunc}}]]
```

This works:
```python
with tf.Graph().as_default():
    tf_arg = tf.convert_to_tensor('abcdef')
    with tf.device('/device:CPU:*'):
        ret = tf.py_function(
            func_with_string_arg,
            [tf_arg],
            tf.dtypes.string
        )
    with tf.Session() as sess:
        print(sess.run(ret))
```
"
26950,FailedPreconditionError when training on TPU,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):YES
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Google Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):1.13.1
- Python version:3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
I want to train my model using AdaBound optimizer but the model after compilation is throwing error using the TPU. 
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
import re
class AdaBoundOptimizer(tf.train.Optimizer):
    """"""Optimizer that implements the AdaBound algorithm.
    See [Luo et al., 2019](https://openreview.net/forum?id=Bkg3g2R9FX)
    ([pdf](https://openreview.net/pdf?id=Bkg3g2R9FX)).
    """"""

    def __init__(self,
                 learning_rate=0.001,
                 final_lr=0.1,
                 beta1=0.9,
                 beta2=0.999,
                 gamma=1e-3,
                 epsilon=1e-8,
                 amsbound=False,
                 decay=0.,
                 weight_decay=0.,
                 exclude_from_weight_decay=None,
                 use_locking=False, name=""AdaBound""):
        super(AdaBoundOptimizer, self).__init__(use_locking, name)

        if final_lr <= 0.:
            raise ValueError(""Invalid final learning rate : {}"".format(final_lr))
        if not 0. <= beta1 < 1.:
            raise ValueError(""Invalid beta1 value : {}"".format(beta1))
        if not 0. <= beta2 < 1.:
            raise ValueError(""Invalid beta2 value : {}"".format(beta2))
        if not 0. <= gamma < 1.:
            raise ValueError(""Invalid gamma value : {}"".format(gamma))
        if epsilon <= 0.:
            raise ValueError(""Invalid epsilon value : {}"".format(epsilon))

        self._lr = learning_rate
        self._beta1 = beta1
        self._beta2 = beta2
        self._final_lr = final_lr
        self._gamma = gamma
        self._epsilon = epsilon
        self._amsbound = amsbound
        self._decay = decay
        self._weight_decay = weight_decay
        self._exclude_from_weight_decay = exclude_from_weight_decay

        self._base_lr = learning_rate

    def apply_gradients(self, grads_and_vars, global_step=None, name=None):
        lr = self._lr
        t = tf.cast(global_step, dtype=tf.float32)

        if self._decay > 0.:
            lr *= (1. / (1. + self._decay * t))

        t += 1

        bias_correction1 = 1. - (self._beta1 ** t)
        bias_correction2 = 1. - (self._beta2 ** t)
        step_size = (lr * tf.sqrt(bias_correction2) / bias_correction1)

        # Applies bounds on actual learning rate
        # lr_scheduler cannot affect final_lr, this is a workaround to apply lr decay
        final_lr = self._final_lr * lr / self._base_lr
        lower_bound = final_lr * (1. - 1. / (self._gamma * t + 1.))
        upper_bound = final_lr * (1. + 1. / (self._gamma * t))

        assignments = []
        for grad, param in grads_and_vars:
            if grad is None and param is None:
                continue

            param_name = self._get_variable_name(param.name)

            m = tf.get_variable(
                name=param_name + ""/adabound_m"",
                shape=param.shape.as_list(),
                dtype=tf.float32,
                trainable=False,
                initializer=tf.zeros_initializer())
            v = tf.get_variable(
                name=param_name + ""/adabound_v"",
                shape=param.shape.as_list(),
                dtype=tf.float32,
                trainable=False,
                initializer=tf.zeros_initializer())
            if self._amsbound:
                v_hat = tf.get_variable(
                    name=param_name + ""/adabound_v_hat"",
                    shape=param.shape.as_list(),
                    dtype=tf.float32,
                    trainable=False,
                    initializer=tf.zeros_initializer())

            m_t = (
                    tf.multiply(self._beta1, m) + tf.multiply(1. - self._beta1, grad))
            v_t = (
                    tf.multiply(self._beta2, v) + tf.multiply(1. - self._beta2, tf.square(grad)))

            if self._amsbound:
                # Maintains the maximum of all 2nd moment running avg. till now
                v_hat_t = tf.maximum(v_hat, v_t)

                # Use the max. for normalizing running avg. of gradient
                denom = (tf.sqrt(v_hat_t) + self._epsilon)
            else:
                denom = (tf.sqrt(v_t) + self._epsilon)

            step_size_p = step_size * tf.ones_like(denom)
            step_size_p_bound = step_size_p / denom

            lr_t = m_t * tf.clip_by_value(t=step_size_p_bound,
                                          clip_value_min=lower_bound,
                                          clip_value_max=upper_bound)
            p_t = param - lr_t

            if self._do_use_weight_decay(param_name):
                p_t += self._weight_decay * param

            update_list = [param.assign(p_t), m.assign(m_t), v.assign(v_t)]
            if self._amsbound:
                update_list.append(v_hat.assign(v_hat_t))

            assignments.extend(update_list)

        # update the global step
        assignments.append(global_step.assign_add(1))

        return tf.group(*assignments, name=name)

    def _do_use_weight_decay(self, param_name):
        """"""Whether to use L2 weight decay for `param_name`.""""""
        if not self._weight_decay:
            return False
        if self._exclude_from_weight_decay:
            for r in self.exclude_from_weight_decay:
                if re.search(r, param_name) is not None:
                    return False
        return True

    @staticmethod
    def _get_variable_name(param_name):
        """"""Get the variable name from the tensor name.""""""
        m = re.match(""^(.*):\\d+$"", param_name)
        if m is not None:
            param_name = m.group(1)
        return param_name


tf.logging.set_verbosity(tf.logging.INFO)
tpu_model = tf.contrib.tpu.keras_to_tpu_model(
      res_model,
      strategy=tf.contrib.tpu.TPUDistributionStrategy(
          tf.contrib.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])
      )
  )
tpu_model.compile(optimizer=AdaBoundOptimizer(learning_rate=0.0001, final_lr=0.1, beta1=0.9, beta2=0.999, amsbound=False), loss=tf.keras.losses.binary_crossentropy, metrics=['acc'])
 
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```
labeled_files size : 220025
test_files size : 57458

FOLD: 0
Train: 
{0: 104725, 1: 71292}
Val: 
{0: 26182, 1: 17824}
INFO:tensorflow:Querying Tensorflow master (grpc://10.111.5.58:8470) for TPU system metadata.
INFO:tensorflow:Found TPU system:
INFO:tensorflow:*** Num TPU Cores: 8
INFO:tensorflow:*** Num TPU Workers: 1
INFO:tensorflow:*** Num TPU Cores Per Worker: 8
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 6439601644184290312)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 5999939926878101922)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 2915973256505920338)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 302180528578565888)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 7907037798004119455)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 8233517434947874609)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 16859822429318257159)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 11771514073813576570)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 8064934956870564867)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 14384107720867386803)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 13575223746004231404)
WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.
Epoch 1/10
INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(128, 96, 96, 3), dtype=tf.float32, name='input_10'), TensorSpec(shape=(128, 1), dtype=tf.float32, name='dense_1_target_10')]
INFO:tensorflow:Overriding default placeholder.
INFO:tensorflow:Remapping placeholder for input_1
INFO:tensorflow:Started compiling
INFO:tensorflow:Finished compiling. Time elapsed: 12.17287826538086 secs
INFO:tensorflow:Setting weights on TPU model.
---------------------------------------------------------------------------
FailedPreconditionError                   Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1333     try:
-> 1334       return fn(*args)
   1335     except errors.OpError as e:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)
   1318       return self._call_tf_sessionrun(
-> 1319           options, feed_dict, fetch_list, target_list, run_metadata)
   1320 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)
   1406         self._session, options, feed_dict, fetch_list, target_list,
-> 1407         run_metadata)
   1408 

FailedPreconditionError: Combined status information from 9 operations:

Status code: Failed precondition [9x]
  Error while reading resource variable tpu_140680564147536//dense_1/kernel/adabound_m from Container: worker. This could mean that the variable was uninitialized. Not found: Resource worker/tpu_140680564147536//dense_1/kernel/adabound_m/N10tensorflow3VarE does not exist.
  	 [[{{node tpu_140680564147536//dense_1/kernel/adabound_m}}]] [1x]
  Error while reading resource variable tpu_140680564147536//dense_1/kernel/adabound_m from Container: worker. This could mean that the variable was uninitialized. Not found: Resource worker/tpu_140680564147536//dense_1/kernel/adabound_m/N10tensorflow3VarE does not exist.
  	 [[{{node tpu_140680564147536//dense_1/kernel/adabound_m}}]]
  	 [[{{node TPUReplicateMetadata}}]] [1x]
  Error while reading resource variable tpu_140680564147536//dense_1/kernel/adabound_m from Container: worker. This could mean that the variable was uninitialized. Not found: Resource worker/tpu_140680564147536//dense_1/kernel/adabound_m/N10tensorflow3VarE does not exist.
  	 [[{{node tpu_140680564147536//dense_1/kernel/adabound_m}}]]
  	 [[{{node cluster/_variable_copy/_80}}]] [7x]
(0 successful operations.)

During handling of the above exception, another exception occurred:

FailedPreconditionError                   Traceback (most recent call last)
<ipython-input-37-fe905384dbd9> in <module>()
     75 
     76   history = tpu_model.fit_generator(data_gen(train,id_label_map,train_batch_size,do_train_augmentations),steps_per_epoch=train_steps,epochs = 10,
---> 77                                    validation_data = data_gen(val,id_label_map,val_batch_size,do_inference_aug),validation_steps = val_steps,callbacks = callbacks)
     78 
     79 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1424         use_multiprocessing=use_multiprocessing,
   1425         shuffle=shuffle,
-> 1426         initial_epoch=initial_epoch)
   1427 
   1428   def evaluate_generator(self,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, **kwargs)
    189       progbar.on_batch_begin(step, batch_logs)
    190 
--> 191       batch_outs = batch_function(*batch_data)
    192       if not isinstance(batch_outs, list):
    193         batch_outs = [batch_outs]

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in train_on_batch(self, x, y, sample_weight, class_weight, reset_metrics)
   1189       else:
   1190         self._make_fit_function()
-> 1191         outputs = self._fit_function(ins)  # pylint: disable=not-callable
   1192 
   1193     if reset_metrics:

/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py in __call__(***failed resolving arguments***)
   1267         tpu_model_ops.infeed_op, tpu_model_ops.execute_op,
   1268         tpu_model_ops.outfeed_op
-> 1269     ], infeed_dict)
   1270     return self._process_outputs(outfeed_outputs)
   1271 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    927     try:
    928       result = self._run(None, fetches, feed_dict, options_ptr,
--> 929                          run_metadata_ptr)
    930       if run_metadata:
    931         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1150     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1151       results = self._do_run(handle, final_targets, final_fetches,
-> 1152                              feed_dict_tensor, options, run_metadata)
   1153     else:
   1154       results = []

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1326     if handle is None:
   1327       return self._do_call(_run_fn, feeds, fetches, targets, options,
-> 1328                            run_metadata)
   1329     else:
   1330       return self._do_call(_prun_fn, handle, feeds, fetches)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1346           pass
   1347       message = error_interpolation.interpolate(message, self._graph)
-> 1348       raise type(e)(node_def, op, message)
   1349 
   1350   def _extend_graph(self):

FailedPreconditionError: Combined status information from 9 operations:

Status code: Failed precondition [9x]
  Error while reading resource variable tpu_140680564147536//dense_1/kernel/adabound_m from Container: worker. This could mean that the variable was uninitialized. Not found: Resource worker/tpu_140680564147536//dense_1/kernel/adabound_m/N10tensorflow3VarE does not exist.
  	 [[node tpu_140680564147536//dense_1/kernel/adabound_m (defined at <ipython-input-36-baab4250bb83>:77) ]] [1x]
  Error while reading resource variable tpu_140680564147536//dense_1/kernel/adabound_m from Container: worker. This could mean that the variable was uninitialized. Not found: Resource worker/tpu_140680564147536//dense_1/kernel/adabound_m/N10tensorflow3VarE does not exist.
  	 [[node tpu_140680564147536//dense_1/kernel/adabound_m (defined at <ipython-input-36-baab4250bb83>:77) ]]
  	 [[node TPUReplicateMetadata (defined at <ipython-input-37-fe905384dbd9>:77) ]] [1x]
  Error while reading resource variable tpu_140680564147536//dense_1/kernel/adabound_m from Container: worker. This could mean that the variable was uninitialized. Not found: Resource worker/tpu_140680564147536//dense_1/kernel/adabound_m/N10tensorflow3VarE does not exist.
  	 [[node tpu_140680564147536//dense_1/kernel/adabound_m (defined at <ipython-input-36-baab4250bb83>:77) ]]
  	 [[{{node cluster/_variable_copy/_80}}]] [7x]
(0 successful operations.)

Caused by op 'tpu_140680564147536//dense_1/kernel/adabound_m', defined at:
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py"", line 16, in <module>
    app.launch_new_instance()
  File ""/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py"", line 477, in start
    ioloop.IOLoop.instance().start()
  File ""/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py"", line 888, in start
    handler_func(fd_obj, events)
  File ""/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py"", line 450, in _handle_events
    self._handle_recv()
  File ""/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py"", line 480, in _handle_recv
    self._run_callback(callback, msg)
  File ""/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py"", line 432, in _run_callback
    callback(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py"", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py"", line 235, in dispatch_shell
    handler(stream, idents, msg)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py"", line 399, in execute_request
    user_expressions, allow_stdin)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py"", line 196, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py"", line 533, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py"", line 2718, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py"", line 2822, in run_ast_nodes
    if self.run_code(code, result):
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py"", line 2882, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-37-fe905384dbd9>"", line 77, in <module>
    validation_data = data_gen(val,id_label_map,val_batch_size,do_inference_aug),validation_steps = val_steps,callbacks = callbacks)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py"", line 1426, in fit_generator
    initial_epoch=initial_epoch)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py"", line 191, in model_iteration
    batch_outs = batch_function(*batch_data)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py"", line 1191, in train_on_batch
    outputs = self._fit_function(ins)  # pylint: disable=not-callable
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py"", line 1260, in __call__
    infeed_manager)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py"", line 1169, in _tpu_model_ops_for_input_specs
    infeed_manager)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py"", line 1079, in _specialize_model
    _model_fn, inputs=[[] for _ in range(self._tpu_assignment.num_towers)])
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu.py"", line 689, in split_compile_and_replicate
    outputs = computation(*computation_inputs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py"", line 1033, in _model_fn
    self._cloned_model._make_fit_function()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py"", line 1926, in _make_fit_function
    '_fit_function', [self.total_loss] + metrics_tensors)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py"", line 1895, in _make_train_function_helper
    params=self._collected_trainable_weights, loss=self.total_loss)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizers.py"", line 763, in get_updates
    grads, global_step=self.iterations)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_optimizer.py"", line 171, in apply_gradients
    return self._opt.apply_gradients(summed_grads_and_vars, global_step, name)
  File ""<ipython-input-36-baab4250bb83>"", line 77, in apply_gradients
    initializer=tf.zeros_initializer())
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py"", line 1479, in get_variable
    aggregation=aggregation)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py"", line 1220, in get_variable
    aggregation=aggregation)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py"", line 530, in get_variable
    return custom_getter(**custom_getter_kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu.py"", line 682, in custom_getter
    return getter(name, *args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py"", line 499, in _true_getter
    aggregation=aggregation)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py"", line 911, in _get_single_variable
    aggregation=aggregation)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py"", line 213, in __call__
    return cls._variable_v1_call(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py"", line 176, in _variable_v1_call
    aggregation=aggregation)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py"", line 155, in <lambda>
    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py"", line 2488, in default_variable_creator
    import_scope=import_scope)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py"", line 217, in __call__
    return super(VariableMetaclass, cls).__call__(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py"", line 294, in __init__
    constraint=constraint)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py"", line 413, in _init_from_args
    graph_mode=self._in_graph_mode)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py"", line 67, in eager_safe_variable_handle
    container=container)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_resource_variable_ops.py"", line 1266, in var_handle_op
    shared_name=shared_name, name=name)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3300, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1801, in __init__
    self._traceback = tf_stack.extract_stack()

FailedPreconditionError (see above for traceback): Combined status information from 9 operations:

Status code: Failed precondition [9x]
  Error while reading resource variable tpu_140680564147536//dense_1/kernel/adabound_m from Container: worker. This could mean that the variable was uninitialized. Not found: Resource worker/tpu_140680564147536//dense_1/kernel/adabound_m/N10tensorflow3VarE does not exist.
  	 [[node tpu_140680564147536//dense_1/kernel/adabound_m (defined at <ipython-input-36-baab4250bb83>:77) ]] [1x]
  Error while reading resource variable tpu_140680564147536//dense_1/kernel/adabound_m from Container: worker. This could mean that the variable was uninitialized. Not found: Resource worker/tpu_140680564147536//dense_1/kernel/adabound_m/N10tensorflow3VarE does not exist.
  	 [[node tpu_140680564147536//dense_1/kernel/adabound_m (defined at <ipython-input-36-baab4250bb83>:77) ]]
  	 [[node TPUReplicateMetadata (defined at <ipython-input-37-fe905384dbd9>:77) ]] [1x]
  Error while reading resource variable tpu_140680564147536//dense_1/kernel/adabound_m from Container: worker. This could mean that the variable was uninitialized. Not found: Resource worker/tpu_140680564147536//dense_1/kernel/adabound_m/N10tensorflow3VarE does not exist.
  	 [[node tpu_140680564147536//dense_1/kernel/adabound_m (defined at <ipython-input-36-baab4250bb83>:77) ]]
  	 [[{{node cluster/_variable_copy/_80}}]] [7x]
(0 successful operations.)"
26948,why do i get empty array at tf.keras.Sequential(layers).trainable_variables?,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows
- TensorFlow version: 2.0.0-alpha0
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip


**why do I get empty array ?**

I'm trying to follow tf2.0 tuts on the website to perform a simple perceptron but seems like the package manager isn't able to pick up the new release but also not throwing any error.
what could be the reason to this?

![well](https://user-images.githubusercontent.com/26624903/54701853-c920fc00-4b5b-11e9-9fcd-ff39fff60284.jpg)
"
26947,Tensorflow 2.0 compile for raspberry pi failed with file not found error,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **Python version**: tried both 2.7 and 3.6
- **Bazel version (if compiling from source)**: 0.13.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**:10.0.130
- **GPU model and memory**: GTX 1070 Mobile 8GB
- **Exact command to reproduce**:
from https://www.tensorflow.org/install/source_rpi
```
git checkout r2.0
sudo tensorflow/tools/ci_build/ci_build.sh PI \
    tensorflow/tools/ci_build/pi/build_raspberry_pi.sh
```
### Describe the problem
I am trying to compile Tensorflow 2.0 for the raspberry pi to install it in Python 2 for use with ROS Kinetic. The pi is using Ubuntu Mate 16.04 and I am doing the cross compilation on my laptop with Ubuntu 16.04.
I used the instructions mentioned on the tensorflow website and tried to compile for both 2.7 and 3. The build fails in both cases with this error:
```
/usr/include/python3.4/pyconfig.h:13:55: fatal error: arm-linux-gnueabihf/python3.4m/pyconfig.h: No such file or directory
 #  include <arm-linux-gnueabihf/python3.4m/pyconfig.h>
                                                       ^
compilation terminated.

```

### Source code / logs
```
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
INFO: Invocation ID: 7644074a-82e2-4b8a-bcb6-10884a6300b8
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
INFO: Invocation ID: 8e36ba59-8da1-4b1c-8840-493d798ee2ca
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
INFO: Invocation ID: e48bec62-a84a-4ec3-89f5-c1a7a71fb94f
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   159    0   159    0     0    418      0 --:--:-- --:--:-- --:--:--   418
  0     0    0  323M    0     0  2932k      0 --:--:--  0:01:53 --:--:-- 7693k
Cloning into '/tmp/openblas_src'...
Note: checking out '5a6a2bed9aff0ba8a18651d5514d029c8cae336a'.

You are in 'detached HEAD' state. You can look around, make experimental
changes and commit them, and you can discard any commits you make in this
state without impacting any branches by performing another checkout.

If you want to create a new branch to retain commits you create, you may
do so (now or later) by using -b with the checkout command again. Example:

  git checkout -b new_branch_name

HEAD is now at 5a6a2be... Merge pull request #1623 from fenrus75/fast-thread
/tmp/toolchain_install//tools/arm-bcm2708/arm-rpi-4.9.3-linux-gnueabihf/bin/arm-linux-gnueabihf-ar: creating ../libopenblas_armv6p-r0.3.1.dev.a
memory.c: In function 'get_num_procs':
memory.c:191:7: warning: unused variable 'n' [-Wunused-variable]
 int i,n;
       ^
memory.c:191:5: warning: unused variable 'i' [-Wunused-variable]
 int i,n;
     ^
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
INFO: Invocation ID: 4ffd1046-cb13-4bd9-836b-68b4e27ba4fb
Loading: 
Loading: 0 packages loaded
Analyzing: 4 targets (3 packages loaded)
Analyzing: 4 targets (4 packages loaded, 0 targets configured)
Analyzing: 4 targets (82 packages loaded, 2059 targets configured)
Analyzing: 4 targets (173 packages loaded, 7791 targets configured)
Analyzing: 4 targets (306 packages loaded, 12072 targets configured)
Analyzing: 4 targets (356 packages loaded, 17197 targets configured)
Analyzing: 4 targets (383 packages loaded, 20919 targets configured)
WARNING: /workspace/tensorflow/python/BUILD:3239:1: in py_library rule //tensorflow/python:standard_ops: target '//tensorflow/python:standard_ops' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: /workspace/tensorflow/python/BUILD:102:1: in py_library rule //tensorflow/python:no_contrib: target '//tensorflow/python:no_contrib' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: /workspace/tensorflow/contrib/metrics/BUILD:16:1: in py_library rule //tensorflow/contrib/metrics:metrics_py: target '//tensorflow/contrib/metrics:metrics_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: /workspace/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: /workspace/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
WARNING: /workspace/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /workspace/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
INFO: Analysed 4 targets (383 packages loaded, 22853 targets configured).
Building: no action
INFO: Found 4 targets...
[5 / 17] [-----] BazelWorkspaceStatusAction stable-status.txt
[43 / 352] no action
[1,465 / 7,100] [-----] Creating runfiles tree bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles [for host] ... (2 actions, 0 running)
[1,922 / 9,165] no action
[2,190 / 10,960] no action
INFO: From Compiling tensorflow/lite/toco/toco_graphviz_dump_options.cc:
cc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++
INFO: From Compiling external/flatbuffers/src/util.cpp:
cc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++
cc1plus: warning: unrecognized command line option ""-Wno-implicit-fallthrough""
[2,625 / 12,575] Compiling external/org_sqlite/sqlite3.c; 1s local ... (7 actions running)
INFO: From Compiling tensorflow/lite/util.cc:
cc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++
cc1plus: warning: unrecognized command line option ""-Wno-extern-c-compat""
INFO: From Compiling tensorflow/lite/c/c_api_internal.c:
In file included from tensorflow/lite/c/c_api_internal.c:16:0:
./tensorflow/lite/c/c_api_internal.h:60:34: warning: 'struct TfLiteContext' declared inside parameter list
   TfLiteStatus (*Refresh)(struct TfLiteContext* context);
                                  ^
./tensorflow/lite/c/c_api_internal.h:60:34: warning: its scope is only this definition or declaration, which is probably not what you want
INFO: From Compiling external/flatbuffers/src/code_generators.cpp:
cc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++
cc1plus: warning: unrecognized command line option ""-Wno-implicit-fallthrough""
INFO: From Compiling tensorflow/lite/simple_memory_arena.cc:
cc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++
cc1plus: warning: unrecognized command line option ""-Wno-extern-c-compat""
INFO: From Compiling external/flatbuffers/src/idl_gen_fbs.cpp:
cc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++
cc1plus: warning: unrecognized command line option ""-Wno-implicit-fallthrough""
INFO: From Compiling tensorflow/lite/minimal_logging_default.cc:
cc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++
cc1plus: warning: unrecognized command line option ""-Wno-extern-c-compat""
INFO: From Compiling tensorflow/lite/minimal_logging.cc:
cc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++
cc1plus: warning: unrecognized command line option ""-Wno-extern-c-compat""
INFO: From Compiling tensorflow/lite/kernels/internal/quantization_util.cc:
cc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++
INFO: From Compiling tensorflow/lite/string_util.cc:
cc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++
tensorflow/lite/string_util.cc: In member function 'int tflite::DynamicBuffer::WriteToBuffer(char**)':
tensorflow/lite/string_util.cc:89:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < offset_.size(); i++) {
                     ^
At global scope:
cc1plus: warning: unrecognized command line option ""-Wno-extern-c-compat""
INFO: From Compiling tensorflow/lite/arena_planner.cc:
cc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++
tensorflow/lite/arena_planner.cc: In member function 'virtual TfLiteStatus tflite::ArenaPlanner::PlanAllocations()':
tensorflow/lite/arena_planner.cc:156:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < graph_info_->num_nodes(); ++i) {
                     ^
tensorflow/lite/arena_planner.cc: In member function 'virtual TfLiteStatus tflite::ArenaPlanner::ExecuteAllocations(int, int)':
tensorflow/lite/arena_planner.cc:196:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < graph_info_->num_tensors(); ++i) {
                     ^
tensorflow/lite/arena_planner.cc: In member function 'TfLiteStatus tflite::ArenaPlanner::CalculateAllocationOfInternalTensors(int)':
tensorflow/lite/arena_planner.cc:287:18: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   if (node_index < graph_info_->num_nodes()) {
                  ^
tensorflow/lite/arena_planner.cc: In member function 'TfLiteStatus tflite::ArenaPlanner::CalculateDeallocationOfInternalTensors(int)':
tensorflow/lite/arena_planner.cc:300:18: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   if (node_index < graph_info_->num_nodes()) {
                  ^
At global scope:
cc1plus: warning: unrecognized command line option ""-Wno-extern-c-compat""
INFO: From Compiling tensorflow/lite/kernels/internal/mfcc_dct.cc:
cc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++
INFO: From Compiling tensorflow/lite/kernels/kernel_util.cc:
cc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++
INFO: From Compiling tensorflow/lite/kernels/internal/mfcc.cc:
cc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++
INFO: From Compiling tensorflow/lite/kernels/internal/reference/portable_tensor_utils.cc:
cc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++
INFO: From Compiling external/flatbuffers/src/idl_gen_text.cpp:
cc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++
cc1plus: warning: unrecognized command line option ""-Wno-implicit-fallthrough""
[2,657 / 12,575] Compiling external/org_sqlite/sqlite3.c; 4s local ... (5 actions running)
INFO: From Compiling external/flatbuffers/src/reflection.cpp:
cc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++
cc1plus: warning: unrecognized command line option ""-Wno-implicit-fallthrough""
INFO: From Compiling tensorflow/lite/kernels/internal/mfcc_mel_filterbank.cc:
cc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++
INFO: From Compiling tensorflow/core/grappler/costs/robust_stats.cc:
cc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++
INFO: From Compiling tensorflow/lite/kernels/internal/spectrogram.cc:
cc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++
INFO: From Compiling external/flatbuffers/src/idl_gen_general.cpp:
cc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++
cc1plus: warning: unrecognized command line option ""-Wno-implicit-fallthrough""
[2,928 / 12,575] Compiling external/org_sqlite/sqlite3.c; 8s local ... (2 actions running)
INFO: From Compiling external/jsoncpp_git/src/lib_json/json_value.cpp:
cc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++
INFO: From Compiling external/jsoncpp_git/src/lib_json/json_writer.cpp:
cc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++
INFO: From Compiling tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:
cc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++
INFO: From Compiling tensorflow/core/lib/db/snapfn.cc:
cc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++
ERROR: /workspace/tensorflow/lite/python/interpreter_wrapper/BUILD:9:1: C++ compilation of rule '//tensorflow/lite/python/interpreter_wrapper:numpy' failed (Exit 1): arm-linux-gnueabihf-gcc failed: error executing command 
  (cd /home/sohaib/tensorflow_pi_compile/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH='' \
    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python \
    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \
    TF_DOWNLOAD_CLANG=0 \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL_SYCL=0 \
    TF_NEED_ROCM=0 \
  /home/sohaib/tensorflow_pi_compile/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/arm_compiler/bin/arm-linux-gnueabihf-gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -DRASPBERRY_PI -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -isystem /usr/include/arm-linux-gnueabihf -isystem /usr/include/python3.4 -isystem /usr/include/ -MD -MF bazel-out/armeabi-opt/bin/tensorflow/lite/python/interpreter_wrapper/_objs/numpy/numpy.pic.d '-frandom-seed=bazel-out/armeabi-opt/bin/tensorflow/lite/python/interpreter_wrapper/_objs/numpy/numpy.pic.o' -fPIC -iquote . -iquote bazel-out/armeabi-opt/genfiles -iquote bazel-out/armeabi-opt/bin -iquote external/local_config_python -iquote bazel-out/armeabi-opt/genfiles/external/local_config_python -iquote bazel-out/armeabi-opt/bin/external/local_config_python -iquote external/bazel_tools -iquote bazel-out/armeabi-opt/genfiles/external/bazel_tools -iquote bazel-out/armeabi-opt/bin/external/bazel_tools -isystem external/local_config_python/numpy_include -isystem bazel-out/armeabi-opt/genfiles/external/local_config_python/numpy_include -isystem bazel-out/armeabi-opt/bin/external/local_config_python/numpy_include -isystem external/local_config_python/python_include -isystem bazel-out/armeabi-opt/genfiles/external/local_config_python/python_include -isystem bazel-out/armeabi-opt/bin/external/local_config_python/python_include '-march=armv7-a' '-mfpu=neon-vfpv4' '-std=gnu11' '-DS_IREAD=S_IRUSR' '-DS_IWRITE=S_IWUSR' -O3 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_1 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_2 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_8 -funsafe-math-optimizations -ftree-vectorize -fomit-frame-pointer -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -no-canonical-prefixes -fno-canonical-system-headers -c tensorflow/lite/python/interpreter_wrapper/numpy.cc -o bazel-out/armeabi-opt/bin/tensorflow/lite/python/interpreter_wrapper/_objs/numpy/numpy.pic.o)
Execution platform: @bazel_tools//platforms:host_platform
cc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++
In file included from /usr/include/python3.4/Python.h:8:0,
                 from ./tensorflow/lite/python/interpreter_wrapper/numpy.h:49,
                 from tensorflow/lite/python/interpreter_wrapper/numpy.cc:17:
/usr/include/python3.4/pyconfig.h:13:55: fatal error: arm-linux-gnueabihf/python3.4m/pyconfig.h: No such file or directory
 #  include <arm-linux-gnueabihf/python3.4m/pyconfig.h>
                                                       ^
compilation terminated.
[4,900 / 12,575] Compiling external/org_sqlite/sqlite3.c; 12s local ... (6 actions running)
INFO: Elapsed time: 35.627s, Critical Path: 12.95s
INFO: 42 processes: 42 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
```"
26945,ImportError: cannot import name 'calibration_pb2',"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):NO
- OS Platform and Distribution ""Debian GNU/Linux 9 (stretch)""
- TensorFlow installed from: source
- TensorFlow version: b'unknown' 1.10.0         
- Python version: 3.5.6
- Bazel version: 0.15.2
- GCC/Compiler version gcc (Debian 6.3.0-18+deb9u1) 6.3.0 20170516
- CUDA/cuDNN version: 10.0.130
- GPU model and memory:
NVIDIA-SMI 410.72       Driver Version: 410.72       CUDA Version: 10.0  
Tesla K80 Memory: 11441MiB 

This issue occours in trainig command.
`python legacy
/train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_inception_v2_pets.config`

**Error Code:**
`  File ""legacy/train.py"", line 51, in <module>
    from object_detection.builders import model_builder
  File ""/home/tensorflow1/models/research/object_detection/builders/model_builder.py"", line 27, in <mo
dule>
    from object_detection.builders import post_processing_builder
  File ""/home//tensorflow1/models/research/object_detection/builders/post_processing_builder.py"", line 
22, in <module>
    from object_detection.protos import post_processing_pb2
  File ""/home/tensorflow1/models/research/object_detection/protos/post_processing_pb2.py"", line 15, in
 <module>
    from object_detection.protos import calibration_pb2 as object__detection_dot_protos_dot_calibration__pb2
ImportError: cannot import name 'calibration_pb2'`

Thanks in advance!
"
26942,Add SetDefaultDevice equivalent in c_api,"TensorFlow version: 1.11, 1.12, 1.3
-Are you willing to contribute it (Yes/No): Yes

In bazel build for tensorflow_cc.lib, SetDefaultGraph does not work as it has previously. 

My recommendation would be to add an entry into the c_api so that one can assign a session to a specific GPU."
26941,[TF2.0] ref types in TF1 vs TF2,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Google Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No
- TensorFlow installed from (source or binary):Binary
- TensorFlow version (use command below):2.0.0-alpha0 and 1.13.0
- Python version:3.6

**TensorFlow 1.13.0**
```python
import tensorflow as tf
import numpy as np
print(tf.__version__)
x = np.random.randn(3, 4)
x_tf = tf.Variable(x)
print(x_tf.dtype)
```
```python
1.13.1
<dtype: 'float64_ref'>
```
**TensorFlow 2.0.0-alpha0**
```python
import tensorflow as tf
import numpy as np
print(tf.__version__)
x = np.random.randn(3, 4)
x_tf = tf.Variable(x)
print(x_tf.dtype)
```
```python
2.0.0-alpha0
<dtype: 'float64'>
```
While playing around with TensorFlow 2.0, I noticed that there is a difference in calling ```tf.Variable```.  Just wanted to know if ```_ref``` is being phased out in TF2.0?"
26940,Allocation of ram/memory while training 50 gb of data,"Hello,

I am training a model and my specs are 

Tesla K80 GPU  12 GB ram
61 gb ram physical ram

my training data set is of 50 gb

I want to know that how tensorflow use the memory for loading the data as it gives ""OOM"" error,
while loading images/dataset how it load whether loads all the in gpu and doesn't utilize the ram 
as while training it uses gpu so does it do half data on ram and half data on gpu, i want to know allocation of memory and how tensorflow does that.

And what if i want to use both gpu and cpu for tensorflow so it doesn't have memory issue so total i will have 61 + 12 gb of ram which can load dataset efficiently.

Can anyone give me exact details that how tensorflow does the computation and allocation of memory for dataset. 

Thanks"
26938,How to use tf.embedding_lookup_sparse_with_distributed_aggregation in feature_column&estimator ?,"Hi, I want to use `tf.embedding_lookup_sparse_with_distributed_aggregation`  in my training program. The model is built with tf.estimator and tf.feature_column . 

But the default is using `safe_embedding_lookup_sparse ` which will cost lots of network traffic.  


So what is the best way to use distributed_aggregation with feature_column ? 

 I think one way is to modify tf.feature_colmun source code. but is there any others ?"
26937,"Typo in the section ""3. Install the TensorFlow pip package""","<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version:
- Doc Link: https://www.tensorflow.org/install/pip


**Describe the documentation issue**
In the ""Install TensorFlow with pip"", section ""3. Install the TensorFlow pip package"", the sentence ""**tensorflow==2.0.0-alpha0-gpu** —Preview TF 2.0 Alpha build with GPU support (unstable, Ubuntu and Windows)"" is wrong.
It's should ""**tensorflow-gpu==2.0.0-alpha0**"" instead of ""**tensorflow==2.0.0-alpha0-gpu**""

Because I can't install with ""pip install --upgrade tensorflow==2.0.0-alpha0-gpu""

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
26935,how to edit kernel weight at certain global_step using estimator,i want to set some weights to zero at certation global step. 
26933,Can we have demo CNN application using ARM micro controller with Camera for object detection please?,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): Experimental Micro-controller version
- Are you willing to contribute it (Yes/No): Not now.



**Describe the feature and the current behavior/state.**

**Will this change the current api? How?**

**Who will benefit with this feature?**

**Any Other info.**
"
26932,tf lite java api model interpretation error,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Nexus 5 emulator, API version 23
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): ('v1.12.0-10426-g4b1ee1a7b1', '1.14.1-dev20190319')
- Python version: Python 2.7.15rc1
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
When loading a custom model using the TF lite api for android, the output tensor dimensions become malformed. This causes an exception:

```
java.lang.IllegalArgumentException: Cannot copy between a TensorFlowLite tensor with shape [0, 2] and a Java object with shape [1, 2].
        at org.tensorflow.lite.Tensor.throwExceptionIfTypeIsIncompatible(Tensor.java:242)
        at org.tensorflow.lite.Tensor.copyTo(Tensor.java:116)
        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:157)
        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:250)
        at org.tensorflow.lite.Interpreter.run(Interpreter.java:228)
        ...
```
It is a custom model, see attachment. Various validation scripts have shown no anomalies in the model.
I have spent some effort on debugging, validating the behavior of the API. However, I get stuck at the native methods.

The code below shows that before execution, the tensors do have the right shape.
```
        network = new Interpreter(modelFile, options);

        logger.info(String.format(""Input tensor count: %d"",network.getInputTensorCount()));
        for(int input_i = 0; input_i< network.getInputTensorCount(); input_i ++ ) {
            Tensor input = network.getInputTensor(input_i);
            logger.info(String.format("" Tensor %d:"", input_i));
            logger.info(String.format(""  Type %s"", toStringName(input.dataType())));
            logger.info(String.format(""  Dimensions %d"", input.numDimensions()));
            logger.info(String.format(""  Shape %s"", Arrays.toString(input.shape())));
        }

        logger.info(String.format(""Output tensor count: %d"",network.getOutputTensorCount()));
        for(int output_i = 0; output_i< network.getInputTensorCount(); output_i ++ ) {
            Tensor output = network.getOutputTensor(output_i);
            logger.info(String.format("" Tensor %d:"", output_i));
            logger.info(String.format(""  Type %s"", toStringName(output.dataType())));
            logger.info(String.format(""  Dimensions %d"", output.numDimensions()));
            logger.info(String.format(""  Shape %s"", Arrays.toString(output.shape())));
        }

        float[][][][] input = adapter.convert(_input);
        float[][] output = {{0f, 0f}};

        network.run(input, output);

```

It seems like in NativeInterpreterWrapper, the size of the output tensor is corrected after model execution, near the line:
```
    run(interpreterHandle, errorHandle);
    long inferenceDurationNanoseconds = System.nanoTime() - inferenceStartNanos;

    // Allocation can trigger dynamic resizing of output tensors, so refresh all output shapes.
    if (needsAllocation) {
      for (int i = 0; i < outputTensors.length; ++i) {
        if (outputTensors[i] != null) {
          outputTensors[i].refreshShape(); // <-- 
        }
      }
    }
```
It seems like this method checks the output dimensions with the c++ backend code, and corrects this in the Java part. At this point, the mismatch arrises.

However, if I perform the check before the model runs, the mismatch is already present.

**Describe the expected behavior**
No exception, no need to resize the tensor to a 0-dimensional array.

**Code to reproduce the issue**
[bugreport.zip](https://github.com/tensorflow/tensorflow/files/2987811/bugreport.zip)
Model: lite_model_v2.zip
Java: LiteEvaluator.java

Dependency:
Equal behavior exists for both
implementation 'org.tensorflow:tensorflow-lite:1.13.1'
and
implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'




**Other info / logs**

"
26929,Please add the window size parameter (and other options) to SSIM,"**System information**
- TensorFlow version (you are using): 1.13
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**
Currently, the SSIM in tf.image.ssim has many parameters fixed inside the script with no way to alter them. Most importantly, the window size parameter is fixed at 11x11. 

However, many people have found that changing the window size when using SSIM as a loss function changes the learning outputs. (See https://arxiv.org/pdf/1511.08861.pdf from the authors who first proposed the use of SSIM as a loss function)

It would be very useful if SSIM and its relatives (e.g. MS-SSIM) could have their parameters (such as window size and sigma) set as defaults but be made alterable in the user interface. This is already the case in the implementation on skimage (skimage.measure.compare_ssim). 


**Will this change the current api? How?**
Not much. If the window size and other parameters are set as defaults, instead of being fixed values, most people will never notice the difference. There would be no backward incompatible changes.


**Who will benefit with this feature?**
People who wish to use SSIM as a loss function.


**Any Other info.**
In the current implementation many variables are fixed in the script instead of being defined in the functions. 

Moreover, functions such as SSIM in the current Tensorflow image_ops implementation are heavily intertwined with many other functions in the page.

This makes it very difficult for outsiders to simply copy-paste the necessary parts out to change only the parts that they need. 

It would be great if the API changed to allow parameter tuning of key hyperparameters such as window size.

Many thanks in advance to those who look into this issue."
26928,Memory always exceeds 10% system memory,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): conda gpu version
- TensorFlow version (use command below): 1.10
- Python version: 3.5
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: 10
- GPU model and memory: Quadro GP100 16Gb

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

In the linked [colab] I try to train a WALSModel, with a sparse matrix with shape `(210000, 44000)`
When I try to run it locally on my Quadro I get: 

```
 Allocator (GPU_0_bfc) ran out of memory trying to allocate 17.32GiB.  Current allocation summary follows.

```

Which, ok, I have a 16GB memory card, that makes sense, I guess.

Since this is run on GPU it kills the process.

So I set my session config as follows to use the CPU which has 100Gb of Ram. Surely enough for the 17.32GiB that TF tried to allocate:

```python
# inside train_fn
config = tf.ConfigProto(device_count = {'GPU': 0})
with tf.Session(config=config) as sess:
    #...
```

and lo and behold:

```
Allocation of 18599850000 exceeds 10% of system memory.
```

I get that the sparse matrix is not exactly small, but it is not excessively large either. Storing the coordinate form on disk is <2 gb. so What is with the rampant memory usage?

Please help.

Full logs in last cell of colab

[colab]: https://colab.research.google.com/drive/1oJDfvIWnf7sY5uFcJ7XhTSEdA1n_3Jxv#scrollTo=8pqa0A7BUrG-


**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26927,Compile error for overlapping nets with Tf.Keras metrics,"**System information**
- Have I written custom code: yes
- OS Platform and Distribution: macOS 10.14
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below):  broken with 1.13.1, works with 1.12.0
- Python version: 3.6.3

**Describe the current behavior**
Compiling concatenated networks with Keras metrics causes an InvalidArgumentError in the input of the second network (You must feed a value for placeholder tensor 'dense_5_target' with dtype float and shape [?,?]). This used to work in 1.12.0 but broke with 1.13.1. Without metrics it's not an issue.

**Describe the expected behavior**
The networks should be trainable individually and in the concatenated version, no matter if we specify metrics or not.

**Code to reproduce the issue**
```
# coding: utf-8

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import Dense, Input

# works with 1.12, fails with 1.13.1
print(tf.__version__)

# Layer sizes of net 1 and 2
n_input_1 = 5
n_output_1 = 4

n_input_2 = 4
n_output_2 = 2

# Generating dummy data
N = 13
x1 = np.random.rand(N, n_input_1)
y1 = np.random.rand(N, n_output_1)

x2 = np.random.rand(N, n_output_1)
y2 = np.random.rand(N, n_output_2)


# Build net1, net2 and net_full
# net_full concats net1 and net2
def build_models():
    input_layer_1 = Input(shape=(n_input_1,))
    output_layer_1 = Dense(n_output_1)(input_layer_1)
    net1 = Model(inputs=input_layer_1, outputs=output_layer_1, name=""net1"")

    input_layer_2 = Input(shape=(n_input_2,), name = ""topmodel_input"")
    output_layer_2 = Dense(n_output_2)(input_layer_2)
    net2 = Model(inputs=input_layer_2, outputs=output_layer_2, name=""net2"")

    net_full = Model(inputs=input_layer_1, outputs=net2(net1.output))
    
    return net2, net_full

net2, net_full = build_models()
# compile with passing metrics only to net2 --> runs
net2.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])
net_full.compile(optimizer='rmsprop', loss='categorical_crossentropy')
net_full.fit(x1, y2)

net2, net_full = build_models()
# compile with passing metrics only to net_full --> runs
net2.compile(optimizer='rmsprop', loss='categorical_crossentropy')
net_full.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])
net_full.fit(x1, y2)

net2, net_full = build_models()
# compile with passing metrics to both net2 AND net_full --> crashes
net2.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])
net_full.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])
net_full.fit(x1, y2)
```

I think this is a bug. If you consider this improper use instead, please let me know.

Best, Boris"
26925,"""tensorflow/core/framework/attr_value.pb.h""","<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version:
- Doc Link:


**Describe the documentation issue**
in ""tensorflow/core/framework/function.h"" file inclued ""tensorflow/core/framework/attr_value.pb.h"" file, but i can not find the file. please tell me where is the dir(""tensorflow/core/framework/attr_value.pb.h""). thanks

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
26924,iOS example compile error,"iOS example app compile error because ""third_party/tensorflow/core/framework/types.h"" file not found. (Xcode 10.1)

I can fix this bug by modify 
`#include ""third_party/tensorflow/core/framework/types.h"" `
to
`#include ""tensorflow/core/framework/types.h"" `
(tensorflow/tensorflow/examples/ios/simple/ios_image_load.h # line20)

Many thanks!"
26922,AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'swapaxes',"
using keras API in tensorflow 2.0"
26918,r2.0-alpha compile on Ubuntu 18.04 has not installed tensorboard nor tf_upgrade_v2 properly,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): AWS Ubuntu 18.04 server
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source from Github
git clone https://github.com/tensorflow/tensorflow.git tfgit
Cloning into 'tfgit'...
remote: Enumerating objects: 2136, done.
remote: Counting objects: 100% (2136/2136), done.
remote: Compressing objects: 100% (985/985), done.
remote: Total 539589 (delta 1286), reused 1621 (delta 1145), pack-reused 537453
Receiving objects: 100% (539589/539589), 319.72 MiB | 39.79 MiB/s, done.
Resolving deltas: 100% (433795/433795), done.
Checking out files: 100% (16348/16348), done.

- TensorFlow version: r2.0-alpha
git checkout r2.0
Branch 'r2.0' set up to track remote branch 'r2.0' from 'origin'.
Switched to a new branch 'r2.0'
- Python version: 3.6.7
$ python3
Python 3.6.7 (default, Oct 22 2018, 11:32:17) 
[GCC 8.2.0] on linux

- Installed using virtualenv? pip? condo?:
Installed at /usr/lib/python3/dist-packages/tensorflow

- Bazel version (if compiling from source):
# wget https://github.com/bazelbuild/bazel/releases/download/0.23.0/bazel-0.23.0-installer-linux-x86_64.sh
# chmod 700 bazel-0.23.0-installer-linux-x86_64.sh
# ./bazel-0.23.0-installer-linux-x86_64.sh

- GCC/Compiler version (if compiling from source):
$ gcc --version
gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0


- CUDA/cuDNN version: Not supported
- GPU model and memory: Not supported



**Describe the problem**

tensor board is not properly installed.
$ tensorboard --logdir=./statsGraph
tensorboard: command not found

Also tf_upgrade_v2 script is not installed properly:

$ tf_upgrade_v2.py
tf_upgrade_v2.py: command not found

**Provide the exact sequence of commands / steps that you executed before running into the problem**

I can get tensorboard to run like this:

$ python3 -m tensorboard.main --logdir=./statsGraphs/
TensorBoard 1.14.0a20190301 at http://***.***.***.***:**** (Press CTRL+C to quit)


**Any other info / logs**
[uploadTF2BUILDlogs.txt](https://github.com/tensorflow/tensorflow/files/2986598/uploadTF2BUILDlogs.txt)

I cannot get tf_upgrade_v2.py script to work:

/usr/lib/python3/dist-packages/tensorflow/tools/compatibility# python3 -m tf_upgrade_v2.py --infile /srv/projects/t1/LSTMBlockCell4HLDropoutAdam.py --outfile /srv/projects/t1/LSTMBlockCell4HLDropoutAdam_v2.py
/usr/bin/python3: Error while finding module specification for 'tf_upgrade_v2.py' (AttributeError: module 'tf_upgrade_v2' has no attribute '__path__')


Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached."
26914,"NNAPI don't support dilateconv,but tflite support.How can I add this funtion in it?is there any way to add coustom op for nnapi",
26912,Can't load integrated model(two models cascade) with multiple inputs (Invalid input_shape argument error),"Hi~

I have made two models which have input with dimension (?, 1771) and output with dimension(?,161) each.

And two models are connected in cascade and new integrated model have 11 inputs with the same size.

When the training is finish and can't load the saved model with below message.

**ValueError: Invalid input_shape argument (None, 1771): model has 0 tensor inputs.**
----------------------------------------------------------------------------------------------
My code is below(for simplicity, only a few important parts)
----------------------------------------------------------------------------------------------
```
import keras
from keras.layers import Activation, Input, Dense, BatchNormalization
from keras.callbacks import ModelCheckpoint
import scipy.io as sio
import numpy as np
from keras.models import load_model,Model

first_model = load_model('first_model .hdf5')
first_model.name='first_model'

for ii in range(11):
    exec(""input_1_""+str(ii)+""=Input(shape=(1771,))"")
    exec('output_1_'+str(ii)+'=first_model(input_1_'+str(ii)+')')

concatenated = keras.layers.concatenate([output_1_0, output_1_1, output_1_2, output_1_3, output_1_4, output_1_5,
                                         output_1_6, output_1_7, output_1_8, output_1_9, output_1_10],name='concat')

second_model = load_model('speech(noisy_to_s).hdf5')
second_model.name='second_model'

x=second_model(concatenated) 

model = Model(inputs=[input_1_0, input_1_1, input_1_2, input_1_3, input_1_4, input_1_5, 
                       input_1_6, input_1_7, input_1_8, input_1_9, input_1_10],outputs=[x])

batch_size = 1024
Adam = keras.optimizers.Adam(lr=0.001)
model.compile(loss='mean_squared_error', optimizer=Adam, metrics=['accuracy'])
checkpointer = ModelCheckpoint(filepath='save.hdf5',
                               monitor='val_loss', verbose=1, save_best_only=True)
history=model.fit([x_train[0:-10,:],x_train[1:-9,:],x_train[2:-8,:],x_train[3:-7,:],x_train[4:-6,:],x_train[5:-5,:],
                   x_train[6:-4,:],x_train[7:-3,:],x_train[8:-2,:],x_train[9:-1,:],x_train[10:,:]],y_train[10:,:],
                  batch_size=batch_size, epochs=200,verbose=0,
                  validation_data=([x_valid[0:-10,:],x_valid[1:-9,:],x_valid[2:-8,:],x_valid[3:-7,:],x_valid[4:-6,:],
                                    x_valid[5:-5,:],x_valid[6:-4,:],x_valid[7:-3,:],x_valid[8:-2,:],x_valid[9:-1,:],
                                    x_valid[10:,:]],y_valid[10:,:]),
                  callbacks=[checkpointer])

model = load_model(checkpointer.filepath)
```"
26907,Specs,"Tensorflow: version 1.7.0 (CPU based installation)
Python: 3.6
OS: Windows 8.1
OS Name	Microsoft Windows 8.1
Version	6.3.9600 Build 9600
Other OS Description 	Not Available
System Model	XPS 8700
Processor	Intel(R) Core(TM) i7-4790 CPU @ 3.60GHz, 3601 Mhz, 4 Core(s), 8 Logical 
Installed Physical Memory (RAM)	24.0 GB
GPU: Nvidia GTX 1050Ti
IDE: Spyder 3.3.1


Aslo this operation

j=1

aa=self.sess.run(tf.map_fn(lambda x: (tf.tensordot(x[0],x[1],0)),elems=tf.convert_to_tensor(self.vt[:,j,:]), tf.convert_to_tensor(self.ht[:,j,:])),dtype=tf.float64))

is twice as slow compared  to:

[np.outer(self.vt[i,j,:],self.ht[i,j,:])*expF[i] for i in self.lRepls]


self.lRepls=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
np.shape(self.vt)=(16,10,784)
np.shape(self.ht)=(16,10,340)
"
26906,Multiple CheckpointSavers when using MonitoredTrainingSession,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS High Sierra
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1
- Python version: 2.7.14
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: None
- GPU model and memory: None


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

I'm currently using MonitoredTrainingSession and passing it the CheckpointSaverHook. It works fine for new models. When I want to restore a model, I give a path to the `checkpoint_dir` arg. However, this also inits the default CheckpointSaver, and it will double save checkpoints in the directory. This is evident if I change `checkpoint_basename` as I get duplicate checkpoints at each global step.

**Describe the expected behavior**

I would not expect the default CheckpointSaver to init, and instead use the params given by the CheckpointSaverHook. 

**Code to reproduce the issue**

```
...
    initializer_hook = _DatasetInitializerHook(init_train, self.train_tfrecords_glob,
                                               queue_name=self.tensorq_name)
    summary_hook = tf.train.SummarySaverHook(save_steps=50, save_secs=None, summary_op=summary_op)
    logging_hook =  tf.train.LoggingTensorHook(tensors={'step': global_step, 'loss': loss,
                                                        'accuracy': acc_op}, 
                                                         every_n_iter=logging_step)
    saver = tf.train.Saver(max_to_keep=2)
    saver_hook = tf.train.CheckpointSaverHook(checkpoint_dir=self.ckpt_dir, 
                                              saver=saver, save_steps=save_step,
                                              checkpoint_basename='latest_model')
    step_hook = tf.train.StepCounterHook(every_n_steps=logging_step)

    hooks = [summary_hook, logging_hook]

    # Checkpoint Saver - Check for new model
    if self.new_model:
      logging.info('Starting New Model...existing ckpts will be deleted')
      hooks.extend([saver_hook, step_hook])
      fl = [f for f in os.listdir(self.ckpt_dir)]
      for f in fl:
        os.remove(os.path.join(self.ckpt_dir, f))
      checkpoint_dir =  None

    else:
      checkpoint_dir = self.ckpt_dir
      hooks.append(saver_hook)
    
    # Start Training
    with tf.train.MonitoredTrainingSession(
        checkpoint_dir=checkpoint_dir,
        hooks=[initializer_hook],
        chief_only_hooks=hooks,
        log_step_count_steps=logging_step,
        config=tf.ConfigProto(allow_soft_placement=True)) as mon_sess:
        while not mon_sess.should_stop():
          mon_sess.run(training_op)

    logging.info(""Training Completed"")
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26904,Cannot add metric to Estimator with binary_classification_head,"I created custom Estimator that uses binary_classification_head. During training I can see only loss and global step in tensorboard.
I was trying to add metric using tf.contrib.estimator.add_metrics but it doesn't work (don't know why). 
I was also trying to add it using similar approach to this one but also with no luck:
https://stackoverflow.com/questions/50120073/tensorflow-metrics-with-custom-estimator
Could you please advise me how can I add some metrics to this? "
26903,Cardinality() of SkipDataset may be not right in some cases,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
The `Cardinality` function of `SkipDatasetOp` is implemented as below:
```C++
 int64 Cardinality() const override {
      int64 n = input_->Cardinality();
      if (n == kInfiniteCardinality || n == kUnknownCardinality) {
        return n;
      }
      return std::max(0LL, n - count_);
    }
```
Given `count_ = -1`  and `input_` is a `RangeDataset` with `start =0`, `end=10`, and `step=1`, then `n` will 10, and the output of `Cardinality()` will be `11`. However, there is no output elements in this case. Will it be more reasonable if the output of `Cardinality()` be `0` for this case?  
"
26902,tf_upgrade_v2 does not preserve file attributes and symbolic links,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):n/a
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):archlinux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:n/a
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below): tf2 preview nightly yesterday
- Python version:3.7
- Bazel version (if compiling from source):n/a
- GCC/Compiler version (if compiling from source):n/a
- CUDA/cuDNN version:n/a
- GPU model and memory:n/a

1. `tf_upgrade_v2` changes executable files to non-executable files. I expect executable files are still executable after the upgrade.

2. `tf_upgrade_v2` always changes symbolic links to regular files. However I expect:
(1)For in-place upgrade, modify the file the link points to, but the symbolic link should be the same.
(2) For non-in-place upgrade, if `--intree` and `--outtree` is used, symbolic links which point to files within the tree should become symbolic links pointing to the new file in the outtree. Symbolic links which point to external files should become a regular file.
(3) For non-in-place single-file upgrade, the output should be a regular file."
26901,tf.range fails on integer tensors,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 16.04.6 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.13.0-rc2-2-gbade323
- Python version: 3.5
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source): 5.4.0 20160609
- CUDA/cuDNN version: 10.0
- GPU model and memory: 1080 Ti

**Describe the current behavior**
```
>>> import tensorflow as tf
>>> tf.enable_eager_execution()
>>> tf.range(10, dtype=tf.float32)
<tf.Tensor: id=3, shape=(10,), dtype=float32, numpy=array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], dtype=float32)>
>>> tf.range(tf.convert_to_tensor(10), dtype=tf.float32)
[...]
ValueError: Tensor conversion requested dtype float32 for Tensor with dtype int32: 'tf.Tensor(10, shape=(), dtype=int32)'
```

**Describe the expected behavior**
`tf.range` should not raise ValueError when passing a Tensor argument instead of an integer literal."
26900,Converted .tflite file and it's output,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: 1.13.1


**Describe the documentation issue**

I am have a tf.keras sequential model which is a binary image classifier as below.

`import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense
from tensorflow.keras import backend as K
from tensorflow.keras.callbacks import TensorBoard, ReduceLROnPlateau
from sklearn.utils import class_weight
import numpy as np

img_width, img_height = 200, 200

train_data_dir = 'augmentedImg/200/training_data'#=============================================================================
validation_data_dir = 'augmentedImg/200/validation_data'#=============================================================================
nb_train_samples = 9009
nb_validation_samples = 2252
epochs = 100
batch_size = 32

layer_size = 64

if K.image_data_format() == 'channels_first':
    input_shape = (3, img_width, img_height)
else:
    input_shape = (img_width, img_height, 3)

model = Sequential()
model.add(Conv2D(layer_size, (3, 3), input_shape=input_shape))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(layer_size, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(layer_size, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(layer_size, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())

#model.add(Dropout(0.5))

model.add(Dense(1))
model.add(Activation('sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

train_datagen = ImageDataGenerator(
    rescale=1. / 255,
    rotation_range=90,
    width_shift_range=0.1,
    height_shift_range=0.1,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True
    )

test_datagen = ImageDataGenerator(rescale=1. / 255)

train_generator = train_datagen.flow_from_directory(
    train_data_dir,
    target_size=(img_width, img_height),
    color_mode='grayscale',
    batch_size=batch_size,
    class_mode='binary')

validation_generator = test_datagen.flow_from_directory(
    validation_data_dir,
    target_size=(img_width, img_height),
    color_mode='grayscale',
    batch_size=batch_size,
    class_mode='binary')

class_weights = class_weight.compute_class_weight(
               'balanced',
                np.unique(train_generator.classes), 
                train_generator.classes)

model.fit_generator(
    train_generator,
    class_weight=class_weights,
    steps_per_epoch=nb_train_samples // batch_size,
    epochs=epochs,
    validation_data=validation_generator,
    validation_steps=nb_validation_samples# // batch_size,
    #callbacks=[tensorboard, reduce_lr]
    )

model.save_weights('model.h5')
print(""End of program"")`

I then convert this into a .tflite file, which I am trying to incorporate into an android app.

When I run the tf.keras model (using model.predict on a new image), it returns a float of either 1 or 0 to signify the class.

However in the .tflite file it returns a number between 0 and 1 (for example 0.20582036)
 What does this number mean? I presume if its 0.20582036 then it's a prediction for class[0] and the confidence in its prediction?  

There seems to be very little documentation regarding the running of converted .tflite files.

Any help would be much appreciated"
26899,Error when importing tensorflow: DLL load failed,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.13.1
- Python version: 3.6
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: using CPU



**Describe the problem**

Originaly, when importing python module keras with tensorflow backend ar error ocurred when importing. To be sure, I simply imported tensorflow and same error was shown: 

> ImportError: DLL load failed: Specified module could not be found.
> 
> Failed to load the native TensorFlow runtime.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Open python in conda prompt or using spyder console:
`import tensorflow`

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

> import tensorflow
> Traceback (most recent call last):
> 
>   File ""<ipython-input-2-d6579f534729>"", line 1, in <module>
>     import tensorflow
> 
>   File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
>     from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
> 
>   File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
>     from tensorflow.python import pywrap_tensorflow
> 
>   File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
>     raise ImportError(msg)
> 
> ImportError: Traceback (most recent call last):
>   File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
>     from tensorflow.python.pywrap_tensorflow_internal import *
>   File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
>     _pywrap_tensorflow_internal = swig_import_helper()
>   File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
>   File ""C:\ProgramData\Anaconda3\lib\imp.py"", line 243, in load_module
>     return load_dynamic(name, filename, file)
>   File ""C:\ProgramData\Anaconda3\lib\imp.py"", line 343, in load_dynamic
>     return _load(spec)
> ImportError: DLL load failed: No se puede encontrar el módulo especificado.
> 
> 
> Failed to load the native TensorFlow runtime.
> 
> See https://www.tensorflow.org/install/errors
> 
> for some common reasons and solutions.  Include the entire stack trace
> above this error message when asking for help."
26893,Compilation failed: Compilation failure: Ran out of memory in memory space vmem. Please file a bug against XLA.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Google Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NO
- TensorFlow installed from (source or binary):Already installed on Colab 
- TensorFlow version (use command below):1.13.1
- Keras version: 2.2.4
- Python version:3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
I'm training my model on cloud TPU colab. My model has ~27M params.
The model is not compiling.
**Describe the expected behavior**
I've used others model using the same procedure and have trained successfully.
But facing problem with this model. Not compiling. 
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
def seresnext_model(input_shape):
  base_model = SEResNextImageNet(input_shape,include_top = False)
  x = base_model.output
  out1 = GlobalMaxPooling2D()(x)
  out2 = GlobalAveragePooling2D()(x)
  out3 = Flatten()(x)
  out = concatenate([out1,out2,out3])
  out = Dropout(0.3)(out)
  out = Dense(256,activation = 'relu')(out)
  out = Dropout(0.3)(out)
  X = Dense(1, activation='sigmoid', kernel_initializer='glorot_uniform', bias_initializer='zeros')(out)
  model =  Model(inputs=base_model.input, outputs=X)
  return model


tf.logging.set_verbosity(tf.logging.INFO)
tpu_model = tf.contrib.tpu.keras_to_tpu_model(
    seresnext_model,
    strategy=tf.contrib.tpu.TPUDistributionStrategy(
        tf.contrib.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])
    )
)
tpu_model.compile(optimizer=tf.train.AdamOptimizer(learning_rate = 3e-4), loss=tf.keras.losses.binary_crossentropy, metrics=['acc'])
filepath = '/content/model.h5'
#clr = CyclicLR(base_lr=2e-4, max_lr=0.006,
#                     step_size=1070.)
checkpoint = ModelCheckpoint(filepath,monitor='val_loss', verbose=1, 
                             save_best_only=True)
history = tpu_model.fit_generator(train_gen,steps_per_epoch = train_steps,validation_data = val_gen,validation_steps = val_steps,epochs = 60,callbacks=[checkpoint],
                                  )
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```
INFO:tensorflow:Querying Tensorflow master (grpc://10.14.174.10:8470) for TPU system metadata.
INFO:tensorflow:Found TPU system:
INFO:tensorflow:*** Num TPU Cores: 8
INFO:tensorflow:*** Num TPU Workers: 1
INFO:tensorflow:*** Num TPU Cores Per Worker: 8
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 2624331463547489192)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 8563543438446983684)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 16583072253587280753)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 3721588361095158121)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 6301764238571034056)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 12096784135351803975)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 8677796372356234775)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 18261783914960296885)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 15034483779723062214)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 660265193242712443)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 14213068833784483275)
WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.
Epoch 1/60
INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id_20'), TensorSpec(shape=(128, 96, 96, 3), dtype=tf.float32, name='input_1_30'), TensorSpec(shape=(128, 1), dtype=tf.float32, name='dense_33_target_50')]
INFO:tensorflow:Overriding default placeholder.
INFO:tensorflow:Remapping placeholder for input_1
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py:302: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
INFO:tensorflow:Started compiling
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-44-3196f76a2e41> in <module>()
     12 checkpoint = ModelCheckpoint(filepath,monitor='val_loss', verbose=1, 
     13                              save_best_only=True)
---> 14 history = tpu_model.fit_generator(train_gen,steps_per_epoch = train_steps,validation_data = val_gen,validation_steps = val_steps,epochs = 60,callbacks=[checkpoint],
     15                                   )

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1424         use_multiprocessing=use_multiprocessing,
   1425         shuffle=shuffle,
-> 1426         initial_epoch=initial_epoch)
   1427 
   1428   def evaluate_generator(self,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, **kwargs)
    189       progbar.on_batch_begin(step, batch_logs)
    190 
--> 191       batch_outs = batch_function(*batch_data)
    192       if not isinstance(batch_outs, list):
    193         batch_outs = [batch_outs]

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in train_on_batch(self, x, y, sample_weight, class_weight, reset_metrics)
   1189       else:
   1190         self._make_fit_function()
-> 1191         outputs = self._fit_function(ins)  # pylint: disable=not-callable
   1192 
   1193     if reset_metrics:

/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py in __call__(***failed resolving arguments***)
   1258     input_specs = infeed_instance.make_input_specs(input_tensors)
   1259     tpu_model_ops = self._tpu_model_ops_for_input_specs(input_specs,
-> 1260                                                         infeed_manager)
   1261     infeed_dict = infeed_instance.make_feed_dict(tpu_model_ops)
   1262 

/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py in _tpu_model_ops_for_input_specs(self, input_specs, infeed_manager)
   1169                                                  infeed_manager)
   1170       self._compilation_cache[shape_key] = new_tpu_model_ops
-> 1171       self._test_model_compiles(new_tpu_model_ops)
   1172 
   1173     return self._compilation_cache[shape_key]

/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py in _test_model_compiles(self, tpu_model_ops)
   1112     if proto.status_error_message:
   1113       raise RuntimeError('Compilation failed: {}'.format(
-> 1114           proto.status_error_message))
   1115 
   1116     end_time = time.time()

RuntimeError: Compilation failed: Compilation failure: Ran out of memory in memory space vmem. It should not be possible to run out of vmem - please file a bug against XLA.

Largest program allocations in vmem:

  XLA label: register allocator spill slots
  Allocation type: scoped

  XLA label: %fusion.276 = (f32[256]{0}, f32[256]{0}, f32[128,24,24,256]{3,0,2,1}) fusion(f32[128,24,24,256]{3,0,2,1}, f32[256]{0}, f32[128,24,24,8]{3,0,2,1}, f32[128,24,24,8]{3,0,2,1}, ...(+33)), kind=kLoop, calls=%fused_computation.272
  Allocation type: scoped

  XLA label: %fusion.276 = (f32[256]{0}, f32[256]{0}, f32[128,24,24,256]{3,0,2,1}) fusion(f32[128,24,24,256]{3,0,2,1}, f32[256]{0}, f32[128,24,24,8]{3,0,2,1}, f32[128,24,24,8]{3,0,2,1}, ...(+33)), kind=kLoop, calls=%fused_computation.272
  Allocation type: scoped

  XLA label: %fusion.276 = (f32[256]{0}, f32[256]{0}, f32[128,24,24,256]{3,0,2,1}) fusion(f32[128,24,24,256]{3,0,2,1}, f32[256]{0}, f32[128,24,24,8]{3,0,2,1}, f32[128,24,24,8]{3,0,2,1}, ...(+33)), kind=kLoop, calls=%fused_computation.272
  Allocation type: scoped

  XLA label: %fusion.276 = (f32[256]{0}, f32[256]{0}, f32[128,24,24,256]{3,0,2,1}) fusion(f32[128,24,24,256]{3,0,2,1}, f32[256]{0}, f32[128,24,24,8]{3,0,2,1}, f32[128,24,24,8]{3,0,2,1}, ...(+33)), kind=kLoop, calls=%fused_computation.272
  Allocation type: scoped
```
"
26889,"tf.keras.fit not working with my model, doesn't feed the output target to placeholder tensor","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I have custom code to build an image classifier network with the tf.keras API
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04 (Linux Mint)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): `'v1.13.1-0-g6612da8951' 1.13.1
`
- Python version: 3.6.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: `release 10.0, V10.0.130` cuDNN: 7.4.2
- GPU model and memory: GeForce GTX 1070


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

When I try to use `model.fit_generator` or `model.fit_on_batch`, I get the following error: 
```python
>>> for x,y in training_gen:
...     break
... 
>>> model.train_on_batch(x,y)
WARNING:tensorflow:From /home/hepcats/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/hepcats/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1188, in train_on_batch
    outputs = self.train_function(ins)  # pylint: disable=not-callable
  File ""/home/hepcats/.local/lib/python3.6/site-packages/tensorflow/python/keras/backend.py"", line 3076, in __call__
    run_metadata=self.run_metadata)
  File ""/home/hepcats/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1439, in __call__
    run_metadata_ptr)
  File ""/home/hepcats/.local/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 528, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'dense_1_target' with dtype float and shape [?,?]
  [[{{node dense_1_target}}]]
  [[{{node metrics/acc/Mean_2}}]]
```

Here's the details about the model: It's a custom classifier which uses [xception](https://arxiv.org/abs/1610.02357) as the feature detector and a custom feedforward network for a classifier.
```python
>>> model.summary()
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
xception (Model)             (None, 8, 8, 2048)        20861480  
_________________________________________________________________
Classifier (Sequential)      (None, 1)                 540929    
=================================================================
Total params: 21,402,409
Trainable params: 540,929
Non-trainable params: 20,861,480
_________________________________________________________________
>>> classifier.summary()
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense (Dense)                (None, 8, 8, 256)         524544    
_________________________________________________________________
flatten (Flatten)            (None, 16384)             0         
_________________________________________________________________
dropout (Dropout)            (None, 16384)             0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 16385     
=================================================================
Total params: 540,929
Trainable params: 540,929
Non-trainable params: 0
_________________________________________________________________
```

The two models are combined with the following code:
```python
ef build_full_model(feature_detector, classifier):
  model = tf.keras.models.Sequential()
  #First get the features from ptdnn
  model.add(feature_detector)
  # for layer in feature_detector.layers:
  #   layer.trainable=False
  #   model.add(layer)
  #Then classify
  model.add(classifier)  
  # for layer in classifier.layers:
  #   model.add(layer)
  #feature_detector should not be trainable if fine_tuning_layers==0
  feature_detector.trainable=False

  # Display layer information for reference
  print('[ASSEMBLE] Full Model Architecture:')
  model.summary()

  # classifier options including metrics and loss function
  classifier.compile(optimizer=tf.keras.optimizers.RMSprop(lr=2e-4),
                loss='binary_crossentropy',
                metrics=['acc',recall,f1])
  model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=2e-5),
                loss='binary_crossentropy',
                metrics=['acc',recall,f1])

  print('[ASSEMBLE] Models compiled successfully')
  return model
```
The issue seems to occur no matter what the input values are. It seems to me that my inputs x and y (coming from my generator) are the correct shape:
```python
>>> print(x.shape,y.shape)
(32, 256, 256, 3) (32,)
```
For the sake of completeness, the training data generator is set to read in images from a directory with some augmentation:
```python
# anonymous function for rotating 90 degrees randomly
  random_90 = lambda im: np.rot90(im,k=np.random.choice(4))
  #define the settings for loading in images including value rescale, 
  #  and random alterations such as scaling, zooming, and flipping
  augmented_gen = ImageDataGenerator(
    rescale=1./255,
    zoom_range=0.1,
    cval=0,
    horizontal_flip=True,
    vertical_flip = True,
    preprocessing_function=random_90,
    )
training_gen = augmented_gen.flow_from_directory(
    train_dir,
    target_size=(256, 256),
    batch_size=32,
    class_mode='binary'
    )
```


**Describe the expected behavior**

I expect the keras API to be able to train my model as is. Presumably this means that the placeholder Tensor should be populated correctly dense_1_target`  in ""/home/hepcats/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py""

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

This should be able to reproduce my issue:
```python

import tensorflow as tf
import numpy as np

# get xception
inshape = (256,256,3)
ptdnn = tf.keras.applications.Xception(
	weights='imagenet',
  	include_top=False,
  	input_shape=inshape)
# build classifier
classifier = tf.keras.models.Sequential(name='Classifier')
  # The classifier input is dense, or fully connected
classifier.add(tf.keras.layers.Dense(256, activation='relu',
  input_shape=feature_shape[1:]))
  # The flatten layer reduces the dimensions of the output
classifier.add(tf.keras.layers.Flatten())
  # Dropout layer prevents overfitting
classifier.add(tf.keras.layers.Dropout(0.5))
  # Output layer is a single neuron sigmoid
classifier.add(tf.keras.layers.Dense(1, activation='sigmoid'))

#Assemble composite model
model = tf.keras.models.Sequential()
  #First get the features from ptdnn
model.add(ptdnn)
  #Then classify
model.add(classifier)  
  #feature_detector should not be trainable if fine_tuning_layers==0
ptdnn.trainable=False

  # Display layer information for reference
print('[ASSEMBLE] Full Model Architecture:')
model.summary()

  # classifier options including metrics and loss function
classifier.compile(optimizer=tf.keras.optimizers.RMSprop(lr=2e-4),
                loss='binary_crossentropy',
                metrics=['acc'])
model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=2e-5),
                loss='binary_crossentropy',
                metrics=['acc'])

# try to fit on random data 
batch_size=1
model.train_on_batch(np.random.rand(batch_size,256,256,3),np.random.rand(batch_size,))

```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

My models use manually defined metrics for recall and F1 score:
```python
def recall(y_true, y_pred):
    """"""Recall metric.

    Only computes a batch-wise average of recall.

    Computes the recall, a metric for multi-label classification of
    how many relevant items are selected.
    """"""
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall

def precision(y_true, y_pred):
    """"""Precision metric.

    Only computes a batch-wise average of precision.

    Computes the precision, a metric for multi-label classification of
    how many selected items are relevant.
    """"""
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision

def f1(y_true, y_pred):
    """""" F1 metric.
    The F1 metric is the harmonic mean of precision and recall
    """"""
    p = precision(y_true, y_pred)
    r = recall(y_true, y_pred)
    return 2*((p*r)/(p+r+K.epsilon()))
```
"
26886,Restoring a saved model and evaluating on a new Tensorflow Data object,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): binary (for GPU)
- TensorFlow version: **GIT**: v1.9.0-0-g25c197e023, **TF**: 1.9.0
- Python version: Python 3.5.2
- CUDA/cuDNN version: 10.0, V10.0.130
- GPU model and memory: 4 X Nvidia GeForce GTX 1080 Ti (11GB)

I have this saved model and I want to restore it. After I restore, I want to evaluate it on a new dataset which I feeding with a Tensorflow Data input pipeline. For convenience I created an LSTM model using MNIST dataset and I am feeding data using reinitializable iterator of Tensorflow Data API.

```
#Ignore the warnings
import warnings
warnings.filterwarnings(""ignore"")

import sys
import pandas as pd
import tensorflow as tf
import numpy as np

import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = (8,7)
%matplotlib inline

old_v = tf.logging.get_verbosity()
tf.logging.set_verbosity(tf.logging.ERROR)
from tensorflow.examples.tutorials.mnist import input_data

#Data parameters
num_inputs = 28
num_classes = 2
num_steps=28

mnist = input_data.read_data_sets(""MNIST_data/"")

X_train = mnist.train.images[mnist.train.labels < 2].reshape((-1, num_steps, num_inputs))
y_train = mnist.train.labels[mnist.train.labels < 2]

X_val = mnist.validation.images[mnist.validation.labels < 2].reshape((-1, num_steps, num_inputs))
y_val = mnist.validation.labels[mnist.validation.labels < 2]

X_test = mnist.test.images[mnist.test.labels < 2].reshape([-1, num_steps, num_inputs])
y_test = mnist.test.labels[mnist.test.labels < 2]

tf.logging.set_verbosity(old_v)

print(X_train.shape)
#(11623, 28, 28)
print(y_train.shape)
#(11623,)
print(X_val.shape)
#(1042, 28, 28)
print(y_val.shape)
#(1042,)
print(X_test.shape)
#(2115, 28, 28)
print(y_test.shape)
#(2115,)

# CREATE THE COMPUTATIONAL GRAPH
initial_learning_rate=0.0001
num_neurons = 128
num_layers = 1

graph = tf.Graph()
with graph.as_default():
    
    features_placeholder = tf.placeholder(dtype=tf.float32, shape=[None, num_steps, num_inputs])
    labels_placeholder = tf.placeholder(tf.int32, shape=[None])

    batch_size = 128

    # create the training dataset
    Xtrain = tf.data.Dataset.from_tensor_slices(features_placeholder)
    # apply a one-hot transformation to each label for use in the neural network
    ytrain = tf.data.Dataset.from_tensor_slices(labels_placeholder).map(lambda z: tf.one_hot(z, 2))
    # zip the x and y training data together and batch and Prefetch data for faster consumption
    train_dataset = tf.data.Dataset.zip((Xtrain, ytrain)).batch(batch_size)

    # create the validation dataset
    Xval = tf.data.Dataset.from_tensor_slices(features_placeholder)
    # apply a one-hot transformation to each label for use in the neural network
    yval = tf.data.Dataset.from_tensor_slices(labels_placeholder).map(lambda z: tf.one_hot(z, 2))
    # zip the x and y validation data together and shuffle, batch etc.
    validation_dataset = tf.data.Dataset.zip((Xval, yval)).batch(batch_size)

    # create the testing dataset
    Xtest = tf.data.Dataset.from_tensor_slices(features_placeholder)
    # apply a one-hot transformation to each label for use in the neural network
    ytest = tf.data.Dataset.from_tensor_slices(labels_placeholder).map(lambda z: tf.one_hot(z, 2))
    # zip the x and y testing data together and shuffle, batch etc.
    testing_dataset = tf.data.Dataset.zip((Xtest, ytest)).batch(batch_size)

    iterator = tf.data.Iterator.from_structure(train_dataset.output_types,train_dataset.output_shapes)
    X, y = iterator.get_next()

    training_init_op = iterator.make_initializer(train_dataset)
    validation_init_op = iterator.make_initializer(validation_dataset)
    testing_init_op = iterator.make_initializer(testing_dataset)

    with tf.name_scope(""graph_inputs""):
        output_keep_prob = tf.placeholder_with_default(1.0, shape=(), name =""output_dropout"")

    def build_lstm_cell(num_neurons, output_keep_prob):
        """"""Returns a dropout-wrapped LSTM-cell.
        See https://stackoverflow.com/a/44882273/2628369 for why this local function is necessary.
        Returns:
        tf.contrib.rnn.DropoutWrapper: The dropout-wrapped LSTM cell.
        """"""
        initializer = tf.contrib.layers.xavier_initializer()
        lstm_cell = tf.contrib.rnn.LSTMCell(num_units=num_neurons, initializer=initializer, forget_bias=1.0, state_is_tuple=True, name='LSTM_cell')
        lstm_cell_drop = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=output_keep_prob)
        return lstm_cell_drop
    
    with tf.name_scope(""LSTM""):
        with tf.name_scope(""Cell""):
            #kernel = tf.get_variable(name = 'kernel_0',shape=[156, 512], initializer=tf.contrib.layers.xavier_initializer(), trainable=False)
            #bias = tf.get_variable(name='bias_0',shape=[512],initializer=tf.zeros_initializer(), trainable=False)
            multi_layer_cell = tf.contrib.rnn.MultiRNNCell([build_lstm_cell(num_neurons, output_keep_prob) for _ in range(num_layers)], state_is_tuple=True)
        with tf.name_scope(""Model""):
            outputs, states = tf.nn.dynamic_rnn(cell=multi_layer_cell, inputs=X, swap_memory=False, time_major = False, dtype=tf.float32)#[Batch_size, time_steps, num_neurons]
        with tf.name_scope(""Graph_Outputs""):
            outputs = tf.transpose(outputs, [1, 0, 2]) # [num_timesteps, batch_size, num_neurons]
            outputs = tf.gather(outputs, int(outputs.get_shape()[0]) - 1) # [batch_size, num_neurons]
        with tf.variable_scope('Softmax'):
            softmax_w = tf.get_variable(name=""softmax_w"", initializer=tf.truncated_normal(shape=[num_neurons, num_classes], stddev=np.sqrt(2.0 /num_neurons), dtype=tf.float32))
            softmax_b = tf.get_variable(name=""softmax_b"", initializer=tf.constant(value= 2.0 / num_neurons, shape=[num_classes], dtype=tf.float32))
            logits= tf.matmul(outputs, softmax_w) + softmax_b #[Batch_size X time_steps, num_classes]
        with tf.name_scope('Predictions'):
            predictions = tf.nn.softmax(logits, name=""predictions"")  #[Batch_size, num_classes]
        with tf.name_scope(""Accuracy""):
                accuracy, accuracy_update_op  = tf.metrics.accuracy(labels = tf.argmax(y,1), predictions = tf.argmax(predictions, axis = 1), name = 'accuracy')
                running_vars_accuracy = tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES, scope=""LSTM/Accuracy"")
        with tf.name_scope(""AUC""):
                auc, auc_update_op  = tf.metrics.auc(labels = tf.argmax(y,1), predictions = predictions[:,1], name = 'auc')
                running_vars_auc = tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES, scope=""LSTM/AUC"")
        with tf.name_scope('Loss'):
            xentropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y, name='xentropy')
            loss = tf.reduce_mean(xentropy, name=""loss"")
        with tf.name_scope('Train'):
            optimizer= tf.train.AdamOptimizer(learning_rate=initial_learning_rate)
            trainer=optimizer.minimize(loss, name=""training_op"")
        with tf.name_scope(""Saver""):
            saver = tf.train.Saver(var_list=tf.trainable_variables()) 
        with tf.name_scope(""init""):
            global_variables_init = tf.global_variables_initializer()
            running_vars_initializer_accuracy = tf.variables_initializer(var_list=running_vars_accuracy)
            running_vars_initializer_auc = tf.variables_initializer(var_list=running_vars_auc)

#EXECUTE THE COMPUTATIONAL GRAPH
#Network parameters
num_epochs = 20
max_checks_without_progress = 200
check_without_progress = 0
best_loss = np.infty
output_keep_var = 0.5

def get_model_params():
    gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)
    return {gvar.op.name: value for gvar, value in zip(gvars, tf.get_default_session().run(gvars))}

def restore_model_params(model_params):
    gvar_names = list(model_params.keys())
    assign_ops = {gvar_name: tf.get_default_graph().get_operation_by_name(gvar_name + ""/Assign"")
                  for gvar_name in gvar_names}
    init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}
    feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}
    tf.get_default_session().run(assign_ops, feed_dict=feed_dict)
    
    
with tf.Session(graph = graph) as sess:
    global_variables_init.run()
    final_model_path = ""./my_deep_model_MNIST.ckpt""
        
    print(""Initialized"")
    # Training cycle
    val_loss = []
    val_accuracy = []
    train_loss = []
    train_accuracy = []
    total_train_batch = int(X_train.shape[0]/ batch_size + 1)
    total_val_batch = int(X_val.shape[0]/ batch_size + 1)
        
    for epoch in range(0, num_epochs):
        avg_cost_train = 0.
        avg_accuracy_train =0
        avg_cost_val = 0.
        avg_accuracy_val =0
        
        
        running_vars_initializer_accuracy.run()
        running_vars_initializer_auc.run()
        sess.run(training_init_op, feed_dict={features_placeholder: X_train, labels_placeholder: y_train})
        # Loop over all batches
        for _ in range(total_train_batch):
            _, miniBatchCost_train, _, _ = sess.run([trainer, loss, accuracy_update_op, auc_update_op], feed_dict={output_keep_prob: output_keep_var})
            avg_cost_train += miniBatchCost_train / total_train_batch
        accuracy_train = sess.run(accuracy)
        AUC_train = sess.run(auc)
        train_loss.append(avg_cost_train)
        train_accuracy.append(avg_accuracy_train)
        
        running_vars_initializer_accuracy.run()
        running_vars_initializer_auc.run()
        sess.run(validation_init_op, feed_dict={features_placeholder: X_val, labels_placeholder: y_val})
        for _ in range(total_val_batch):
            miniBatchCost_val, _, _ = sess.run([loss, accuracy_update_op, auc_update_op])
            avg_cost_val += miniBatchCost_val / total_val_batch
        accuracy_val = sess.run(accuracy)
        AUC_val = sess.run(auc)
        val_loss.append(avg_cost_val)
        val_accuracy.append(avg_accuracy_val)
        
        if avg_cost_val < best_loss:
            save_path = saver.save(sess, final_model_path)
            best_params = get_model_params()
            best_loss = avg_cost_val
            check_without_progress = 0
            save_message = ""*""
        else:
            check_without_progress +=1
            save_message = """"
            if check_without_progress > max_checks_without_progress:
                print(""Stopping Early! Loss has not improved in {} epochs"".format(max_checks_without_progress))
                break
    
        print(""Epoch: {:d}-"".format(epoch), \
              ""Training Loss: {:.6f}, "".format(avg_cost_train), \
              ""Training Accuracy: {:>.2%}, "".format(accuracy_train), \
              ""Training AUC: {:>.2%},"".format(AUC_train), \
              ""Validation Loss: {:.6f}, "".format(avg_cost_val), \
              ""Validation Accuracy: {:>.2%},"".format(accuracy_val), \
              ""Validation AUC: {:>.2%},"".format(AUC_val),\
              save_message)
    print(""Optimization Finished!"")

    # If we used early stopping then rollback to the best model found
    if best_params:
        restore_model_params(best_params)
    
    total_test_batch = int(X_test.shape[0]/ batch_size + 1)
    running_vars_initializer_accuracy.run()
    running_vars_initializer_auc.run()
    sess.run(testing_init_op, feed_dict={features_placeholder: X_test, labels_placeholder: y_test})
    for _ in range(total_test_batch):
        sess.run([accuracy_update_op, auc_update_op])
    accuracy_test= sess.run(accuracy)
    auc_test= sess.run(auc)
    print(""Final test accuracy: {:>.2%}"".format(accuracy_test), ""Final test AUC: {:>6.1%}"".format(auc_test))
    
    plt.plot(train_loss, label='Train Loss')
    plt.plot(val_loss, label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Cost')
    plt.title(""Loss"")
    plt.legend()
    plt.show()
```

The model is saved. I try to restore model and use it to evaluate on ANOTHER dataset object (assuming test set of MNIST is a new dataset).

```
tf.reset_default_graph()
with tf.Session() as sess:
    new_saver = tf.train.import_meta_graph('my_deep_model_MNIST.ckpt.meta')
    new_saver.restore(sess, tf.train.latest_checkpoint('./'))
    print(""Restored Operations from MetaGraph"")
    g = tf.get_default_graph()
    
    predictions = g.get_tensor_by_name('LSTM/Predictions/predictions:0')
    
    accuracy_update_op = g.get_tensor_by_name('LSTM/Accuracy/accuracy/update_op:0')
    accuracy = g.get_tensor_by_name('LSTM/Accuracy/accuracy/value:0')
    
    auc_update_op = g.get_tensor_by_name('LSTM/AUC/auc/update_op:0')
    auc = g.get_tensor_by_name('LSTM/AUC/auc/value:0')
    
    ################ Let's start another dataset object################
    batch_size = 128
    Xtest = mnist.test.images[mnist.test.labels < 2].reshape([-1, num_steps, num_inputs])
    ytest = mnist.test.labels[mnist.test.labels < 2]

    featuresplaceholder = tf.placeholder(dtype=tf.float32, shape=[None, num_steps, num_inputs])
    labelsplaceholder = tf.placeholder(tf.int32, shape=[None])
    

    train = tf.data.Dataset.from_tensor_slices(featuresplaceholder)
    train = tf.data.Dataset.from_tensor_slices(labelsplaceholder).map(lambda z: tf.one_hot(z, 2))
    dataset = tf.data.Dataset.zip((train, train)).batch(batch_size)

    iterator = tf.data.Iterator.from_structure(dataset.output_types,dataset.output_shapes)
    X, y = iterator.get_next()
    dataset_init_op = iterator.make_initializer(dataset)
    ###############################################################
    
    total_test_batch = int(Xtest.shape[0]/ batch_size + 1)
    tf.local_variables_initializer().run()
    sess.run(dataset_init_op, feed_dict={featuresplaceholder: Xtest, labelsplaceholder: ytest})
    for _ in range(total_test_batch):
        sess.run([accuracy_update_op, auc_update_op])
    accuracy_test= sess.run(accuracy)
    auc_test= sess.run(auc)
    print(""Final test accuracy: {:>.2%}"".format(accuracy_test), ""Final test AUC: {:>6.1%}"".format(auc_test))
```

However, I am having an `FailedPreconditionError` error, even though I initialize the iterator using ` sess.run(dataset_init_op, feed_dict={featuresplaceholder: Xtest, labelsplaceholder: ytest})`. That is really strange because the same code works without using Tensorflow Data API and using only `feed_dict`. 

```
---------------------------------------------------------------------------
FailedPreconditionError                   Traceback (most recent call last)
/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1321     try:
-> 1322       return fn(*args)
   1323     except errors.OpError as e:

/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)
   1306       return self._call_tf_sessionrun(
-> 1307           options, feed_dict, fetch_list, target_list, run_metadata)
   1308 

/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)
   1408           self._session, options, feed_dict, fetch_list, target_list,
-> 1409           run_metadata)
   1410     else:

FailedPreconditionError: GetNext() failed because the iterator has not been initialized. Ensure that you have run the initializer operation for this iterator before getting the next element.
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,28,28], [?,2]], output_types=[DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Iterator)]]
	 [[Node: IteratorGetNext/_11 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_17_IteratorGetNext"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]

During handling of the above exception, another exception occurred:

FailedPreconditionError                   Traceback (most recent call last)
<ipython-input-2-20929b658f9d> in <module>()
     37     sess.run(dataset_init_op, feed_dict={featuresplaceholder: Xtest, labelsplaceholder: ytest})
     38     for _ in range(total_test_batch):
---> 39         sess.run([accuracy_update_op, auc_update_op])
     40     accuracy_test= sess.run(accuracy)
     41     auc_test= sess.run(auc)

/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    898     try:
    899       result = self._run(None, fetches, feed_dict, options_ptr,
--> 900                          run_metadata_ptr)
    901       if run_metadata:
    902         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1133     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1134       results = self._do_run(handle, final_targets, final_fetches,
-> 1135                              feed_dict_tensor, options, run_metadata)
   1136     else:
   1137       results = []

/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1314     if handle is None:
   1315       return self._do_call(_run_fn, feeds, fetches, targets, options,
-> 1316                            run_metadata)
   1317     else:
   1318       return self._do_call(_prun_fn, handle, feeds, fetches)

/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1333         except KeyError:
   1334           pass
-> 1335       raise type(e)(node_def, op, message)
   1336 
   1337   def _extend_graph(self):

FailedPreconditionError: GetNext() failed because the iterator has not been initialized. Ensure that you have run the initializer operation for this iterator before getting the next element.
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,28,28], [?,2]], output_types=[DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Iterator)]]
	 [[Node: IteratorGetNext/_11 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_17_IteratorGetNext"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]

Caused by op 'IteratorGetNext', defined at:
  File ""/usr/lib/python3.5/runpy.py"", line 184, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.5/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py"", line 16, in <module>
    app.launch_new_instance()
  File ""/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py"", line 486, in start
    self.io_loop.start()
  File ""/home/musara1/.local/lib/python3.5/site-packages/tornado/platform/asyncio.py"", line 148, in start
    self.asyncio_loop.run_forever()
  File ""/usr/lib/python3.5/asyncio/base_events.py"", line 345, in run_forever
    self._run_once()
  File ""/usr/lib/python3.5/asyncio/base_events.py"", line 1312, in _run_once
    handle._run()
  File ""/usr/lib/python3.5/asyncio/events.py"", line 125, in _run
    self._callback(*self._args)
  File ""/home/musara1/.local/lib/python3.5/site-packages/tornado/ioloop.py"", line 743, in _run_callback
    ret = callback()
  File ""/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py"", line 536, in <lambda>
    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))
  File ""/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py"", line 450, in _handle_events
    self._handle_recv()
  File ""/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py"", line 480, in _handle_recv
    self._run_callback(callback, msg)
  File ""/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py"", line 432, in _run_callback
    callback(*args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/zmq/eventloop/minitornado/stack_context.py"", line 275, in null_wrapper
    return fn(*args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py"", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py"", line 233, in dispatch_shell
    handler(stream, idents, msg)
  File ""/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py"", line 399, in execute_request
    user_expressions, allow_stdin)
  File ""/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py"", line 208, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py"", line 537, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py"", line 2662, in run_cell
    raw_cell, store_history, silent, shell_futures)
  File ""/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py"", line 2785, in _run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py"", line 2901, in run_ast_nodes
    if self.run_code(code, result):
  File ""/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py"", line 2961, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-2-20929b658f9d>"", line 3, in <module>
    new_saver = tf.train.import_meta_graph('my_deep_model_MNIST.ckpt.meta')
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py"", line 1960, in import_meta_graph
    **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/meta_graph.py"", line 744, in import_scoped_meta_graph
    producer_op_list=producer_op_list)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py"", line 432, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/importer.py"", line 442, in import_graph_def
    _ProcessNewOps(graph)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/importer.py"", line 234, in _ProcessNewOps
    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 3563, in _add_new_tf_operations
    for c_op in c_api_util.new_tf_operations(self)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 3563, in <listcomp>
    for c_op in c_api_util.new_tf_operations(self)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 3450, in _create_op_from_tf_operation
    ret = Operation(c_op, self)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 1740, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

FailedPreconditionError (see above for traceback): GetNext() failed because the iterator has not been initialized. Ensure that you have run the initializer operation for this iterator before getting the next element.
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,28,28], [?,2]], output_types=[DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Iterator)]]
	 [[Node: IteratorGetNext/_11 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_17_IteratorGetNext"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
```"
26882,ERROR: /home/user/.cache/bazel/_bazel_user/b4774fbdb8542988b4e302c9e073f145/external/com_google_absl/absl/types/BUILD.bazel:190:1: C++ compilation of rule '@com_google_absl//absl/types:bad_variant_access' failed (Exit 1) on benchmark_model tool build,"- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
```
lsb_release -a
	No LSB modules are available.
	Distributor ID:	Ubuntu
	Description:	Ubuntu 16.04.5 LTS
	Release:	16.04
	Codename:	xenial
```
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
Build from source
- TensorFlow version:
Fresh master
```
commit 3b3da345340a4ff5f4c587ee38e5a468f252aee1 (HEAD -> master, origin/master, origin/HEAD)
Author: Juhyun Lee <impjdi@google.com>
Date:   Tue Mar 19 05:36:06 2019 -0700

    Changed return type of run() from boolean to void.

    PiperOrigin-RevId: 239171696
```
- Python version:
```
python -c ""import sys;print(sys.version)""
3.5.2 (default, Nov 23 2017, 16:37:01)
[GCC 5.4.0 20160609]
```
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
```
bazel version
	WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
	INFO: Invocation ID: 1f934444-2ee8-4d1c-9a2c-534fdf65195d
	Build label: 0.22.0
	Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
	Build time: Mon Jan 28 12:58:08 2019 (1548680288)
	Build timestamp: 1548680288
	Build timestamp as int: 1548680288
```
- GCC/Compiler version (if compiling from source):
```
gcc -v
Using built-in specs.
COLLECT_GCC=gcc
COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/6/lto-wrapper
Target: x86_64-linux-gnu
Configured with: ../src/configure -v --with-pkgversion='Ubuntu 6.4.0-17ubuntu1~16.04' --with-bugurl=file:///usr/share/doc/gcc-6/README.Bugs --enable-languages=c,ada,c++,java,go,d,fortran,objc,obj-c++ --prefix=/usr --with-as=/usr/bin/x86_64-linux-gnu-as --with-ld=/usr/bin/x86_64-linux-gnu-ld --program-suffix=-6 --program-prefix=x86_64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-libmpx --enable-plugin --with-system-zlib --disable-browser-plugin --enable-java-awt=gtk --enable-gtk-cairo --with-java-home=/usr/lib/jvm/java-1.5.0-gcj-6-amd64/jre --enable-java-home --with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-6-amd64 --with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-6-amd64 --with-arch-directory=amd64 --with-ecj-jar=/usr/share/java/eclipse-ecj.jar --with-target-system-zlib --enable-objc-gc=auto --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu
Thread model: posix
gcc version 6.4.0 20180424 (Ubuntu 6.4.0-17ubuntu1~16.04)
```
- CUDA/cuDNN version:
```
nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2017 NVIDIA Corporation
Built on Fri_Sep__1_21:08:03_CDT_2017
Cuda compilation tools, release 9.0, V9.0.176
```
- GPU model and memory:
GeForce GTX TITAN X
GeForce GTX 1080 Ti

Command to reproduce:

```
git clone git@github.com:tensorflow/tensorflow.git
cd tensorflow
bazel build -c opt --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --config monolithic tensorflow/tools/benchmark:benchmark_model
```

Output:
```
	WARNING: /data/user/external_projects/tensorflow/tensorflow/core/BUILD:1794:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/cc/saved_model:loader.h' directly. You should either move the file to this package or depend on an appropriate rule there
	WARNING: /data/user/external_projects/tensorflow/tensorflow/core/BUILD:1794:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/distributed_runtime:server_lib.h' directly. You should either move the file to this package or depend on an appropriate rule there
	INFO: Analysed target //tensorflow/tools/benchmark:benchmark_model (72 packages loaded, 4810 targets configured).
	INFO: Found 1 target...
	ERROR: /home/user/.cache/bazel/_bazel_user/b4774fbdb8542988b4e302c9e073f145/external/com_google_absl/absl/types/BUILD.bazel:190:1: C++ compilation of rule '@com_google_absl//absl/types:bad_variant_access' failed (Exit 1)
	Target //tensorflow/tools/benchmark:benchmark_model failed to build
	Use --verbose_failures to see the command lines of failed build steps.
	INFO: Elapsed time: 6.057s, Critical Path: 0.06s
	INFO: 0 processes.
	FAILED: Build did NOT complete successfully
```

Looks like error changes every time I run command:

Run 2:
```
	...
	INFO: Analysed target //tensorflow/tools/benchmark:benchmark_model (0 packages loaded, 0 targets configured).
	INFO: Found 1 target...
	ERROR: /home/user/.cache/bazel/_bazel_user/b4774fbdb8542988b4e302c9e073f145/external/com_google_googletest/BUILD.bazel:57:1: C++ compilation of rule '@com_google_googletest//:gtest' failed (Exit 1)
	Target //tensorflow/tools/benchmark:benchmark_model failed to build
	Use --verbose_failures to see the command lines of failed build steps.
	INFO: Elapsed time: 0.304s, Critical Path: 0.05s
	INFO: 0 processes.
	FAILED: Build did NOT complete successfully
```

Run 3:
```
	...
	INFO: Analysed target //tensorflow/tools/benchmark:benchmark_model (0 packages loaded, 0 targets configured).
	INFO: Found 1 target...
	ERROR: /home/user/.cache/bazel/_bazel_user/b4774fbdb8542988b4e302c9e073f145/external/com_google_absl/absl/strings/BUILD.bazel:32:1: C++ compilation of rule '@com_google_absl//absl/strings:strings' failed (Exit 1)
	Target //tensorflow/tools/benchmark:benchmark_model failed to build
	Use --verbose_failures to see the command lines of failed build steps.
	INFO: Elapsed time: 0.236s, Critical Path: 0.02s
	INFO: 0 processes.
	FAILED: Build did NOT complete successfully
```
"
26881,ImportError: cannot import name 'audio' from 'tensorflow._api.v1',"Am new in Tensor flow,i was running my first example in tensor flow but found this issue.
How can i go about it    

""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))

ConverterError: TOCO failed. See console for info.
Traceback (most recent call last):
  File ""D:\Software\Scripts\toco_from_protos-script.py"", line 6, in <module>
    from tensorflow.lite.toco.python.toco_from_protos import main
  File ""D:\Software\lib\site-packages\tensorflow\__init__.py"", line 31, in <module>
    from tensorflow._api.v1 import audio
ImportError: cannot import name 'audio' from 'tensorflow._api.v1' (D:\Software\lib\site-packages\tensorflow\_api\v1\__init__.py)"
26879,TF 2.0: tf.stack can cause a segmentation fault,"**System information**
- Have I written custom code: yes
- OS Platform and Distribution: Linux Ubuntu 18.04
- Mobile device: N/A
- TensorFlow installed from: binary
- TensorFlow version: 2.0.0-alpha0, 2.0.0-dev20190319
- Python version: 3.6.7
- Bazel version: N/A
- GCC/Compiler version: N/A
- CUDA/cuDNN version: 10.1 / 7.4.2
- GPU model and memory: GeForce GTX 1080 Ti (11 GB)

**Describe the current behavior**
The Python interpreter crashes with SIGSEGV (Segmentation Fault); according to gdb the fault occurs in ```EagerTensor_CheckExact(_object const*) ()```.

**Describe the expected behavior**
No segmentation fault.
Ideally a stacked tensor returned (I was adapting code I developed interactively in eager execution mode, where it worked, in a Jupyter notebook for addition to a Keras based model), or an error that the argument cannot be a tensor (this is the TF1 behavior):
```
TypeError: Expected list for 'values' argument to 'pack' Op, not <tf.Tensor 'input_1:0' shape=(?, 128, 128, 1) dtype=float32>.
``` 

**Code to reproduce the issue**
```python
import tensorflow as tf
from tensorflow.keras.layers import Input

print(tf.__version__)

input_ = Input((128, 128, 1), dtype='float32')
print(input_)
output = tf.stack(input_, axis=1)
```"
26877,TF 2.0.0a0 doesn't find attributes after fresh installation,"**System information**
- OS Platform and Distribution: **Windows 10 Pro x64**
- TensorFlow installed from (source or binary): **binary (for GPU)** https://files.pythonhosted.org/packages/a3/a1/adff98d96de3d623f53f2e002d6e5b3936857c168a99efa632b7db89da05/tensorflow_gpu-2.0.0a0-cp37-cp37m-win_amd64.whl
- TensorFlow version: **2.0.0a0 for GPU**
- Python version: **3.7.2 64-bit (3.6.8 x64)**
- Installed using: **virtualenv & pip**
- CUDA/cuDNN version: **10.0 | 7.5.0 (7.4.2)**
- GPU model and memory: **Nvidia RTX 2080 Ti (11GB)**
- Others: **Visual Studio 2017 Community | Nvidia-Drivers 419.35**

<details>
  <summary>Package Versions</summary>
- absl-py 0.7.1
- astor 0.7.1
- gast 0.2.2
- google-pasta 0.1.4
- grpcio 1.19.0
- h5py 2.9.0
- Keras-Applications 1.0.7
- Keras-Preprocessing 1.0.9
- Markdown 3.0.1
- numpy 1.16.2
- pip 19.0.3
- protobuf 3.7.0
- setuptools 40.8.0
- six 1.12.0
- tb-nightly 1.14.0a20190301
- tensorflow-gpu 2.0.0a0
- termcolor 1.1.0
- tf-estimator-nightly 1.14.0.dev2019030115
- Werkzeug 0.14.1
- wheel 0.33.1
</details>

**Describe the problem**
Hi you guys! Thanks in advance!
I'm trying to install tensorflow-gpu 2.0.0a0 in an venv using pip as described at https://www.tensorflow.org/install/pip via _'pip install tensorflow-gpu==2.0.0-alpha0'_.
It works fine with TF 1.13.1 (GPU/CPU) but it doesn't for 2.0.0a0 (GPU/CPU)! The GPU and CPU version of 2.0.0a0 has the same error.

First I installed everything according to guides on a fresh system.
After installation I test it with the recommended cmd-line:

_python -c ""import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))""_

As for the error, what I get is:

_Traceback (most recent call last):
  File ""\<string\>"", line 1, in \<module\>
AttributeError: module 'tensorflow' has no attribute 'enable_eager_execution'_

I tried using python 3.6.8 and cuDNN 7.4.2. Also I tried using the newest versions of tf-estimator-nightly and tb-nightly. Tried it outside the venv. Tried it without _'tf.enable_eager_execution();'_ but then it has no attribute _'random_normal'_. Tried using .whl from other sites.

Since TF 1.13.1-gpu works and the error is so strangely unspecific, I have no idea what to try next. I saw issues with similar problems, but those solutions didn't work for me or the issue was similar but not this one. Please help!"
26873,Incorrect epoch number in TensorBoard callback when using batch-level metrics,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, see below
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): pip install tensorflow
- TensorFlow version (use command below): `tensorflow==1.13.1` and `tf-nightly==1.14.1.dev20190318` (`v1.12.0-10310-g3db23915df 1.14.1-dev20190318`)
- Python version: 3.7.2
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a


**Describe the current behavior**
When collecting batch-level metrics, epochs are incorrectly numbered with the current `_samples_seen`

**Describe the expected behavior**
When collecting batch-level metrics, epochs should be numbered with the epoch number

**Code to reproduce the issue**
```
from tensorflow import keras


class MyTensorBoard1(keras.callbacks.TensorBoard):
    def _write_custom_summaries(self, step, logs=None):
        print(logs, ""at step ="", step, ""with regular callback"")


class MyTensorBoard2(keras.callbacks.TensorBoard):
    def _write_custom_summaries(self, step, logs=None):
        print(logs, ""at step ="", step, ""with per-batch callback"")


layer = keras.layers.Input(shape=(1,))
model = keras.models.Model(inputs=layer, outputs=layer)
model.compile(optimizer='adam', loss='mse')
Callback1 = MyTensorBoard1()
Callback2 = MyTensorBoard2(update_freq='batch')
model.fit(x=list(range(6)), y=list(range(6)), epochs=10, callbacks=[Callback1, Callback2])
```

To run this code in current nightly, remove `Callback1` and focus on output of first epoch. Notice `at step = 6` in first epoch:

```
Epoch 1/10
{'batch_loss': 0.0} at step = 0 with per-batch callback
{'epoch_loss': 0.0} at step = 6 with per-batch callback
6/6 [==============================] - 0s 15ms/sample - loss: 0.0000e+00
```

**Other info / logs**
In my opinion, the bug is here:
https://github.com/tensorflow/tensorflow/blob/6612da89516247503f03ef76e974b51a434fb52e/tensorflow/python/keras/callbacks.py#L1158

and still here:
https://github.com/tensorflow/tensorflow/blob/c66b603990b9404dc1eb57de9d595aa0ffc8197f/tensorflow/python/keras/callbacks.py#L1288

In `on_epoch_end`, `step` should always be set to `epoch`."
26870,ImportError: DLL load failed: The specified module could not be found.  Failed to load the native TensorFlow runtime.,"System information:
Have I written custom code: No
OS Platform and Distribution: Windows 10 Home
Mobile device: None
TensorFlow installed from: binary with pip
TensorFlow version: 1.13.1
Python Version: 3.6.7
Bazel version: not installed
CUDA/cuDNN version: CUDA 10.1, cuDNN 7.5.0.56
GPU model and memory: GeForce GTX 1050 TCC/WDDM
NVIDIA Driver version: 419.35
TensorRT version: 5.0.4.3

Exact command to reproduce:
pip install --force-reinstall tensorflow-gpu
python
import tensorflow as tf

Problem:
I got this error when importing tensorflow. I have checked the environment variables. I have installed Visual Studio Community 2017 and NVIDIA CUDA Visual Studio Integration 10.1.
I have read and tried to follow the solutions from other similar issue (#22794 and #22872), but have not succeeded in fixing the problem.

Log:
C:\Users\Me>python
Python 3.6.7 (v3.6.7:6ec5cf24b7, Oct 20 2018, 13:35:33) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Program Files\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Program Files\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Program Files\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Program Files\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

Failed to load the native TensorFlow runtime."
26869,Converting to TF Lite with tf.lite.OpsSet.SELECT_TF_OPS is bounded,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 16.04
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
v1.13.1-0-g6612da8951
- Python version:
3.5.6 (Anaconda)
- CUDA/cuDNN version:
CPU version

I am trying to export a very simple model (see code below) into TensorFlow Lite with loss function [1] and gradients [2] calculation support using the official tutorial [3].

**Describe the current behavior**
The model can be converted only when global step variable and custom cross entropy implementation are used (but it cannot be run on Android). Without one of them or both the converting process crashes.

**Describe the expected behavior**
The model should be converted with standard cross entropy implementation and without explicitly defined global step variable.

**Code to reproduce the issue**
```python
import numpy as np
import tensorflow as tf


use_custom_crossentropy = True
use_global_step = True


inputs = np.array([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])
labels = np.array([[0.0], [1.0], [1.0], [0.0]])

global_step = tf.Variable(0, trainable=False, name='global_step')
x = tf.placeholder(tf.float32, shape=(None, 2), name='Input')
y = tf.placeholder(tf.float32, shape=(None, 1), name='Output')

with tf.name_scope('layer1'):
    w1 = tf.Variable(tf.random_uniform([2, 2], -1., 1.), name='w1')
    b1 = tf.Variable(tf.zeros([2]), name='b1')
    l1 = tf.add(tf.matmul(x, w1), b1, name='l1')
    l1 = tf.nn.sigmoid(l1)

with tf.name_scope('layer2'):
    w2 = tf.Variable(tf.random_uniform([2, 1], -1., 1.), name='w2')
    b2 = tf.Variable(tf.zeros([1]), name='b2')
    model = tf.add(tf.matmul(l1, w2), b2, name='model')

for variable in tf.trainable_variables():
    var_name = variable.name.split(':')[0].replace('/', '_')
    var_value_ph = tf.placeholder(tf.float32, shape=variable.shape, name='{}_ph'.format(var_name))
    var_value_assign = tf.assign(variable, var_value_ph, name='{}_assign'.format(var_name))
    print(var_name, var_value_ph.name, var_value_assign.name)

if use_custom_crossentropy:
    cost = tf.nn.sigmoid(model)
    cost = tf.maximum(cost, 0.0) - cost * y + tf.log(1 + tf.exp(-tf.abs(cost)))
    cost = tf.reduce_mean(cost, name='cost')
else:
    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=model),
                          name='cost')
optimizer = tf.train.AdamOptimizer(name='optimizer')
if use_global_step:
    train_op = optimizer.minimize(cost, global_step=global_step)
else:
    train_op = optimizer.minimize(cost)

init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)

for step in range(10000):
    sess.run(train_op, feed_dict={x: inputs, y: labels})
print(sess.run(model, feed_dict={x: inputs}))

converter = tf.lite.TFLiteConverter.from_session(sess,
                                                 [x, y], [model, cost, train_op])
converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,
                        tf.lite.OpsSet.SELECT_TF_OPS]
tflite_model = converter.convert()
with open('xor_old.tflite', 'wb') as f:
    f.write(tflite_model)
```

**Other info / logs**
```
use_custom_crossentropy = True
use_global_step = True
```
```
WARNING:tensorflow:From $HOME/miniconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
layer1_w1 layer1_w1_ph:0 layer1_w1_assign:0
layer1_b1 layer1_b1_ph:0 layer1_b1_assign:0
layer2_w2 layer2_w2_ph:0 layer2_w2_assign:0
layer2_b2 layer2_b2_ph:0 layer2_b2_assign:0
2019-03-19 12:09:59.552926: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-03-19 12:09:59.586599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2592000000 Hz
2019-03-19 12:09:59.587967: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5601b1c8d640 executing computations on platform Host. Devices:
2019-03-19 12:09:59.587988: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
[[-6.7735147]
 [-7.4516478]
 [ 4.8298645]
 [-6.928589 ]]
WARNING:tensorflow:From $HOME/miniconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/lite/python/lite.py:591: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.compat.v1.graph_util.convert_variables_to_constants
WARNING:tensorflow:From $HOME/miniconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.compat.v1.graph_util.extract_sub_graph
```
```
use_custom_crossentropy = False
use_global_step = True
```
```
WARNING:tensorflow:From $HOME/miniconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprec
ated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
layer1_w1 layer1_w1_ph:0 layer1_w1_assign:0
layer1_b1 layer1_b1_ph:0 layer1_b1_assign:0
layer2_w2 layer2_w2_ph:0 layer2_w2_assign:0
layer2_b2 layer2_b2_ph:0 layer2_b2_assign:0
2019-03-19 12:10:47.189917: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-03-19 12:10:47.214732: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2592000000 Hz
2019-03-19 12:10:47.215498: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x560ffe1fcd90 executing computations on platform Host. Devices:
2019-03-19 12:10:47.215538: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
[[-3.9300842]
 [ 3.913724 ]
 [ 3.9203634]
 [-3.8768034]]
WARNING:tensorflow:From $HOME/miniconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/lite/python/lite.py:591: convert_variables_to_constants (from tensorflow.python.framework.graph_util_i
mpl) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.compat.v1.graph_util.convert_variables_to_constants
WARNING:tensorflow:From $HOME/miniconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_uti
l_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.compat.v1.graph_util.extract_sub_graph
Traceback (most recent call last):
  File ""./training_old_version.py"", line 58, in <module>
    tflite_model = converter.convert()
  File ""$HOME/miniconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/lite/python/lite.py"", line 455, in convert
    **converter_kwargs)
  File ""$HOME/miniconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/lite/python/convert.py"", line 442, in toco_convert_impl
    input_data.SerializeToString())
  File ""$HOME/miniconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/lite/python/convert.py"", line 205, in toco_convert_protos
    ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.
2019-03-19 12:10:51.425153: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Log1p
2019-03-19 12:10:51.434088: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: BroadcastGradientArgs
2019-03-19 12:10:51.434177: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: BroadcastGradientArgs
2019-03-19 12:10:51.434246: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Reciprocal
2019-03-19 12:10:51.434268: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: BroadcastGradientArgs
2019-03-19 12:10:51.434696: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: BroadcastGradientArgs
2019-03-19 12:10:51.434748: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: SigmoidGrad
2019-03-19 12:10:51.434761: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: BroadcastGradientArgs
2019-03-19 12:10:51.434834: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ApplyAdam
2019-03-19 12:10:51.434848: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ApplyAdam
2019-03-19 12:10:51.434860: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ApplyAdam
2019-03-19 12:10:51.434871: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ApplyAdam
2019-03-19 12:10:51.434884: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Assign
2019-03-19 12:10:51.434902: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Assign
2019-03-19 12:10:51.434918: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: AssignAdd
2019-03-19 12:10:51.436899: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 110 operators, 148 arrays (0 quantized)
2019-03-19 12:10:51.437594: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 21 operators, 30 arrays (0 quantized)
2019-03-19 12:10:51.438536: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 21 operators, 30 arrays (0 quantized)
2019-03-19 12:10:51.438758: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 15 operators, 24 arrays (0 quantized)
2019-03-19 12:10:51.438853: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 15 operators, 24 arrays (0 quantized)
2019-03-19 12:10:51.438930: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 256 bytes, theoretical optimal value: 192 bytes.
2019-03-19 12:10:51.439212: W tensorflow/lite/toco/tflite/operator.cc:1768] Op Log1p is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.
2019-03-19 12:10:51.439273: W tensorflow/lite/toco/tflite/operator.cc:1768] Op Log1p is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.
2019-03-19 12:10:51.439319: E tensorflow/lite/toco/toco_tooling.cc:421] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of h
ow this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error w
ith --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, EXP, FULLY_CONNECTED, GREATER_EQUAL, LOGISTIC, M
EAN, MUL, NEG, SELECT, SUB, ZEROS_LIKE. Here is a list of operators for which you will need custom implementations: Log1p.
Traceback (most recent call last):
  File ""$HOME/miniconda3/envs/tf-cpu/bin/toco_from_protos"", line 11, in <module>
    sys.exit(main())
  File ""$HOME/miniconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 59, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""$HOME/miniconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""$HOME/miniconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 33, in execute
    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)
Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://
github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error w
ith --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, EXP, FULLY_CONNECTED, GREATER_EQUAL, LOGISTIC, M
EAN, MUL, NEG, SELECT, SUB, ZEROS_LIKE. Here is a list of operators for which you will need custom implementations: Log1p.
```
```
use_custom_crossentropy = True
use_global_step = False
```
```
WARNING:tensorflow:From $HOME/miniconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprec
ated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
layer1_w1 layer1_w1_ph:0 layer1_w1_assign:0
layer1_b1 layer1_b1_ph:0 layer1_b1_assign:0
layer2_w2 layer2_w2_ph:0 layer2_w2_assign:0
layer2_b2 layer2_b2_ph:0 layer2_b2_assign:0
2019-03-19 12:13:04.851757: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-03-19 12:13:04.874754: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2592000000 Hz
2019-03-19 12:13:04.875673: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55832b67a430 executing computations on platform Host. Devices:
2019-03-19 12:13:04.875706: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
[[-6.0365562]
 [-6.613834 ]
 [ 5.176443 ]
 [-6.16683  ]]
WARNING:tensorflow:From $HOME/miniconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/lite/python/lite.py:591: convert_variables_to_constants (from tensorflow.python.framework.graph_util_i
mpl) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.compat.v1.graph_util.convert_variables_to_constants
WARNING:tensorflow:From $HOME/miniconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_uti
l_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.compat.v1.graph_util.extract_sub_graph
Traceback (most recent call last):
  File ""./training_old_version.py"", line 58, in <module>
    tflite_model = converter.convert()
  File ""$HOME/miniconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/lite/python/lite.py"", line 455, in convert
    **converter_kwargs)
  File ""$HOME/miniconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/lite/python/convert.py"", line 442, in toco_convert_impl
    input_data.SerializeToString())
  File ""$HOME/miniconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/lite/python/convert.py"", line 205, in toco_convert_protos
    ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.
2019-03-19 12:13:09.381975: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: BroadcastGradientArgs
2019-03-19 12:13:09.391489: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: BroadcastGradientArgs
2019-03-19 12:13:09.391555: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Reciprocal
2019-03-19 12:13:09.391587: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: BroadcastGradientArgs
2019-03-19 12:13:09.391865: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: BroadcastGradientArgs
2019-03-19 12:13:09.391898: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: BroadcastGradientArgs
2019-03-19 12:13:09.391924: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Sign
2019-03-19 12:13:09.391942: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: SigmoidGrad
2019-03-19 12:13:09.391957: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: BroadcastGradientArgs
2019-03-19 12:13:09.391988: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: SigmoidGrad
2019-03-19 12:13:09.392002: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: BroadcastGradientArgs
2019-03-19 12:13:09.392080: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ApplyAdam
2019-03-19 12:13:09.392096: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ApplyAdam
2019-03-19 12:13:09.392108: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ApplyAdam
2019-03-19 12:13:09.392120: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ApplyAdam
2019-03-19 12:13:09.392134: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Assign
2019-03-19 12:13:09.392148: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Assign
2019-03-19 12:13:09.393103: F tensorflow/lite/toco/tooling_util.cc:905] Check failed: GetOpWithOutput(model, output_array) Specified output array ""optimizer"" is not produced by any op in this graph. Is it
 a typo? To silence this message, pass this flag:  allow_nonexistent_arrays
Aborted (core dumped)
```
Below presented a log from Android for the successfully converted model.
```
2019-03-19 11:24:38.739 32452-32452/com.example.tflite E/AndroidRuntime: FATAL EXCEPTION: main
    Process: com.example.tflite, PID: 32452
    java.lang.RuntimeException: Unable to start activity ComponentInfo{com.example.tflite/com.example.tflite.MainActivity}: java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: cannot compute AssignAdd as input #0(zero-based) was expected to be a int32_ref tensor but is a int32 tensor
    	 (while executing 'AssignAdd' via Eager)Node number 15 (DELEGATE) failed to invoke.
    
        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3086)
        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:3229)
        at android.app.servertransaction.LaunchActivityItem.execute(LaunchActivityItem.java:78)
        at android.app.servertransaction.TransactionExecutor.executeCallbacks(TransactionExecutor.java:108)
        at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:68)
        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1926)
        at android.os.Handler.dispatchMessage(Handler.java:106)
        at android.os.Looper.loop(Looper.java:213)
        at android.app.ActivityThread.main(ActivityThread.java:6981)
        at java.lang.reflect.Method.invoke(Native Method)
        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:493)
        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1445)
     Caused by: java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: cannot compute AssignAdd as input #0(zero-based) was expected to be a int32_ref tensor but is a int32 tensor
    	 (while executing 'AssignAdd' via Eager)Node number 15 (DELEGATE) failed to invoke.
    
        at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)
        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:149)
        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:275)
        at com.example.tflite.MainActivity.onCreate(MainActivity.java:53)
        at android.app.Activity.performCreate(Activity.java:7326)
        at android.app.Activity.performCreate(Activity.java:7317)
        at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1271)
        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3066)
        	... 11 more
```

[1] https://github.com/tensorflow/tensorflow/blob/v1.13.1/tensorflow/lite/toco/tflite/whitelisted_flex_ops.cc#L333-L334
[2] https://github.com/tensorflow/tensorflow/blob/v1.13.1/tensorflow/lite/toco/tflite/whitelisted_flex_ops.cc#L35
[3] https://www.tensorflow.org/lite/guide/ops_select#converting_the_model"
26868,What is the corresponding API for tf.nn.rnn_cell._linear in tf 2.0?,"Just as the title suggests, I want to adopt an older version code to tf2.0, thanks"
26867,transform_graph tools fails to produce output for a particular graph definition/also strange behavior of the tool,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
n/a
- TensorFlow installed from (source or binary):
source
- TensorFlow version (use command below):
v1.12.0-10232-g9a43dfe 1.13.1
- Python version:
3.7.1
- Bazel version (if compiling from source):
Build label: 0.19.2
Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Mon Nov 19 16:25:09 2018 (1542644709)
Build timestamp: 1542644709
Build timestamp as int: 1542644709
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
n/a
- GPU model and memory:
n/a


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
I was trying to use `transform_graph` tool to produce quantized version of a very simple graph. I noticed that in certain configurations of the graph(they are simple structurally) the tools complains about missing ops and so on. An example message:
```
2019-03-19 07:55:57.750401: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected MobilenetV1/conv1/batch_normalization_v1/beta to be preserved.
2019-03-19 07:55:57.750414: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected MobilenetV1/conv1/batch_normalization_v1/gamma to be preserved.
2019-03-19 07:55:57.750423: W tensorflow/tools/graph_transforms/transform_utils.cc:448] Generator function didn't preserve needed nodes, copying old replacements back in instead.
```
I tried to create a minimalistic reproducible example, where I started to notice more strange behavior. I will attach the python code for graph_def building below. For now this is the pb file I'm trying to work with:
[saved_model.pb.tar.gz](https://github.com/tensorflow/tensorflow/files/2982144/saved_model.pb.tar.gz)

In my setup the tools fails with the following error:
`2019-03-19 07:59:51.381714: E tensorflow/tools/graph_transforms/transform_utils.cc:577] Invalid input batch_normalization/gamma for node batch_normalization/FusedBatchNorm - name: ""batch_normalization/FusedBatchNorm""`

I call the the tool with the following configuration:
`$tbin --in_graph=$in_graph --out_graph=$out_graph_clean_quant --inputs=$inputs --outputs=$outputs --transforms='add_default_attributes strip_unused_nodes() remove_nodes(op=Identity, op=CheckNumerics)  quantize_weights quantize_nodes fold_batch_norms fold_old_batch_norms strip_unused_nodes sort_by_execution_order'`

The tool will not fail if we change the depthwise conv to a normal conv2d. FusedBatchNorm does not lose any inputs, it just works. 

Another observation: if we remove the following transformations `strip_unused_nodes() remove_nodes(op=Identity, op=CheckNumerics)` the tool will not fail.

Another observation. If we remove two aforementioned transformations, this is a piece of graph we get:
![image](https://user-images.githubusercontent.com/6204851/54589454-940e8f80-4a1d-11e9-946f-a24d220908e3.png)

For some reason two FusedBatchNorm nodes receive Beta\Gamma from the same nodes. I'm guessing this should not be the case. Furthermore, inside each of this nodes gamma\variance and beta\mean pairs receive values from the same reading ops. Note that there are only two additional inputs to FusedBatchNorm.
![image](https://user-images.githubusercontent.com/6204851/54589580-ecde2800-4a1d-11e9-9679-b58f3b963735.png)

Hopefully it all makes sense.

**Describe the expected behavior**
Tool should not fail when FusedBatchNorm is followed by DepthwiseConv2dNative.


**Code to reproduce the issue**
Here is some python self-contained code to reproduce the problem:

```
from tensorflow.contrib import keras
import tensorflow as tf
import os


def freeze_graph(model_dir, checkpoint_name, output_node_names):
    """"""Extract the sub graph defined by the output nodes and convert
    all its variables into constant

    Args:
        model_dir: the root folder containing the checkpoint state file
        output_node_names: a string, containing all the output node's names,
                            comma separated
    """"""
    if not tf.gfile.Exists(model_dir):
        raise AssertionError(
            ""Export directory doesn't exists. Please specify an export ""
            ""directory: %s"" % model_dir)

    if not output_node_names:
        print(""You need to supply the name of a node to --output_node_names."")
        return -1

    absolute_model_dir = os.path.split(checkpoint_name)[0]
    output_graph = os.path.join(absolute_model_dir, ""saved_model.pb"")

    clear_devices = True

    # We start a session using a temporary fresh Graph
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        saver = tf.train.Saver()
        saver.restore(sess, checkpoint_name)
        output_graph_def = tf.graph_util.convert_variables_to_constants(
            sess,  # The session is used to retrieve the weights
            tf.get_default_graph().as_graph_def(),  # The graph_def is used to retrieve the nodes
            output_node_names  # The output node names are used to select the usefull nodes
        )
        with tf.gfile.GFile(output_graph, ""wb"") as f:
            f.write(output_graph_def.SerializeToString())

        with tf.gfile.GFile(output_graph, ""rb"") as f:
            output_graph_def.ParseFromString(f.read())
        print(""%d ops in the final graph."" % len(output_graph_def.node))


save_folder = os.path.join(os.path.expanduser('~'), 'tmp')
os.makedirs(save_folder, exist_ok=True)

initializer = tf.contrib.layers.xavier_initializer_conv2d()
wdw = tf.get_variable(shape=(3, 3, 32, 1), initializer=initializer, name=""weights"")
wc = tf.get_variable(shape=(1, 1, 32, 1), initializer=initializer, name=""weights1"")

conv_out = tf.placeholder(shape=(1, 300, 300, 3), dtype=tf.float32)
conv_out = tf.nn.conv2d(conv_out, tf.get_variable(shape=(1, 1, 3, 32), initializer=initializer,name='w1'),
                        strides=[1, 1, 1, 1],
                        padding='SAME')
conv_out = keras.layers.BatchNormalization()(conv_out, training=False)

conv_out = tf.nn.depthwise_conv2d(conv_out, wdw, strides=[1, 1, 1, 1], padding='SAME')
# conv_out = tf.nn.conv2d(conv_out, tf.get_variable(shape=(1, 1, 32, 32), initializer=initializer, name=""weights32""), strides=[1, 1, 1, 1], padding='SAME')
conv_out = keras.layers.BatchNormalization()(conv_out, training=False)
conv_out = tf.nn.relu(conv_out)
#
conv_out = tf.nn.conv2d(conv_out, wc, strides=[1, 1, 1, 1], padding='SAME')
conv_out = keras.layers.BatchNormalization()(conv_out, training=False)
conv_out = tf.nn.relu(conv_out)

saver = tf.train.Saver()
checkpoint_name = os.path.join(save_folder, 'model.chkpt')
with tf.Session() as s:
    s.run(tf.global_variables_initializer())

    save_path = saver.save(s, checkpoint_name)

freeze_graph(save_folder, checkpoint_name, [conv_out.op.name])
print(conv_out.op.name)
```

**Other info / logs**
Please, do let me know if I missed something. 
Also, I might be misunderstanding something - please let me know if this is the case"
26866,Tensorflow Hardware Requirements,Does the tensorflow library require a CPU with AVX support? It's not mentioned anywhere in the documentation. I've tried import tensorflow on several machines and it only fails in the machine without AVX support.
26864,[TF2] tf.saved_model.save does not support type annotations,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): pip wheel
- TensorFlow version (use command below): `tf-nightly-2.0-preview==2.0.0.dev20190318`
- Python version: 3.7

**Describe the current behavior**
Given the following code:

```python
import tensorflow as tf

class A(tf.Module):
    @tf.function
    def func(self, x: int):
        pass


a = A() 
tf.saved_model.save(a, export_dir=""."")
```

I get the following traceback:

```python
Traceback (most recent call last):
  File ""scratch.py"", line 10, in <module>
    tf.saved_model.save(a, export_dir=""."")
  File ""/opt/anaconda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py"", line 812, in save
    saveable_view, asset_info.asset_index)
  File ""/opt/anaconda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py"", line 561, in _serialize_object_graph
    _write_object_proto(obj, obj_proto, asset_file_def_index)
  File ""/opt/anaconda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py"", line 577, in _write_object_proto
    function_serialization.serialize_function(obj))
  File ""/opt/anaconda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/saved_model/function_serialization.py"", line 81, in serialize_function
    function_spec_proto = _serialize_function_spec(function.function_spec, coder)
  File ""/opt/anaconda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/saved_model/function_serialization.py"", line 29, in _serialize_function_spec
    proto.fullargspec.CopyFrom(coder.encode_structure(function_spec.fullargspec))
  File ""/opt/anaconda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/saved_model/nested_structure_coder.py"", line 85, in encode_structure
    return self._map_structure(nested_structure, self._get_encoders())
  File ""/opt/anaconda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/saved_model/nested_structure_coder.py"", line 69, in _map_structure
    return do(pyobj, recursion_fn)
  File ""/opt/anaconda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/saved_model/nested_structure_coder.py"", line 224, in do_encode
    pair.value.CopyFrom(encode_fn(named_tuple_value._asdict()[key]))
  File ""/opt/anaconda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/saved_model/nested_structure_coder.py"", line 69, in _map_structure
    return do(pyobj, recursion_fn)
  File ""/opt/anaconda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/saved_model/nested_structure_coder.py"", line 193, in do_encode
    encoded_dict.dict_value.fields[key].CopyFrom(encode_fn(value))
  File ""/opt/anaconda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/saved_model/nested_structure_coder.py"", line 71, in _map_structure
    ""No encoder for object [%s] of type [%s]."" % (str(pyobj), type(pyobj)))
tensorflow.python.saved_model.nested_structure_coder.NotEncodableError: No encoder for object [<class 'int'>] of type [<class 'type'>].
```

**Describe the expected behavior**

Ideally, saving this would work just fine and not crash on the type annotation."
26854,Tensorflow 2.0: where is tf.contrib.layers.layer_norm?,Cannot find tf.contrib.layers.layer_norm in TF 2.0
26853,Process finished with exit code -1073741819 (0xC0000005),"This is my simple coding:-

<em>import tensorflow as tf
print(""Hi"");</em>

**output**
Process finished with exit code -1073741819 (0xC0000005)

The ""Hi"" is not printed. I searched online for this issue, but seems like none of the people has this, so it has been bugging me for few days.

No GPU involved, just the CPU version of tensorflow.

I tried v1.13, v1.12, v1.11, and v.1.10, all gave the me the same issue.
I tried with python 3.5.*, 3.6.*, and 3.7.*, also same result.
Tried with LiClipse, PyCharm, Atom, all gave the same result as well.

Windows 10 x64.

It works before, but suddenly one day it didn't work anymore, so I have no idea what happen.
Tried to uninstall any apps that installed within few weeks, but didn't solve the issue.

I really wish someone can help me to solve this issue."
26852,For Python what happens to tf.contrib.data.AUTOTUNE in 2.0 as contrib is not there in 2.0 and what do we use instead ?,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
26851,[TF 2.0] tf.distribute.cluster_resolver.TPUClusterResolver() fails on Colab,"Using latest `tf-nightly-gpu-2.0-preview` as of today.

Consider the following code:

```python
import tensorflow as tf
resolver = tf.distribute.cluster_resolver.TPUClusterResolver()
tf.tpu.experimental.initialize_tpu_system(resolver)
tpu_strategy = tf.distribute.experimental.TPUStrategy(resolver)
```

It fails on Colab:

```
<ipython-input-4-8454559c747f> in <module>()
----> 1 tf.distribute.cluster_resolver.TPUClusterResolver()

AttributeError: module 'tensorflow._api.v1.distribute' has no attribute 'cluster_resolver'
```

This code comes from the [TF 2.0 documentation](https://www.tensorflow.org/alpha/guide/distribute_strategy#tpustrategy)."
26846,Cannot feed single element in ELMo TF-HUB embedder,"Hi,
I am working in building a Deep Neural Network Classifier starting from an ELMo embedding module that embeds string, using Tensorflow Hub Module. I am using this model definition:

```
url = ""https://tfhub.dev/google/elmo/2""
embed = hub.Module(url, trainable=True)

def make_elmo_embedding(x):
    embeddings = embed(tf.squeeze(tf.cast(x, tf.string)), signature=""default"", as_dict=True)[""elmo""]
    return embeddings

# elmo embedding dimension
elmo_dim = 1024

# Input Layers
elmo_input = Input(shape=(None, ), dtype=""string"")

# Hidden Layers
elmo_embedding = Lambda(make_elmo_embedding, output_shape=(None, elmo_dim))(elmo_input)


x = Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2))(elmo_embedding)
x = Dense(32, activation='relu')(x)
predict = Dense(2, activation='sigmoid')(x)
model = Model(inputs=[elmo_input], outputs=predict)
model.compile(loss='mse', optimizer='sgd')
```
This code works ok if the input of my network is a list with more than two strings, for example ['hello' , 'my name is Simone']. 
If my input is a list with only one element (['hello']), this error appears: 

```
tensorflow.python.framework.errors_impl.InvalidArgumentError:
 input must be a vector, got shape: [] .   [[{{node lambda_1/module_apply_default/StringSplit}}]]
```
This error is reproducible both in training mode ( batch_size = 1) and inference mode ( if I want to make inference on 1 element for example). 

Looking at the error ```node lambda_1/module_apply_default/StringSplit}``` I think that the problem is in the Lambda layer, for the fact that in function ```make_elmo_embedding``` the tf.squeeze operation cancels the dimension = 1 and the input vector results of dimension = []. I tried to delete ```tf.squeeze``` but other errors occured. 
What you suggest to solve this situation in order to make this model definition compatible both for single and multiple example lists? 

Thanks
I am using Tensorflow 1.13.1


"
26844,Tensorflow 2.0: please do not deprecate important functions!!!,"According to Tensorflow 2.0, many important functions are deprecated, such as:
tf.layers.dense()
tf.layers.dropout()
tf.layers.flatten()
tf.layers.batch_normalization()
...
Since these functions are very widely used, and also very helpful for building complicted models, would you please keep them in the future versions?"
26843,SVD segfaults when given a 0x0 matrix,"**System information**
- Have I written custom code: Yes.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 on WSL, also Colab
- TensorFlow installed from (source or binary): binary (pip, no GPU)
- TensorFlow version (use command below): b'v1.13.1-0-g6612da8951' 1.13.1
- Python version: 3.6.8 (anaconda)

**Describe the current behavior**
Running `tf.svd([[]])` leads to a segfault.

**Describe the expected behavior**
This should probably raise an exception. Numpy returns something meaningless in this case, so is not a very good role model!

**Code to reproduce the issue**
```python
import tensorflow as tf
tf.enable_v2_behavior()
tf.svd([[]])
```
"
26842,SVD handles small singular values poorly,"**System information**
- Have I written custom code: Yes.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 on WSL, also Colab
- TensorFlow installed from (source or binary): binary (pip, no GPU)
- TensorFlow version (use command below): b'v1.13.1-0-g6612da8951' 1.13.1
- Python version: 3.6.8 (anaconda)

**The problem**
Performing an SVD of [this float32 matrix](https://github.com/tensorflow/tensorflow/files/2979211/matrix.zip) using `tf.svd()` (on CPU) results in `NaN` values in the `u` and `v` factors (no `NaN`s in the singular values). Numpy's (MKL's) SVD copes with this matrix fine, and returns many more nonzero singular values than the TF implementation. TF's SVD seems to zero singular values below a threshold. This is fine, assuming these values would have otherwise been inaccurate. However, the NaN's occur in the singular vectors corresponding to the *nonzero* singular values. Note also that not all the singular values are in descending order! This seems like a bug.

**Describe the expected behavior**
TensorFlow's SVD should be more robust to poorly-conditioned matrices.

**Code to reproduce the issue**
```python
import numpy as np
import tensorflow as tf
tf.enable_v2_behavior()
M = np.load(""matrix.npy"")
s_tf, u, v = tf.svd(M)
print(tf.norm(u).numpy(), tf.norm(v).numpy())  # prints ""nan nan""

u, s_np, vh = np.linalg.svd(M)
print(np.linalg.norm(u), np.linalg.norm(vh))  # prints some numbers

print(s_np)
print(s_tf.numpy())  # smaller singular values are zeroed
```

I think this is essentially the same as #8905, except that here I am using single precision. The ability of the SVD to handle ill-conditioned matrices is important for our application.
"
26841,Terribly slow conditional updates,"I'm trying to implement conditional  tensor updates such as:


#######################################################
Given i:

tf.scatter_update(self.vOut,i,[tf.cond( tf.less(tf.cast(tf.random_uniform([1])[0],tf.float64),tf.minimum(tf.cast(1.,tf.float64),  tf.exp(-self.dUk(vv,hh,vIn,hIn,i,j,krepl)))),
                 lambda:tf.slice(vv,[j,0],[1,self.n_v])[0],lambda:tf.slice(vIn,[i,j,0],[1,1,self.n_v])[0][0]) for j in kk])


 def U(self,x,y,x1,y1):
            return tf.multiply(tf.cast(self.P1,tf.float64),tf.exp(-((tf.norm(x-y)+tf.norm(x1-y1))/self.P2)**2.))

  def dU(self,x,y,v,z,x1,y1,v1,z1):    
          return    tf.subtract(self.U(x,y,x1,y1),self.U(v,z,v1,z1))

 def dUk(self,vv,hh,vIn,hIn,i,j,krepl):    
          return    tf.reduce_sum([tf.cond(tf.not_equal(p,i),
             lambda: self.dU(tf.slice(vv,[j,0],[1,self.n_v])[0],tf.slice(vIn,[p,j,0],[1,1,self.n_v])[0][0], 
             tf.slice(vIn,[i,j,0],[1,1,self.n_v])[0][0],tf.slice(vIn,[p,j,0],[1,1,self.n_v])[0][0],
             tf.slice(hh,[j,0],[1,self.n_h])[0],tf.slice(hIn,[p,j,0],[1,1,self.n_h])[0][0],
             tf.slice(hIn,[i,j,0],[1,1,self.n_h])[0][0],tf.slice(hIn,[p,j,0],[1,1,self.n_h])[0][0]
             ),
             lambda: tf.cast(0., tf.float64)) for p in  krepl])
 #############################################################

The problem i'm encountering is being  terribly slow compared to just numpy computation, i.e.

pure dUk computation:
dUk time (TF): 2.5306854248046875
dUk time (Numpy) time: 0.0

Update computation:
Time for Scatter (TF): 12.936962366104126
Time for Scatter (Numpy): 0.015621185302734375

Everything is done on CPU.

Is it normal to be that slow ?"
26839,Numpy like slicing on Tensors,"I was wondering if it is possible to implement Numpy like slicing annd updating a[1:10,2:20....]  in Tensorflow. It would make life much easier. Right now it code just gets bigger and uglier and bug prone.

"
26838,Trying to play alarm when it detects a person on android using tesnorflow,"
I am trying to play a sound when i detect a person and it does not process more images while sound is playing and wait n number of seconds after the sound stops playing before it processes any more images. this is to prevent overlapping sounds being played. 
I made changes to the line number below , it plays the sound when it detects an object that i tell it to, but problem is it does not detect anything after that it just freezes. 

https://github.com/rojanulak/MarsRoverPhoto/blob/master/tensorflow-master/tensorflow/examples/android/src/org/tensorflow/demo/DetectorActivity.java#L331
https://github.com/rojanulak/MarsRoverPhoto/blob/master/tensorflow-master/tensorflow/examples/android/src/org/tensorflow/demo/CameraActivity.java#L215

"
26836,ERROR!!! AttributeError: module '_pywrap_tensorflow_internal' has no attribute 'TF_ImportGraphDefOptionsSetDefaultDevice',"my programm error!
Using TensorFlow backend.
Traceback (most recent call last):

  File ""<ipython-input-2-658d4b6817ec>"", line 1, in <module>
    runfile('C:/Users/svm2717/Desktop/test.py', wdir='C:/Users/svm2717/Desktop')

  File ""D:\ANACONDA\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 705, in runfile
    execfile(filename, namespace)

  File ""D:\ANACONDA\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 102, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""C:/Users/svm2717/Desktop/test.py"", line 8, in <module>
    from keras.datasets import mnist

  File ""D:\ANACONDA\lib\site-packages\keras\__init__.py"", line 3, in <module>
    from . import utils

  File ""D:\ANACONDA\lib\site-packages\keras\utils\__init__.py"", line 6, in <module>
    from . import conv_utils

  File ""D:\ANACONDA\lib\site-packages\keras\utils\conv_utils.py"", line 9, in <module>
    from .. import backend as K

  File ""D:\ANACONDA\lib\site-packages\keras\backend\__init__.py"", line 89, in <module>
    from .tensorflow_backend import *

  File ""D:\ANACONDA\lib\site-packages\keras\backend\tensorflow_backend.py"", line 5, in <module>
    import tensorflow as tf

  File ""D:\ANACONDA\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import

  File ""D:\ANACONDA\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow

  File ""D:\ANACONDA\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *

  File ""D:\ANACONDA\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 1393, in <module>
    TF_ImportGraphDefOptionsSetDefaultDevice = _pywrap_tensorflow_internal.TF_ImportGraphDefOptionsSetDefaultDevice

AttributeError: module '_pywrap_tensorflow_internal' has no attribute 'TF_ImportGraphDefOptionsSetDefaultDevice'

CUDA:8.0
"
26835,[TF2.0] KerasLayer cannot be loaded from .h5,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
Run on Colab example from copied from tensorflow/hub: https://colab.research.google.com/drive/1ymmzlWHieCuXR7UjXezULnYsNsDeHc9m

Version:  2.0.0-alpha0
Eager mode:  True
Hub version:  0.3.0
GPU is available

**The issue**
When I try to run Colab example from tensorflow/hub for image retraining and then save/load model according to [the documentation](https://www.tensorflow.org/alpha/guide/keras/saving_and_serializing), there are some issues with it.

After retraining, when we try to save the model::
```
# Save the model
model.save('path_to_my_model.h5')
```
This error occurs (already described here: #26811 )
```
//...skipped, see ticket above for details ^
h5py/_objects.pyx in h5py._objects.with_phil.wrapper()
h5py/_objects.pyx in h5py._objects.with_phil.wrapper()
h5py/h5o.pyx in h5py.h5o.link()
RuntimeError: Unable to create link (name already exists)
```

But actually `path_to_my_model.h5` file is created. Then when I try to load it via:
```
# Recreate the exact same model purely from the file
new_model = keras.models.load_model('path_to_my_model.h5')
new_model.summary()
```
I get an error:
```
ValueError: Unknown layer: KerasLayer
```

Entire stacktrace:
```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-15-0ceb2f1e6ad5> in <module>()
----> 1 new_model = keras.models.load_model('path_to_my_model.h5')
      2 
      3 new_model.summary()

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/hdf5_format.py in load_model(filepath, custom_objects, compile)
    213     model_config = json.loads(model_config.decode('utf-8'))
    214     model = model_config_lib.model_from_config(model_config,
--> 215                                                custom_objects=custom_objects)
    216 
    217     # set weights

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/model_config.py in model_from_config(config, custom_objects)
     53                     '`Sequential.from_config(config)`?')
     54   from tensorflow.python.keras.layers import deserialize  # pylint: disable=g-import-not-at-top
---> 55   return deserialize(config, custom_objects=custom_objects)
     56 
     57 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/serialization.py in deserialize(config, custom_objects)
     93       module_objects=globs,
     94       custom_objects=custom_objects,
---> 95       printable_module_name='layer')

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)
    190             custom_objects=dict(
    191                 list(_GLOBAL_CUSTOM_OBJECTS.items()) +
--> 192                 list(custom_objects.items())))
    193       with CustomObjectScope(custom_objects):
    194         return cls.from_config(cls_config)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py in from_config(cls, config, custom_objects)
    349     for layer_config in layer_configs:
    350       layer = layer_module.deserialize(layer_config,
--> 351                                        custom_objects=custom_objects)
    352       model.add(layer)
    353     if not model.inputs and build_input_shape:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/serialization.py in deserialize(config, custom_objects)
     93       module_objects=globs,
     94       custom_objects=custom_objects,
---> 95       printable_module_name='layer')

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)
    179     config = identifier
    180     (cls, cls_config) = class_and_config_for_serialized_keras_object(
--> 181         config, module_objects, custom_objects, printable_module_name)
    182 
    183     if hasattr(cls, 'from_config'):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py in class_and_config_for_serialized_keras_object(config, module_objects, custom_objects, printable_module_name)
    164     cls = module_objects.get(class_name)
    165     if cls is None:
--> 166       raise ValueError('Unknown ' + printable_module_name + ': ' + class_name)
    167   return (cls, config['config'])
    168 

ValueError: Unknown layer: KerasLayer
```

Entire colab to review: https://colab.research.google.com/drive/1ymmzlWHieCuXR7UjXezULnYsNsDeHc9m"
26833,TOCO/TFLite Converter not working,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 1.14.1-dev20190314
- Python version: Python3.5.2
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
I am trying to convert a yolo model to tflite, I have generated the .pb file. I am able to get predictions working on Tensorflow using a session run. While converting the same .pb file to tflite through the python api it throws the following error 

ConverterError: TOCO failed. See console for info.
2019-03-18 16:34:02.329321: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1012 operators, 1562 arrays (0 quantized)
2019-03-18 16:34:02.392648: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1012 operators, 1562 arrays (0 quantized)
2019-03-18 16:34:03.052789: F tensorflow/lite/toco/graph_transformations/propagate_fixed_sizes.cc:722] Check failed: start_array.data_type == ArrayDataType::kInt32 Range op inputs must be int32.
Fatal Python error: Aborted

Current thread 0x00007f254cb60700 (most recent call first):
 File ""/usr/local/lib/python3.5/dist-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 33 in execute
 File ""/usr/local/lib/python3.5/dist-packages/absl/app.py"", line 251 in _run_main
 File ""/usr/local/lib/python3.5/dist-packages/absl/app.py"", line 300 in run
 File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py"", line 40 in run
 File ""/usr/local/lib/python3.5/dist-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 59 in main
 File ""/usr/local/bin/toco_from_protos"", line 11 in <module>
Aborted (core dumped)

If i do it using the tflite_convert commandline function I am able to generate the tflite file however the predictions are horrible and they are not working at all. 

When I am predicting using tensorflow graph the output tensor is as follows:

array([[[-9.8978394e+01, -1.3161835e+01,  1.3285416e+02, ...,
         8.2985699e-02,  7.4172002e-01,  3.4630752e-01],
       [-4.7924599e+01, -1.0267001e+02,  9.4346962e+01, ...,
         3.8488835e-02,  7.7538919e-01,  4.5305699e-01],
       [-1.7301196e+02, -1.9800723e+02,  2.0602176e+02, ...,
         1.3685286e-02,  7.9602063e-01,  5.4424524e-01],
       ...,
       [ 3.9877176e+02,  4.0956854e+02,  4.2418051e+02, ...,
         5.5837166e-01,  4.6688780e-01,  3.5518968e-01],
       [ 4.0362198e+02,  3.9286102e+02,  4.1983481e+02, ...,
         7.3692095e-01,  4.5420480e-01,  5.1171225e-01],
       [ 3.8272733e+02,  4.0869794e+02,  4.4242825e+02, ...,
         6.1386418e-01,  6.0733950e-01,  3.9755249e-01]]], dtype=float32)

Output tensor from tensorflow lite for same image and same configuration
array([[[-7.69360809e+01, -9.79493904e+00,  1.13176025e+02, ...,
         4.96306121e-02,  5.01996398e-01,  5.42349815e-01],
       [-4.04162636e+01, -8.99354248e+01,  8.99407501e+01, ...,
         1.81048810e-02,  4.99900073e-01,  7.85123646e-01],
       [-1.43075928e+02, -1.83144836e+02,  1.79637024e+02, ...,
         4.17280197e-03,  6.15250051e-01,  8.26103389e-01],
       ...,
       [ 3.99175842e+02,  4.08667114e+02,  4.26062622e+02, ...,
         5.26856482e-01,  5.87077022e-01,  3.69019330e-01],
       [ 4.06181152e+02,  3.69634338e+02,  4.14379700e+02, ...,
         8.10274899e-01,  4.71839100e-01,  6.61600232e-01],
       [ 3.91825989e+02,  4.08976501e+02,  4.34132080e+02, ...,
         5.45497298e-01,  6.76900625e-01,  4.91856068e-01]]],
     dtype=float32)

Code to convert graph to tflite [not working]: 
tflite_yolo_model='test.tflite'
with tf.Session(graph=frozenGraph,config=config) as sess:
   frozen_graph_def = tf.graph_util.convert_variables_to_constants(
     sess, sess.graph_def, ['output_boxes'])
converter=tf.lite.TFLiteConverter.from_frozen_graph(frozen_model,['inputs'],['output_boxes'])
converter.target_ops=[tf.lite.OpsSet.TFLITE_BUILTINS,tf.lite.OpsSet.SELECT_TF_OPS]
tflite_model=converter.convert()
#tflite_model=tf.lite.toco_convert(frozen_graph_def, input_tensors=[img_resized],output_tensors=[boxes])
open(tflite_yolo_model, ""wb"").write(tflite_model)
"
26831,Update README.md file of Tensorflow Lite Android example,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: 1.7.1
- Doc Link: - 


**Describe the documentation issue**

Update the README.md file of Tensorflow Lite Android Example.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**

Yes."
26829,Saving and Restoring SingularMoniteredSession in tensorflow 1.4.1,"I want to save and restore a SingularMoniteredSession in my code 

```
with g.as_default():
    .......
    with tf.train.SingularMoniteredSession() as sess:
        child_ops = .......
        sess.run(child_ops)
        controller_ops=..........
        sess.run(controller_ops)
```
I want to save and restore the controller ops session here(second session). but i wasn't able to find any documentation regarding it."
26828,Is it possible to do inference using a pre-trained tensorflow model from inside cuda kernel?,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
26826,1.13.1 wheels for Python 3.7 do not work on RHEL/CentOS7,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RedHat Enterprise Linux 7.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.13.1
- Python version: 2.7.2
- Installed using virtualenv? pip? conda?: virtualenv+pip
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the problem**

We noticed that TensorFlow 1.13.1 now comes with wheels for Python 3.7 as well:

tensorflow-1.13.1-cp37-cp37m-manylinux1_x86_64.whl

We first upgraded to 1.13.1 on our existing Python 3.6, which worked
fine. Then we tried upgrading to Python 3.7, but even importing
tensorflow fails there, because the shared libraries need a newer
libstdc++:

E   ImportError: Traceback (most recent call last):
E     File ""/home/anon/git/sensors/venv3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
E       from tensorflow.python.pywrap_tensorflow_internal import *
E     File ""/home/anon/git/sensors/venv3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
E       _pywrap_tensorflow_internal = swig_import_helper()
E     File ""/home/anon/git/sensors/venv3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
E       _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
E     File ""/opt/anon/python-3.7.2-master-201903180831/lib/python3.7/imp.py"", line 242, in load_module
E       return load_dynamic(name, filename, file)
E     File ""/opt/anon/python-3.7.2-master-201903180831/lib/python3.7/imp.py"", line 342, in load_dynamic
E       return _load(spec)
E   ImportError: /lib64/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by /home/anon/git/sensors/venv3/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
E   
E   
E   Failed to load the native TensorFlow runtime.
E   
E   See https://www.tensorflow.org/install/errors
E   
E   for some common reasons and solutions.  Include the entire stack trace
E   above this error message when asking for help.

This is error is logical, because RHEL/CentOS 7 only has support for 1.3.7 and earlier:

(venv3) [anon@vdesk sensors]$ uname -a 
Linux vdesk.localdomain 3.10.0-957.5.1.el7.x86_64 #1 SMP Wed Dec 19 10:46:58 EST 2018 x86_64 x86_64 x86_64 GNU/Linux
(venv3) [anon@vdesk sensors]$ cat /etc/redhat-release 
Red Hat Enterprise Linux Server release 7.6 (Maipo)
(venv3) [anon@vdesk sensors]$ strings /lib64/libstdc++.so.6 | grep CXXABI
CXXABI_1.3
CXXABI_1.3.1
CXXABI_1.3.2
CXXABI_1.3.3
CXXABI_1.3.4
CXXABI_1.3.5
CXXABI_1.3.6
CXXABI_1.3.7
CXXABI_TM_1
(venv3) [anon@vdesk sensors]$ gcc --version
gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-36)
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

If you unzip the 3.6 and 3.7 wheels for TF 1.13.1, you can see that the 3.6 wheel is compiled on Ubuntu 14.04 with GCC 4.8:

[anon@vdesk python]$ strings python36/tensorflow-1.13.1.data/purelib/tensorflow/libtensorflow_framework.so | grep -i ubuntu
GCC: (Ubuntu 4.8.5-4ubuntu8~14.04.2) 4.8.5

and the 3.7 wheel is compiled on Ubuntu 16.04 with GCC 5.4:

[anon@vdesk python]$ strings python37/tensorflow-1.13.1.data/purelib/tensorflow/libtensorflow_framework.so | grep -i ubuntu
GCC: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609

RHEL/CentOS 7 also has GCC 4.8.x, so that's why the Python 3.6 wheels work.

Is there any chance you could build the Python 3.7 wheels on Ubuntu 14.04 as well?




**Provide the exact sequence of commands / steps that you executed before running into the problem**

Just importing tensorflow; works fine with the Python3.6 wheels.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26824,The contents of Transfer learning tutorial and its github source are not the same.,"https://www.tensorflow.org/alpha/tutorials/images/transfer_learning
This is the tutorial for transfer learning for tensorflow 2.0
but if you click on run on colab or view source on github buttons,
then the contents are not the same 
(colab link : https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning.ipynb
github link :
https://github.com/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning.ipynb
)

which one is the latest one and which one I should follow?

Thank you in advance!"
26823,Performance hit in WALSModel?,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): from conda
- TensorFlow version (use command below): 1.10 gpu from conda
- Python version:  3.7 from conda
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: 10
- GPU model and memory: GX 1070 maxq


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

Suddenly running update row operations became laggy and never progressed...

**Describe the expected behavior**

update row operations should not run indefinetly 

```python


from mymodule  import ex
import gc, resource, multiprocessing

def run_ex(print_usage=False, *args, **kwargs):
    ex.run(config_updates=kwargs)
    if print_usage:
        usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
        print('resources used: {}'.format(usage))
    gc.collect()

def pool_run(*args, processes=1, print_usage=False, **kwargs):
    with multiprocessing.Pool(processes=processes) as pool:
        results = pool.apply(run_ex, (print_usage, *args), kwargs)
    gc.collect()
    return results

pool_run(processes=1, print_usage=True, **opts)


# ex is a wrapper that calls

def train_fn(
    model,                                   # <--- WALSModel
    input_tensor,                         # tf.SparseTensor
    iterations:int,
    sp_sparse=None,                 # sp.sparse.coo_matrix
    sacred_run=None,              
    print_progress:bool=True,
    logger=None
):
    os.environ['KMP_DUPLICATE_LIB_OK']='True'
    # extract row and column factors
    row_factor = model.row_factors[0]
    col_factor = model.col_factors[0]

    # update operations
    row_update_op = model.update_row_factors(sp_input=input_tensor)#[1]
    col_update_op = model.update_col_factors(sp_input=input_tensor)#[1]



    with tf.Session() as sess:
        # init model
        sess.run(model.initialize_op)
        sess.run(model.worker_init)

        if print_progress:
            status = Sil(total=iterations)
        for i in range(iterations):
            if logger is not None: logger.debug('Training iteration {}/{}'.format(str(i), str(iterations)))
            # update rows
            if logger is not None: logger.debug('Updating rows')   # <--- I see this in logs
            sess.run(model.row_update_prep_gramian_op)
            sess.run(model.initialize_row_update_op)
            _, _, loss, reg, _ = sess.run(row_update_op)
            if sacred_run is not None: sacred_run.log_scalar(""loss.row"", loss, i)   

            # update cols
            if logger is not None: logger.debug('Updating columns')   # <--- never makes it here
            sess.run(model.col_update_prep_gramian_op)
            sess.run(model.initialize_col_update_op)
            _, _, loss, reg, _ = sess.run(col_update_op)
            if sacred_run is not None: sacred_run.log_scalar(""loss.col"", loss, i)

            # update status
            if print_progress:
                status.tick(prefix='iteration')

            if sacred_run is not None and sp_sparse is not None:
                if logger is not None: logger.debug('Calculating rmse.')
                rf = row_factor.eval(session=sess)
                cf = col_factor.eval(session=sess)
                it_rmse = rmse(sp_sparse, rf, cf)
                sacred_run.log_scalar(""rmse"", it_rmse, i)


        # eval row / col factors
        output_row = row_factor.eval(session=sess)
        output_col = col_factor.eval(session=sess)
        sess.close()
    os.environ['KMP_DUPLICATE_LIB_OK']='False'
    return output_row, output_col

```



**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26821,tf.cond on a variable. FailedPreconditionError in tf.global_variables_initializer(),"I am running into  FailedPreconditionError error in tf.global_variables_initializer(). I have zeroed-in on the following part of the code to be the cultprit:

    def __init__(...):
        ...
        self.global_step = tf.get_variable(initializer=tf.zeros_initializer(), trainable=False, shape=(), name='global_step')
        ...
        step_rampup_value = self.step_rampup(self.global_step, self.rampup_length)

    def step_rampup(self, global_step, rampup_length):
        result = tf.cond(global_step < rampup_length,
                         lambda: tf.constant(0.0),
                         lambda: tf.constant(1.0))
        return tf.identity(result, name=""step_rampup"")
    session.run(tf.global_variables_initilizer())

self.global_step is to be incremented by 1 by optimizer at each iteration. It's value has to change. So, that is the behavior i want.

Error message:

    FailedPreconditionError ...
    506         with tf.Session(graph=highgraph) as session:
    --> 507             session.run(tf.global_variables_initializer())
    ...
    FailedPreconditionError: Attempting to use uninitialized value global_step
	 [[node global_step/read (defined at NML_U/sNeural.py:103)  = Identity[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](global_step)]]

Why is that part of the code is culprit?
Because, The following code works

    def __init__(...):
        ...
        self.global_step = tf.get_variable(initializer=tf.zeros_initializer(), trainable=False, shape=(), name='global_step')
        ...
        step_rampup_value = self.step_rampup(self.global_step, self.rampup_length)

    def step_rampup(self, global_step, rampup_length):
        result = tf.cond(global_step.initialized_value() < rampup_length,
                         lambda: tf.constant(0.0),
                         lambda: tf.constant(1.0))
        return tf.identity(result, name=""step_rampup"")
    session.run(tf.global_variables_initilizer())

but that will evaluate the conditional with the initialized value of self.global_step(=0) each time which is not the intended behavior

Also,

This code works as well:

    def __init__(...):
        ...
        self.global_step = tf.get_variable(initializer=tf.zeros_initializer(), trainable=False, shape=(), name='global_step')
        self.global_step = tf.assign(self.global_step,0.)
        ...
        step_rampup_value = self.step_rampup(self.global_step, self.rampup_length)

    def step_rampup(self, global_step, rampup_length):
        result = tf.cond(global_step < rampup_length,
                         lambda: tf.constant(0.0),
                         lambda: tf.constant(1.0))
        return tf.identity(result, name=""step_rampup"")
    session.run(tf.global_variables_initilizer())

But (maybe) this will again not lead to the dependency on global_step but  instead on assign op which will keep assigning 0 to self.global_step

How do i go about achieving the behavior 


**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26819,how to assign trainable variable to a tensor to form another trainable variable,"### System information

TensorFlow version : 1.12
Are you willing to contribute it : Yes

### Describe the feature and the current behavior/state.

This issue occurs in my nerual network. The data A stored in TFrecord file is with shape [batch_size,  A_size * A_size]. I want to use a fully connected layer to increase the dimension to get a bigger tensor B with shape [batch_size, B_size * B_size]. Then zero-padding B to get a bigger C with shape [batch_size, C_size, C_size]. 

> Note: B[i,:,:] is a block extracted from A[i,:,:][position_height_index * B_size:(position_height_index + 1) * B_size, position_width_index * B_size:(position_width_index + 1) * B_size]
 
Here is my code:
```python
input_size = A_size * A_size
output_size = B_size * B_size

 w = tf.Variable(tf.random_normal([input_size, output_size]))
 b = tf.Variable(0.0)

# A is a data tensor with shape [batch_size,  A_size * A_size]

 prediction = tf.matmul(A,w)+b

 C = tf.Variable(np.zeros([batch_size, C_size * C_size]), True, dtype = tf.float32)
 
for i in range(batch_size):
            
            # initialize temp
            temp = tf.Variable(np.zeros([C_size, C_size]), True, dtype=tf.float32)
            
            # assign reshaped prediction to temp with corresponding positions
            temp[position_height_index[i] * B_size:(position_height_index[i] + 1) * B_size,
            position_width_index[i] * B_size:(position_width_index[i] + 1) * 
            B_size].assign(tf.reshape( prediction[i], [B_size, B_size]))
            
           # assign reshaped temp to C
           C[i].assign(tf.reshape(temp, [1, C_size * C_size]))
     
  ```
### My Question: 
When i print C, all elements in C are still zero. It should be non-zero because i assign prediction to C, what's wrong. I assign trainable variable prediction to a zero tesnor C to zero-padding predcition. I hope C is trainable. Is it allowed?

### Will this change the current api? How?
No
### Who will benefit with this feature?
Some researchers of image processing
Any Other info."
26818,[TF2.0] Unknown TensorShape error when convert tf.keras model to tflite,"*Environment*
Win 7 64bit
PyCharm
Anaconda Python 3.6
tensorflow-gpu 2.0.0a0


*issue*
I trained a tf.keras model, now I want to convert it to tflite file.
The following is my implement:

saved_model.save(model, 'testM')
model = tf.saved_model.load('testM')
concrete_func = model.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]

converter = tf.lite.TFLiteConverter.from_concrete_function(concrete_func)
tflite_model = converter.convert()
open(""new_classificaton.tflite"", ""wb"").write(tflite_model)

When I execute it with PyCharm, the following error log is reported:

Traceback (most recent call last):
  File ""E:/Keras/PycharmProjects/7241/HelloTF2/HelloWorld.py"", line 73, in <module>
    tflite_model = converter.convert()
  File ""C:\ProgramData\Anaconda3\envs\HelloTF2\lib\site-packages\tensorflow\lite\python\lite.py"", line 261, in convert
    shape_list = tensor.get_shape().as_list()
  File ""C:\ProgramData\Anaconda3\envs\HelloTF2\lib\site-packages\tensorflow\python\framework\tensor_shape.py"", line 1128, in as_list
    raise ValueError(""as_list() is not defined on an unknown TensorShape."")
ValueError: as_list() is not defined on an unknown TensorShape.

Is this a bug? Or I misunderstand the api usage?"
26816,AttributeError: 'module' object has no attribute 'Session' in tensorflow/tensorflow:latest-gpu-jupyter  TF 2.0-alpah0 in Docker Container ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04, 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary/From official Docker Container
- TensorFlow version (use command below);
- Python version:2.0.0-alpha0
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: Using TF GPU Docker/Official
- GPU model and memory:NVIDIA V100 ,32 Gb,NVIDIA-SMI 384.145

**Describe the current behavior**

When I run a simple code snippet I get the following error
`AttributeError: 'module' object has no attribute 'Session'

```
# Creates a graph.
with tf.device('/gpu:0'):
  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
c = tf.matmul(a, b)
# Creates a session with log_device_placement set to True.
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
# Runs the op.
print(sess.run(c))
```
*Output*
```

AttributeError                            Traceback (most recent call last)
<ipython-input-7-9c6adda4f265> in <module>()
      5 c = tf.matmul(a, b)
      6 # Creates a session with log_device_placement set to True.
----> 7 sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
      8 # Runs the op.

**AttributeError: 'module' object has no attribute 'Session'**

```

**Describe the expected behavior**

The basic code should work. The TF in the Docker container should be updated and  build

**Code to reproduce the issue**
Given

**Other info / logs**
Reinstalling TF in Docker container works
pip install --upgrade --force-reinstall tensorflow-gpu
(this installs TF 1.13 version and unistalls the TF 2.0)

"
26814,[TF2.0] Loading a Saved Model failed with `AttributeError: '_UserObject' object has no attribute '_create_or_restore_slot_variable'`,"I am using the latest TF2.0 pip package.

Consider the following code:

```python
import tensorflow as tf
import numpy as np

class ToyModel(tf.keras.Model):
    """"""A simple linear model.""""""

    def __init__(self):
        super().__init__()
        self.l1 = tf.keras.layers.Dense(5)

    @tf.function(input_signature=[tf.TensorSpec([None, 1], tf.float32, name=""input_func"")])
    def call(self, x):
        return self.l1(x)

def toy_dataset():
    inputs = tf.range(10.)[:, None]
    outputs = inputs * 5. + tf.range(5.)[None, :]
    # TODO: switch `tuple` to `dict`.
    dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))
    return dataset.repeat(10).batch(2).shuffle(buffer_size=5)


dataset = toy_dataset()
optimizer = tf.keras.optimizers.Adam(0.1)

model = ToyModel()
model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])
history = model.fit(dataset, epochs=5)

# Inference works
inputs = np.array([[0, 5]], dtype=np.float32).T
print(model(inputs))

# Export to Saved Model
model_path = ""/tmp/saved_model""
tf.saved_model.save(model, model_path)

# Load model
saved_model = tf.saved_model.load(model_path)

# Inference from Saved Model
#inputs = np.array([[0, 5]], dtype=np.float32).T
#saved_model(inputs)
```

It fails with:

```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-2-4a1364881187> in <module>
     37 
     38 # Load model
---> 39 saved_model = tf.saved_model.load(model_path)
     40 
     41 # Inference from Saved Model

~/local/conda/envs/tf/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py in load(export_dir, tags)
    322       loader = _Loader(object_graph_proto,
    323                        saved_model_proto,
--> 324                        export_dir)
    325       root = loader.get(0)
    326   else:

~/local/conda/envs/tf/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py in __init__(self, object_graph_proto, saved_model_proto, export_dir)
     62     self._setup_functions_structures()
     63     self._setup_functions_captures()
---> 64     self._restore_checkpoint()
     65 
     66     for node in self._nodes:

~/local/conda/envs/tf/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py in _restore_checkpoint(self)
    151     saver = util.TrackableSaver(graph_view.ObjectGraphView(self.get(0)))
    152     saver._file_prefix_placeholder = constant_op.constant(variables_path)
--> 153     load_status = saver.restore(variables_path)
    154     load_status.assert_existing_objects_matched()
    155     checkpoint = load_status._checkpoint

~/local/conda/envs/tf/lib/python3.7/site-packages/tensorflow/python/training/tracking/util.py in restore(self, save_path)
   1094         graph_view=self._graph_view)
   1095     base.CheckpointPosition(checkpoint=checkpoint, proto_id=0).restore(
-> 1096         self._graph_view.root)
   1097     load_status = CheckpointLoadStatus(
   1098         checkpoint,

~/local/conda/envs/tf/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py in restore(self, trackable)
    207         # This object's correspondence with a checkpointed object is new, so
    208         # process deferred restorations for it and its dependencies.
--> 209         restore_ops = trackable._restore_from_checkpoint_position(self)  # pylint: disable=protected-access
    210         if restore_ops:
    211           self._checkpoint.new_restore_ops(restore_ops)

~/local/conda/envs/tf/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py in _restore_from_checkpoint_position(self, checkpoint_position)
    777           ._single_restoration_from_checkpoint_position(
    778               checkpoint_position=current_position,
--> 779               visit_queue=visit_queue)))
    780     return restore_ops
    781 

~/local/conda/envs/tf/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py in _single_restoration_from_checkpoint_position(self, checkpoint_position, visit_queue)
    804             child_position)
    805       else:
--> 806         if child_position.bind_object(trackable=local_object):
    807           # This object's correspondence is new, so dependencies need to be
    808           # visited. Delay doing it so that we get a breadth-first dependency

~/local/conda/envs/tf/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py in bind_object(self, trackable)
    249                       slot_name=slot_restoration.slot_name))
    250         else:
--> 251           optimizer_object._create_or_restore_slot_variable(  # pylint: disable=protected-access
    252               slot_variable_position=CheckpointPosition(
    253                   checkpoint=checkpoint,

AttributeError: '_UserObject' object has no attribute '_create_or_restore_slot_variable'
```"
26813,can't import tensorflow.keras properly,"I,m writing my code in vscode edit with tensorflow=1.13.1 version and anaconda virtual environment. But when I write 'from tensorflow.keras import layers',it give me a warning:
""unresolved import 'tensorflow.keras'(unresolved import)"". 
The code can run as I expected,no errors. But because tensorflow.keras can't be imported properly,the auto-completion and intelligent hint function can't work,I need to search the function's usage everytime. I have thought it's the problem of vscode, but the problem came as well when I use pycharm IDE. Have anyone has the same problem? Is there anyone can help me?
"
26812,Can't declare tf.Variable in @tf.function decorated function,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Archlinux
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-dev20190317
- Python version: 3.6

**Describe the current behavior**

A function that correctly works in eager execution can't be decorated with `@tf.function` if declares a `tf.Variable` in the function body.

The error message, reported below, is misleading since it talks about a non-first invocation when the function is invoked only once.

```
ValueError: tf.function-decorated function tried to create variables on non-first call.
```

**Describe the expected behavior**

Calling a function decorated with the `@tf.function` should produce the same output as the same function without the decoration.

**Code to reproduce the issue**

import tensorflow as tf

```python
import tensorflow as tf

@tf.function
def f():
    a = tf.constant([[10, 10], [11., 1.]])
    x = tf.constant([[1., 0.], [0., 1.]])
    b = tf.Variable(12.)
    y = tf.matmul(a, x) + b
    return y

print(f())
```
"
26811,[TF2.0] Bug when saving weights with custom layers,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
docker image from tensorflow/tensorflow:2.0.0a0-gpu-jupyter
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
git version 'v1.12.0-9492-g2c319fb415'
tensorflow version '2.0.0-alpha0'
- Python version:
2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
cuda version 415.27
- GPU model and memory:
GTX 1080Ti

**Describe the current behavior**
If custom layers are used weights of the model cannot be saved using model.save_weights() or exported to the savedmethod. I am using the code provided in the Custom Layer code provided in the documentation at https://www.tensorflow.org/alpha/guide/keras/custom_layers_and_models#you_can_optionally_enable_serialization_on_your_layers

**Describe the expected behavior**
The model to save its weights properly

**Code to reproduce the issue**
```python
import numpy as np
import tensorflow as tf
layers = tf.keras.layers
keras = tf.keras

# From documentation until the next comment
class Linear(layers.Layer):

    def __init__(self, units=32, **kwargs):
        super(Linear, self).__init__(**kwargs)
        self.units = units

    def build(self, input_shape):
        self.w = self.add_weight(shape=(input_shape[-1], self.units),
                                 initializer='random_normal',
                                 trainable=True)
        self.b = self.add_weight(shape=(self.units,),
                                 initializer='random_normal',
                                 trainable=True)

    def call(self, inputs):
        return tf.matmul(inputs, self.w) + self.b

    def get_config(self):
        config = super(Linear, self).get_config()
        config.update({'units': self.units})
        return config
    

layer = Linear(10)
config = layer.get_config()
print(config)
new_layer = Linear.from_config(config)

# Creating a layer and saving its weights
data = np.random.random((1000, 10))
labels = np.random.random((1000, 10))
inputs = keras.Input((10,))
outputs = layer(inputs)
model = keras.Model(inputs, outputs)
config = model.get_config()
print(config)
print(model.summary())
model.compile(optimizer=tf.keras.optimizers.RMSprop(0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])
model.fit(data, labels, batch_size=10, epochs=1)

model.save_weights(""temp/layers_weights"")
```

**Other info / logs**
```
{'units': 10, 'dtype': None, 'trainable': True, 'name': 'linear_5'}
{'layers': [{'class_name': 'InputLayer', 'config': {'dtype': 'float32', 'batch_input_shape': (None, 10), 'name': 'input_5', 'sparse': False}, 'inbound_nodes': [], 'name': 'input_5'}, {'class_name': 'Linear', 'config': {'units': 10, 'dtype': 'float32', 'trainable': True, 'name': 'linear_5'}, 'inbound_nodes': [['input_5', 0, 0, {}]], 'name': 'linear_5'}], 'input_layers': ['input_5', 0, 0], 'output_layers': ['linear_5', 0, 0], 'name': 'model_4'}
Model: ""model_4""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_5 (InputLayer)         [(None, 10)]              0         
_________________________________________________________________
linear_5 (Linear)            (None, 10)                110       
=================================================================
Total params: 110
Trainable params: 110
Non-trainable params: 0
_________________________________________________________________
None
1000/1000 [==============================] - 1s 523us/sample - loss: 37.3735 - accuracy: 0.1060

------------------------------------------------------------
AttributeError             Traceback (most recent call last)
<ipython-input-7-32331e9dcd27> in <module>()
     47 model.fit(data, labels, batch_size=10, epochs=1)
     48 
---> 49 model.save_weights(""temp/layers_weights"")

/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/network.pyc in save_weights(self, filepath, overwrite, save_format)
   1409              'saved.\n\nConsider using a TensorFlow optimizer from `tf.train`.')
   1410             % (optimizer,))
-> 1411       self._trackable_saver.save(filepath, session=session)
   1412       # Record this checkpoint so it's visible from tf.train.latest_checkpoint.
   1413       checkpoint_management.update_checkpoint_state_internal(

/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/tracking/util.pyc in save(self, file_prefix, checkpoint_number, session)
    976     save_path, new_feed_additions = self._save_cached_when_graph_building(
    977         file_prefix=file_prefix_tensor,
--> 978         object_graph_tensor=object_graph_tensor)
    979     if new_feed_additions:
    980       feed_dict.update(new_feed_additions)

/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/tracking/util.pyc in _save_cached_when_graph_building(self, file_prefix, object_graph_tensor)
    916     (named_saveable_objects, graph_proto,
    917      feed_additions) = self._gather_saveables(
--> 918          object_graph_tensor=object_graph_tensor)
    919     if (self._last_save_object_graph != graph_proto
    920         # When executing eagerly, we need to re-create SaveableObjects each time

/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/tracking/util.pyc in _gather_saveables(self, object_graph_tensor)
    882     """"""Wraps _serialize_object_graph to include the object graph proto.""""""
    883     (named_saveable_objects, graph_proto,
--> 884      feed_additions) = self._graph_view.serialize_object_graph()
    885     if object_graph_tensor is None:
    886       with ops.device(""/cpu:0""):

/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/tracking/graph_view.pyc in serialize_object_graph(self)
    379     trackable_objects, path_to_root = self._breadth_first_traversal()
    380     return self._serialize_gathered_objects(
--> 381         trackable_objects, path_to_root)
    382 
    383   def frozen_saveable_objects(self, object_map=None, to_graph=None):

/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/tracking/graph_view.pyc in _serialize_gathered_objects(self, trackable_objects, path_to_root, object_map)
    335     object_names = object_identity.ObjectIdentityDictionary()
    336     for obj, path in path_to_root.items():
--> 337       object_names[obj] = _object_prefix_from_path(path)
    338     node_ids = object_identity.ObjectIdentityDictionary()
    339     for node_id, node in enumerate(trackable_objects):

/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/tracking/graph_view.pyc in _object_prefix_from_path(path_to_root)
     62   return ""/"".join(
     63       (_escape_local_name(trackable.name)
---> 64        for trackable in path_to_root))
     65 
     66 

/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/tracking/graph_view.pyc in <genexpr>((trackable,))
     62   return ""/"".join(
     63       (_escape_local_name(trackable.name)
---> 64        for trackable in path_to_root))
     65 
     66 

/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/tracking/graph_view.pyc in _escape_local_name(name)
     55   # edges traversed to reach the variable, so we escape forward slashes in
     56   # names.
---> 57   return (name.replace(_ESCAPE_CHAR, _ESCAPE_CHAR + _ESCAPE_CHAR)
     58           .replace(r""/"", _ESCAPE_CHAR + ""S""))
     59 

AttributeError: 'NoneType' object has no attribute 'replace'
```"
26810,Gesture Classification Web App (many bugs on JS code),"- index.js : function loadModel does not exist
line **32** `const mobilenet = await tf.loadModel(
`
i replaced it for `loadLayersModel` as the only function with a similar name
`const mobilenet = await tf.loadLayersModel(`


- ui.js : two missing carry return
line **151** `} ui.donePredicting =`
replaced by 
`}`
`ui.donePredicting =`
line **181** `} let mouseDown = false;`
replaced by 
`}`
`let mouseDown = false;`

- webcam.js
line 39 ` const webcamImage = tf.fromPixels(this.webcamElement);`

i replaced it for `browser.fromPixels`, i found it inspecting tfjs.js code
` const webcamImage = tf.browser.fromPixels(this.webcamElement);`

after those changes application worked correctly ...
"
26809,[TF 2.0] Cannot load Keras Sequential model witn InputLayer from h5.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **YES**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Debian Stable**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): **CPU, both TF-2.0.0a0 and tf-nightly-2.0-preview-2.0.0.dev20190315**
- Python version: **3.5.3**
- Bazel version (if compiling from source): **N/A**
- GCC/Compiler version (if compiling from source): **N/A**
- CUDA/cuDNN version: **N/A**
- GPU model and memory: **N/A**

**Describe the current behavior**
When a `Sequential` Keras model contains `InputLayer` and it is saved, it cannot be loaded and fails with a message
```
ValueError: You are trying to load a weight file containing 1 layers into a model with 0 layers.
```

**Describe the expected behavior**
The model can be loaded correctly.

**Code to reproduce the issue**
```python
import numpy as np
import tensorflow as tf

inputs = np.arange(10)
outputs = 2 * inputs

model = tf.keras.Sequential([
    tf.keras.layers.InputLayer(input_shape=[1]),
    tf.keras.layers.Dense(1),
])
model.compile(
    optimizer=tf.keras.optimizers.Adam(),
    loss=tf.keras.losses.MeanSquaredError(),
    metrics=[tf.keras.metrics.MeanSquaredError()]
)
model.fit(inputs, outputs)
model.save(""model.h5"")

loaded_model = tf.keras.models.load_model(""model.h5"")
```

**Other info / logs**
The problem is in the fact that `Sequential.layers` does not return the `InputLayer`. To quote comments from the method:
```python
  def layers(self):
    # Historically, `sequential.layers` only returns layers that were added
    # via `add`, and omits the auto-generated `InputLayer` that comes at the
    # bottom of the stack.
    # `Trackable` manages the `_layers` attributes and does filtering
    # over it.
```
The problem is that if `InputLayer` was added manually with a specified `input_shape`, then it is an error not to serialize it -- because then the following layers do not know what the input shape is.

Workarounds:
- if instead of `InputLayer` you add `input_shape` to the `Dense` layer, it works. However, input `dtype` cannot be specified in this way and when you pass `tf.int32` on input, using an `InputLayer` is required
- in Functional API the `tf.keras.layers.Input` _is_ serialized in the model (and, funnily, as a `InputLayer`).

Solutions:
- when serializing a `Sequential` model, all layers need to be serialized (and no filtering of `InputLayer` performed)."
26808,[TF 2.0] unconnected_gradients = 'zero' does not work,"**System information**
- OS Platform and Distribution: MacOS 10.14.3 
- TensorFlow installed from binary
- TensorFlow version: 2.0.0a0
- Python version: 3.7.2

I try to get gradients w.r.t. model parameters. Though I was getting None values. Here is an example:

```
> import tensorflow as tf
> import tensorflow.keras.layers as layers

> model = tf.keras.Sequential()
> model.add(layers.Dense(10, input_shape=(2,)))
> with tf.GradientTape() as tape:
>   loss = tf.random.normal((10, 10))
> grads = tape.gradient(loss, model.trainable_variables, unconnected_gradients='zero')
> print(grads)
[None, None]
```

I expect these values to be zero. Though they are not."
26807,[TF 2.0] tf 2 doesn't allow static unrolling of loops which can be slower,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 1.13.1
- Python version: Python 3.6.7
- CUDA/cuDNN version: 10.0/7.4
- GPU model and memory: GeForce GTX 1060, Compute Capability 6.1, 6GB RAM

**Describe the current behavior**

There doesn't appear to be any way to unroll a loop statically when using `tf.function`. In tensorflow 1 I was able to create lots of nodes in a graph using a for loop, when using `tf.function` autograph replaces this with a `tf.while_loop` which seems to be slower in some cases.

**Describe the expected behavior**

There should be a way to statically unroll a loop when using `tf.function` for performance reasons.

**Code to reproduce the issue**

This code calculates a matrix exponential using a taylor series. I also noticed that `tf.linalg.expm`has gotten a lot slower but that was not my main interest in this issue (I believe the implementation for that has changed to support autodiff, see https://github.com/tensorflow/tensorflow/issues/15465)

output (using the latest `tensorflow-gpu` and `tensorflow-gpu==2.0.0-alpha` respectively from pip):
``` shell
(tf1) $ python expm.py 
BENCHMARKS:
tf took 0.1753 seconds for 25 iterations
taylor took 0.0146 seconds for 25 iterations

(tf2) $ python expm.py 
BENCHMARKS:
tf took 0.7331 seconds for 25 iterations
taylor took 1.0135 seconds for 25 iterations
taylor_v2 took 0.5931 seconds for 25 iterations
```

input:
``` python
import tensorflow as tf
import numpy as np
import time as tm

MATRIX_DIM = 1000
TAYLOR_SUM = 30
EPS = 1e-2
WARMUP = 10
ITERATIONS = 25


def benchmark(name, fn):
    for _ in range(WARMUP):
        value = fn()
    start = tm.time()
    for _ in range(ITERATIONS):
        value = fn()
    end = tm.time()
    runtime = end - start
    print(f""{name} took {runtime:.4f} seconds for {ITERATIONS} iterations"")
    return value


def taylor_expm(x, n):
    x_0 = tf.eye(tf.shape(x)[0], dtype=x.dtype)
    x_i = x
    y = x_0 + x_i
    for i in range(2, n + 1):
        x_i = (x_i @ x) / tf.cast(i, x.dtype)
        y = y + x_i
    return y


np.random.seed(42)

x = tf.constant(np.random.uniform(-0.5, 0.5, [MATRIX_DIM, MATRIX_DIM]), tf.float32)

if tf.__version__.startswith(""2""):

    @tf.function
    def taylor_expm_v2(x, n):
        return taylor_expm(x, n)

    print(""\nBENCHMARKS:"")
    tf_expm = benchmark(""tf"", lambda: tf.linalg.expm(x))
    my_expm = benchmark(""taylor"", lambda: taylor_expm(x, TAYLOR_SUM))
    my_expm_v2 = benchmark(""taylor_v2"", lambda: taylor_expm_v2(x, TAYLOR_SUM))

    np.testing.assert_allclose(tf_expm.numpy(), my_expm.numpy(), atol=EPS)
    np.testing.assert_allclose(tf_expm.numpy(), my_expm_v2.numpy(), atol=EPS)
else:
    tf_expm_ = tf.linalg.expm(x)
    my_expm_ = taylor_expm(x, TAYLOR_SUM)

    with tf.Session() as sess:
        print(""\nBENCHMARKS:"")
        tf_expm = benchmark(""tf"", lambda: sess.run(tf_expm_))
        my_expm = benchmark(""taylor"", lambda: sess.run(my_expm_))

    np.testing.assert_allclose(tf_expm, my_expm, atol=EPS)
```"
26806,tflite runtime error with depthwise conv2D ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows/Ubuntu
- TensorFlow installed from (source or binary): Tried source and binary
- Python version: 3.6
- CUDA/cuDNN version: 10/7.3
- GPU model and memory: RTX 2070

**Describe the current behavior**

I run into the following error when trying to run a tflite model.
`RuntimeError: tensorflow/lite/kernels/depthwise_conv.cc:104 params->depth_multiplier * SizeOfDimension(input, 3) != SizeOfDimension(filter, 3) (0 != 6)Node number 7 (DEPTHWISE_CONV_2D) failed to prepare.`

The code runs fine normally, just not after exporting to tflite.
https://www.tensorflow.org/lite/guide/ops_compatibility -> States that the op is supported. 
I know it has been used for the MobileNet variants before.
I specified rate=[1,1] and the input kernel is constant (tf.constant).

This error has also been reported in the comments of https://github.com/tensorflow/tensorflow/issues/20798
I have tried with tf v1.12.0 and v2.0.

**Describe the expected behavior**
In my case, the depth multiplier should be 1 and the size of the input channel dimension should be 6. 
I'm not sure why it says it is 0?

**Code to reproduce the issue**
I will provide a small snippet of code to reproduce the error, if it is not well known."
26804,when using model_to_estimator on a keras model -> accuracy and loss are not stored in events.out.tfevents.xxxxxxxxxx,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Yes
- TensorFlow version (use command below):
- Python version: 3.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**

when using a keras model and transforming it with `model_to_estimator` accuracy and loss are not stored in`events.out.tfevents.xxxxxxxxxx` (used by TensorBoard for visualiztion). I am using `tf.estimator.train()` or `tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)` and in both cases I only get:
`dict_keys(['global_step/sec', 'loss_1'])`

using `tf.estimator.train()` I am expecting loss and accuray in `events.out.tfevents.xxxxxxxxxx` for the training dataset

using `tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)` I am expecting loss and accuray in `events.out.tfevents.xxxxxxxxxx` for the training dataset abd testing datset.

The info are computed and display in the logfiles:
```
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Evaluation [10/100]
INFO:tensorflow:Evaluation [20/100]
INFO:tensorflow:Evaluation [30/100]
INFO:tensorflow:Evaluation [40/100]
INFO:tensorflow:Evaluation [50/100]
INFO:tensorflow:Evaluation [60/100]
INFO:tensorflow:Evaluation [70/100]
INFO:tensorflow:Finished evaluation at 2019-03-16-12:28:05
INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.9766614, global_step = 1000, loss = 0.074898034
INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt-1000
```
Is this expected ? It is documented somehwere ? without such info in TensorBoard then it is impossible to see if the model is overfitting or not. I am not even sure which info is displayed in the logfile, right ? or is there some other magic way to get such info ?

why tf.summary information are not propagated from keras to estimator model ? We don't have much control over the estimator model when it is converted from Keras

One option is to use a estimator model using keras layer so we can control everything but since Keras is now the official high level API from TensorFlow, I was thinking I could just transform it to an estimator model. I will be happy to test TF 2.0 if this will fix my issue. 

**Describe the expected behavior**

when I am using the same keras model and running the `fit` method, I got in the `events.out.tfevents.xxxxxxxxxx` file:

`dict_keys(['batch_acc', 'batch_loss', 'epoch_acc', 'epoch_loss', 'epoch_val_acc', 'epoch_val_loss'])`

I got 6 variables inluding accuracy and loss for the training and test datset. This is what Iam expected.

**Code to reproduce the issue**
code to inspect `events.out.tfevents.xxxxxxxxxx` files:
```
from tensorboard.backend.event_processing import event_accumulator
import numpy as np

def load_data_tensorboard(path):
    event_acc = event_accumulator.EventAccumulator(path)
    event_acc.Reload()
    data = {}
    
    for tag in sorted(event_acc.Tags()[""scalars""]):
        x, y = [], []
        for scalar_event in event_acc.Scalars(tag):
            x.append(scalar_event.step)
            y.append(scalar_event.value)
        data[tag] = (np.asarray(x), np.asarray(y))
    return data
```
one notebook contain the code but this is ""work in progress"":
https://github.com/tarrade/proj_DL_models_and_pipelines_with_GCP/blob/master/notebook/08-Mnist_keras_estimator.ipynb"
26803,[TF 2.0] timestep-wise sample weights not working with tf.data.Dataset and tf.keras.Model,"**System information**
- TensorFlow version (use command below): tf-nightly-2.0-preview

**Describe the current behavior**

With some model such as object detection one need to weight losses per locations. In this case, the output tensor can have the following shape `[N, T, ...]` where N the batch size and T the locations. 
According to the documentation `tf.keras.Model` can handle timestep-wise sample weight by setting ` sample_weight_mode=""temporal""`. 
But this seems not to be the case when we use `tf.Dataset` as inputs.

**Code to reproduce the issue**

```
import tensorflow as tf


class MyModel(tf.keras.Model):

    def __init__(self):
        super(MyModel, self).__init__(name='')

        self.l1 = tf.keras.layers.Conv2D(10, 3)
        self.o1 = tf.keras.layers.Conv2D(2, 1)
        self.o2 = tf.keras.layers.Conv2D(3, 1, name='o2')

        # print(self._output_names)

    def call(self, inputs):
        batch_size = tf.shape(inputs)[0]
        y = self.l1(inputs)
        y1 = self.o1(y)
        y1 = tf.reshape(y1, [batch_size, -1, 2])
        y2 = self.o2(y)
        y2 = tf.reshape(y2, [batch_size, -1, 3])

        return y1, y2


class Loss1(tf.keras.losses.Loss):

    def call(self, targets, predictions):
        losses = tf.math.abs(predictions - targets)
        return tf.reduce_sum(losses, axis=2)


x = tf.random.uniform([10, 16, 16, 3])
y1 = tf.random.uniform([10, 14*14, 2])
y2 = tf.random.uniform([10, 14*14, 3])
w1 = tf.random.uniform([10, 14*14, 1])
w2 = tf.random.uniform([10, 14*14, 1])


data = tf.data.Dataset.from_tensor_slices((x, (y1, y2), {'output_1': w1, 'output_2': w2})).batch(5).repeat()


model = MyModel()
optimizer = tf.keras.optimizers.SGD(0.001)

model.compile(optimizer=optimizer,
              loss=[Loss1(), Loss1()],
              loss_weights=[1., 1.],
              sample_weight_mode=""temporal"",
              run_eagerly=True)


model.fit(data, epochs=5, steps_per_epoch=1)
```

**Other info / logs**

```
ValueError: Found a sample_weight array with shape (5, 196, 1). In order to use timestep-wise sample weights, you should specify sample_weight_mode=""temporal"" in compile(). If you just mean to use sample-wise weights, make sure your sample_weight array is 1D.
```"
26802,New issue cross-compiling latest tensorflow source for arm,"System information:
Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No.
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 18.10 under Hyper-V Virtual Machine (Windows 10)
Resource dedicated to virtual machine:
4GB Ram - 4 cores 
TensorFlow installed from (source or binary):
Source
TensorFlow version (use command below):
Latest from tensorflow's github page
Python version:
3.6.7
Docker version:
18.09.3

I'm building the source for raspberry pi 3B+ and I followed the instructions on this page:
[https://www.tensorflow.org/install/source_rpi](https://www.tensorflow.org/install/source_rpi)

The error on the precedure:

ERROR: /workspace/tensorflow/core/kernels/BUILD:3004:1: C++ compilation of rule '//tensorflow/core/kernels:matrix_exponential_op' failed (Exit 1): arm-linux-gnueabihf-gcc failed: error executing command 
  (cd /home/emperon/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH='' \
    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_DOWNLOAD_CLANG=0 \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL_SYCL=0 \
    TF_NEED_ROCM=0 \
  /home/emperon/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/arm_compiler/bin/arm-linux-gnueabihf-gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -DRASPBERRY_PI -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -isystem /usr/include/arm-linux-gnueabihf -isystem /usr/include/python3.4 -isystem /usr/include/ -MD -MF bazel-out/armeabi-opt/bin/tensorflow/core/kernels/_objs/matrix_exponential_op/matrix_exponential_op.pic.d '-frandom-seed=bazel-out/armeabi-opt/bin/tensorflow/core/kernels/_objs/matrix_exponential_op/matrix_exponential_op.pic.o' -fPIC -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -DTF_USE_SNAPPY -iquote . -iquote bazel-out/armeabi-opt/genfiles -iquote bazel-out/armeabi-opt/bin -iquote external/com_google_absl -iquote bazel-out/armeabi-opt/genfiles/external/com_google_absl -iquote bazel-out/armeabi-opt/bin/external/com_google_absl -iquote external/bazel_tools -iquote bazel-out/armeabi-opt/genfiles/external/bazel_tools -iquote bazel-out/armeabi-opt/bin/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/armeabi-opt/genfiles/external/eigen_archive -iquote bazel-out/armeabi-opt/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/armeabi-opt/genfiles/external/local_config_sycl -iquote bazel-out/armeabi-opt/bin/external/local_config_sycl -iquote external/nsync -iquote bazel-out/armeabi-opt/genfiles/external/nsync -iquote bazel-out/armeabi-opt/bin/external/nsync -iquote external/gif_archive -iquote bazel-out/armeabi-opt/genfiles/external/gif_archive -iquote bazel-out/armeabi-opt/bin/external/gif_archive -iquote external/jpeg -iquote bazel-out/armeabi-opt/genfiles/external/jpeg -iquote bazel-out/armeabi-opt/bin/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/armeabi-opt/genfiles/external/protobuf_archive -iquote bazel-out/armeabi-opt/bin/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/armeabi-opt/genfiles/external/com_googlesource_code_re2 -iquote bazel-out/armeabi-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/armeabi-opt/genfiles/external/farmhash_archive -iquote bazel-out/armeabi-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/armeabi-opt/genfiles/external/fft2d -iquote bazel-out/armeabi-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/armeabi-opt/genfiles/external/highwayhash -iquote bazel-out/armeabi-opt/bin/external/highwayhash -iquote external/zlib_archive -iquote bazel-out/armeabi-opt/genfiles/external/zlib_archive -iquote bazel-out/armeabi-opt/bin/external/zlib_archive -iquote external/double_conversion -iquote bazel-out/armeabi-opt/genfiles/external/double_conversion -iquote bazel-out/armeabi-opt/bin/external/double_conversion -iquote external/snappy -iquote bazel-out/armeabi-opt/genfiles/external/snappy -iquote bazel-out/armeabi-opt/bin/external/snappy -iquote external/hwloc -iquote bazel-out/armeabi-opt/genfiles/external/hwloc -iquote bazel-out/armeabi-opt/bin/external/hwloc -isystem external/eigen_archive -isystem bazel-out/armeabi-opt/genfiles/external/eigen_archive -isystem bazel-out/armeabi-opt/bin/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/armeabi-opt/genfiles/external/nsync/public -isystem bazel-out/armeabi-opt/bin/external/nsync/public -isystem external/gif_archive/lib -isystem bazel-out/armeabi-opt/genfiles/external/gif_archive/lib -isystem bazel-out/armeabi-opt/bin/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/armeabi-opt/genfiles/external/protobuf_archive/src -isystem bazel-out/armeabi-opt/bin/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/armeabi-opt/genfiles/external/farmhash_archive/src -isystem bazel-out/armeabi-opt/bin/external/farmhash_archive/src -isystem external/zlib_archive -isystem bazel-out/armeabi-opt/genfiles/external/zlib_archive -isystem bazel-out/armeabi-opt/bin/external/zlib_archive -isystem external/double_conversion -isystem bazel-out/armeabi-opt/genfiles/external/double_conversion -isystem bazel-out/armeabi-opt/bin/external/double_conversion -isystem external/hwloc/hwloc -isystem bazel-out/armeabi-opt/genfiles/external/hwloc/hwloc -isystem bazel-out/armeabi-opt/bin/external/hwloc/hwloc -isystem external/hwloc/include -isystem bazel-out/armeabi-opt/genfiles/external/hwloc/include -isystem bazel-out/armeabi-opt/bin/external/hwloc/include '-march=armv7-a' '-mfpu=neon-vfpv4' '-std=gnu11' '-DS_IREAD=S_IRUSR' '-DS_IWRITE=S_IWUSR' -O3 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_1 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_2 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_8 -funsafe-math-optimizations -ftree-vectorize -fomit-frame-pointer -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' -DTENSORFLOW_MONOLITHIC_BUILD -pthread -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -no-canonical-prefixes -fno-canonical-system-headers -c tensorflow/core/kernels/matrix_exponential_op.cc -o bazel-out/armeabi-opt/bin/tensorflow/core/kernels/_objs/matrix_exponential_op/matrix_exponential_op.pic.o)
Execution platform: @bazel_tools//platforms:host_platform
cc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++
In file included from external/eigen_archive/unsupported/Eigen/MatrixFunctions:59:0,
                 from ./third_party/eigen3/unsupported/Eigen/MatrixFunctions:1,
                 from tensorflow/core/kernels/matrix_exponential_op.cc:19:
external/eigen_archive/unsupported/Eigen/src/MatrixFunctions/MatrixFunction.h: In member function 'MatrixType Eigen::internal::MatrixFunctionAtomic<MatrixType>::compute(const MatrixType&) [with MatrixType = Eigen::Matrix<std::complex<float>, -1, -1>]':
external/eigen_archive/unsupported/Eigen/src/MatrixFunctions/MatrixFunction.h:100:1: internal compiler error: in decompose_normal_address, at rtlanal.c:5799
 }
 ^
Please submit a full bug report,
with preprocessed source if appropriate.
See <http://gcc.gnu.org/bugs.html> for instructions.
INFO: Elapsed time: 2614.056s, Critical Path: 113.01s
INFO: 3861 processes: 3861 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
"
26801,tf.keras.losses.SparseCategoricalCrossentropy bug,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):pip
- TensorFlow version (use command below):2.0.0-alpha0 , cpu version
- Python version:3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


**Describe the current behavior**
try the demo in  tf.keras.losses.SparseCategoricalCrossentropy code annotation:
  cce = tf.keras.losses.SparseCategoricalCrossentropy()
  loss = cce([0, 1, 2],  [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]])
  print('Loss: ', loss.numpy()) 

got error:          AttributeError: 'list' object has no attribute 'dtype'

"
26800,saved_model_cli tensorrt convert bug: saved_model_main_op collection and it's related operation was mistakenly pruned.,"Hi, guys,

Tensorflow serving has released 1.13.0 recently, which adds a support for TF-TRT, and I'm trying to introduce it into our production enviroment.

There is a good introduction for this feature: https://medium.com/tensorflow/optimizing-tensorflow-serving-performance-with-nvidia-tensorrt-6d8a2347869a

According to the post, I have to convert the SavedModel into a TRT-optimized one first with the help of `saved_model_cli`, and then serve it in Tensorflow Serving.

It all goes well with the example the post provides, but it failed in my case. **After I converted my own model and served it in Tensorflow Serving, the server threw 'Failed precondition: Table not initialized.' error.** I searched the related source code and finally figured out what happened.

There is an index_to_string subgraph in my model, which is mainly composed of a HashTableV2 Operation and a InitializeTableV2 Operation. There's also a collection named 'saved_model_main_op' which finally points to the InitializeTableV2 Operation. When TensorFlow Serving loads the SavedModel, it tries to initialize the HashTable via executing the operations in `saved_model_main_op` collection. **But after `saved_model_cli` converted the graph into a TensorRT-Optimized one, The 'saved_model_main_op' collection and its related Operation has been pruned. As a result, tf serving failed to initialize the table.**

This is the partial graph before conversion:

![](https://raw.githubusercontent.com/monklof/assets/master/original-graph-tb.png ""graph before conversion"")

After conversion:

![](https://raw.githubusercontent.com/monklof/assets/master/table_not_initialized_bug.png ""graph after conversion"")


**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7.5
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.13.1
- Python version: 2.7
- Bazel version (if compiling from source): 0.19.2
- GCC/Compiler version (if compiling from source): 4.8.5
- CUDA/cuDNN version: Cuda 9.0; cuDNN 7.3
- GPU model and memory: Tesla V100, 16G

**Describe the current behavior**

as mentioned above.

**Describe the expected behavior**

The `saved_model_main_op` collection and it's related op should be preserved after conversion.

**Code to reproduce the issue**


This is the code for exporting SavedModel:

```python
# -*- coding: utf-8 -*-

import os.path

# This is a placeholder for a Google-internal import.

import tensorflow as tf
import tensorflow.contrib.slim as slim
from nets import resnet_v2
from preprocessing import  vgg_preprocessing as vgg

tf.app.flags.DEFINE_string('checkpoint_dir', '/opt/zhoulinyuan/inception_v4',
                           """"""Directory where to read training checkpoints."""""")
tf.app.flags.DEFINE_string('output_dir', '/tmp/inception_v4_porn_output',
                           """"""Directory where to export inference model."""""")
tf.app.flags.DEFINE_integer('model_version', 4,
                            """"""Version number of the model."""""")
tf.app.flags.DEFINE_integer('image_size', 224,
                            """"""Needs to provide same value as in training."""""")
FLAGS = tf.app.flags.FLAGS

NUM_CLASSES = 3
NUM_TOP_CLASSES = 3

def export():
  # Create index->synset mapping
  synsets = []

  with tf.Graph().as_default():
    # Build inference model.
    # Please refer to Tensorflow inception model for details.

    # Input transformation.
    serialized_tf_example = tf.placeholder(tf.string, name='tf_example')
    feature_configs = {
        'image/encoded': tf.FixedLenFeature(
            shape=[], dtype=tf.string),
    }
    tf_example = tf.parse_example(serialized_tf_example, feature_configs)
    jpegs = tf_example['image/encoded']
    images = tf.map_fn(preprocess_image, jpegs, dtype=tf.float32)

    # Run inference.
    # logits, _ = inception_model.inference(images, NUM_CLASSES + 1)

    # Run inference.
    with slim.arg_scope(resnet_v2.resnet_arg_scope()):
      logits, _ = resnet_v2.resnet_v2_50(images, NUM_CLASSES, is_training=False)
    logits = tf.nn.softmax(logits)

    # Transform output to topK result.
    values, indices = tf.nn.top_k(logits, NUM_TOP_CLASSES)

    class_descriptions = ['0_xx', '1_yy', '2_zz']
    class_tensor = tf.constant(class_descriptions)

    table = tf.contrib.lookup.index_to_string_table_from_tensor(class_tensor)
    classes = table.lookup(tf.to_int64(indices))

    saver = tf.train.Saver()
    with tf.Session() as sess:
      # Restore variables from training checkpoints.
      saver.restore(sess, FLAGS.checkpoint_dir)
      
      # keys = sess.graph.get_all_collection_keys()
      sess.graph.clear_collection('resnet_v2_50/_end_points')

      # Export inference model.
      output_path = os.path.join(
          tf.compat.as_bytes(FLAGS.output_dir),
          tf.compat.as_bytes(str(FLAGS.model_version)))
      print 'Exporting trained model to', output_path
      builder = tf.saved_model.builder.SavedModelBuilder(output_path)

      # Build the signature_def_map.
      classify_inputs_tensor_info = tf.saved_model.utils.build_tensor_info(
          serialized_tf_example)
      classes_output_tensor_info = tf.saved_model.utils.build_tensor_info(
          classes)
      scores_output_tensor_info = tf.saved_model.utils.build_tensor_info(values)

      classification_signature = (
          tf.saved_model.signature_def_utils.build_signature_def(
              inputs={
                  tf.saved_model.signature_constants.CLASSIFY_INPUTS:
                      classify_inputs_tensor_info
              },
              outputs={
                  tf.saved_model.signature_constants.CLASSIFY_OUTPUT_CLASSES:
                      classes_output_tensor_info,
                  tf.saved_model.signature_constants.CLASSIFY_OUTPUT_SCORES:
                      scores_output_tensor_info
              },
              method_name=tf.saved_model.signature_constants.
              CLASSIFY_METHOD_NAME))

      predict_inputs_tensor_info = tf.saved_model.utils.build_tensor_info(jpegs)
      prediction_signature = (
          tf.saved_model.signature_def_utils.build_signature_def(
              inputs={'images': predict_inputs_tensor_info},
              outputs={
                  'classes': classes_output_tensor_info,
                  'scores': scores_output_tensor_info
              },
              method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME
          ))

      legacy_init_op = tf.group(
          tf.tables_initializer(), name='legacy_init_op')
      builder.add_meta_graph_and_variables(
          sess, [tf.saved_model.tag_constants.SERVING],
          signature_def_map={
              'predict_images':
                  prediction_signature,
              tf.saved_model.signature_constants.
              DEFAULT_SERVING_SIGNATURE_DEF_KEY:
                  classification_signature,
          },
          legacy_init_op=legacy_init_op)

      builder.save()
      print 'Successfully exported model to %s' % FLAGS.output_dir


def preprocess_image(image_buffer):
  """"""Preprocess JPEG encoded bytes to 3D float Tensor.""""""

  # Decode the string as an RGB JPEG.
  # Note that the resulting image contains an unknown height and width
  # that is set dynamically by decode_jpeg. In other words, the height
  # and width of image is unknown at compile-time.
  image = tf.image.decode_jpeg(image_buffer, channels=3)
  # image = vgg._aspect_preserving_resize(image, vgg._RESIZE_SIDE_MAX)
  image = vgg._aspect_preserving_resize(image, vgg._RESIZE_SIDE_MIN)
  image = vgg._central_crop([image], FLAGS.image_size, FLAGS.image_size)[0]
  image.set_shape([FLAGS.image_size, FLAGS.image_size, 3])
  image = tf.to_float(image)
  image = vgg._mean_image_subtraction(image, [vgg._R_MEAN, vgg._G_MEAN, vgg._B_MEAN])
  return image


def main(unused_argv=None):
  export()


if __name__ == '__main__':
  tf.app.run()

```

the conversion command:

```
python /usr/lib/python2.7/site-packages/tensorflow/python/tools/saved_model_cli.py convert --dir /INPUTPATH/ --output_dir /OUTPATH/ --tag_set serve  tensorrt --max_workspace_size_bytes 1073741824 --max_batch_size 224 --precision_mode FP32 --is_dynamic_op True  --minimum_segment_size 10
```

**Other info / logs**

$ tensorflow_model_server --port=8413 --rest_api_port=8414 --model_name=resnet --model_base_path=/workdir/tmp/trttest_pb6/  
2019-03-14 23:14:17.626533: I tensorflow_serving/model_servers/server.cc:82] Building single TensorFlow model file config:  model_name: resnet model_base_path: /workdir/tmp/trttest_pb6/
2019-03-14 23:14:17.626858: I tensorflow_serving/model_servers/server_core.cc:461] Adding/updating models.
2019-03-14 23:14:17.626876: I tensorflow_serving/model_servers/server_core.cc:558]  (Re-)adding model: resnet
2019-03-14 23:14:17.727170: I tensorflow_serving/core/basic_manager.cc:739] Successfully reserved resources to load servable {name: resnet version: 1}
2019-03-14 23:14:17.727195: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: resnet version: 1}
2019-03-14 23:14:17.727208: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: resnet version: 1}
2019-03-14 23:14:17.727228: I external/org_tensorflow/tensorflow/contrib/session_bundle/bundle_shim.cc:363] Attempting to load native SavedModelBundle in bundle-shim from: /workdir/tmp/trttest_pb6/1
2019-03-14 23:14:17.727242: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /workdir/tmp/trttest_pb6/1
2019-03-14 23:14:17.878579: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }
2019-03-14 23:14:18.977544: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: name: Tesla V100-PCIE-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.38 pciBusID: 0000:05:00.0 totalMemory: 15.78GiB freeMemory: 15.36GiB
2019-03-14 23:14:18.977591: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2019-03-14 23:14:19.762198: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-03-14 23:14:19.762240: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0
2019-03-14 23:14:19.762248: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N
2019-03-14 23:14:19.762739: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14843 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:05:00.0, compute capability: 7.0)
2019-03-14 23:14:20.050500: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:182] Restoring SavedModel bundle.
2019-03-14 23:14:20.050589: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:192] The specified SavedModel has no variables; no checkpoints were restored. File does not exist: /workdir/tmp/trttest_pb6/1/variables/variables.index
2019-03-14 23:14:20.050607: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:285] SavedModel load for tags { serve }; Status: success. Took 2323360 microseconds.
2019-03-14 23:14:20.050644: I tensorflow_serving/servables/tensorflow/saved_model_warmup.cc:101] No warmup data file found at /workdir/tmp/trttest_pb6/1/assets.extra/tf_serving_warmup_requests
2019-03-14 23:14:20.050759: I tensorflow_serving/core/loader_harness.cc:86] Successfully loaded servable version {name: resnet version: 1}
2019-03-14 23:14:20.081590: I tensorflow_serving/model_servers/server.cc:313] Running gRPC ModelServer at 0.0.0.0:8413 ...
[warn] getaddrinfo: address family for nodename not supported
2019-03-14 23:14:20.090390: I tensorflow_serving/model_servers/server.cc:333] Exporting HTTP/REST API at:localhost:8414 ...
[evhttp_server.cc : 237] RAW: Entering the event loop ...
2019-03-14 23:14:29.297322: I external/org_tensorflow/tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:496] Building a new TensorRT engine for map/while/TRTEngineOp_1 with batch size 224
2019-03-14 23:14:29.541322: W external/org_tensorflow/tensorflow/contrib/tensorrt/log/trt_logger.cc:34] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.
2019-03-14 23:14:29.541399: W external/org_tensorflow/tensorflow/contrib/tensorrt/log/trt_logger.cc:34] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.
2019-03-14 23:14:29.541421: W external/org_tensorflow/tensorflow/contrib/tensorrt/log/trt_logger.cc:34] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.
2019-03-14 23:14:31.924073: I external/org_tensorflow/tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:496] Building a new TensorRT engine for TRTEngineOp_0 with batch size 224
2019-03-14 23:14:47.935236: W external/org_tensorflow/tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at lookup_table_op.cc:809 : Failed precondition: Table not initialized.

**My Solution**

I have fixed this bug in my way, and it works in my case, but I'm not sure if it is the correct way to do so. Also, I'd like to contribute to tensorflow, but there are so many versions and branches of tensorflow, which one should I send a Pull-Request to?

The Patch: https://github.com/monklof/tensorflow/pull/1/files

Thanks for checking this issue, I'm looking forward to hearing from you soon.
"
26799,[TF 2.0] Logs from model.fit is not conform to the documentation for Molde with multiple outputs,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- TensorFlow version (use command below): tf-nightly-gpu-2.0-preview

**Describe the current behavior**

I have a model with multiple outputs. Precisely, an object detector which outputs locations and classes of object. The model is compiled as follow:

```
ssd.compile(optimizer=optimizer, 
            loss={'output_1': loc_loss, 'output_2': class_loss},
            loss_weights=[1., 1.]
           )
```

From the documentation, the loss to be optimized should be `loss = loc_loss + class_loss` but
fit outputs the following logs to the console:

```
Epoch 1/5
  37/1000 [>.............................] - ETA: 23:46 - loss: 176724.1514 - output_1_loss: 0.0212 - output_2_loss: 0.0449
```

Clearly, the value of `loss` is not the sum of the two losses.

Is it a real bug or a misunderstanding on the meaning of `output_1_loss` and `output_2_loss` ?
"
26797,dilated convolution leads to incorrect graph optimization,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):archlinux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:n/a
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):b'v1.13.1-0-g6612da8' 1.13.1
- Python version:3.7
- Bazel version (if compiling from source):n/a
- GCC/Compiler version (if compiling from source):n/a
- CUDA/cuDNN version: 10.0 / 7.5.0
- GPU model and memory: GTX 1080Ti

The following code:
```python
import numpy as np
import tensorflow as tf

x = tf.placeholder(name='input', shape=[1, 1024, None, None], dtype=tf.float32)

hidden = tf.layers.Conv2D(256, 3, activation=tf.nn.relu, dilation_rate=2, data_format='channels_first', padding='SAME')(x)

label_logits = tf.layers.Conv2D(15, 1, data_format='channels_first', padding='SAME')(hidden)
shp1 = tf.shape(label_logits)

label_logits = tf.squeeze(label_logits, 0)
shp2 = tf.shape(label_logits)

sess = tf.Session()
sess.run(tf.global_variables_initializer())
with sess.as_default():
    v = np.random.rand(1, 1024, 200, 19)
    print(sess.run([shp1, shp2], feed_dict={x: v}))
```
runs on GPU, and prints:
```
[array([  1,  15, 200,  19], dtype=int32), array([ 15, 256, 256], dtype=int32)]
```

The shape of tensor was changed by a `squeeze`. This is terribly wrong.

The issue was originally reported at https://github.com/tensorpack/tensorpack/issues/1110

![0316-23:43:26](https://user-images.githubusercontent.com/1381301/54486423-57883b80-4845-11e9-8137-c48c20aec743.png)
"
26794,1.13 ImportError when importing tf.contrib,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6.7
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: 10.1
- GPU model and memory:  GeForce 940MX, 4GB

**Describe the current behavior**
When I import tf.contrib I get the following Error:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/julian/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/contrib/__init__.py"", line 40, in <module>
    from tensorflow.contrib import distribute
  File ""/home/julian/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/contrib/distribute/__init__.py"", line 33, in <module>
    from tensorflow.contrib.distribute.python.tpu_strategy import TPUStrategy
  File ""/home/julian/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/tpu_strategy.py"", line 27, in <module>
    from tensorflow.contrib.tpu.python.ops import tpu_ops
  File ""/home/julian/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/contrib/tpu/__init__.py"", line 73, in <module>
    from tensorflow.contrib.tpu.python.tpu.keras_support import tpu_model as keras_to_tpu_model
  File ""/home/julian/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py"", line 62, in <module>
    from tensorflow.contrib.tpu.python.tpu import tpu
  File ""/home/julian/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/tpu.py"", line 24, in <module>
    from tensorflow.contrib.compiler import xla
  File ""/home/julian/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/contrib/compiler/xla.py"", line 28, in <module>
    from tensorflow.python.estimator import model_fn as model_fn_lib
  File ""/home/julian/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/estimator/model_fn.py"", line 26, in <module>
    from tensorflow_estimator.python.estimator import model_fn
ImportError: cannot import name 'model_fn'

**Describe the expected behavior**

**Code to reproduce the issue**
`python -c ""import tensorflow.contrib""`

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26793,@tf.function doesn't compile functions specified as parameters to other functions,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab.
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.0.0-alpha0
- Python version: 3.6.7 (default, Oct 22 2018, 11:32:17) \n[GCC 8.2.0]
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: ??
- GPU model and memory: ??

**Describe the current behavior**

Code annotated with `@tf.function` behaves differently depending on whether functions are called and captured as variables, or called and passed directly to other functions. 

In particular, it seems the autograph magic doesn't get applied to functions that are only called within the parameter list of other functions.

I may be misinterpreting what exactly is going wrong, but certainly the behavior shown in the colab below is incorrect.

**Describe the expected behavior**

Capturing via an intermediate variable should never change code behavior.

**Code to reproduce the issue**

https://colab.research.google.com/drive/1CcWfHnGkFehUN8LYsbODf_fNSsQncE0G
"
26792,Tensorflow Keras not adding trainable variable to Model in TF 1.13 (works in TF 1.12),"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13
- Python version: 3.6 (also tried 3.7)
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: GeForce 840M 8gigs ram


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
I am trying to add a trainable variable y to my model. The following code adds a trainable variable y to my model in TF 1.12. It does not work in TF 1.13

    y = K.variable([0.0], dtype=tf.float32, name='y')
    add_y = Lambda(lambda x: tf.math.add(x,y))
    add_y.trainable_weights.append(y)

**Describe the expected behavior**
Model should be updated with the variable y as a trainable weight

**Code to reproduce the issue**

    import tensorflow as tf
    import tensorflow.keras.backend as K
    from tensorflow.keras import Model
    from tensorflow.keras.layers import Input, Lambda
    inputs = Input(shape=(1,))
    y = K.variable([0.0], dtype=tf.float32, name='x')
    add_y = Lambda(lambda x: tf.math.add(x,y))
    add_y.trainable_weights.append(y)
    outputs = add_y(inputs)
    model = Model(inputs=inputs, outputs=outputs)
    model.summary()

Model.summary() shows no trainable variables
**Other info / logs**
NONE
"
26791,Tensorflow Keras get_trainable_weights.append not working in TF 1.13,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13
- Python version: 3.6 (also tried 3.7)
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: GeForce 840M 8gigs ram


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
I am trying to add a trainable variable y to my model. The following code adds a trainable variable y to my model in TF 1.12. It does not work in TF 1.13
Currently the operation 
    y = K.variable([0.0], dtype=tf.float32, name='y')
    add_y = Lambda(lambda x: tf.math.add(x,y))
    add_y.trainable_weights.append(y)

**Describe the expected behavior**
Model should be updated with the variable y as a trainable weight

**Code to reproduce the issue**
    import tensorflow as tf
    import tensorflow.keras.backend as K
    from tensorflow.keras import Model
    from tensorflow.keras.layers import Input, Lambda
    inputs = Input(shape=(1,))
    y = K.variable([0.0], dtype=tf.float32, name='x')
    add_y = Lambda(lambda x: tf.math.add(x,y))
    add_y.trainable_weights.append(y)
    outputs = add_y(inputs)
    model = Model(inputs=inputs, outputs=outputs)
    model.summary()

Model.summary() shows no trainable variables
**Other info / logs**
NONE
"
26790,Failed to load native Tensorflow Runtime - Nvidia MX150 Python 3.7,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10 64bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip install through anaconda prompt
- TensorFlow version: 1.13.1
- Python version: 3.7.2
- Installed using virtualenv? pip? conda?: pip install 
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1
- GPU model and memory: Nvidia GeForce MX150



**Describe the problem**

Installed Tensorflow, CUDA, CUDNN, tried to import tensorflow as tf, got the follow message.
```
(tensorflow) C:\Users\Shaurya>python
Python 3.7.2 (default, Feb 21 2019, 17:35:59) [MSC v.1915 64 bit (AMD64)] :: Anaconda, Inc. on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\Shaurya\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Shaurya\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Shaurya\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Shaurya\AppData\Local\conda\conda\envs\tensorflow\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Shaurya\AppData\Local\conda\conda\envs\tensorflow\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Shaurya\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Shaurya\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Shaurya\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Shaurya\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Shaurya\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Shaurya\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Shaurya\AppData\Local\conda\conda\envs\tensorflow\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Shaurya\AppData\Local\conda\conda\envs\tensorflow\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```

**Any other info / logs**
Haven't installed Keras yet. I activated tensorflow, which happened and then tried the same (as shown in the result above, but that didn't help either.

I tried this before installing CUDNN and then after installing it as well, again to no avail."
26789,[TF2.0] Optimizer for Linear models,"
**System information**
- TensorFlow version: 2.0.0-alpha0
- Doc Link: 
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/estimator/LinearEstimator
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/estimator/LinearClassifier
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/estimator/LinearRegressor


**Describe the documentation issue**

The examples in these documents use tf.train.FtrlOptimizer which is not available in TF2.0.
```python
# Or estimator using the FTRL optimizer with regularization.
estimator = LinearRegressor(
    feature_columns=[categorical_column_a,
                     categorical_feature_a_x_categorical_feature_b],
    optimizer=tf.train.FtrlOptimizer(
      learning_rate=0.1,
      l1_regularization_strength=0.001
    ))
```

I tried to use tf.optimizers.Ftrl instead, but it gives the following error:

```python
# Or estimator using the FTRL optimizer with regularization.
estimator = LinearRegressor(
    feature_columns=[categorical_column_a,
                     categorical_feature_a_x_categorical_feature_b],
    optimizer=tf.optimizers.Ftrl(
      learning_rate=0.1,
      l1_regularization_strength=0.001
    ))

ValueError: The given object is not an Optimizer instance. Given: <tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object
at 0x1293a32b0>
```

I think the documentation should be updated, but I am not sure the correct way of defining optimizer. Could please someone take a look at this?

Thanks,"
26788,[TF 2.0] Keras in graph mode crashes when fitting data with L2 regularizer set to 0.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **YES**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Debian Stable**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): **CPU, both TF-2.0.0a0 and tf-nightly-2.0-preview-2.0.0.dev20190315**
- Python version: **3.5.3**
- Bazel version (if compiling from source): **N/A**
- GCC/Compiler version (if compiling from source): **N/A**
- CUDA/cuDNN version: **N/A**
- GPU model and memory: **N/A**


**Describe the current behavior**
The below code (using zero L2 regularization) fails with
```
AttributeError: Tensor.graph is meaningless when eager execution is enabled.
```

**Describe the expected behavior**
The code runs without a failure.

**Code to reproduce the issue**
```python
import numpy as np
import tensorflow as tf

inputs = np.arange(10)
outputs = 2 * inputs

l2 = tf.keras.regularizers.L1L2(l2=0.0)
model = tf.keras.Sequential(
    [tf.keras.layers.Dense(1, input_shape=[1], kernel_regularizer=l2)]
)
model.compile(
    optimizer=tf.keras.optimizers.Adam(),
    loss=tf.keras.losses.MeanSquaredError(),
    metrics=[tf.keras.metrics.MeanSquaredError()]
)
model.fit(inputs, outputs)
```

**Other info / logs**
- Running with `model.run_eagerly` works.
- Running with non-zero L2 regularization works.

The problem is in https://github.com/tensorflow/tensorflow/blob/b57c7d71eff5914a503d15130cb90a240b3bcf40/tensorflow/python/keras/engine/network.py#L651
where a `ops.get_default_graph()` is called.

The problem is probably connected to the fact that L1L2 regularizer returns an explicit 0 (`K.constant(0.)`) when no l1/l2 is set (while otherwise summing weights and applying l1/l2)."
26786,[tflite][java] Interpreter.reset_all_variables() is not supported on Android.,"`Interpreter` does not support `reset_all_variables()` in the current Android API of TF Lite.

This function is critical for RNNs because without it all the batches after the first one produce wrong results.

**System information**
- OS Platform and Distribution: Linux Ubuntu 18.04
- Mobile device: Pixel 3 XL
- TensorFlow installed from: binary
- TensorFlow version: v1.12.0-9708-gafab5b3 1.14.1-dev20190306
- Python version: Python 3.7.1"
26784,"[TF 2.0] Respect masking in Keras loss reduction (i.e., support an equivalent of default TF 1.0 loss reduction SUM_OVER_NONZERO_WEIGHTS)","**System information**
- TensorFlow version (you are using): **TF 2.0.0a0**
- Are you willing to contribute it (Yes/No): **Yes**

**Describe the feature and the current behavior/state.**

In TF 1.0, default loss reduction was `SUM_OVER_NONZERO_WEIGHTS`. For NLP with sequences on inputs, it normalized losses by number of _valid elements_ (i.e., sum of non-padding words in input sentences).

With Keras API, default reduction `SUM_OVER_BATCH_SIZE` does not respect masking, so if a batch of sequences is passed on input, it is normalized by total batch size including padding (masked) elements. No reduction in Keras is available which would recover the previous TF 1.0 default `SUM_OVER_NONZERO_WEIGHTS` reduction.

**Will this change the current api? How?**

My proposal is to respect masking in the `SUM_OVER_BATCH_SIZE` reduction. If a mask is set, then `BATCH_SIZE` should probably correspond to the number of unmasked elements anyway.

Alternatively, a new reduction `SUM_OVER_MASKED_BATCH_SIZE` could be added.

**Who will benefit with this feature?**

I think anyone using masked losses, especially if _upgrading from TF 1_."
26783,tf.Tensor documentation in TF2,"**System information**
- TensorFlow version: 2.0 alpha
- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/Tensor


**Describe the documentation issue**

The documentation of `tf.Tensor` in the 2.0 section is still about the `tf.Tensor` as a symbolic tensor that contains the result of a  `tf.Operation` and it states that the only way to get its value is to use a `tf.Session` to run the node.

This is no more the truth in tf2:

- `tf.Session` is no more available
- `tf.Tensor` holds the value of the computation and we can extract it using `.numpy()`.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**

Yes, I can submit a PR if needed - in the PR I can remove any reference to `tf.Session` and replace the introduction, explaining what is a `tf.Tensor` in Tensorflow 2.0."
26781,Confusing documentation for  to tf.Graph in Tensorflow 2,"**System information**

> tensorflow-estimator-2.0-preview==1.14.0.dev2019031500
> tensorflow-probability==0.5.0

- Doc Link : https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/Graph

**Describe the documentation issue**
I believe for TF2 these docs can be more explicit that tf.Graph should not be used directly, and instead provide guidance similar to what Alex does in his talk
https://www.youtube.com/watch?v=Up9CvRLIIIw

The wording that `A default Graph is always registered` is confusing in TF2 verbiage. and I believe the warning that the graph will not be executed eagerly should also be made more prominent, potentially by moving it up.

The confusion compounds because after using this method the user is left with a `tf.Tensor` but we're advised not to use `tf.Session` so it's not entirely clear what should be done next to evaluate the graph.

Some of my confusion is visible in this thread (Big thanks to @brianwa84 for the help)
https://github.com/pymc-devs/pymc4/pull/93#issuecomment-473490764 for reference

Thanks in advance!

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
If I can get some help on how to word things correctly then yes
"
26780,bazel  quantize:quantize_graph ERROR:quantize_graph failed to build,"**System information**
- Ubuntu 18.04
download from https://github.com/tensorflow/tensorflow/
- Python version: 3.6
- Bazel version:0.23.2
- CUDA/cuDNN version:cuda10/cudnn7

**Describe the problem**
bazel build tensorflow/contrib/quantize:quantize_graph

`ERROR: /home/nnir712/project/tensorflow-master/tensorflow/core/grappler/costs/BUILD:281:1: C++ compilation of rule '//tensorflow/core/grappler/costs:op_level_cost_estimator' failed (Exit 1)
In file included from ./tensorflow/core/grappler/costs/op_level_cost_estimator.h:19:0,
                 from tensorflow/core/grappler/costs/op_level_cost_estimator.cc:16:
./tensorflow/core/grappler/costs/cost_estimator.h:58:52: error: 'INFINITY' was not declared in this scope
              double intermediate_read_gb_per_sec = INFINITY,
                                                    ^~~~~~~~
./tensorflow/core/grappler/costs/cost_estimator.h:59:53: error: 'INFINITY' was not declared in this scope
              double intermediate_write_gb_per_sec = INFINITY)
                                                     ^~~~~~~~
./tensorflow/core/grappler/costs/cost_estimator.h: In constructor 'tensorflow::grappler::DeviceInfo::DeviceInfo()':
./tensorflow/core/grappler/costs/cost_estimator.h:46:17: error: 'INFINITY' was not declared in this scope
       : gigaops(INFINITY),
                 ^~~~~~~~
tensorflow/core/grappler/costs/op_level_cost_estimator.cc: In member function 'virtual tensorflow::grappler::DeviceInfo tensorflow::grappler::OpLevelCostEstimator::GetDeviceInfo(const tensorflow::DeviceProperties&) const':
tensorflow/core/grappler/costs/op_level_cost_estimator.cc:442:39: error: call to 'tensorflow::grappler::DeviceInfo::DeviceInfo(double, double, double, double)' uses the default argument for parameter 3, which is not yet defined
   return DeviceInfo(gflops, gb_per_sec);
                                       ^
tensorflow/core/grappler/costs/op_level_cost_estimator.cc:442:39: error: call to 'tensorflow::grappler::DeviceInfo::DeviceInfo(double, double, double, double)' uses the default argument for parameter 4, which is not yet defined
In file included from ./tensorflow/core/platform/default/logging.h:24:0,
                 from ./tensorflow/core/platform/logging.h:25,
                 from ./tensorflow/core/lib/core/status.h:25,
                 from ./tensorflow/core/grappler/costs/cost_estimator.h:19,
                 from ./tensorflow/core/grappler/costs/op_level_cost_estimator.h:19,
                 from tensorflow/core/grappler/costs/op_level_cost_estimator.cc:16:
./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetFilterDim(tensorflow::gtl::ArraySlice<T>, tensorflow::FilterTensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':
./tensorflow/core/util/tensor_format.h:467:54:   required from here
./tensorflow/core/util/tensor_format.h:441:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   CHECK(index >= 0 && index < dimension_attribute.size())
./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
./tensorflow/core/util/tensor_format.h:441:3: note: in expansion of macro 'CHECK'
   CHECK(index >= 0 && index < dimension_attribute.size())
   ^
Target //tensorflow/contrib/quantize:quantize_graph failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1340.504s, Critical Path: 68.45s
INFO: 1816 processes: 1816 local.
`"
26778,[TF2.0] Possibly wrong path to decode_predictions in keras,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version: 3.6.5_1/2.7.15
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: -
- GPU model and memory: -


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

Currently, Mobilenet_V2 Keras in `tensorflow/python/keras/applications/mobilenet_v2.py` references the function `decode_predictions` from `keras_applications.mobilenet_v2` [here](https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet_v2.py)
 
**Describe the expected behavior**

`decode_predictions` seems to be from `keras_applications.imagenet_utils` [here](https://github.com/keras-team/keras-applications/blob/master/keras_applications/imagenet_utils.py) instead. 

**Code to reproduce the issue**

Provide a reproducible test case that is the bare minimum necessary to generate the problem.
NA

**Other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
NA"
26777,How do I allow growth with eager execution?,"This is a generic question.   I am working locally from a copy of [a Colab notebook](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb), which uses eager execution.

I notice that the notebook allocates all of the memory on my GPU right away.  So I can track what commands use what memory, I would like it to not do that.  According to [this StackOverflow article:](https://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory) I should do this in the code before it grabs all the memory:

```
config = tf.ConfigProto()
config.gpu_options.allow_growth=True
sess = tf.Session(config=config)
```

However, I notice that with eager execution, there is no Session.

So question/bug/feature request: How do I allow growth with eager execution?
"
26776,Improve testing infrastructure to prevent breaking open source builds,"Recently pushes to the master branch done by tensorflow gardener frequently breaking the open source builds from the master and that is hindering the development on top of the tree by requested/suggested by google engineers through various configurations. Would it be possible to improve the testing of internal to external merges to make sure that opensource builds are not broken by a push from internal repositories?.

For example a push made today left master in broken state. In line https://github.com/tensorflow/tensorflow/blob/9a43dfeac58477c37cb356e3759b053d2bbd0247/tensorflow/tensorflow.bzl#L2189
some environment variables are referred however bazel clears the environment for genrules, we end up with an error message like below and can't build TF right now. It is possible to fix such issues but it likely to cost time for multiple people. Would it be possible to add a test that does a build with the instructions at [installation/build from source](https://www.tensorflow.org/install/source) still works.

`   (cd /ssdscratch/.cache/bazel/_bazel_skama/184fadfe649ebd4ee26f07bbb482f004/execroot/org_tensorflow && \
  exec env - \
    PATH=/ssdscratch/work/XLAInt8/tensorflow/build/bin:/home/skama/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games \
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/tensorflow/tools/build_info/gen_build_info --raw_generate ""bazel-out/host/genfiles/tensorflow/python/platform/build_info.py"" --build_config cuda --key_value  cuda_version_number=${TF_CUDA_VERSION} cudnn_version_number=${TF_CUDNN_VERSION} ')
Execution platform: @bazel_tools//platforms:host_platform
/bin/bash: TF_CUDA_VERSION: unbound variable
Target //tensorflow/tools/pip_package:build_pip_package failed to build
`"
26771,LSTM simple example ,"Hello,
I'm writing a simple LSTM function that executes the following operations on a random input x:

i{t}=sigma(W{i}x{t}+R{i}h{t-1}+b{i})
f{t}=sigma(W{f}x{t}+R{f}h{t-1}+b{f})
o{t}=sigma(W{o}x{t}+R{o}h{t-1}+b{o})
c'{t}=tanh(W{c}x{t}+R{c}h{t-1}+b{c})
c{t}=f{t} o c{t-1}+i{t} o c'{t}
h{t}=o{t} o tanh(c{t})

To test the performance of my code I want to use Tensorflow XLA. With all the examples I found, I could not create a simple function as this ( I couldn't find out how to initialize a random size input with random values, initialize weights and define the output size)
Could anyone help me implement this function using Tensorflow XLA ?
"
26769,Failed to run optimizer ArithmeticOptimizer,"I am trying to build a graph which uses a conv2dTranspose using keras layers. I ran into following error:

Here is a sample code which reproduces same error: 
```
import numpy as np
from tensorflow.keras.layers import Conv2D, Conv2DTranspose
from tensorflow.keras.layers import Input
from tensorflow.keras.models import Model

a = Input(shape=(512, 16, 16))
b = Conv2DTranspose(filters=192, kernel_size=2, strides=2, padding='same', data_format=""channels_first"")(a)
model = Model(inputs=a, outputs=b)
inp = np.random.rand(10, 512, 16, 16)
model.predict(inp)
```

```2019-03-16 03:22:24.181247: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node conv2d_transpose/strided_slice_3. Error: Pack node (conv2d_transpose/stack) axis attribute is out of bounds: 0
2019-03-16 03:22:24.181306: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node conv2d_transpose/strided_slice_4. Error: Pack node (conv2d_transpose/stack) axis attribute is out of bounds: 0
2019-03-16 03:22:24.181334: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node conv2d_transpose/strided_slice_5. Error: Pack node (conv2d_transpose/stack) axis attribute is out of bounds: 0
2019-03-16 03:22:24.181379: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node conv2d_transpose/strided_slice_6. Error: Pack node (conv2d_transpose/stack) axis attribute is out of bounds: 0
2019-03-16 03:22:24.186380: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node conv2d_transpose/strided_slice_3. Error: Pack node (conv2d_transpose/stack) axis attribute is out of bounds: 0
2019-03-16 03:22:24.186435: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node conv2d_transpose/strided_slice_4. Error: Pack node (conv2d_transpose/stack) axis attribute is out of bounds: 0
2019-03-16 03:22:24.186469: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node conv2d_transpose/strided_slice_5. Error: Pack node (conv2d_transpose/stack) axis attribute is out of bounds: 0
2019-03-16 03:22:24.186501: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node conv2d_transpose/strided_slice_6. Error: Pack node (conv2d_transpose/stack) axis attribute is out of bounds: 0
```
Can anyone explain cause of this error?"
26766,"Tensorflow 2.0 Alpha, tf.concat, ConcatV2 requires tf.double input under eager mode","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): 
```
conditions = tf.keras.layers.Lambda(
            lambda x: tf.concat(x, axis=-1)
        )(self.condition_inputs)
```
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac
device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.0.0a0
- Python version: 3.6.5

**Describe the current behavior**
```
tensorflow.python.framework.errors_impl.InvalidArgumentError: cannot compute ConcatV2 as input #1(zero-based) was expected to be a double tensor but is a float tensor [Op:ConcatV2] name: concat
```

original code

```
get_values([observations_np, actions_np])
```
would casue the above error under eager. However, there is no error without eager.

But, if force the numpy inputs to be tf.double tf Variable undr eager mode, it can go through:

```
get_values([tf.Variable(observations_np, dtype=tf.double), tf.Variable(actions_np, dtype=tf.double)])
``

"
26764,Build Error For a CPU-only build on macOS Mojave ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution: macOS Mojave 10.14.3
- Mobile device: NA
- TensorFlow installed from (source or binary): trying to build from source (build issue)
- TensorFlow version: r1.12
- Python version: Python 3.6.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:42:37)
- Installed using virtualenv? pip? conda?: trying to build in a conda environment
- Bazel version (if compiling from source): bazel-0.15-darwin-x86_64
- GCC/Compiler version (if compiling from source): LLVM version 10.0.0 (clang-1000.11.45.5)
- CUDA/cuDNN version: not installed (CPU-only build)
- GPU model and memory: Radeon R9 M370X 2048 (CPU-only build)

**Describe the problem**

It seems like `toUpper` is defined twice. Once as a function in STL in and once as a macro in Python headers (`pyport.h`). There are other similar errors for `tolower`, `isspace` and such. See the log for more details.

Steps to reproduce the problem:
1. Check out the r1.12 branch of Tensorflow from Github.
2. Create a conda environment with Python 3.6.5 and activate it.
3. Pip install the dependencies: pip six numpy wheel mock keras_applications keras_preprocessing as described [here](https://www.tensorflow.org/install/source).
4. Install bazel 0.15 from [here](https://github.com/bazelbuild/bazel/releases/tag/0.15.0).
5. Navigate to the Tensorflow root folder and `./configure`.
6. Use the default values for python and dist-packages (first two questions).
7. Answer No to remaining questions during configuration.
8. `bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`.
9. Wait ~2 hours for bazel to build ~7400 files.

You should see a similar error message (and other similar errors) to the following:

`ERROR: /Users/siavash/clarius/tensorflow/tensorflow/python/eager/BUILD:10:1: C++ compilation of rule '//tensorflow/python/eager:pywrap_tfe_lib' failed (Exit 1)
In file included from tensorflow/python/eager/pywrap_tfe_src.cc:18:
In file included from ./tensorflow/python/eager/pywrap_tfe.h:22:
In file included from ./tensorflow/core/lib/core/status.h:23:
In file included from bazel-out/darwin-opt/genfiles/tensorflow/core/lib/core/error_codes.pb.h:9:
In file included from external/protobuf_archive/src/google/protobuf/stubs/common.h:39:
In file included from /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/iostream:38:
In file included from /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/ios:216:
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/__locale:518:15: error: C++ requires a type specifier for all declarations
    char_type toupper(char_type __c) const
              ^
bazel-out/darwin-opt/genfiles/external/local_config_python/python_include/pyport.h:706:29: note: expanded from macro 'toupper'
#define toupper(c) towupper(btowc(c))
`

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

[error.log](https://github.com/tensorflow/tensorflow/files/2972514/error.log)
"
26763,[TF2.0] Embedding batch_input_shape not aware of distribute.MirroredStrategy(),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes, modified an [example from Seedbank](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/sequences/text_generation.ipynb#scrollTo=MtCrdfzEI2N0) to use with TF2.0
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip binary
- TensorFlow version (use command below): ` 2.0.0-alpha0`; `git v1.12.0-9492-g2c319fb415`
- Python version: 3.6
- Bazel version (if compiling from source): NIL
- GCC/Compiler version (if compiling from source): NIL
- CUDA/cuDNN version: 10.0
- GPU model and memory: V100 16GB

**Describe the current behavior**

1. when `batch_input_shape` is **not** specified in `tf.keras.layers.Embedding`
With or without `distribute.MirroredStrategy()`, model works perfectly fine
2. when `batch_input_shape` **is** specified in `tf.keras.layers.Embedding`
Without `distribute.MirroredStrategy()`, model works perfectly fine, But with `distribute.MirroredStrategy()`, **`tf.data.Dataset` splits the inputs to the model correctly, but the model's expected input is not correct**

* The model replicas **each** expect `batchsize_per_replica * replica` (the un-split output from `Dataset`) instead of  `batchsize_per_replica` (split output from `Dataset`).
* If the model's `batch_input_shape` or `Dataset` output is adjusted to match the above expectation, the keras Model immediately errors out as it expects `batchsize_per_replica * replica`, split to `batchsize_per_replica` as an input to **each** model replica.

""Illustrated Example"":
```
> batchsize = 4*128
> batchsize_per_replica = 128
> model batch_input_shape = 4*128
> model replica expects 4*128 causing error

> batchsize = 4*128
> batchsize_per_replica = 128
> model batch_input_shape = 128
> model expects 4*128 causing error
```

It seems like the problem is everywhere else, the batch size etc. is calculated correctly, except when `batch_input_shape` is specified in `tf.keras.layers.Embedding`. If anyone is wondering why we need to specify this, this is for stateful RNNs to work.

**Describe the expected behavior**

When the scope is `distribute.MirroredStrategy()`, `tf.keras.layers.Embedding` specified `batch_input_shape` should also be divided by the number of replicas.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

The notebook below contains code that will reproduce the error on a **multi-GPU** system. On Colab, there is only one GPU hence it runs fine, since `distribute.MirroredStrategy()` only creates one replica. However, with two or more replicas on a multi-GPU system, the error is observed.

[Notebook presented on Google Colab](https://colab.research.google.com/drive/1R3h2Jf9rKCsi952KLcg7b8PtPyyXgx6b)

There is a header/section that shows the model training with and without `distribute.MirroredStrategy()`. 

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Following the Colab notebook above, when training is run on a 4 GPU system (overall `batch_size=128`, `batchsize_per_replica=32`), the error is:

```
Invalid input_h shape: [1,128,256] [1,32,256]
	 [[{{node replica_3/unified_lstm_2/CudnnRNN}}]]
	 [[replica_1/loss/dense_1_loss/loss/weighted_loss/broadcast_weights/assert_broadcastable/is_valid_shape/else/_48/has_valid_nonscalar_shape/then/_207/has_invalid_dims/concat/_374]] [Op:__inference_keras_scratch_graph_10991]
```

Notebook that shows the entire run resulting in the above error can be seen [here](https://nbviewer.jupyter.org/github/tlkh/arxiv-lm/blob/master/tf_distributed_embedding_bugreport.ipynb)

If you modify `batch_input_shape` to match for `batchsize_per_replica` (32):

```
ValueError: The batch output shape of your `Dataset` is 128, which is incompatible with the specified batch size of your Input Layer: 32
```

In all cases, `model.summary()` gives the same result:

```
Model: ""sequential_1""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (128, None, 128)          8320      
_________________________________________________________________
unified_lstm_2 (UnifiedLSTM) (128, None, 256)          394240    
_________________________________________________________________
unified_lstm_3 (UnifiedLSTM) (128, None, 256)          525312    
_________________________________________________________________
dense_1 (Dense)              (128, None, 65)           16705     
=================================================================
Total params: 944,577
Trainable params: 944,577
Non-trainable params: 0
```

**The exact same code will run fine on single GPU system even with `distribute.MirroredStrategy()` as the scope**, you can [view Colab demo](https://colab.research.google.com/drive/1R3h2Jf9rKCsi952KLcg7b8PtPyyXgx6b)"
26759,Keras HDF5 with int8/bfloat16 support,"
**System information**
- TensorFlow version (you are using): TensorFlow 1.12
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**

The TensorFlow protobuf saved model format is aware of reduced precision (int8), but I don't think the Keras HDF5 saved format is.  

**Will this change the current api? How?**

I'd like to have a way to convert a Keras HDF5 model to int8. So when I do a `tf.keras.model.predict()` it will use the INT8 operations rather than the FP32.

**Who will benefit with this feature?**

Anyone using the tf.Keras API.

**Any Other info.**
For example, TF Lite can do this:  https://www.tensorflow.org/lite/performance/post_training_quantization
"
26758,tensorflow-gpu using only 10% of my GPU,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 (1809)
- TensorFlow installed from (source or binary): binary (pip install tensorflow-gpu)
- TensorFlow version (use command below): b'v1.10.0-rc1-19-g656e7a2b34' 1.10.0
- Python version: 3.6.8
- CUDA/cuDNN version: 9.0
- GPU model and memory: GTX 960 2GB




**Describe the current behavior**
While trying to train a neural network with my GTX960 after installing tensorflow-gpu, and choosing my GPU with the below code, I can see on the Windows task manager that it's only using about 10% of the GPU, and thus making it way slower than training it with the CPU. 


```
config = tf.ConfigProto()
config.gpu_options.allow_growth = False
config.gpu_options.per_process_gpu_memory_fraction = 1
session = tf.Session(config = config)

with tf.device(""/device:GPU:0""):
    model = Sequential()
    ...
```




**Describe the expected behavior**
The GPU should be used almost if not entirely.

**Code to reproduce the issue**
you'd need the whole neural network code plus the datasets and the rest...

**Other info / logs**
>python predictor_LSTM_all.py
Using TensorFlow backend.
2019-03-15 17:38:18.822616: I T:\src\github\tensorflow\tensorflow\core\platform\cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2019-03-15 17:38:19.109114: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1405] Found device 0 with properties:
name: GeForce GTX 960 major: 5 minor: 2 memoryClockRate(GHz): 1.2785
pciBusID: 0000:01:00.0
totalMemory: 2.00GiB freeMemory: 1.64GiB
2019-03-15 17:38:19.117459: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1484] Adding visible gpu devices: 0
2019-03-15 17:38:19.527190: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-03-15 17:38:19.532341: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:971]      0
2019-03-15 17:38:19.535210: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:984] 0:   N
2019-03-15 17:38:19.538575: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2048 MB memory) -> physical GPU (device: 0, name: GeForce GTX 960, pci bus id: 0000:01:00.0, compute capability: 5.2)
Epoch 1/10
2019-03-15 17:38:21.045228: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1484] Adding visible gpu devices: 0
2019-03-15 17:38:21.049900: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-03-15 17:38:21.054218: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:971]      0
2019-03-15 17:38:21.057053: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:984] 0:   N
2019-03-15 17:38:21.060264: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2048 MB memory) -> physical GPU (device: 0, name: GeForce GTX 960, pci bus id: 0000:01:00.0, compute capability: 5.2)
15709/15709 [==============================] - 39s 2ms/step - loss: 33.3039 - acc: 0.1775

(using the CPU only takes about 7-10 seconds to finish the first epoch while here it shows how it takes almost 40 seconds with the GPU.
"
26755,Request for Leaky Relu quantization support,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:
1.13.1
- **Python version**:
2.7.12
- **Bazel version (if compiling from source)**:
0.21
- **GCC/Compiler version (if compiling from source)**:
5.4.0
- **CUDA/cuDNN version**:
No
- **GPU model and memory**:- *
*Exact command to reproduce**:
tflite_convert --output_file=yolo2.tflite --graph_def_file=yolo2.pb --input_arrays=input_1 --output_arrays=conv2d_23/BiasAdd --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_dev_values=127 --default_ranges_min=0 --default_ranges_max=255

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
While trying to convert yolo-v2 tensorflow model to quantized tflite model, tflite_convert complains that LeakRelu quantization is not supported yet.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

2019-03-15 11:04:15.496254: F tensorflow/lite/toco/graph_transformations/quantize.cc:491] Unimplemented: this graph contains an operator of type LeakyRelu for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).
Aborted

"
26749,TfLite on object detection with QUANTIZED_UINT8.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04.
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: XiaoMi 8.
- **TensorFlow installed from (source or binary)**: pip install tensorflow-gpu==1.9.0
- **TensorFlow version (use command below)**: 1.9.0
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0/7.1
- **GPU model and memory**: enough.
- **Exact command to reproduce**:

### Describe the problem
I train a model with only multi-layer conv+bn+relu/sigmoid using `tf.contrib.quantize.create_training_graph()` and test it with `tf.contrib.quantize.create_eval_graph()`, every seems ok.

Then I freeze the *.ckpt file to *.pb file with `tensorflow.python.tools.freeze_graph.freeze_graph`.
Finally, I convert the *.pb file to *.tflite file with the following code, and again every is ok.

```
    lite8 = tf.contrib.lite.toco_convert(
        input_tensors=[inT],
        output_tensors=[outT],
        input_data=graph.as_graph_def(),
        # default_ranges_stats=(-6.0, 6.0),
        quantized_input_stats=[(127.5, 128.0)],
        # inference_type=tf.contrib.lite.constants.FLOAT,
        inference_type=tf.contrib.lite.constants.QUANTIZED_UINT8,
    )

    with open(lite8Path, 'wb') as txt:
        txt.write(lite8)
```

**Here is the question:**
The output of my model is the concat of 2 tensors, the first is the output of `cnn+bn`, the second one is the output of `cnn+bn+sigmoid`, **both should be type of float32**. However, when I run the quantized model on android phone with the following code, I am force to get output tensor of type of `uint8` rather than 'float32'.

```
        byte outB[][][] = new byte[1][NUM * NUM][2 + 4]; # The output is a tensor of rank 3.
        org.tensorflow.lite.Interpreter.run(input_value, outB); # The outB must be uint8 (byte in android) here.

        float outF[][][] = new Float[1][NUM * NUM][2 + 4];
        org.tensorflow.lite.Interpreter.run(input_value, outF); # The application crash in this line, saying the output dtype is not match.
```

**I wonder how I can get `float32` output rather than `uint8` when using quantized .tflite file ?**
**Thank you very much !!!**

**In short, I have convert a pb file to tflite with uint8 quantization. I would like to know how to convert the output from uint8 back to float so that the previous code (that work on float type output) can still work.**

### Source code / logs
Clear enough, need not to code.
"
26740,run bidirectional_sequence_lstm_test.py error,"clone code from git and cd tensorflow/tensorflow/lite/experimental/examples/lstm dir 
run python bidirectional_sequence_lstm_test.py get errors:

Traceback (most recent call last):
  File ""bidirectional_sequence_lstm_test.py"", line 23, in <module>
    from tensorflow.lite.experimental.examples.lstm.rnn import bidirectional_dynamic_rnn
ImportError: No module named lite.experimental.examples.lstm.rnn"
26739,[doc] use links in deprecation notice,"Many deprecated functions currently contain a notice such as
```
Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use tf.data.experimental.ignore_errors().
```
It would be really nice if the suggested replacement would actually link to the corresponding documentation."
26738,TF 2.0 Keras model utilizing another model with metrics cannot fit/evaluate in graph mode (mistakenly using the metrics of the inner model),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **YES**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Debian Stable**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): **CPU, both TF-2.0.0a0 and tf-nightly-2.0-preview-2.0.0.dev20190315**
- Python version: **3.5.3**
- Bazel version (if compiling from source): **N/A**
- GCC/Compiler version (if compiling from source): **N/A**
- CUDA/cuDNN version: **N/A**
- GPU model and memory: **N/A**

**Describe the current behavior**
When running the script below, it fails during execution with error
```
tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'dense_target' with dtype float and shape [?,?]
```

**Describe the expected behavior**
The code should not crash.

**Code to reproduce the issue**
```python
import numpy as np
import tensorflow as tf

inputs = np.arange(10)
outputs = 2 * inputs

inner_model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=[1])])
inner_model.compile(optimizer=tf.keras.optimizers.Adam(),
                    loss=tf.keras.losses.MeanSquaredError(),
                    metrics=[tf.keras.metrics.MeanSquaredError()])

outer_inputs = tf.keras.layers.Input(shape=[1])
outer_outputs = inner_model(outer_inputs)
outer_model = tf.keras.Model(inputs=outer_inputs, outputs=outer_outputs)
outer_model.compile(optimizer=tf.keras.optimizers.Adam(),
                    loss=tf.keras.losses.MeanSquaredError(),
                    metrics=[tf.keras.metrics.MeanSquaredError()])
outer_model.evaluate(inputs, outputs) # crashes
outer_model.fit(inputs, outputs) # also crashes
```


**Other info / logs**
- `outer_model.predict` works
- when `outer_model.run_eagerly=True`, then `outer_model.{evaluate/fit}` works
- when the inner model has no metrics (`metrics=[]` in `inner_model.compile`), then `outer_model.{evaluate/fit}` works

I have trace the problem to an incorrect FuncGraph generated -- it mistakenly uses the metrics from the _inner graph_ (i.e., the metrics calculations utilize the _original_ placeholders of the inner graph).
"
26736,tflite's TRANSPOSE_CONV is much slower than tfmobile ...,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:Zenfone 5Z
- TensorFlow installed from (source or binary):source
- TensorFlow version (use command below): nightly build 20190314
- Python version:3.6
- Bazel version (if compiling from source):0.21.0
- GCC/Compiler version (if compiling from source):5.4
- CUDA/cuDNN version:not used
- GPU model and memory:not used

**Describe the current behavior**

I measured performance of tf-mobile, tf-lite on Zenfone 5Z  / Snapdragon 845 / Android 8.0 by C++ benchmark model tool (arm64 build), and I find that the speed of tf-lite's TRANSPOSE_CONV is much slower than tf-mobile's one.

I used the attached custom model [models.zip](https://github.com/tensorflow/tensorflow/files/2970561/models.zip)  for benchmark .
The attached tflite is converted by toco_convert from the attached pb file.

**Summary**

The following table shows average computing time of 50 times predict.

|              | Threads|Conv2D     | TransposeConv2D | All        |
|:-------------|--------:|-----------:|----------------:|-----------:|
| TFMobile     | 1|251.561 ms |       35.585 ms | 310.380 ms |
| TFMobile     | 4|190.228 ms |       78.047 ms | 295.469 ms |
| TFMobile     | 16|87.586 ms |       20.264 ms | 122.102 ms |
| TFLite       | 1|294.214 ms |      562.609 ms | 880.674 ms |
| TFLite       | 4|75.783 ms |      560.368 ms | 659.156 ms |
| TFLite       | 16|55.597 ms |      561.441 ms | 641.541 ms |

**TFMobile 1 threads In:1x256x256x3 Performance**
`
benchmark_model --graph=/data/local/tmp/UpResnet5-X2-H256-W256-C3.pb --max_num_runs=50 --num_threads=1 --input_layer_shape=""1,256,256,3""
`
```
native : benchmark_model.cc:469 Graph: [/data/local/tmp/UpResnet5-X2-H256-W256-C3.pb]
native : benchmark_model.cc:470 Init ops:
native : benchmark_model.cc:471 Input layers: [input:0]
native : benchmark_model.cc:472 Input shapes: [1,256,256,3]
native : benchmark_model.cc:473 Input types: [float]
native : benchmark_model.cc:474 Output layers: [output:0]
native : benchmark_model.cc:475 Target layers: []
native : benchmark_model.cc:476 Num runs: [50]
native : benchmark_model.cc:477 Inter-inference delay (seconds): [-1.0]
native : benchmark_model.cc:478 Inter-benchmark delay (seconds): [-1.0]
native : benchmark_model.cc:480 Num threads: [1]
native : benchmark_model.cc:481 Benchmark name: []
native : benchmark_model.cc:482 Output prefix: []
native : benchmark_model.cc:483 Show sizes: [0]
native : benchmark_model.cc:484 Warmup runs: [1]
native : benchmark_model.cc:251 Loading TensorFlow.
native : benchmark_model.cc:258 Got config, 0 devices
can't determine number of CPU cores: assuming 4
native : benchmark_model.cc:496 Initialized session in 0.03605s
native : benchmark_model.cc:327 Running benchmark for max 1 iterations, max -1 seconds without detailed stat logging, with -1s sleep between inferences
native : benchmark_model.cc:361 count=1 curr=637203

native : benchmark_model.cc:327 Running benchmark for max 50 iterations, max 10 seconds without detailed stat logging, with -1s sleep between inferences
native : benchmark_model.cc:361 count=30 first=616452 curr=309802 min=309536 max=618552 avg=342551 std=92067

native : benchmark_model.cc:327 Running benchmark for max 50 iterations, max 10 seconds with detailed stat logging, with -1s sleep between inferences
native : benchmark_model.cc:361 count=33 first=310795 curr=311214 min=310428 max=311479 avg=310885 std=265

native : benchmark_model.cc:600 Average inference timings in us: Warmup: 637203, no stats: 342550, with stats: 310885
native : stat_summarizer.cc:85 ============================== Run Order ==============================
native : stat_summarizer.cc:85 	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
native : stat_summarizer.cc:85 	                    NoOp	            0.000	    0.014	    0.010	  0.003%	  0.003%	     0.000	        1	_SOURCE
native : stat_summarizer.cc:85 	                   Const	            0.017	    0.016	    0.009	  0.003%	  0.006%	     0.000	        1	Const
native : stat_summarizer.cc:85 	                   Const	            0.029	    0.005	    0.004	  0.001%	  0.007%	     0.000	        1	conv2d_transpose/strided_slice/stack
native : stat_summarizer.cc:85 	                   Const	            0.034	    0.004	    0.004	  0.001%	  0.009%	     0.000	        1	conv2d_transpose/strided_slice/stack_1
native : stat_summarizer.cc:85 	                   Const	            0.040	    0.004	    0.004	  0.001%	  0.010%	     0.000	        1	conv2d_transpose/strided_slice/stack_2
native : stat_summarizer.cc:85 	                   Const	            0.046	    0.003	    0.003	  0.001%	  0.011%	     0.000	        1	conv2d_transpose/strided_slice_1/stack
native : stat_summarizer.cc:85 	                   Const	            0.051	    0.003	    0.003	  0.001%	  0.012%	     0.000	        1	conv2d_transpose/strided_slice_1/stack_1
native : stat_summarizer.cc:85 	                   Const	            0.055	    0.004	    0.003	  0.001%	  0.013%	     0.000	        1	conv2d_transpose/strided_slice_1/stack_2
native : stat_summarizer.cc:85 	                   Const	            0.059	    0.003	    0.003	  0.001%	  0.014%	     0.000	        1	conv2d_transpose/strided_slice_2/stack
native : stat_summarizer.cc:85 	                   Const	            0.064	    0.004	    0.003	  0.001%	  0.015%	     0.000	        1	conv2d_transpose/strided_slice_2/stack_1
native : stat_summarizer.cc:85 	                   Const	            0.069	    0.003	    0.003	  0.001%	  0.016%	     0.000	        1	conv2d_transpose/strided_slice_2/stack_2
native : stat_summarizer.cc:85 	                   Const	            0.074	    0.003	    0.003	  0.001%	  0.017%	     0.000	        1	conv2d_transpose/mul/y
native : stat_summarizer.cc:85 	                   Const	            0.078	    0.004	    0.003	  0.001%	  0.018%	     0.000	        1	conv2d_transpose/mul_1/y
native : stat_summarizer.cc:85 	                   Const	            0.083	    0.004	    0.003	  0.001%	  0.019%	     0.000	        1	conv2d_transpose/stack/3
native : stat_summarizer.cc:85 	                   Const	            0.087	    0.003	    0.003	  0.001%	  0.020%	     0.000	        1	output/Minimum/y
native : stat_summarizer.cc:85 	                   Const	            0.092	    0.003	    0.003	  0.001%	  0.021%	     0.000	        1	output/y
native : stat_summarizer.cc:85 	                    _Arg	            0.096	    0.005	    0.003	  0.001%	  0.022%	     0.000	        1	_arg_input_0_0
native : stat_summarizer.cc:85 	               MirrorPad	            0.103	    2.747	    2.678	  0.863%	  0.884%	   811.200	        1	MirrorPad
native : stat_summarizer.cc:85 	                   Const	            2.802	    0.007	    0.006	  0.002%	  0.886%	     0.000	        1	batch_normalization/beta/read/_0__cf__0
native : stat_summarizer.cc:85 	                   Const	            2.810	    0.004	    0.004	  0.001%	  0.887%	     0.000	        1	batch_normalization/gamma/read/_1__cf__1
native : stat_summarizer.cc:85 	                   Const	            2.815	    0.004	    0.004	  0.001%	  0.889%	     0.000	        1	batch_normalization/moving_mean/read/_2__cf__2
native : stat_summarizer.cc:85 	                   Const	            2.820	    0.004	    0.003	  0.001%	  0.890%	     0.000	        1	batch_normalization/moving_variance/read/_3__cf__3
native : stat_summarizer.cc:85 	                   Const	            2.825	    0.005	    0.003	  0.001%	  0.891%	     0.000	        1	batch_normalization_1/beta/read/_4__cf__4
native : stat_summarizer.cc:85 	                   Const	            2.830	    0.003	    0.003	  0.001%	  0.892%	     0.000	        1	batch_normalization_1/gamma/read/_5__cf__5
native : stat_summarizer.cc:85 	                   Const	            2.835	    0.004	    0.004	  0.001%	  0.893%	     0.000	        1	batch_normalization_1/moving_mean/read/_6__cf__6
native : stat_summarizer.cc:85 	                   Const	            2.840	    0.004	    0.003	  0.001%	  0.894%	     0.000	        1	batch_normalization_1/moving_variance/read/_7__cf__7
native : stat_summarizer.cc:85 	                   Const	            2.845	    0.006	    0.005	  0.002%	  0.896%	     0.000	        1	conv2d/Conv2D/ReadVariableOp/_8__cf__8
native : stat_summarizer.cc:85 	                  Conv2D	            2.852	   31.104	   31.094	 10.018%	 10.914%	  6390.144	        1	conv2d/Conv2D
native : stat_summarizer.cc:85 	                    Relu	           33.975	    0.855	    0.864	  0.279%	 11.192%	     0.000	        1	activation/Relu
native : stat_summarizer.cc:85 	          FusedBatchNorm	           34.849	    8.088	    8.000	  2.577%	 13.770%	  6390.528	        1	batch_normalization/FusedBatchNorm
native : stat_summarizer.cc:85 	                   Const	           42.882	    0.012	    0.012	  0.004%	 13.774%	     0.000	        1	conv2d_1/Conv2D/ReadVariableOp/_9__cf__9
native : stat_summarizer.cc:85 	                  Conv2D	           42.897	   57.605	   57.749	 18.606%	 32.379%	  6390.144	        1	conv2d_1/Conv2D
native : stat_summarizer.cc:85 	          FusedBatchNorm	          100.677	    6.175	    6.276	  2.022%	 34.401%	     0.384	        1	batch_normalization_1/FusedBatchNorm
native : stat_summarizer.cc:85 	                    Relu	          106.967	    0.854	    0.866	  0.279%	 34.680%	     0.000	        1	activation_1/Relu
native : stat_summarizer.cc:85 	                   Const	          107.846	    0.016	    0.015	  0.005%	 34.685%	     0.000	        1	conv2d_2/Conv2D/ReadVariableOp/_10__cf__10
native : stat_summarizer.cc:85 	                  Conv2D	          107.865	   57.759	   57.751	 18.607%	 53.291%	  6390.144	        1	conv2d_2/Conv2D
native : stat_summarizer.cc:85 	                     Add	          165.648	    1.811	    1.839	  0.592%	 53.884%	     0.000	        1	add
native : stat_summarizer.cc:85 	                   Const	          167.501	    0.013	    0.014	  0.005%	 53.889%	     0.000	        1	conv2d_3/Conv2D/ReadVariableOp/_11__cf__11
native : stat_summarizer.cc:85 	                  Conv2D	          167.521	  104.822	  104.968	 33.819%	 87.708%	 12582.912	        1	conv2d_3/Conv2D
native : stat_summarizer.cc:85 	                    Relu	          272.521	    1.732	    1.741	  0.561%	 88.269%	     0.000	        1	activation_2/Relu
native : stat_summarizer.cc:85 	                   Shape	          274.272	    0.026	    0.024	  0.008%	 88.276%	     0.016	        1	conv2d_transpose/Shape
native : stat_summarizer.cc:85 	            StridedSlice	          274.303	    0.022	    0.024	  0.008%	 88.284%	     0.004	        1	conv2d_transpose/strided_slice
native : stat_summarizer.cc:85 	            StridedSlice	          274.335	    0.021	    0.009	  0.003%	 88.287%	     0.004	        1	conv2d_transpose/strided_slice_1
native : stat_summarizer.cc:85 	                     Mul	          274.349	    0.012	    0.011	  0.003%	 88.291%	     0.000	        1	conv2d_transpose/mul
native : stat_summarizer.cc:85 	            StridedSlice	          274.363	    0.008	    0.011	  0.003%	 88.294%	     0.004	        1	conv2d_transpose/strided_slice_2
native : stat_summarizer.cc:85 	                     Mul	          274.378	    0.005	    0.004	  0.001%	 88.295%	     0.000	        1	conv2d_transpose/mul_1
native : stat_summarizer.cc:85 	                    Pack	          274.385	    0.015	    0.016	  0.005%	 88.301%	     0.016	        1	conv2d_transpose/stack
native : stat_summarizer.cc:85 	                   Const	          274.407	    0.010	    0.009	  0.003%	 88.304%	     0.000	        1	conv2d_transpose/conv2d_transpose/ReadVariableOp/_12__cf__12
native : stat_summarizer.cc:85 	     Conv2DBackpropInput	          274.419	   35.577	   35.586	 11.465%	 99.769%	 15728.640	        1	conv2d_transpose/conv2d_transpose
native : stat_summarizer.cc:85 	                 Minimum	          310.038	    0.359	    0.370	  0.119%	 99.888%	     0.000	        1	output/Minimum
native : stat_summarizer.cc:85 	                 Maximum	          310.415	    0.424	    0.338	  0.109%	 99.997%	     0.000	        1	output
native : stat_summarizer.cc:85 	                 _Retval	          310.759	    0.010	    0.010	  0.003%	100.000%	     0.000	        1	_retval_output_0_0
native : stat_summarizer.cc:85 
native : stat_summarizer.cc:85 ============================== Top by Computation Time ==============================
native : stat_summarizer.cc:85 	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
native : stat_summarizer.cc:85 	                  Conv2D	          167.521	  104.822	  104.968	 33.819%	 33.819%	 12582.912	        1	conv2d_3/Conv2D
native : stat_summarizer.cc:85 	                  Conv2D	          107.865	   57.759	   57.751	 18.607%	 52.426%	  6390.144	        1	conv2d_2/Conv2D
native : stat_summarizer.cc:85 	                  Conv2D	           42.897	   57.605	   57.749	 18.606%	 71.032%	  6390.144	        1	conv2d_1/Conv2D
native : stat_summarizer.cc:85 	     Conv2DBackpropInput	          274.419	   35.577	   35.586	 11.465%	 82.497%	 15728.640	        1	conv2d_transpose/conv2d_transpose
native : stat_summarizer.cc:85 	                  Conv2D	            2.852	   31.104	   31.094	 10.018%	 92.515%	  6390.144	        1	conv2d/Conv2D
native : stat_summarizer.cc:85 	          FusedBatchNorm	           34.849	    8.088	    8.000	  2.577%	 95.092%	  6390.528	        1	batch_normalization/FusedBatchNorm
native : stat_summarizer.cc:85 	          FusedBatchNorm	          100.677	    6.175	    6.276	  2.022%	 97.114%	     0.384	        1	batch_normalization_1/FusedBatchNorm
native : stat_summarizer.cc:85 	               MirrorPad	            0.103	    2.747	    2.678	  0.863%	 97.977%	   811.200	        1	MirrorPad
native : stat_summarizer.cc:85 	                     Add	          165.648	    1.811	    1.839	  0.592%	 98.570%	     0.000	        1	add
native : stat_summarizer.cc:85 	                    Relu	          272.521	    1.732	    1.741	  0.561%	 99.131%	     0.000	        1	activation_2/Relu
native : stat_summarizer.cc:85 
native : stat_summarizer.cc:85 ============================== Top by Memory Use ==============================
native : stat_summarizer.cc:85 	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
native : stat_summarizer.cc:85 	     Conv2DBackpropInput	          274.419	   35.577	   35.586	 11.465%	 11.465%	 15728.640	        1	conv2d_transpose/conv2d_transpose
native : stat_summarizer.cc:85 	                  Conv2D	          167.521	  104.822	  104.968	 33.819%	 45.285%	 12582.912	        1	conv2d_3/Conv2D
native : stat_summarizer.cc:85 	          FusedBatchNorm	           34.849	    8.088	    8.000	  2.577%	 47.862%	  6390.528	        1	batch_normalization/FusedBatchNorm
native : stat_summarizer.cc:85 	                  Conv2D	          107.865	   57.759	   57.751	 18.607%	 66.469%	  6390.144	        1	conv2d_2/Conv2D
native : stat_summarizer.cc:85 	                  Conv2D	           42.897	   57.605	   57.749	 18.606%	 85.074%	  6390.144	        1	conv2d_1/Conv2D
native : stat_summarizer.cc:85 	                  Conv2D	            2.852	   31.104	   31.094	 10.018%	 95.092%	  6390.144	        1	conv2d/Conv2D
native : stat_summarizer.cc:85 	               MirrorPad	            0.103	    2.747	    2.678	  0.863%	 95.955%	   811.200	        1	MirrorPad
native : stat_summarizer.cc:85 	          FusedBatchNorm	          100.677	    6.175	    6.276	  2.022%	 97.977%	     0.384	        1	batch_normalization_1/FusedBatchNorm
native : stat_summarizer.cc:85 	                   Shape	          274.272	    0.026	    0.024	  0.008%	 97.985%	     0.016	        1	conv2d_transpose/Shape
native : stat_summarizer.cc:85 	                    Pack	          274.385	    0.015	    0.016	  0.005%	 97.990%	     0.016	        1	conv2d_transpose/stack
native : stat_summarizer.cc:85 
native : stat_summarizer.cc:85 Number of nodes executed: 52
native : stat_summarizer.cc:85 ============================== Summary by node type ==============================
native : stat_summarizer.cc:85 	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
native : stat_summarizer.cc:85 	                  Conv2D	        4	   251.561	    81.056%	    81.056%	 31753.344	        4
native : stat_summarizer.cc:85 	     Conv2DBackpropInput	        1	    35.585	    11.466%	    92.522%	 15728.640	        1
native : stat_summarizer.cc:85 	          FusedBatchNorm	        2	    14.275	     4.600%	    97.121%	  6390.912	        2
native : stat_summarizer.cc:85 	                    Relu	        3	     3.469	     1.118%	    98.239%	     0.000	        3
native : stat_summarizer.cc:85 	               MirrorPad	        1	     2.677	     0.863%	    99.102%	   811.200	        1
native : stat_summarizer.cc:85 	                     Add	        1	     1.839	     0.593%	    99.694%	     0.000	        1
native : stat_summarizer.cc:85 	                 Minimum	        1	     0.369	     0.119%	    99.813%	     0.000	        1
native : stat_summarizer.cc:85 	                 Maximum	        1	     0.337	     0.109%	    99.922%	     0.000	        1
native : stat_summarizer.cc:85 	                   Const	       28	     0.126	     0.041%	    99.962%	     0.000	       28
native : stat_summarizer.cc:85 	            StridedSlice	        3	     0.041	     0.013%	    99.976%	     0.012	        3
native : stat_summarizer.cc:85 	                   Shape	        1	     0.024	     0.008%	    99.983%	     0.016	        1
native : stat_summarizer.cc:85 	                    Pack	        1	     0.016	     0.005%	    99.988%	     0.016	        1
native : stat_summarizer.cc:85 	                     Mul	        2	     0.014	     0.005%	    99.993%	     0.000	        2
native : stat_summarizer.cc:85 	                 _Retval	        1	     0.010	     0.003%	    99.996%	     0.000	        1
native : stat_summarizer.cc:85 	                    NoOp	        1	     0.009	     0.003%	    99.999%	     0.000	        1
native : stat_summarizer.cc:85 	                    _Arg	        1	     0.003	     0.001%	   100.000%	     0.000	        1
native : stat_summarizer.cc:85 
native : stat_summarizer.cc:85 Timings (microseconds): count=33 first=310208 curr=310701 min=309938 max=310969 avg=310380 std=264
native : stat_summarizer.cc:85 Memory (bytes): count=33 curr=54684140(all same)
native : stat_summarizer.cc:85 52 nodes observed
native : stat_summarizer.cc:85 
```

**TFMobile 16 threads In:1x256x256x3 Performance**
`
benchmark_model --graph=/data/local/tmp/UpResnet5-X2-H256-W256-C3.pb --max_num_runs=50 --num_threads=16 --input_layer_shape=""1,256,256,3""
`
```
native : benchmark_model.cc:469 Graph: [/data/local/tmp/UpResnet5-X2-H256-W256-C3.pb]
native : benchmark_model.cc:470 Init ops:
native : benchmark_model.cc:471 Input layers: [input:0]
native : benchmark_model.cc:472 Input shapes: [1,256,256,3]
native : benchmark_model.cc:473 Input types: [float]
native : benchmark_model.cc:474 Output layers: [output:0]
native : benchmark_model.cc:475 Target layers: []
native : benchmark_model.cc:476 Num runs: [50]
native : benchmark_model.cc:477 Inter-inference delay (seconds): [-1.0]
native : benchmark_model.cc:478 Inter-benchmark delay (seconds): [-1.0]
native : benchmark_model.cc:480 Num threads: [16]
native : benchmark_model.cc:481 Benchmark name: []
native : benchmark_model.cc:482 Output prefix: []
native : benchmark_model.cc:483 Show sizes: [0]
native : benchmark_model.cc:484 Warmup runs: [1]
native : benchmark_model.cc:251 Loading TensorFlow.
native : benchmark_model.cc:258 Got config, 0 devices
can't determine number of CPU cores: assuming 4
native : benchmark_model.cc:496 Initialized session in 0.036933s
native : benchmark_model.cc:327 Running benchmark for max 1 iterations, max -1 seconds without detailed stat logging, with -1s sleep between inferences
native : benchmark_model.cc:361 count=1 curr=144572

native : benchmark_model.cc:327 Running benchmark for max 50 iterations, max 10 seconds without detailed stat logging, with -1s sleep between inferences
native : benchmark_model.cc:361 count=50 first=121095 curr=120424 min=118295 max=159110 avg=123218 std=7596

native : benchmark_model.cc:327 Running benchmark for max 50 iterations, max 10 seconds with detailed stat logging, with -1s sleep between inferences
native : benchmark_model.cc:361 count=50 first=120234 curr=123678 min=117632 max=144145 avg=122647 std=4286

native : benchmark_model.cc:600 Average inference timings in us: Warmup: 144572, no stats: 123217, with stats: 122646
native : stat_summarizer.cc:85 ============================== Run Order ==============================
native : stat_summarizer.cc:85 	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
native : stat_summarizer.cc:85 	                    NoOp	            0.000	    0.012	    0.008	  0.006%	  0.006%	     0.000	        1	_SOURCE
native : stat_summarizer.cc:85 	                   Const	            0.015	    0.016	    0.009	  0.007%	  0.014%	     0.000	        1	Const
native : stat_summarizer.cc:85 	                   Const	            0.026	    0.004	    0.003	  0.003%	  0.016%	     0.000	        1	conv2d_transpose/strided_slice/stack
native : stat_summarizer.cc:85 	                   Const	            0.030	    0.004	    0.004	  0.003%	  0.019%	     0.000	        1	conv2d_transpose/strided_slice/stack_1
native : stat_summarizer.cc:85 	                   Const	            0.035	    0.006	    0.004	  0.003%	  0.022%	     0.000	        1	conv2d_transpose/strided_slice/stack_2
native : stat_summarizer.cc:85 	                   Const	            0.040	    0.005	    0.003	  0.003%	  0.025%	     0.000	        1	conv2d_transpose/strided_slice_1/stack
native : stat_summarizer.cc:85 	                   Const	            0.045	    0.004	    0.003	  0.003%	  0.027%	     0.000	        1	conv2d_transpose/strided_slice_1/stack_1
native : stat_summarizer.cc:85 	                   Const	            0.049	    0.003	    0.003	  0.003%	  0.030%	     0.000	        1	conv2d_transpose/strided_slice_1/stack_2
native : stat_summarizer.cc:85 	                   Const	            0.055	    0.003	    0.003	  0.002%	  0.033%	     0.000	        1	conv2d_transpose/strided_slice_2/stack
native : stat_summarizer.cc:85 	                   Const	            0.059	    0.004	    0.003	  0.002%	  0.035%	     0.000	        1	conv2d_transpose/strided_slice_2/stack_1
native : stat_summarizer.cc:85 	                   Const	            0.064	    0.004	    0.003	  0.002%	  0.037%	     0.000	        1	conv2d_transpose/strided_slice_2/stack_2
native : stat_summarizer.cc:85 	                   Const	            0.068	    0.004	    0.003	  0.002%	  0.040%	     0.000	        1	conv2d_transpose/mul/y
native : stat_summarizer.cc:85 	                   Const	            0.072	    0.003	    0.003	  0.002%	  0.042%	     0.000	        1	conv2d_transpose/mul_1/y
native : stat_summarizer.cc:85 	                   Const	            0.077	    0.003	    0.003	  0.002%	  0.044%	     0.000	        1	conv2d_transpose/stack/3
native : stat_summarizer.cc:85 	                   Const	            0.081	    0.004	    0.003	  0.002%	  0.046%	     0.000	        1	output/Minimum/y
native : stat_summarizer.cc:85 	                   Const	            0.085	    0.004	    0.003	  0.002%	  0.049%	     0.000	        1	output/y
native : stat_summarizer.cc:85 	                    _Arg	            0.089	    0.004	    0.003	  0.003%	  0.051%	     0.000	        1	_arg_input_0_0
native : stat_summarizer.cc:85 	               MirrorPad	            0.095	    1.112	    1.351	  1.107%	  1.158%	   811.200	        1	MirrorPad
native : stat_summarizer.cc:85 	                   Const	            1.471	    0.006	    0.006	  0.005%	  1.163%	     0.000	        1	batch_normalization/beta/read/_0__cf__0
native : stat_summarizer.cc:85 	                   Const	            1.479	    0.005	    0.004	  0.003%	  1.166%	     0.000	        1	batch_normalization/gamma/read/_1__cf__1
native : stat_summarizer.cc:85 	                   Const	            1.485	    0.005	    0.004	  0.003%	  1.170%	     0.000	        1	batch_normalization/moving_mean/read/_2__cf__2
native : stat_summarizer.cc:85 	                   Const	            1.491	    0.005	    0.003	  0.003%	  1.172%	     0.000	        1	batch_normalization/moving_variance/read/_3__cf__3
native : stat_summarizer.cc:85 	                   Const	            1.496	    0.006	    0.004	  0.003%	  1.175%	     0.000	        1	batch_normalization_1/beta/read/_4__cf__4
native : stat_summarizer.cc:85 	                   Const	            1.501	    0.008	    0.004	  0.003%	  1.178%	     0.000	        1	batch_normalization_1/gamma/read/_5__cf__5
native : stat_summarizer.cc:85 	                   Const	            1.505	    0.007	    0.004	  0.003%	  1.181%	     0.000	        1	batch_normalization_1/moving_mean/read/_6__cf__6
native : stat_summarizer.cc:85 	                   Const	            1.511	    0.006	    0.003	  0.003%	  1.184%	     0.000	        1	batch_normalization_1/moving_variance/read/_7__cf__7
native : stat_summarizer.cc:85 	                   Const	            1.516	    0.006	    0.004	  0.003%	  1.187%	     0.000	        1	conv2d/Conv2D/ReadVariableOp/_8__cf__8
native : stat_summarizer.cc:85 	                  Conv2D	            1.522	    8.478	    8.902	  7.290%	  8.477%	  6390.144	        1	conv2d/Conv2D
native : stat_summarizer.cc:85 	                    Relu	           10.455	    1.142	    1.071	  0.878%	  9.355%	     0.000	        1	activation/Relu
native : stat_summarizer.cc:85 	          FusedBatchNorm	           11.540	    3.001	    2.754	  2.256%	 11.611%	  6390.528	        1	batch_normalization/FusedBatchNorm
native : stat_summarizer.cc:85 	                   Const	           14.332	    0.011	    0.012	  0.010%	 11.620%	     0.000	        1	conv2d_1/Conv2D/ReadVariableOp/_9__cf__9
native : stat_summarizer.cc:85 	                  Conv2D	           14.348	   23.520	   23.289	 19.073%	 30.694%	  6390.144	        1	conv2d_1/Conv2D
native : stat_summarizer.cc:85 	          FusedBatchNorm	           37.672	    2.232	    2.291	  1.877%	 32.570%	     0.384	        1	batch_normalization_1/FusedBatchNorm
native : stat_summarizer.cc:85 	                    Relu	           39.983	    0.692	    0.830	  0.680%	 33.250%	     0.000	        1	activation_1/Relu
native : stat_summarizer.cc:85 	                   Const	           40.828	    0.012	    0.012	  0.010%	 33.260%	     0.000	        1	conv2d_2/Conv2D/ReadVariableOp/_10__cf__10
native : stat_summarizer.cc:85 	                  Conv2D	           40.844	   23.039	   23.440	 19.197%	 52.457%	  6390.144	        1	conv2d_2/Conv2D
native : stat_summarizer.cc:85 	                     Add	           64.319	    2.242	    2.501	  2.048%	 54.505%	     0.000	        1	add
native : stat_summarizer.cc:85 	                   Const	           66.836	    0.012	    0.012	  0.010%	 54.515%	     0.000	        1	conv2d_3/Conv2D/ReadVariableOp/_11__cf__11
native : stat_summarizer.cc:85 	                  Conv2D	           66.854	   31.680	   31.959	 26.174%	 80.689%	 12582.912	        1	conv2d_3/Conv2D
native : stat_summarizer.cc:85 	                    Relu	           98.846	    1.627	    1.682	  1.377%	 82.066%	     0.000	        1	activation_2/Relu
native : stat_summarizer.cc:85 	                   Shape	          100.543	    0.027	    0.026	  0.022%	 82.088%	     0.016	        1	conv2d_transpose/Shape
native : stat_summarizer.cc:85 	            StridedSlice	          100.576	    0.023	    0.021	  0.017%	 82.105%	     0.004	        1	conv2d_transpose/strided_slice
native : stat_summarizer.cc:85 	            StridedSlice	          100.605	    0.018	    0.007	  0.006%	 82.111%	     0.004	        1	conv2d_transpose/strided_slice_1
native : stat_summarizer.cc:85 	                     Mul	          100.617	    0.010	    0.012	  0.010%	 82.121%	     0.000	        1	conv2d_transpose/mul
native : stat_summarizer.cc:85 	            StridedSlice	          100.632	    0.007	    0.008	  0.007%	 82.127%	     0.004	        1	conv2d_transpose/strided_slice_2
native : stat_summarizer.cc:85 	                     Mul	          100.644	    0.003	    0.004	  0.003%	 82.131%	     0.000	        1	conv2d_transpose/mul_1
native : stat_summarizer.cc:85 	                    Pack	          100.651	    0.013	    0.015	  0.012%	 82.143%	     0.016	        1	conv2d_transpose/stack
native : stat_summarizer.cc:85 	                   Const	          100.672	    0.008	    0.009	  0.007%	 82.150%	     0.000	        1	conv2d_transpose/conv2d_transpose/ReadVariableOp/_12__cf__12
native : stat_summarizer.cc:85 	     Conv2DBackpropInput	          100.684	   19.043	   20.264	 16.596%	 98.746%	 15728.640	        1	conv2d_transpose/conv2d_transpose
native : stat_summarizer.cc:85 	                 Minimum	          120.986	    1.088	    0.878	  0.719%	 99.465%	     0.000	        1	output/Minimum
native : stat_summarizer.cc:85 	                 Maximum	          121.874	    0.465	    0.634	  0.519%	 99.985%	     0.000	        1	output
native : stat_summarizer.cc:85 	                 _Retval	          122.517	    0.010	    0.019	  0.015%	100.000%	     0.000	        1	_retval_output_0_0
native : stat_summarizer.cc:85 
native : stat_summarizer.cc:85 ============================== Top by Computation Time ==============================
native : stat_summarizer.cc:85 	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
native : stat_summarizer.cc:85 	                  Conv2D	           66.854	   31.680	   31.959	 26.174%	 26.174%	 12582.912	        1	conv2d_3/Conv2D
native : stat_summarizer.cc:85 	                  Conv2D	           40.844	   23.039	   23.440	 19.197%	 45.371%	  6390.144	        1	conv2d_2/Conv2D
native : stat_summarizer.cc:85 	                  Conv2D	           14.348	   23.520	   23.289	 19.073%	 64.444%	  6390.144	        1	conv2d_1/Conv2D
native : stat_summarizer.cc:85 	     Conv2DBackpropInput	          100.684	   19.043	   20.264	 16.596%	 81.041%	 15728.640	        1	conv2d_transpose/conv2d_transpose
native : stat_summarizer.cc:85 	                  Conv2D	            1.522	    8.478	    8.902	  7.290%	 88.331%	  6390.144	        1	conv2d/Conv2D
native : stat_summarizer.cc:85 	          FusedBatchNorm	           11.540	    3.001	    2.754	  2.256%	 90.587%	  6390.528	        1	batch_normalization/FusedBatchNorm
native : stat_summarizer.cc:85 	                     Add	           64.319	    2.242	    2.501	  2.048%	 92.635%	     0.000	        1	add
native : stat_summarizer.cc:85 	          FusedBatchNorm	           37.672	    2.232	    2.291	  1.877%	 94.511%	     0.384	        1	batch_normalization_1/FusedBatchNorm
native : stat_summarizer.cc:85 	                    Relu	           98.846	    1.627	    1.682	  1.377%	 95.888%	     0.000	        1	activation_2/Relu
native : stat_summarizer.cc:85 	               MirrorPad	            0.095	    1.112	    1.351	  1.107%	 96.995%	   811.200	        1	MirrorPad
native : stat_summarizer.cc:85 
native : stat_summarizer.cc:85 ============================== Top by Memory Use ==============================
native : stat_summarizer.cc:85 	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
native : stat_summarizer.cc:85 	     Conv2DBackpropInput	          100.684	   19.043	   20.264	 16.596%	 16.596%	 15728.640	        1	conv2d_transpose/conv2d_transpose
native : stat_summarizer.cc:85 	                  Conv2D	           66.854	   31.680	   31.959	 26.174%	 42.770%	 12582.912	        1	conv2d_3/Conv2D
native : stat_summarizer.cc:85 	          FusedBatchNorm	           11.540	    3.001	    2.754	  2.256%	 45.026%	  6390.528	        1	batch_normalization/FusedBatchNorm
native : stat_summarizer.cc:85 	                  Conv2D	           40.844	   23.039	   23.440	 19.197%	 64.223%	  6390.144	        1	conv2d_2/Conv2D
native : stat_summarizer.cc:85 	                  Conv2D	           14.348	   23.520	   23.289	 19.073%	 83.296%	  6390.144	        1	conv2d_1/Conv2D
native : stat_summarizer.cc:85 	                  Conv2D	            1.522	    8.478	    8.902	  7.290%	 90.587%	  6390.144	        1	conv2d/Conv2D
native : stat_summarizer.cc:85 	               MirrorPad	            0.095	    1.112	    1.351	  1.107%	 91.693%	   811.200	        1	MirrorPad
native : stat_summarizer.cc:85 	          FusedBatchNorm	           37.672	    2.232	    2.291	  1.877%	 93.570%	     0.384	        1	batch_normalization_1/FusedBatchNorm
native : stat_summarizer.cc:85 	                    Pack	          100.651	    0.013	    0.015	  0.012%	 93.582%	     0.016	        1	conv2d_transpose/stack
native : stat_summarizer.cc:85 	                   Shape	          100.543	    0.027	    0.026	  0.022%	 93.604%	     0.016	        1	conv2d_transpose/Shape
native : stat_summarizer.cc:85 
native : stat_summarizer.cc:85 Number of nodes executed: 52
native : stat_summarizer.cc:85 ============================== Summary by node type ==============================
native : stat_summarizer.cc:85 	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
native : stat_summarizer.cc:85 	                  Conv2D	        4	    87.586	    71.748%	    71.748%	 31753.344	        4
native : stat_summarizer.cc:85 	     Conv2DBackpropInput	        1	    20.264	    16.600%	    88.348%	 15728.640	        1
native : stat_summarizer.cc:85 	          FusedBatchNorm	        2	     5.045	     4.133%	    92.481%	  6390.912	        2
native : stat_summarizer.cc:85 	                    Relu	        3	     3.582	     2.934%	    95.415%	     0.000	        3
native : stat_summarizer.cc:85 	                     Add	        1	     2.500	     2.048%	    97.463%	     0.000	        1
native : stat_summarizer.cc:85 	               MirrorPad	        1	     1.351	     1.107%	    98.570%	   811.200	        1
native : stat_summarizer.cc:85 	                 Minimum	        1	     0.877	     0.718%	    99.288%	     0.000	        1
native : stat_summarizer.cc:85 	                 Maximum	        1	     0.634	     0.519%	    99.807%	     0.000	        1
native : stat_summarizer.cc:85 	                   Const	       28	     0.116	     0.095%	    99.903%	     0.000	       28
native : stat_summarizer.cc:85 	            StridedSlice	        3	     0.036	     0.029%	    99.932%	     0.012	        3
native : stat_summarizer.cc:85 	                   Shape	        1	     0.026	     0.021%	    99.953%	     0.016	        1
native : stat_summarizer.cc:85 	                 _Retval	        1	     0.018	     0.015%	    99.968%	     0.000	        1
native : stat_summarizer.cc:85 	                     Mul	        2	     0.015	     0.012%	    99.980%	     0.000	        2
native : stat_summarizer.cc:85 	                    Pack	        1	     0.014	     0.011%	    99.992%	     0.016	        1
native : stat_summarizer.cc:85 	                    NoOp	        1	     0.007	     0.006%	    99.998%	     0.000	        1
native : stat_summarizer.cc:85 	                    _Arg	        1	     0.003	     0.002%	   100.000%	     0.000	        1
native : stat_summarizer.cc:85 
native : stat_summarizer.cc:85 Timings (microseconds): count=50 first=119656 curr=123144 min=117106 max=143566 avg=122102 std=4282
native : stat_summarizer.cc:85 Memory (bytes): count=50 curr=54684140(all same)
native : stat_summarizer.cc:85 52 nodes observed
native : stat_summarizer.cc:85 
```

**TFLite 1 threads In:1x256x256x3 Performance w/o NNAPI**
`
benchmark_model --graph=/data/local/tmp/UpResnet5-X2-H256-W256-C3.tflite --num_threads=1
`
```
STARTING!
Min num runs: [50]
Min runs duration (seconds): [1]
Inter-run delay (seconds): [-1]
Num threads: [1]
Benchmark name: []
Output prefix: []
Min warmup runs: [1]
Min warmup runs duration (seconds): [0.5]
Graph: [/data/local/tmp/UpResnet5-X2-H256-W256-C3.tflite]
Input layers: []
Input shapes: []
Use nnapi : [0]
Allow fp16 : [0]
nnapi error: requires android sdk version to be at least 27
Loaded model /data/local/tmp/UpResnet5-X2-H256-W256-C3.tflite
resolved reporter
Initialized session in 17.527ms
Running benchmark for at least 1 iterations and at least 0.5 seconds
count=1 curr=1058871

Running benchmark for at least 50 iterations and at least 1 seconds
count=50 first=885931 curr=881142 min=851668 max=903937 avg=880688 std=12426

============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	              MIRROR_PAD	            0.000	   11.161	   11.156	  1.267%	  1.267%	     0.000	        1	[MirrorPad]
	                 CONV_2D	           11.158	    9.939	   10.067	  1.143%	  2.410%	     0.000	        1	[activation/Relu]
	                     MUL	           21.226	    1.432	    1.521	  0.173%	  2.583%	     0.000	        1	[batch_normalization/FusedBatchNorm_mul_0]
	                     ADD	           22.748	    1.491	    1.520	  0.173%	  2.755%	     0.000	        1	[batch_normalization/FusedBatchNorm]
	                 CONV_2D	           24.269	   82.282	   82.912	  9.415%	 12.170%	     0.000	        1	[activation_1/Relu]
	                 CONV_2D	          107.182	   82.992	   82.822	  9.404%	 21.574%	     0.000	        1	[conv2d_2/Conv2D]
	                     ADD	          190.005	    1.450	    1.574	  0.179%	 21.753%	     0.000	        1	[add]
	                 CONV_2D	          191.579	  117.950	  118.416	 13.446%	 35.199%	     0.000	        1	[activation_2/Relu]
	          TRANSPOSE_CONV	          309.996	  569.173	  562.609	 63.884%	 99.083%	     0.000	        1	[conv2d_transpose/conv2d_transpose]
	                 MINIMUM	          872.607	    4.092	    4.072	  0.462%	 99.545%	     0.000	        1	[output/Minimum]
	                 MAXIMUM	          876.680	    3.955	    4.004	  0.455%	100.000%	     0.000	        1	[output]

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	          TRANSPOSE_CONV	          309.996	  569.173	  562.609	 63.884%	 63.884%	     0.000	        1	[conv2d_transpose/conv2d_transpose]
	                 CONV_2D	          191.579	  117.950	  118.416	 13.446%	 77.330%	     0.000	        1	[activation_2/Relu]
	                 CONV_2D	           24.269	   82.282	   82.912	  9.415%	 86.745%	     0.000	        1	[activation_1/Relu]
	                 CONV_2D	          107.182	   82.992	   82.822	  9.404%	 96.149%	     0.000	        1	[conv2d_2/Conv2D]
	              MIRROR_PAD	            0.000	   11.161	   11.156	  1.267%	 97.416%	     0.000	        1	[MirrorPad]
	                 CONV_2D	           11.158	    9.939	   10.067	  1.143%	 98.559%	     0.000	        1	[activation/Relu]
	                 MINIMUM	          872.607	    4.092	    4.072	  0.462%	 99.021%	     0.000	        1	[output/Minimum]
	                 MAXIMUM	          876.680	    3.955	    4.004	  0.455%	 99.476%	     0.000	        1	[output]
	                     ADD	          190.005	    1.450	    1.574	  0.179%	 99.655%	     0.000	        1	[add]
	                     MUL	           21.226	    1.432	    1.521	  0.173%	 99.827%	     0.000	        1	[batch_normalization/FusedBatchNorm_mul_0]

Number of nodes executed: 11
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	          TRANSPOSE_CONV	        1	   562.609	    63.884%	    63.884%	     0.000	        1
	                 CONV_2D	        4	   294.214	    33.408%	    97.292%	     0.000	        4
	              MIRROR_PAD	        1	    11.156	     1.267%	    98.559%	     0.000	        1
	                 MINIMUM	        1	     4.072	     0.462%	    99.021%	     0.000	        1
	                 MAXIMUM	        1	     4.004	     0.455%	    99.476%	     0.000	        1
	                     ADD	        2	     3.093	     0.351%	    99.827%	     0.000	        2
	                     MUL	        1	     1.521	     0.173%	   100.000%	     0.000	        1

Timings (microseconds): count=50 first=885917 curr=881128 min=851653 max=903921 avg=880674 std=12426
Memory (bytes): count=0
11 nodes observed


Average inference timings in us: Warmup: 1.05887e+06, Init: 17527, no stats: 880688
```

**TFLite 16 threads In:1x256x256x3 Performance w/o NNAPI**
`
benchmark_model --graph=/data/local/tmp/UpResnet5-X2-H256-W256-C3.tflite --num_threads=16
`
```
STARTING!
Min num runs: [50]
Min runs duration (seconds): [1]
Inter-run delay (seconds): [-1]
Num threads: [16]
Benchmark name: []
Output prefix: []
Min warmup runs: [1]
Min warmup runs duration (seconds): [0.5]
Graph: [/data/local/tmp/UpResnet5-X2-H256-W256-C3.tflite]
Input layers: []
Input shapes: []
Use nnapi : [0]
Allow fp16 : [0]
nnapi error: requires android sdk version to be at least 27
Loaded model /data/local/tmp/UpResnet5-X2-H256-W256-C3.tflite
resolved reporter
Initialized session in 17.382ms
Running benchmark for at least 1 iterations and at least 0.5 seconds
count=1 curr=814662

Running benchmark for at least 50 iterations and at least 1 seconds
count=50 first=632324 curr=652388 min=617231 max=683660 avg=641555 std=13138

============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	              MIRROR_PAD	            0.000	   11.208	   11.343	  1.768%	  1.768%	     0.000	        1	[MirrorPad]
	                 CONV_2D	           11.344	   11.457	   11.713	  1.826%	  3.594%	     0.000	        1	[activation/Relu]
	                     MUL	           23.059	    2.024	    1.853	  0.289%	  3.883%	     0.000	        1	[batch_normalization/FusedBatchNorm_mul_0]
	                     ADD	           24.912	    1.510	    1.803	  0.281%	  4.164%	     0.000	        1	[batch_normalization/FusedBatchNorm]
	                 CONV_2D	           26.716	   11.487	   12.328	  1.922%	  6.085%	     0.000	        1	[activation_1/Relu]
	                 CONV_2D	           39.044	   11.615	   11.867	  1.850%	  7.935%	     0.000	        1	[conv2d_2/Conv2D]
	                     ADD	           50.912	    1.470	    1.421	  0.222%	  8.157%	     0.000	        1	[add]
	                 CONV_2D	           52.334	   19.418	   19.692	  3.069%	 11.226%	     0.000	        1	[activation_2/Relu]
	          TRANSPOSE_CONV	           72.027	  554.071	  561.442	 87.515%	 98.741%	     0.000	        1	[conv2d_transpose/conv2d_transpose]
	                 MINIMUM	          633.470	    4.022	    4.066	  0.634%	 99.374%	     0.000	        1	[output/Minimum]
	                 MAXIMUM	          637.537	    4.028	    4.014	  0.626%	100.000%	     0.000	        1	[output]

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	          TRANSPOSE_CONV	           72.027	  554.071	  561.442	 87.515%	 87.515%	     0.000	        1	[conv2d_transpose/conv2d_transpose]
	                 CONV_2D	           52.334	   19.418	   19.692	  3.069%	 90.584%	     0.000	        1	[activation_2/Relu]
	                 CONV_2D	           26.716	   11.487	   12.328	  1.922%	 92.506%	     0.000	        1	[activation_1/Relu]
	                 CONV_2D	           39.044	   11.615	   11.867	  1.850%	 94.355%	     0.000	        1	[conv2d_2/Conv2D]
	                 CONV_2D	           11.344	   11.457	   11.713	  1.826%	 96.181%	     0.000	        1	[activation/Relu]
	              MIRROR_PAD	            0.000	   11.208	   11.343	  1.768%	 97.949%	     0.000	        1	[MirrorPad]
	                 MINIMUM	          633.470	    4.022	    4.066	  0.634%	 98.583%	     0.000	        1	[output/Minimum]
	                 MAXIMUM	          637.537	    4.028	    4.014	  0.626%	 99.209%	     0.000	        1	[output]
	                     MUL	           23.059	    2.024	    1.853	  0.289%	 99.498%	     0.000	        1	[batch_normalization/FusedBatchNorm_mul_0]
	                     ADD	           24.912	    1.510	    1.803	  0.281%	 99.778%	     0.000	        1	[batch_normalization/FusedBatchNorm]

Number of nodes executed: 11
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	          TRANSPOSE_CONV	        1	   561.441	    87.515%	    87.515%	     0.000	        1
	                 CONV_2D	        4	    55.597	     8.666%	    96.182%	     0.000	        4
	              MIRROR_PAD	        1	    11.343	     1.768%	    97.950%	     0.000	        1
	                 MINIMUM	        1	     4.065	     0.634%	    98.583%	     0.000	        1
	                 MAXIMUM	        1	     4.013	     0.626%	    99.209%	     0.000	        1
	                     ADD	        2	     3.223	     0.502%	    99.711%	     0.000	        2
	                     MUL	        1	     1.852	     0.289%	   100.000%	     0.000	        1

Timings (microseconds): count=50 first=632310 curr=652370 min=617213 max=683643 avg=641541 std=13137
Memory (bytes): count=0
11 nodes observed


Average inference timings in us: Warmup: 814662, Init: 17382, no stats: 641555
```"
26735,[2.0] tf.numpy_function logs deprecation warning,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Ubuntu 16.04
- TensorFlow installed from: binary
- TensorFlow version: `2.0.0.dev20190311`
- Python version: 3.6.6
- CUDA/cuDNN version: 10.0

**Describe the current behavior**

When using `tf.numpy_function`, a warning is logged about `tf.py_func` being deprecated.

**Describe the expected behavior**

As a V2 symbol, `tf.numpy_function` should not produce a deprecation warning.

**Code to reproduce the issue**

```python
import tensorflow as tf
tf.numpy_function(lambda x: x, [tf.zeros([5])], [tf.float32])
```

```text
W0315 11:05:55.860109 139695358637824 deprecation.py:323] From /home/klein/dev/OpenNMT-tf/envv2/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py:476: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
```"
26733,"Expected to see 2 array(s), but instead got the following list of 1 arrays","I want to create a model that receive one image and compute the image by two Softmax(two output). The code is:

-------------------------------------------------
`base_model = InceptionV3(include_top=False)`
`x = base_model.output`
`x = GlobalAveragePooling2D()(x)`

# first Softmax
`x_1 = Dense(1024, activation='relu')(x)`
`predictions_1 = Dense(4, activation='softmax')(x_1)`

# second Softmax
`x_2 = Dense(1024, activation='relu')(x)`
`predictions_2 = Dense(4, activation='softmax')(x_2)`

`my_model = Model(inputs=base_model.input, outputs=[predictions_1,predictions_2])`

# train
`my_model.compile(...)`
`my_model.fit_generator(...)`
-------------------------------------------------

When training, I got error:

ValueError: 
Expected to see 2 array(s), but instead got the following list of 1 arrays: [array([[0., 0., 0., 1.],
       [0., 0., 0., 1.],
       [0., 0., 0., 1.],
       [0., 1., 0., 0.],
       [1., 0., 0., 0.],
       [0., 1., 0., 0.],
       [0., 1., 0., 0.],
       [0., 1., 0., 0.],..."
26732,[TF2.0] Error from Tensorboard with keras callback ,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):win10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:no
- TensorFlow installed from (source or binary): pip install tensorflow-gpu==2.0.0-alpha0
- TensorFlow version (use command below):conda list 2.0.0a0
- Python version:Python 3.6.8 :: Anaconda custom (64-bit)
- CUDA/cuDNN version:V10.0.130/7.5.0
- GPU model and memory:GTX 1050 TI

**Describe the current behavior**
I want to try tensorboard from keras, it work if tensorflow-gpu=1.13.1 & tensorboard=1.13.1,
but get the error if tensorflow-gpu=2.0.0a0 & tb-nightly=1.14.0a20190301 as below:
```
Epoch 1/50
   32/60000 [..............................] - ETA: 11:31 - loss: 2.3852 - acc: 0.1562
---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
<ipython-input-4-aadf56b04ffa> in <module>
----> 1 model.fit(x_train, y_train, epochs=50, callbacks=[tensorboard_callback])
      2 
      3 model.evaluate(x_test, y_test)

~\Anaconda3\envs\lab\lib\site-packages\tensorflow\python\keras\engine\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    863           validation_steps=validation_steps,
    864           validation_freq=validation_freq,
--> 865           steps_name='steps_per_epoch')
    866 
    867   def evaluate(self,

~\Anaconda3\envs\lab\lib\site-packages\tensorflow\python\keras\engine\training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)
    361         # Callbacks batch end.
    362         batch_logs = cbks.make_logs(model, batch_logs, batch_outs, mode)
--> 363         callbacks._call_batch_hook(mode, 'end', batch_index, batch_logs)
    364         progbar.on_batch_end(batch_index, batch_logs)
    365 

~\Anaconda3\envs\lab\lib\site-packages\tensorflow\python\keras\callbacks.py in _call_batch_hook(self, mode, hook, batch, logs)
    225     for callback in self.callbacks:
    226       batch_hook = getattr(callback, hook_name)
--> 227       batch_hook(batch, logs)
    228     self._delta_ts[hook_name].append(time.time() - t_before_callbacks)
    229 

~\Anaconda3\envs\lab\lib\site-packages\tensorflow\python\keras\callbacks.py in on_train_batch_end(self, batch, logs)
    507     """"""
    508     # For backwards compatibility.
--> 509     self.on_batch_end(batch, logs=logs)
    510 
    511   def on_test_batch_begin(self, batch, logs=None):

~\Anaconda3\envs\lab\lib\site-packages\tensorflow\python\keras\callbacks_v1.py in on_batch_end(self, batch, logs)
    360     self._total_batches_seen += 1
    361     if self._is_profiling:
--> 362       profiler.save(self.log_dir, profiler.stop())
    363       self._is_profiling = False
    364     elif (not self._is_profiling and

~\Anaconda3\envs\lab\lib\site-packages\tensorflow\python\eager\profiler.py in save(logdir, result)
    141       logdir, 'plugins', 'profile',
    142       datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))
--> 143   gfile.MakeDirs(plugin_dir)
    144   maybe_create_event_file(logdir)
    145   with gfile.Open(os.path.join(plugin_dir, 'local.trace'), 'wb') as f:

~\Anaconda3\envs\lab\lib\site-packages\tensorflow\python\lib\io\file_io.py in recursive_create_dir(dirname)
    446     errors.OpError: If the operation fails.
    447   """"""
--> 448   recursive_create_dir_v2(dirname)
    449 
    450 

~\Anaconda3\envs\lab\lib\site-packages\tensorflow\python\lib\io\file_io.py in recursive_create_dir_v2(path)
    462   """"""
    463   with errors.raise_exception_on_not_ok_status() as status:
--> 464     pywrap_tensorflow.RecursivelyCreateDir(compat.as_bytes(path), status)
    465 
    466 

~\Anaconda3\envs\lab\lib\site-packages\tensorflow\python\framework\errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)
    546             None, None,
    547             compat.as_text(c_api.TF_Message(self.status.status)),
--> 548             c_api.TF_GetCode(self.status.status))
    549     # Delete the underlying status object from memory otherwise it stays alive
    550     # as there is a reference to status from this from the traceback due to

NotFoundError: Failed to create a directory: logs/fit/20190315-164851\plugins\profile\2019-03-15_16-48-53; No such file or directory
```
if reinstall tensorflow==2.0.0-alpha0 and tf-nightly-gpu got another error:
```
---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
<ipython-input-6-aadf56b04ffa> in <module>
----> 1 model.fit(x_train, y_train, epochs=50, callbacks=[tensorboard_callback])
      2 
      3 model.evaluate(x_test, y_test)

~\Anaconda3\envs\lab\lib\site-packages\tensorflow\python\keras\engine\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    871           validation_steps=validation_steps,
    872           validation_freq=validation_freq,
--> 873           steps_name='steps_per_epoch')
    874 
    875   def evaluate(self,

~\Anaconda3\envs\lab\lib\site-packages\tensorflow\python\keras\engine\training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)
    202       samples=num_samples_or_steps,
    203       verbose=0,  # Handle ProgBarLogger separately in this loop.
--> 204       mode=mode)
    205   # TODO(omalleyt): Handle ProgBar as part of Callbacks once hooks are ready.
    206   progbar = training_utils.get_progbar(model, count_mode)

~\Anaconda3\envs\lab\lib\site-packages\tensorflow\python\keras\callbacks.py in configure_callbacks(callbacks, model, do_validation, batch_size, epochs, steps_per_epoch, samples, verbose, count_mode, mode)
     94   # Set callback model
     95   callback_model = model._get_callback_model()  # pylint: disable=protected-access
---> 96   callback_list.set_model(callback_model)
     97 
     98   set_callback_parameters(

~\Anaconda3\envs\lab\lib\site-packages\tensorflow\python\keras\callbacks.py in set_model(self, model)
    208     self.model = model
    209     for callback in self.callbacks:
--> 210       callback.set_model(model)
    211 
    212   def _call_batch_hook(self, mode, hook, batch, logs=None):

~\Anaconda3\envs\lab\lib\site-packages\tensorflow\python\keras\callbacks.py in set_model(self, model)
   1213     self.model = model
   1214     with context.eager_mode():
-> 1215       self._initialize_writers()
   1216       if self.write_graph:
   1217         if model.run_eagerly:

~\Anaconda3\envs\lab\lib\site-packages\tensorflow\python\keras\callbacks.py in _initialize_writers(self)
   1251       return summary_ops_v2.create_file_writer_v2(path)
   1252 
-> 1253     self._train_writer = create_writer('train')
   1254     self._writers.append(self._train_writer)
   1255     self._validation_writer = create_writer('validation')

~\Anaconda3\envs\lab\lib\site-packages\tensorflow\python\keras\callbacks.py in create_writer(subdir)
   1249     def create_writer(subdir):
   1250       path = os.path.join(self.log_dir, subdir)
-> 1251       return summary_ops_v2.create_file_writer_v2(path)
   1252 
   1253     self._train_writer = create_writer('train')

~\Anaconda3\envs\lab\lib\site-packages\tensorflow\python\ops\summary_ops_v2.py in create_file_writer_v2(logdir, max_queue, flush_millis, filename_suffix, name)
    377               filename_suffix=filename_suffix),
    378           name=name,
--> 379           v2=True)
    380 
    381 

~\Anaconda3\envs\lab\lib\site-packages\tensorflow\python\ops\summary_ops_v2.py in __init__(self, shared_name, init_op_fn, name, v2)
    197     # TODO(nickfelt): cache other constructed ops in graph mode
    198     self._init_op_fn = init_op_fn
--> 199     self._init_op = init_op_fn(self._resource)
    200     self._v2 = v2
    201     self._closed = False

~\Anaconda3\envs\lab\lib\site-packages\tensorflow\python\ops\gen_summary_ops.py in create_summary_file_writer(writer, logdir, max_queue, flush_millis, filename_suffix, name)
    190       else:
    191         message = e.message
--> 192       _six.raise_from(_core._status_to_exception(e.code, message), None)
    193   # Add nodes to the TensorFlow graph.
    194   _, _, _op = _op_def_lib._apply_op_helper(

~\Anaconda3\envs\lab\lib\site-packages\six.py in raise_from(value, from_value)

NotFoundError: Failed to create a directory: logs/fit/20190315-171835\train; No such file or directory [Op:CreateSummaryFileWriter]
```

Any suggestion to fix? Thanks.

**Describe the expected behavior**
should be trained and logged

**Code to reproduce the issue**
```
from __future__ import absolute_import, division, print_function
import tensorflow as tf
import datetime
from tensorflow.keras.callbacks import TensorBoard

mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation='softmax')
])

log_dir=""logs/fit/"" + datetime.datetime.now().strftime(""%Y%m%d-%H%M%S"")
tensorboard_callback = TensorBoard(log_dir)

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
model.fit(x_train, y_train, epochs=50, callbacks=[tensorboard_callback])
model.evaluate(x_test, y_test)
```

**Other info / logs**"
26731,Error building Tensorflow Lite on AARCH64,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): N/A
- TensorFlow version: the latest
- Python version: N/A
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):  gcc version 5.4.0 20160609 
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the problem**
I am trying to build Tensorflow Lite for ARM64 boards.
I followed the instructions on  https://tensorflow.google.cn/lite/guide/build_arm64 and executed the following commands:

    sudo apt-get update
    sudo apt-get install crossbuild-essential-arm64
    ./tensorflow/lite/tools/make/download_dependencies.sh
    ./tensorflow/lite/tools/make/build_aarch64_lib.sh

But at the last step got lots of errors such as: 


./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3823:22: note: use -flax-vector-conversions to permit conversions between vectors with differing element types or numbers of subparts
       filter_reg_0_b = vdupq_n_u8(kSignBit);
                      ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3823:22: error: cannot convert ‘uint8x16_t {aka __vector(16) unsigned char}’ to ‘int8x16_t {aka __vector(16) signed char}’ in assignment
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3824:22: error: cannot convert ‘uint8x16_t {aka __vector(16) unsigned char}’ to ‘int8x16_t {aka __vector(16) signed char}’ in assignment
       filter_reg_1_b = vdupq_n_u8(kSignBit);
                      ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3825:22: error: cannot convert ‘uint8x16_t {aka __vector(16) unsigned char}’ to ‘int8x16_t {aka __vector(16) signed char}’ in assignment
       filter_reg_2_b = vdupq_n_u8(kSignBit);
                      ^
In file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:21:0,
                 from tensorflow/lite/kernels/depthwise_conv.cc:25:
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:50:71: error: cannot convert ‘int8x16_t {aka __vector(16) signed char}’ to ‘uint64x2_t {aka __vector(2) long unsigned int}’ for argument ‘2’ to ‘uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)’
   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)
                                                                       ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3828:24: note: in expansion of macro ‘vld1q_lane_s8x8’
       filter_reg_0_a = vld1q_lane_s8x8(filter_block_ptr, filter_reg_0_a, 0);
                        ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:50:71: error: cannot convert ‘int8x16_t {aka __vector(16) signed char}’ to ‘uint64x2_t {aka __vector(2) long unsigned int}’ for argument ‘2’ to ‘uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)’
   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)
                                                                       ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3830:24: note: in expansion of macro ‘vld1q_lane_s8x8’
       filter_reg_0_b = vld1q_lane_s8x8(filter_block_ptr, filter_reg_0_b, 0);
                        ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:50:71: error: cannot convert ‘int8x16_t {aka __vector(16) signed char}’ to ‘uint64x2_t {aka __vector(2) long unsigned int}’ for argument ‘2’ to ‘uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)’
   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)
                                                                       ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3832:24: note: in expansion of macro ‘vld1q_lane_s8x8’
       filter_reg_0_a = vld1q_lane_s8x8(filter_block_ptr, filter_reg_0_a, 1);
                        ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:50:71: error: cannot convert ‘int8x16_t {aka __vector(16) signed char}’ to ‘uint64x2_t {aka __vector(2) long unsigned int}’ for argument ‘2’ to ‘uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)’
   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)
                                                                       ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3834:24: note: in expansion of macro ‘vld1q_lane_s8x8’
       filter_reg_1_a = vld1q_lane_s8x8(filter_block_ptr, filter_reg_1_a, 0);
                        ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:50:71: error: cannot convert ‘int8x16_t {aka __vector(16) signed char}’ to ‘uint64x2_t {aka __vector(2) long unsigned int}’ for argument ‘2’ to ‘uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)’
   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)
                                                                       ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3836:24: note: in expansion of macro ‘vld1q_lane_s8x8’
       filter_reg_1_b = vld1q_lane_s8x8(filter_block_ptr, filter_reg_1_b, 0);
                        ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:50:71: error: cannot convert ‘int8x16_t {aka __vector(16) signed char}’ to ‘uint64x2_t {aka __vector(2) long unsigned int}’ for argument ‘2’ to ‘uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)’
   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)
                                                                       ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3838:24: note: in expansion of macro ‘vld1q_lane_s8x8’
       filter_reg_1_a = vld1q_lane_s8x8(filter_block_ptr, filter_reg_1_a, 1);
                        ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:50:71: error: cannot convert ‘int8x16_t {aka __vector(16) signed char}’ to ‘uint64x2_t {aka __vector(2) long unsigned int}’ for argument ‘2’ to ‘uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)’
   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)
                                                                       ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3840:24: note: in expansion of macro ‘vld1q_lane_s8x8’
       filter_reg_2_a = vld1q_lane_s8x8(filter_block_ptr, filter_reg_2_a, 0);
                        ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:50:71: error: cannot convert ‘int8x16_t {aka __vector(16) signed char}’ to ‘uint64x2_t {aka __vector(2) long unsigned int}’ for argument ‘2’ to ‘uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)’
   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)
                                                                       ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3842:24: note: in expansion of macro ‘vld1q_lane_s8x8’
       filter_reg_2_b = vld1q_lane_s8x8(filter_block_ptr, filter_reg_2_b, 0);
                        ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:50:71: error: cannot convert ‘int8x16_t {aka __vector(16) signed char}’ to ‘uint64x2_t {aka __vector(2) long unsigned int}’ for argument ‘2’ to ‘uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)’
   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)
                                                                       ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3844:24: note: in expansion of macro ‘vld1q_lane_s8x8’
       filter_reg_2_a = vld1q_lane_s8x8(filter_block_ptr, filter_reg_2_a, 1);
                        ^
In file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:21:0,
                 from tensorflow/lite/kernels/depthwise_conv.cc:25:
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3846:57: error: cannot convert ‘const uint8x16_t {aka const __vector(16) unsigned char}’ to ‘int8x16_t {aka __vector(16) signed char}’ for argument ‘2’ to ‘int8x16_t veorq_s8(int8x16_t, int8x16_t)’
       filter_reg_0_a = veorq_s8(filter_reg_0_a, sign_bit);
                                                         ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3847:57: error: cannot convert ‘const uint8x16_t {aka const __vector(16) unsigned char}’ to ‘int8x16_t {aka __vector(16) signed char}’ for argument ‘2’ to ‘int8x16_t veorq_s8(int8x16_t, int8x16_t)’
       filter_reg_0_b = veorq_s8(filter_reg_0_b, sign_bit);
                                                         ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3848:57: error: cannot convert ‘const uint8x16_t {aka const __vector(16) unsigned char}’ to ‘int8x16_t {aka __vector(16) signed char}’ for argument ‘2’ to ‘int8x16_t veorq_s8(int8x16_t, int8x16_t)’
       filter_reg_1_a = veorq_s8(filter_reg_1_a, sign_bit);
                                                         ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3849:57: error: cannot convert ‘const uint8x16_t {aka const __vector(16) unsigned char}’ to ‘int8x16_t {aka __vector(16) signed char}’ for argument ‘2’ to ‘int8x16_t veorq_s8(int8x16_t, int8x16_t)’
       filter_reg_1_b = veorq_s8(filter_reg_1_b, sign_bit);
                                                         ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3850:57: error: cannot convert ‘const uint8x16_t {aka const __vector(16) unsigned char}’ to ‘int8x16_t {aka __vector(16) signed char}’ for argument ‘2’ to ‘int8x16_t veorq_s8(int8x16_t, int8x16_t)’
       filter_reg_2_a = veorq_s8(filter_reg_2_a, sign_bit);
                                                         ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3851:57: error: cannot convert ‘const uint8x16_t {aka const __vector(16) unsigned char}’ to ‘int8x16_t {aka __vector(16) signed char}’ for argument ‘2’ to ‘int8x16_t veorq_s8(int8x16_t, int8x16_t)’
       filter_reg_2_b = veorq_s8(filter_reg_2_b, sign_bit);
                                                         ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h: In static member function ‘static void tflite::optimized_ops::depthwise_conv::PackMacroBlock<(tflite::DepthwiseConvImplementation)3, (tflite::DepthwiseConvDepthMultiplication)0, 0>::PackMacroBlockNeon(const uint8*, int8*, const tflite::optimized_ops::depthwise_conv::DepthwiseConvDotProdParams*)’:
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3954:53: error: cannot convert ‘uint8x16_t {aka __vector(16) unsigned char}’ to ‘const int8x16_t {aka const __vector(16) signed char}’ in initialization
     const int8x16_t perm_data_0 = vld1q_u8(perm_data);
                                                     ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3955:58: error: cannot convert ‘uint8x16_t {aka __vector(16) unsigned char}’ to ‘const int8x16_t {aka const __vector(16) signed char}’ in initialization
     const int8x16_t perm_data_1 = vld1q_u8(perm_data + 16);
                                                          ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3956:58: error: cannot convert ‘uint8x16_t {aka __vector(16) unsigned char}’ to ‘const int8x16_t {aka const __vector(16) signed char}’ in initialization
     const int8x16_t perm_data_2 = vld1q_u8(perm_data + 32);
                                                          ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3957:58: error: cannot convert ‘uint8x16_t {aka __vector(16) unsigned char}’ to ‘const int8x16_t {aka const __vector(16) signed char}’ in initialization
     const int8x16_t perm_data_3 = vld1q_u8(perm_data + 48);



How can I fix it?"
26730,"tflite GPU only support  one batch input?if not,how can I set my mult-dim inputs like [16,256,256,3]","here is my log in java:
java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: GpuDelegate Prepare: First dimension is supposed to be BATCH and always equal to 1.Node number 19 (GpuDelegate) failed to prepare.
"
26728,libc.so.6 : GLIBC_2.14 not found. Instruct Tensorflow to look for glibc in custom path.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.13.1
- Python version: 3.6.2
- Installed using virtualenv? pip? conda?: Still at build phase
- Bazel version (if compiling from source): 0.20.0
- GCC/Compiler version (if compiling from source): gcc-5.5.0 (installed from Linuxbrew)
- CUDA/cuDNN version: N/A
- GPU model and memory: 16GB RAM


**Describe the problem**

My computer has a rather old `glibc-2.12` and since I'm not an admin, there's nothing I can do to change that. Upon trying to build Tensorflow, I'd get an error along the lines of `$SECRET_PATH/lib64/libc.so.6 : GLIBC_2.14 not found`, which implies that the Tensorflow build system looks for `glibc` in the standard system paths.

However, I have been able to install `glibc-2.23` into `$HOME/.linuxbrew`. I cannot simply do `LD_LIBRARY_PATH=$HOME/.linuxbrew/lib:$LD_LIBRARY_PATH <tensorflow build commands>` because it would segfault all the other utilities.

How do I instruct the Tensorflow build system to look for `glibc` within `$HOME/.linuxbrew`? Which files should I edit?

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
$ cd ~
$ mkdir build-bazel
$ cd build-bazel
$ wget https://github.com/bazelbuild/bazel/releases/download/0.20.0/bazel-0.20.0-dist.zip
$ unzip bazel-0.20.0-dist.zip
$ env EXTRA_BAZEL_ARGS=""--host_javabase=@local_jdk//:jdk"" bash ./compile.sh
$ cd ~
$ mkdir build-tensorflow
$ cd build-tensorflow
$ wget https://codeload.github.com/tensorflow/tensorflow/zip/v1.13.1 -O tensorflow-1.13.1.zip
$ unzip tensorflow-1.13.1.zip
$ cd tensorflow-1.13.1
$ PATH=$HOME/build-bazel/output:$PATH ./configure
$ PATH=$HOME/build-bazel/output:$PATH bazel build --config=monolithic //tensorflow/tools/pip_package:build_pip_package
```

In another issue #26432, I had succeeded in building Tensorflow with Python 3.7.2 (installed from Linuxbrew, and likely linked with the Linuxbrew `glibc-2.23`). In this issue, however, I am building it with Python 3.6.2 (official office Python), which I suspect may have been linked with the old `glibc-2.12`.

For various reasons I need to build with Python 3.6.2.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26727,[Feature] Support RISCV with tfcompile --target_tuple,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.13.0
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
third_party/llvm/llvm.autogenerated.BUILD supports AArch64, ARM, PowerPC, and X86 in llvm_target_list, so I can use 'tfcompile --target_triple=aarch64-none-android' to generate  AArch64 ELF object.
It seems the tool that generates llvm.autogenerated.BUILD is not released yet, I suggest adding a RISCV target in llvm.autogenerated.BUILD.

**Will this change the current api? How?**
No.
**Who will benefit with this feature?**
People who use RISCV cpus.
**Any Other info.**
"
26726,densetnet.pb not correct result,"Hi, 
I download to model from https://storage.googleapis.com/download.tensorflow.org/models/tflite/model_zoo/upload_20180427/densenet_2018_04_27.tgz
There are:
* densenet.pb
* label.txt
* readme.md (said that densenet.pb use vgg_preprocess)
inside `densenet_2018_04_27`:

 I try one random picture `bike.jpg`, the result top1 is `828`. However, the 828-th category in `label.txt` is not something like bike.

My proprocess is:
```
 img=cv2.imread(img_path)
 img=cv2.resize(img_path,(224,224))
 img=img.astype(np.float32)[...,::-1]
 img-=[123.68,116.78,103.94]
 img/=128.
```
"
26725,BaseCollectiveExecutor::StartAbort Invalid argument: Upper bound check fail is reported with CollectiveAllReduceStrategy,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  CentOS Linux release 7.4.1708 (Core)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): ('v1.13.1-0-g6612da8951', '1.13.1')
- Python version: 2.7
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0
- GPU model and memory: Tesla P100, 16G


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
When running transformer within the tensor2tensor library with CollectiveAllReduceStrategy via little customized change for distributed training in t2t only, it reported below error:

INFO:tensorflow:Graph was finalized.
WARNING:tensorflow:From /home/xh/tf-1.13.1/lib/python2.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
INFO:tensorflow:Restoring parameters from /home/xh/datasets/t2t_train_enzh_multi-gpu-ok/translate_enzh_wmt32k/transformer-transformer_base/model.ckpt-179000
2019-03-14 15:30:36.585213: I tensorflow/core/distributed_runtime/master_session.cc:1192] Start master session 4c2e055e28f41c2f with config: device_filters: ""/job:worker/task:0"" device_filters: ""/job:worker/task:0"" gpu_options { per_process_gpu_memory_fraction: 0.95 } allow_soft_placement: true graph_options { optimizer_options { global_jit_level: OFF } rewrite_options { scoped_allocator_optimization: ON scoped_allocator_opts { enable_op: ""CollectiveReduce"" enable_op: ""CollectiveReduce"" enable_op: ""CollectiveReduce"" enable_op: ""CollectiveReduce"" } } } experimental { collective_group_leader: ""/job:worker/replica:0/task:0"" }
WARNING:tensorflow:From /home/xh/tf-1.13.1/lib/python2.7/site-packages/tensorflow/python/training/saver.py:1070: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file utilities to get mtimes.
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Initialize strategy
INFO:tensorflow:Saving checkpoints for 179000 into /home/xh/datasets/t2t_train_enzh_multi-gpu-ok/translate_enzh_wmt32k/transformer-transformer_base/model.ckpt.
2019-03-14 15:32:18.009875: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally
2019-03-14 15:32:28.447121: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:101] Filling up shuffle buffer (this may take a while): 387 of 512
2019-03-14 15:32:31.537519: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:140] Shuffle buffer filled.
2019-03-14 15:32:31.695275: W tensorflow/core/common_runtime/base_collective_executor.cc:203] BaseCollectiveExecutor::StartAbort Invalid argument: Upper bound check fail for input 1 from node training/gradients/transformer/parallel_0_5/transformer/transformer/symbol_modality_32610_512_1/softmax/concat_grad/Slice_7 to node scoped_allocator_concat_232 input bounds = [0x7fb060a84f00, 0x7fb060e7ff00] backing_tensor bounds = [0x7faf8da62700, 0x7faf8fe35700]
	 [[{{node scoped_allocator_concat_232}}]]
2019-03-14 15:32:31.708208: W tensorflow/core/common_runtime/base_collective_executor.cc:203] BaseCollectiveExecutor::StartAbort Invalid argument: Upper bound check fail for input 1 from node training/gradients/transformer/parallel_0_5/transformer/transformer/symbol_modality_32610_512_1/softmax/concat_grad/Slice_7 to node scoped_allocator_concat_232 input bounds = [0x7fb060a84f00, 0x7fb060e7ff00] backing_tensor bounds = [0x7faf8da62700, 0x7faf8fe35700]
	 [[{{node scoped_allocator_concat_232}}]]
	 [[{{node _send_add_1_0}}]]
InvalidArgumentError: InvalidA...ntError()

**Describe the expected behavior**
What does above error log mean, and is there any way to fix it? thanks.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26721,Inconsistent encoding leads to AttributeError (reopen),"System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N/A
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Red Hat 7.5
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
TensorFlow installed from (source or binary): compiled from source
TensorFlow version (use command below): r13.1
Python version: 3.6.3
Bazel version (if compiling from source): 0.22
GCC/Compiler version (if compiling from source): 4.8.5
CUDA/cuDNN version: 10.0
GPU model and memory: P5000/16G
DeepMind open-sourced the implementation of IMPALA: https://github.com/deepmind/scalable_agent

For parallelism, they wrap a mechanism which is class-based in doing so, the file is: https://github.com/deepmind/scalable_agent/blob/master/py_process.py

There is a code snippet at the beginning of the file:
```

  class Zeros(object):
    def __init__(self, dim0):
      self._dim0 = dim0
    def compute(self, dim1):
      return np.zeros([self._dim0, dim1], dtype=np.int32)
    @staticmethod
    def _tensor_specs(method_name, kwargs, constructor_kwargs):
      dim0 = constructor_kwargs['dim0']
      dim1 = kwargs['dim1']
      if method_name == 'compute':
        return tf.contrib.framework.TensorSpec([dim0, dim1], tf.int32)
  with tf.Graph().as_default():
    p = py_process.PyProcess(Zeros, 1)
    result = p.proxy.compute(2)
    with tf.train.SingularMonitoredSession(
        hooks=[py_process.PyProcessHook()]) as session:
      print(session.run(result))  # Prints [[0, 0]].

```
however, when I tried to run it, I got the following error:

```
2019-03-01 17:37:16.260732: W tensorflow/core/framework/op_kernel.cc:1389] Unknown: AttributeError: 'Zeros' object has no attribute 'b'compute''
Traceback (most recent call last):

  File ""/home/yuming/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py"", line 207, in __call__
    ret = func(*args)

  File ""/data/yuming/eeg-dpg/py_process.py"", line 89, in py_call
    raise result

AttributeError: 'Zeros' object has no attribute 'b'compute''

```
I suspect maybe there is a mismatch between encoding (python interpreter and source code?), but not for sure, since I have no problem in running the code if I use Python 2.7.

Could anyone please take some effort on looking into it?

(The first people look into it just casually and unprofessionally close it, but I would think it is a bug. If you do think it is not a bug, you can close it since I got a workaround (back to python 2.7). I already posted it on stackoverflow)"
26719,how to use the saved model after trained to detect new pictures,"Hi all,
this might be repeated question.
I trained classifier on cats, dogs. and saved the model. Now I want to use the saved trained model to detect new pictures. How to do this? Is there a code for this?

Thanx"
26716,cannot use a placeholder dependent tensor as a initializer in get_variable,"How does one go about creating variables with placeholders dependent tensors as initializers? The following graph breaks down with:



    InvalidArgumentError: You must feed a value for placeholder 'Placeholder_1' with dtype float
    	 [[node Placeholder_1 (defined at <ipython-input-10-b8d54264dc85>:3)  = Placeholder[dtype=DT_FLOAT, shape=[], _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

My code:

    tf.reset_default_graph()
    a = tf.placeholder(dtype=tf.float32,shape=())
    d = tf.placeholder(dtype=tf.float32,shape=())
    b = tf.get_variable(name='b',initializer=d)
    c=a+d
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        print(sess.run(c, feed_dict={a:5.,d:10.}))


The documentation on initializers in tensorflow says:
>Initializer for the variable if one is created. Can either be an initializer object or a Tensor. If it's a Tensor, its shape must be known unless validate_shape is False.

However if i comment out the line where i create b the code seems to run. My fetch is not even dependent upon b.

How do i go about creating variables that initialize according to some placeholder?"
26715,Docker latest images are using tf 2.0.0a0,"
**System information**
- Have I written custom code: **No**
- OS Platform and Distribution: **Linux Ubuntu 16.04**
- TensorFlow installed from: **binary**
- TensorFlow version (use command below): **2.0.0a0** (script is however broken, see below)
- Python version: 2.7.12 / 3.5.2
- CUDA/cuDNN version: **any**
- GPU model and memory: **any**


> You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
> You can also obtain the TensorFlow version with
> python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
> 
While this is not the issue itself, its worth mentioning that both are broken 
```
Traceback (most recent call last):
  File ""/tmp/check_tf.py"", line 2, in <module>
    print(""tf.VERSION = %s"" % tf.VERSION)
AttributeError: 'module' object has no attribute 'VERSION'
```

**Describe the current behavior**
**The Docker `latest` family contains tensorflow version 2.0.0a0.** 

**Describe the expected behavior**
The Docker `latest` family should contain  tensorflow version 1.13.*. 

According to the docs (and docker convention), the `latest` docker images should contain the latest  **stable** version, as in 1.13 while the nightly images should contain the unstable images, such us alphas, betas and RCs
**Code to reproduce the issue**
`docker run -it --rm tensorflow/tensorflow  pip list 2>&1 | grep tensorflow`
`docker run -it --rm tensorflow/tensorflow:latest-py3  pip list 2>&1 | grep tensorflow`
(the issue persists in docker versions as well)

**Other info / logs**
<img width=""311"" alt=""Screen Shot 2019-03-14 at 22 56 17"" src=""https://user-images.githubusercontent.com/1699509/54392157-8f5d7a80-46af-11e9-8b00-9e4e1979db6a.png"">
"
26714,Error when using the class_weight parameter in the fit function in tensorflow.python.keras,"- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow version: 1.13.1
- Python version: 3.6

I wanted to test my network on a small set of records with two imbalanced classes (0 and 1). I was going to use the _class_weight_ parameter in the _fit_ function to improve the balance, but unfortunately there were problems probably associated with tensor support. Without the _class_weight_ parameter, the fit function works correctly. I also tried to use tensorflow.python.keras.layers.Input as an input to the Xception model, but with the same effect.

```
import tensorflow as tf
from tensorflow.python.keras.layers import Dense, Dropout
from tensorflow.python.keras.applications.xception import Xception, preprocess_input
from tensorflow.python.keras.models import Model
from tensorflow.python.keras.optimizers import Adam

# parsing images and labels from TFRecords
def parse_function(proto):
    example = {'image_raw': tf.FixedLenFeature([], tf.string), 'label': tf.FixedLenFeature([], tf.int64)}
    parsed_example = tf.parse_single_example(proto, example)
    image = tf.decode_raw(parsed_example['image_raw'], tf.uint8)
    image = tf.reshape(image, [HEIGHT, WIDTH, DEPTH])
    image = preprocess_input(tf.cast(image, tf.float32))
    return image, parsed_example['label']

def get_data(filepath, schuffle_size=32, batch_size=8, prefetch=1, repeat=None, num_parallel_calls=1):
    dataset = tf.data.TFRecordDataset(filepath)
    if schuffle_size != 0:
        dataset = dataset.shuffle(schuffle_size)
    dataset = dataset.repeat(repeat)
    dataset = dataset.map(parse_function, num_parallel_calls=num_parallel_calls)
    dataset = dataset.batch(batch_size)
    dataset = dataset.prefetch(prefetch)
    iterator = dataset.make_one_shot_iterator()
    return iterator

def build_model(number_of_neurons_in_dense_layer, dropout, learning_rate):
    base_model = Xception(weights='imagenet', include_top=False, pooling='avg', input_shape=(HEIGHT, WIDTH, 3))
    for layer in base_model.layers:
        layer.trainable = True
    x = base_model.output
    x = Dropout(dropout)(x)
    x = Dense(number_of_neurons_in_dense_layer, activation='relu')(x)
    x = Dropout(dropout)(x)
    logits = Dense(NUMBER_OF_CLASSES, activation='softmax')(x)
    model = Model(inputs=base_model.input, outputs=logits)
    model.compile(optimizer=Adam(lr=learning_rate), loss='sparse_categorical_crossentropy', metrics=['categorical_accuracy'])
    return model

global NUMBER_OF_CLASSES, HEIGHT, WIDTH, DEPTH
NUMBER_OF_CLASSES = 2
...
CLASS_WEIGHTS = {
        0: 1,
        1: 7
       }
model = build_model(64, 0.4, 0.001)
train = get_data(..., 8, 2, num_parallel_calls=8)
val = get_data(...., 0, 4, num_parallel_calls=8)
model.fit(train, validation_data=val, epochs=3,steps_per_epoch=8//2,
           validation_steps=8//4, shuffle=False, 
           class_weight=CLASS_WEIGHTS)
```

```
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py"", line 51, in _wrapfunc
    return getattr(obj, method)(*args, **kwds)
AttributeError: 'Tensor' object has no attribute 'reshape'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""./model.py"", line 137, in main
    model.fit(train, validation_data=val, epochs=3, steps_per_epoch=8 // 2, validation_steps=8 // 4, shuffle=False, class_weight=CLASS_WEIGHTS)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py"", line 776, in fit
    shuffle=shuffle)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py"", line 2432, in _standardize_user_data
    feed_sample_weight_modes)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py"", line 2431, in <listcomp>
    for (ref, sw, cw, mode) in zip(y, sample_weights, class_weights,
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_utils.py"", line 758, in standardize_weights
    y_classes = np.reshape(y, y.shape[0])
  File ""/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py"", line 279, in reshape
    return _wrapfunc(a, 'reshape', newshape, order=order)
  File ""/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py"", line 61, in _wrapfunc
    return _wrapit(obj, method, *args, **kwds)
  File ""/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py"", line 41, in _wrapit
    result = getattr(asarray(obj), method)(*args, **kwds)
TypeError: __index__ returned non-int (type NoneType)
```"
26713,Large Variation in Compute Times,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.1 LTS
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.12.0
- Python version: Python 2.7.15rc1
- Bazel version (if compiling from source): 0.21
- GCC/Compiler version (if compiling from source): (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
- CUDA/cuDNN version: 10.0/7.4.2
- GPU model and memory: NVIDIA Corporation GP104GL [Quadro P5000] (2 GPUs)


**Describe the current behavior**
I am running a toy matrix multiplication example multiple times on TensorFlow ([code here](https://github.com/xilenteyex/toy_matmul_tf)). When I look into the timeline for multiple iterations, I see an increase in the time to compute the same graph over multiple iterations. Specifically, look at these three timelines:
[Timeline 1](https://github.com/xilenteyex/toy_matmul_tf/blob/master/logs/timeline_matmul_2.15_1_.ctf.json)
[Timeline 3](https://github.com/xilenteyex/toy_matmul_tf/blob/master/logs/timeline_matmul_2.15_3_.ctf.json)
[Timeline 8](https://github.com/xilenteyex/toy_matmul_tf/blob/master/logs/timeline_matmul_2.15_8_.ctf.json)
For **MatMul_25**, [first iteration](https://github.com/xilenteyex/toy_matmul_tf/blob/master/logs/timeline_matmul_2.15_1_.ctf.json) shows compute time to be ~8000 ms, while [3rd iteration](https://github.com/xilenteyex/toy_matmul_tf/blob/master/logs/timeline_matmul_2.15_3_.ctf.json) shows that compute time is ~15000 ms and [8th iteration](https://github.com/xilenteyex/toy_matmul_tf/blob/master/logs/timeline_matmul_2.15_8_.ctf.json) shows that compute time is ~27000 ms.

**Describe the expected behavior**
I expect compute times of the same matrix multiplication operation to be stay similar over multiple iterations instead of increasing linearly. Is this the expected behavior or am I missing something ?

**Code to reproduce the issue**
[code to rerun the toy example can be found here.](https://github.com/xilenteyex/toy_matmul_tf)

**Other info / logs**
[Timelines generated when I ran the code are in this folder](https://github.com/xilenteyex/toy_matmul_tf/tree/master/logs)
"
26711,Numpy operation on List of Tensor is considerably slow.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Docker
- TensorFlow version (use command below): 2.0.0-alpha
- Python version: 3.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.
- GPU model and memory: V100


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
Numpy operations on a list of Tensor is considerably slow. Commands to reproduce the results and corresponding time are shown below in the screenshot below.

![image](https://user-images.githubusercontent.com/18187806/54388179-8f3f8980-4673-11e9-8009-a076402333c4.png)




**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26709,Mirrored Strategy example code erroring out,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
rhel 7.5
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 1.12.0
- Python version: 2.7
- Bazel version (if compiling from source): 0.15.0
- GCC/Compiler version (if compiling from source): 4.8.5
- CUDA/cuDNN version: release 9.1, V9.1.85
- GPU model and memory:  C4130 K80, 30404 MB


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
error 

**Describe the expected behavior**
model to fit using the gpus 

**Code to reproduce the issue**
```
features = tf.data.Dataset.from_tensors([1.]).repeat(10000).batch(10)
labels = tf.data.Dataset.from_tensors([1.]).repeat(10000).batch(10)
train_dataset = tf.data.Dataset.zip((features, labels))

distribution = tf.contrib.distribute.MirroredStrategy(['/device:CPU:0', '/device:GPU:0', '/device:GPU:1'])

with distribution.scope():
  inputs = tf.keras.layers.Input(shape=(1,))
  predictions = tf.keras.layers.Dense(1)(inputs)
  model = tf.keras.models.Model(inputs=inputs, outputs=predictions)

  model.compile(loss='mean_squared_error',
                optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.2))

model.fit(train_dataset, epochs=5, steps_per_epoch=10)

```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```
ValueError                                Traceback (most recent call last)
<ipython-input-7-4446125ee1ea> in <module>()
----> 1 model.fit(train_dataset, epochs=5, steps_per_epoch=10)

/jump/software/rhel7/python27_tensorflow-1.12.0/lib/tensorflow/python/keras/engine/training.pyc in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)
   1637           initial_epoch=initial_epoch,
   1638           steps_per_epoch=steps_per_epoch,
-> 1639           validation_steps=validation_steps)
   1640
   1641   def evaluate(self,

/jump/software/rhel7/python27_tensorflow-1.12.0/lib/tensorflow/python/keras/engine/training_arrays.pyc in fit_loop(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)
     84       ValueError: in case of invalid arguments.
     85   """"""
---> 86   model._make_train_function()
     87   f = model.train_function
     88

/jump/software/rhel7/python27_tensorflow-1.12.0/lib/tensorflow/python/keras/engine/training.pyc in _make_train_function(self)
    698           # Training updates
    699           updates = self.optimizer.get_updates(
--> 700               params=self._collected_trainable_weights, loss=self.total_loss)
    701         # Unconditional updates
    702         updates += self.get_updates_for(None)

/jump/software/rhel7/python27_tensorflow-1.12.0/lib/tensorflow/python/keras/optimizers.pyc in get_updates(self, loss, params)
    730       # incremented in optimizer.apply_gradients()
    731       self.updates = []
--> 732       grads = self.optimizer.compute_gradients(loss, params)
    733       opt_update = self.optimizer.apply_gradients(
    734           grads, global_step=self.iterations)

/jump/software/rhel7/python27_tensorflow-1.12.0/lib/tensorflow/python/training/optimizer.pyc in compute_gradients(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)
    509     var_list += ops.get_collection(ops.GraphKeys._STREAMING_MODEL_PORTS)
    510     # pylint: enable=protected-access
--> 511     processors = [_get_processor(v) for v in var_list]
    512     if not var_list:
    513       raise ValueError(""No variables to optimize."")

/jump/software/rhel7/python27_tensorflow-1.12.0/lib/tensorflow/python/training/optimizer.pyc in _get_processor(v)
    202     # True if and only if `v` was initialized eagerly.
    203     return _DenseResourceVariableProcessor(v)
--> 204   if v.op.type == ""VarHandleOp"":
    205     return _DenseResourceVariableProcessor(v)
    206   if isinstance(v, variables.Variable):

/jump/software/rhel7/python27_tensorflow-1.12.0/lib/tensorflow/contrib/distribute/python/values.pyc in op(self)
    306                               self._primary_var.op.graph,
    307                               self._primary_var.op.type)
--> 308     return self.get().op
    309
    310   @property

/jump/software/rhel7/python27_tensorflow-1.12.0/lib/tensorflow/contrib/distribute/python/values.pyc in get(self, device)
     74       six.raise_from(
     75           ValueError(""Device %s not found in %s (current device %s)"" %
---> 76                      (device, self._index.keys(), device_util.current())), e)
     77
     78   def on_device(self, device):

/jump/software/rhel7/python27_six-1.10.0/lib/python2.7/site-packages/six.py in raise_from(value, from_value)
    716 else:
    717     def raise_from(value, from_value):
--> 718         raise value
    719
    720

ValueError: Device /replica:0/task:0/device:CPU:0 not found in ['/replica:0/task:0/device:GPU:0', '/replica:0/task:0/device:GPU:1', '/replica:0/task:0/device:GPU:2', '/replica:0/task:0/device:GPU:3'] (current device )
```"
26708,Attempting to convert model from basic_classification dataset to TFLite,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- OS Platform: Windows 10 x64
- Tensorflow installed from: tf-nightly-2.0-preview
- TensorFlow version: 2.0 Preview
- Python version: 3.6
- Installed using: pip
- Doc Link:
https://www.tensorflow.org/alpha/tutorials/keras/basic_classification
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/r2/convert/concrete_function.md

I am starting from the basic_classification dataset and attempting to save the model result as tflite but can't figure out how to perform the operation correctly.  When I export the saved model it tells me that the function will only be available through the v1 compatibility library and that 'Export includes no default signature'.  Trying to create the concrete function directly off the generated model returns the error 'Sequential object has no attribute 'signatures''.  

It would be really nice if those example datasets included in their instructions how to save the model as a tflite model.

I'd also like to restore a v1 frozen model and save it as a tflite model but can't figure that out either.  The instructions about concrete functions appear to imply that only Tensorflow 2.0 models (properly created) can be converted to tflite models.

Can v1 models be saved as v2 tflite models or do v1 models have to be saved using the compat.v1 TFLiteConverter only?"
26707,[TF2.0] Sequential API serialization bug,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
docker image from tensorflow/tensorflow:2.0.0a0-gpu-jupyter
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
git version 'v1.12.0-9492-g2c319fb415'
tensorflow version '2.0.0-alpha0'
- Python version:
2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
cuda version 415.27
- GPU model and memory:
GTX 1080Ti

**Describe the current behavior**
If I serialize a model built using the Sequential API and recreate the model from the config it fails.



**Describe the expected behavior**
The model to be initialized from the configuration

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import tensorflow as tf
import numpy as np

data = np.random.random((1000, 32))
labels = np.random.random((1000, 10))
model = tf.keras.models.Sequential([
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation='softmax')
])

# The compile step specifies the training configuration
model.compile(optimizer=tf.keras.optimizers.RMSprop(0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.fit(data, labels, batch_size=32, epochs=5)
config = model.get_config()
new_model = tf.keras.Model.from_config(config)
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
------------------------------------------------------------
KeyError                   Traceback (most recent call last)
<ipython-input-48-db6b212995c2> in <module>()
     12 model.fit(data, labels, batch_size=32, epochs=5)
     13 config = model.get_config()
---> 14 new_model = keras.Model.from_config(config)

/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/network.pyc in from_config(cls, config, custom_objects)
   1229     # First, we create all layers and enqueue nodes to be processed
   1230     for layer_data in config['layers']:
-> 1231       process_layer(layer_data)
   1232     # Then we process nodes in order of layer depth.
   1233     # Nodes that cannot yet be processed (if the inbound node

/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/network.pyc in process_layer(layer_data)
   1208           ValueError: In case of improperly formatted `layer_data` dict.
   1209       """"""
-> 1210       layer_name = layer_data['name']
   1211 
   1212       # Instantiate layer.

KeyError: 'name'
```
```
print(config)
{'build_input_shape': (None, 32),
 'layers': [{'class_name': 'Dense',
   'config': {'activation': 'relu',
    'activity_regularizer': None,
    'bias_constraint': None,
    'bias_initializer': {'class_name': 'Zeros', 'config': {}},
    'bias_regularizer': None,
    'dtype': 'float32',
    'kernel_constraint': None,
    'kernel_initializer': {'class_name': 'GlorotUniform',
     'config': {'seed': None}},
    'kernel_regularizer': None,
    'name': 'dense_6',
    'trainable': True,
    'units': 128,
    'use_bias': True}},
  {'class_name': 'Dropout',
   'config': {'dtype': 'float32',
    'name': 'dropout_3',
    'noise_shape': None,
    'rate': 0.2,
    'seed': None,
    'trainable': True}},
  {'class_name': 'Dense',
   'config': {'activation': 'softmax',
    'activity_regularizer': None,
    'bias_constraint': None,
    'bias_initializer': {'class_name': 'Zeros', 'config': {}},
    'bias_regularizer': None,
    'dtype': 'float32',
    'kernel_constraint': None,
    'kernel_initializer': {'class_name': 'GlorotUniform',
     'config': {'seed': None}},
    'kernel_regularizer': None,
    'name': 'dense_7',
    'trainable': True,
    'units': 10,
    'use_bias': True}}],
 'name': 'sequential_3'}
```"
26703,[2.0] Combining metrics places updates on the wrong graph,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Ubuntu 16.04
- TensorFlow installed from: binary
- TensorFlow version: `2.0.0.dev20190311`
- Python version: 3.6.6
- CUDA/cuDNN version: 10.0

**Describe the current behavior**

I wanted to define a custom metric that is the combination of 2 metrics:

```python
import tensorflow as tf

class F1(tf.metrics.Metric):

  def __init__(self, **kwargs):
    super(F1, self).__init__(**kwargs)
    self.precision = tf.metrics.Precision()
    self.recall = tf.metrics.Recall()

  @property
  def updates(self):
    return self.precision.updates + self.recall.updates

  def update_state(self, y_true, y_pred):
    self.precision.update_state(y_true, y_pred)
    self.recall.update_state(y_true, y_pred)

  def result(self):
    precision = self.precision.result()
    recall = self.recall.result()
    return (2 * precision * recall) / (recall + precision)
```

But when run in graph mode, the `updates` operations are placed on the wrong graph which raises an error in the Estimator code:

https://github.com/tensorflow/estimator/blob/v2.0.0-alpha/tensorflow_estimator/python/estimator/model_fn.py#L497

Am I missing something here? The `F1` implementation looks correct so I suspect this is a bug.

**Describe the expected behavior**

The updates should be placed on the default graph.

**Code to reproduce the issue**

```python
with tf.Graph().as_default() as graph:
  precision = tf.metrics.Precision()
  precision.update_state([0, 0, 1], [1, 0, 1])
  print(precision.updates[0].graph is graph)  # True

  f1 = F1()
  f1.update_state([0, 0, 1], [1, 0, 1])
  print(f1.updates[0].graph is graph)  # False
```"
26700,keras: Enabling eager mode breaks weights setting per kwargs in Layer constructors,"Enabling eager mode breaks the ability to specify layer weights in the constructor.

For example passing all-zeros weights to a dense layer results in all zero outputs (for `use_bias=False`). 
This is however not the case if eager mode is enabled, as the zero weights are not set; it seems to be known (see TODO by @fchollet), but I can't see why `set_weights` should not be called in non-eager mode: https://github.com/tensorflow/tensorflow/blob/5f071f2eb2321a8e4f46f16f706e9d743ec55554/tensorflow/python/keras/engine/base_layer.py#L660


to easily reproduce comment in/out the `tf.enable_eager_execution()` bellow:

```python
import unittest
import numpy as np
from tensorflow.python import keras as tfk
import tensorflow as tf

#tf.enable_v2_behavior()
tf.enable_eager_execution()

class EagerWeightsTest(unittest.TestCase):

    def test_eager_weights(self):
        weights = np.zeros((10, 11))                           # use zero weights
        dense = tfk.layers.Dense(units=11, input_shape=(10,),
                                 weights=[weights],
                                 use_bias=False)

        model = tfk.models.Sequential()
        model.add(dense)
        model.compile(""adam"", tfk.losses.mae)

        logits = model.predict(np.random.random((3, 10)))

        self.assertTrue(np.allclose(np.zeros((3, 11)),          # expect zero outputs
                                    logits))

```"
26699,sparse_cataegorical_accuracy buggued in Keras LSTM when return_sequences=True,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): not for the original issue, then I tried some workarounds
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.13.6
- TensorFlow installed from (source or binary): pip install tensorflow
- TensorFlow version (use command below): 1.12.0, 1.13.1 and 2.0.0-dev20190313
- Python version: 3.6.7
- We are using CPU here.

**Describe the current behavior**

When training a simple neural network (one embedding layer, one LSTM layer, one dense layer) with `return_sequences=True`, the computation of sparse_categorical_accuracy crashes. The reason of the crash seems different in tensorflow 1.12.0 from one hand, and in tf 1.13.1 and tf 2.0.0 from another hand.

The setting is simple and common for each tensorflow version. See the notebooks in [this dedicated repository](https://github.com/durandg12/keras-lstm-issue) for the complete code. I generate 640 sequences of 5 random integers between 0 and 11. And I want to train an LSTM to predict the 5th element of a sequence given the 4 previous ones, with a batch size of 64. Hence the code looks like this:

```
sequence_length = 4
number_of_categories = 12
BATCH_SIZE = 64
total_examples = 640
steps_per_epoch = total_examples // BATCH_SIZE
raw_data = np.random.randint(0, number_of_categories, size=(total_examples, sequence_length+1))
raw_dataset = tf.data.Dataset.from_tensor_slices(raw_data)
```

**Describe the expected behavior**

The training should not crash because of the accuracy computation. No matter the tensorflow version, note that the training does not crash and the accuracy is properly computed when `return_sequences=False`.

**Code to reproduce the issue**

Please find all the code I used in the notebooks that I have put in [this dedicated repository](https://github.com/durandg12/keras-lstm-issue).

**Other info / logs**

For tensorflow 1.12.0, the error I get is:`Incompatible shapes: [64] vs. [64,4] [Op:Equal]`. `y_true` and `y_pred` should both be of shape `[64,4]` so I quickly figured out where the problem comes from. In the code of `sparse_categorical_accuracy` [found here](https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/python/keras/metrics.py), the first line `y_true = math_ops.reduce_max(y_true, axis=-1)` shrinks one dimension of `y_true` for apparently no reason. Hence, with a custom function that simply removes this line, training succeeds, with both `return_sequences=True` and `return_sequences=False`. See [the attached notebook](https://github.com/durandg12/keras-lstm-issue/blob/master/Fail%20LSTM%201.12.0.ipynb).

For tensorflow 1.13.1 and 2.0.0 the situation is more unclear and I couldn't find a workaround this time. This time, the error message is:

```
Can not squeeze dim[1], expected a dimension of 1, got 4
	 [[{{node metrics/sparse_categorical_accuracy_tf13/Squeeze}}]] [Op:StatefulPartitionedCall]
```

and this seems to be due to the lines

```
if (len(K.int_shape(y_true)) == len(K.int_shape(y_pred))):
    y_true = array_ops.squeeze(y_true, [-1])
```
in the code of `sparse_categorical_accuracy` [found here](https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/python/keras/metrics.py) [and here](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/metrics.py). `y_true` seems to have the expected `[64,4]` shape so we cannot squeeze it on its dimension `1`. But I don't get why we would want to squeeze it in the first place. Looking at the comment line `# If the shape of y_true is (num_samples, 1), squeeze to (num_samples,)` in the code, it is clear that something is not working as intended. My attempt to workaround was to comment the previous lines, so as to use the same workaround as I did with tensorflow 1.12.0. But this time it didn't work for an obscure reason that I can't really understand. The error message I get is:

```
Invalid reduction dimension (2 for input with 2 dimension(s)
	 [[{{node metrics_1/sparse_categorical_accuracy_tf13_v2/Sum}}]] [Op:StatefulPartitionedCall]
```
There seems to be an error with the dimensions of the input data of `sparse_categorical_accuracy`. But `y_true` should be of shape `[64,4]`, and `y_pred`should be of shape `[64,4,12]` before applying ` y_pred = math_ops.argmax(y_pred, axis=-1)` and of shape `[64,4]` after.

So I made an experiment where I replaced the sparse categorical accuracy function by a function simply computing the rank of `y_true` (this is the function `ndim_y_true` in the attached notebooks) and I found even more confusing results. Whereas for 1.12.0, `ndim_y_true` yields `2` as expected for a `[64,4]` tensor, in 1.13.1 and 2.0.0 it yields `3`. This does not make sense to me, especially since when trying to apply `y_true = array_ops.squeeze(y_true, [-1])`, the error message properly aknowledge that the last dimension is dimension `1` (recall the `Can not squeeze dim[1]` above).

When `return_sequences=False`, `ndim_y_true` also yields the strange result of `2` (this time, in the three versions of tensorflow I tried), while I expect `y_true` to be of shape `[64,]` in this case.

As a side note, it seems that adding a `TimeDistributed` layer to the `Dense` layer or not when `return_sequences=True` does not change anything."
26697,some tests in //tensorflow/lite/testing/ fail to build,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
-  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.1 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: commit 6583b83dc9393118c60a9c11f15453c650fab89f
- Python version: 2.7.15rc1
- Installed using virtualenv? pip? conda?: sources
- Bazel version (if compiling from source): 0.20.0
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
- CUDA/cuDNN version: -
- GPU model and memory: -


**Describe the problem**
Build errors occur in zip tests

**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel test -k --verbose_failures --config=opt -- //tensorflow/lite/testing/...

**Any other info / logs**
```
ERROR: /local/people/kamil_herba/tensorflow-x86/tensorflow/lite/testing/BUILD:21:2: Couldn't build file tensorflow/lite/testing/gather.zip: Executing genrule //tensorflow/lite/testing:gather.zip.files failed (Se
gmentation fault): bash failed: error executing command 
  (cd /root/.cache/bazel/_bazel_root/eee9defa2fa4c4fc557baa005719ebd9/execroot/org_tensorflow && \
  exec env - \
    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PYTHON_BIN_PATH=/usr/local/bin/python \
    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \
    TF_DOWNLOAD_CLANG=0 \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL_SYCL=0 \
    TF_NEED_ROCM=0 \
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/tensorflow/lite/testing/generate_examples --toco bazel-out/host/bin/tensorflow/lite/toco/toco  --zip_to_output gathe
r.zip  bazel-out/k8-opt/genfiles/tensorflow/lite/testing'): bash failed: error executing command 
  (cd /root/.cache/bazel/_bazel_root/eee9defa2fa4c4fc557baa005719ebd9/execroot/org_tensorflow && \
  exec env - \
    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PYTHON_BIN_PATH=/usr/local/bin/python \
    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \
    TF_DOWNLOAD_CLANG=0 \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL_SYCL=0 \
    TF_NEED_ROCM=0 \
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/tensorflow/lite/testing/generate_examples --toco bazel-out/host/bin/tensorflow/lite/toco/toco  --zip_to_output gathe
r.zip  bazel-out/k8-opt/genfiles/tensorflow/lite/testing')
2019-03-14 11:48:24.311471: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2600000000 Hz
2019-03-14 11:48:24.316264: I tensorflow/compiler/xla/service/service.cc:167] XLA service 0x4648a70 executing computations on platform Host. Devices:
2019-03-14 11:48:24.316342: I tensorflow/compiler/xla/service/service.cc:174]   StreamExecutor device (0): <undefined>, <undefined>
W0314 11:48:24.329004 140192886912768 deprecation.py:323] From /root/.cache/bazel/_bazel_root/eee9defa2fa4c4fc557baa005719ebd9/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/lite/testing/generate_examples
.runfiles/org_tensorflow/tensorflow/lite/testing/generate_examples.py:511: remove_training_nodes (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.remove_training_nodes`
/bin/bash: line 1:  8879 Segmentation fault      (core dumped) bazel-out/host/bin/tensorflow/lite/testing/generate_examples --toco bazel-out/host/bin/tensorflow/lite/toco/toco --zip_to_output gather.zip bazel-ou
t/k8-opt/genfiles/tensorflow/lite/testing
ERROR: /local/people/kamil_herba/tensorflow-x86/tensorflow/lite/testing/BUILD:21:2: Couldn't build file tensorflow/lite/testing/gather_toco-flex.zip: Executing genrule //tensorflow/lite/testing:gather_toco-flex.
zip.files failed (Segmentation fault): bash failed: error executing command 
  (cd /root/.cache/bazel/_bazel_root/eee9defa2fa4c4fc557baa005719ebd9/execroot/org_tensorflow && \
  exec env - \
    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PYTHON_BIN_PATH=/usr/local/bin/python \
    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \
    TF_DOWNLOAD_CLANG=0 \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL_SYCL=0 \
    TF_NEED_ROCM=0 \
```"
26696,tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  ubuntu16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):  binary, tf-nightly
- TensorFlow version (use command below):   1.14.1
- Python version:  3.6.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:   no cuda, only cpu
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

I wanna to convert .pb model to tflite model.

**Describe the expected behavior**

********************
the following errors occured:

Traceback (most recent call last):
  File ""onnx2tf.py"", line 33, in <module>
    main(sys.argv)
  File ""onnx2tf.py"", line 27, in main
    tflite_model = converter.convert()
  File ""/home/riddick/.local/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 762, in convert
    **converter_kwargs)
  File ""/home/riddick/.local/lib/python3.6/site-packages/tensorflow/lite/python/convert.py"", line 442, in toco_convert_impl
    input_data.SerializeToString())
  File ""/home/riddick/.local/lib/python3.6/site-packages/tensorflow/lite/python/convert.py"", line 205, in toco_convert_protos
    ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.
2019-03-14 03:21:21.151470: I tensorflow/lite/toco/import_tensorflow.cc:1335] Converting unsupported operation: PyFunc
2019-03-14 03:21:21.316309: F tensorflow/lite/toco/import_tensorflow.cc:114] Check failed: attr.value_case() == AttrValue::kType (1 vs. 6)
Fatal Python error: Aborted

******************

**Code to reproduce the issue**

*******************************

    input_arrays = [""image""]
    output_arrays = [""Add""]
    converter = tf.lite.TFLiteConverter.from_frozen_graph(pbPath, input_arrays, output_arrays)
    tflite_model = converter.convert()
    open(tflitePath, ""wb"").write(tflite_model)

*****************************

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

my model has been uploaded to google drive:
[https://drive.google.com/open?id=1V4ovRkU6Tv78oxCHjLZRClnubnDz9YzC](url)"
26693,remove Node for tflite,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.12
- Are you willing to contribute it (Yes/No):NO



**Describe the feature and the current behavior/state.** 

Thanks.


**Will this change the current api? How?** NO

**Who will benefit with this feature?** Everyone

**Any Other info.** None
"
26691,"Collision with built in python ""logging"" module","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac OS Mojave
- TensorFlow installed from (source or binary): pip 
- TensorFlow version (use command below): 2.0.0 alpha-0
- Python version: 3.7.2

**Describe the current behavior**
When using Python internal ""logging"" module with Tensorflow, a logging module emits below warning message.

> WARNING: Logging before flag parsing goes to stderr.

Also, it prints out duplicate logging message with my own formatter and something internally defined format(I guess) as below.

> $ python3 test.py 
[INFO|test.py:29] 2019-03-14 17:30:14,402 >> Hello World.
WARNING: Logging before flag parsing goes to stderr.
I0314 17:30:14.402168 4528047552 test.py:29] Hello World.
[INFO|test.py:30] 2019-03-14 17:30:14,402 >> This is message 1
I0314 17:30:14.402369 4528047552 test.py:30] This is message 1

When I try without importing Tensorflow, then it works properly.

> $ python3 test.py 
[INFO|test.py:29] 2019-03-14 17:37:40,172 >> Hello World.
[INFO|test.py:30] 2019-03-14 17:37:40,172 >> This is message 1

**Describe the expected behavior**
Maybe stream handler is colliding each other.

**Code to reproduce the issue**
```
import logging
import tensorflow as tf

formatter = logging.Formatter('[%(levelname)s|%(filename)s:%(lineno)s] %(asctime)s >> %(message)s')

stream_logger = logging.getLogger('logger_stream')
stream_logger.setLevel(logging.INFO)

# Set handler
streamHandler = logging.StreamHandler()
streamHandler.setFormatter(formatter)
stream_logger.addHandler(streamHandler)

stream_logger.info(""Hello World"")
stream_logger.info(""This is message 1"")
```"
26690,the elapse time shown in timeline not match with count by hand,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1
- Python version: 2.7.5
- CUDA/cuDNN version: 10/7
- GPU model and memory: 16G



**Describe the current behavior**

I have ran a program and use time.time()-start_time to calculate the elapse time (I have run 1000 times to get the average time), which is 5ms. But when use timeline in one run, I got 10ms+. So I doubt that, does `FULL_TRACE` will cost more time?
"
26689,Installation issue with tensorflow : pywrap_tensorflow_internal ,"**System information**
- Windows 10 64 bit 
- TensorFlow installed from (source or binary): pip install tensorflow
- TensorFlow version (or github SHA if from source): tensorflow 1.12.0



```
Using TensorFlow backend.
Traceback (most recent call last):
  File ""C:\Users\Osama\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])
  File ""C:\Program Files (x86)\Python37-32\lib\imp.py"", line 296, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Osama\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Osama\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Osama\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow_internal
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'
During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""FirstVideoDetection.py"", line 1, in <module>
    from imageai.Detection import VideoObjectDetection
  File ""C:\Users\Osama\AppData\Roaming\Python\Python37\site-packages\imageai\Detection\__init__.py"", line 3, in <module>
    from imageai.Detection.keras_retinanet.models.resnet import resnet50_retinanet
  File ""C:\Users\Osama\AppData\Roaming\Python\Python37\site-packages\imageai\Detection\keras_retinanet\models\resnet.py"", line 19, in <module>
    import keras
  File ""C:\Users\Osama\AppData\Roaming\Python\Python37\site-packages\keras\__init__.py"", line 3, in <module>
    from . import utils
  File ""C:\Users\Osama\AppData\Roaming\Python\Python37\site-packages\keras\utils\__init__.py"", line 6, in <module>
    from . import conv_utils
  File ""C:\Users\Osama\AppData\Roaming\Python\Python37\site-packages\keras\utils\conv_utils.py"", line 9, in <module>
    from .. import backend as K
  File ""C:\Users\Osama\AppData\Roaming\Python\Python37\site-packages\keras\backend\__init__.py"", line 89, in <module>
    from .tensorflow_backend import *
  File ""C:\Users\Osama\AppData\Roaming\Python\Python37\site-packages\keras\backend\tensorflow_backend.py"", line 5, in <module>
    import tensorflow as tf
  File ""C:\Users\Osama\AppData\Roaming\Python\Python37\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Osama\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Osama\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Osama\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])
  File ""C:\Program Files (x86)\Python37-32\lib\imp.py"", line 296, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow_internal'
During handling of the above exception, another exception occurred:
```



I'm working on object detection in video, i've already installed the tensorflow packages, when i run the code it gives me an error stating

 **ModuleNotFoundError: No module named 'tensorflow.python._pywrap_tensorflow_internal'**

**_### How do i solve this error ? ? ?_** 

"
26688,[TF2.0] Run sample of experts is not work on local,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):win10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:no
- TensorFlow installed from (source or binary): pip install tensorflow-gpu==2.0.0-alpha0
- TensorFlow version (use command below):conda list 2.0.0a0
- Python version:Python 3.6.8 :: Anaconda custom (64-bit)
- CUDA/cuDNN version:V10.0.130/7.5.0
- GPU model and memory:GTX 1050 TI


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
I follow this page:
https://www.tensorflow.org/alpha#for-experts and like to 
https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/quickstart/advanced.ipynb
and run cell-1 is work fine on colab
but i copy the cell-1 code and run it from jupyter notebook in local and got error message:
```
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-24-30fd6eda57f1> in <module>
----> 1 import tensorflow_datasets as tfds
      2 from tensorflow.keras.layers import Dense, Flatten, Conv2D
      3 from tensorflow.keras import Model

ModuleNotFoundError: No module named 'tensorflow_datasets'
```

**Describe the expected behavior**
it'll be work

**Other info / logs**

"
26687,Iterate on dataset/iterator when using @tf.function,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Archlinux
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-dev20190312
- Python version: 3.6
- CUDA/cuDNN version: 10
- GPU model and memory: 1080 Ti

**Describe the current behavior**

Can't convert the dataset/iterator to its graph representation: I'm forced to loop in eager mode, but I want to have a graph representation of the loop too (since I need to export a SavedModel that contains the loop itself).

**Describe the expected behavior**

The iterator / the dataset should be converted in its graph representation.

**Code to reproduce the issue**

A code that loops using Tensorflow primitives (like `for i in tf.range(10)`) can be converted into its graph representation without any problem, while a code that creates a python iterator from a dataset object (using `iter(dataset)`) can't.

Moreover, maybe because we are in the early stage of the development, I'm unable to convert a loop that loops over a dataset to its graph representation.

```python
import tensorflow as tf

def real_gen():
    for i in range(10):
        yield i

dataset = tf.data.Dataset.from_generator(real_gen, (tf.float32))

@tf.function
def itertest():
    tf.print(next(iter(dataset)))
itertest()

@tf.function
def loopiter():
    for real in dataset:
        tf.print(real)
loopiter()
```

The `itertest()` call fails because `RuntimeError: dataset.__iter__() is only supported when eager execution is enabled.`

While the `loopiter()` call fails because `tensorflow.python.framework.errors_impl.NotFoundError: Function __inference_Dataset_flat_map_flat_map_fn_22 is not defined.
         [[{{node ReduceDataset}}]] [Op:__inference_loopiter_35]`"
26686,"When using tf.gradients in the forward pass, my codes stop at `compute_gradients`","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): binary (conda install)
- TensorFlow version (use command below): 1.9.0
- Python version: 3.5
- CUDA/cuDNN version: 390.25
- GPU model and memory: GTX1080, 11GB


### 1. What I did:
My codes like this example: [tensorpack faster rcnn example](https://github.com/tensorpack/tensorpack/blob/f417c49fe45759fb2c69cafcabe6613ea85ec469/examples/FasterRCNN/train.py#L98)

I used DetectModel as the base class to implement the detection model of the SSD architecture. The training, optimizer, and get_inference_tensor_names are unchanged, the build_graph is reimplemented in the subclass, and the preprocess is modified. Other modules (such as anchor_generator, target_assign), I used the code in tensorflow offical models] (https://github.com/tensorflow/models/tree/master/research/object_detection).

my codes as follow
```

class MyMetaArch(ModelDesc):

    def inputs(self):
        ret = [
            tf.placeholder(tf.float32, (None, None, None, 1), 'image'),
            tf.placeholder(tf.float32, (8, None, 4), 'gt_boxes'),
            tf.placeholder(tf.int64, (8, None,), 'gt_labels')
        ]
        return ret

    def build_graph(self, *inputs):
        inputs = dict(zip(self.input_names, inputs))
        images = inputs[""image""]

        x = Conv2D('conv1', images, filters=16, kernel_size=3, strides=(2, 2))
        x = Conv2D('conv2', x, filters=32, kernel_size=3, strides=(2, 2))
        x = Conv2D('conv3', x, filters=64, kernel_size=3, strides=(2, 2))
        x = Conv2D('conv4', x, filters=64, kernel_size=3, strides=(2, 2))
        ...
        class_prediction_with_background = tf.concat(...) # concat multi branches 
        class_prediction_with_background = class_prediction_with_background [:,:,1] # face detector, only care about the foreground

        gradients = tf.gradients(class_prediction_with_background , [images])[0]
        regular_vals = tf.reduce_mean(gradients)
        ...
        class_losses = class_losses + regular_vals
```
### 2. What I observed:
(1) **Include the ENTIRE logs here:**
But there are not any errors. The runtime log as follows， and and I added some extra logs.

```
[32m[0312 17:31:05 @interface.py:86][0m setup_graph
[32m[0312 17:31:05 @tower.py:217][0m _setup_input
[32m[0312 17:31:05 @input_source.py:220][0m Setting up the queue 'QueueInput/input_queue' with size Tensor(""QueueInput/input_queue_Size:0"", shape=(), dtype=int32) for CPU prefetching ...
[32m[0312 17:31:05 @tower.py:219][0m _setup_graph
[32m[0312 17:31:05 @trainers.py:186][0m SyncMultiGPUTrainerReplicated _setup_graph
[32m[0312 17:31:05 @trainers.py:189][0m _make_get_grad_fn
[32m[0312 17:31:05 @trainers.py:191][0m call_for_each_tower
[32m[0312 17:31:05 @training.py:109][0m Building graph for training tower 0 on device /gpu:0 ...
[32m[0312 17:31:05 @training.py:116][0m override_to_local_variable /gpu:0
[32m[0312 17:31:05 @tower.py:248][0m get_grad_fn: start
[32m[0312 17:31:05 @tower.py:280][0m NOT XLA_COMPILE: compute_grad_from_inputs
[32m[0312 17:31:05 @tower.py:253][0m compute_grad_from_inputs start
[32m[0312 17:31:05 @model_desc.py:262][0m start build_graph
[32m[0312 17:31:06 @registry.py:125][0m conv1/conv1 input: [8, 644, 644, 1]
[32m[0312 17:31:06 @registry.py:133][0m conv1/conv1 output: [8, 160, 160, 12]
...
[32m[0312 17:31:07 @registry.py:125][0m conv3_1 input: [8, 20, 20, 64]
[32m[0312 17:31:08 @registry.py:133][0m conv3_1 output: [8, 20, 20, 128]
[32m[0312 17:31:08 @registry.py:125][0m conv3_2 input: [8, 22, 22, 128]
[32m[0312 17:31:08 @registry.py:133][0m conv3_2 output: [8, 10, 10, 256]
[32m[0312 17:31:08 @registry.py:125][0m conv4_1 input: [8, 10, 10, 256]
[32m[0312 17:31:08 @registry.py:133][0m conv4_1 output: [8, 10, 10, 128]
[32m[0312 17:31:08 @registry.py:125][0m conv4_2 input: [8, 12, 12, 128]
[32m[0312 17:31:08 @registry.py:133][0m conv4_2 output: [8, 5, 5, 256]
[32m[0312 17:31:08 @registry.py:125][0m prediction_layers/box_encoding_predictor_0 input: [8, 20, 20, 64]
[32m[0312 17:31:08 @registry.py:133][0m prediction_layers/box_encoding_predictor_0 output: [8, 20, 20, 84]
[32m[0312 17:31:08 @registry.py:125][0m prediction_layers/class_predictor_0 input: [8, 20, 20, 64]
[32m[0312 17:31:08 @registry.py:133][0m prediction_layers/class_predictor_0 output: [8, 20, 20, 42]
[32m[0312 17:31:08 @registry.py:125][0m prediction_layers/box_encoding_predictor_1 input: [8, 10, 10, 256]
[32m[0312 17:31:08 @registry.py:133][0m prediction_layers/box_encoding_predictor_1 output: [8, 10, 10, 4]
[32m[0312 17:31:08 @registry.py:125][0m prediction_layers/class_predictor_1 input: [8, 10, 10, 256]
[32m[0312 17:31:08 @registry.py:133][0m prediction_layers/class_predictor_1 output: [8, 10, 10, 2]
[32m[0312 17:31:08 @registry.py:125][0m prediction_layers/box_encoding_predictor_2 input: [8, 5, 5, 256]
[32m[0312 17:31:08 @registry.py:133][0m prediction_layers/box_encoding_predictor_2 output: [8, 5, 5, 4]
[32m[0312 17:31:08 @registry.py:125][0m prediction_layers/class_predictor_2 input: [8, 5, 5, 256]
[32m[0312 17:31:08 @registry.py:133][0m prediction_layers/class_predictor_2 output: [8, 5, 5, 2]
[32m[0312 17:31:11 @FaceboxesModel.py:325][0m cls_losses: Tensor(""tower0/Loss/Loss_1/mul:0"", shape=(8, 8525), dtype=float32, device=/device:GPU:0)
[32m[0312 17:31:13 @FaceboxesModel.py:236][0m start regularize_cost
[32m[0312 17:31:13 @regularize.py:95][0m regularize_cost() found 35 variables to regularize.
[32m[0312 17:31:13 @regularize.py:20][0m The following tensors will be regularized: conv1/conv1/W:0,...,conv3_1/W:0, conv3_2/W:0, conv4_1/W:0, conv4_2/W:0,...
[32m[0312 17:31:13 @FaceboxesModel.py:239][0m end regularize_cost
[32m[0312 17:31:13 @FaceboxesModel.py:242][0m start add_n
[32m[0312 17:31:13 @FaceboxesModel.py:244][0m end add_n
[32m[0312 17:31:13 @model_desc.py:264][0m end build_graph
[32m[0312 17:31:13 @model_desc.py:268][0m _build_graph_get_cost return Tensor(""tower0/total_cost:0"", shape=(), dtype=float32, device=/device:GPU:0)
[32m[0312 17:31:13 @tower.py:255][0m get_cost_fn Tensor(""tower0/total_cost:0"", shape=(), dtype=float32, device=/device:GPU:0)
[32m[0312 17:31:13 @tower.py:268][0m get_opt_fn
```
The code seems to be stuck at this line: https://github.com/tensorpack/tensorpack/blob/master/tensorpack/train/tower.py#L261
The function is `compute_gradients`"
26685,build from src for //tensorflow/python/keras/api:keras_python_api_gen_compat_v1 failed,"commit e7f508844eab13927f32abdb5d7f0eb4e91caee0
**System information**

== cat /etc/issue ===============================================
Linux luban-350 3.10.0-514.16.1.el7.x86_64 #1 SMP Wed Apr 12 15:04:24 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""7 (Core)""
VERSION_ID=""7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""

== are we in docker =============================================
Yes

== compiler =====================================================
c++ (GCC) 8.3.0
Copyright (C) 2018 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux luban-350 3.10.0-514.16.1.el7.x86_64 #1 SMP Wed Apr 12 15:04:24 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy                            1.14.1   
protobuf                         3.5.2    
tensorflow                       1.5.0rc1 
tensorflow-tensorboard           1.5.1    

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.5.0-rc1
tf.GIT_VERSION = v1.5.0-rc1-4-g04b3f6c
tf.COMPILER_VERSION = v1.5.0-rc1-4-g04b3f6c
Sanity check: array([1], dtype=int32)
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""tensorflow/python/pywrap_tensorflow.py"", line 25, in <module>
    from tensorflow.python.platform import self_check
  File ""tensorflow/python/platform/self_check.py"", line 27, in <module>
    raise ImportError(""Could not import tensorflow. Do not import tensorflow ""
ImportError: Could not import tensorflow. Do not import tensorflow from its source directory; change directory to outside the TensorFlow source tree, and relaunch your Python interpreter from there.

== env ==========================================================
LD_LIBRARY_PATH /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Thu Mar 14 15:31:33 2019       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 410.72       Driver Version: 410.72       CUDA Version: 10.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla P40           Off  | 00000000:02:00.0 Off |                  N/A |
| N/A   25C    P0    52W / 250W |     10MiB / 22919MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla P40           Off  | 00000000:03:00.0 Off |                  N/A |
| N/A   22C    P0    53W / 250W |     10MiB / 22919MiB |     64%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla P40           Off  | 00000000:83:00.0 Off |                  N/A |
| N/A   41C    P0    70W / 250W |  16779MiB / 22919MiB |     93%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla P40           Off  | 00000000:84:00.0 Off |                  N/A |
| N/A   23C    P0    53W / 250W |     10MiB / 22919MiB |     99%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-10.0/lib64/libcudart_static.a
/usr/local/cuda-10.0/lib64/libcudart.so.10.0.130
/usr/local/cuda-10.0/doc/man/man7/libcudart.7
/usr/local/cuda-10.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.0/doc/man/man7/libcudart.7
/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.0/lib64/libcudart.so.9.0.176
/usr/local/cuda-9.0/lib64/libcudart_static.a
[luban@luban-350 tensorflow]$ 



**Describe the problem**

bazel  build -c opt -- //tensorflow/python/tools/...


**Any other info / logs**
```
cc1plus: warning: unrecognized command line option '-Wno-writable-strings'
ERROR: /home/luban/zhanghui/tensorflow/tensorflow/python/keras/api/BUILD:28:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1 failed (Exit 1)
Traceback (most recent call last):
  File ""/home/luban/.cache/bazel/_bazel_luban/64ebbf8410c89b3c0f2d97b092db8898/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 27, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""/home/luban/.cache/bazel/_bazel_luban/64ebbf8410c89b3c0f2d97b092db8898/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 83, in <module>
    from tensorflow.python import keras
  File ""/home/luban/.cache/bazel/_bazel_luban/64ebbf8410c89b3c0f2d97b092db8898/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/keras/__init__.py"", line 30, in <module>
    from tensorflow.python.keras import datasets
  File ""/home/luban/.cache/bazel/_bazel_luban/64ebbf8410c89b3c0f2d97b092db8898/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/keras/datasets/__init__.py"", line 25, in <module>
    from tensorflow.python.keras.datasets import imdb
  File ""/home/luban/.cache/bazel/_bazel_luban/64ebbf8410c89b3c0f2d97b092db8898/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/keras/datasets/imdb.py"", line 25, in <module>
    from tensorflow.python.keras.preprocessing.sequence import _remove_long_seq
  File ""/home/luban/.cache/bazel/_bazel_luban/64ebbf8410c89b3c0f2d97b092db8898/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/keras/preprocessing/__init__.py"", line 21, in <module>
    import keras_preprocessing
ImportError: No module named keras_preprocessing
INFO: Elapsed time: 2623.895s, Critical Path: 178.05s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]
INFO: 11634 processes: 11634 local.
FAILED: Build did NOT complete successfully
```
"
26684,Repeatedly allocating a graph and summary writer leaks memory,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I have
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): `v1.12.0-10061-gf3954bf900 1.13.1`
- Python version: 3.7.2
- Bazel version (if compiling from source): 0.23.1
- GCC/Compiler version (if compiling from source): `Apple LLVM version 10.0.0 (clang-1000.11.45.5)`
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

Repeatedly allocating a graph and making a summary writer leaks memory.

**Describe the expected behavior**

Memory should be freed when the graph leaves scope.

**Code to reproduce the issue**

```
#!/usr/bin/env python3

import resource
import tensorflow as tf

prev = 0
while True:
    peak = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(f'peak memory = {peak:,} (+{peak-prev:,})')
    prev = peak

    with tf.Graph().as_default(), tf.init_scope():
        tf.contrib.summary.create_file_writer('/tmp/tb')
```

**Other info / logs**

Here's what the output looks like:

```
peak memory = 174,493,696 (+174,493,696)
  
WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
If you depend on functionality not listed there, please file an issue.

peak memory = 202,215,424 (+27,721,728)
peak memory = 202,264,576 (+49,152)
peak memory = 202,309,632 (+45,056)
peak memory = 202,358,784 (+49,152)
peak memory = 202,432,512 (+73,728)
peak memory = 202,473,472 (+40,960)
peak memory = 202,522,624 (+49,152)
peak memory = 202,567,680 (+45,056)
peak memory = 202,604,544 (+36,864)
peak memory = 202,641,408 (+36,864)
peak memory = 202,694,656 (+53,248)
peak memory = 202,739,712 (+45,056)
peak memory = 202,784,768 (+45,056)
peak memory = 202,829,824 (+45,056)
peak memory = 202,878,976 (+49,152)
peak memory = 202,919,936 (+40,960)
peak memory = 202,981,376 (+61,440)
peak memory = 203,026,432 (+45,056)
peak memory = 203,067,392 (+40,960)
...
peak memory = 999,665,664 (+49,152)
peak memory = 999,718,912 (+53,248)
peak memory = 999,768,064 (+49,152)
peak memory = 999,817,216 (+49,152)
peak memory = 999,866,368 (+49,152)
peak memory = 999,915,520 (+49,152)
peak memory = 999,964,672 (+49,152)
peak memory = 1,000,009,728 (+45,056)
peak memory = 1,000,058,880 (+49,152)
peak memory = 1,000,108,032 (+49,152)
peak memory = 1,000,161,280 (+53,248)
peak memory = 1,000,202,240 (+40,960)
peak memory = 1,000,255,488 (+53,248)
...
```"
26683,TFRecordWriter.flush() do not seem to flush,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.5 LTS
TensorFlow installed from (source or binary): pip
TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0
Python version: 3.5.2:
CUDA/cuDNN version: cuda-9.0, cudnn-7.3.1
GPU model and memory: Titan Xp, Titan V



**Describe the current behavior**
```
import tensorflow as tf

mystring = 'start blahblahblah end\n' * 10000

# https://www.zlib.net/manual.html
Z_NO_FLUSH = 0
Z_PARTIAL_FLUSH = 1
Z_SYNC_FLUSH = 2
Z_FULL_FLUSH = 3

writer = tf.python_io.TFRecordWriter('data', tf.io.TFRecordOptions(flush_mode=Z_NO_FLUSH))
writer.write(mystring.encode())
# << - at this point, mystring is not fully written on disk as expected.
writer.flush()
# << - still not fully written
writer.close()
# << - now it is fully written
```

I found `flush_mode` option in `tf.io.TFRecordOptions`.
Although it is only effect `compression_type` is None but anyway I tried from `0` (Z_NO_FLUSH) to `3` (Z_FULL_FLUSH) as stated in https://www.zlib.net/manual.html, none of them work.
Do I miss something or is it a bug?
"
26680,boolean_mask with all-zero mask produces allocator error,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):archlinux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:n/a
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):b'v1.13.1-0-g6612da8' 1.13.1
- Python version:3.7
- Bazel version (if compiling from source):n/a
- GCC/Compiler version (if compiling from source):n/a
- CUDA/cuDNN version:10.0/7.5.0
- GPU model and memory:GTX1080ti

This code:
```python
import numpy as np
import tensorflow as tf
shp = [100, 100]
x = tf.placeholder(tf.bool, shape=shp, name='x')
y = tf.random.uniform(shape=shp)
z = tf.boolean_mask(y, x)
sess = tf.InteractiveSession()
false = np.zeros(shp).astype(np.bool)
sess.run(z, feed_dict={x: false})
```
runs on GPU, and prints:
```
2019-03-13 12:13:35.552607: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2019-03-13 12:13:35.577673: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3501330000 Hz
2019-03-13 12:13:35.579052: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55d524877dc0 executing computations on platform Host. Devices:
2019-03-13 12:13:35.579096: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2019-03-13 12:13:35.691632: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55d52431d150 executing computations on platform CUDA. Devices:
2019-03-13 12:13:35.691689: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1
2019-03-13 12:13:35.692485: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:65:00.0
totalMemory: 10.91GiB freeMemory: 8.92GiB
2019-03-13 12:13:35.692524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2019-03-13 12:13:36.081417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-03-13 12:13:36.081451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2019-03-13 12:13:36.081457: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
2019-03-13 12:13:36.081720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8642 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)
2019-03-13 12:13:36.082699: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
2019-03-13 12:13:36.123279: E tensorflow/core/common_runtime/bfc_allocator.cc:373] tried to deallocate nullptr
```
There is an error in the end. It is not fatal, the code returns correct results. But I assume it is still a bug that is worth fixing.
If `x` is not full of zeros, it does not produce such errors.

UPDATE: this issue does not exist in 1.12"
26679,Error in bazel-out/x64_windows-opt/genfiles\tensorflow/compiler/xla/xla_data.pb.h,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64 bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: checkout from `master` branch
- Python version: Python 3.6.8 :: Anaconda, Inc.
- Installed using virtualenv? pip? conda?: conda 4.6.8
- Bazel version (if compiling from source): 0.22.0
- GCC/Compiler version (if compiling from source): cl.exe C/C++ Optimizing Compiler Version 19.16.27027.1 for x64, Visual Studio Build Tools 2017
- CUDA/cuDNN version: CUDA Toolkit V10.0.130, cuDNN 7.5.0
- GPU model and memory: GTX 1060 6GB

**Describe the problem**

Compile error.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
(neural) Stepii@STEPII D:\Neural
$ git clone https://github.com/tensorflow/tensorflow.git
Cloning into 'tensorflow'...
remote: Enumerating objects: 7251, done.
remote: Counting objects: 100% (7251/7251), done.
remote: Compressing objects: 100% (2280/2280), done.
remote: Total 557207 (delta 4925), reused 7233 (delta 4916), pack-reused 549956
Receiving objects: 100% (557207/557207), 330.05 MiB | 378.00 KiB/s, done.
Resolving deltas: 100% (448922/448922), done.
Checking out files: 100% (16777/16777), done.

(neural) Stepii@STEPII D:\Neural
$ cd tensorflow

(neural) Stepii@STEPII D:\Neural\tensorflow
$ python configure.py
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
INFO: Invocation ID: 1f6b8c12-47e3-4c5e-80f4-5d3d74fb17b1
You have bazel 0.22.0- (@non-git) installed.
Please specify the location of python. [Default is D:\Programs\Anaconda3\envs\neural\python.exe]:


Found possible Python library paths:
  D:\Programs\Anaconda3\envs\neural\lib\site-packages
Please input the desired Python library path to use.  Default is [D:\Programs\Anaconda3\envs\neural\lib\site-packages]

Do you wish to build TensorFlow with XLA JIT support? [y/N]: y
XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]:
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10.0]:


Please specify the location where CUDA 10.0 toolkit is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0]:


Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]:


Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0]:


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 6.1


Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]: /arch:AVX2


Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:
Eigen strong inline overridden.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=gdr            # Build with GDR support.
        --config=verbs          # Build with libverbs support.
        --config=ngraph         # Build with Intel nGraph support.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=noignite       # Disable Apache Ignite support.
        --config=nokafka        # Disable Apache Kafka support.
        --config=nonccl         # Disable NVIDIA NCCL support.

(neural) Stepii@STEPII D:\Neural\tensorflow
$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

[...]

ERROR: D:/neural/tensorflow/tensorflow/compiler/xla/service/cpu/BUILD:572:1: C++ compilation of rule '//tensorflow/compiler/xla/service/cpu:runtime_fft' failed (Exit 2): python.exe failed: error executing command
  cd C:/users/stepii/_bazel_stepii/5mniti2w/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0
    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\ATLMFC\include;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\ATLMFC\lib\x64;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\lib\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\MSBuild\15.0\bin\Roslyn;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\\x64;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\;C:\Program Files (x86)\Windows Kits\10\bin\10.0.17763.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\\MSBuild\15.0\bin;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\Tools\;;C:\Windows\system32;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=D:/Programs/Anaconda3/envs/neural/python.exe
    SET PYTHON_LIB_PATH=D:/Programs/Anaconda3/envs/neural/lib/site-packages
    SET TEMP=C:\Users\Stepii\AppData\Local\Temp
    SET TF_CONFIGURE_IOS=0
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1
    SET TF_CUDA_VERSION=10.0
    SET TF_CUDNN_VERSION=7
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
    SET TMP=C:\Users\Stepii\AppData\Local\Temp
  D:/Programs/Anaconda3/envs/neural/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Ibazel-out/x64_windows-opt/bin /Iexternal/nsync /Ibazel-out/x64_windows-opt/genfiles/external/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/genfiles/external/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /showIncludes /MD /O2 /DNDEBUG -w -DWIN32_LEAN_AND_MEAN /arch:AVX2 -DEIGEN_AVOID_STL_ARRAY /Fobazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/service/cpu/_objs/runtime_fft/runtime_fft.o /c tensorflow/compiler/xla/service/cpu/runtime_fft.cc
Execution platform: @bazel_tools//platforms:host_platform
bazel-out/x64_windows-opt/genfiles\tensorflow/compiler/xla/xla_data.pb.h(290): error C2059: syntax error: 'constant'
bazel-out/x64_windows-opt/genfiles\tensorflow/compiler/xla/xla_data.pb.h(290): error C3805: 'constant': unexpected token, expected either '}' or a ','
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 1458.697s, Critical Path: 391.20s
INFO: 3277 processes: 3277 local.
FAILED: Build did NOT complete successfully
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Full log [here](https://github.com/tensorflow/tensorflow/files/3044240/FullLog.txt)
"
26677,"tf.argmax axis argument error returns ""Use the `axis` argument instead""","i also try  tf.math.argmax instance of  tf.argmax which is also produce same error.

y_pred_cls = tf.argmax(y_pred, dimension=1, axis=None)
'''y_pred_cls = tf.math.argmax(
    y_pred,
    axis=None,
    name=None,
    dimension=1
)'''"
26674,tests inside tflite/kernels/internal fail to build,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
-  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.1 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: commit 6583b83dc9393118c60a9c11f15453c650fab89f
- Python version: 2.7.15rc1
- Installed using virtualenv? pip? conda?: sources
- Bazel version (if compiling from source): 0.20.0
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
- CUDA/cuDNN version: -
- GPU model and memory: -



**Describe the problem**
Some tests inside tflite/kernels/internal (6 out of 11) fail to build. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel test --verbose_failures --config=opt //tensorflow/lite/kernels/internal/... -k

**Any other info / logs**
```
ERROR: /local/people/kamil_herba/tensorflow-x86/tensorflow/lite/kernels/internal/BUILD:713:1: Couldn't build file tensorflow/lite/kernels/internal/_objs/batch_to_space_nd_test/batch_to_space_nd_test.o: undeclare
d inclusion(s) in rule '//tensorflow/lite/kernels/internal:batch_to_space_nd_test':
this rule is missing dependency declarations for the following files included by 'tensorflow/lite/kernels/internal/batch_to_space_nd_test.cc':
  'tensorflow/lite/kernels/internal/tensor_ctypes.h'
In file included from ./tensorflow/lite/kernels/internal/common.h:42:0,
                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:37,
                 from tensorflow/lite/kernels/internal/batch_to_space_nd_test.cc:15:
external/arm_neon_2_x86_sse/NEON_2_SSE.h: In function 'void vst1q_lane_f32(float32_t*, float32x4_t, int)':
external/arm_neon_2_x86_sse/NEON_2_SSE.h:9725:31: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]
     *(ptr) =  *((float*)&ilane);
                               ^
external/arm_neon_2_x86_sse/NEON_2_SSE.h: In function 'float32_t vgetq_lane_f32(float32x4_t, int)':
external/arm_neon_2_x86_sse/NEON_2_SSE.h:11964:22: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]
     return *(float*)&ilane;
                      ^
ERROR: /local/people/kamil_herba/tensorflow-x86/tensorflow/lite/kernels/internal/BUILD:619:1: Couldn't build file tensorflow/lite/kernels/internal/_objs/resize_bilinear_test/resize_bilinear_test.o: undeclared in
clusion(s) in rule '//tensorflow/lite/kernels/internal:resize_bilinear_test':
this rule is missing dependency declarations for the following files included by 'tensorflow/lite/kernels/internal/resize_bilinear_test.cc':
  'tensorflow/lite/kernels/internal/tensor_ctypes.h'
In file included from ./tensorflow/lite/kernels/internal/common.h:42:0,
                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:37,
                 from tensorflow/lite/kernels/internal/resize_bilinear_test.cc:20:
external/arm_neon_2_x86_sse/NEON_2_SSE.h: In function 'void vst1q_lane_f32(float32_t*, float32x4_t, int)':
external/arm_neon_2_x86_sse/NEON_2_SSE.h:9725:31: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]
     *(ptr) =  *((float*)&ilane);
                               ^
external/arm_neon_2_x86_sse/NEON_2_SSE.h: In function 'float32_t vgetq_lane_f32(float32x4_t, int)':
external/arm_neon_2_x86_sse/NEON_2_SSE.h:11964:22: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]
     return *(float*)&ilane;
                      ^
ERROR: /local/people/kamil_herba/tensorflow-x86/tensorflow/lite/kernels/internal/BUILD:631:1: Couldn't build file tensorflow/lite/kernels/internal/_objs/resize_nearest_neighbor_test/resize_nearest_neighbor_test.
o: undeclared inclusion(s) in rule '//tensorflow/lite/kernels/internal:resize_nearest_neighbor_test':
this rule is missing dependency declarations for the following files included by 'tensorflow/lite/kernels/internal/resize_nearest_neighbor_test.cc':
  'tensorflow/lite/kernels/internal/tensor_ctypes.h'
In file included from ./tensorflow/lite/kernels/internal/common.h:42:0,
                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:37,
                 from tensorflow/lite/kernels/internal/resize_nearest_neighbor_test.cc:20:
external/arm_neon_2_x86_sse/NEON_2_SSE.h: In function 'void vst1q_lane_f32(float32_t*, float32x4_t, int)':
external/arm_neon_2_x86_sse/NEON_2_SSE.h:9725:31: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]
     *(ptr) =  *((float*)&ilane);
                               ^
external/arm_neon_2_x86_sse/NEON_2_SSE.h: In function 'float32_t vgetq_lane_f32(float32x4_t, int)':
external/arm_neon_2_x86_sse/NEON_2_SSE.h:11964:22: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]
     return *(float*)&ilane;

ERROR: /local/people/kamil_herba/tensorflow-x86/tensorflow/lite/kernels/internal/BUILD:660:1: Couldn't build file tensorflow/lite/kernels/internal/_objs/logsoftmax_quantized_test/logsoftmax_quantized_test.o: und
eclared inclusion(s) in rule '//tensorflow/lite/kernels/internal:logsoftmax_quantized_test':
this rule is missing dependency declarations for the following files included by 'tensorflow/lite/kernels/internal/logsoftmax_quantized_test.cc':
  'tensorflow/lite/kernels/internal/tensor_ctypes.h'
In file included from ./tensorflow/lite/kernels/internal/common.h:42:0,
                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:37,
                 from tensorflow/lite/kernels/internal/logsoftmax_quantized_test.cc:26:
external/arm_neon_2_x86_sse/NEON_2_SSE.h: In function 'void vst1q_lane_f32(float32_t*, float32x4_t, int)':
external/arm_neon_2_x86_sse/NEON_2_SSE.h:9725:31: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]
     *(ptr) =  *((float*)&ilane);
                               ^
external/arm_neon_2_x86_sse/NEON_2_SSE.h: In function 'float32_t vgetq_lane_f32(float32x4_t, int)':
external/arm_neon_2_x86_sse/NEON_2_SSE.h:11964:22: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]
     return *(float*)&ilane;
                      ^
ERROR: /local/people/kamil_herba/tensorflow-x86/tensorflow/lite/kernels/internal/BUILD:643:1: Couldn't build file tensorflow/lite/kernels/internal/_objs/softmax_quantized_test/softmax_quantized_test.o: undeclare
d inclusion(s) in rule '//tensorflow/lite/kernels/internal:softmax_quantized_test':
this rule is missing dependency declarations for the following files included by 'tensorflow/lite/kernels/internal/softmax_quantized_test.cc':
  'tensorflow/lite/kernels/internal/tensor_ctypes.h'
In file included from ./tensorflow/lite/kernels/internal/common.h:42:0,
                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:37,
                 from tensorflow/lite/kernels/internal/softmax_quantized_test.cc:26:
external/arm_neon_2_x86_sse/NEON_2_SSE.h: In function 'void vst1q_lane_f32(float32_t*, float32x4_t, int)':
external/arm_neon_2_x86_sse/NEON_2_SSE.h:9725:31: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]
     *(ptr) =  *((float*)&ilane);
                               ^
external/arm_neon_2_x86_sse/NEON_2_SSE.h: In function 'float32_t vgetq_lane_f32(float32x4_t, int)':
external/arm_neon_2_x86_sse/NEON_2_SSE.h:11964:22: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]
     return *(float*)&ilane;

ERROR: /local/people/kamil_herba/tensorflow-x86/tensorflow/lite/kernels/internal/BUILD:682:1: Couldn't build file tensorflow/lite/kernels/internal/_objs/log_quantized_test/log_quantized_test.o: undeclared inclusion(s) in rule '//tensorflow/lite/kernels/internal:log_quantized_test':
this rule is missing dependency declarations for the following files included by 'tensorflow/lite/kernels/internal/log_quantized_test.cc':
  'tensorflow/lite/kernels/internal/tensor_ctypes.h'
In file included from ./tensorflow/lite/kernels/internal/common.h:42:0,
                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:37,
                 from tensorflow/lite/kernels/internal/log_quantized_test.cc:30:
external/arm_neon_2_x86_sse/NEON_2_SSE.h: In function 'void vst1q_lane_f32(float32_t*, float32x4_t, int)':
external/arm_neon_2_x86_sse/NEON_2_SSE.h:9725:31: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]
     *(ptr) =  *((float*)&ilane);
                               ^
external/arm_neon_2_x86_sse/NEON_2_SSE.h: In function 'float32_t vgetq_lane_f32(float32x4_t, int)':
external/arm_neon_2_x86_sse/NEON_2_SSE.h:11964:22: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]
     return *(float*)&ilane;
```"
26673,[Document] no module name resize_image_with_pad,"https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/image/resize_image_with_pad

there no function ""resize_image_with_pad"" in module ""image"". Its name is: ""resize_with_pad""
"
26672,Not able to convert Keras model to tflite - TF 2.0 alpha,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not yet to that phase
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 2.0.0
- Python version: 2.7.12
- Bazel version (if compiling from source): 0.23.0
- GCC/Compiler version (if compiling from source): 5.4.0 20160609
- CUDA/cuDNN version: CPU
- GPU model and memory: CPU

**Current Behavior**
On conversion to tflite from keras model, fused batch norm creates a resource node issue (before fine tuning: dog_cat_classification_initial), and conv2d creates a resource node issue(after fine tuning: dog_cat_classification_final).
On conversion to tflite from a keras model using TF2.0, extra resource nodes are getting generated for each and every op that is available. So, upon conversion to tflite, all the resource nodes sum up to form the input, which does not allow the tflite to work as expected. 

**expected behavior**
Batch normalization should be supported by TFLITE. 
TFLite produced as an output of the tflite conversion process, should produce no resource nodes as it was the case in TF1.12

**Code to reproduce the issue**
This is a code to convert the mobilenet based classification model into tflite. This is developed using TF2.0 following the official dogs and cats classification published in the tensorflow page.
I have attached with this issue, the h5 file and hdf5 file extracted from the same model. I hope we can try invoking the same model and try creating the tflite file. The same code creates different issues with the same model architecture in two different situations(initial and fine tuned).

`import tensorflow as tf`
`model = tf.keras.models.load_model('dog_cat_classification_initial.hdf5')`
`@tf.function`
`def to_save(x):      return model(x)`

`from tensorflow.lite.python import lite`
`from tensorflow.lite.python.interpreter import Interpreter`
`from tensorflow.python import keras`
`from tensorflow.python.eager import def_function`
`from tensorflow.python.framework import constant_op`
`from tensorflow.python.framework import dtypes`
`from tensorflow.python.framework import tensor_spec`
`from tensorflow.python.framework import test_util`
`from tensorflow.python.ops import variables`
`from tensorflow.python.platform import test`
`from tensorflow.python.saved_model.load import load`
`from tensorflow.python.saved_model.save import save`
`from tensorflow.python.training.tracking import tracking`
`tf.keras.backend.set_learning_phase(False)`
`concrete_func = to_save.get_concrete_function(tensor_spec.TensorSpec([None, 160, 160, 3], dtypes.float32))`
`converter = lite.TFLiteConverterV2.from_concrete_function(concrete_func)`
`tflite_model = converter.convert()`
`open(""new_classificaton.tflite"",""wb"").write(tflite_model)`

Model file:


[tf2_tflite_issues.zip](https://github.com/tensorflow/tensorflow/files/2961884/tf2_tflite_issues.zip)

**Other info / logs**

**dog_cat_classification_initial.hdf5**

2019-03-13 19:52:41.281172: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2594290000 Hz
2019-03-13 19:52:41.281379: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x5aee1c0 executing computations on platform Host. Devices:
2019-03-13 19:52:41.281401: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): <undefined>, <undefined>
WARNING: Logging before flag parsing goes to stderr.
W0313 19:53:20.606553 140373983291136 hdf5_format.py:261] Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.
2019-03-13 19:53:21.310751: I tensorflow/core/grappler/devices.cc:53] Number of eligible GPUs (core count >= 8): 0 (Note: TensorFlow was not compiled with CUDA support)
2019-03-13 19:53:21.310873: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session
2019-03-13 19:53:21.344653: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:666] Optimization results for grappler item: graph_to_optimize
2019-03-13 19:53:21.344712: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:668]   function_optimizer: Graph size after: 775 nodes (0), 960 edges (0), time = 1.54ms.
2019-03-13 19:53:21.344726: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:668]   function_optimizer: Graph size after: 775 nodes (0), 960 edges (0), time = 1.95ms.
Traceback (most recent call last):
  File ""conversion.py"", line 26, in <module>
    tflite_model = converter.convert()
  File ""/home/santhosh/venv_tf2/local/lib/python2.7/site-packages/tensorflow/lite/python/lite.py"", line 246, in convert
    self._func)
  File ""/home/santhosh/venv_tf2/local/lib/python2.7/site-packages/tensorflow/python/framework/convert_to_constants.py"", line 173, in convert_variables_to_constants_v2
    ""data"": tensor_data[input_name],
KeyError: u'sequential/mobilenetv2_1.00_160/bn_Conv1/FusedBatchNorm/ReadVariableOp/resource'

**dog_cat_classification_final.hdf5**
2019-03-13 19:52:08.602599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2594290000 Hz
2019-03-13 19:52:08.602794: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x545d360 executing computations on platform Host. Devices:
2019-03-13 19:52:08.602817: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): <undefined>, <undefined>
2019-03-13 19:52:25.827825: I tensorflow/core/grappler/devices.cc:53] Number of eligible GPUs (core count >= 8): 0 (Note: TensorFlow was not compiled with CUDA support)
2019-03-13 19:52:25.827926: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session
2019-03-13 19:52:25.844869: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:666] Optimization results for grappler item: graph_to_optimize
2019-03-13 19:52:25.844920: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:668]   function_optimizer: Graph size after: 775 nodes (0), 960 edges (0), time = 0.703ms.
2019-03-13 19:52:25.844930: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:668]   function_optimizer: Graph size after: 775 nodes (0), 960 edges (0), time = 1.121ms.
Traceback (most recent call last):
  File ""conversion.py"", line 26, in <module>
    tflite_model = converter.convert()
  File ""/home/santhosh/venv_tf2/local/lib/python2.7/site-packages/tensorflow/lite/python/lite.py"", line 246, in convert
    self._func)
  File ""/home/santhosh/venv_tf2/local/lib/python2.7/site-packages/tensorflow/python/framework/convert_to_constants.py"", line 173, in convert_variables_to_constants_v2
    ""data"": tensor_data[input_name],
KeyError: u'sequential/mobilenetv2_1.00_160/Conv1/Conv2D/ReadVariableOp/resource'
"
26671,googletest.h used in open source project,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
-  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.1 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: commit 6583b83dc9393118c60a9c11f15453c650fab89f
- Python version: 2.7.15rc1
- Installed using virtualenv? pip? conda?: sources
- Bazel version (if compiling from source): 0.20.0
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
- CUDA/cuDNN version: -
- GPU model and memory: -



**Describe the problem**
During compilation I get an error:
```
ERROR: /local/people/kamil_herba/tensorflow-x86/tensorflow/lite/testing/kernel_test/BUILD:81:1: Couldn't build file tensorflow/lite/testing/kernel_test/_objs/input_generator_test/input_generator_test.o: C++ comp
ilation of rule '//tensorflow/lite/testing/kernel_test:input_generator_test' failed (Exit 1)
tensorflow/lite/testing/kernel_test/input_generator_test.cc:21:44: fatal error: testing/base/public/googletest.h: No such file or directory
compilation terminated.
```
I guess that ```googletest.h``` is some internal Google header file which shouldn't be use in open source projects.
**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel test tensorflow/lite/testing/kernel_test:input_generator_test

**Any other info / logs**
@liyunlu0618 could you take a look on that?

Other similar problems from the same commit
- tensorflow/lite/testing/kernel_test/generate_diff_report.cc:
```
tensorflow/lite/testing/kernel_test/generate_diff_report.cc:22:3: error: 'string' was not declared in this scope
   string base, test, output;
```
- tensorflow/lite/testing/kernel_test/util_test.cc
```
testing/base/public/googletest.h: No such file or directory
```
- tensorflow/lite/testing/kernel_test/diff_analyzer_test.cc:
```googletest.h``` is not used but ```FLAGS_test_tmpdir``` which probably comes from this header is used so it should be removed as well.

"
26668,Benchmarker of tensorflow/lite initialize tensorflow,"Current implementation of tensorflow/lite/testing have initialization of TensorFlow core.

https://github.com/tensorflow/tensorflow/blob/069c503b4ac5bf4c89aeb38332685e427e0d3e03/tensorflow/lite/testing/init_tensorflow.cc#L23-L30

Why this is needed? Benchmarker always include binary of tensorflow. I think this is enough.

```diff
diff --git a/tensorflow/lite/testing/init_tensorflow.cc b/tensorflow/lite/testing/init_tensorflow.cc
index ed4d123744..d8d639f410 100644
--- a/tensorflow/lite/testing/init_tensorflow.cc
+++ b/tensorflow/lite/testing/init_tensorflow.cc
@@ -20,12 +20,5 @@ limitations under the License.
 #include ""tensorflow/core/platform/init_main.h""
 
 namespace tflite {
-void InitTensorFlow() {
-  static const char* kFakeName = ""fake program name"";
-  int argc = 1;
-  char* fake_name_copy = strdup(kFakeName);
-  char** argv = &fake_name_copy;
-  ::tensorflow::port::InitMain(kFakeName, &argc, &argv);
-  free(fake_name_copy);
-}
+void InitTensorFlow() {}
 }  // namespace tflite
```
"
26667,Multipose outputs,"**System information**
- TensorFlow version: tf-lite
- Doc Link: https://www.tensorflow.org/lite/models/pose_estimation/overview
https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5


**By using the getOutputTensors function in tflite interpreter, I can get that there are 4 output arrays with dimension of  1*23*17*17, 1*23*17*34, 1*23*17*64 and 1*23*17*1. However in the tflite medium article or website, the usage of full output is not explained. How can I get the pose coordinates in the image? In the heatmap array, I am getting very low confidence scores close to 0.00 for every pixel. Here is what I have tried : https://stackoverflow.com/questions/55136861/how-to-use-outputs-of-posenet-model-in-tflite**

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue? - No**
"
26665,"tensorflow 2.0, variable_scope(), TypeError: __call__() got an unexpected keyword argument 'partition_info'","I have convert a CNN model from tf1.x to tf2.0 by using ` tf_upgrade_v2 `, but when i used this converted model, i got an error:

`File ""/home/hsw/virtual_env/tf2.0/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 2492, in default_variable_creator
    import_scope=import_scope, distribute_strategy=distribute_strategy)
  File ""/home/hsw/virtual_env/tf2.0/lib/python3.6/site-packages/tensorflow/python/ops/variables.py"", line 216, in __call__
    return super(VariableMetaclass, cls).__call__(*args, **kwargs)
  File ""/home/hsw/virtual_env/tf2.0/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 422, in __init__
    constraint=constraint)
  File ""/home/hsw/virtual_env/tf2.0/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 545, in _init_from_args
    initial_value() if init_from_fn else initial_value,
  File ""/home/hsw/virtual_env/tf2.0/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 886, in <lambda>
    shape.as_list(), dtype=dtype, partition_info=partition_info)
TypeError: __call__() got an unexpected keyword argument 'partition_info'`

it seems like something wrong in `variables.py`, and the converted model such as like this :

        with tf.compat.v1.variable_scope('backbone', reuse=tf.compat.v1.AUTO_REUSE):
            net = tf.compat.v1.layers.separable_conv2d(inputs, 16, 3, 1, 'same',
                                         activation=tf.nn.elu,
                                         depthwise_initializer=tf.keras.initializers.glorot_normal(),
                                         pointwise_initializer=tf.keras.initializers.glorot_normal(),
                                         name='conv1')
            net = tf.compat.v1.layers.max_pooling2d(net, 2, 2, padding='same')
            net = tf.compat.v1.layers.separable_conv2d(net, 32, 3, 1, 'same',
                                         activation=tf.nn.elu,
                                         depthwise_initializer=tf.keras.initializers.glorot_normal(),
                                         pointwise_initializer=tf.keras.initializers.glorot_normal(),
                                         name='conv2')

how should do to solve this problem?"
26664,Using create_training_graph with act_quant not found error ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 16.04
- TensorFlow installed from (source or binary):
Binary
- TensorFlow version (use command below):
1.9.0
- Python version:
3.6
- CUDA/cuDNN version:
9.0
- GPU model and memory:

After the addition of the quantization training code like:

    ......
    full_precision_graph = tf.get_default_graph()
    with tf.variable_scope(params['model_scope']):
    build_the_network()
    tensorflow.contrib.quantize.create_training_graph(input_graph=full_precision_graph, quant_delay=0)
    ......


I intend to train a int8 model from a pre-trained f32 ckpt model. However, the error raised as 

    ......
    netname/act_quant/max not found in checkpoint
    ......


I am wondering if something wrong with my usage of the quantization creating. I only defined the train quantization net and the screen did print out something like the following information before the error occurred

    ......
    INFO:tensorflow:Skipping netname/conv3_eltwise4/add, because its followed by an activation.
    ...."
26662,"tf.logging does not exist in tf 2.0, yet is used in example","<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: 2.0.0-alpha0
- Doc Link: 
https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l02c01_celsius_to_fahrenheit.ipynb
and 
https://classroom.udacity.com/courses/ud187/lessons/e0c70c77-5584-4f83-a47b-a67a6172ae75/concepts/fe91023e-9699-418a-8f4e-58c6acad1169


**Describe the documentation issue**
If you try to execute this code in a python REPL, it fails at

    tf.logging.set_verbosity(tf.logging.ERROR)

with 

    AttributeError: 'module' object has no attribute 'logging'

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**

Depends on how you want it. As of https://www.tensorflow.org/versions/r2.0/api_docs/python/tf, it seems the logging API does not exist. Of the methods in https://stackoverflow.com/questions/35911252/disable-tensorflow-debugging-information, only 

    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' 

worked.
"
26661,what does USE_EIGEN_TENSOR used?,"There are some  USE_EIGEN_TENSOR macros, What does it used for?
"
26659,how to enable gpu_options properties when in eager distributed execution of multi-gpus,"I use MirroredStrategy to create distributed execution. And I use the ConfigProto to set gpu_options.allow_growth and set to tf.enable_eager_execution(config=config). However after I run the distributed code in eager mode, my 4 gpus in the same machine are allocated all their memory. It seems that the  gpu_options does not work.

I use tensorflow1.12 and python3.6."
26658,"Dataset object is not an iterator, although I can iterate on it.","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Archlinux
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-dev20190312
- Python version: 3.6
- CUDA/cuDNN version: 10
- GPU model and memory: 1080 Ti

**Describe the current behavior**

> TypeError: 'DatasetV1Adapter' object is not an iterator

**Describe the expected behavior**:

Should fetch the first element of the iterator, thus print 10.

**Code to reproduce the issue**

```python
import tensorflow as tf

def real_gen():
    for _ in range(10):
        yield 10.0

dataset = tf.data.Dataset.from_generator(real_gen, (tf.float32))
# Fails
print(next(dataset))
```

Since I can iterate on the dataset using a for loop

```python
# It works
for real in dataset:
    print(real)
```

I do expect I can treat the dataset as an iterator, thus extracting the next element by calling `next(dataset)`.

"
26657,"Session.run() doesn't accepts string of image bytes(2nd Parameter), as it usually does","*System information**
- Os Ubuntu 
- Tensorflow 1.13.1
- Python 2.7
Code:
`image_data = tf.gfile.FastGFile(image_path, 'rb').read()`
`prediction = sess.run('final_result:0', {'Placeholder:0': image_data})`

Error:
`could not convert string to float`"
26656,tensorflow serving embedding get Truncate？,"NodeDef mentions attr 'Truncate' not in Op<name=Cast; signature=x:SrcT -> y:DstT; attr=SrcT:type; attr=DstT:type>; NodeDef: embedding_1/Cast = Cast[DstT=DT_INT32, SrcT=DT_FLOAT, Truncate=false, _output_shapes=[[?,15]], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_input_1_0_0). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).%0A%09 [[Node: embedding_1/Cast = Cast[DstT=DT_INT32, SrcT=DT_FLOAT, Truncate=false, _output_shapes=[[?,15]], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_input_1_0_0)]]|"
26653,Multi-GPU workstation crashes during tf.Session(),"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):`Linux 5.0.0-arch1-1-ARCH #1 SMP PREEMPT Mon Mar 4 14:11:43 UTC 2019 x86_64 GNU/Linux`
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:None
- TensorFlow installed from (source or binary):`community python-tensorflow-opt-cuda`
- TensorFlow version (use command below):`1.13.1`
- Python version:`3.7.2`
- Bazel version (if compiling from source):None
- GCC/Compiler version (if compiling from source):None
- CUDA/cuDNN version:`V10.0.130` / `7.5.0`
- GPU model and memory: 2 x `Geforce GTX 1080 Ti 11GB`; Driver Version: `418.43`

**Describe the current behavior**
The workstation completely crashes if a `tf.Session()` is created when multiple GPUs are present.

I will roll back the last driver updates and post any updates.
Not sure if this is an error tensorflow can fix, maybe it is just a faulty driver.

**Describe the expected behavior**
Workstation should not crash.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```python
import tensorflow as tf
s = tf.Session()
```
or, in short
`python -c ""import tensorflow as tf; s = tf.Session()""`
the following line crashes as well
`CUDA_VISIBLE_DEVICES=""0,1"" python -c ""import tensorflow as tf; s = tf.Session()""`

**Other info / logs**
The Problem exists only if multiple GPUs are present, so the following code works as expected:
- `CUDA_VISIBLE_DEVICES="""" python -c ""import tensorflow as tf; s = tf.Session()""`
```
2019-03-13 09:49:14.192325: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3193000000 Hz
2019-03-13 09:49:14.193496: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55f504e31540 executing computations on platform Host. Devices:
2019-03-13 09:49:14.193508: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2019-03-13 09:49:14.201742: E tensorflow/stream_executor/cuda/cuda_driver.cc:300] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2019-03-13 09:49:14.201756: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:161] retrieving CUDA diagnostic information for host: ***
2019-03-13 09:49:14.201773: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:168] hostname: ***
2019-03-13 09:49:14.201820: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:192] libcuda reported version is: 418.43.0
2019-03-13 09:49:14.201833: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:196] kernel reported version is: 418.43.0
2019-03-13 09:49:14.201837: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:303] kernel version seems to match DSO: 418.43.0
```
- `CUDA_VISIBLE_DEVICES=""0"" python -c ""import tensorflow as tf; s = tf.Session()""`
```
2019-03-13 09:50:13.918948: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3193000000 Hz
2019-03-13 09:50:13.919562: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55be94d7cdb0 executing computations on platform Host. Devices:
2019-03-13 09:50:13.919598: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2019-03-13 09:50:14.009982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-03-13 09:50:14.010633: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55be949e86f0 executing computations on platform CUDA. Devices:
2019-03-13 09:50:14.010646: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1
2019-03-13 09:50:14.011034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:01:00.0
totalMemory: 10.91GiB freeMemory: 10.32GiB
2019-03-13 09:50:14.011044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2019-03-13 09:50:14.778440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-03-13 09:50:14.778463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2019-03-13 09:50:14.778467: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
2019-03-13 09:50:14.778759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9970 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
```
- `CUDA_VISIBLE_DEVICES=""1"" python -c ""import tensorflow as tf; s = tf.Session()""`
```
2019-03-13 09:50:19.398946: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3193000000 Hz
2019-03-13 09:50:19.400142: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55eb2ee074e0 executing computations on platform Host. Devices:
2019-03-13 09:50:19.400177: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2019-03-13 09:50:19.480237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-03-13 09:50:19.480807: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55eb2e7cc660 executing computations on platform CUDA. Devices:
2019-03-13 09:50:19.480820: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1
2019-03-13 09:50:19.481144: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:02:00.0
totalMemory: 10.92GiB freeMemory: 10.77GiB
2019-03-13 09:50:19.481169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2019-03-13 09:50:19.790419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-03-13 09:50:19.790445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2019-03-13 09:50:19.790464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
2019-03-13 09:50:19.790752: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10411 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)
```

I ran the code on a different machine with only one GPU and it worked just fine.
```
2019-03-13 09:55:10.169250: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2019-03-13 09:55:10.193346: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3801830000 Hz
2019-03-13 09:55:10.194746: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55667abb6310 executing computations on platform Host. Devices:
2019-03-13 09:55:10.194784: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2019-03-13 09:55:10.992667: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55667bfd5a40 executing computations on platform CUDA. Devices:
2019-03-13 09:55:10.992720: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): TITAN RTX, Compute Capability 7.5
2019-03-13 09:55:10.994345: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: 
name: TITAN RTX major: 7 minor: 5 memoryClockRate(GHz): 1.77
pciBusID: 0000:65:00.0
totalMemory: 23.62GiB freeMemory: 23.45GiB
2019-03-13 09:55:10.994378: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2019-03-13 09:55:11.302104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-03-13 09:55:11.302139: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2019-03-13 09:55:11.302143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
2019-03-13 09:55:11.302652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22722 MB memory) -> physical GPU (device: 0, name: TITAN RTX, pci bus id: 0000:65:00.0, compute capability: 7.5)
```"
26651,resize_image_with_pad is not available as described in documentation,"
**System information**
- TensorFlow version: 2.0.0-alpha
- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/image/resize_image_with_pad


**Describe the documentation issue**
This is not available in 2.0.0 it is renamed as tf.image.resize_with_pad
"
26649,Documentation link broken for what is transfer learning?,"Doc Link: https://www.tensorflow.org/js/tutorials/transfer/image-classification

The above link is broken.


"
26648,How To build And deploy Tensor flow label_image code wit Qt Envirnment,"


**System information**
- Linux Ubuntu 16.04
- TensorFlow installed from source
- TensorFlow version:1.7.0
- Python version:Python 2.7.12
- Installed using  pip
- Bazel version (if compiling from source):Build label: 0.23.1
- GCC/Compiler version (if compiling from source):gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.11) 
- CUDA/cuDNN version: CUDA10
- GPU model and memory:nvidia  6GB


<em>
I have build and test the tensor flow with label_image test code. I have used bazel on ubuntu 16.04 (x64) system. I have build and test both the python and c++ code.


Now I want to build the label_image c++ code with my project in Qt.
         For the same i have tried to build my project by including -ltensorflow_framework generated via tensor flow build through bazel, but not succeeded.

Also I have tried by building label_image as library and tried to include this label_image output in my Qt Project, but this was also not sccessfull.

 Can you please let me know the best way to complete my task (build label_image c++ code with Qt)?
        I am interested to build the tensor flow with gpu support.

please point me to the exact example or step by step guide to build the target.
</em>"
26645,Testing guide page not exist (404),"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: 1.13
- Doc Link: https://www.tensorflow.org/api_guides/python/test


**Describe the documentation issue**
[Testing guide](https://www.tensorflow.org/api_guides/python/test) not exist which linked from [tf.test page](https://www.tensorflow.org/api_docs/python/tf/test).

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
I don't know if the page originally exists. 🤔 "
26643,Inconsistent argument for compression between TFRecordWriter and TFRecordDataset,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
MacOSX 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
tf.version.VERSION='2.0.0-dev20190311'
tf.version.GIT_VERSION='v1.12.0-9917-gf988edacf4'
- Python version:
3.6.8
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
`TFRecordOptions`'s `compression_type` argument expects an int (defined in `tf.io.TFRecordCompressionType`), while `tf.data.TFRecordDataset`'s `compression_type` must be a string.  IMHO, TensorFlow 2.0 should eliminate this sort of inconsistency, it would be more pythonic:

```bash
$ python -m this | grep ""do it""
There should be one-- and preferably only one --obvious way to do it.
```

**Describe the expected behavior**
Both should be consistent (and accepting a string would provide the simplest API).  Any other function that expects a `compression_type` should also respect the same API.

**Code to reproduce the issue**
For example:

```python
GZIP = tf.io.TFRecordCompressionType.GZIP
options = tf.io.TFRecordOptions(compression_type=GZIP)
with tf.io.TFRecordWriter(""my_compressed.tfrecord"", options) as f:
    f.write(b""This is the first record"")
    f.write(b""And this is the second record"")

dataset = tf.data.TFRecordDataset([""my_compressed.tfrecord""], compression_type=GZIP)
```

**Other info / logs**
Here is the stacktrace:

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-3-40e8c78e4ba4> in <module>
      5     f.write(b""And this is the second record"")
      6
----> 7 dataset = tf.data.TFRecordDataset([""my_compressed.tfrecord""], compression_type=GZIP)

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/data/ops/readers.py in __init__(self, filenames, compression_type, buffer_size, num_parallel_reads)
    168
    169     if num_parallel_reads is None:
--> 170       self._impl = filenames.flat_map(read_one_file)
    171     else:
    172       self._impl = filenames.interleave(

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py in flat_map(self, map_func)
   1043       Dataset: A `Dataset`.
   1044     """"""
-> 1045     return FlatMapDataset(self, map_func)
   1046
   1047   def interleave(self,

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py in __init__(self, input_dataset, map_func)
   3063     self._input_dataset = input_dataset
   3064     self._map_func = StructuredFunctionWrapper(
-> 3065         map_func, self._transformation_name(), dataset=input_dataset)
   3066     if not isinstance(self._map_func.output_structure, DatasetStructure):
   3067       raise TypeError(""`map_func` must return a `Dataset` object."")

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py in __init__(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)
   2386           ops.GraphKeys.TABLE_INITIALIZERS))
   2387
-> 2388       self._function = wrapper_fn._get_concrete_function_internal()
   2389       if add_to_graph:
   2390         self._function.add_to_graph(ops.get_default_graph())

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal(self, *args, **kwargs)
   1299     """"""Bypasses error checking when getting a graph function.""""""
   1300     graph_function = self._get_concrete_function_internal_garbage_collected(
-> 1301         *args, **kwargs)
   1302     # We're returning this concrete function to someone, and they may keep a
   1303     # reference to the FuncGraph without keeping a reference to the

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   1293     if self.input_signature:
   1294       args, kwargs = None, None
-> 1295     graph_function, _, _ = self._maybe_define_function(args, kwargs)
   1296     return graph_function
   1297

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   1556           or call_context_key not in self._function_cache.missed):
   1557         self._function_cache.missed.add(call_context_key)
-> 1558         graph_function = self._create_graph_function(args, kwargs)
   1559         self._function_cache.primary[cache_key] = graph_function
   1560         return graph_function, args, kwargs

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   1489             arg_names=arg_names,
   1490             override_flat_arg_shapes=override_flat_arg_shapes,
-> 1491             capture_by_value=self._capture_by_value),
   1492         self._function_attributes)
   1493

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    692                                           converted_func)
    693
--> 694       func_outputs = python_func(*func_args, **func_kwargs)
    695
    696       # invariant: `func_outputs` contains only Tensors, IndexedSlices,

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py in wrapper_fn(*args)
   2379           attributes=defun_kwargs)
   2380       def wrapper_fn(*args):  # pylint: disable=missing-docstring
-> 2381         ret = _wrapper_helper(*args)
   2382         ret = self._output_structure._to_tensor_list(ret)
   2383         return [ops.convert_to_tensor(t) for t in ret]

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py in _wrapper_helper(*args)
   2324         nested_args = (nested_args,)
   2325
-> 2326       ret = func(*nested_args)
   2327       # If `func` returns a list of tensors, `nest.flatten()` and
   2328       # `ops.convert_to_tensor()` would conspire to attempt to stack

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/data/ops/readers.py in read_one_file(filename)
    165
    166     def read_one_file(filename):
--> 167       return _TFRecordDataset(filename, compression_type, buffer_size)
    168
    169     if num_parallel_reads is None:

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/data/ops/readers.py in __init__(self, filenames, compression_type, buffer_size)
    105         compression_type,
    106         argument_default="""",
--> 107         argument_dtype=dtypes.string)
    108     self._buffer_size = convert.optional_param_to_tensor(
    109         ""buffer_size"",

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/data/util/convert.py in optional_param_to_tensor(argument_name, argument_value, argument_default, argument_dtype)
     30   if argument_value is not None:
     31     return ops.convert_to_tensor(
---> 32         argument_value, dtype=argument_dtype, name=argument_name)
     33   else:
     34     return constant_op.constant(

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype, dtype_hint)
   1048   preferred_dtype = deprecation.deprecated_argument_lookup(
   1049       ""dtype_hint"", dtype_hint, ""preferred_dtype"", preferred_dtype)
-> 1050   return convert_to_tensor_v2(value, dtype, preferred_dtype, name)
   1051
   1052

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor_v2(value, dtype, dtype_hint, name)
   1106       name=name,
   1107       preferred_dtype=dtype_hint,
-> 1108       as_ref=False)
   1109
   1110

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors)
   1184
   1185     if ret is None:
-> 1186       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1187
   1188     if ret is NotImplemented:

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)
    302                                          as_ref=False):
    303   _ = as_ref
--> 304   return constant(v, dtype=dtype, name=name)
    305
    306

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name)
    243   """"""
    244   return _constant_impl(value, dtype, shape, name, verify_shape=False,
--> 245                         allow_broadcast=True)
    246
    247

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)
    281       tensor_util.make_tensor_proto(
    282           value, dtype=dtype, shape=shape, verify_shape=verify_shape,
--> 283           allow_broadcast=allow_broadcast))
    284   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)
    285   const_tensor = g.create_op(

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)
    465       nparray = np.empty(shape, dtype=np_dt)
    466     else:
--> 467       _AssertCompatible(values, dtype)
    468       nparray = np.array(values, dtype=np_dt)
    469       # check to them.

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py in _AssertCompatible(values, dtype)
    370     else:
    371       raise TypeError(""Expected %s, got %s of type '%s' instead."" %
--> 372                       (dtype.name, repr(mismatch), type(mismatch).__name__))
    373
    374

TypeError: Expected string, got 2 of type 'int' instead.
```"
26642,Will LSTMBlockFusedCell be supported in tensorflow 2.0?,"I noticed that keras in tensorflow 2.0 unifies backend between CPU and GPU with CuDNN, but it's a little slow on CPU. Will you add LSTMBlockFusedCell or LSTMBlockCell to tf 2.0, since they have better performance?
"
26639,Nasnet models don't support custom image sizes even if include_top is set to False,"The general idea for fine-tuning pre-trained models with a custom image size is to set the `include_top` parameter to `False` when loading the models. However it doesn't seem to be working with the Nasnet models in `tf.keras` so far. All other models including Inception are working fine.

__Note:__ I was using tensorflow 2.0 alpha so I'm not sure if that is the problem.

I believe maybe some issue somewhere in checking dimension size along with the `include_top` flag but I might be wrong.

Following is the stack trace.

```

Code executed:
nasnet = tf.keras.applications.nasnet.NASNetLarge(include_top=False, weights='imagenet', 
                                                                                  input_shape=(100, 100, 3))

Error Message:
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-100-64f6d45dc54d> in <module>
      1 nasnet = tf.keras.applications.nasnet.NASNetLarge(include_top=False, weights='imagenet', 
----> 2                                                                                 input_shape=(100, 100, 3))
      3 nasnet.summary()

/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/applications/__init__.py in wrapper(*args, **kwargs)
     68       kwargs['models'] = models
     69       kwargs['utils'] = utils
---> 70     return base_fun(*args, **kwargs)
     71   return wrapper
     72 

/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/applications/nasnet.py in NASNetLarge(*args, **kwargs)
     37 @keras_modules_injection
     38 def NASNetLarge(*args, **kwargs):
---> 39   return nasnet.NASNetLarge(*args, **kwargs)
     40 
     41 

/opt/anaconda3/lib/python3.6/site-packages/keras_applications/nasnet.py in NASNetLarge(input_shape, include_top, weights, input_tensor, pooling, classes, **kwargs)
    364                   classes=classes,
    365                   default_size=331,
--> 366                   **kwargs)
    367 
    368 

/opt/anaconda3/lib/python3.6/site-packages/keras_applications/nasnet.py in NASNet(input_shape, penultimate_filters, num_blocks, stem_block_filters, skip_reduction, filter_multiplier, include_top, weights, input_tensor, pooling, classes, default_size, **kwargs)
    166                                       data_format=backend.image_data_format(),
    167                                       require_flatten=True,
--> 168                                       weights=weights)
    169 
    170     if backend.image_data_format() != 'channels_last':

/opt/anaconda3/lib/python3.6/site-packages/keras_applications/imagenet_utils.py in _obtain_input_shape(input_shape, default_size, min_size, data_format, require_flatten, weights)
    290                                  'and loading `imagenet` weights, '
    291                                  '`input_shape` should be ' +
--> 292                                  str(default_shape) + '.')
    293         return default_shape
    294     if input_shape:

ValueError: When setting `include_top=True` and loading `imagenet` weights, `input_shape` should be (331, 331, 3).
```"
26638, Filter out empty strings as post processing,"Let users filter out empty strings as post-processing if they do want to.
Re indices and shapes, the right answer is to have proper ragged and sparse tensor support to tf.boolean_mask.

This is a follow up to issue #26368 . and PR #26475 . 
"
26636,Add a origin url param to load_data function in keras.datasets,"**System information**
- TensorFlow version (you are using): 2.0.0-alpha0
- Are you willing to contribute it (Yes/No): Yes


**Describe the feature and the current behavior/state.**

`keras.datasets` is a nice place for beginners to start, but `storage.googleapis.com` is blocked in some country, it makes it impossible to directly download the dataset. I propose to add a URL param to the `load_data` function of the dataset. So we could provide some mirror URL for users in our country.

**Will this change the current API? How?**
Yes, will add a param with default value None to all `load_data` function in `tensorflow/python/keras/datasets`

**Who will benefit with this feature?**

All new beginners in countries blocked `storage.googleapis.com`

**Any Other info.**
"
26635,Transformer step/sec decrease over time to 0,"**System information**
**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):**
Yes
**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):**
Ubuntu 18.04
**- TensorFlow installed from (source or binary):**
Binary / pip install
**- TensorFlow version**
1.13.1
**- Python version:**
3.6.7
**- CUDA/cuDNN version:**
CUDA = 10
CUDNN_VERSION 7.5
**- GPU model and memory:**
8X V100 16 GB

(We observed same behavior with CUDA 9.2 & TF 1.12 compiled for CUDA 9.2.)
[Docker Image](https://gitlab.com/nvidia/cuda/blob/ubuntu18.04/9.2/devel/cudnn7/Dockerfile)

**Problem:** 
Training steps/s while using a Transformer model repeatedly drops from 20 steps/s to <1 step/s.  
This is internally reproducible. 
GPU usage plummets to ~0% during the periods at <1 step/s

**Context:**
We train with a [Transformer model](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py#L175).

Training behaves poorly with [Transformer](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py#L175), but works well with [Universal Transformer](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/research/universal_transformer.py#L41).

In both scenarios, we use 4x P100, subword tokens (https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/text_encoder.py#L448), and [MirroredStrategy Enabled](https://www.tensorflow.org/api_docs/python/tf/contrib/distribute/MirroredStrategy) distribution strategy. 

(We observed the same behavior with 8x V100, as well.)

Ablation tests on 1) P100 versus V100, 2) MirroredStrategy versus t2t’s built-in multi-GPU, and 3) Universal Transformer versus Transformer reveal that #3 is the driving variable.

We’re using tensor2tensor’s transformer / universal_transformer implementations. 
The hparams used are transformer_tiny and universal_transformer_tiny. One noteworthy deviation from common usage is that our hparams.max_length value is large (2500) and our batch size is often small, since our median sequence length is 750 tokens.

**Describe the current behaviour**
With Transformer, training run’s step/sec alternates between roughly 20 step/sec and 0.5 step/sec.

For the first 3-4 hours it is biased towards 20 steps/sec. For the next is ~2 it hours it  begins dropping to 0.5 step/sec more frequently, before finally dropping almost exclusively to 0.5 step/sec.

If we restart our training process from a checkpoint that was made when the model ran slowly, the behaviour repeats itself, starting at 20 step/sec before dropping back down to 0.5. 

Below are two graphs The first is a graph of our model’s step/sec degradation over a training run. The median value early on is ~20 (with some occasional drops to below 5 step/sec) but as the training continues our performance drops to almost 0 step/sec.
Note the vertical axis is logarithmic
![image](https://user-images.githubusercontent.com/43351375/54249496-c64c5880-4516-11e9-86ad-3a6d6f1c940b.png)

 The second graph shows 3.25 consecutive runs of our model, where each restart picks up a checkpoint the previous run generated. These restarts were not caused an error, but are due to our system automatically preempting gpu intensive jobs after 24 hours. Note the consistent degradation in performance after every restart.
![image](https://user-images.githubusercontent.com/43351375/54249582-0b708a80-4517-11e9-9287-663c54799d00.png)



**Describe the expected behavior**

Our runs with universal transformer exhibit a completely flat step/sec curve. The figure below shows the expected behaviour in red in terms of step/sec variance. Note that the model’s step/sec exhibit almost no variation except for the sharp drops attributed to evaluation steps.
![Transformer vs Universal Transformer Step_Sec Decrease (2)](https://user-images.githubusercontent.com/43351375/54287868-dc8cff80-457c-11e9-8c64-52dbfa98c833.png)

**Other info / logs**
Our GPU utilization (as measured by nvidia-smi) is tightly coupled with the above graph. Where our step/sec is high, our gpu utilization is nearly always at 50%, occasionally dropping to 0 for a second or two before shooting back up. When our step/sec consistently drops to below one, our gpu utilization is mostly at 0%. Every few minutes it will briefly shoot up to 50% and then drop to 0% a second later.

In terms of auc performance, our transformer model continues to improve even as the step/sec decay. 

Note this is a sibling issue to: https://github.com/tensorflow/tensor2tensor/issues/1484"
26634,ImportError: DLL load failed: The specified module could not be found.,"I have this error, how can i solve this problem?

ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""generate_tfrecord.py"", line 17, in <module>
    import tensorflow as tf
  File ""C:\Users\User\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\User\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\User\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\User\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\User\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\User\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\User\Anaconda3\envs\tensorflow1\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\User\Anaconda3\envs\tensorflow1\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
26633,about  deeplearning models center repository.,"The technology of in-depth learning has been developed for a long time. At present, it is not clear whether there is a model library of the type of Maven docker central library. If not, I hope we can build it. If there is, please let me know. Thanks."
26629,"Keras Error: Variable does not exist, or was not created with tf.get_variable()","```
import tensorflow.keras as ks
def scope_error_test():
    input_holder = tf.placeholder(dtype=tf.float32,
                                  shape=(None, 368, 368, 3),
                                  name='input')
    with tf.variable_scope(""scope_1""):
        with tf.variable_scope(""scope_2""):
            conv1 = ks.layers.Conv2D(kernel_size=7,
                                     filters=64,
                                     strides=2,
                                     padding='same',
                                     activation=tf.nn.relu,
                                     name='conv1')(input_holder)
            pool1 = ks.layers.MaxPool2D(pool_size=3, padding='same',
                                        strides=2,
                                        name='pool1')(inputs=conv1)
    print(pool1.get_shape().as_list())
with tf.Session() as sess:
        scope_error_test()
        sess.run(tf.global_variables_initializer())
        print(tf.global_variables())
        with tf.variable_scope('', reuse=True):
            for variable in tf.global_variables():
                var_name = variable.name.split(':')[0]
                var_tf = tf.get_variable(var_name)
```


In this example code, I want to understand why I get this error: ValueError: Variable scope_1/scope_2/conv1/kernel does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=tf.AUTO_REUSE in VarScope?

Note: I used tensorflow.contrib.layers instead of tensorflow.keras.layers and It was working correctly! Could be a bug !?

I have tried tensorflow 1.9.0 and tensorflow 1.12.0"
26628,Pypi file for tensorflow-gpu 1.10.1 for Python 3.6 manylinux is zero bytes,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Amazon Linux 2012.3
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.10.1
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I have a requirements.txt file that contains the following:

```
tensorflow-gpu==1.10.1 \
    --hash=sha512:cf7ee7c41166f936c06b4041efab5d901019cc0b6caba69e6645bcb5886243dfcf072f71119692eec31359585da3bede39f208e75e3b7dc84d9c99fe33cf18d6 \
    --hash=sha512:ae6d1c332e8128435f61cab9ae8a770aac897eafa94f4abc4937a9d288622bc717e73663f1941f0211b6effadf6ac08d20a70829a48e5663f2f3638e8e61b9ff
```
I use the following command in my Dockerfile to install tensorflow-gpu and other dependencies:
```
RUN pip install --no-deps --upgrade-strategy only-if-needed --no-cache-dir --require-hashes -r ./$REQUIREMENTS_TF_FILE
```
However, I get an error that the hash of the file downloaded from Pypi doesn't match the hash in my requirements file. I went to the Pypi website and tried to download the file [tensorflow_gpu-1.10.1-cp36-cp36m-manylinux1_x86_64.whl](https://files.pythonhosted.org/packages/3f/c3/000755084b5e7b5a11df1b9166a54936075ec280b7a615cecce42973fc8b/tensorflow_gpu-1.10.1-cp36-cp36m-manylinux1_x86_64.whl ). I found that the file is zero bytes!

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26627,Improve tf_compile to allow emitting HLO module instead of/in addition to executable,Currently tf_compile (tensorflow/compiler/aot) always emits executable. But there are use cases where dumping HLO module is more desired. This is a tracking bug to implement it.
26626,TOCO failed for tensorflow model using Universal Sentence Encoder module ,"Hi, all. 

I'm trying to export a TensorFlow model to the TensorFlow Lite format for eventual use in Firebase ML Kit.

The model is a one-step Universal Sentence Encoder text embedding using the TensorFlow Hub module. I'm getting a converter error that seems to be due to unsupported ops. 

Tried setting the `allow_custom_ops` and `target_ops` to no avail.

Would anyone be kind enough to point out any silly mistakes I've made, or point me to other options for converting USE to a TensorFlow Lite model?

<hr>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave Version 10.14.1
- TensorFlow installed from (source or binary): pip
- TensorFlow version (or github SHA if from source): 1.13.1
- Python version: 3.7.0
- Code running out of a Jupyter Notebook (hence the printed traceback at the top)

**Provide the text output from tflite_convert**

```
INFO:tensorflow:Saver not created because there are no variables in the graph to restore
I0312 15:25:26.630760 4626404800 saver.py:1483] Saver not created because there are no variables in the graph to restore
INFO:tensorflow:Froze 95 variables.
I0312 15:25:33.715335 4626404800 graph_util_impl.py:268] Froze 95 variables.
INFO:tensorflow:Converted 95 variables to const ops.
I0312 15:25:36.719951 4626404800 graph_util_impl.py:301] Converted 95 variables to const ops.
---------------------------------------------------------------------------
ConverterError                            Traceback (most recent call last)
<ipython-input-24-d7f2951cbdfb> in <module>
     14 
     15     converter = tf.lite.TFLiteConverter.from_session(session, [input_text], [embedding])
---> 16     tflite_model = converter.convert()
     17     open(""converted_model.tflite"", ""wb"").write(tflite_model)

~/.virtualenvs/py371/lib/python3.7/site-packages/tensorflow/lite/python/lite.py in convert(self)
    453           input_tensors=self._input_tensors,
    454           output_tensors=self._output_tensors,
--> 455           **converter_kwargs)
    456     else:
    457       result = _toco_convert_graph_def(

~/.virtualenvs/py371/lib/python3.7/site-packages/tensorflow/lite/python/convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, *args, **kwargs)
    440   data = toco_convert_protos(model_flags.SerializeToString(),
    441                              toco_flags.SerializeToString(),
--> 442                              input_data.SerializeToString())
    443   return data
    444 

~/.virtualenvs/py371/lib/python3.7/site-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str)
    203       stderr = _try_convert_to_unicode(stderr)
    204       raise ConverterError(
--> 205           ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
    206   finally:
    207     # Must manually cleanup files.

ConverterError: TOCO failed. See console for info.
2019-03-12 15:25:49.976435: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: HashTableV2
2019-03-12 15:25:49.993799: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-03-12 15:25:50.245977: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace
2019-03-12 15:25:50.246040: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace
2019-03-12 15:25:50.246082: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace
2019-03-12 15:25:50.246103: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace
2019-03-12 15:25:50.246138: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace
2019-03-12 15:25:50.246169: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace
2019-03-12 15:25:50.246204: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace
2019-03-12 15:25:50.246234: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace
2019-03-12 15:25:50.246265: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace
2019-03-12 15:25:50.246289: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace
2019-03-12 15:25:50.246317: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace
2019-03-12 15:25:50.246350: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace
2019-03-12 15:25:50.246380: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace
2019-03-12 15:25:50.246413: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace
2019-03-12 15:25:50.246441: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace
2019-03-12 15:25:50.246478: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace
2019-03-12 15:25:50.246508: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace
2019-03-12 15:25:50.246542: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace
2019-03-12 15:25:50.246570: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace
2019-03-12 15:25:50.246603: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace
2019-03-12 15:25:50.246631: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace
2019-03-12 15:25:50.246663: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace
2019-03-12 15:25:50.246690: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace
2019-03-12 15:25:50.246709: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace
2019-03-12 15:25:50.246741: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace
2019-03-12 15:25:50.246769: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace
2019-03-12 15:25:50.246800: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace
2019-03-12 15:25:50.246829: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace
2019-03-12 15:25:50.246879: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: StringJoin
2019-03-12 15:25:50.247652: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace
2019-03-12 15:25:50.247691: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: StringSplit
2019-03-12 15:25:50.248189: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: SparseFillEmptyRows
2019-03-12 15:25:50.248347: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Where
2019-03-12 15:25:50.248362: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: GatherNd
2019-03-12 15:25:50.248417: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: StringToHashBucketFast
2019-03-12 15:25:50.248447: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: LookupTableFindV2
2019-03-12 15:25:50.248461: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: LookupTableSizeV2
2019-03-12 15:25:50.248574: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Where
2019-03-12 15:25:50.248717: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Size
2019-03-12 15:25:50.249474: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Cos
2019-03-12 15:25:50.249814: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Where
2019-03-12 15:25:50.250088: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff
2019-03-12 15:25:50.250380: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff
2019-03-12 15:25:50.250680: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff
2019-03-12 15:25:50.251614: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff
2019-03-12 15:25:50.252034: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: GatherNd
2019-03-12 15:25:50.252208: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff
2019-03-12 15:25:50.252508: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff
2019-03-12 15:25:50.252756: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ScatterNd
2019-03-12 15:25:50.253019: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff
2019-03-12 15:25:50.253320: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff
2019-03-12 15:25:50.253626: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff
2019-03-12 15:25:50.254453: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff
2019-03-12 15:25:50.254821: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: GatherNd
2019-03-12 15:25:50.254965: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff
2019-03-12 15:25:50.255260: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff
2019-03-12 15:25:50.255486: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ScatterNd
2019-03-12 15:25:50.255722: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff
2019-03-12 15:25:50.255989: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff
2019-03-12 15:25:50.256269: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff
2019-03-12 15:25:50.257084: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff
2019-03-12 15:25:50.257453: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: GatherNd
2019-03-12 15:25:50.257599: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff
2019-03-12 15:25:50.258020: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff
2019-03-12 15:25:50.258314: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ScatterNd
2019-03-12 15:25:50.258708: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff
2019-03-12 15:25:50.259042: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff
2019-03-12 15:25:50.259322: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff
2019-03-12 15:25:50.260269: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff
2019-03-12 15:25:50.260736: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: GatherNd
2019-03-12 15:25:50.260881: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff
2019-03-12 15:25:50.261149: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff
2019-03-12 15:25:50.261371: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ScatterNd
2019-03-12 15:25:50.261605: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff
2019-03-12 15:25:50.261883: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff
2019-03-12 15:25:50.262145: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff
2019-03-12 15:25:50.262942: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff
2019-03-12 15:25:50.263375: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: GatherNd
2019-03-12 15:25:50.263510: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff
2019-03-12 15:25:50.263780: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff
2019-03-12 15:25:50.263991: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ScatterNd
2019-03-12 15:25:50.264199: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff
2019-03-12 15:25:50.264456: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff
2019-03-12 15:25:50.264696: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff
2019-03-12 15:25:50.265468: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff
2019-03-12 15:25:50.265813: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: GatherNd
2019-03-12 15:25:50.265944: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff
2019-03-12 15:25:50.266190: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff
2019-03-12 15:25:50.266453: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ScatterNd
2019-03-12 15:25:50.661198: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1583 operators, 2816 arrays (0 quantized)
2019-03-12 15:25:50.712913: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1583 operators, 2816 arrays (0 quantized)
2019-03-12 15:25:50.824860: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 1077 operators, 1955 arrays (0 quantized)
2019-03-12 15:25:50.938753: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 1040 operators, 1918 arrays (0 quantized)
2019-03-12 15:25:50.987522: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 1039 operators, 1916 arrays (0 quantized)
2019-03-12 15:25:51.045394: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 1039 operators, 1916 arrays (0 quantized)
2019-03-12 15:25:51.086728: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 192 bytes, theoretical optimal value: 192 bytes.
2019-03-12 15:25:51.104668: E tensorflow/lite/toco/toco_tooling.cc:421] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, DIV, EQUAL, EXPAND_DIMS, FLOOR_DIV, FLOOR_MOD, FULLY_CONNECTED, GATHER, L2_NORMALIZATION, LESS, LOGICAL_NOT, MAXIMUM, MEAN, MUL, NOT_EQUAL, PACK, PAD, RANGE, REDUCE_MAX, RESHAPE, RSQRT, SELECT, SHAPE, SIN, SLICE, SOFTMAX, SPARSE_TO_DENSE, SQUARE, SQUEEZE, STRIDED_SLICE, SUB, SUM, TANH, TRANSPOSE. Here is a list of operators for which you will need custom implementations: BatchMatMul, Cos, DynamicPartition, DynamicStitch, GatherNd, HashTableV2, ListDiff, LookupTableFindV2, LookupTableSizeV2, RegexReplace, ScatterNd, Size, SparseFillEmptyRows, StringJoin, StringSplit, StringToHashBucketFast, Where.

Traceback (most recent call last):
  File ""/Users/ejonokuchi/.virtualenvs/py371/bin/toco_from_protos"", line 10, in <module>
    sys.exit(main())
  File ""/Users/ejonokuchi/.virtualenvs/py371/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 59, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""/Users/ejonokuchi/.virtualenvs/py371/lib/python3.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/Users/ejonokuchi/.virtualenvs/py371/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 33, in execute
    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)
Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, DIV, EQUAL, EXPAND_DIMS, FLOOR_DIV, FLOOR_MOD, FULLY_CONNECTED, GATHER, L2_NORMALIZATION, LESS, LOGICAL_NOT, MAXIMUM, MEAN, MUL, NOT_EQUAL, PACK, PAD, RANGE, REDUCE_MAX, RESHAPE, RSQRT, SELECT, SHAPE, SIN, SLICE, SOFTMAX, SPARSE_TO_DENSE, SQUARE, SQUEEZE, STRIDED_SLICE, SUB, SUM, TANH, TRANSPOSE. Here is a list of operators for which you will need custom implementations: BatchMatMul, Cos, DynamicPartition, DynamicStitch, GatherNd, HashTableV2, ListDiff, LookupTableFindV2, LookupTableSizeV2, RegexReplace, ScatterNd, Size, SparseFillEmptyRows, StringJoin, StringSplit, StringToHashBucketFast, Where.

```

Model code:
```
module_url = 'https://tfhub.dev/google/universal-sentence-encoder-large/3'

universal_sentence_encoder = hub.Module(
    module_url,
    trainable=False,
    name='universal_sentence_encoder',
    tags=None
)

with tf.Session() as session:
    session.run(tf.global_variables_initializer())
    session.run(tf.tables_initializer())
    
    input_text = tf.placeholder(tf.string, shape=[None])
    embedding = universal_sentence_encoder(input_text)

    # these flags were set in response to the traceback message
    tf.lite.TFLiteConverter.allow_custom_ops = True
    tf.lite.TFLiteConverter.target_ops = set([
        tf.lite.OpsSet.TFLITE_BUILTINS,
        tf.lite.OpsSet.SELECT_TF_OPS
    ])

    converter = tf.lite.TFLiteConverter.from_session(session, [input_text], [embedding])
    tflite_model = converter.convert()
    open(""converted_model.tflite"", ""wb"").write(tflite_model)
```"
26625,tf.sparse_tensor_to_dense does not have a gradient,"This issue has never been solved, why so eager to close it?
see #6391, #22543"
26624,delete me pls,delete pls
26623,DetectNet alternative,"Are there any common alternatives (in tensorflow ecosystem) to Nvidia DetectNet network (object detection), 
which use bbox layer?"
26622,Export tf.lookup.TextFileIndex in 2.0,"**System information**
- TensorFlow version: `2.0.0.dev20190311`
- Are you willing to contribute it: No

**Describe the feature and the current behavior/state.**

The exported class `tf.lookup.TextFileInitializer` takes as argument `key_index` and `value_index` but the enum `TextFileIndex` is not available via the API.

The `tf.lookup` module should export this enum.

**Will this change the current api? How?**

This will add a new symbol covered by backward compatibility guarantees.

**Who will benefit with this feature?**

Users of `tf.lookup.TextFileInitializer`."
26621,Compilation error of TensorFlow Lite on Windows,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): I don't use python since building TensorFlow Lite
- Python version: See above
- Bazel version (if compiling from source): See above
- GCC/Compiler version (if compiling from source): gcc (Rev2, Built by MSYS2 project) 8.3.0
- CUDA/cuDNN version: No
- GPU model and memory: No

**Describe the current behavior**

Compilation error

```
tensorflow/lite/experimental/c/c_api.cc:80:29: error: function 'void TFL_InterpreterOptionsSetErrorReporter(TFL_InterpreterOptions*, void (*)(void*, const char*, va_list), void*)' definition is marked dllimport
 TFL_CAPI_EXPORT extern void TFL_InterpreterOptionsSetErrorReporter(
                             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
```

**Describe the expected behavior**

Success to compile

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

See tensorflow/lite/experimental/c/c_api.cc This is broken C syntax when TFL_CAPI_EXPORT is `__declspec(dllimport)`

This is small step to way to reproduce on Windows.

foo.cc
```
__declspec(dllimport) extern void foo() {
}
```
```
$ g++ foo.cc
foo.cc:1:35: error: function 'void foo()' definition is marked dllimport
 __declspec(dllimport) extern void foo() {
```

**Other info / logs**

See above

I created pull-request https://github.com/tensorflow/tensorflow/pull/26615
"
26620,No improvement in performance of mobilenet_v1_1.0_224 on TFLite for GPU,"**System information**
- Have I written custom code: yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 8.0.0
- Mobile device: Samsung S9+, Sony XPeria XZ2
- TensorFlow installed from (source or binary): binary `'org.tensorflow:tensorflow-lite:0.0.0-gpu-experimental'`
- TensorFlow version (use command below): - (used pretrained *.tflite file)
- Python version: - (used pretrained *.tflite file)
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: -
- GPU model and memory: Exynos 9810 Octa (Samsung), Sanpdragon 845 (Sony)

**Describe the current behavior**

No differences in performance for Samsung S9+ between CPU and GPU versions.

**Describe the expected behavior**

According to [this TensorFlow's article on Medium](https://medium.com/tensorflow/tensorflow-lite-now-faster-with-mobile-gpus-developer-preview-e15797e6dee7) expected at least 3x improvement.

**Code to reproduce the issue**
TFLite archive for MobileNet v1 (224x224) has been downloaded from link from [article, mentioned above](https://medium.com/tensorflow/tensorflow-lite-now-faster-with-mobile-gpus-developer-preview-e15797e6dee7)

Code:
```java
    private final int ImageHeight = 224;
    private final int ImageWidth = 224;
    private final int ImageChannels = 3;
    private final int ImageSize = ImageHeight * ImageWidth * ImageChannels;

    private final String tfliteFileName = ""mobilenet_v1_1.0_224.tflite"";

    public String run(Context context, AssetManager assetManager) throws IOException {
        AssetFileDescriptor fd = assetManager.openFd(tfliteFileName);
        FileChannel fileChannel = new FileInputStream(fd.getFileDescriptor()).getChannel();
        MappedByteBuffer model =
            fileChannel.map(FileChannel.MapMode.READ_ONLY, fd.getStartOffset(), fd.getDeclaredLength());

        int runCount = 100;
        long elapsedTime = 0;

        GpuDelegate delegate = new GpuDelegate();
        Interpreter.Options options = (new Interpreter.Options()).addDelegate(delegate);

        try(Interpreter tflite = new Interpreter(model, options)) {
            Map<Integer, Object> output = new HashMap<Integer, Object>();
            output.put(Integer.valueOf(0), new float[1][1001]);

            Object inputData = 0;
            float[][][][] in = new float[1][ImageHeight][ImageWidth][ImageChannels];
            // For example: filling blob uniformly from -5. to 5. in Channel-first format.
            int pos = 0;
            for(int c = 0; c < ImageChannels; ++c) {
                for(int j = 0; j < ImageHeight; ++j) {
                    for(int i = 0; i < ImageWidth; ++i) {
                        in[0][j][i][c] = 10 * (float)(pos++) / (ImageSize - 1) - 5;
                    }
                }
            }
            inputData = in;
            Object[] input = new Object[] { inputData };
            tflite.runForMultipleInputsOutputs(input, output);

            for (int i = 0; i < runCount; i++) {
                // run the second time to make sure all the buffers (and other lazy stuff) have been initialized
                long timeStart = System.currentTimeMillis();
                tflite.runForMultipleInputsOutputs(input, output);
                float[][] res = (float[][]) output.get(0);
                elapsedTime += System.currentTimeMillis() - timeStart;
            }
        }

        delegate.close();

        return ""RUN completed: "" + (elapsedTime / runCount) + "" ms"";
    }
```
**Other info / logs**

***Samsung (Exynos GPU)***

Version with GPUDelegate is a little bit slower (170 ms per run vs 140 ms)

***Sony XPeria (Sanpdragon GPU)***

Version with GPUDelegate is a little bit faster (102 ms per run vs 125 ms).

The warning `WARNING: op code #42 cannot be handled by this delegate.` **does not** appear in LogCat during the execution, so it doesn't look like GpuDelegate couldn't handle some operation.
"
26617,Keras RNN Layer does not work with dynamic_decode,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A
- TensorFlow installed from (source or binary):From pip
- TensorFlow version (use command below): tf-nightly-gpu-1.14.1.dev20190310
- Python version: 3.7.2
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10, 7.5 for CUDA Version 10.0
- GPU model and memory: GTX 1060 6GB


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
Using tf.keras.layers.RNN does not work in place of tf.nn.dynamic_rnn, or at least not in a way that I can find. Everytime I attempted to use it, I would get 
`Shape must be rank 2 but is rank 1 for 'decode/Training_Decoder/decoder/while/BasicDecoderStep/decoder/attention_wrapper/lstm_cell_3/MatMul_4' (op: 'MatMul') with input shapes: [1024], [1024,1024]`
but as soon as I swap to dynamic_rnn, it executes perfectly. However I see that dynamic_rnn will be removed. 
**Describe the expected behavior**
tf.keras.RNN should be equivalent to dynamic_rnn
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
There was too much code so I used pastebin.
https://pastebin.com/iJ5diuie
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26616,Dependency on absl_py needs to be updated for compatibility with Bazel version >= 0.25,"In Bazel 0.23,

```
bazel build --nobuild //tensorflow/tools/pip_package:build_pip_package --incompatible_remove_old_python_version_api
```

has a breakage in `@absl_py`. I submitted an upstream fix that should appear on github today, with a release occurring tomorrow. When that lands, [workspace.bzl](https://github.com/tensorflow/tensorflow/blob/a4c589d5c754493c86f10cc71022fe38d184001a/tensorflow/workspace.bzl#L323-L336) will need to be updated to reference the new abseil-py release.

The fix relies on a feature introduced in Bazel 0.22. The incompatible change flag should be flipped in Bazel 0.25, at which point versions of TF without this fix will break by default.

For more info see bazelbuild/bazel#7308."
26612,Casting not necessary,"Tell me if I'm wrong, but in `tensorflow/lite/kernels/internal/reference/integer_ops/softmax.h`, is casting to <int 32> necessary? (Line 89 & 90)"
26611,pix2pix_eager.ipynb error: NotImplementedError: exceptions are not yet supported,"I run the [pix2pix_eager.ipynb](https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/contrib/eager/python/examples/pix2pix/pix2pix_eager.ipynb) on colab. 

At the step [training](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/pix2pix/pix2pix_eager.ipynb#scrollTo=a1zZmKmvOH85):
`train(train_dataset, EPOCHS)`

The error log: 
```
---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
<ipython-input-22-d152560ca122> in <module>()
----> 1 train(train_dataset, EPOCHS)

<ipython-input-18-24e63cd58368> in train(dataset, epochs)
      6 
      7       with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
----> 8         gen_output = generator(input_image, training=True)
      9 
     10         disc_real_output = discriminator(input_image, target, training=True)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    590       else:
    591         # Eager execution on data tensors.
--> 592         outputs = self.call(inputs, *args, **kwargs)
    593         self._handle_activity_regularization(inputs, outputs)
    594         return outputs

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)
    862   def __call__(self, *args, **kwargs):
    863     """"""Calls a graph function specialized to the inputs.""""""
--> 864     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
    865     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
    866 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   1174                 self._input_signature,
   1175                 autograph=self._autograph,
-> 1176                 arg_names=arg_names),
   1177             self._function_attributes)
   1178         if self._input_signature:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, add_control_dependencies, arg_names, op_return_value)
    446         tf_decorator.rewrap(python_func, original_func, converted_func)
    447 
--> 448       func_outputs = python_func(*func_args, **func_kwargs)
    449 
    450       # invariant: `func_outputs` contains only Tensors, IndexedSlices,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in bound_method_wrapper(*args, **kwargs)
   1654     # If __wrapped__ was replaced, then it is always an unbound function
   1655     # that takes self as first argument.
-> 1656     return wrapped_fn(weak_instance(), *args, **kwargs)
   1657 
   1658   # pylint: disable=protected-access

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    439                   strip_decorators=(def_function.function,),
    440                   optional_features=(),
--> 441               ), *args, **kwargs)
    442 
    443         # Wrapping around a decorator allows checks like tf_inspect.getargspec

/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, owner, options, *args, **kwargs)
    287       experimental_partial_types=partial_types)
    288 
--> 289   result = converted_f(*effective_args, **kwargs)
    290 
    291   # The converted function's closure is simply inserted into the function's

/tmp/tmply9ws8vn.py in tf__call(self, x, training)
      1 from __future__ import print_function
      2 def tf__call(self, x, training):
----> 3   x1 = ag__.converted_call('down1', self, ag__.converter.ConversionOptions(recursive=True, verbose=0, strip_decorators=(function, defun, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), x, training=training)
      4   x2 = ag__.converted_call('down2', self, ag__.converter.ConversionOptions(recursive=True, verbose=0, strip_decorators=(function_1, defun_1, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), x1, training=training)
      5   x3 = ag__.converted_call('down3', self, ag__.converter.ConversionOptions(recursive=True, verbose=0, strip_decorators=(function_2, defun_2, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), x2, training=training)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, owner, options, *args, **kwargs)
    285       experimental_strip_decorators=options.strip_decorators,
    286       experimental_verbose=options.verbose,
--> 287       experimental_partial_types=partial_types)
    288 
    289   result = converted_f(*effective_args, **kwargs)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in to_graph(entity, recursive, arg_values, arg_types, experimental_optional_features, experimental_strip_decorators, experimental_verbose, experimental_partial_types)
    407       uncompiled_modules=config.DEFAULT_UNCOMPILED_MODULES)
    408   _, name, namespace = conversion.entity_to_graph(entity, program_ctx,
--> 409                                                   arg_values, arg_types)
    410 
    411   nodes = []

/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/conversion.py in entity_to_graph(o, program_ctx, arg_values, arg_types)
    137     node, name, ns = function_to_graph(o, program_ctx, arg_values, arg_types)
    138   elif tf_inspect.ismethod(o):
--> 139     node, name, ns = function_to_graph(o, program_ctx, arg_values, arg_types)
    140   # TODO(mdan,yashkatariya): Remove when object conversion is implemented.
    141   elif hasattr(o, '__class__'):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/conversion.py in function_to_graph(f, program_ctx, arg_values, arg_types, owner_type)
    335       owner_type=owner_type)
    336   context = converter.EntityContext(namer, entity_info, program_ctx)
--> 337   node = node_to_graph(node, context)
    338 
    339   if isinstance(node, gast.Lambda):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/conversion.py in node_to_graph(node, context)
    373   # TODO(mdan): Insert list_comprehensions somewhere.
    374 
--> 375   node = converter.standard_analysis(node, context, is_initial=True)
    376   # Past this point, line numbers are no longer accurate so we ignore the
    377   # source.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/core/converter.py in standard_analysis(node, context, is_initial)
    469   # TODO(mdan): Consider not running all analyses every time.
    470   # TODO(mdan): Don't return a node because it's modified by reference.
--> 471   graphs = cfg.build(node)
    472   node = qual_names.resolve(node)
    473   node = activity.resolve(node, context.info, None)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/pyct/cfg.py in build(node)
    819 def build(node):
    820   visitor = AstToCfg()
--> 821   visitor.visit(node)
    822   return visitor.cfgs

/usr/lib/python3.6/ast.py in visit(self, node)
    251         method = 'visit_' + node.__class__.__name__
    252         visitor = getattr(self, method, self.generic_visit)
--> 253         return visitor(node)
    254 
    255     def generic_visit(self, node):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/pyct/cfg.py in visit_FunctionDef(self, node)
    672     self._process_basic_statement(node.args)
    673     for stmt in node.body:
--> 674       self.visit(stmt)
    675 
    676     self.builder.exit_section(node)

/usr/lib/python3.6/ast.py in visit(self, node)
    251         method = 'visit_' + node.__class__.__name__
    252         visitor = getattr(self, method, self.generic_visit)
--> 253         return visitor(node)
    254 
    255     def generic_visit(self, node):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/pyct/cfg.py in visit_With(self, node)
    814       self._process_basic_statement(item)
    815     for stmt in node.body:
--> 816       self.visit(stmt)
    817 
    818 

/usr/lib/python3.6/ast.py in visit(self, node)
    251         method = 'visit_' + node.__class__.__name__
    252         visitor = getattr(self, method, self.generic_visit)
--> 253         return visitor(node)
    254 
    255     def generic_visit(self, node):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/pyct/cfg.py in visit_If(self, node)
    727     self.builder.new_cond_branch(node)
    728     for stmt in node.body:
--> 729       self.visit(stmt)
    730 
    731     self.builder.new_cond_branch(node)

/usr/lib/python3.6/ast.py in visit(self, node)
    251         method = 'visit_' + node.__class__.__name__
    252         visitor = getattr(self, method, self.generic_visit)
--> 253         return visitor(node)
    254 
    255     def generic_visit(self, node):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/pyct/cfg.py in visit_With(self, node)
    814       self._process_basic_statement(item)
    815     for stmt in node.body:
--> 816       self.visit(stmt)
    817 
    818 

/usr/lib/python3.6/ast.py in visit(self, node)
    251         method = 'visit_' + node.__class__.__name__
    252         visitor = getattr(self, method, self.generic_visit)
--> 253         return visitor(node)
    254 
    255     def generic_visit(self, node):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/pyct/cfg.py in visit_If(self, node)
    731     self.builder.new_cond_branch(node)
    732     for stmt in node.orelse:
--> 733       self.visit(stmt)
    734 
    735     self.builder.exit_cond_section(node)

/usr/lib/python3.6/ast.py in visit(self, node)
    251         method = 'visit_' + node.__class__.__name__
    252         visitor = getattr(self, method, self.generic_visit)
--> 253         return visitor(node)
    254 
    255     def generic_visit(self, node):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/pyct/cfg.py in visit_Try(self, node)
    800     if node.handlers:
    801       # TODO(mdan): Should we still support bare try/except? Might be confusing.
--> 802       raise NotImplementedError('exceptions are not yet supported')
    803 
    804     self._exit_lexical_scope(node)

NotImplementedError: exceptions are not yet supported
```


"
26610,TensorFlow 1.13 build fails with MPI support,"**System information:**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7.4
- TensorFlow installed from source
- TensorFlow version: 1.13
- Python version: 3.6.4
- Bazel version: 0.19 (recompiled from https://copr-be.cloud.fedoraproject.org/results/vbatts/bazel/fedora-27-x86_64/00823004-bazel/bazel-0.19.1-1.fc27.src.rpm since Bazel version provided by COPR for RHEL 7.4 is 0.23, that is not compatible with TensorFlow 1.13)
- GCC/Compiler version: GCC 4.8.5 
- with no CUDA support
- Open MPI version: 2.0.4


**Problem description:**
Build fails with MPI support with the following message:

```
ERROR: /tmp/tensorflow/sources/tensorflow/tensorflow/BUILD:573:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1)
[<machine>:30644] mca_base_component_repository_open: unable to open mca_patcher_overwrite: <path-to-openmpi-2.0.4>/lib/openmpi/mca_patcher_overwrite.so: undefined symbol: mca_patcher_base_patch_t_class (ignored)
[<machine>:30644] mca_base_component_repository_open: unable to open mca_shmem_posix: <path-to-openmpi-2.0.4>/lib/openmpi/mca_shmem_posix.so: undefined symbol: opal_show_help (ignored)
[<machine>:30644] mca_base_component_repository_open: unable to open mca_shmem_sysv: <path-to-openmpi-2.0.4>/lib/openmpi/mca_shmem_sysv.so: undefined symbol: opal_show_help (ignored)
[<machine>:30644] mca_base_component_repository_open: unable to open mca_shmem_mmap: <path-to-openmpi-2.0.4>/lib/openmpi/mca_shmem_mmap.so: undefined symbol: opal_show_help (ignored)
--------------------------------------------------------------------------
It looks like opal_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during opal_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_shmem_base_select failed
  --> Returned value -1 instead of OPAL_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_init failed
  --> Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned ""Error"" (-1) instead of ""Success"" (0)
--------------------------------------------------------------------------
*** An error occurred in MPI_Init_thread
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[<machine>:30644] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```
I must say that I am clueless since I cannot understand which script/program is launched that triggers this error. If I understand well, it crashes while applying `tensorflow:tf_python_api_gen_v1` rule that comes from `tensorflow/BUILD`, but it is unclear to me where this rule is actually applied.

**Sequence of commands:**
```
./configure
/usr/local/bin/bazel --output_user_root=/tmp/bazel/build build --config=opt  //tensorflow/tools/pip_package:build_pip_package --distdir=builddeps
```

**Additional infos:**
- My Open MPI installation is working flawlessly appart from this issue.
- Note that I am building TensorFlow behind a proxy and was therefore forced to download all the required dependencies in a `distdir` folder.
- I first tried to build TensorFlow from master, but had first another issue (`InitPartial` signature change was not reported in `tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc` and I had to patch this file) but ended up with a similar build fail.
- For the sake of completeness, I also gave a try with GNU 8.2.0 -- even though I doubted it had anything to do with the problem at stake."
26608,Subsequent gather_nd Operation leads to error: Tensor.graph is meaningless when eager execution is enabled.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-alpha0
- Python version: python 3.5
- CUDA/cuDNN version: 10.
- GPU model and memory: TITAN

**Describe the current behavior**
Gradient computation fails with the message: ""Tensor.graph is meaningless when eager execution is enabled.""
 
The gradient becomes an IndexedSlices through the second gather_nd operation. On the IndexedSlices the check in https://github.com/tensorflow/tensorflow/blob/8cb1aa18882be9142a5e4d3977de0a076ce2c791/tensorflow/python/framework/ops.py#L5956-L5957 evaluates to True and in line 5965 the .graph attribute is called which in EagerExecution causes the error mentioned above.
https://github.com/tensorflow/tensorflow/blob/8cb1aa18882be9142a5e4d3977de0a076ce2c791/tensorflow/python/framework/ops.py#L5965

A workaround/fix for the problem can be achieved by adding another check to the if clause making sure that op_input is not an IndexedSlices: `and not isinstance(op_input, IndexedSlices)`

```python
    if (isinstance(op_input, (Operation, _TensorLike)) and
       ((not isinstance(op_input, Tensor)) or type(op_input) == Tensor) and
       (not isinstance(op_input, IndexedSlices))):  # pylint: disable=unidiomatic-typecheck
```

**Describe the expected behavior**

Gradients to be computed correctly

**Code to reproduce the issue**


```python
import tensorflow as tf


class DebugModel(tf.keras.Model):
  def __init__(self):
    super(DebugModel, self).__init__()
    self.dense = tf.keras.layers.Dense(20, activation='relu', input_shape=(5,))

  def __call__(self, input, *args, **kwargs):
    x = self.dense(input)
    ind = [[0], [1], [2]]
    x = tf.gather_nd(x, ind)
    ind = [[0], [1]]
    x = tf.gather_nd(x, ind)
    return x

my_model = DebugModel()

my_optim = tf.keras.optimizers.Adam(1e-3)
with tf.GradientTape() as tape:
  out = my_model(tf.random.uniform((16,5)))
  my_loss = -tf.reduce_mean(out)
  grads = tape.gradient(my_loss, my_model.trainable_weights)
  my_optim.apply_gradients(zip(grads, my_model.trainable_weights))
```

"
26607,How to run tflite with float input,"Hi Sir,
  I am trying to run tflite on mobile to classify sound event.
  The input of the model is mfcc features from the sound stream, which's type are float32.
  I have converted the pb model to tflite model, but I cannot feed the input data whith float32.

It will be throw exception as follow whether I setting the post_training_quantize to True or not
 **cannot convert between a tensorflowlite tensor with type float32 and a java object of type**
"
26606,AttributeError: module 'tensorflow.python.ops.image_ops' has no attribute 'is_jpeg',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Window 10 Pro 64 bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.6
- Python version: 3.6.7
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:9.0
- GPU model and memory: Nvidia Quadro P4000 - 8GB



While running the train.py - it fails with the attribute error



**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
PS D:\Nanda\DeepLabs\models-master\research\deeplab> python train.py --logtostderr --train_split=""train"" --model_variant=""xcep
tion_65"" --atrous_rates=6 --atrous_rates=12 --atrous_rates=18 --output_stride=16 --decoder_output_stride=4 --train_crop_size=5
13 --train_crop_size=513 --train_batch_size=4 --training_number_of_steps=100 --fine_tune_batch_norm=true --tf_initial_checkpoi
nt=""D:/Nanda/DeepLabs/models-master/research/deeplab/datasets/Datathon/exp/train_on_trainval_set/init_models/deeplabv3_pascal_
train_aug/model.ckpt"" --train_logdir=""D:/Nanda/DeepLabs/models-master/research/deeplab/datasets/Datathon/exp/train_on_trainval
_set/train/"" --dataset_dir=""D:/Nanda/DeepLabs/models-master/research/deeplab/datasets/Datathon/tfrecord""
INFO:tensorflow:Training on train set
Traceback (most recent call last):
  File ""train.py"", line 499, in <module>
    tf.app.run()
  File ""C:\Users\nvenugop\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\platform\app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""train.py"", line 451, in main
    dataset.get_one_shot_iterator(), dataset.num_of_classes,
  File ""D:\Nanda\DeepLabs\models-master\research\deeplab\datasets\data_generator.py"", line 335, in get_one_shot_iterator
    .map(self._parse_function)
  File ""C:\Users\nvenugop\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 790, in map
    return MapDataset(self, map_func)
  File ""C:\Users\nvenugop\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 1597, in __init__
    self._map_func.add_to_graph(ops.get_default_graph())
  File ""C:\Users\nvenugop\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\framework\function.py"", line 486, in add_to_graph
    self._create_definition_if_needed()
  File ""C:\Users\nvenugop\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\framework\function.py"", line 321, in _create_definition_if_needed
    self._create_definition_if_needed_impl()
  File ""C:\Users\nvenugop\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\framework\function.py"", line 338, in _create_definition_if_needed_impl
    outputs = self._func(*inputs)
  File ""C:\Users\nvenugop\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 1562, in tf_map_func
    ret = map_func(nested_args)
  File ""D:\Nanda\DeepLabs\models-master\research\deeplab\datasets\data_generator.py"", line 246, in _parse_function
    image = _decode_image(parsed_features['image/encoded'], channels=3)
  File ""D:\Nanda\DeepLabs\models-master\research\deeplab\datasets\data_generator.py"", line 223, in _decode_image
    tf.image.is_jpeg(content),
AttributeError: module 'tensorflow.python.ops.image_ops' has no attribute 'is_jpeg'"
26605,how to build armeabi-v7a tensorflow-lite.a with ndk?,
26602,Partial function specified through keyword on first position in tf.function,"Wrapping in tf.function a partial with first argument specified:
```
def f(x, y):
  return x + y

partial_func = functools.partial(f, x=5)
tf_func = tf.function(partial_func)

print(tf_func(5))
```

This does not work in Python2.x, because [tf_inspect.getfullargspec cannot represent such construct](https://github.com/tensorflow/tensorflow/blob/c46d383150564c8b72b05acc65182c16f7221694/tensorflow/python/util/tf_inspect.py#L175) using Argspec.

Unfortunately this also does not work in Python3, where Argspecs are already capable of representing this:
```
TypeError: tf__f() got multiple values for argument 'x'
```"
26601,Can I attach a c++ object to a tensor so that the object can be forwarded through the network and share the same lifetime with the tensor?,"I'm trying to develop a complex network in which, some custom c++ ops output c++ objects that are to be received by other custom ops.  One solution is by serializing the c++ object into a byte tensor, but this way adds (de)serialization overhead. Another solution is only outputting the address of the object and storing the actual object as a data member of the c++ op instance, but this way only works in graph mode. Is there a better solution for this problem? Thank you."
26600,"InvalidArgumentError: No OpKernel was registered to support Op 'CudnnRNN' used by {{node cu_dnnlstm/CudnnRNN}}with these attrs: [seed=0, dropout=0, T=DT_FLOAT, input_mode=""linear_input"", direction=""unidirectional"", rnn_mode=""lstm"", is_training=true, seed2=0]","
I am user macintosh  


code 


```
`import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, LSTM #, CuDNNLSTM


mnist = tf.keras.datasets.mnist  # mnist is a dataset of 28x28 images of handwritten digits and their labels
(x_train, y_train),(x_test, y_test) = mnist.load_data()  # unpacks images to x_train/x_test and labels to y_train/y_test

x_train = x_train/255.0
x_test = x_test/255.0

print(x_train.shape)
print(x_train[0].shape)

model = Sequential()


model.add(LSTM(128, input_shape=(x_train.shape[1:]), activation='relu', return_sequences=True))
model.add(Dropout(0.2))

model.add(LSTM(128, activation='relu'))
model.add(Dropout(0.1))

model.add(Dense(32, activation='relu'))
model.add(Dropout(0.2))

model.add(Dense(10, activation='softmax'))

opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)

# Compile model
model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer=opt,
    metrics=['accuracy'],
)

model.fit(x_train,
          y_train,
          epochs=3,
          validation_data=(x_test, y_test))`
```

output 

```
`(60000, 28, 28)
(28, 28)
Train on 60000 samples, validate on 10000 samples
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1333     try:
-> 1334       return fn(*args)
   1335     except errors.OpError as e:

/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)
   1316       # Ensure any changes to the graph are reflected in the runtime.
-> 1317       self._extend_graph()
   1318       return self._call_tf_sessionrun(

/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py in _extend_graph(self)
   1351     with self._graph._session_run_lock():  # pylint: disable=protected-access
-> 1352       tf_session.ExtendSession(self._session)
   1353 

InvalidArgumentError: No OpKernel was registered to support Op 'CudnnRNN' used by {{node cu_dnnlstm/CudnnRNN}}with these attrs: [seed=0, dropout=0, T=DT_FLOAT, input_mode=""linear_input"", direction=""unidirectional"", rnn_mode=""lstm"", is_training=true, seed2=0]
Registered devices: [CPU]
Registered kernels:
  <no registered kernels>

	 [[{{node cu_dnnlstm/CudnnRNN}}]]

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-7-d1c9ab123f67> in <module>
     41           y_train,
     42           epochs=3,
---> 43           validation_data=(x_test, y_test))

/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)
    878           initial_epoch=initial_epoch,
    879           steps_per_epoch=steps_per_epoch,
--> 880           validation_steps=validation_steps)
    881 
    882   def evaluate(self,

/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)
    249     # Setup work for each epoch
    250     epoch_logs = {}
--> 251     model.reset_metrics()
    252     callbacks.on_epoch_begin(epoch, epoch_logs, mode=mode)
    253     progbar.on_epoch_begin(epoch, epoch_logs)

/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in reset_metrics(self)
   1117     if hasattr(self, 'metrics'):
   1118       for m in self.metrics:
-> 1119         m.reset_states()
   1120       if self._distribution_strategy:
   1121         training_distributed._reset_metrics(self)  # pylint: disable=protected-access

/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/metrics.py in reset_states(self)
    458     """"""
    459     for v in self.variables:
--> 460       K.set_value(v, 0)
    461 
    462   @abc.abstractmethod

/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py in set_value(x, value)
   2845         x._assign_placeholder = assign_placeholder
   2846         x._assign_op = assign_op
-> 2847       get_session().run(assign_op, feed_dict={assign_placeholder: value})
   2848 
   2849 

/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py in get_session()
    480   if not _MANUAL_VAR_INIT:
    481     with session.graph.as_default():
--> 482       _initialize_variables(session)
    483   return session
    484 

/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py in _initialize_variables(session)
    756     # marked as initialized.
    757     is_initialized = session.run(
--> 758         [variables_module.is_variable_initialized(v) for v in candidate_vars])
    759     uninitialized_vars = []
    760     for flag, v in zip(is_initialized, candidate_vars):

/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    927     try:
    928       result = self._run(None, fetches, feed_dict, options_ptr,
--> 929                          run_metadata_ptr)
    930       if run_metadata:
    931         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1150     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1151       results = self._do_run(handle, final_targets, final_fetches,
-> 1152                              feed_dict_tensor, options, run_metadata)
   1153     else:
   1154       results = []

/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1326     if handle is None:
   1327       return self._do_call(_run_fn, feeds, fetches, targets, options,
-> 1328                            run_metadata)
   1329     else:
   1330       return self._do_call(_prun_fn, handle, feeds, fetches)

/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1346           pass
   1347       message = error_interpolation.interpolate(message, self._graph)
-> 1348       raise type(e)(node_def, op, message)
   1349 
   1350   def _extend_graph(self):

InvalidArgumentError: No OpKernel was registered to support Op 'CudnnRNN' used by node cu_dnnlstm/CudnnRNN (defined at <ipython-input-6-3c5692df850b>:19) with these attrs: [seed=0, dropout=0, T=DT_FLOAT, input_mode=""linear_input"", direction=""unidirectional"", rnn_mode=""lstm"", is_training=true, seed2=0]
Registered devices: [CPU]
Registered kernels:
  <no registered kernels>

	 [[node cu_dnnlstm/CudnnRNN (defined at <ipython-input-6-3c5692df850b>:19) ]]

Caused by op 'cu_dnnlstm/CudnnRNN', defined at:
  File ""/anaconda3/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/anaconda3/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py"", line 16, in <module>
    app.launch_new_instance()
  File ""/anaconda3/lib/python3.7/site-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/anaconda3/lib/python3.7/site-packages/ipykernel/kernelapp.py"", line 505, in start
    self.io_loop.start()
  File ""/anaconda3/lib/python3.7/site-packages/tornado/platform/asyncio.py"", line 132, in start
    self.asyncio_loop.run_forever()
  File ""/anaconda3/lib/python3.7/asyncio/base_events.py"", line 528, in run_forever
    self._run_once()
  File ""/anaconda3/lib/python3.7/asyncio/base_events.py"", line 1764, in _run_once
    handle._run()
  File ""/anaconda3/lib/python3.7/asyncio/events.py"", line 88, in _run
    self._context.run(self._callback, *self._args)
  File ""/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py"", line 758, in _run_callback
    ret = callback()
  File ""/anaconda3/lib/python3.7/site-packages/tornado/stack_context.py"", line 300, in null_wrapper
    return fn(*args, **kwargs)
  File ""/anaconda3/lib/python3.7/site-packages/tornado/gen.py"", line 1233, in inner
    self.run()
  File ""/anaconda3/lib/python3.7/site-packages/tornado/gen.py"", line 1147, in run
    yielded = self.gen.send(value)
  File ""/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py"", line 357, in process_one
    yield gen.maybe_future(dispatch(*args))
  File ""/anaconda3/lib/python3.7/site-packages/tornado/gen.py"", line 326, in wrapper
    yielded = next(result)
  File ""/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py"", line 267, in dispatch_shell
    yield gen.maybe_future(handler(stream, idents, msg))
  File ""/anaconda3/lib/python3.7/site-packages/tornado/gen.py"", line 326, in wrapper
    yielded = next(result)
  File ""/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py"", line 534, in execute_request
    user_expressions, allow_stdin,
  File ""/anaconda3/lib/python3.7/site-packages/tornado/gen.py"", line 326, in wrapper
    yielded = next(result)
  File ""/anaconda3/lib/python3.7/site-packages/ipykernel/ipkernel.py"", line 294, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/anaconda3/lib/python3.7/site-packages/ipykernel/zmqshell.py"", line 536, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py"", line 2819, in run_cell
    raw_cell, store_history, silent, shell_futures)
  File ""/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py"", line 2845, in _run_cell
    return runner(coro)
  File ""/anaconda3/lib/python3.7/site-packages/IPython/core/async_helpers.py"", line 67, in _pseudo_sync_runner
    coro.send(None)
  File ""/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py"", line 3020, in run_cell_async
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py"", line 3185, in run_ast_nodes
    if (yield from self.run_code(code, result)):
  File ""/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py"", line 3267, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-6-3c5692df850b>"", line 19, in <module>
    model.add(CuDNNLSTM(128, input_shape=(x_train.shape[1:]), return_sequences=True))
  File ""/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/checkpointable/base.py"", line 442, in _method_wrapper
    method(self, *args, **kwargs)
  File ""/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py"", line 164, in add
    layer(x)
  File ""/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 701, in __call__
    return super(RNN, self).__call__(inputs, **kwargs)
  File ""/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 554, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/cudnn_recurrent.py"", line 111, in call
    output, states = self._process_batch(inputs, initial_state)
  File ""/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/cudnn_recurrent.py"", line 501, in _process_batch
    is_training=True)
  File ""/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_cudnn_rnn_ops.py"", line 142, in cudnn_rnn
    seed2=seed2, is_training=is_training, name=name)
  File ""/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 3300, in create_op
    op_def=op_def)
  File ""/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 1801, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'CudnnRNN' used by node cu_dnnlstm/CudnnRNN (defined at <ipython-input-6-3c5692df850b>:19) with these attrs: [seed=0, dropout=0, T=DT_FLOAT, input_mode=""linear_input"", direction=""unidirectional"", rnn_mode=""lstm"", is_training=true, seed2=0]
Registered devices: [CPU]
Registered kernels:
  <no registered kernels>

	 [[node cu_dnnlstm/CudnnRNN (defined at <ipython-input-6-3c5692df850b>:19) ]]`
```"
26599,from tensorflow.contrib import layers -> Cannot import name autograph,"'from tensorflow.contrib import layers'

This simple line ends up with the following error : ImportError: cannot import name autograph

Why ?
"
26598,"Getting error - ""tf.gfile.Stat()"" gets result that ""file does not exist"", but it does exists","There are files in ""neg"" directory:
In [81]: tf.gfile.ListDirectory(""s3://lookalike-sample/novel_user/ds=20190310/neg"")
Out[81]:
['_SUCCESS',
 'part-00002.gz',
 'part-00003.gz',
 'part-00004.gz']

But when I input:
tf.gfile.Stat(""s3://lookalike-sample/novel_user/ds=20190310/neg/part-00002.gz"")

getting error:
NotFoundError: Object s3://lookalike-sample/novel_user/ds=20190310/neg/part-00002.gz does not exist

- OS Platform and Distribution ( Linux Ubuntu 16.04):
- TensorFlow version 1.8.0
- Python version:2.7.4


"
26597,Writing to S3 fails occasionally,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
ubuntu 16.04 (docker)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
pip installed into docker image
- TensorFlow version (use command below):
1.12.0-gpu
- Python version:
3.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
Occasionally when I write to an S3 file like so:
```python
def s3_print(*args,**kwargs):
    with tf.gfile.GFile('s3://bucket/log.txt','a') as f:
        print(*args,**kwargs, file=f)
```
I get errors like this
```
Traceback (most recent call last):
  File ""/root/train.py"", line 602, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/root/train.py"", line 289, in main
    s3_print('Distinct elements: {}'.format(elements))
  File ""/root/train.py"", line 113, in tee_print
    print(*args,**kwargs, file=f)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/lib/io/file_io.py"", line 208, in __exit__
    self.close()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/lib/io/file_io.py"", line 240, in close
    pywrap_tensorflow.Set_TF_Status_from_Status(status, ret_status)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py"", line 528, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.UnknownError: XAmzContentSHA256Mismatch: Unable to parse ExceptionName: XAmzContentSHA256Mismatch Message:
```

**Describe the expected behavior**

**Other info / logs**
Googling the keyword `XAmzContentSHA256Mismatch` led me to a bunch of s3 related issues, so I'm guessing that's related.

I'm not sure how to work around this problem so any guidance would be appreciated."
26596,Getting error - Expects arg[0] to be float but uint8 is provided while using bytebuffer for image data in tensorflow inference.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, but have only modified the model name in tensorflow/examples/android and tried passing bytebuffer of bytes for the imagedata.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Yes, android phones - one plus 6
- TensorFlow installed from (source or binary): github/tensorflow
- TensorFlow version (use command below):1.13.1
- Python version: 3.6
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
I have modified the tensorflow/examples/android/demo to use my custom tf model(created from mobilenet_v1 trained for posenet tfjs). Instead of float values, am passing the image as a byte buffer using the below code:-

private String[] outputNames;
ByteBuffer imgData = null;

        imgData =
                ByteBuffer.allocateDirect(
                          DIM_BATCH_SIZE  //1
                * getImageSizeX()    //513
                * getImageSizeY()    //513
                * DIM_PIXEL_SIZE   //3
                * getNumBytesPerChannel());    //4
        imgData.order(ByteOrder.nativeOrder());

        int pixel = 0;

        for (int i = 0; i < 513; ++i) {
            for (int j = 0; j < 513; ++j) {
                final int val = intValues[pixel++];
                imgData.putFloat( ((val >> 16) & 0xFF));
                imgData.putFloat( ((val >> 8) & 0xFF));
                imgData.putFloat( (val & 0xFF));
            }
        }

Image size is 513 * 513.

1. The interfaceinference.feed is running without any issues.

inferenceInterface.feed(inputName, imgData, 1, inputSize, inputSize, 3, 4);

2. It fails when I try to run interfaceinference.run.

inferenceInterface.run(outputNames, logStats);

Here outputNames is a String[]. Although in the tensorflow interface.java code, it seems to handle the feed for Byte Buffers, the tensor created is a byte buffer - uint8. Then how is the run failing?

**Describe the expected behavior**

The inferenceInterface.run should be able to handle bytebuffer inputs as it is being handled in feed. 

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

Code is provided above. Can reuse the existing model as in tensorflow/examples/android/demo and modify the TensorflowImageClassifier.java to pass the input image as a byteBuffer.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

    
    --------- beginning of crash
2019-03-12 11:46:54.161 3453-3469/org.tensorflow.demo E/AndroidRuntime: FATAL EXCEPTION: inference
    Process: org.tensorflow.demo, PID: 3453
    java.lang.IllegalArgumentException: Expects arg[0] to be float but uint8 is provided
        at org.tensorflow.Session.run(Native Method)
        at org.tensorflow.Session.access$100(Session.java:48)
        at org.tensorflow.Session$Runner.runHelper(Session.java:314)
        at org.tensorflow.Session$Runner.run(Session.java:264)
        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:228)
        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:197)
"
26595,Issues with running compiled versions of tensorflow for different machines and CFLAGS,"This is a bug  related to https://bugs.gentoo.org/show_bug.cgi?id=679714.

Here is the system information:
**### System information**
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**2019-03-12 05:23:07.606918: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX2 instructions, but these aren't available on your machine.**

Actual system is an IvyBridge non AVX2 CPU. GCC version 8.3.0. No LTO. Simple use flags. 

How did this happen? I compiled tensor flow on my build machine. The build machine has loads of RAM big fans and is suited for compling stuff. I wanted to run tensorflow on my notebook, which was a bit older and more limited. I tried building tensor flow from the gentoo ebuild directly, with specific CPU flags for the CPU that I was going to run tensorflow on. I then ran into the gentoo bug 679714. This is where the libtensorflow_framework.so is missing the symbol __cpu_model (the flags I used on the build machine were **-O2 -march=ivybridge -mcx16 -msahf -mno-movbe -maes -mpclmul -mpopcnt -mno-abm -mno-lwp -mno-fma -mno-fma4 -mno-xop -mno-bmi -mno-bmi2 -mno-tbm -mavx -mno-avx2 -msse4.1 -msse4.2 -mno-lzcnt -mno-rtm -mno-hle -mrdrnd -mf16c -mfsgsbase -mno-rdseed -mno-prfchw -mno-adx -mfxsr -mxsave -mxsaveopt -mtune=ivybridge -minline-all-stringops -finline-functions -fmerge-all-constants**). I changed the CFLAGS to ""-### O2"" only and tensorflow ran fine on the build machine. I tried to run a python script (python versions 3.6 and 2.7) found was told that the CPU didn't support AVX2.



"
26594,How to input a list of string values to Estimator,"# I want to use tfhub'emlo to build a text classifier, but the elmo need a string values placeholder. in order to input feature to estimator, I need to use the tf.feature_column building the input feature columns.But how can i input a list of string values to estimator?
"
26593,Cannot get tensorflow to work. DLL load failed and DLL initialization routine failed,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows OS 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.12.0
- Python version: 3.6
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

I cant seem to import tensorflow even though I have supposedly installed CUDA and followed an online tutorial. 
This is my error when I type import tensorflow:

import tensorflow Traceback (most recent call last): File ""C:\Users\User\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in from tensorflow.python.pywrap_tensorflow_internal import * File ""C:\Users\User\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in _pywrap_tensorflow_internal = swig_import_helper() File ""C:\Users\User\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description) File ""C:\Users\User\AppData\Local\conda\conda\envs\tensorflow\lib\imp.py"", line 243, in load_module return load_dynamic(name, filename, file) File ""C:\Users\User\AppData\Local\conda\conda\envs\tensorflow\lib\imp.py"", line 343, in load_dynamic return _load(spec)

ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last): File """", line 1, in File ""C:\Users\User\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow__init__.py"", line 24, in from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import File ""C:\Users\User\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python__init__.py"", line 49, in from tensorflow.python import pywrap_tensorflow File ""C:\Users\User\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in raise ImportError(msg) ImportError: Traceback (most recent call last): File ""C:\Users\User\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in from tensorflow.python.pywrap_tensorflow_internal import * File ""C:\Users\User\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in _pywrap_tensorflow_internal = swig_import_helper() File ""C:\Users\User\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description) File ""C:\Users\User\AppData\Local\conda\conda\envs\tensorflow\lib\imp.py"", line 243, in load_module return load_dynamic(name, filename, file) File ""C:\Users\User\AppData\Local\conda\conda\envs\tensorflow\lib\imp.py"", line 343, in load_dynamic return _load(spec)

ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime.

This is what I get when I run tensorflow_self_check.py: ERROR: Failed to import the TensorFlow module.

Python version is 3.6.
TensorFlow is installed at: C:\Users\User\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow
Could not load 'cudart64_80.dll'. The GPU version of TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 8.0 from this URL: https://developer.nvidia.com/cuda-toolkit
Could not load 'nvcuda.dll'. The GPU version of TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Typically it is installed in 'C:\Windows\System32'. If it is not present, ensure that you have a CUDA-capable GPU with the correct driver installed.
"
26591,Cannot create a SavedModel with a DenseFeatures layer,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
MacOSX 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
VERSION='2.0.0-dev20190311'
GIT_VERSION='v1.12.0-9917-gf988edacf4'
- Python version:
3.6.8
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
Cannot create a `SavedModel` with a `DenseFeatures` layer, I get an exception: `AttributeError: 'str' object has no attribute 'shape'` (see full stacktrace below).

**Describe the expected behavior**
I except to be able to save any tf.keras model (except those with dynamic layers) that uses only standard layers.

**Code to reproduce the issue**

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.train import Example, Features, Feature, FloatList
import numpy as np

# Create a simple TFRecord file
ages = np.random.rand(100)*50 + 20
heights = np.random.rand(100)*40 + 150
weights = 0.75 * heights - 75. + ages * 0.1 + np.random.rand(100) * 20
with tf.io.TFRecordWriter(""weights.tfrecord"") as f:
    for ex_age, ex_height, ex_weight in zip(ages, heights, weights):
        example = Example(features=Features(feature={
            ""age"": Feature(float_list=FloatList(value=[ex_age])),
            ""height"": Feature(float_list=FloatList(value=[ex_height])),
            ""weight"": Feature(float_list=FloatList(value=[ex_weight]))
        }))
        f.write(example.SerializeToString())

# Create a TFRecordDataset to read the data
age = tf.feature_column.numeric_column(""age"")
height = tf.feature_column.numeric_column(""height"")
weight = tf.feature_column.numeric_column(""weight"")
columns = [age, height, weight]
feature_descriptions = tf.feature_column.make_parse_example_spec(columns)
def parse_examples(serialized_examples):
    features = tf.io.parse_example(serialized_examples,
                                   feature_descriptions)
    targets = features.pop(""weight"")
    return features, targets
dataset = tf.data.TFRecordDataset([""weights.tfrecord""])
dataset = dataset.shuffle(100).batch(32).map(parse_examples)

# Create, train and use the model with a DenseFeatures layer
model = keras.models.Sequential([
    keras.layers.DenseFeatures(columns[:-1]),
    keras.layers.Dense(1)
])
model.compile(loss=""mse"", optimizer=keras.optimizers.SGD(lr=1e-5))
history = model.fit(dataset, epochs=5)
y_pred = model.predict({""age"": tf.constant([25.]),
                        ""height"": tf.constant([180.])})

# Saving using the save() method works fine
model.save(""my_weight_model.h5"")

# Saving to a SavedModel fails
tf.saved_model.save(model, ""my_weight_model.savedmodel"") # AttributeError!
```

**Other info / logs**
Here is the stacktrace:

```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-116-5ddc1d5acd67> in <module>
     39 
     40 model.save(""my_weight_model.h5"")
---> 41 tf.saved_model.save(model, ""my_weight_model.savedmodel"")

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py in save(obj, export_dir, signatures)
    773   if signatures is None:
    774     signatures = signature_serialization.find_function_to_export(
--> 775         checkpoint_graph_view)
    776 
    777   signatures = signature_serialization.canonicalize_signatures(signatures)

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/saved_model/signature_serialization.py in find_function_to_export(saveable_view)
     63   # If the user did not specify signatures, check the root object for a function
     64   # that can be made into a signature.
---> 65   functions = saveable_view.list_functions(saveable_view.root)
     66   signature = functions.get(DEFAULT_SIGNATURE_ATTR, None)
     67   if signature is not None:

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py in list_functions(self, obj)
    109     obj_functions = self._functions.get(obj, None)
    110     if obj_functions is None:
--> 111       obj_functions = obj._list_functions_for_serialization()  # pylint: disable=protected-access
    112       self._functions[obj] = obj_functions
    113     return obj_functions

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in _list_functions_for_serialization(self)
   1793   def _list_functions_for_serialization(self):
   1794     return {
-> 1795         '_default_save_signature': saving_utils.trace_model_call(self)
   1796     }
   1797 

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/saving/saving_utils.py in trace_model_call(model, input_signature)
     76     for input_tensor, input_name in zip(inputs, input_names):
     77       input_specs.append(tensor_spec.TensorSpec(
---> 78           shape=input_tensor.shape, dtype=input_tensor.dtype,
     79           name=input_name))
     80     # The input signature of the call function is a list with one element, since

AttributeError: 'str' object has no attribute 'shape'
```"
26590,[tf.keras.layers.LSTM] Initializer fails with input_length parameter,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
```
4.15.0-46-generic #49~16.04.1-Ubuntu SMP Tue Feb 12 17:45:24 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux
```
- TensorFlow installed from (source or binary): `conda` as binary.
- TensorFlow version (use command below): `1.10`, `1.12`, and `1.13` confirmed
- Python version: `Python 3.6.6 |Anaconda, Inc.| (default, Oct  9 2018, 12:34:16)`

(The following are irrelevant since I'm not even running with a session or in eager mode)
- CUDA/cuDNN version: CUDA 9.0
- GPU model and memory: `GeForce GTX TITAN X`

**MWE**
```
import tensorflow as tf
lstm = tf.keras.layers.LSTM(512, input_length=32)
```
**Current behavior**
Here's `python` error message:
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""~/.local/anaconda/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 2230, in __init__
    **kwargs)
  File ""~/.local/anaconda/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 462, in __init__
    super(RNN, self).__init__(**kwargs)
  File ""~/.local/anaconda/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/checkpointable/base.py"", line 474, in _method_wrapper
    method(self, *args, **kwargs)
  File ""~/.local/anaconda/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 138, in __init__
    raise TypeError('Keyword argument not understood:', kwarg)
TypeError: ('Keyword argument not understood:', 'input_length')
```

**Expected behavior**
`LSTM` class inherits from `RNN`, which has `input_length` as a parameter as described [here](https://www.tensorflow.org/versions/r1.13/api_docs/python/tf/keras/layers/RNN). Therefore the constructor of `RNN` should use this parameter but it does not as you can see [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/recurrent.py#L391-L399) in the code."
26589,Undefined python symbols building TensorFlow with Python 3.7 on Mac,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Source
- TensorFlow version: Git: df3a3375941b9e920667acfe72fb4c33a8f45503
- Python version: 3.7.2
- Installed using virtualenv? pip? conda?: Source
- Bazel version (if compiling from source): 0.23.1
- GCC/Compiler version (if compiling from source): Apple LLVM version 10.0.0 (clang-1000.11.45.5)
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the problem**

Building TensorFlow produces errors about missing Python symbols:

```
% bazel build --color=yes --config=opt --cxxopt=-std=c++1z --cxxopt=-stdlib=libc++ //tensorflow/tools/pip_package:build_pip_package
...
ERROR: /private/var/tmp/_bazel_irving/d4bad52af7e8ccc6c91f211db4181990/external/protobuf_archive/BUILD:626:1: Linking of rule '@protobuf_archive//:python/google/protobuf/internal/_api_implementation.so' failed (Exit 1)
Undefined symbols for architecture x86_64:
  ""_PyModule_AddIntConstant"", referenced from:
      _PyInit__api_implementation in api_implementation.o
  ""_PyModule_Create2"", referenced from:
      _PyInit__api_implementation in api_implementation.o
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 143.602s, Critical Path: 33.56s
INFO: 427 processes: 427 local.
FAILED: Build did NOT complete successfully
```

I don't seem to be the only person with this problem: https://stackoverflow.com/questions/55089427/installing-tensorflow-on-osx-clang-error-linker-command-failed-with-exit-cod

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
./configure
bazel build --color=yes --config=opt --cxxopt=-std=c++1z --cxxopt=-stdlib=libc++ //tensorflow/tools/pip_package:build_pip_package
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26588,module 'tensorflow' has no attribute 'zeroes' in Graph guide,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: 1.12.0
- Doc Link: https://www.tensorflow.org/guide/graphs


**Describe the documentation issue**

On the documentation an example was given using tf.zeroes().

However, the module 'tensorflow' has no attribute 'zeroes'. 

```
with tf.device(""/job:ps/task:0""):
  weights_1 = tf.Variable(tf.truncated_normal([784, 100]))
  biases_1 = tf.Variable(tf.zeroes([100]))

with tf.device(""/job:ps/task:1""):
  weights_2 = tf.Variable(tf.truncated_normal([100, 10]))
  biases_2 = tf.Variable(tf.zeroes([10]))

with tf.device(""/job:worker""):
  layer_1 = tf.matmul(train_batch, weights_1) + biases_1
  layer_2 = tf.matmul(train_batch, weights_2) + biases_2
```
The documentation should use tf.zeros() instead

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
yes"
26587,Tensorflow 2.0a tf.function variable batch size,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX Mojave
- TensorFlow installed from (source or binary): pip installed 
- TensorFlow version (use command below): 2.0a
- Python version: 3.7.2

Hi,

I'm experimenting with irregular batch size and I ran accross this strange performance behavior.
Consider the following code:

```python
import numpy as np
import tensorflow as tf

network = tf.keras.Sequential([
    tf.keras.layers.LSTM(32),
    tf.keras.layers.Dense(1)
])

def f(x):
    return tf.squeeze(network(x))


tf_f = tf.function(f)

def g(use_tf=False, variable_batch_size=False):
    np.random.seed(0)
    
    l = lambda: 90
    if variable_batch_size:
        l = lambda: np.random.randint(80, 101)
        
    fun = tf_f if use_tf else f
    for _ in range(1000):
        x = np.random.randn(l(), 3, 2).astype('float32')
        fun(x)
```

Not using the wrapped tf_function performs equally fine whether variable_batch_size is on/off:

```python
%time g()  # 3.92s
%time g(variable_batch_size=True)  # 3.85s
```

However for the tf_function version I get (after warm up)

```python
%time g(use_tf=True)   # 451ms
%time g(use_tf=True, variable_batch_size=True)  # 4.34s !!!
```

I'm a bit surprise to see arrayprint at the top of the list when looking at the profiler

```python
%prun -s cumulative g(use_tf=True, variable_batch_size=True)

       13112280 function calls (12256090 primitive calls) in 6.265 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    6.265    6.265 {built-in method builtins.exec}
        1    0.000    0.000    6.265    6.265 <string>:1(<module>)
        1    0.006    0.006    6.265    6.265 <ipython-input-34-9a908934c266>:1(g)
     1000    0.005    0.000    6.227    0.006 def_function.py:407(__call__)
     1000    0.002    0.000    6.222    0.006 function.py:1285(__call__)
     1000    0.018    0.000    5.650    0.006 function.py:1526(_maybe_define_function)
      950    0.006    0.000    5.512    0.006 ops.py:829(__repr__)
      950    0.004    0.000    5.482    0.006 ops.py:215(numpy_text)
      950    0.003    0.000    5.464    0.006 {built-in method builtins.repr}
      950    0.008    0.000    5.462    0.006 arrayprint.py:1397(_array_repr_implementation)
      950    0.004    0.000    5.433    0.006 arrayprint.py:518(array2string)
      950    0.003    0.000    5.422    0.006 arrayprint.py:463(wrapper)
      950    0.007    0.000    5.418    0.006 arrayprint.py:480(_array2string)
      950    0.002    0.000    3.703    0.004 arrayprint.py:707(_formatArray)
857140/950    1.477    0.000    3.701    0.004 arrayprint.py:716(recurser)
      950    0.004    0.000    1.707    0.002 arrayprint.py:411(_get_format_function)
      950    0.002    0.000    1.699    0.002 arrayprint.py:367(<lambda>)
      950    0.020    0.000    1.697    0.002 arrayprint.py:834(__init__)
...
```"
26586,tensorflow lite: error while converting frozen model to lite format,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10 pro
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary):  Using Anaconda navigator
- TensorFlow version (use command below): v1.12.0-9901-gf380d8b8e5 1.14.1-dev20190309
- Python version:  Python 3.6.8
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: using cpu version
- GPU model and memory: na


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
I am trying to quantize a model that is made using tfslim library. I froze it using graph_util.convert_variables_to_constants function and tried to convert it to tflite formate for quantization. I am using win 10 and tf-nighlty. the following error appears:
TOCO failed. See console for info.
2019-03-12 00:20:27.142300: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 221 operators, 366 arrays (0 quantized)
2019-03-12 00:20:27.147170: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 221 operators, 366 arrays (0 quantized)
2019-03-12 00:20:27.243872: F tensorflow/lite/toco/graph_transformations/propagate_fixed_sizes.cc:722] Check failed: start_array.data_type == ArrayDataType::kInt32 Range op inputs must be int32.
Fatal Python error: Aborted

Current thread 0x00001f44 (most recent call first):
  File ""c:\anaconda3\lib\site-packages\tensorflow\lite\toco\python\toco_from_protos.py"", line 33 in execute
  File ""c:\anaconda3\lib\site-packages\absl\app.py"", line 251 in _run_main
  File ""c:\anaconda3\lib\site-packages\absl\app.py"", line 300 in run
  File ""c:\anaconda3\lib\site-packages\tensorflow\python\platform\app.py"", line 40 in run
  File ""c:\anaconda3\lib\site-packages\tensorflow\lite\toco\python\toco_from_protos.py"", line 59 in main
  File ""C:\anaconda3\Scripts\toco_from_protos.exe\__main__.py"", line 9 in <module>
  File ""c:\anaconda3\lib\runpy.py"", line 85 in _run_code
  File ""c:\anaconda3\lib\runpy.py"", line 193 in _run_module_as_main








**Describe the expected behavior**

Should convert frozen pb file to tflite with int8 quantization


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
# pb file in the below link 
https://drive.google.com/open?id=1Ba6aEuv2VTGD_bct24UBiMh7MFe_jkOr


graph_def_file = ""frozen_model_yolov3-tiny.pb""
input_arrays = [""input_to_model""]
output_arrays = [""concat_1""]

converter = tf.lite.TFLiteConverter.from_frozen_graph(
  graph_def_file, input_arrays, output_arrays)
tflite_model = converter.convert()
open(""converted_tiny_model.tflite"", ""wb"").write(tflite_model)



**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.





"
26584,"Tensorflow Lite Java Interpreter methods throw Exceptions, but this is not in the method declaration","Methods such as `Interpreter.run` throw Java exceptions originating from native, but the method declaration does not include `throws Exception`. It would be helpful to beginners to include this. Thanks!"
26581,Wrapped tf.nn.RNNCell* layers are incompatible with tf.keras.layers.Bidirectional,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Ubuntu 16.04
- TensorFlow installed: binary
- TensorFlow version: `2.0.0.dev20190311`
- Python version: 3.6.6

**Describe the current behavior**

In the TensorFlow 2.0 preview, the `tf.nn.RNNCellDropoutWrapper` and `tf.nn.RNNCellResidualWrapper` wrappers are incompatible with `tf.keras.layers.Bidirectional`. It raises an ""Unknown layer"" exception.

**Describe the expected behavior**

The RNN classes in `tf.nn` should be compatible with all Keras RNN layers.

**Code to reproduce the issue**

```python
import tensorflow as tf

cell = tf.keras.layers.LSTMCell(10)
cell = tf.nn.RNNCellResidualWrapper(cell)

rnn = tf.keras.layers.RNN(cell)
rnn = tf.keras.layers.Bidirectional(rnn)
```

**Other info / logs**

```text
Traceback (most recent call last):
  File ""test/residual_wrapper.py"", line 30, in <module>
    rnn = tf.keras.layers.Bidirectional(rnn)
  File ""/lib/python3.6/site-packages/tensorflow/python/keras/layers/wrappers.py"", line 393, in __init__
    self.backward_layer = layer.__class__.from_config(config)
  File ""/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 920, in from_config
    cell = deserialize_layer(config.pop('cell'), custom_objects=custom_objects)
  File ""/lib/python3.6/site-packages/tensorflow/python/keras/layers/serialization.py"", line 95, in deserialize
    printable_module_name='layer')
  File ""/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py"", line 181, in deserialize_keras_object
    config, module_objects, custom_objects, printable_module_name)
  File ""/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py"", line 166, in class_and_config_for_serialized_keras_object
    raise ValueError('Unknown ' + printable_module_name + ': ' + class_name)
ValueError: Unknown layer: ResidualWrapperV2
```

@qlzh727 "
26580,custom layer ,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
26579,[1.13] Incomplete information in the warning message on using tf.contrib.predictor,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: 1.13.1
- Doc Link: https://www.tensorflow.org/api_docs/python/tf/contrib/predictor/from_saved_model https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/saved_model/load https://www.tensorflow.org/alpha/guide/keras/saving_and_serializing


**Describe the documentation issue**

On using `tf.contrib.predictor.from_saved_model` to get predictions from a saved_model, the warning message says that there _will_ be a new function to load saved models in TensorFlow 2.0, and nothing else.

Specifically, consider the following snippet of code (from https://colab.research.google.com/github/suyash/transformer/blob/master/imdb_sentiment_demo.ipynb)

```py
p = predictor.from_saved_model(
    export_dir, 
    input_names={""input_text"": ""input_text:0""}, 
    output_names={
        ""prediction"": ""dense/Softmax:0"",
        ""attention_0"": ""attention/attention_weights:0"",
        ""attention_1"": ""attention_1/attention_weights:0"",
    },
)
outputs = p({""input_text"": inputs})
print(outputs[""prediction""])
print(outputs[""attention_0""])
print(outputs[""attention_1""])
```

This prints the following warning message

```
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/predictor/saved_model_predictor.py:153: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.
```

I have trying to figure out extracting multiple outputs from the same saved model. The current guide for saved models at alpha (https://www.tensorflow.org/alpha/guide/keras/saving_and_serializing) does not provide information on loading, while the equivalent for stable (https://www.tensorflow.org/guide/saved_model) does.

I am trying to use the new `tf.saved_model.load` function. I have prepared a demo for my use-case at https://colab.research.google.com/gist/suyash/2c7e5b77ea4d94d5c3b8c3e178d06878. Specifically, I create a model with dense layers, convert it to an estimator, trained it while defining an `Exporter` for evaluation checkpoints.

Now, when I load using the new function, I am able to obtain values for the `dense_2` tensor, however in an equivalent scenario, how do I get the values of the `dense_1` tensor in the current approach. As I show in my predictor example, the output of the predictor will have 3 keys, ""prediction"", ""attention_0"" and ""attention_1"". How do I get the output of the ""dense_1"" tensor in my 2.0 example?

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
26578,Pip No matching distribution found for tensorflow-gpu==2.0.0-alpha0,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26577,"Accuracy Metric automatically selected, fails in certain cases","**System information**
- Uses a basic CNN MNIST Keras example
- CentOS7
- TensorFlow installed from: Anaconda
- TensorFlow version: Both 1.12-gpu and 2.0.0-alpha0-cpu
- Python version: 3.6

**Describe the current behavior**
* When using a ""built in"" loss function, the ""accuracy"" metric is **automatically** resolved to the ""correct one"".
* When using a custom loss function that ""touches"" `y_pred` and `y_true` in any way (even trivially as seen below), ""accuracy"" is no longer automatically resolved.

**Preamble:**
```
import tensorflow as tf
import tensorflow.keras.backend as K
from tensorflow.keras import datasets, layers, models

(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()

train_images = train_images.reshape((60000, 28, 28, 1))
test_images = test_images.reshape((10000, 28, 28, 1))

# Normalize pixel values to be between 0 and 1
train_images, test_images = train_images / 255.0, test_images / 255.0
```

**""Working"" Example:**
```
def customLoss1():
    return tf.keras.losses.sparse_categorical_crossentropy
model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(10, activation='softmax'))

model.compile(loss=customLoss1(), optimizer='adam', metrics=['accuracy',])
model.fit(train_images, train_labels, epochs=1)
```

**""Broken"" Example:**
```
def customLoss2(y_true, y_pred):
    return K.sparse_categorical_crossentropy(y_true, y_pred)

model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(10, activation='softmax'))

model.compile(loss=customLoss2(), optimizer='adam', metrics=['accuracy',])
model.fit(train_images, train_labels, epochs=1)
```

**Describe the expected behavior**
I am not sure if this is intended, a bug, or a reasonable failure. 

**Broken Example Now Works:**
In order to get the ""Broken"" exmaple to ""work"" correctly, you have to specify the correct accuracy metric directly:

```
model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(10, activation='softmax'))

model.compile(loss=customLossWorks(), optimizer='adam', metrics=['sparse_categorical_accuracy'])
model.fit(train_images, train_labels, epochs=1)
```

**Other info / logs**
* This was originally posted under #26490 but @timudk helped clarify my problem so I reposted as a separate issue."
26576,SparseTensor takes ages to initialize and can not be saved independently,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): sorta
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.11
- Python version: 3.5
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: 9
- GPU model and memory: Titan 


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**


It takes a while for me to generate a `SparseTensor`


```python
# dense is an n x m matrix

sparse = coo_matrix(dense) # almost instantaneous 

# for legibility
sparse_indicies = list(zip(
    sparse.row.astype(np.int64).tolist(), 
    sparse.col.astype(np.int64).tolist()
)) # almost instantaneous

type_casted = (sparse.data).astype(np.float32) # almost instantaneous

# takes ages (ok it takes several minutes...)
input_tensor = tf.SparseTensor(
    indices     = sparse_indicies,
    values      = type_casted,
    dense_shape = sparse.shape
) 

# save to file so I can load it to memory locally if it exists.

```

How can I save it just by itself? I have tried pickle and npy without success.

```python
import pickle, numpy as np

filename = os.path.expanduser('~/tmp/test.tmp')

with open(fn, 'wb') as f:
    pickle.dump(input_tensor, f)
    # throws ""TypeError: can't pickle _thread.RLock objects""


np.save(fn, input_tensor)
# throws ""TypeError: can't pickle _thread.RLock objects""


```



**Describe the expected behavior**

either SparseTensor should be more performant in initialization or should be saveable as a pickle object so I don't always have to re-initialize it

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
(see above)

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26574,Is it possible to set the random seed for tf.feature_column.categorical_column_with_hash_bucket on operation-level？,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.13.rc1
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
 API:tf.feature_column.categorical_column_with_hash_bucket can not set random seed
**Will this change the current api? How?** 
Yes, It will add a new parametre to set the random seed.
**Who will benefit with this feature?**
The one who want to get two different hash bucket on the same field.
**Any Other info.**
"
26573,Tensorboard is not showing the last checkpoint's evaluation result,"I trained some object detection models with custom data for 4K steps using TensorFlow Object Detection API, and evaluated them during the training. Evaluation is done for all checkpoints, I watched the results on the console.

However, I can't see the last two checkpoints' evaluation results on Tensorboard for some raeson. It shows the evaluation result of 3K steps, and nothing after that. I can see that evaluation is completed on the console, and also in the folder.

There are no error messages on the console when I start Tensorboard. I can see that training results are uploaded completely to Tensorboard, the only missing thing is the last evaluation results.

I tried evaluating the latest checkpoints again, but nothing changed. At the end of the evaluation I receive a message saying that metrics are recorded to summary...

The training checkpoints are saved in every 10 minutes, and evaluation takes 12 minutes. But even in this case, I expect the latest checkpoint's evaluation results to be there.

When I try to download the csv file from Tensorboard I also can't see the last two checkpoints' evaluations.

What could be the reason?

```
I0311 16:57:21.281645 MainThread program.py:165] Not bringing up TensorBoard, but inspecting event files.
I0311 16:57:21.281645 140028330256128 program.py:165] Not bringing up TensorBoard, but inspecting event files.
======================================================================
Processing event files... (this can take a few minutes)
======================================================================

Found event files in:
./CN_flow1_95/eval
./CN_flow1_95/train

These tags are in ./CN_flow1_95/eval:
audio -
histograms -
images
   image-0
   image-1
   image-2
   image-3
   image-4
   image-5
   image-6
   image-7
   image-8
   image-9
scalars
   Losses/Loss/BoxClassifierLoss/classification_loss
   Losses/Loss/BoxClassifierLoss/localization_loss
   Losses/Loss/RPNLoss/localization_loss
   Losses/Loss/RPNLoss/objectness_loss
   PascalBoxes_PerformanceByCategory/AP@0.5IOU/b'cyclist'
   PascalBoxes_PerformanceByCategory/AP@0.5IOU/b'motorcyclist'
   PascalBoxes_PerformanceByCategory/AP@0.5IOU/b'pedestrian'
   PascalBoxes_Precision/mAP@0.5IOU
tensor -
======================================================================

Event statistics for ./CN_flow1_95/eval:
audio -
graph
   first_step           0
   last_step            0
   max_step             0
   min_step             0
   num_steps            1
   outoforder_steps     []
histograms -
images
   first_step           0
   last_step            4112
   max_step             4112
   min_step             0
   num_steps            7
   outoforder_steps     []
scalars
   first_step           0
   last_step            4112
   max_step             4112
   min_step             0
   num_steps            7
   outoforder_steps     []
sessionlog:checkpoint -
sessionlog:start -
sessionlog:stop -
tensor -
======================================================================

These tags are in ./CN_flow1_95/train:
audio -
histograms
   ModelVars/...
images -
scalars
   Losses/TotalLoss
   Losses/clone_0/Loss/BoxClassifierLoss/classification_loss
   Losses/clone_0/Loss/BoxClassifierLoss/localization_loss
   Losses/clone_0/Loss/RPNLoss/localization_loss
   Losses/clone_0/Loss/RPNLoss/objectness_loss
   Losses/clone_1/Loss/BoxClassifierLoss/classification_loss
   Losses/clone_1/Loss/BoxClassifierLoss/localization_loss
   Losses/clone_1/Loss/RPNLoss/localization_loss
   Losses/clone_1/Loss/RPNLoss/objectness_loss
   Losses/clone_2/Loss/BoxClassifierLoss/classification_loss
   Losses/clone_2/Loss/BoxClassifierLoss/localization_loss
   Losses/clone_2/Loss/RPNLoss/localization_loss
   Losses/clone_2/Loss/RPNLoss/objectness_loss
   batch/fraction_of_150_full
   clone_0/Losses/clone_0//clone_loss
   global_step/sec
   queue/prefetch_queue/fraction_of_5_full
tensor -
======================================================================

Event statistics for ./CN_flow1_95/train:
audio -
graph
   first_step           0
   last_step            0
   max_step             0
   min_step             0
   num_steps            1
   outoforder_steps     []
histograms
   first_step           0
   last_step            4110
   max_step             4110
   min_step             0
   num_steps            28
   outoforder_steps     []
images -
scalars
   first_step           0
   last_step            4110
   max_step             4110
   min_step             0
   num_steps            54
   outoforder_steps     []
sessionlog:checkpoint
   first_step           1
   last_step            4111
   max_step             4111
   min_step             1
   num_steps            7
   outoforder_steps     []
sessionlog:start
   outoforder_steps     []
   steps                [0, 4110]
sessionlog:stop
   outoforder_steps     []
   steps                [0, 0]
tensor -
======================================================================


```"
26572,"Cuda 10: Failed to get device properties, error code: 3","I am using [keras-retinanet](https://github.com/fizyr/keras-retinanet) (0.5.0) to perform object detection on images using its TensorFlow backend. When I `load_model` their pre-trained COCO model, the GPU is initialized as expected:
```
Using TensorFlow backend.
WARNING:tensorflow:From .../virtualenv/lib64/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
2019-03-11 11:26:25.063507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-03-11 11:26:25.064403: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55c2e09b7bb0 executing computations on platform CUDA. Devices:
2019-03-11 11:26:25.064432: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1
2019-03-11 11:26:25.066831: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2993245000 Hz
2019-03-11 11:26:25.067138: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55c2e06ce580 executing computations on platform Host. Devices:
2019-03-11 11:26:25.067163: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2019-03-11 11:26:25.068038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6705
pciBusID: 0000:01:00.0
totalMemory: 10.92GiB freeMemory: 10.77GiB
2019-03-11 11:26:25.068063: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2019-03-11 11:26:25.069446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-03-11 11:26:25.069467: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0
2019-03-11 11:26:25.069477: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N
2019-03-11 11:26:25.070214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10481 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
.../virtualenv/lib64/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
```

However, when calling `predict_on_batch` on this model afterwards, the GPU cannot be initialized any more:
```
2019-03-11 11:26:52.699096: E tensorflow/core/grappler/clusters/utils.cc:83] Failed to get device properties, error code: 3
2019-03-11 11:26:52.947157: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node clipped_boxes/strided_slice_2. Error: Pack node (boxes/stack) axis attribute is out of bounds: 2 
2019-03-11 11:26:52.947225: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node clipped_boxes/strided_slice_3. Error: Pack node (boxes/stack) axis attribute is out of bounds: 2 
2019-03-11 11:26:52.947245: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node clipped_boxes/strided_slice_4. Error: Pack node (boxes/stack) axis attribute is out of bounds: 2 
2019-03-11 11:26:52.947271: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node clipped_boxes/strided_slice_5. Error: Pack node (boxes/stack) axis attribute is out of bounds: 2 
2019-03-11 11:26:53.215964: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node clipped_boxes/strided_slice_2. Error: Pack node (boxes/stack) axis attribute is out of bounds: 2 
2019-03-11 11:26:53.216024: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node clipped_boxes/strided_slice_3. Error: Pack node (boxes/stack) axis attribute is out of bounds: 2 
2019-03-11 11:26:53.216042: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node clipped_boxes/strided_slice_4. Error: Pack node (boxes/stack) axis attribute is out of bounds: 2 
2019-03-11 11:26:53.216068: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node clipped_boxes/strided_slice_5. Error: Pack node (boxes/stack) axis attribute is out of bounds: 2 
2019-03-11 11:26:53.354507: E tensorflow/stream_executor/cuda/cuda_driver.cc:1131] failed to enqueue async memcpy from host to device: CUDA_ERROR_NOT_INITIALIZED: initialization error; GPU dst: 0x7fc4f4b62900; host src: 0x7fc77b201a00; size: 4=0x4
```

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no custom code, but TensorFlow is called through keras-retinanet
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Gentoo (Base System release 2.6)
- TensorFlow installed from (source or binary): binary from PyPi
- TensorFlow version: 1.13.1
- Python version: 3.6.5
- CUDA/cuDNN version: 10.0.130 / 7.4.2.24
- GPU model and memory: Nvidia GeForce GTX 1080 Ti (11 GB memory)
- Nvidia driver version: 418.43

**Describe the current behavior**
see above

**Describe the expected behavior**
There should be no error initializing the GPU when using `predict_on_batch`. On other machines I have used older versions (TF 1.10.0 with CUDA 9.0) and it worked fine.

**Code to reproduce the issue**
* Install CUDA 10, cuDNN 7.4.2
* Install tensorflow-gpu 1.13.1 (or 1.13.0rc1/rc2) and keras-retinanet 0.5.0 into a virtualenv from PyPi
* Download pre-trained COCO model from [here](https://github.com/fizyr/keras-retinanet/releases/download/0.5.0/resnet50_coco_best_v2.1.0.h5)
* Load the model using `model = keras_retinanet.models.load_model('path/to/model.h5')` (will generate first blob of output above)
* Predict bounding boxes on an image via `model.predict_on_batch(np.expand_dims(my_image, axis=0))` (will generate second blob of output above)

**Other info / logs**
I am reporting this here and not at the keras-retinanet project because it seems to be a TensorFlow internal issue. Furthermore, it has worked fine with the same version of keras-retinanet when using older versions of TF and CUDA (see above).

Thank you!"
26571,CPU int8 inference,"Ubuntu 16.04
- TensorFlow version (you are using):1.12
- Are you willing to contribute it (Yes/No):No



**Describe the feature and the current behavior/state.** When will conv layer support **int8** in **cpu**

**Will this change the current api? How?** Add int8 datatype

**Who will benefit with this feature?** Inference time will be reduced in cnn models

**Any Other info.**
"
26569,"Can you please add opportunity to use ""MobileNet"" model with more labels, pls","private List<Map<String, Object>> detectObjectOnBinary(HashMap args) throws IOException {
    byte[] binary = (byte[])args.get(""binary"");
    String model = args.get(""model"").toString();
    double threshold = (double)args.get(""threshold"");
    float THRESHOLD = (float)threshold;
    List<Double> ANCHORS = (ArrayList)args.get(""anchors"");
    int BLOCK_SIZE = (int)args.get(""blockSize"");
    int NUM_BOXES_PER_BLOCK = (int)args.get(""numBoxesPerBlock"");
    int NUM_RESULTS_PER_CLASS = (int)args.get(""numResultsPerClass"");

    ByteBuffer imgData = ByteBuffer.wrap(binary);

    if (model.equals(""SSDMobileNet"")) {
      return parseSSDMobileNet(imgData, NUM_RESULTS_PER_CLASS, THRESHOLD);
    } else {
      return parseYOLO(imgData, BLOCK_SIZE, NUM_BOXES_PER_BLOCK, ANCHORS, THRESHOLD, NUM_RESULTS_PER_CLASS);
    }
  }

sorry, no link, old browser"
26567,Tensorflow 2.0 with CUDA 9,"The default installation were on CUDA 10 of tensorflow 2.

How to install tensorflow 2.0 with cuda 9?"
26566,Unknown default keras 'categorical_crossentropy' loss function when converting with .h5 to .tflite ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
-> I used just default code from the rstudio/keras library, no custom loss function
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-> Ubuntu 16.04.5 LTS (Xenial Xerus) debian
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
-> tensorflow installed via rstudio/keras and install.packages(""tensorflow"")
- **TensorFlow version (use command below)**:
->1.13.1
- **Python version**:
-> 2.7.12
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
-> Cuda compilation tools, release 9.0, V9.0.176
- **GPU model and memory**:
-> P2-Instance AWS EC""
     8*  K80-GPUs from NVIDIA, 64 vCPUs, 732 GiB Host-Speicher 
- **Exact command to reproduce**:
`import tensorflow as tf

converter = tf.lite.TFLiteConverter.from_keras_model_file(""test.h5"")
tflite_model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(tflite_model)`


### Describe the problem
I am trying to convert a keras model from .h5 to .tflite file.
I created the model in R using rstudio/keras and everything works fine.
Once I saved the model as .h5 file, I'm switching to Python (because there is no R converter from .5 to .tflite). But when I'm trying to convert the keras model with the default code from the TensorFlow Lite documentation: [,](https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter) I am getting the following error:

`Traceback (most recent call last):
  File ""/tmp/RtmpWIBDmu/chunk-code-849197e0a8f.txt"", line 3, in <module>
    converter = tf.lite.TFLiteConverter.from_keras_model_file(""test.h5"")
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/lite.py"", line 370, in from_keras_model_file
    keras_model = _keras.models.load_model(model_file)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/saving.py"", line 266, in load_model
    sample_weight_mode=sample_weight_mode)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/checkpointable/base.py"", line 442, in _method_wrapper
    method(self, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training.py"", line 282, in compile
    loss_function = training_utils.get_loss_function(loss)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training_utils.py"", line 873, in get_loss_function
    return losses.get(loss)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/losses.py"", line 594, in get
    return deserialize(identifier)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/losses.py"", line 585, in deserialize
    printable_module_name='loss function')
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py"", line 212, in deserialize_keras_object
    function_name)
ValueError: Unknown loss function:loss_categorical_crossentropy`

But as you can see in the source code, I am using the a default keras loss function?

### Source code / logs
This is the Code to create and train the model
`library(keras)
batch_size <- 128
num_classes <- 10
epochs <- 5

# Input image dimensions
img_rows <- 28
img_cols <- 28
fashion_mnist <- dataset_fashion_mnist()
train_images <- fashion_mnist$train$x
train_labels <- fashion_mnist$train$y
test_images <- fashion_mnist$test$x
test_labels <- fashion_mnist$test$y

train_images <- array_reshape(train_images, c(nrow(train_images), img_rows, img_cols, 1))
test_images <- array_reshape(test_images, c(nrow(test_images), img_rows, img_cols, 1))
input_shape <- c(img_rows, img_cols, 1)

# Transform RGB values into [0,1] range
train_images <- train_images / 255
test_images <- test_images / 255

train_labels <- to_categorical(train_labels, num_classes)
test_labels <- to_categorical(test_labels, num_classes)

model <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu',input_shape = input_shape) %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_flatten() %>% 
  layer_dense(units = 128, activation = 'relu') %>% 
  layer_dense(units = num_classes, activation = 'softmax')

model %>% compile(
  loss = loss_categorical_crossentropy,
  optimizer = optimizer_adadelta(),
  metrics = c('accuracy')
)

model %>% fit(
  train_images, train_labels,
  batch_size = batch_size,
  epochs = epochs,
  validation_split = 0.2,
  callbacks = list(callback_tensorboard(log_dir = ""./logs/mnist"")
))

score <- model %>% evaluate(test_images, test_labels)

cat('Test loss:', score$loss, ""\n"")
cat('Test accuracy:', score$acc, ""\n"")

save_model_hdf5(model,""test.h5"")
`
"
26565,bazel build tensorflow/python/tools:freeze_graph Error:no package '@icu//',"when i use freeza_graph ,i face a problem:
$bazel build tensorflow/python/tools:freeze_graph
Starting local Bazel server and connecting to it...
INFO: Invocation ID: 1327e781-b5f7-4f42-9ce5-dc35448da7db
ERROR: /mnt/hgfs/share1/tensorflow-master/tensorflow/core/kernels/BUILD:4791:1: no such package '@icu//': java.io.IOException: thread interrupted and referenced by '//tensorflow/core/kernels:string_util'
ERROR: Analysis of target '//tensorflow/python/tools:freeze_graph' failed; build aborted: no such package '@icu//': java.io.IOException: thread interrupted
INFO: Elapsed time: 612.379s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (194 packages loaded, 10048 target\
s configured)
    Fetching @icu; fetching 547s
How I can solove this problem?"
26564,Homepage display issue: www.tensorflow.org,"It is ok. Thanks!
Спасибо!"
26559,tensorflow1.12 hangs at LocalMaster::RunStep with tf.train.MonitoredTrainingSession,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source 
- TensorFlow version (use command below): 1.12
- Python version: 3.6
- Bazel version (if compiling from source): 0.18
- GCC/Compiler version (if compiling from source): 5.4
- CUDA/cuDNN version: 
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
b'v1.12.0-18-gd60b574' 1.12.0

**Describe the current behavior**
In distributed tensorflow, some workers hang at the last batch of dataset when the dataset epoch is 1 and the step in StopAtStepHook() is very large. What's more, the number of samples in the last batch of dataset may less than the batch_size.
If we use tf.train.StopAtStepHook() to stop training, all workers can exit successfully.
**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
In local_master.cc, WaitForNotification() function is as follows:
```
Status WaitForNotification(CallOptions* call_options,
                           const int64 default_timeout_in_ms, Notification* n) {
  int64 timeout_in_ms = call_options->GetTimeout();
  if (timeout_in_ms == 0) {
    timeout_in_ms = default_timeout_in_ms;
  }
  if (timeout_in_ms > 0) {
    int64 timeout_in_us = timeout_in_ms * 1000;
    bool notified = WaitForNotificationWithTimeout(n, timeout_in_us);
    if (!notified) {
      call_options->StartCancel();
      // The call has borrowed pointers to the request and response
      // messages, so we must still wait for the call to complete.
      n->WaitForNotification();
      return errors::DeadlineExceeded(""Operation timed out."");
    }
  } else {
    n->WaitForNotification();
  }
  return Status::OK();
}
```
Workers hang at 'n->WaitForNotification()'. We confuse that WaitForNotificationWithTimeout(n, timeout_in_us) has been called, why WaitForNotification() is called again ?

bt is as follows:
```
#0  syscall () at ../sysdeps/unix/sysv/linux/x86_64/syscall.S:38
#1  0x00007f475f56e1ac in nsync::futex (uaddr=0x555c3d471368, op=393, val=0, timeout=0x0, uaddr2=0x0, val3=-1) at external/nsync/platform/linux/src/nsync_semaphore_futex.c:21
#2  0x00007f475f56e357 in nsync::nsync_mu_semaphore_p_with_deadline (s=0x555c3d471368, abs_deadline=...) at external/nsync/platform/linux/src/nsync_semaphore_futex.c:108
#3  0x00007f475f56d1f6 in nsync::nsync_sem_wait_with_cancel_ (w=0x555c3d471360, abs_deadline=..., cancel_note=0x0) at external/nsync/internal/sem_wait.c:36
#4  0x00007f475f569277 in nsync::nsync_cv_wait_with_deadline_generic (pcv=0x7fff46c23e20, pmu=0x7fff46c23e10, lock=0x7f475f568fdf <nsync::void_mu_lock(void*)>, 
    unlock=0x7f475f568ffa <nsync::void_mu_unlock(void*)>, abs_deadline=..., cancel_note=0x0) at external/nsync/internal/cv.c:246
#5  0x00007f475f5699f5 in nsync::nsync_cv_wait_with_deadline (pcv=0x7fff46c23e20, pmu=0x7fff46c23e10, abs_deadline=..., cancel_note=0x0) at external/nsync/internal/cv.c:440
#6  0x00007f475f569a32 in nsync::nsync_cv_wait (pcv=0x7fff46c23e20, pmu=0x7fff46c23e10) at external/nsync/internal/cv.c:450
#7  0x00007f474de90bf5 in tensorflow::condition_variable::wait (this=0x7fff46c23e20, lock=...) at tensorflow/core/platform/default/mutex.cc:72
#8  0x00007f4756864a5d in tensorflow::Notification::WaitForNotification (this=0x7fff46c23e10) at ./tensorflow/core/platform/default/notification.h:54
#9  0x00007f4756c3dc34 in tensorflow::(anonymous namespace)::WaitForNotification (call_options=0x7fff46c24090, default_timeout_in_ms=0, n=0x7fff46c23e10)
    at tensorflow/core/distributed_runtime/local_master.cc:39
#10 0x00007f4756c3e439 in tensorflow::LocalMaster::RunStep (this=0x555c3ba54780, call_options=0x7fff46c24090, request=0x555c38dbae60, response=0x555c38dbb110)
    at tensorflow/core/distributed_runtime/local_master.cc:105
#11 0x00007f475686d03d in tensorflow::GrpcSession::RunProto (this=0x555c3b986280, call_options=0x7fff46c24090, req=0x555c38dbae60, resp=0x555c38dbb110)
    at tensorflow/core/distributed_runtime/rpc/grpc_session.cc:289
#12 0x00007f475686c7d6 in tensorflow::GrpcSession::RunHelper (this=0x555c3b986280, run_options=..., inputs=std::vector of length 0, capacity 0, 
    output_tensor_names=std::vector of length 4, capacity 4 = {...}, target_node_names=std::vector of length 1, capacity 1 = {...}, outputs=0x7fff46c242f0, run_metadata=0x7fff46c24350, 
    prun_handle="""") at tensorflow/core/distributed_runtime/rpc/grpc_session.cc:221
#13 0x00007f475686ce35 in tensorflow::GrpcSession::Run (this=0x555c3b986280, run_options=..., inputs=std::vector of length 0, capacity 0, 
    output_tensor_names=std::vector of length 4, capacity 4 = {...}, target_node_names=std::vector of length 1, capacity 1 = {...}, outputs=0x7fff46c242f0, run_metadata=0x7fff46c24350)
    at tensorflow/core/distributed_runtime/rpc/grpc_session.cc:270
#14 0x00007f4756832f34 in tensorflow::SessionRef::Run (this=0x555c3ba38880, run_options=..., inputs=std::vector of length 0, capacity 0, 
    output_tensor_names=std::vector of length 4, capacity 4 = {...}, target_node_names=std::vector of length 1, capacity 1 = {...}, outputs=0x7fff46c242f0, run_metadata=0x7fff46c24350)
    at tensorflow/python/client/session_ref.cc:427
#15 0x00007f4756aeeabc in TF_Run_Helper (session=0x555c3ba38880, handle=0x0, run_options=0x555c38437160, input_pairs=std::vector of length 0, capacity 0, 
    output_tensor_names=std::vector of length 4, capacity 4 = {...}, c_outputs=0x7fff46c246d8, target_oper_names=std::vector of length 1, capacity 1 = {...}, run_metadata=0x555c38dd9b70, 
    status=0x555c38ddf6d0) at tensorflow/c/c_api.cc:783
```


Using 'thread apply all bt' in gdb, we can see there are two threads wait at cond_var_.wait(l) in BackgroundWorker::WorkerLoop() method.
```
(gdb) thread apply all bt
Thread 204 (Thread 0x7f366b7fe700 (LWP 457)):
#0  syscall () at ../sysdeps/unix/sysv/linux/x86_64/syscall.S:38
#1  0x00007f3a48857cac in nsync::futex (uaddr=0x7f379c001d08, op=393, val=0, timeout=0x0, uaddr2=0x0, val3=-1) at external/nsync/platform/linux/src/nsync_semaphore_futex.c:21
#2  0x00007f3a48857e57 in nsync::nsync_mu_semaphore_p_with_deadline (s=0x7f379c001d08, abs_deadline=...) at external/nsync/platform/linux/src/nsync_semaphore_futex.c:108
#3  0x00007f3a48856cf6 in nsync::nsync_sem_wait_with_cancel_ (w=0x7f379c001d00, abs_deadline=..., cancel_note=0x0) at external/nsync/internal/sem_wait.c:36
#4  0x00007f3a48852d77 in nsync::nsync_cv_wait_with_deadline_generic (pcv=0x558523ec4f98, pmu=0x558523ec4f88, lock=0x7f3a48852adf <nsync::void_mu_lock(void*)>, 
    unlock=0x7f3a48852afa <nsync::void_mu_unlock(void*)>, abs_deadline=..., cancel_note=0x0) at external/nsync/internal/cv.c:246
#5  0x00007f3a488534f5 in nsync::nsync_cv_wait_with_deadline (pcv=0x558523ec4f98, pmu=0x558523ec4f88, abs_deadline=..., cancel_note=0x0) at external/nsync/internal/cv.c:440
#6  0x00007f3a48853532 in nsync::nsync_cv_wait (pcv=0x558523ec4f98, pmu=0x558523ec4f88) at external/nsync/internal/cv.c:450
#7  0x00007f3a3717abf5 in tensorflow::condition_variable::wait (this=0x558523ec4f98, lock=...) at tensorflow/core/platform/default/mutex.cc:72
#8  0x00007f3a36ccb475 in tensorflow::data::BackgroundWorker::WorkerLoop (this=0x558523ec4f80) at tensorflow/core/framework/dataset.cc:318
#9  0x00007f3a36ccb1f5 in tensorflow::data::BackgroundWorker::BackgroundWorker(tensorflow::Env*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)::{lambda()#1}::operator()() const () at tensorflow/core/framework/dataset.cc:287
#10 0x00007f3a36ccb887 in std::_Function_handler<void(), tensorflow::data::BackgroundWorker::BackgroundWorker(tensorflow::Env*, const string&)::<lambda()> >::_M_invoke(const std::_Any_data &) (__functor=...) at /usr/include/c++/5/functional:1871
#11 0x00007f3a36cab688 in std::function<void ()>::operator()() const (this=0x558522569ff8) at /usr/include/c++/5/functional:2267
#12 0x00007f3a3717d7c8 in std::_Bind_simple<std::function<void ()> ()>::_M_invoke<>(std::_Index_tuple<>) (this=0x558522569ff8) at /usr/include/c++/5/functional:1531
#13 0x00007f3a3717d765 in std::_Bind_simple<std::function<void ()> ()>::operator()() (this=0x558522569ff8) at /usr/include/c++/5/functional:1520
#14 0x00007f3a3717d704 in std::thread::_Impl<std::_Bind_simple<std::function<void ()> ()> >::_M_run() (this=0x558522569fe0) at /usr/include/c++/5/thread:115
#15 0x00007f3a5475f678 in std::execute_native_thread_routine_compat (__p=<optimized out>)
    at /opt/conda/conda-bld/compilers_linux-64_1534514838838/work/.build/x86_64-conda_cos6-linux-gnu/src/gcc/libstdc++-v3/src/c++11/thread.cc:94
#16 0x00007f3a68b266ba in start_thread (arg=0x7f366b7fe700) at pthread_create.c:333
#17 0x00007f3a6885c41d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109

Thread 203 (Thread 0x7f3805137700 (LWP 456)):
#0  syscall () at ../sysdeps/unix/sysv/linux/x86_64/syscall.S:38
#1  0x00007f3a48857cac in nsync::futex (uaddr=0x7f378d8850a8, op=393, val=0, timeout=0x0, uaddr2=0x0, val3=-1) at external/nsync/platform/linux/src/nsync_semaphore_futex.c:21
#2  0x00007f3a48857e57 in nsync::nsync_mu_semaphore_p_with_deadline (s=0x7f378d8850a8, abs_deadline=...) at external/nsync/platform/linux/src/nsync_semaphore_futex.c:108
#3  0x00007f3a48856cf6 in nsync::nsync_sem_wait_with_cancel_ (w=0x7f378d8850a0, abs_deadline=..., cancel_note=0x0) at external/nsync/internal/sem_wait.c:36
#4  0x00007f3a48852d77 in nsync::nsync_cv_wait_with_deadline_generic (pcv=0x5585225338f0, pmu=0x5585225338e0, lock=0x7f3a48852adf <nsync::void_mu_lock(void*)>, 
    unlock=0x7f3a48852afa <nsync::void_mu_unlock(void*)>, abs_deadline=..., cancel_note=0x0) at external/nsync/internal/cv.c:246
#5  0x00007f3a488534f5 in nsync::nsync_cv_wait_with_deadline (pcv=0x5585225338f0, pmu=0x5585225338e0, abs_deadline=..., cancel_note=0x0) at external/nsync/internal/cv.c:440
#6  0x00007f3a48853532 in nsync::nsync_cv_wait (pcv=0x5585225338f0, pmu=0x5585225338e0) at external/nsync/internal/cv.c:450
#7  0x00007f3a3717abf5 in tensorflow::condition_variable::wait (this=0x5585225338f0, lock=...) at tensorflow/core/platform/default/mutex.cc:72
#8  0x00007f3a36ccb475 in tensorflow::data::BackgroundWorker::WorkerLoop (this=0x5585225338d8) at tensorflow/core/framework/dataset.cc:318
#9  0x00007f3a36ccb1f5 in tensorflow::data::BackgroundWorker::BackgroundWorker(tensorflow::Env*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)::{lambda()#1}::operator()() const () at tensorflow/core/framework/dataset.cc:287
#10 0x00007f3a36ccb887 in std::_Function_handler<void(), tensorflow::data::BackgroundWorker::BackgroundWorker(tensorflow::Env*, const string&)::<lambda()> >::_M_invoke(const std::_Any_data &) (__functor=...) at /usr/include/c++/5/functional:1871
#11 0x00007f3a36cab688 in std::function<void ()>::operator()() const (this=0x558523c0df78) at /usr/include/c++/5/functional:2267
#12 0x00007f3a3717d7c8 in std::_Bind_simple<std::function<void ()> ()>::_M_invoke<>(std::_Index_tuple<>) (this=0x558523c0df78) at /usr/include/c++/5/functional:1531
#13 0x00007f3a3717d765 in std::_Bind_simple<std::function<void ()> ()>::operator()() (this=0x558523c0df78) at /usr/include/c++/5/functional:1520
#14 0x00007f3a3717d704 in std::thread::_Impl<std::_Bind_simple<std::function<void ()> ()> >::_M_run() (this=0x558523c0df60) at /usr/include/c++/5/thread:115
#15 0x00007f3a5475f678 in std::execute_native_thread_routine_compat (__p=<optimized out>)
    at /opt/conda/conda-bld/compilers_linux-64_1534514838838/work/.build/x86_64-conda_cos6-linux-gnu/src/gcc/libstdc++-v3/src/c++11/thread.cc:94
#16 0x00007f3a68b266ba in start_thread (arg=0x7f3805137700) at pthread_create.c:333
#17 0x00007f3a6885c41d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109
```"
26558,Posenet Tensorflow.js documentation doesn't specify how to feed it video ,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version:
- Doc Link:


**Describe the documentation issue**

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
26557,tf.keras.optimizers.Adam with tf.stop_gradient leads to ValueError: An operation has `None` for gradient.,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.

### **System information**
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 16.04.5 LTS (GNU/Linux 4.15.0-45-generic x86_64)

TensorFlow installed from (source or binary):
pip

TensorFlow version (use command below):
'1.12.0'

Python version:
Python 3.5.2

Bazel version (if compiling from source):
N/A

GCC/Compiler version (if compiling from source):
N/A

CUDA/cuDNN version:
9.0.176

GPU model and memory:
Tesla V100

Exact command to reproduce:
```
import os
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (LSTM, Activation, BatchNormalization, Bidirectional,
                                     Conv1D, Dense, Dropout, Input, Lambda, Masking,
                                     TimeDistributed)

import numpy as np
os.environ[""CUDA_VISIBLE_DEVICES""]=""1""


Input_layer = Input(shape=(2, ))
Dense1 = Dense(8, input_shape=(2, ))(Input_layer)
Dense1_stop = Lambda(lambda x: tf.stop_gradient(x))(Dense1)
Dense2 = Dense(4)(Dense1_stop)
Dense3 = Dense(1, activation='softmax')(Dense2)
model = Model(inputs=Input_layer, outputs=Dense3)

# model.compile(optimizer=tf.train.AdamOptimizer(), loss='mse')
model.compile(optimizer=tf.keras.optimizers.Adam(), loss='mse')


x = np.random.uniform(0, 1, (100, 2))
y = np.random.uniform(0, 1, (100, 1))
model.fit(x=x, y=y, validation_split=0.2)
```
### **Describe the problem：**
This problem occurs when I change tf.train.AdamOptimizer to tf.keras.optimizers.Adam. I'm not sure whether it is a bug or I wrongly written the codes. Therefore, I put it here. Thanks for your attention.

### **Source code / logs：**
```
Traceback (most recent call last):
  File ""optimizer_test.py"", line 25, in <module>
    model.fit(x=x, y=y, validation_split=0.2)
  File ""/home/jcc/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py"", line 1639, in fit
    validation_steps=validation_steps)
  File ""/home/jcc/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training_arrays.py"", line 86, in fit_loop
    model._make_train_function()
  File ""/home/jcc/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py"", line 700, in _make_train_function
    params=self._collected_trainable_weights, loss=self.total_loss)
  File ""/home/jcc/.local/lib/python3.5/site-packages/tensorflow/python/keras/optimizers.py"", line 457, in get_updates
    grads = self.get_gradients(loss, params)
  File ""/home/jcc/.local/lib/python3.5/site-packages/tensorflow/python/keras/optimizers.py"", line 85, in get_gradients
    raise ValueError('An operation has `None` for gradient. '
ValueError: An operation has `None` for gradient. Please make sure that all of your ops have a gradient defined (i.e. are differentiable). Common ops without gradient: K.argmax, K.round, K.eval.
```"
26555,"Does ""TensorFlow-experimental"" has a new version for iOS?  ","""TensorFlow-experimental"" pod's version now is 1.1.1，I need a new version to match the latest source code, and how can I build a ""TensorFlow-experimental"" framework?"
26553,tf-2.0.0-alpha0 bazel build failure in Docker - Extension file not found. Unable to load package for '//tensorflow:version_check.bzl':,"**System information**
- Linux Ubuntu 18.04.2 LTS:
- TensorFlow installed from tar.gz source file
- TensorFlow version: 2.0.0.alpha0
- Python version: 3.5.2 (provided in Docker image)
- Build inside Docker container (https://www.tensorflow.org/install/source#gpu_support_2)
- Bazel version - 0.19.2 (provided in docker image)
- GCC/Compiler version (if compiling from source): gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.11)  (provided in docker image)
- CUDA/cuDNN version: /usr/local/cuda-10.0 (provided in docker image)
- GPU model and memory: i7 980x  - no AVX instruction set so building locally with GPU support

**Describe the problem**
The bazel build step fails with the following messages:

root@67e84911cbdf:/tensorflow# bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
Starting local Bazel server and connecting to it...
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Repository rule 'build_bazel_rules_swift' returned: {""remote"": ""https://github.com/bazelbuild/rules_swift.git"", ""commit"": ""001736d056d7eae20f1f4da41bc9e6f036857296"", ""shallow_since"": ""2019-01-18"", ""init_submodules"": False, ""verbose"": False, ""strip_prefix"": """", ""patches"": [], ""patch_tool"": ""patch"", ""patch_args"": [""-p0""], ""patch_cmds"": [], ""name"": ""build_bazel_rules_swift""}
ERROR: error loading package '': Extension file not found. Unable to load package for '//tensorflow:version_check.bzl': BUILD file not found on package path
ERROR: error loading package '': Extension file not found. Unable to load package for '//tensorflow:version_check.bzl': BUILD file not found on package path
INFO: Elapsed time: 8.103s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
root@67e84911cbdf:


**Provide the exact sequence of commands / steps that you executed before running into the problem**

Following the instructions here: https://www.tensorflow.org/install/source#gpu_support_2

NOTE: These instructions work fine for the included tf-1.12 source and I've used them with a different Docker image for tf-1.13/

1) Start a Docker container using this command to mount a tf-2.0.0 directory within Docker

docker run --runtime=nvidia -it -w /tensorflow -v /home/mark/Docker_Run:/mnt -v /home/mark/CODE/tensorflow-2.0.0-alpha0:/tensorflow_source -e HOST_PERMS=""$(id -u):$(id -g)""     tensorflow/tensorflow:devel-gpu-py3 bash

This image provides python-3.5.2, bazel-0.19.2 & cuda-10. It has a /tensorflow_src directory with tf-1.12 and nothing in the /tensorflow directory.

2) Copy the tf-2.0.0-alpha0 code from the external mounted /tensorflow_source directory into the internal /tensorflow directory:

rm -rf /tensorflow
mkdir /tensorflow
cp -R /tensorflow_source/. /tensorflow

3) Run the configure step accepting all default values

cd /tensorflow
./configure

4) Run the bazel build step which fails with the message above.

NOTE: I've looked at a lot of Docker images and felt this one had the right stuff other than the alpha0 tensorflow codebase. If there's a better docker image or a better way to get the alpha0 code into the Docker image than mounting it outside of Docker please advise."
26552, Feature Request: Better control of train and evaluation modes,"**System information**
- TensorFlow version (you are using): tf 2.0 alpha
- Are you willing to contribute it (Yes/No): Yes

Current approach for handling train and evaluation modes involves passing additional argument to `call` method like this

```
def call(self, inputs, training=None):
    ...
```

If we would consider that we have some model with deep nested layers that contain `Dropout`, `Batch Norm` and other layers, then it would be necessary to pass `training` argument to all of them through several calls.

This approach have several drawbacks.

First of all, it could be potential bug in the model, since programmer could forget that propagation of some layer depends on training state.
Also, passing `training` argument is extremely verbose.

At the same time, PyTorch solves this task in much more elegant way – every layer have additional state `train: bool`. Since we are aware of nested layers for some model, setting `training = False` on topmost layer will set all children layers to this mode without being necessary to explicitly handle passing training state argument.

Implementation of this feature will not change current API, but will add new methods/functionality to TensorFlow. It could be two additional methods `Model.train()` and `Model.eval()` that recursively change every layer state, which affects on `call` method like `training` argument do.

This feature will simplify creation of custom models for both production and research tasks. It will make code much more clear and will make TensorFlow more attractive for active PyTorch users.
"
26551,Unable to install on Raspbian Stretch with Virtual Environments,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspbian Stretch Lite
- TensorFlow installed from (source or binary): using `pip` 
- TensorFlow version: 1.12
- Python version: 2.7
- Installed using virtualenv? pip? conda?: `virtualenv`

**Describe the problem**
I wanted to install Tensorflow for Python 2.7 over a clean install of Raspbian Stretch Lite on Raspberry Pi 3 B+, these are the steps I followed before encountering the problem:

1. `sudo apt-get update && sudo apt-get upgrade`
1. `wget https://bootstrap.pypa.io/get-pip.py`
1. `sudo python3 get-pip.py`
1. `sudo pip install virtualenv virtualenvwrapper`
1. `nano ~/.bashrc` (_add standard paths for virtualenvs_)
1. `source .bashrc`
1. `mkvirtualenv tf -p python2`
1. `sudo apt-get install libhdf5-serial-dev libopenblas-dev liblapack-dev libeigen3-dev python-dev`
1. `pip install tensorflow`
`  Could not find a version that satisfies the requirement tensorflow (from versions: )`
`No matching distribution found for tensorflow`

Any idea why it did not find the Tensorflow package? Isn't `(from versions: )` strange since it should detect the installed Python?

Many thanks

**UPDATE**: I tried also creating a Python 3 virtual environment and repeat the installation, but I receive the same error"
26549,[Lite] Assertion failure if shape of dynamic output tensor changes between invoke()s,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 1.13.1 (cpu)
- Python version: 3.6.7
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Moved from #26248.**

If an op:

* depends on a (1) dynamic tensor, and (2) a normal intermediate tensor
* the computation of (2) is performed before (1)
* dims of (1) change between `Interpreter::invoke()`s

Then assertion failure `ERROR: tensorflow/lite/simple_memory_arena.cc:100 erased_allocs_count != 1 (0 != 1)` is triggered.

An unit test is attached in #26248. On my box it is also reproducible by the Python snippet below, which generate a model dynamically. However, my reviewer can only reproduce the issue by unit test, but not by script, so the script may miss something. To help debugging, a TFLite model file which I can reproduce the issue with is uploaded: [failed.tar.gz](https://github.com/tensorflow/tensorflow/files/2949840/failed.tar.gz)

```python
import numpy as np
import tensorflow as tf


i = tf.placeholder(name='i', dtype=tf.float32, shape=[1])
p = tf.placeholder(name='p', dtype=tf.int32, shape=[1, 2])
a = tf.placeholder(name='a', dtype=tf.float32, shape=[1])

n = tf.negative(a)
o = tf.add(tf.pad(i, p), n)

test_input = np.array([0], dtype=np.float32)
test_pad = np.array([[2,2]], dtype=np.int32)
test_pad_2 = np.array([[4,4]], dtype=np.int32)
test_a = np.array([1], dtype=np.float32)


def test_tf():
    with tf.Session() as sess:
        out = sess.run(o, {i: test_input, p: test_pad, a: test_a})
        assert out.shape == (5,)

        out = sess.run(o, {i: test_input, p: test_pad_2, a: test_a})
        assert out.shape == (9,)

test_tf()


# convert
with tf.Session() as sess:
    conv = tf.lite.TFLiteConverter.from_session(sess, [i, p, a], [o])
    lite_model_bytes = conv.convert()

    # with open('failed.tflite', mode='wb') as fp:
    #     fp.write(lite_model_bytes)


def test_tflite():
    interp = tf.lite.Interpreter(model_content=lite_model_bytes)

    #### I just hard-coded them:
    # print(interp.get_input_details())
    # print(interp.get_output_details())

    interp.allocate_tensors()
    interp.set_tensor(4, test_input)
    interp.set_tensor(5, test_pad)
    interp.set_tensor(3, test_a)

    interp.invoke()
    assert interp.get_tensor(0).shape == (5,)

    interp.set_tensor(5, test_pad_2)

    interp.invoke()
    assert interp.get_tensor(0).shape == (9,)

test_tflite()

```
"
26548,refuse﻿ find example ,add example learning TensorFlow 2.0 to comment type of warning ecology place. Sample https://photos.google.com/search/battery https://photos.google.com/search/refuse﻿
26547,Tutorial: TF 2.0 - Text generation with keras.layer.LSTM - Fails to Generate Sensible Text,"**System information**
- TensorFlow version: `'2.0.0-alpha0'`
- Doc Link: https://www.tensorflow.org/alpha/tutorials/sequences/text_generation#generate_text

**Describe the documentation issue**

The model, after 10 epochs (the default), generates ""unreadable"" text. This is through running the default Colab notebook via the docs/as loaded from GitHub: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/text_generation.ipynb

```py
print(generate_text(model, start_string=u""ROMEO: ""))
```
```sh
# Output
ROMEO: VINLNKENKJVENVJNKJ3NKJKENqUKCJUKENQENVENJVENKNQKJJNKJQNK$$zzzzzzzzzzz33YYWK3JVAYYYY3NJV3NKJVJQNUKKENJNVAJJNKKxJJNKJKKJJJNKYYYJJNUJNUKC$UJVENKEVBWENVQNKJjUSJHJJJNKDENVKENVQVQVKENU$UJHENJJNJNXQGDVxjUKAQCHENKYYYJJ3NVJVMANQNKJQKENVENVJNNJNHJQUKENXENKENV3NJNUMENSENVVENVENVANQNVJQKQVHMJJNJNJNYUKENJVJNKNVANUKEVxx3NXUKENVQVUJKEN3NKJHJUKENX$qUKENQKJJNKJNNUKENKENVNQNVJKYJJNUJHENVJHYJJVANJJKYV3NVYWQJK3KJVJQUKKCJKYNVJQKEQNKEENNKJQJJNKYV3NKJJNUQDNKJKzzxJ3NV3NQNUKENKEQNKENV3NKYVQVANCHENKJJNKYVJYAN$YY33YYVJXMANQNKENVNVAVDNVANKJQNLYVQVHIVAJKJQUKENKENKJJqUKEJNNQSKCjUKENJHIVEN3NENVUVENVNVANJNVAYYVENQVKEJV3NKJx3NKYJJxQVNVHENVANMJVYYJQNUSQ3VKENVYYVNVAQXKENNKYVJJNKJQNKENX3NJXYNVJVUKJQKKJxJQNQKKENV3NKYVANJjUJQVENNKEMx3NXJNQUVQNKENRQKENQKEXKNRJNKYYYVQNVQVOVE3NHJJXKKENNQVHENVANJKENJJNN$VQUJKHENVQCHINQEVK:JVJHYYYYWqUKEMKYVJJNQVQVKAJENJQQKKENUSK$JJHEV:NVENV3NVJNJVENVENKYVJVMAYB$YYVJJNJJNUKCUKExxx3NVAQYEYYYYY3Y3VJYYYVJMJNJKK3JJNKKYV3NJNKYVQVJQNKEKYCjUKYV3NVEVJKYYYJJJjUKCENDNzzzzRNUVENVVQLNJVAKKENKJNNEQKEJNNKYYV$
```

Training output w/ loss:
```py
history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])
```
```sh
Epoch 1/10
172/172 [==============================] - 37s 213ms/step - loss: 2.6031
Epoch 2/10
172/172 [==============================] - 33s 190ms/step - loss: 1.9073
Epoch 3/10
172/172 [==============================] - 34s 198ms/step - loss: 1.6539
Epoch 4/10
172/172 [==============================] - 33s 194ms/step - loss: 1.5161
Epoch 5/10
172/172 [==============================] - 34s 196ms/step - loss: 1.4312
Epoch 6/10
172/172 [==============================] - 33s 189ms/step - loss: 1.3695
Epoch 7/10
172/172 [==============================] - 33s 190ms/step - loss: 1.3196
Epoch 8/10
172/172 [==============================] - 34s 199ms/step - loss: 1.2751
Epoch 9/10
172/172 [==============================] - 34s 196ms/step - loss: 1.2337
Epoch 10/10
172/172 [==============================] - 34s 196ms/step - loss: 1.1941
```

I've validated:

* The source data on GCS is OK - https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt
* The char2idx mapping is correct / reversible (as expected; no typos)
* Reduced the temperature - predictions are still invalid (as expected; they aren't really close to what we attempted to train)
* Ran for 30 epochs (loss: 0.6435)

Continuing to debug (far from an expert) but filing this as a heads-up."
26546, ModuleNotFoundError: No module named 'tensorflow.compat.v2',"Was going through the Jupyter notebook on TFP that was associated with the TF Dev Summit:

https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Probabilistic_Layers_Regression.ipynb
Run

Running the line 

`import tensorflow.compat.v2 as tf`

Returns the error:

> ModuleNotFoundError: No module named 'tensorflow.compat.v2'

Running tf.VERSION shows it's 1.13. 

I'm assuming this is related to not having TF 2.0 installed, and only having T 1.13 installed.
Is a separate, 2nd installation of TF (2.0) needed to import tensorflow.compat.v2 or can both be installed into the same virtual environment?

Just want to make sure installing both alongside each other won't break either or both.

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

> b'unknown' 1.13.1
"
26545,Issue on converting yolo to tflite with bazel-bin toco,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.10
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):  master


**Provide the text output from tflite_convert**

```
I tensorflow/lite/toco/import_tensorflow.cc:1335] Converting unsupported operation: ExtractImagePatches
2019-03-10 16:38:02.722378: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 211 operators, 370 arrays (0 quantized)
2019-03-10 16:38:02.727060: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 211 operators, 370 arrays (0 quantized)
2019-03-10 16:38:03.193935: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 97 operators, 189 arrays (0 quantized)
2019-03-10 16:38:03.195613: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 97 operators, 189 arrays (0 quantized)
2019-03-10 16:38:03.196705: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 97 operators, 189 arrays (0 quantized)
2019-03-10 16:38:03.198405: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 68550208 bytes, theoretical optimal value: 66453504 bytes.
2019-03-10 16:38:03.202350: E tensorflow/lite/toco/toco_tooling.cc:456] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONCATENATION, CONV_2D, MAXIMUM, MAX_POOL_2D, MUL, PAD. Here is a list of operators for which you will need custom implementations: ExtractImagePatches.
We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONCATENATION, CONV_2D, MAXIMUM, MAX_POOL_2D, MUL, PAD. Here is a list of operators for which you will need custom implementations: ExtractImagePatches.

```

Also, please include a link to a GraphDef or the model if possible.

Model is retrainer yolo

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26544,Timing hook giving wrong execution time,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google colab env
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): !pip3 install -U tensorflow-gpu==2.0.0-alpha0
- TensorFlow version (use command below): 2.0.0-alpha0
- Python version:3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

Timing hook is showing the training time as 12.11 sec, whereas elapsed time from python by getting time info before and after the step is : 70.4 sec

Timing hook and elapsed time should be same within a limit
[Fashion_mnist_estimators.zip](https://github.com/tensorflow/tensorflow/files/2949587/Fashion_mnist_estimators.zip)


**Code to reproduce the issue**
Attached ehere

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26543,loss becoming 'nan' and accuracy dropping to 5% using tf.keras,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>


**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google colab env
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): !pip3 install -U tensorflow==2.0.0-alpha0
- TensorFlow version (use command below): 2.0.0-alpha0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: no GPU used


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

""loss becoming 'nan' and accuracy dropping to 5% using tf.keras, with fashion-mnist, on 4th epoch (sometimes it happens on 7th / 8th epoch)

""loss should not become 'nan' and accuracy should never drop unexpectedly
Print output : 
Epoch #1	 Loss: 0.679400	Accuracy: 0.729167
Epoch #2	 Loss: 0.558087	Accuracy: 0.770833
Epoch #3	 Loss: 0.487591	Accuracy: 0.812500
Epoch #4	 Loss: 0.429859	Accuracy: 0.833333
**Epoch #5	 Loss: nan	        Accuracy: 0.052083**
[Fashion_mnist_with_keras_eager_and_tf_data.zip](https://github.com/tensorflow/tensorflow/files/2949581/Fashion_mnist_with_keras_eager_and_tf_data.zip)


**Code to reproduce the issue**
Code attached in jupyter notebook format

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26539,[TF 2.0 API Docs] tf.zeros_like,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version:  2.0
- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/zeros_like


**Describe the documentation issue**

- **Raises listed and defined**
  No raises listed

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
26538,[TF 2.0 API Docs] tf.zeros_initializer,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version:  2.0
- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/zeros_initializer


**Describe the documentation issue**

- **Raises listed and defined**
  No raises listed

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
26537,[TF 2.0 API Docs] tf.zeros,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version:  2.0
- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/zeros


**Describe the documentation issue**

- **Visuals, if Applicable**
  No visuals are included.

- **Raises listed and defined**
  No raises listed

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
26536,tf.keras.layers.Dense can't set attribute when subclassing tf.keras.Model,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave 10.14.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-alpha0
- Python version: 3.6.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
Using the Keras model subclassing API, Dense layer cannot be instantiated at any point in `__init__` after instantiation of Conv2D layer. 

Defining the same model using the Sequential API gives no errors. Similarly, Functional API gives no error message. Reverting to TF 1.13.1 with eager execution enabled shows the same results.

**Describe the expected behavior**
`tf.keras.layers.Dense` should be correctly instantiated when defining a model using Keras model subclassing API, just like how Sequential and Functional APIs behave.

**Code to reproduce the issue**
```python
# Model subclassing, error
class Discriminator(tf.keras.Model):
  def __init__(self):
    super(Discriminator, self).__init__(name='Discriminator')
    self.conv1 = layers.Conv2D(64, kernel_size=(5, 5), strides=(2, 2),
                              padding='same')
    self.conv1_bn = layers.BatchNormalization()
    self.conv1_out = layers.LeakyReLU()
    
    self.conv2 = layers.Conv2D(128, kernel_size=(5, 5), strides=(2, 2),
                              padding='same')
    self.conv2_bn = layers.BatchNormalization()
    self.conv2_out = layers.LeakyReLU()
    
    self.conv3 = layers.Conv2D(256, kernel_size=(5, 5), strides=(1, 1),
                               padding='same')
    self.conv3_bn = layers.BatchNormalization()
    self.conv3_out = layers.LeakyReLU()

    self.flatten = layers.Flatten()
    # Error occurs here, code does run without dense layer
    self.output = layers.Dense(1)
    
  def call(self, input, training=True):
    conv1 = self.conv1(input)
    conv1_bn = self.conv1_bn(conv1)
    conv1 = self.conv1_out(conv1_bn)

    conv2 = self.conv2(conv1)
    conv2_bn = self.conv2_bn(conv2)
    conv2 = self.conv2_out(conv2_bn)
    
    conv3 = self.conv3(conv2)
    conv3_bn = self.conv3_bn(conv3)
    conv3 = self.conv3_out(conv3_bn)
    
    flatten = self.flatten(conv3)
    output = self.output(flatten)
    
    return output
```
```python
# Sequential API, no error
def make_discriminator_sequentialmodel():
    model = tf.keras.Sequential()
    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', 
                                     input_shape=[28, 28, 1]))
    model.add(layers.LeakyReLU())
    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))
    model.add(layers.LeakyReLU())
    model.add(layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'))
    model.add(layers.LeakyReLU())
    model.add(layers.Flatten())
    model.add(layers.Dense(1))
     
    return model
  
discriminator_sequential = make_discriminator_sequentialmodel()
fake_input = np.ones((32, 28, 28, 1), dtype=np.float32)
print(discriminator_sequential(fake_input))
```

```python
# Functional API, no error
def make_discriminator_functionalmodel():
  inputs = tf.keras.Input(shape=(28, 28, 1))
  
  x = layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same')(inputs)
  x = layers.LeakyReLU()(x)
  x = layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same')(x)
  x = layers.LeakyReLU()(x)
  x = layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same')(x)
  x = layers.LeakyReLU()(x)
  x = layers.Flatten()(x)
  x = layers.Dense(1)(x)
  
  model = tf.keras.Model(inputs=inputs, outputs=x)
  
  return model

discriminator_functional = make_discriminator_functionalmodel()
fake_input = np.ones((32, 28, 28, 1), dtype=np.float32)
print(discriminator_functional(fake_input))
```
**Other info / logs**
Traceback from model subclassing code:
```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-12-1e75b671d09c> in <module>()
     48     return logits
     49 
---> 50 D = Discriminator()
     51 fake_input = np.ones((32, 28, 28, 1), dtype=np.float32)
     52 d_o = D.call(fake_input, training=True)

<ipython-input-12-1e75b671d09c> in __init__(self)
     25 
     26     self.flatten = layers.Flatten()
---> 27     self.output = layers.Dense(1)
     28 
     29   def call(self, input, training=True):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py in __setattr__(self, name, value)
    410                            ' Always start with this line.')
    411 
--> 412     super(Network, self).__setattr__(name, value)
    413 
    414     # Keep track of metric instance created in subclassed model/layer.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __setattr__(self, name, value)
   1778         # Exclude @property.setters from tracking
   1779         hasattr(self.__class__, name)):
-> 1780       super(Layer, self).__setattr__(name, value)
   1781       return
   1782 

AttributeError: can't set attribute
```
"
26535,[TF 2.0 API Docs] tf.unique,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version:  2.0
- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/unique


**Describe the documentation issue**

- **Visuals, if Applicable**
  No visuals are included.

- **Raises listed and defined**
  No raises listed

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
26534,[TF 2.0 API Docs] tf.unstack,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version:  2.0
- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/unstack


**Describe the documentation issue**

- **Visuals, if Applicable**
  No visuals are included.


**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
26533,[TF 2.0 API Docs] tf.argsort,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version:  2.0
- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/argsort


**Describe the documentation issue**

- **Usage example**
  No usage example is provided.

- **Visuals, if Applicable**
  No visuals are included.


**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
26532,[TF 2.0 API Docs] tf.math.argmin,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version:  2.0
- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math/argmin


**Describe the documentation issue**

- **Usage example**
  No usage example is provided.

- **Visuals, if Applicable**
  No visuals are included.

- **Raises listed and defined**
  No raises listed

Much like #26530, documentation for tf.math.argmin is created from a generated file ``python/ops/gen_math_ops.py``; a link to the file that generates ``python/ops/gen_math_ops.py`` would be handy for users.

Related files to be updated: ``tensorflow/core/api_def/base_api/api_def_ArgMin.pbtxt``, ``/tensorflow/core/ops/math_ops.cc``
**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
26531,TensorFlow Official website tutorials error,"
**System information**
- TensorFlow version:
- Doc Link: https://tensorflow.google.cn/tutorials/keras/basic_classification#preprocess_the_data

**Describe the documentation issue**
plt.show()  missing


"
26530,[TF 2.0 API Docs] tf.math.argmax,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version:  2.0
- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math/argmax


**Describe the documentation issue**

- **Usage example**
  No usage example is provided.

- **Visuals, if Applicable**
  No visuals are included.

- **Raises listed and defined**
  No raises listed

Much like #25802, documentation for tf.math.argmax is created from a generated file ``python/ops/gen_math_ops.py``; a link to the file that generates ``python/ops/gen_math_ops.py`` would be handy for users.

Related files to be updated: ``tensorflow/core/api_def/base_api/api_def_ArgMax.pbtxt``, ``/tensorflow/core/ops/math_ops.cc``
**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
26527,Tensorflow segfaults with simple convolutional network,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 18.04 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Pip
- TensorFlow version (use command below): 
tensorflow                     1.13.1       
tensorflow-estimator     1.13.0       
b'v1.13.1-0-g6612da8951' 1.13.1
- Python version: 3.6.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: 1070 TI


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
Receiving a segmentation fault when I run this code

**Describe the expected behavior**
Expected model to be filled

**Code to reproduce the issue**
```
def load_image_into_numpy_array(image_path):
  image = Image.open(image_path)
  (im_width, im_height) = image.size
  return np.array(image.getdata()).reshape(
      (im_height, im_width, 3)).astype(np.uint8)


if __name__ == ""__main__"":
    TEST_IMAGE_PATHS = 'train'
    output = {}
    model = keras.Sequential([
        keras.layers.Conv2D(64, kernel_size=3, activation=tf.nn.relu, input_shape=(360, 640,  3)),
        keras.layers.Flatten(),
        keras.layers.Dense(128, activation=tf.nn.relu),
        keras.layers.Dense(2, activation=tf.nn.softmax)
    ])
    model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
    labels = {}
    print(""Loaded labels"")
    with open(""labels.json"") as label_f:
        labels = json.loads(''.join(label_f.readlines()))
    train_images = []
    train_labels = []
    count = 0
    for (dirpath, dirnames, filenames) in os.walk(TEST_IMAGE_PATHS):
        for filename in filenames:
            if filename in labels:
                img = load_image_into_numpy_array(os.path.join(dirpath, filename))
                train_images.append(img)
                train_labels.append([labels[filename]])
        else:
            continue
        break
    train_images = np.array(train_images)
    train_labels = np.array(train_labels)
    model.fit(train_images, train_labels, epochs=5, batch_size=396)
```

All images in TEST_IMAGE_PATHS are 360x640 

**Other info / logs**
Here's the gdb traceback

```

Thread 1 ""python3"" received signal SIGSEGV, Segmentation fault.
__memmove_avx_unaligned_erms () at ../sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S:490
490	../sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S: No such file or directory.
(gdb) bt
#0  __memmove_avx_unaligned_erms () at ../sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S:490
#1  0x00007fffcdd033bd in TF_NewTensor () from /home/cjds/.local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00007fffcdc5b8d5 in tensorflow::PyArrayToTF_Tensor(_object*, std::unique_ptr<TF_Tensor, tensorflow::detail::TFTensorDeleter>*) ()
   from /home/cjds/.local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00007fffcdc5c5e5 in tensorflow::NdarrayToTensor(_object*, tensorflow::Tensor*) () from /home/cjds/.local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00007fffcdb06ad3 in tensorflow::(anonymous namespace)::RunCallableHelper(tensorflow::Session*, long, _object*, TF_Status*, absl::InlinedVector<_object*, 8ul, std::allocator<_object*> >*, TF_Buffer*) ()
   from /home/cjds/.local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00007fffcdb0765c in tensorflow::TF_SessionRunCallable(TF_Session*, long, _object*, TF_Status*, absl::InlinedVector<_object*, 8ul, std::allocator<_object*> >*, TF_Buffer*) ()
   from /home/cjds/.local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#6  0x00007fffcdab59e0 in _wrap_TF_SessionRunCallable () from /home/cjds/.local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
```"
26526,importing scope with map_fn twice causes internal bug,"**System information**
- Code is custom written
- System OS Ubuntu 16.04
- Mobile version N/A
- Installed from binary
- Tested on TF versions 1.12.0 and 1.13.1
- Python version 3.5
- Bazel version N/A
- Compiler version N/A
- CUDA/CUDNN 9/7.5
- Graphics card GTX 1080 12GB

**CURRENT BEHAVIOUR**: 
A graph is used that holds multiple map_fn functions inside it, trying to load the graph multiple times into different scopes as ""towers"" throws an internal bug error. The error is as follows:
```

Invalid loop structure: Loop ""create_t_fn/while/while_context"" has more than one LoopCond node: 
{{node tower_1/create_t_fn/while/LoopCond}} and {{node tower_0/create_t_fn/while/LoopCond}}. This is an internal bug, please file a bug report with instructions on how to reproduce the error.

```

**EXPECTED BEHAVIOUR**

I should be able to load multiple graphs into different scopes, I have used this method of loading multiple graphs before, but none of those graphs used a map_fn function

**CODE TO REPRODUCE ERROR**

```
with self.graph.as_default():
            for i, device in enumerate(gpu_device):
                with tf.device('/device:GPU:%d'%device):
                    tf.import_graph_def(graph_def, name='tower_%d' % i)
                    with tf.name_scope('tower_%d' % i):
                        print('...... Placing  graph into tower_%d on /device:GPU:%d' %(i, device))

                        input_image_tensor = self.graph.get_tensor_by_name('tower_%d/input_img:0' % i)
                        input_boxes_tensor = self.graph.get_tensor_by_name('tower_%d/bbox:0' % i)

                        keypoints_tensor = self.graph.get_tensor_by_name('tower_%d/kpt:0' % i)

                        self.tower_input_image_tensors.append(input_image_tensor)
                        self.tower_input_boxes_tensors.append(input_boxes_tensor)
                        self.tower_ouput_kpt_tensors.append(keypoints_tensor)
```

The above code is used to load the graphs, the error is incurred only when I try to execute it using the code below:

```
for t_idx, tower_data in enumerate(tower_data_splits):
            if len(tower_data) > 0:

                feed_dict[self.tower_input_boxes_tensors[t_idx]] = tower_data
                feed_dict[self.tower_input_image_tensors[t_idx]] = image.copy()
                print(""Tower"",t_idx,"" gets data of shape "",tower_data.shape)
                
                out_tensors.append(self.tower_ouput_kpt_tensors[t_idx])

        with self.sess.graph.as_default():
            tower_outs = self.sess.run(out_tensors,feed_dict)

```
**Other info / logs**

The graph is custom created in TF, usage of the map_fn function can be seen below

```
T = tf.identity(tf.map_fn(lambda x: tf.convert_to_tensor([
            [x[0,0], x[0,1],0],
            [x[1,0], x[1,1],0],
            [  0,      0,   1]], name = ""convert_t""),set_val_udv, name = ""create_t_fn""),name=""T"")
        
```

```
scale_map = tf.identity(tf.map_fn(lambda x: tf.convert_to_tensor([
            [x, x, 1],
            [x, x, 1],
            [1, 1, 1]], dtype=tf.float32),scale, name = ""create_scale_fn""),name=""scale_map"")

```

The issues states that there's an issue in the first map_fn, but it will exist for any and all map_fn

I have tried setting the `TF_ENABLE_WHILE_V2` and `TF_ENABLE_COND_V2` variables both via 
```
os.environ['TF_ENABLE_WHILE_V2'] = '1'
os.environ['TF_ENABLE_COND_V2'] = '1'
```
and via
```
export TF_ENABLE_WHILE_V2 = '1'
export TF_ENABLE_COND_V2 = '1'
```

as  suggested by another issue, but this does not change anything, another implied quick fix was to add a name to all map_fn calls, but even that does not fix it"
26525,Failed to import TRTEngineOp,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): custom code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6.3
- CUDA/cuDNN version:9.0
- GPU model and memory:Tesla P4


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
Failed to load a tensorrt trt_converted saved model. 
```
    meta_graph_def = tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], model_path)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 324, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/saved_model/loader_impl.py"", line 269, in load
    return loader.load(sess, tags, import_scope, **saver_kwargs)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/saved_model/loader_impl.py"", line 420, in load
    **saver_kwargs)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/saved_model/loader_impl.py"", line 350, in load_graph
    meta_graph_def, import_scope=import_scope, **saver_kwargs)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1457, in _import_meta_graph_with_return_elements
    **kwargs))
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/meta_graph.py"", line 806, in import_scoped_meta_graph_with_return_elements
    return_elements=return_elements)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 399, in import_graph_def
    _RemoveDefaultAttrs(op_dict, producer_op_list, graph_def)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 159, in _RemoveDefaultAttrs
    op_def = op_dict[node.op]
KeyError: 'TRTEngineOp'

```
But when I add extra import statements, the original code works. I must import tensorflow.contrib.tensorrt as trt, but this is a unused import for my code.
**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
`tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], tensorrt_saved_mode_path)`
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26524,Can't import tensorflow,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Enterprise
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Not sure (pip install tensorflow-gpu in conda prompt)
- TensorFlow version: Latest stable release with GPU support 
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: conda with a self-created virtual environment
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0
- GPU model and memory: 1070-Ti 



**Describe the problem**
Successfully installed tensorflow and keras through pip install in conda prompt. But then failed to import tensorflow.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
import tensorflow as tf

**Any other info / logs**
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
~\.conda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

~\.conda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in <module>
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

~\.conda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

~\.conda\envs\tensorflow\lib\imp.py in load_module(name, file, filename, details)
    242         else:
--> 243             return load_dynamic(name, filename, file)
    244     elif type_ == PKG_DIRECTORY:

~\.conda\envs\tensorflow\lib\imp.py in load_dynamic(name, path, file)
    342             name=name, loader=loader, origin=path)
--> 343         return _load(spec)
    344 

ImportError: DLL load failed: 找不到指定的模組。

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-1-64156d691fe5> in <module>
----> 1 import tensorflow as tf

~\.conda\envs\tensorflow\lib\site-packages\tensorflow\__init__.py in <module>
     22 
     23 # pylint: disable=g-bad-import-order
---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
     25 
     26 from tensorflow._api.v1 import app

~\.conda\envs\tensorflow\lib\site-packages\tensorflow\python\__init__.py in <module>
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 # Protocol buffers

~\.conda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     72 for some common reasons and solutions.  Include the entire stack trace
     73 above this error message when asking for help."""""" % traceback.format_exc()
---> 74   raise ImportError(msg)
     75 
     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""C:\Users\user\.conda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\user\.conda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\user\.conda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\user\.conda\envs\tensorflow\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\user\.conda\envs\tensorflow\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: 找不到指定的模組。

"
26519,Build Tensorflow 2.0 Alpha From Source,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.14
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.0 alpha
- Python version: 3.7.2
- Bazel version (if compiling from source): 0.20
- GCC/Compiler version (if compiling from source): Apple LLVM version 10.0.0 (clang-1000.10.44.4)
- CUDA/cuDNN version: No
- GPU model and memory: No



**Describe the problem**
Builded Tensorflow 2.0 Alpha from source but tf does not enable eager by default.
What should I do to enable eager by default

**Provide the exact sequence of commands / steps that you executed before running into the problem**

bazel build --config=opt --config=noaws --config=nohdfs --config=noignite --config=nokafka --config=nonccl --config=nogcp //tensorflow/tools/pip_package:build_pip_package


**Any other info / logs**

import tensorflow as tf
In[3]: tf.__version__
Out[3]: '2.0.0-alpha0'
tf.executing_eagerly()
Out[4]: False
a = tf.ones(shape=(3, 2))
a
Out[6]: <tf.Tensor 'ones:0' shape=(3, 2) dtype=float32>
"
26516,Session crashed for an unknown reason,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
 
 import tensorflow as tf

- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
After installing tensorflow 2.0 in colab,  on executing command 'import tensorflow as tf', I get and error message ""**Your session crashed for an unknown reason**.""
**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

    The code below is from the Colab tutorials for TF2.0 in Boosted Tree Models.

from __future__ import absolute_import, division, print_function
import numpy as np
import pandas as pd
from IPython.display import clear_output
!pip install tensorflow==2.0.0-alpha0
import tensorflow as tf
tf.random.set_seed(123)



**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26515,RuntimeError: You must compile your model before using it.,"System information

OS Platform and Distribution : Windows 10
TensorFlow installed from (source or binary): anaconda promp
TensorFlow version (use command below): '1.8.0'
Python version: 3.6
Bazel version (if compiling from source): na
GCC/Compiler version (if compiling from source): na
CUDA/cuDNN version: 9/na
GPU model and memory: gtx1060
Exact command to reproduce: na
Anaconda platform:yes


I am trying to run the following model in eager mode  but I get and error that I need to compile my model although I have model.compile in my code

```


import tensorflow as tf
tf.enable_eager_execution()
from keras.datasets import cifar10

(x_train, y_train), (x_test, y_test) = cifar10.load_data()

#one hot encoding
y_train_one_hot=tf.keras.utils.to_categorical(y_train,num_classes=10)
y_test_one_hot=tf.keras.utils.to_categorical(y_test,num_classes=10)

#Transofrm them to a float32 type
x_train = x_train.astype('float32')
x_test = x_test.astype('float32')

#normalize the images
x_train /= 255
x_test /= 255


def base_model_v2():

    #input_layer = tf.keras.layers.Input(shape=(32, 32, 3), name=""input_layer"")
    num_classes=10
    weight_decay = 1e-4
    input_shape=x_train.shape[1:]
    #
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Conv2D(32, (3,3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay), 
                                     input_shape=input_shape))
    model.add(tf.keras.layers.Activation('elu'))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.Conv2D(32, (3,3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))
    model.add(tf.keras.layers.Activation('elu'))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))
    model.add(tf.keras.layers.Dropout(0.2))
 
    model.add(tf.keras.layers.Conv2D(64, (3,3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))
    model.add(tf.keras.layers.Activation('elu'))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.Conv2D(64, (3,3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))
    model.add(tf.keras.layers.Activation('elu'))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))
    model.add(tf.keras.layers.Dropout(0.3))
 
    model.add(tf.keras.layers.Conv2D(128, (3,3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))
    model.add(tf.keras.layers.Activation('elu'))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.Conv2D(256, (3,3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))
    model.add(tf.keras.layers.Activation('elu'))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))
    model.add(tf.keras.layers.Dropout(0.4))
 
    model.add(tf.keras.layers.Flatten())
    model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))
    
    
    
    return model
 

    
batch_size = 256

epochs = 50
datagen = tf.keras.preprocessing.image.ImageDataGenerator(
    rotation_range=15,
    width_shift_range=0.1,
    height_shift_range=0.1,
    horizontal_flip=True)

datagen.fit(x_train)


cif10=base_model_v2()

#opt_rms=tf.keras.optimizers.RMSprop(lr=0.001,decay=1e-6)

opt=tf.train.AdamOptimizer(learning_rate=0.0001)
cif10.compile(optimizer=opt,loss=""categorical_crossentropy"", metrics=['accuracy'])


    

#Fit the model on the batches generated by datagen.flow().
history2 = cif10.fit_generator(datagen.flow(x_train, y_train_one_hot,batch_size=batch_size),
                               validation_data=(x_test,y_test_one_hot),epochs=epochs )
```

 
"
26513,Could not find any TPU devices on Colab using TF 2.0 Alpha,"

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab 
- TensorFlow installed from (source or binary): source using `pip`
- TensorFlow version (use command below): 2.0.0-alpha0
- Python version: 3.6

**Describe the current behavior**

Error occurred when try to run the colab notebook shown in [TF 2.0 Alpha: Distributed Training in TensorFlow](https://www.tensorflow.org/alpha/guide/distribute_strategy) for TPUStrategy:

```python
resolver = tf.distribute.cluster_resolver.TPUClusterResolver()
tf.tpu.experimental.initialize_tpu_system(resolver)
tpu_strategy = tf.distribute.experimental.TPUStrategy(resolver)

# output: RuntimeError: Could not find any TPU devices
# (Detailed Error message shown below)
```

I had enabled Colab runtime to `TPU`, and even checked there indeed is a TPU available:

```python
def check_tpu_statue():
    import os
    
    if 'COLAB_TPU_ADDR' not in os.environ:
      print('ERROR: Not connected to a TPU runtime')
    else:
      tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']
      print ('TPU address is', tpu_address)

check_tpu_statue()
# output: TPU address is grpc://10.70.191.234:8470
``` 

**Describe the expected behavior**

TPU devices can be found on Colab when runtime is changed to `TPU` and using:
- `tf.tpu.experimental.initialize_tpu_system(resolver)`
- `tpu_strategy = tf.distribute.experimental.TPUStrategy(resolver)`

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
!pip install tensorflow-gpu==2.0.0-alpha0
import tensorflow as tf

resolver = tf.distribute.cluster_resolver.TPUClusterResolver()
tf.tpu.experimental.initialize_tpu_system(resolver)
tpu_strategy = tf.distribute.experimental.TPUStrategy(resolver)
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```text
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-15-9ec182bf3b8d> in <module>()
      1 resolver = tf.distribute.cluster_resolver.TPUClusterResolver()
----> 2 tf.tpu.experimental.initialize_tpu_system(resolver)
      3 tpu_strategy = tf.distribute.experimental.TPUStrategy(resolver)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/tpu/tpu_strategy_util.py in initialize_tpu_system(cluster_resolver)
     89     # pylint: enable=protected-access
     90 
---> 91     with ops.device(get_first_tpu_host_device(cluster_resolver)):
     92       output = tpu_functional_ops.TPUPartitionedCall(
     93           args=[], device_ordinal=0, Tout=[dtypes.string], f=func_name)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/tpu/tpu_strategy_util.py in get_first_tpu_host_device(cluster_resolver)
     41         [x for x in context.list_devices() if ""device:TPU:"" in x])
     42     if not tpu_devices:
---> 43       raise RuntimeError(""Could not find any TPU devices"")
     44     spec = tf_device.DeviceSpec.from_string(tpu_devices[0])
     45     task_id = spec.task

RuntimeError: Could not find any TPU devices
```"
26512,Broken link in TF 2.0 Alpha: Distributed Training in TensorFlow documentation,"**System information**
- TensorFlow version: TF 2.0 Alpha
- Doc Link: https://www.tensorflow.org/alpha/guide/distribute_strategy#examples_and_tutorials

**Describe the documentation issue**

The second link (the `Tutorial` text shown below) in `Examples and Tutorials` section is missing:

```text
2. Tutorial to train Fashion MNIST with TPUStrategy (currently uses disable_eager_execution)
```

![image](https://user-images.githubusercontent.com/3454980/54072195-bdf7e180-42ba-11e9-9c09-3de5687cc016.png)

"
26511,FailedPreconditionError ,"Please help me out 

FailedPreconditionError                   Traceback (most recent call last)
<ipython-input-27-72245a7797cb> in <module>()
      6     #session.run(adapt_policy_mean, feed_dict=rl.feed_dict)
      7     sess = tf.Session()
----> 8     sess.run(adapt_policy_mean, feed_dict=rl.feed_dict)

/home/ayan/slearn/venv/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)"
26510,tf.print() support of `sep` and `end` arguments,"**System information**
- TensorFlow version (you are using):
tf.version.GIT_VERSION: 'v1.12.0-9807-ga86f6286b4'
tf.version.VERSION: '2.0.0-dev20190308'
- Are you willing to contribute it (Yes/No):
Yes

**Describe the feature and the current behavior/state.**
Right now `tf.print()` does not support the `sep` and `end` arguments of Python's `print()` function.

**Will this change the current api? How?**
Yes, it will add two more optional arguments to the `tf.print()` function.

**Who will benefit with this feature?**
Everyone who wants to use `tf.print()` just as naturally as Python's `print()` function.

**Any Other info.**
The following code demonstrates how you could use this new feature to display progress during training:

```python
import tensorflow as tf

# Uncomment these lines to see the expected output
#tf.print = print
#tf.function = lambda f: f
#tf.range = range

@tf.function
def train(n_epochs, n_steps):
    for epoch in tf.range(1, n_epochs + 1):
        tf.print(""Epoch "", epoch, ""/"", n_epochs, sep="""")
        for step in tf.range(1, n_steps + 1):
            tf.print(""\rStep "", step, ""/"", n_steps, sep="""", end="""")
            # ... training loop
        tf.print()

train(n_epochs=10, n_steps=10000)
```

The expected output looks like this:

```
Epoch 1/10
Step 10000/10000
Epoch 2/10
Step 10000/10000
Epoch 3/10
Step 10000/10000
Epoch 4/10
Step 10000/10000
Epoch 5/10
Step 10000/10000
Epoch 6/10
Step 578/10000   <= in progress
```"
26505,tf.function decorator converts ragged tensors to their dense equivalent,"**System information**
- Have I written custom code: No
- OS Platform and Distribution: Windows 10 (will also test on Ubuntu)
- TensorFlow installed from: binary
- TensorFlow version: 2.0 preview
- Python version: 3.6.6
- CUDA/cuDNN version: 10
- GPU model and memory: GTX 1070 8gb

**Describe the current behavior**
When using `@tf.function` a function with a ragged input it will return the dense equivalent of the ragged tensor.  When the decorator is omitted it returns a ragged tensor.

**Describe the expected behavior**
Return a ragged tensor regardless of the use of `@tf.function`

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
@tf.function
def do_nothing(x):
    return x

rt = tf.ragged.constant([[[1, 2, 3], [4]],
                         [[5], [], [6]],
                         [[7]],
                         [[8, 9], [10]]])

do_nothing(rt)
```
```
<tf.Tensor: id=51310, shape=(10,), dtype=int32, numpy=array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])>
```

```
# @tf.function
def do_nothing(x):
    return x

rt = tf.ragged.constant([[[1, 2, 3], [4]],
                         [[5], [], [6]],
                         [[7]],
                         [[8, 9], [10]]])

do_nothing(rt)
```
```
tf.RaggedTensor(values=<tf.RaggedTensor [[1, 2, 3], [4], [5], [], [6], [7], [8, 9], [10]]>, row_splits=tf.Tensor([0 2 5 6 8], shape=(5,), dtype=int64))
```

The ragged tensor documentation hints that this might be intended or necessary, is it?
https://www.tensorflow.org/guide/ragged_tensors#rank_and_ragged_rank"
26504,Disconnected graph when concatenating two models,"
**System information**
 Linux Ubuntu 16.04,
TensorFlow from docker image 
tf versionv1.13.1, python v3.5.2, cuda V10.0.130
gpu 1060ti  6GB

Trying to build a new model based on part of pre-trained model,

Here's some cleaned out code.

Let's imagine we got model1 trained, and want to add some layers defined in model2: 

    from tensorflow.keras.layers import  Conv2D, Activation
    from tensorflow.keras.models import Model, Sequential
    
    model1 = Sequential([
        Conv2D(2, (3,3), padding='same', input_shape=(6,6,1)),
        Activation('relu')
    ])
    model2 = Sequential([
        Conv2D(3, (3,3), padding='same', input_shape=(6,6,2)),
        Activation('softmax')
    ])
    
    model_merge = Model(inputs=model1.input, 
                        outputs=Activation('softmax')(model2(model1.get_layer('conv2d').output)))
 
It looks a bit messy, but I want to demonstrate it's not disconnected by adding a softmax activation here.

summary of model1:

    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    conv2d (Conv2D)              (None, 6, 6, 2)           20        
    _________________________________________________________________
    activation (Activation)      (None, 6, 6, 2)           0         
    =================================================================
    Total params: 20
    Trainable params: 20
    Non-trainable params: 0
    _________________________________________________________________

   
summary of model2:

    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    conv2d_4 (Conv2D)            (None, 6, 6, 3)           57        
    _________________________________________________________________
    activation_4 (Activation)    (None, 6, 6, 3)           0         
    =================================================================
    Total params: 57
    Trainable params: 57
    Non-trainable params: 0
    _________________________ 
And summary of model_merge:

    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    conv2d_input (InputLayer)    (None, 6, 6, 1)           0         
    _________________________________________________________________
    conv2d (Conv2D)              (None, 6, 6, 2)           20        
    _________________________________________________________________
    sequential_2 (Sequential)    (None, 6, 6, 3)           57        
    _________________________________________________________________
    activation_4 (Activation)    (None, 6, 6, 3)           0         
    =================================================================
    Total params: 77
    Trainable params: 77
    Non-trainable params: 0
    _________________________________________________________________

Let's prove this merged model is not disconnected:

    layers = [layer.output for layer in model_merge.layers]
    test1 = Model(inputs=model_merge.input, outputs=layers[-1])
Everything works just fine.

test1's summary:

    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    conv2d_input (InputLayer)    (None, 6, 6, 1)           0         
    _________________________________________________________________
    conv2d (Conv2D)              (None, 6, 6, 2)           20        
    _________________________________________________________________
    sequential_2 (Sequential)    (None, 6, 6, 3)           57        
    _________________________________________________________________
    activation_4 (Activation)    (None, 6, 6, 3)           0         
    =================================================================
    Total params: 77
    Trainable params: 77
    Non-trainable params: 0
    _________________________________________________________________


Here's the tragedy:

    test2 = Model(inputs=model_merge.input, outputs=layers[-2])
The most important feed back:

    ValueError: Graph disconnected: cannot obtain value for tensor Tensor(""conv2d_2_input:0"", shape=(?, 6, 6, 2), dtype=float32) at layer ""conv2d_2_input"". The following previous layers were accessed without issue: []

full feedback:

    ValueErrorTraceback (most recent call last)
    <ipython-input-18-946b325081c1> in <module>
    ----> 1 test = Model(inputs=model_merge.input, outputs=layers[-2])
    
    /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py in __init__(self, *args, **kwargs)
        119 
        120   def __init__(self, *args, **kwargs):
    --> 121     super(Model, self).__init__(*args, **kwargs)
        122     # Create a cache for iterator get_next op.
        123     self._iterator_get_next = weakref.WeakKeyDictionary()
    
    /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/network.py in __init__(self, *args, **kwargs)
         79         'inputs' in kwargs and 'outputs' in kwargs):
         80       # Graph network
    ---> 81       self._init_graph_network(*args, **kwargs)
         82     else:
         83       # Subclassed network
    
    /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/checkpointable/base.py in _method_wrapper(self, *args, **kwargs)
        440     self._setattr_tracking = False  # pylint: disable=protected-access
        441     try:
    --> 442       method(self, *args, **kwargs)
        443     finally:
        444       self._setattr_tracking = previous_value  # pylint: disable=protected-access
    
    /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/network.py in _init_graph_network(self, inputs, outputs, name)
        219     # Keep track of the network's nodes and layers.
        220     nodes, nodes_by_depth, layers, layers_by_depth = _map_graph_network(
    --> 221         self.inputs, self.outputs)
        222     self._network_nodes = nodes
        223     self._nodes_by_depth = nodes_by_depth
    
    /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/network.py in _map_graph_network(inputs, outputs)
       1850                              'The following previous layers '
       1851                              'were accessed without issue: ' +
    -> 1852                              str(layers_with_complete_input))
       1853         for x in node.output_tensors:
       1854           computable_tensors.append(x)
    
    ValueError: Graph disconnected: cannot obtain value for tensor Tensor(""conv2d_2_input:0"", shape=(?, 6, 6, 2), dtype=float32) at layer ""conv2d_2_input"". The following previous layers were accessed without issue: []

It's really driving me crazy,

Any ideas?

code to reproduce error:

    from tensorflow.keras.layers import Conv2D, Activation
    from tensorflow.keras.models import Model, Sequential
    model1 = Sequential([
        Conv2D(2, (3,3), padding='same', input_shape=(6,6,1)),
        Activation('relu')
    ])
    model2 = Sequential([
        Conv2D(3, (3,3), padding='same', input_shape=(6,6,2)),
        Activation('softmax')
    ])
    
    model_merge = Model(inputs=model1.input, 
                        outputs=Activation('softmax')(model2(model1.get_layer('conv2d').output)))
    layers = [layer.output for layer in model_merge.layers]
    test = Model(inputs=model_merge.input, outputs=layers[-2])"
26503,"Exception in thread ""main"" java.lang.UnsatisfiedLinkError: Cannot find TensorFlow native library for OS: linux, architecture: i386.","java -cp libtensorflow-1.12.0.jar:. -Djava.library.path=./jni HelloTensorFlow

Java HotSpot(TM) Server VM warning: You have loaded library /home/administrator/soft/jni/libtensorflow_jni.so which might have disabled stack guard. The VM will try to fix the stack guard now.
It's highly recommended that you fix the library with 'execstack -c <libfile>', or link it with '-z noexecstack'.
Exception in thread ""main"" java.lang.UnsatisfiedLinkError: Cannot find TensorFlow native library for OS: linux, architecture: i386. See https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java/README.md for possible solutions (such as building the library from source). Additional information on attempts to find the native library can be obtained by adding org.tensorflow.NativeLibrary.DEBUG=1 to the system properties of the JVM.
	at org.tensorflow.NativeLibrary.load(NativeLibrary.java:77)
	at org.tensorflow.TensorFlow.init(TensorFlow.java:66)
	at org.tensorflow.TensorFlow.<clinit>(TensorFlow.java:70)
	at org.tensorflow.Graph.<clinit>(Graph.java:361)
	at HelloTensorFlow.main(HelloTensorFlow.java:8)
"
26502,IDE cannot resolve module tf.keras,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 18.10
- TensorFlow installed from (source or binary):
binary (pip)
- TensorFlow version (use command below):
2.0_alpha
- Python version:
3.6

**Describe the current behavior**
`import tensorflow as tf`
then pyCharm cannot resolve module tf.keras and report an error:
`Cannot find reference 'keras' in '__init__.py'`
But when running program, everything works well.

**Describe the expected behavior**
tf.keras imported successfully with autocomplete of pyCharm.

**Code to reproduce the issue**
`import tensorflow as tf`

**Other info / logs**
It seems that there is no import command for `keras` module in `__init_.py` of tensorflow package.
When I added `from tensorflow.python import keras` to `__init__.py`manually, everything work well.
Maybe there are some problem for package importing after `keras` was moved from `_api` to `python`.


"
26501,How to use libtensorflow-lite.a and tflite model on Raspi 3?,"1. I want to install TensorFlow Lite on the Raspi. 
I'm reading the instructions to cross compile TensorFlow Lite https://www.tensorflow.org/lite/guide/build_rpi?hl=ru, but I have no idea what to do after generating libtensorflow-lite.a .

2. Also, I have downloaded and converted mobilenet_v1_1.0_224.pb to .fb format.
But how do I use the .fb model to detect any object?
"
26500,"LSTM recurrent kernel ( hidden state ) makes Host to GPU transaction for every sequence, which looks quite inefficient.","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.5 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): build from source
- TensorFlow version (use command below): v1.13.0-7-g9ca8321 1.12.0
- Python version: Python 3.6.1 :: Anaconda 4.4.0 (64-bit)
- Bazel version (if compiling from source): Build label: 0.19.2
- GCC/Compiler version (if compiling from source): gcc version 5.4.0 (GCC)
- CUDA/cuDNN version: 
I got cuda version by checking nvcc verison. it is Cuda compilation tools, release 10.0, V10.0.130.
cudnn 7.5.0
- GPU model and memory: v100

I am evaluating the inference performance of 2-level stacked LSTM model. 
As you could see in the code below, my model has 2 LSTM cell and one time-distributed (FC) layer.
Because i want to make close investigation of how inference works, i utilize google timeline tool.

**Describe the current behavior**
Whenever new sequence starts, the recurrent kernel ( hidden state of lstm cell ) makes Host to GPU data transaction. Because this data transfer is initiated by ReadVariableOp which is connected to matmul op of a lstm hidden state, i am pretty sure that a data is the weight of the hidden state.
What i am curious is that why the weight has to move from H2D? if right, where is the D2H transaction? Doesn't hidden state be renewed every new sequence?

**Describe the expected behavior**
I think there should be two kinds of correct behavior.
1. For every sequence of lstm, GPU renew the weight, and this is moved to Host memory (D2H transaction - which doesn't exist at the timeline result). And then the weight goes to GPU again at the next sequence. 
2. No Transaction. the hidden state is renewed and reused only inside the GPU memory. In this case, i could see the H2D transaction (which is observed at the timeline result)

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import tensorflow as tf
import tensorflow.keras.backend as K
import time

from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Activation, LSTM, RepeatVector, Input, TimeDistributed
from tensorflow.python.client import timeline

# configuration
iteration = 1
batch = 1024
seq_len = 26
fvec = 256#1024
unit = 256 
dtype = tf.float32
np_dtype = 'float32'

# set precision of model
K.set_floatx(np_dtype)
print(K.floatx())

# session setting
K.clear_session()
config = tf.ConfigProto() 
sess = tf.Session(config=config)
K.set_session(sess)

# make random input data
inp_dims = ( batch, seq_len, fvec)
input_data = np.random.random_sample(inp_dims).astype(np_dtype)

with tf.device('/device:GPU:0'):
	input_box = tf.placeholder(dtype, shape=(None, seq_len,fvec))
	out = LSTM(unit  , activation='sigmoid', input_shape=(seq_len, fvec),  implementation=2)(input_box) # 1st LSTM
	out = RepeatVector(seq_len)(out)
	out = LSTM(unit, activation='sigmoid', return_sequences=True, implementation=2)(out)
	out = TimeDistributed(Dense(fvec))(out)

# variable initialization
init = tf.global_variables_initializer()
sess.run(init)

# for tracing
run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
run_metadata = tf.RunMetadata()

# check time lapse
start_time = time.time()
for i in range(iteration):
  output = sess.run(out, feed_dict={input_box:input_data} , options = run_options , run_metadata= run_metadata)
  print (type(output), output.shape)
end_time = time.time()

# get trace information
tl= timeline.Timeline(run_metadata.step_stats)
ctf = tl.generate_chrome_trace_format(show_memory=True)

# store the trace information
with open (""./all_model_trace_"" + np_dtype +""_infer_0306_cpumem.json"", 'w') as f:
  f.write(ctf)

#save graph file..
sess = K.get_session()
graph_writer = tf.summary.FileWriter(""./output_4/graph"", sess.graph)
tf.train.write_graph(sess.graph, './output_4/', 'saved_model.pb', as_text=True)

# print time lapse
time_lapse = end_time - start_time
print(time_lapse)
```
"
26499,Providing a Prebuilt Binary for TFLite Benchmark,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): latest
- Are you willing to contribute it (Yes/No): Y



**Describe the feature and the current behavior/state.**
There are many people benchmarking their tflite models using the https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark . However, it seems that this does not provide a pre-built binary, so we have to build it by ourselves. Since there are many people needing to benchmark, I think the pre-built binary can benefit a lot of people and save their time. I would appreciate it if it could be done.

**Will this change the current api? How?**
N

**Who will benefit with this feature?**
Everyone who uses tflite benchmark

**Any Other info.**
N/A
"
26498,Bazel always rebuild all code even I just modify single cpp file when config=cuda is used ,"**System information**

- TensorFlow version (you are using): master branch
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
When I change any file in tensorflow/compiler, bazel always rebuild all code when config=cuda is used, I did not hit the bug after remove config=cuda.

**Will this change the current api? How?**
No.

**Who will benefit with this feature?**
Anyone who follow https://www.tensorflow.org/install/source

**Any Other info.**"
26497,[TF 2.0 API Documentation Issue] `tf.lookup.StaticHashTable` usage example is incorrect,"**System information**
- TensorFlow version: 2.0 preview
- Doc Link: [`tf.lookup.StaticHashTable`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/lookup/StaticHashTable#class_statichashtable)


**Describe the documentation issue**

The example usage describes using `StaticHashTable.init.run()`, which is not possible since there is no `init` attribute (and it's missing from the documentation). [This comment](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/ops/lookup_ops.py#L321) in `lookup_ops.py` indicates that it's definitely not the correct way to initialize the table in TF 2.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**

I don't think so, because I really have no idea what the correct way to initialize the table is :)

I've tried a few things [as shown here](https://gist.github.com/zmjjmz/81138b0d62764fabe00085cd7091dd14#file-tokenize_layer_tf2-py-L50) but to no avail, usually ending in a `FailedPreconditionError` :(
"
26496,tflite_convert -- Unknown layer: VladPooling,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OSX 10.14.3
- TensorFlow installed from (source or binary): source
- TensorFlow version (or github SHA if from source): 1.13.1


**Provide the text output from tflite_convert**

```
ValueError: Unknown layer: VladPooling
```

Also, please include a link to a GraphDef or the model if possible.
https://drive.google.com/open?id=1M_SXoW1ceKm3LghItY2ENKKUn3cWYfZm
(from: https://github.com/WeidiXie/VGG-Speaker-Recognition)

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Command line: tflite_convert --output_file=weights_keras.tflite --keras_model_file=weights_keras.h5 

Result:
Traceback (most recent call last):
  File ""/anaconda3/bin/tflite_convert"", line 11, in <module>
    sys.exit(main())
  File ""/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py"", line 442, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/anaconda3/lib/python3.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py"", line 438, in run_main
    _convert_model(tflite_flags)
  File ""/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py"", line 122, in _convert_model
    converter = _get_toco_converter(flags)
  File ""/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py"", line 109, in _get_toco_converter
    return converter_fn(**converter_kwargs)
  File ""/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/lite.py"", line 370, in from_keras_model_file
    keras_model = _keras.models.load_model(model_file)
  File ""/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/saving.py"", line 234, in load_model
    model = model_from_config(model_config, custom_objects=custom_objects)
  File ""/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/saving.py"", line 324, in model_from_config
    return deserialize(config, custom_objects=custom_objects)
  File ""/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/serialization.py"", line 74, in deserialize
    printable_module_name='layer')
  File ""/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py"", line 192, in deserialize_keras_object
    list(custom_objects.items())))
  File ""/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py"", line 1263, in from_config
    process_layer(layer_data)
  File ""/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py"", line 1249, in process_layer
    layer = deserialize_layer(layer_data, custom_objects=custom_objects)
  File ""/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/serialization.py"", line 74, in deserialize
    printable_module_name='layer')
  File ""/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py"", line 181, in deserialize_keras_object
    config, module_objects, custom_objects, printable_module_name)
  File ""/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py"", line 166, in class_and_config_for_serialized_keras_object
    raise ValueError('Unknown ' + printable_module_name + ': ' + class_name)
ValueError: Unknown layer: VladPooling"
26495,softmax_cross_entropy_with_logits_v2 doesn't support hypergradients of labels,"Current implementation of tf.nn.softmax_cross_entropy_with_logits_v2 doesn't correctly implement the  hypergradients w.r.t labels. I implemented a hypergradient demo where I used softmax_cross_entropy_with_logits_v2 to compute meta_loss, and a custom function to compute meta_loss2. We should either expect that:
```
tf.gradients(meta_loss, [a]) == tf.gradients(meta_loss2, [a])
```

Or that 
```
tf.gradients(meta_loss, [a]) == None
```

Instead, TensorFlow incorrectly returns 0s for hypergradients of softmax_cross_entropy_with_logits_v2 w.r.t labels.


Code to reproduce:
```
import tensorflow as tf


a = tf.get_variable(""a"", (2, 2)) * 10
b = tf.get_variable(""b"", (2, 2)) * 10
a2 = tf.get_variable(""a2"", (2, 2)) * 10

labels = tf.nn.softmax(a, -1)
labels2 = tf.nn.softmax(a2, -1)
loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=b))
grads = tf.gradients(loss, [b])

b_tp1 = b + grads[0]
meta_loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels2, logits=b_tp1)

logits = tf.nn.log_softmax(b, -1)
loss2 = tf.reduce_sum(-tf.reduce_sum(labels * logits))
grads2 = tf.gradients(loss2, [b])
b2_tp1 = b + grads2[0]

meta_loss2 = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels2, logits=b2_tp1)

sess = tf.Session()
sess.run(tf.global_variables_initializer())
print(sess.run([tf.gradients(meta_loss, [a]), tf.gradients(meta_loss2, [a])]))
```

**System information**
- TensorFlow-gpu 1.10.1
"
26493,Be able to save a stateful operation,"**System information**
- TensorFlow version (you are using): tf 1.13
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
Current behavior is the saver will crash trying to save a stateful operation.
Feature behavior is the saver will have an additional option to save stateful operation with the known limitation that you will loose the state of your operation.

This could be useful if you want to save the state of your dataset iterator but your pipeline contains stateful operation (random crop for example).

**Will this change the current api? How?**
Yes, by adding a new option?

**Who will benefit with this feature?**
Anyone who wants to save a stateful operation.

**Any Other info.**
"
26492,[TF 2.0 API Docs] tf.math.atan and tf.math.asin,"**System information**
- TensorFlow version: 2.0
- Doc Links:
   tf.math.atan: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math/atan
   tf.math.asin: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math/asin

**Describe the documentation issue**
Documentation for `tf.math.asin` and `tf.math.atan` is created from a generated file, `python/ops/gen_math_ops.py`. The documentation can be modified by editing the appropriate `.pbtxt` files within the `tensorflow/tensorflow/core/api_def/base_api` directory of the source repository.

Both of these math operations could use a clear description, usage examples, and a list specifying the errors raised by these operations.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
Definitely :smile: "
26490,Using Custom Loss in Keras ,"**System information**
- Uses a basic CNN MNIST Keras example
- CentOS7
- TensorFlow installed from: Anaconda
- TensorFlow version: Both 1.12-gpu and 2.0.0-alpha0-cpu
- Python version: 3.6

**Describe the current behavior**
I am trying to use a custom loss function in Keras. Ive simplified its use below, and discover that if I try to personally ""touch"" `y_pred` and `y_true` in anyway (even trivially as seen below), the model:

- Does **not** achieve high accuracy/categorgical accuracy/etc.
- **Does** achieve ""low-loss""

In the code below I try to basic approaches:

1. `customLossThatWorks` is just an alias for a built in loss - This gets me to a low loss value, and high accuracy
2. Any of the other losses that actually ""touch"" the `y_` arguments, even one that is directly copied and pasted, achieve the same low loss, but not accuracy.

**Describe the expected behavior**
Write a custom loss that works, as per the documentation.

**Code to reproduce the issue**
```
import tensorflow as tf
import tensorflow.keras.backend as K
from tensorflow.keras import datasets, layers, models

(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()

train_images = train_images.reshape((60000, 28, 28, 1))
test_images = test_images.reshape((10000, 28, 28, 1))

# Normalize pixel values to be between 0 and 1
train_images, test_images = train_images / 255.0, test_images / 255.0

def customLossThatWorks():
    return tf.keras.losses.sparse_categorical_crossentropy

# def customLoss(y_true, y_pred):
#     return K.sparse_categorical_crossentropy(y_true, y_pred)

# def customLoss():
#     def loss(y_true,y_pred):
#         return K.sparse_categorical_crossentropy(y_true, y_pred)
#     return loss
    
# def customLoss():
#     def loss(y_true,y_pred):
#         return tf.keras.losses.sparse_categorical_crossentropy(y_true,y_pred)
#     return loss

# copied from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/losses.py#L867
def customLoss(y_true, y_pred, from_logits=False, axis=-1):
    return K.sparse_categorical_crossentropy(
      y_true, y_pred, from_logits=from_logits, axis=axis)

model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(10, activation='softmax'))
model.compile(loss=customLoss, optimizer='adam', metrics=['accuracy', 'categorical_accuracy'])
model.fit(train_images, train_labels, epochs=1)
```

**Other info / logs**
* **Note**: If the custom loss function handles `y_pred` and `y_true` directly, you just pass the function without invoking it: `=customLoss` vs `=customLoss()`


* Run the above, commenting out different versions of the loss and changing the compilation line accordingly: `model.compile(loss=customLoss(), optimizer='adam', metrics=['accuracy', 'categorical_accuracy'])` to observe the behavior
* The answer in #16721 and the [SO question](https://stackoverflow.com/questions/48654851/customized-loss-in-tensorflow-with-keras) generated from it, don't address this problem."
26488,TOCO reports SymbolAlreadyExposedError:  Symbol AttrValue is already exposed as ().,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Arch Linux 4.20.1-arch1-1-ARCH
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary):
Source
- TensorFlow version (use command below):
`master` (reproduction instructions below)
- Python version:
3.7.2
- Bazel version (if compiling from source):
0.21.0-1
- GCC/Compiler version (if compiling from source):
8.2.1
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
Running `toco` with or without arguments throws this traceback:
```
$ toco
Traceback (most recent call last):
  File ""/auxiliary//home/ajarthurs/Development/data/python/bin/toco"", line 6, in <module>
    from tensorflow.lite.python.tflite_convert import main
  File ""/auxiliary/home/ajarthurs/Development/data/python/tensorflow/__init__.py"", line 33, in <module>
    from tensorflow_estimator import __path__ as _new_path
  File ""/auxiliary/home/ajarthurs/Development/data/python/tensorflow_estimator/__init__.py"", line 8, in <module>
    from tensorflow_estimator._api.v1 import estimator
  File ""/auxiliary/home/ajarthurs/Development/data/python/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>
    from tensorflow_estimator._api.v1.estimator import experimental
  File ""/auxiliary/home/ajarthurs/Development/data/python/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>
    from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder
  File ""/auxiliary/home/ajarthurs/Development/data/python/tensorflow_estimator/python/estimator/canned/dnn.py"", line 23, in <module>
    from tensorflow.python.feature_column import feature_column
  File ""/auxiliary/home/ajarthurs/Development/data/python/tensorflow_core/python/__init__.py"", line 157, in <module>
    tf_export(v1=['AttrValue'])(AttrValue)
  File ""/auxiliary/home/ajarthurs/Development/data/python/tensorflow_core/python/util/tf_export.py"", line 334, in __call__
    self.set_attr(undecorated_func, api_names_attr, self._names)
  File ""/auxiliary/home/ajarthurs/Development/data/python/tensorflow_core/python/util/tf_export.py"", line 346, in set_attr
    (func.__name__, getattr(func, api_names_attr)))  # pylint: disable=protected-access
tensorflow_core.python.util.tf_export.SymbolAlreadyExposedError: Symbol AttrValue is already exposed as ().
```

**Describe the expected behavior**
```
$ toco
usage: toco [-h] --output_file OUTPUT_FILE
            (--graph_def_file GRAPH_DEF_FILE | --saved_model_dir SAVED_MODEL_DIR | --keras_model_file KERAS_MODEL_FILE)
            [--output_format {TFLITE,GRAPHVIZ_DOT}]
            [--inference_type {FLOAT,QUANTIZED_UINT8}]
            [--inference_input_type {FLOAT,QUANTIZED_UINT8}]
            [--input_arrays INPUT_ARRAYS] [--input_shapes INPUT_SHAPES]
            [--output_arrays OUTPUT_ARRAYS]
            [--saved_model_tag_set SAVED_MODEL_TAG_SET]
            [--saved_model_signature_key SAVED_MODEL_SIGNATURE_KEY]
            [--std_dev_values STD_DEV_VALUES] [--mean_values MEAN_VALUES]
            [--default_ranges_min DEFAULT_RANGES_MIN]
            [--default_ranges_max DEFAULT_RANGES_MAX]
            [--post_training_quantize] [--drop_control_dependency]
            [--reorder_across_fake_quant]
            [--change_concat_input_ranges {TRUE,FALSE}] [--allow_custom_ops]
            [--target_ops TARGET_OPS] [--dump_graphviz_dir DUMP_GRAPHVIZ_DIR]
            [--dump_graphviz_video]
toco: error: the following arguments are required: --output_file
```

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

This issue first appeared when I switched to TF commit 13f022a946c7fbfeb1786488d278b675bd027a87. TOCO worked fine when I was on TF commit 12606ff846566dd842444f27f7fed4f911a58580.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26487,"Using ""for input_image in tf.data.Dataset: "" TypeError: zip argument #1 must support iteration ","I try using the tf,data.Dataset to load my own images from local directory.

But when I want to feed batch_size data to model, I got a TypeError: zip argument #1 must support iteration

Here is my code

```
Import tensorflow as tf
tf.enable_eager_execution()


import os
import time
import numpy as np
import matplotlib.pyplot as plt
import PIL
from IPython.display import clear_output

train_dataset = tf.data.Dataset.list_files(image_path + 'train_imgs/*png')
train_dataset = train_dataset.shuffle(200)
train_dataset = train_dataset.map(lambda x: load_image(x))
train_dataset = train_dataset.batch(16)

val_dataset = tf.data.Dataset.list_files(image_path + 'val_imgs/*.png')
val_dataset = val_dataset.shuffle(200)
val_dataset = val_dataset.map(lambda x: load_image(x))
val_dataset = val_dataset.batch(16)

test_dataset = tf.data.Dataset.list_files(image_path + 'test_imgs/*.png')
test_dataset = test_dataset.shuffle(200)
test_dataset = test_dataset.map(lambda x: load_image(x))
test_dataset = test_dataset.batch(16)

# train/test dataset batch shape: (16, 256, 256, 1)

class Downsample(tf.keras.Model):
    def __init__(self, filters, size, apply_batchnorm=True):
        super(Downsample, self).__init__()
        self.apply_batchnorm = apply_batchnorm
        
        initializer = tf.random_normal_initializer(0., 0.02)
        
        self.conv1 = tf.keras.layers.Conv2D(filters, 
                                            (size, size), 
                                            strides=2, 
                                            padding='same', 
                                            kernel_initializer=initializer)
        if self.apply_batchnorm:
            self.batchnorm = tf.keras.layers.BatchNormalization()
    
    def call(self, x, training):
        x = self.conv1(x)
        if self.apply_batchnorm:
            x = self.batchnorm(x, training=training)
        x = tf.nn.leaky_relu(x)
        return x

class Upsample(tf.keras.Model):
    def __init__(self, filters, size, apply_dropout=False):
        super(Upsample, self).__init__()
        self.apply_dropout = apply_dropout
        initializer = tf.random_normal_initializer(0., 0.02)
        
        self.up_conv = tf.keras.layers.Conv2DTranspose(filters,
                                                      (size, size),
                                                      strides=2,
                                                      padding='same',
                                                      kernel_initializer=initializer,
                                                      use_bias=False)
        self.batchnorm = tf.keras.layers.BatchNormalization()
        if self.apply_dropout:
            self.dropout = tf.keras.layers.Dropout(0.5)
    
    def call(self, x, training):
        x = self.up_conv(x)
        x = self.batchnorm(x, training=training)
        if self.apply_dropout:
            x = self.dropout(x, training=training)
        x = tf.nn.relu(x)
        return x

class Network(tf.keras.Model):
    def __init__(self):
        super(Network, self).__init__()
        initializer = tf.random_normal_initializer(0., 0.02)
        
        self.down1 = Downsample(64, 4, apply_batchnorm=False)
        self.down2 = Downsample(128, 4)
        self.down3 = Downsample(256, 4)
        self.down4 = Downsample(512, 4)
        self.down5 = Downsample(512, 4)
        self.down6 = Downsample(512, 4)
        self.down7 = Downsample(512, 4)
        self.down8 = Downsample(512, 4)
        
        self.up1 = Upsample(512, 4, apply_dropout=True)
        self.up2 = Upsample(512, 4, apply_dropout=True)
        self.up3 = Upsample(512, 4, apply_dropout=True)
        self.up4 = Upsample(512, 4)
        self.up5 = Upsample(256, 4)
        self.up6 = Upsample(128 ,4)
        self.up7 = Upsample(64, 4)
        
        self.last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS,
                                                   (4, 4),
                                                   strides=2,
                                                   padding='same',
                                                   kernel_initializer=initializer)
        
        @tf.contrib.eager.defun
        def call(self, x, training):
            # x shape == (bs, 256, 256, 3)    
            x1 = self.down1(x, training=training) # (bs, 128, 128, 64)
            x2 = self.down2(x1, training=training) # (bs, 64, 64, 128)
            x3 = self.down3(x2, training=training) # (bs, 32, 32, 256)
            x4 = self.down4(x3, training=training) # (bs, 16, 16, 512)
            x5 = self.down5(x4, training=training) # (bs, 8, 8, 512)
            x6 = self.down6(x5, training=training) # (bs, 4, 4, 512)
            x7 = self.down7(x6, training=training) # (bs, 2, 2, 512)
            x8 = self.down8(x7, training=training) # (bs, 1, 1, 512)

            x9 = self.up1(x8, x7, training=training) # (bs, 2, 2, 1024)
            x10 = self.up2(x9, x6, training=training) # (bs, 4, 4, 1024)
            x11 = self.up3(x10, x5, training=training) # (bs, 8, 8, 1024)
            x12 = self.up4(x11, x4, training=training) # (bs, 16, 16, 1024)
            x13 = self.up5(x12, x3, training=training) # (bs, 32, 32, 512)
            x14 = self.up6(x13, x2, training=training) # (bs, 64, 64, 256)
            x15 = self.up7(x14, x1, training=training) # (bs, 128, 128, 128)

            x16 = self.last(x15) # (bs, 256, 256, 3)
            x16 = tf.nn.tanh(x16)
            
            return x16

def train(dataset, epochs):
    for epoch in range(epochs):
        start = time.time()
        
        for input_image in dataset:
#             print(input_image.shape)
            with tf.GradientTape() as net_tape:
                output = net(input_image, training=True)
                loss = net_loss(input_image, output)
                
            net_gradients = net_tape.gradient(loss, net.variables)
            optimizer.apply_gradients(zip(net_gradients, net.variables))
        
        if epoch % 1 == 0:
            clear_output(wait=True)
            for inp, tar in test_dataset.take(1):
                generate_images(net, inp, tar)
        
        # saving checkpoint evey 20 epochs
        if (epoch + 1) % 20 == 0:
            checkpoint.save(file_prefix=checkpoint_prefix)
        
        print('Time taken for epoch {} is {} sec\n'.format(epoch+1, time.time()-start))

train(train_dataset, epochs)`

Now I got a Error:

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-36-d0e3ab41d250> in <module>()
----> 1 train(train_dataset, epochs)

<ipython-input-35-58c1c7be4acb> in train(dataset, epochs)
      6 #             print(input_image.shape)
      7             with tf.GradientTape() as net_tape:
----> 8                 output = net(input_image, training=True)
      9                 loss = net_loss(input_image, output)
     10 

/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    701 
    702       if not in_deferred_mode:
--> 703         outputs = self.call(inputs, *args, **kwargs)
    704         if outputs is None:
    705           raise ValueError('A layer\'s `call` method should return a Tensor '

/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/network.py in call(self, inputs, training, mask)
    718     outputs, _ = self._run_internal_graph(inputs,
    719                                           training=training,
--> 720                                           mask=masks)
    721     return outputs
    722 

/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/network.py in _run_internal_graph(self, inputs, training, mask)
    853     # does not return a list the same size as `call`
    854     tensor_map = {}
--> 855     for x, y, mask in zip(self.inputs, inputs, masks):
    856       tensor_map[str(id(x))] = (y, mask)
    857 

TypeError: zip argument #1 must support iteration


input_image Tensor shape is (16, 256, 256, 1)"
26486,tf_upgrade_v2  fails if the file contains f-strings and gives pasta.base.annotate.AnnotationError,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): pip install tensorflow==2.0.0-alpha0
- TensorFlow version (use command below): 2.0.0-alpha0
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior**
`tf_upgrade_v2` fails if the file contains f-strings

**Describe the expected behavior**
`tf_upgrade_v2` does not fails if the file contains f-strings

**Code to reproduce the issue**
File `foo.py`:
```
print(f'tf_upgrade_v2 fails to convert f-strings, like this one: {42}')
```
Command that produces the error: `tf_upgrade_v2 --infile foo.py --outfile foo_tf20.py`

**Other info / logs**
```
Traceback (most recent call last):
  File ""/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py"", line 1161, in visit
    super(AstAnnotator, self).visit(node)
  File ""/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py"", line 127, in visit
    super(BaseVisitor, self).visit(node)
  File ""/anaconda3/envs/tf2.0/lib/python3.6/ast.py"", line 253, in visit
    return visitor(node)
  File ""/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py"", line 47, in wrapped
    f(self, node, *args, **kwargs)
  File ""/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py"", line 1213, in visit_Num
    self.attr(node, 'content', contentargs, deps=('n',), default=str(node.n))
  File ""/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py"", line 1352, in attr
    attr_parts.append(attr_val())
  File ""/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py"", line 1210, in <lambda>
    contentargs = [lambda: self.tokens.next_of_type(token_number_type).src]
  File ""/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/token_generator.py"", line 347, in next_of_type
    self.lines[token.start[0] - 1]))
ValueError: Expected 'NUMBER' but found ')'
line 1: print(f'tf_upgrade_v2 fails to convert f-strings, like this one: {42}')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/anaconda3/envs/tf2.0/bin/tf_upgrade_v2"", line 10, in <module>
    sys.exit(main())
  File ""/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow/tools/compatibility/tf_upgrade_v2_main.py"", line 110, in main
    args.input_file, output_file, upgrade)
  File ""/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow/tools/compatibility/tf_upgrade_v2_main.py"", line 33, in process_file
    upgrader.process_file(in_filename, out_filename)
  File ""/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow/tools/compatibility/ast_edits.py"", line 494, in process_file
    temp_file)
  File ""/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow/tools/compatibility/ast_edits.py"", line 548, in process_opened_file
    self.update_string_pasta("""".join(lines), in_filename))
  File ""/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow/tools/compatibility/ast_edits.py"", line 510, in update_string_pasta
    t = pasta.parse(text)
  File ""/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/__init__.py"", line 25, in parse
    annotator.visit(t)
  File ""/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py"", line 1161, in visit
    super(AstAnnotator, self).visit(node)
  File ""/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py"", line 127, in visit
    super(BaseVisitor, self).visit(node)
  File ""/anaconda3/envs/tf2.0/lib/python3.6/ast.py"", line 253, in visit
    return visitor(node)
  File ""/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py"", line 47, in wrapped
    f(self, node, *args, **kwargs)
  File ""/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py"", line 211, in visit_Module
    self.generic_visit(node)
  File ""/anaconda3/envs/tf2.0/lib/python3.6/ast.py"", line 261, in generic_visit
    self.visit(item)
  File ""/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py"", line 1161, in visit
    super(AstAnnotator, self).visit(node)
  File ""/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py"", line 127, in visit
    super(BaseVisitor, self).visit(node)
  File ""/anaconda3/envs/tf2.0/lib/python3.6/ast.py"", line 253, in visit
    return visitor(node)
  File ""/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py"", line 47, in wrapped
    f(self, node, *args, **kwargs)
  File ""/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py"", line 574, in visit_Expr
    self.visit(node.value)
  File ""/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py"", line 1161, in visit
    super(AstAnnotator, self).visit(node)
  File ""/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py"", line 127, in visit
    super(BaseVisitor, self).visit(node)
  File ""/anaconda3/envs/tf2.0/lib/python3.6/ast.py"", line 253, in visit
    return visitor(node)
  File ""/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py"", line 47, in wrapped
    f(self, node, *args, **kwargs)
  File ""/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py"", line 703, in visit_Call
    any_args = self.visit_Call_arguments35(node)
  File ""/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py"", line 770, in visit_Call_arguments35
    self.visit(arg)
  File ""/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py"", line 1161, in visit
    super(AstAnnotator, self).visit(node)
  File ""/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py"", line 127, in visit
    super(BaseVisitor, self).visit(node)
  File ""/anaconda3/envs/tf2.0/lib/python3.6/ast.py"", line 253, in visit
    return visitor(node)
  File ""/anaconda3/envs/tf2.0/lib/python3.6/ast.py"", line 261, in generic_visit
    self.visit(item)
  File ""/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py"", line 1161, in visit
    super(AstAnnotator, self).visit(node)
  File ""/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py"", line 127, in visit
    super(BaseVisitor, self).visit(node)
  File ""/anaconda3/envs/tf2.0/lib/python3.6/ast.py"", line 253, in visit
    return visitor(node)
  File ""/anaconda3/envs/tf2.0/lib/python3.6/ast.py"", line 263, in generic_visit
    self.visit(value)
  File ""/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py"", line 1163, in visit
    raise AnnotationError(e)
pasta.base.annotate.AnnotationError: Expected 'NUMBER' but found ')'
line 1: print(f'tf_upgrade_v2 fails to convert f-strings, like this one: {42}')
```"
26484,Android / TFlite call results in a NPE,"**System information**


- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04):**
Linux Ubuntu 18.04
- **TensorFlow installed from (source or binary):**
Source
- **TensorFlow version training (or github SHA if from source):**
1.11.0 + GPU support
- **custom model**
based on ssdlite_mobilenet_v2_coco (https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md)
- **TensorFlow version convert (or github SHA if from source):**
tried with 1.11 and 1.12

we are trying to do object detection in an android app. To do this we used a ssdlite_mobilenet_v2_coco pretrained network and continued training our own dataset.

We created the tflite model using these scripts:

**tflite_convert**

```
python3 ~/tensorflow/models/research/object_detection/export_tflite_ssd_graph.py \
--pipeline_config_path input/ssdlite_mobilenet_v2_coco/pipeline.config \
--trained_checkpoint_prefix input/ssdlite_mobilenet_v2_coco/model.ckpt-381700 \
--output_directory output/ \
--add_postprocessing_op=true

tflite_convert \
--output_file=output/ssdlite_mobilenet_v2_coco.tflite \
--graph_def_file=input/ssdlite_mobilenet_v2_coco.pb \
--input_arrays=FLOAT \
--output_arrays=concat,concat_1 \
--input_shape=1,300,300,3
```
What this app basically does, it takes a pre-recorded video, decodes it frame-by-frame using FFmpegMediaMetadataRetriever and passes the bitmap into tflite to detect objects there. The app is built with gradle and we are using 'org.tensorflow:tensorflow-lite:1.12.0', but we basically get the same error with 1.11.

We scale the bitmap down to 300x300 and convert it from ARGB into 3 float channels and call tflite like this:

```
og.v(TAG, ""Feeding TFLite"")
outputLocations = Array(1) { Array(NUM_DETECTIONS) { FloatArray(4) } }
outputClasses = Array(1) { FloatArray(NUM_DETECTIONS) }
outputScores = Array(1) {FloatArray(NUM_DETECTIONS)}
numDetections = FloatArray(1)

val inputArray = arrayOf<Any>(imgData!!)
val outputMap = HashMap<Int, Any>()
outputMap.put(0, outputLocations!!)
outputMap.put(1, outputClasses!!)
outputMap.put(2, outputScores!!)
outputMap.put(3, numDetections!!)


Log.v(TAG, ""Running TFLite"")
tflite!!.runForMultipleInputsOutputs(inputArray, outputMap)
Log.v(TAG, ""Returning from TFLite"")

val recognitions = ArrayList<Recognition>(NUM_DETECTIONS)
```
The error we are getting is:

```
2019-02-28 11:52:33.486 26807-26879/com.package.xxxxxx A/libc: Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x0 in tid 26879 (.xxxxxx), pid 26807 (.xxxxxx)
2019-02-28 11:52:33.600 26890-26890/? A/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
2019-02-28 11:52:33.601 26890-26890/? A/DEBUG: Build fingerprint: 'google/sdk_gphone_x86/generic_x86:9/PSR1.180720.075/5124027:userdebug/dev-keys'
2019-02-28 11:52:33.601 26890-26890/? A/DEBUG: Revision: '0'
2019-02-28 11:52:33.601 26890-26890/? A/DEBUG: ABI: 'x86'
2019-02-28 11:52:33.604 26890-26890/? A/DEBUG: pid: 26807, tid: 26879, name: .xxxxxx  >>> com.package.xxxxxx <<<
2019-02-28 11:52:33.604 26890-26890/? A/DEBUG: signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x0
2019-02-28 11:52:33.604 26890-26890/? A/DEBUG: Cause: null pointer dereference
2019-02-28 11:52:33.604 26890-26890/? A/DEBUG:     eax 00000000  ebx 00000000  ecx 00000000  edx 00000000
2019-02-28 11:52:33.604 26890-26890/? A/DEBUG:     edi c75094a8  esi 00000000
2019-02-28 11:52:33.604 26890-26890/? A/DEBUG:     ebp c75090f8  esp c7509070  eip c757c77b
2019-02-28 11:52:33.606 26890-26890/? A/DEBUG: backtrace:
2019-02-28 11:52:33.606 26890-26890/? A/DEBUG:     #00 pc 0007277b  /data/app/com.package.xxxxxx-yGil4a0ylttBBhPIo26SWA==/lib/x86/libtensorflowlite_jni.so
2019-02-28 11:52:33.606 26890-26890/? A/DEBUG:     #01 pc 00074fe0  /data/app/com.package.xxxxxx-yGil4a0ylttBBhPIo26SWA==/lib/x86/libtensorflowlite_jni.so
2019-02-28 11:52:33.606 26890-26890/? A/DEBUG:     #02 pc 0007590d  /data/app/com.package.xxxxxx-yGil4a0ylttBBhPIo26SWA==/lib/x86/libtensorflowlite_jni.so
2019-02-28 11:52:33.606 26890-26890/? A/DEBUG:     #03 pc 000755b0  /data/app/com.package.xxxxxx-yGil4a0ylttBBhPIo26SWA==/lib/x86/libtensorflowlite_jni.so
2019-02-28 11:52:33.606 26890-26890/? A/DEBUG:     #04 pc 00076322  /data/app/com.package.xxxxxx-yGil4a0ylttBBhPIo26SWA==/lib/x86/libtensorflowlite_jni.so
2019-02-28 11:52:33.606 26890-26890/? A/DEBUG:     #05 pc 0013389c  /data/app/com.package.xxxxxx-yGil4a0ylttBBhPIo26SWA==/lib/x86/libtensorflowlite_jni.so
2019-02-28 11:52:33.607 26890-26890/? A/DEBUG:     #06 pc 00132fa7  /data/app/com.package.xxxxxx-yGil4a0ylttBBhPIo26SWA==/lib/x86/libtensorflowlite_jni.so
2019-02-28 11:52:33.607 26890-26890/? A/DEBUG:     #07 pc 00132e37  /data/app/com.package.xxxxxx-yGil4a0ylttBBhPIo26SWA==/lib/x86/libtensorflowlite_jni.so
2019-02-28 11:52:33.607 26890-26890/? A/DEBUG:     #08 pc 0016550e  /data/app/com.package.xxxxxx-yGil4a0ylttBBhPIo26SWA==/lib/x86/libtensorflowlite_jni.so
2019-02-28 11:52:33.607 26890-26890/? A/DEBUG:     #09 pc 0008f065  /system/lib/libc.so (__pthread_start(void*)+53)
2019-02-28 11:52:33.607 26890-26890/? A/DEBUG:     #10 pc 0002485b  /system/lib/libc.so (__start_thread+75)
2019-02-28 11:52:34.197 1761-1761/? E//system/bin/tombstoned: Tombstone written to: /data/tombstones/tombstone_35
```
As you can see the NPE occurs deep in the libtensorflow and we are basically running out of ideas what we could do to fix it, so any help is appreciated. It happens on both a physical device and the android sandbox (API 28)

We used https://github.com/baxterai/tfliteSSDminimalWorkingExample/blob/master/android/tfliteSSDminimalWorkingExample/app/src/main/java/com/example/user/tflitessdminimalworkingexample/TFLiteObjectDetectionAPIModel.java as a starting point and also the Tensorflow tflite demo form the tensorflow repository.
"
26482,ImportError: cannot import name 'Activation',"Hi,

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No. Just importing.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Manjaro
- TensorFlow installed from (source or binary):
via pip install tensorflow
- TensorFlow version (use command below):
1.13.1
- Python version:
3.6

**Describe the current behavior**

with tensorflow-1.13.1 one cannot import keras.Model and keras.layer with

from tensorflow.keras import Model

-> this gives the following error:

    from tensorflow.python.keras.applications.densenet import Activation; ImportError: cannot import name 'Activation'

**Describe the expected behavior**
Expected that I can do the following import:
from tensorflow.keras import Model


**Other info / logs**

However, importing with

from tensorflow import keras 

and then use keras.Model or keras.layer works fine"
26481,"Tensorflow error, sess.run(); Iris dataset for logistics regression","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- Windows 10:
- TensorFlow installed from conda-forge:
- TensorFlow version 1.5.0:
- Python version 3.6


I am trying to perform logistic regression on the iris dataset that I got from UCL, i am using tensorflow and i feel it is giving me the error in the feed_dict part of the error line. I tried changing the shape of the the x_input and y_input according to the error that occurred using np.reshape() but it didn't help.


```
`#training data
x_input=data.loc[0:105, ['SEPAL LENGTH','SEPAL WIDTH','PETAL LENGTH','PETAL WIDTH']]
temp=data['FLOWER']
y_input=temp[0:106]
#test data
x_test=data.loc[106:149, ['SEPAL LENGTH','SEPAL WIDTH','PETAL LENGTH','PETAL WIDTH']]
y_test=temp[106:150]

#placeholders and variables. input has 4 features and output has 3 classes
x=tf.placeholder(tf.float32,shape=[None, 4])
y_=tf.placeholder(tf.float32,shape=[None, 3])
#weight and bias
W=tf.Variable(tf.zeros([4,3]))
b=tf.Variable(tf.zeros([3]))


# X is placeholdre for iris features. We will feed data later on
X = tf.placeholder(tf.float32, [None, 4])
# y is placeholder for iris labels. We will feed data later on
Y = tf.placeholder(tf.float32, [3, None])

w = tf.Variable(tf.zeros([4,3]))
b = tf.Variable(tf.zeros([3]))


# model 
#softmax function for multiclass classification
y = tf.nn.softmax(tf.matmul(x, W) + b)
#loss function
cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))
#optimiser -
train_step = tf.train.AdamOptimizer(0.01).minimize(cross_entropy)
#calculating accuracy of our model 
correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
#session parameters
sess = tf.InteractiveSession()
#initialising variables
init = tf.initialize_all_variables()
sess.run(init)
#number of interations
epochs=2000

for step in range(epochs):
           _, c=sess.run([train_step,cross_entropy], feed_dict={x: x_input.as_matrix(), y_:y_input.as_matrix()}) #error line
           if step%500==0:
               print(c)`
```

Error stack looks like this

```
`ValueError                                Traceback (most recent call last)
<ipython-input-13-aa781a046f65> in <module>
      1 for step in range(epochs):
----> 2            _, c=sess.run([train_step,cross_entropy], feed_dict={x: x_input.as_matrix(), y_:y_input.as_matrix()})
      3            if step%500==0:
      4                print(c)

c:\users\hp\anaconda3\envs\maskrcnn\lib\site-packages\tensorflow\python\client\session.py in run(self, fetches, feed_dict, options, run_metadata)
    893     try:
    894       result = self._run(None, fetches, feed_dict, options_ptr,
--> 895                          run_metadata_ptr)
    896       if run_metadata:
    897         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

c:\users\hp\anaconda3\envs\maskrcnn\lib\site-packages\tensorflow\python\client\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1102                 'Cannot feed value of shape %r for Tensor %r, '
   1103                 'which has shape %r'
-> 1104                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))
   1105           if not self.graph.is_feedable(subfeed_t):
   1106             raise ValueError('Tensor %s may not be fed.' % subfeed_t)

ValueError: Cannot feed value of shape (106,) for Tensor 'Placeholder_1:0', which has shape '(?, 3)'`


```"
26480,Where is gen_experimental_dataset_ops?,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 2.0.0


**Provide the text output from tflite_convert**

```
from tensorflow.python.ops import gen_experimental_dataset_ops
```
in `tensorflow/python/data/experimental/ops/batching.py`
Is it the `gen_experimental_dataset_ops` disappeared? I can not find where the `gen_experimental_dataset_ops` is defined.
"
26479,[TF2.0] Histogram summary unexpected keyword 'values' in summary_v2.py,"
**System information**
- OS Platform and Distribution: Linux 18.04
- TensorFlow version 2.0-alpha
- Python 3.6.8

**Describe the current behavior**

Trying to write any tensor as histogram summary:

> /tmp/cpu-env/lib/python3.6/site-packages/tensorboard/plugins/histogram/summary_v2.py in _buckets(data, bucket_count)
     89   if bucket_count is None:
     90     bucket_count = DEFAULT_BUCKET_COUNT
---> 91   with tf.name_scope('buckets', values=[data, bucket_count]):
     92     tf.debugging.assert_scalar(bucket_count)
     93     tf.debugging.assert_type(bucket_count, tf.int32)
TypeError: __init__() got an unexpected keyword argument 'values'

**Code to reproduce the issue**
`tf.summary.histogram('SampleKernel', tf.random.normal(shape=[3,3,3,32], dtype=tf.float32))`
"
26478,Met unsupported operator of type Cast when quantize model get from quantize aware training,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
yes, i tried to use quantize aware training, post training quantization and tflite converter to train and convert a custom mobilenetv2 model
- OS Platform and Distribution :
macos 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
None
- TensorFlow installed from (source or binary):
conda install tensorflow
- TensorFlow version (use command below):
tensorflow 1.13
- Python version:
python 3.6.8
- Bazel version (if compiling from source):
none
- GCC/Compiler version (if compiling from source):
none
- CUDA/cuDNN version:
none
- GPU model and memory:
none


i was training a mobilenetv2 model, using quantize aware training defined in tf.contrib.quantize, following the direction in [https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantize](url)

 tflite convert was used to convert the quantize aware trained model to 8bit fixed point. everything was fine until i saved frozen graph. i then use toco command line tool, and got this error massage:
""2019-03-08 16:13:24.208096: F ./tensorflow/lite/toco/toco_tooling.h:38] Check failed: s.ok() Unimplemented: this graph contains an operator of type Cast for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).
Fatal Python error: Aborted""

i also tried tflite converter python api, convert from session, convert from frozen graph and convert from saved models, and all got the same error message. by printing out ops in the gragh, i found some ""Cast"" ops in the graph, in batch norm layers mostly. i guess these cast operators were used building training graphs and folding batch norms, and should be deleted when building eval graph or freezing the graph. but i still get them in my frozen graph, so i would like to ask for some help to get rid of them (and other unsupported ops) and convert the model to 8bit

the nodes in graph_def is saved here [https://www.dropbox.com/s/mev825dzrzh7hrv/log.txt?dl=0](url) 
saved frozen graph is available here [https://www.dropbox.com/s/8xbropujwwxm5tb/frozen_graph.pb?dl=0](url)



"
26477,ImportError,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version: 1.13
- Python version: 3.7
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1/7.5
- GPU model and memory: Nvidia GeForce 1060 6GB



**Describe the problem**
When I run my code, I get the error included here.
**Provide the exact sequence of commands / steps that you executed before running into the problem**
import numpy as np
import tensorflow as tf

**Any other info / logs**
Exception has occurred: ImportError
Traceback (most recent call last):   File ""C:\Users\Jonny\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>     from tensorflow.python.pywrap_tensorflow_internal import *   File ""C:\Users\Jonny\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>     _pywrap_tensorflow_internal = swig_import_helper()   File ""C:\Users\Jonny\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)   File ""C:\Users\Jonny\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 242, in load_module     return load_dynamic(name, filename, file)   File ""C:\Users\Jonny\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 342, in load_dynamic     return _load(spec) ImportError: DLL load failed: The specified module could not be found.   Failed to load the native TensorFlow runtime.  See https://www.tensorflow.org/install/errors  for some common reasons and solutions.  Include the entire stack trace above this error message when asking for help.
"
26476,Code cifar10 uses CPU instead of GPU,"`
import time
time.sleep(1.5)
from keras.datasets import cifar10
from matplotlib import pyplot
from scipy.misc import toimage

import numpy
from keras.datasets import cifar10
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.layers import Flatten
from keras.constraints import maxnorm
from keras.optimizers import SGD
from keras.layers.convolutional import Conv2D
from keras.layers.convolutional import MaxPooling2D
from keras.utils import np_utils
from keras import backend as K
K.set_image_dim_ordering('th')

seed = 7
numpy.random.seed(seed)

(X_train, y_train), (X_test, y_test) = cifar10.load_data()


X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
X_train = X_train / 255.0
X_test = X_test / 255.0

y_train = np_utils.to_categorical(y_train)
y_test = np_utils.to_categorical(y_test)
num_classes = y_test.shape[1]

model = Sequential()
model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), padding='same', activation='relu', kernel_constraint=maxnorm(3)))
model.add(Dropout(0.2))
model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Flatten())
model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation='softmax'))

epochs = 25
lrate = 0.01
decay = lrate/epochs
sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)
model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
print(model.summary())


model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=32)

scores = model.evaluate(X_test, y_test, verbose=0)
print(""Accuracy: %.2f%%"" % (scores[1]*100))`


**OUTPUT OF CODE**

Using TensorFlow backend.
2019-03-08 13:22:50.598025: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-03-08 13:22:50.617421: E tensorflow/stream_executor/cuda/cuda_driver.cc:300] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2019-03-08 13:22:50.617482: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] retrieving CUDA diagnostic information for host: linuxbox-1
2019-03-08 13:22:50.617489: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:170] hostname: linuxbox-1
2019-03-08 13:22:50.617516: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:194] libcuda reported version is: 390.116.0
2019-03-08 13:22:50.617533: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:198] kernel reported version is: 390.77.0
2019-03-08 13:22:50.617539: E tensorflow/stream_executor/cuda/cuda_diagnostics.cc:308] kernel version 390.77.0 does not match DSO version 390.116.0 -- cannot find working devices in this configuration


can anyone help me with performance?@tensorflow"
26474,Android TFLite benchmark performance issue with deeplab segmentation model (DepthwiseConv2d),"**System information**
The benchmark tests were carried out using the following tools and devices:-
Bazel version:Build label: 0.23.1
Tensorflow Version: 1.13.0
Android Device: OnePlus 3


**Describe the current behavior**
We trained a segmentation model using deeplab with mobilenet with TF 1.13.0 to replicate the
segmentation model by tensorflow i.e 'deeplabv3_257_mv_gpu.tflite'(Using sample code  in repo for pascal voc).
However there was significant time difference for average running time for 'DepthwiseConv2d', when we benchmarked the two tflite models with 'tflite android benchmark tool'.
In the official model avg time was around 28 ms; whereas in our own model it was around 103 ms.
The only difference between the two float models being the quantization levels i.e Ours was 0-255
and official model has -1 to 0.99.Also, there is a difference in model size(Official:2.7Mb vs. Ours:3.3Mb).

It seems the official model is using a newer kernel for depthwiseconv2d and hence runs significantly faster compared to our trained model.Was the official model trained or optimised using TF2.0 tools?What could be done to achieve similar speed (DepthwiseConv2d) using TF 1.13.0 or do we have to migrate and retrain our model using TF 2.0?

**Describe the expected behavior**
The depthwise conv2d performance should be same in the official model and the replicated model.

**Code to reproduce the issue**
Benchmark commands:-

```
# bazel build -c opt   --config=android_arm64   --cxxopt='--std=c++11' --copt=-DTFLITE_PROFILING_ENABLED tensorflow/lite/tools/benchmark:benchmark_model

# adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/deeplabv3_257_mv_gpu.tflite --num_threads=1
```

Benchmark results:-

_Official model:-_

```
Number of nodes executed: 70
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	                 CONV_2D	       38	   137.792	    76.447%	    76.447%	     0.000	       38
	       DEPTHWISE_CONV_2D	       17	    27.808	    15.428%	    91.875%	     0.000	       17
	         RESIZE_BILINEAR	        3	    13.577	     7.533%	    99.408%	     0.000	        3
	           CONCATENATION	        1	     0.528	     0.293%	    99.701%	     0.000	        1
	                     ADD	       10	     0.410	     0.227%	    99.928%	     0.000	       10
	         AVERAGE_POOL_2D	        1	     0.129	     0.072%	   100.000%	     0.000	        1

Timings (microseconds): count=50 first=180683 curr=179967 min=177601 max=186179 avg=180284 std=1486
Memory (bytes): count=0
70 nodes observe
```


_Replicated model:-_

```
Number of nodes executed: 70
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	                 CONV_2D	       38	   141.189	    54.356%	    54.356%	     0.000	       38
	       DEPTHWISE_CONV_2D	       17	   103.263	    39.755%	    94.110%	     0.000	       17
	         RESIZE_BILINEAR	        3	    14.178	     5.458%	    99.568%	     0.000	        3
	           CONCATENATION	        1	     0.554	     0.213%	    99.782%	     0.000	        1
	                     ADD	       10	     0.437	     0.168%	    99.950%	     0.000	       10
	         AVERAGE_POOL_2D	        1	     0.130	     0.050%	   100.000%	     0.000	        1

Timings (microseconds): count=50 first=258703 curr=259361 min=255581 max=269627 avg=259789 std=3844
Memory (bytes): count=0
70 nodes observed
```

**Other info / logs**

However using TF-12.0 with Bazel 0.16.0, the 'benchmark_model' built failed for 'official model'; but worked with replicated model.

Bazel build:-

```
# bazel build -c opt   --config=android_arm64   --cxxopt='--std=c++11' --linkopt='-llog' --copt=-DTFLITE_PROFILING_ENABLED tensorflow/contrib/lite/tools/benchmark:benchmark_model

# adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/deeplabv3_257_mv_gpu.tflite --num_threads=1
```


TF-Lite benchmark:-

adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/deeplabv3_257_mv_gpu.tflite --num_threads=1
adb: /opt/intel/intelpython27/lib/libcrypto.so.1.0.0: no version information available (required by adb)
STARTING!
Num runs: [50]
Inter-run delay (seconds): [-1]
Num threads: [1]
Benchmark name: []
Output prefix: []
Warmup runs: [1]
Graph: [/data/local/tmp/deeplabv3_257_mv_gpu.tflite]
Input layers: []
Input shapes: []
Use nnapi : [0]
nnapi error: unable to open library libneuralnetworks.so
Loaded model /data/local/tmp/deeplabv3_257_mv_gpu.tflite
resolved reporter
Didn't find op for builtin opcode 'DEPTHWISE_CONV_2D' version '2'

Registration failed.

Failed to construct interpreter
Aborted

Can you provide the corresponding optimised pb file before tflite conversion, for the same model ?"
26473,The use of graph_editor.graph_replace,"Hi, I'm quite new to tensorflow and I came from this question: [tensorflow-replace-an-operation-with-an-other](https://stackoverflow.com/questions/47081900/tensorflow-replace-an-operation-with-an-other) , which is asked quite a long time ago and I've got similar need.
I have a tacotron model, freezed, with LSTMBlockCell and dynamic_rnn involved, which seems to not have a quantization version in tensorflow/tools/graph_transforms/quantize_nodes. 
The idea is that quant the weights and bias, and replace things in the graph first, maybe add dequant nodes later and modify the lstmblockcell itself. I tried to process the first step in this way:

```
tensors = [n for n in session.graph.as_graph_def().node]
for t in tensors:
  if 'lstm_cell' in t.name:
    lstm_cell = graph.get_tensor_by_name(t.name + ':0')
    val = session.run(lstm_cell)
    quant_val = quant_weights(val)
    new_t = tf.convert_to_tensor(quant_val, np.uint8)
    tf.contrib.graph_editor.graph_replace(lstm_cell, {lstm_cell : new_t})
```
And it returned:
```
Traceback (most recent call last):
  File ""edit_graph.py"", line 131, in <module>
    load('my_graph.pb')
  File ""edit_graph.py"", line 100, in load
    tf.contrib.graph_editor.graph_replace(lstm_cell, {lstm_cell : new_t}, reuse_dst_scope = True)
  File ""/home/admin/.local/lib/python3.6/site-packages/tensorflow/contrib/graph_editor/transform.py"", line 736, in graph_replace
    raise ValueError(""Targets and replacements are not connected!"")
ValueError: Targets and replacements are not connected!
```
Could you kindly point out the mistake I made? Thanks in advance.
"
26472,Keras model save/load is totally wrong with sessions,"## Bug report: Keras `model.save()` does not work with sessions

Actually, it only works with the default session.

I know tf.keras is mainly targetting at TF 2.0, but it really doesn't play nice with conventional TF-1.x ways (e.g. Session). I encountered this bug and nailed down why it happens after a day. Arguably, I believe this is a severe bug and hard to realize when one encounters. This is also somewhat related to #26430.

**In summary (the current behavior)**: if one is using a custom session created (say `MonitoredSession`, as it was a conventional way of opening and sessions in TF 1.x way), Keras model's load/save will be COMPLETELY not working.

### System information

- OS Platform and Distribution: Linux, but agnostic to Platform
- TensorFlow installed from (source or binary): binary, pypi
- TensorFlow version (use command below): **1.13.1**
- Python version: 3.6

## Code to reproduce the issue

Please see this notebook:

https://gist.github.com/wookayin/0712261896799e0f655df50e910055df

Or in a code:

```python
import tensorflow as tf

class MyModel(tf.keras.models.Model):
    def __init__(self):
        super(MyModel, self).__init__()
        self.layer = tf.keras.layers.Dense(5)
        
    def __call__(self, x):
        return self.layer(x)
    
model = MyModel()

# Build the model
model(tf.zeros([3, 10]))

# this is something like a train_op that ""changes"" the content of the variable.
assign_op = model.variables[1].assign( tf.ones_like(model.variables[1]) )

# Let's create a session
sess = tf.train.SingularMonitoredSession()
#sess.run(tf.global_variables_initializer())

# we can see the 'bias' variable is initialized to ZERO
assert sess.run(model.variables[1]).mean() == 0.0

# Now let's make the 'bias' variable to all one...
sess.run(assign_op)

# sure it is ONE ...
assert sess.run(model.variables[1]).mean() == 1.0

# Let's save the Keras model parameter. The bias is set to ONE, right ?????
# Since model.save_weights try to create a new op (another bug #26430)
# and the graph has been finalized, we will 'unfinalize' the graph with a bit of hack
sess.graph._finalized = False
model.save_weights('/tmp/keras-one.h5')
sess.graph.finalize()


# Let's see what is stored in the model file ....
import h5py
h = h5py.File(""/tmp/keras-one.h5"")
assert h['dense']['dense']['bias:0'].value.mean() == 1.0     # <------ This will fail
# Actual output is: array([0., 0., 0., 0., 0.], dtype=float32)
```

The above code does the following:

* Build a Keras model
* Initialize the layer (bias will be zero)
* Update the trainable variable (e.g. bias: 0->1)
* Save the model into a H5 File
* Read the model file, but the bias variable is not 1 but 0 (same as the initialization)



### Additional Information

* `tf.Session()` behaves exactly the same as with `tf.train.MonitoredSession()`. In this case we need to run custom variable initializer ops.
* However, when `tf.InteractiveSession()` is used, it works exactly as expected (the readout from `keras-one.h5` is 1.0)
* It would work well in the eager mode.


## A work-around

If we change the code as follows, it would work.

```
with sess.as_default():
    model.save_weights('/tmp/keras-one.h5')
```

A quick note: 'SingularMonitoredSession' object has no attribute 'as_default'


This is because [Keras only plays with **the default session**][keras-session] (see #26430 as well) and in this example the manually created session was never registered as default. Kinda makes sense as there is no way for `model.save()` function to be aware of the session... However, this leads to a **very inconsistent and unexpected behavior.** If no session is registered as default, `model.save()` should instead throw an error.

[keras-session]: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L401"
26470,Broken link in a notebook,"Notebook - Intro to CNNs (TF 2.0 Alpha tutorial - images): https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/images/intro_to_cnns.ipynb
In the last cell there is a broken link (404): https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/_index_/advanced.ipynb (""_As you can see, our simple CNN has achieved a test accuracy of over 99%. Not bad for a few lines of code! For another style of writing a CNN (using the Keras Subclassing API and a GradientTape) head [here](https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/_index_/advanced.ipynb)._"")"
26469,"[doc]: dead link in ""Recurrent Neural Networks""","

**System information**
- TensorFlow version: master
- Doc Link: https://www.tensorflow.org/tutorials/sequences/recurrent#language_modeling

**Describe the documentation issue**


The link <https://catalog.ldc.upenn.edu/ldc99t42> for "" Penn Tree Bank"" is no long accessible.

The correct URL might be https://catalog.ldc.upenn.edu/LDC99T42 , that is, upper cases for ""LDC99T42""


"
26468,Tensorflow.loadLibrary() not working on Windows,"Running:

`TensorFlow.loadLibrary(""ner-dl/win/_lstm_ops.so"")`

Returns:

```
java.lang.UnsatisfiedLinkError: ner-dl\win\_lstm_ops.so not found
	at org.tensorflow.TensorFlow.loadLibrary(TensorFlow.java:47)
```

We're trying to load contrib .so files dynamically from java API. These .so files are generated on Windows by installing tensorflow through pip. Using python 3.6.8 to do so and retrieving them from the library source code. These libraries are from tensorflow.contrib, since this is what we're using on our graphs. To me, this sounds like a path resolving issue, since it's the same error than making up an invented path.

Works fine when using appropriate .so files on Linux and Mac.

Same happens if using absolute paths btw:
```
java.lang.UnsatisfiedLinkError: C:\Users\saifa\IdeaProjects\spark-nlp\src\main\resources\ner-dl\win\_lstm_ops.so not found
	at org.tensorflow.TensorFlow.loadLibrary(TensorFlow.java:47)
```

It seems this also happened to this poor guy here:
https://stackoverflow.com/questions/50115117/using-ops-from-tensorflow-contrb-on-windows-via-java-api


**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
`TensorFlow.loadLibrary(""ner-dl/win/_lstm_ops.so"")`
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
Binary from pip
- TensorFlow version (use command below):
`b'v1.12.0-rc2-3-ga6d8ffae09' 1.12.0`
- Python version:
Using Java API
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
Running
`TensorFlow.loadLibrary(""ner-dl/win/_lstm_ops.so"")`
returns 
```
java.lang.UnsatisfiedLinkError: ner-dl\win\_lstm_ops.so not found
	at org.tensorflow.TensorFlow.loadLibrary(TensorFlow.java:47)
```

**Describe the expected behavior**
Should be able to load libraries dynamically on Windows. This works fine from Linux and Mac.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
`TensorFlow.loadLibrary(""ner-dl/win/_lstm_ops.so"")`
"
26467,[CI] The PR Checkers of TenrsorFlow are broken.,"It is strange. It seems that the PR Checkers are suddenly broken. Anyone why this issue is still not fixed?
As you can see, Many PRs have been not reviewed by reviewers because the Tensorflow CI facilities are still a running status. BTW, Why this issue is generated? What is a possible reasons? Who is in charge of the CI facilities of the TensorFlow github repository?

```bash
* Ubuntu CC Expected — Waiting for status to be reported  Required
* Ubuntu Sanity Expected — Waiting for status to be reported  Required
* import/copybara Expected — Waiting for status to be reported  Required
```

* Screenshot: 

![image](https://user-images.githubusercontent.com/82404/54006477-d5db4280-41a0-11e9-974f-2708aa2f39ec.png)

![image](https://user-images.githubusercontent.com/82404/54006516-f6a39800-41a0-11e9-98b0-b45a32135c78.png)
"
26466,Java API can not create empty tensor,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):MAC OS 10.14.1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NO
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.8
- Python version:None
- Bazel version (if compiling from source):None
- GCC/Compiler version (if compiling from source):None
- CUDA/cuDNN version:None
- GPU model and memory:None

C++ API is able to create empty tensor
```cpp
        Tensor ts_indices(tensorflow::DT_INT64, tensorflow::TensorShape(
                   { 0, 2 }));
```
However, Java API will raise Exception.
```java
        long[][] index = new long[0][2];
        Tensors.create(index);
 ```
> java.lang.IllegalArgumentException: cannot create Tensors with a 0 dimension
	at org.tensorflow.Tensor.fillShape(Tensor.java:703)
	at org.tensorflow.Tensor.create(Tensor.java:142)
	at org.tensorflow.Tensor.create(Tensor.java:115)
	at org.tensorflow.Tensors.create(Tensors.java:336)"
26465,Wrong built-in precision metric for estimator,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 1.13.0
- Python version: 3.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
During my experiment with binary classification, I've noticed a significant discrepancy between _tf.estimator.DNNClassifier_ built-in metric of _precision_ from the one computed by _tf.metrics.precision_.

**Describe the expected behavior**
If the calculations were correct. The score of _precision_ should agree with that of _tf.metrics.precision_.

**Code to reproduce the issue**
```
import numpy as np
import tensorflow as tf

# tf.enable_eager_execution()

# Metadata describing the text columns
CSV_COLUMNS = ""..."".split(',')

DEFAULTS = [...]

KEY_COLUMN = 'KEY'

LABEL_COLUMN = 'LABEL'

# Create an input function reading a file using the Dataset API
# Then provide the results to the Estimator API
def read_dataset(filename, mode, batch_size = 128):
    def _input_fn():
        def decode_csv(value_column):
            columns = tf.decode_csv(value_column, record_defaults=DEFAULTS)
            features = dict(zip(CSV_COLUMNS, columns))
            
            label = features.pop(LABEL_COLUMN)
            
            return features, label

        # Create list of files that match pattern
        file_list = tf.gfile.Glob(filename)

        # Create dataset from file list
        dataset = (tf.data.TextLineDataset(file_list).skip(1)  # Read text file, skip header
                     .map(decode_csv))  # Transform each elem by applying decode_csv fn
        
        if mode == tf.estimator.ModeKeys.TRAIN:
            num_epochs = None # indefinitely
            dataset = dataset.shuffle(buffer_size=10*batch_size)
            dataset = dataset.repeat(num_epochs).batch(batch_size)
        else:
            num_epochs = 1 # end-of-input after this
            dataset = dataset.repeat(num_epochs).batch(10000)
        
        return dataset
    return _input_fn

# Define feature columns
def get_categorical(key, hash_bucket_size):
    return tf.feature_column.categorical_column_with_hash_bucket(key, hash_bucket_size)

def get_numeric(key):
    return tf.feature_column.numeric_column(key)

def get_cols():
  # Define column types
  return [\
             ...
tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_identity('PAXCOUNT',10)),
             ...
      ]

# Create serving input function to be able to serve predictions later using provided inputs
def serving_input_fn():
    feature_placeholders = {
        ...
        'PAXCOUNT': tf.placeholder(tf.int32, [None]),
        ...
        KEY_COLUMN: tf.placeholder_with_default(tf.constant(['nokey']), [None])
    }
    features = {
        key: tf.expand_dims(tensor, -1)
        for key, tensor in feature_placeholders.items()
    }
    return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)

# I've defined additional metric here
def my_precision(labels, predictions):
    return {'my_precision': tf.metrics.precision(labels, 
                                                 tf.strings.to_number(predictions['classes']))}

# I've defined additional metric here
def my_recall(labels, predictions):
    return {'my_recall': tf.metrics.recall(labels,
                                           tf.strings.to_number(predictions['classes']))}

# I've defined additional metric here
def my_f1_score(labels, predictions):
    return {'my_f1_score': tf.contrib.metrics.f1_score(labels, 
                                                    tf.strings.to_number(predictions['classes']))}

estimator = tf.estimator.DNNClassifier(    
                   hidden_units = [256,128,64],
                   feature_columns = get_cols(),
                   model_dir = outdir,
                   n_classes = 2, 
                   optimizer = tf.train.AdamOptimizer(learning_rate=0.001),
                   batch_norm = False)
    
# Custom metric is added here
estimator = tf.estimator.add_metrics(estimator, my_f1_score)
estimator = tf.estimator.add_metrics(estimator, my_precision)
estimator = tf.estimator.add_metrics(estimator, my_recall)

estimator.train(input_fn=read_dataset('train.csv', mode = tf.estimator.ModeKeys.TRAIN), steps = 1000)
estimator.evaluate(input_fn=read_dataset('eval.csv', mode = tf.estimator.ModeKeys.EVAL))
```

**Other info / logs**
**Output of Evaluation**
```
{'accuracy': 0.73864836,
 'accuracy_baseline': 0.5066931,
 'auc': 0.8154568,
 'auc_precision_recall': 0.8016961,
 'average_loss': 0.5324211,
 'label/mean': 0.5066931,
 'loss': 7318.703,
 'my_f1_score': 0.6467407,
 'my_precision': 0.6260541,
 'my_recall': 0.66884124,
 'precision': 0.78366596,
 'prediction/mean': 0.46659818,
 'recall': 0.66884094,
 'global_step': 100}
```
Notice _precision_ is 0.78 while _my_precision_ is 0.62, thats a big difference!

Additionally, I have also added _my_ recall_ and _my_f1 score_, using _tf.metrics.recall_ and _tf.contrib.metrics.f1_score_ respectively.

Taking F1_Score = 2 * ((Precision * Recall) / (Precision + Recall)) to validate F1Score, it seems to agree with _my_precision_ instead of the built-in _precision_ metric.

Does anyone face a similar issue like the above?
"
26464,segment erro:tensorflow/tensorflow/examples/label_image/main.cc,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):source
- TensorFlow version (use command below):r1.13
- Python version:3.6
- Bazel version (if compiling from source):0.23
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:9.0/7.0
- GPU model and memory:1080ti/11gb


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
tensorflow/tensorflow/examples/label_image/main.cc
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
i think the reason is ""TF_RETURN_IF_ERROR(env->NewRandomAccessFile(filename, &file));""
https://github.com/tensorflow/tensorflow/blob/5fad2f50d610fdb765f4f99c7e0531b7a3ddbe94/tensorflow/examples/label_image/main.cc#L100"
26463,Load data tutorial model fails to converge,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version:
- Doc Link:

site/en/tutorials/load_data/images.ipynb

*Describe the documentation issue**
The model fails to converge most of times. I changed the steps per epoch to run through all the data but it fails to converge. A lot of times just stuck in high loss low accuracy. 

I noted last layer output logits. It this expected?

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
26462,glibc,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
26461,[TF2 Upgrade] contrib usage and several other TF 1.x APIs uncaught by TF2 upgrade script,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian 8
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): pip binary
- TensorFlow version (use command below): TF 2 nightly as of 3/7/19 (comparison version is 1.13.1
- Python version: 3.4.2
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the current behavior**

In order to make sure that my crusty old TF 1.x code works on TF 2, I'm using the `tf_upgrade_v2` script [as documented](https://github.com/tensorflow/docs/blob/master/site/en/r2/guide/upgrade.md). 

However, on the first bit of [code](https://gist.github.com/zmjjmz/2ee2090233217c0d1be2dc975e5c3a47) I tried I was told that it 'Detected 0 issues that require attention', and the report it gave was essentially blank. Additionally, the diff between the old file and the new file showed no changes!

This would have been great, except there's a few things that are very obviously incorrect.

- On lines 42 and 43, there's a use of `tf.contrib.lookup` which should be moved to `tf.lookup`.
- On line 63 there's a call to `tf.sparse_tensor_to_dense` which should be moved to `tf.sparse.to_dense`
- On lines 118-121 there's a tf.Session context and a `tables_initializer` which no longer exist.

**Describe the expected behavior**

I would hope that the above 4 issues would be pointed out by the upgrade script as needing my attention, even if it cannot (for some reason, although I imagine the one on line 63 can be done with a string replacement) fix them automatically. 

Note: I still haven't gotten this script (despite some manual upgrading) to work on TF2, but it (pre-manual upgrade) works fine on TF 1.13.1, although it gives me a bunch a bunch of deprecation warnings :)

**Code to reproduce the issue**

See the attached [gist](https://gist.github.com/zmjjmz/2ee2090233217c0d1be2dc975e5c3a47)
"
26460,GPU Device Selector in TensorFlow 2.0,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.0
- Are you willing to contribute it (Yes/No): Happy to help as much as I can!

**Describe the feature and the current behavior/state.**
TensorFlow 1.x support specifying GPU devices to use:
```python
# Horovod: pin GPU to be used to process local rank (one GPU per process)
config = tf.ConfigProto()
config.gpu_options.visible_device_list = str(hvd.local_rank())
```

There's no comparable API in TensorFlow 2.0.  The closest option is to use the `CUDA_VISIBLE_DEVICES` environment variable.  Unfortunately, `CUDA_VISIBLE_DEVICES` prevents processes from doing `cudaMemcpy` from/to devices not owned by the process.  There's a significant performance degradation when NCCL is used with P2P communication disabled.

The ask is to add an API to TensorFlow 2.0 to enable device selection.

**Will this change the current api? How?**
Yes, will introduce an API to select GPU devices to use.

**Who will benefit with this feature?**
Users of [Horovod](http://horovod.ai).

**Any Other info.**
cc @azaks2 @alextp @jaingaurav @guptapriya "
26459,TensorFlow 1.13.1 with Docker under Raspbian Stretch,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution: Docker version 18.09.0, build 4d60db4 on top of Linux stf-provider 4.14.34-hypriotos-v7+ #1 SMP Sun Apr 22 14:57:31 UTC 2018 armv7l GNU/Linux. Raspbian is 9.8 in the Docker container.
- TensorFlow installed from binary: `pip3 install tensorflow`
- TensorFlow version: 1.13.1 
- Python version: 3.5.3
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: nope



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```bash
root@ed47bf54107c:/tmp# python3 -c ""import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))""
```

gives

```bash
/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.4 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5
  return f(*args, **kwds)
/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: builtins.type size changed, may indicate binary incompatibility. Expected 432, got 412
  return f(*args, **kwds)
tf.Tensor(-611.39856, shape=(), dtype=float32)
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
If you ever want to give it a try, here is the image:

```bash
docker pull gounthar/tensorflow-armv7:1.13
```

I tried another image from the Data Science Docker Stack:

```bash
fedec07fb96b        elswork/tf-opencv:latest   ""/bin/bash""              4 minutes ago       Up 4 minutes                                        practical_dirac
```
and I got the same kind of error:

```bash
root@fedec07fb96b:/# python3 tensorFlow.py
/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.4 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5
  return f(*args, **kwds)
/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: builtins.type size changed, may indicate binary incompatibility. Expected 432, got 412
  return f(*args, **kwds)
```
"
26458,TypeError in test files,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 1.13.1
- Python version: 3.7
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

I encountered these when running tests after building tensorflow using bazel on Windows:
```
======================================================================
ERROR: testLossesForwarded (__main__.ListTests)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""%PREFIX%\lib\site-packages\tensorflow\python\framework\test_util.py"", line 946, in decorated
    f(self, *args, **kwargs)
  File ""%PREFIX%\lib\site-packages\tensorflow\python\framework\test_util.py"", line 1105, in decorated
    f(self, *args, **kwargs)
  File ""\\?\C:\Users\nwani\AppData\Local\Temp\Bazel.runfiles_iqsmzf36\runfiles\org_tensorflow\py_test_dir\tensorflow\python\training\checkpointable\data_structures_test.py"", line 128, in testLossesForwarded
    model = HasList()
  File ""\\?\C:\Users\nwani\AppData\Local\Temp\Bazel.runfiles_iqsmzf36\runfiles\org_tensorflow\py_test_dir\tensorflow\python\training\checkpointable\data_structures_test.py"", line 60, in __init__
    list(sequence=[core.Dense(11)]) + [core.Dense(12)]))
TypeError: list() takes no keyword arguments

======================================================================
ERROR: testTracking (__main__.ListTests)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""%PREFIX%\lib\site-packages\tensorflow\python\framework\test_util.py"", line 946, in decorated
    f(self, *args, **kwargs)
  File ""%PREFIX%\lib\site-packages\tensorflow\python\framework\test_util.py"", line 1105, in decorated
    f(self, *args, **kwargs)
  File ""\\?\C:\Users\nwani\AppData\Local\Temp\Bazel.runfiles_iqsmzf36\runfiles\org_tensorflow\py_test_dir\tensorflow\python\training\checkpointable\data_structures_test.py"", line 78, in testTracking
    model = HasList()
  File ""\\?\C:\Users\nwani\AppData\Local\Temp\Bazel.runfiles_iqsmzf36\runfiles\org_tensorflow\py_test_dir\tensorflow\python\training\checkpointable\data_structures_test.py"", line 60, in __init__
    list(sequence=[core.Dense(11)]) + [core.Dense(12)]))
TypeError: list() takes no keyword arguments
```"
26456,sparsetensor-input in the form of ids and values as FeatureColumns for tf.estimator use,"I have been struggling with this issue for a while and in TF Dev Summit 2019, also brought it with @mrry who helped me a lot but still it is a fully unresolved issue.

I have input data in the form of ids and values; that is sparse representation for a giant input feature whose non-zero element indices are stored in ids and corresponding values are stored in values. I would like to use estimators but estimators require feature_column format as high-level APIs to dead with my input and currently I don't see how I can use feature-columns with my data format---please note I already have done all the feature engineering and representations and stored them in my ids, values so no need for bucketizzation, and other feature transformations that feature_columns are used for. Also the inputs are in the form of sparsetensors since I am dealing with a giant feature-vector and sparse representation will save a lot of computation and memory. How can I do it with estimators and go around the ecisting feature_column format?
"
26455,where are the conflow flow ops like abort used?,"Previous, TF control flow primitives are Switch, Merge, Enter, NextIteration, and Exit, we can see them in a simple while_loop graph. Currently, in your r1.13 and r2.0 [docs](https://www.tensorflow.org/versions/r2.0/api_docs/cc/group/control-flow-ops), seems the control flow ops changed a lot, how could I see this ops in a graphdef? are they still in while_loop and cond?"
26454,"[Bug report] wrong container setting in OpsTestBase::AddResourceInput, easily fix","File: tensorflow/core/kernels/ops_testutil.h
Class: OpsTestBase
Function: [AddResourceInput](https://github.com/wendy2003888/tensorflow/commit/18b867eabd1bb8f3540ee56a8f47f96d9f3bc20d)

resource container set to empty when using default container.

Due to CLA problem,  I can't contribute rn. 
Please review pull reques [#26428](https://github.com/tensorflow/tensorflow/pull/26428)  from ppwwyyxx. 

Thank you

<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary (python3 pip)
- TensorFlow version (use command below): 1.12.0
- Python version: 3.5.2
- Bazel version (if compiling from source): 0.22.0
- GCC/Compiler version (if compiling from source):  5.4.0
- CUDA/cuDNN version: 10.0 
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26453,Allow py_function to support functions that return RaggedTensor,"**System information**
- TensorFlow version (you are using): 1.13.1
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**
`py_function` only supports functions that return `Tensor`s. However the wrapped function is executed in eager mode and therefore ideally should support other return types consistent with eager mode. As of tf-1.13.1, `py_function` attempts to convert the returned objects to `Tensor`s, but a `RaggedTensor` cannot be converted directly to a `Tensor`. Any attempt to return a `RaggedTensor` raises an exception during the attempted conversion. E.g.,

```python
with tf.Graph().as_default():
    elements = [[1., 2., 3.], [4., 5.]]
    ragged1 = tf.ragged.constant(elements)
    def py_func():
        return tf.ragged.constant(elements)
    
    ragged2 = tf.py_function(
        py_func, [], tf.dtypes.float32
    )
    with tf.Session() as sess:
        print(sess.run(ragged1))
        print(sess.run(ragged2))

>>> <tf.RaggedTensorValue [[1.0, 2.0, 3.0], [4.0, 5.0]]>
>>> ...
>>> ValueError: TypeError: object of type 'RaggedTensor' has no len()
```

I propose that `py_function` detect which output arguments, if any, are `RaggedTensor`s and returns them without attempting to convert to `Tensor`s. If the proposal is rejected, I suggest that the documentation is updated to make clearer (either in the API or guides) that a `RaggedTensor` is not a suitable return type for functions wrapped by `py_function`. 

**Will this change the current api? How?** No.

**Who will benefit with this feature?** Anyone who uses `RaggedTensor` in conjunction with `py_function`.

**Any Other info.**
As a workaround, one can construct a `RaggedTensor` from the output of `py_function`. E.g.,

```python
with tf.Graph().as_default():
    elements = [[1., 2., 3.], [4., 5.]]
    ragged1 = tf.ragged.constant(elements)
    def py_func():
        lengths = [len(element) for element in elements]
        return sum(elements, []), lengths
    
    concatenated, lengths = tf.py_function(
        py_func, [], [tf.dtypes.float32, tf.dtypes.int64]
    )
    ragged2 = tf.RaggedTensor.from_row_lengths(concatenated, lengths)
    with tf.Session() as sess:
        print(sess.run(ragged1))
        print(sess.run(ragged2))

>>> <tf.RaggedTensorValue [[1.0, 2.0, 3.0], [4.0, 5.0]]>
>>> <tf.RaggedTensorValue [[1.0, 2.0, 3.0], [4.0, 5.0]]>
```
"
26452,"cpu tensorflow win: 7, DLL load failed with error code -1073741795","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): https://www.tensorflow.org/install/pip
 TensorFlow version: 1.12
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):-
- GCC/Compiler version (if compiling from source):-
- CUDA/cuDNN version: CUDA 9.2, cuDNN64_7
- GPU model and memory: NVIDIA quadro 2000 graphic card, Intel Xeon.





**Describe the problem**

Traceback (most recent call last):
  File ""C:\Users\azak\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", lin
e 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\azak\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal
.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\azak\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal
.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\azak\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\azak\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed with error code -1073741795

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Users\azak\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module
>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\azak\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in
<module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\azak\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", lin
e 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\azak\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", lin
e 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\azak\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal
.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\azak\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal
.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\azak\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\azak\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed with error code -1073741795


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.




**Any other info / logs**

I have problem with CPU model!!!!!!!!!!!!!!!!!!

I have problem with cpu version. only 1.5 workes for me and faced error for installing CUDA 9.0. therefore I installed CUDA 9.2! what is the source od error? I tried different version of tensorflow and cuda!!!!!!

thanks."
26451,"cpu tensorflow win: 7, DLL load failed with error code -1073741795","Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template

System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 7
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
TensorFlow installed from (source or binary): https://www.tensorflow.org/install/pip
TensorFlow version: 1.12
Python version: 3.6.8
Installed using virtualenv? pip? conda?: pip
Bazel version (if compiling from source):-
GCC/Compiler version (if compiling from source):-
CUDA/cuDNN version: CUDA 9.2, cuDNN64_7
GPU model and memory: NVIDIA quadro 2000 graphic card, Intel Xeon.
Describe the problem

Traceback (most recent call last):
File ""C:\Users\azak\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", lin
e 58, in 
from tensorflow.python.pywrap_tensorflow_internal import *
File ""C:\Users\azak\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal
.py"", line 28, in 
_pywrap_tensorflow_internal = swig_import_helper()
File ""C:\Users\azak\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal
.py"", line 24, in swig_import_helper
_mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
File ""C:\Users\azak\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
return load_dynamic(name, filename, file)
File ""C:\Users\azak\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
return _load(spec)
ImportError: DLL load failed with error code -1073741795

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
File """", line 1, in 
File ""C:\Users\azak\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_init_.py"", line 24, in <module

from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
File ""C:\Users\azak\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python_init_.py"", line 49, in

from tensorflow.python import pywrap_tensorflow
File ""C:\Users\azak\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", lin
e 74, in 
raise ImportError(msg)
ImportError: Traceback (most recent call last):
File ""C:\Users\azak\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", lin
e 58, in 
from tensorflow.python.pywrap_tensorflow_internal import *
File ""C:\Users\azak\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal
.py"", line 28, in 
_pywrap_tensorflow_internal = swig_import_helper()
File ""C:\Users\azak\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal
.py"", line 24, in swig_import_helper
_mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
File ""C:\Users\azak\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
return load_dynamic(name, filename, file)
File ""C:\Users\azak\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
return _load(spec)
ImportError: DLL load failed with error code -1073741795

Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions. Include the entire stack trace
above this error message when asking for help.

Any other info / logs

I have problem with CPU model!!!!!!!!!!!!!!!!!!

I have problem with cpu version. only 1.5 workes for me and faced error for installing CUDA 9.0. therefore I installed CUDA 9.2! what is the source of error? I tried different version of tensorflow and cuda!!!!!!

thanks.

"
26449,[tensorflow.org] Site not working; Potential service worker issue,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version:
- Doc Link: https://www.tensorflow.org


**Describe the documentation issue**

None of http://tensorflow.org  is working on Safari (latest macOS) for me; seems related to Service Worker. Works fine on Safari Technology Preview and on @googlechrome.
Might have visited the page during the last 48h (pre-launch).

![skjermbilde 2019-03-06 kl 23 11 45](https://user-images.githubusercontent.com/939844/53964361-81c45580-40ef-11e9-9812-d42291dfb970.png)

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
Yes, if applicable."
26448,cpu build failed with openmpi,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution : ubuntu 18.04 (64-bit FriendlyDesktop image file based on Ubuntu desktop 18.04 64bit)
- Mobile device: friendlyARM's NanoPC-T4
- TensorFlow installed from (source or binary):source
- TensorFlow version:1.13.0
- Python version:3.6
- Installed using virtualenv? pip? conda?: non
- Bazel version (if compiling from source):0.19.2
- GCC/Compiler version (if compiling from source): gcc 7.3
- CUDA/cuDNN version: non
- GPU model and memory: non



**Describe the problem**
missing links related to OpenMPI(ex:mpi.h, libmpi.so, ...)
when building with bazel 
error message: tensorflow/pyhton/4301:0 missing input file thirdparty/mpi:mpi:h

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26447,Add SAMD21 or SAMD51 TensorFlow Lite for Microcontrollers,"A widely used platform is the SAMD21 and SAMD51 family with a large number of development boards
https://www.adafruit.com/product/4064
https://www.adafruit.com/product/2772
https://www.sparkfun.com/products/13672

Support:
- C++11
- Arduino
- Circuitpython
- Makecode
- Rust

they should be considered to be ported

**System information**
- TensorFlow version (you are using): Lite
- Are you willing to contribute it (Yes/No): I can help with test

Thanks!
"
26446,tensorflow 1.13.1: module 'tensorflow._api.v1.keras.applications' has no attribute 'resnet',"Hi every one

I have used Google Colab and when use keras resnet, it raise this error: module 'tensorflow._api.v1.keras.applications' has no attribute 'resnet'

my code 

```
import tensorflow as tf
from tensorflow import keras

model = keras.applications.resnet.ResNet50(weights=None,classes=4, input_shape=(150,150,3))
model.compile(optimizer=tf.keras.optimizers.Adam(0.01), 
                loss='binary_crossentropy',
                metrics=['accuracy'])
```

my tensorflow is 1.13.1, as default of google colab

Other keras application like Mobinet, VGG16 still work fine

Thank for you help"
26445,How to install TF_GAN,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Win7 x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No
- TensorFlow installed from (source or binary):binary
- TensorFlow version:0.1.0
- Python version:3.6
- Installed using virtualenv? pip? conda?:pip
- Bazel version (if compiling from source):No
- GCC/Compiler version (if compiling from source):No
- CUDA/cuDNN version:No
- GPU model and memory:No



**Describe the problem**
How to install TF-GAN in tensorflow 0.1.0?
**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26444,"Tensorflow.js API documentation example issue line 19, incorect code, build error","In the object callback on **line 19** in the bottom example on this page:
[See-sample-code-for-node.js-usage](https://www.tensorflow.org/js/tutorials/setup#see-sample-code-for-node.js-usage) has a little error.

> Currently you have

`onEpochEnd: (epoch, log) => console.log(`Epoch ${epoch}: loss = ${log.loss}`);`

> You need to remove the ;

`onEpochEnd: (epoch, log) => console.log(`Epoch ${epoch}: loss = ${log.loss}`)`

> Compile error is as follows

```
SyntaxError: Unexpected token ;
    at new Script (vm.js:80:7)
    at createScript (vm.js:274:10)
    at Object.runInThisContext (vm.js:326:10)
    at Module._compile (internal/modules/cjs/loader.js:664:28)
    at Object.Module._extensions..js (internal/modules/cjs/loader.js:712:10)
    at Module.load (internal/modules/cjs/loader.js:600:32)
    at tryModuleLoad (internal/modules/cjs/loader.js:539:12)
    at Function.Module._load (internal/modules/cjs/loader.js:531:3)
    at Function.Module.runMain (internal/modules/cjs/loader.js:754:12)
    at startup (internal/bootstrap/node.js:283:19)
```

>  Node -v

`v11.11.0`"
26442,TFLite LSTM example produces different results for Lite and for standard Tensorflow for variable length input.,"I have extended the LSTM example for TFLite `unidirectional_sequence_lstm_test.py` in `tensorflow/tensorflow/lite/experimental/examples/lstm` and it returns different results for standard Tensorflow and Tensorflow Lite.

The example project is available here: [https://github.com/MiloslavPalaxo/tflite-lstm-text](https://github.com/MiloslavPalaxo/tflite-lstm-text)

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.12.0-9708-gafab5b3 1.14.1-dev20190306
- Python version: 3.7
sary to generate the problem.

**Other info / logs**

2019-03-07 14:12:33.453487: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3400115000 Hz
2019-03-07 14:12:33.454111: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x2644ff0 executing computations on platform Host. Devices:
2019-03-07 14:12:33.454139: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): <undefined>, <undefined>
WARNING: Logging before flag parsing goes to stderr.
W0307 14:12:33.472815 139655812351808 deprecation.py:506] From /home/XXX/.virtualenvs/tftest/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0307 14:12:33.495823 139655812351808 deprecation.py:323] From /home/XXX/.virtualenvs/tftest/lib/python3.7/site-packages/tensorflow/lite/experimental/examples/lstm/rnn.py:218: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
W0307 14:12:33.645129 139655812351808 deprecation.py:506] From /home/XXX/.virtualenvs/tftest/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py:883: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0307 14:12:34.316469 139655812351808 deprecation.py:323] From /home/XXX/ideaprojects/tflite-lstm-text/sentiment-lite3.py:151: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

W0307 14:13:07.583182 139655812351808 deprecation.py:323] From /home/XXX/.virtualenvs/tftest/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
W0307 14:13:07.947111 139655812351808 deprecation.py:323] From /home/XXX/ideaprojects/tflite-lstm-text/sentiment-lite3.py:191: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
W0307 14:13:07.947545 139655812351808 deprecation.py:323] From /home/XXX/.virtualenvs/tftest/lib/python3.7/site-packages/tensorflow/python/framework/graph_util_impl.py:247: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
W0307 14:13:08.154012 139655812351808 deprecation.py:323] From /home/XXX/.virtualenvs/tftest/lib/python3.7/site-packages/tensorflow/python/tools/optimize_for_inference_lib.py:113: remove_training_nodes (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.remove_training_nodes`
[[0.5784946  0.17042728 0.25107813]
 [0.2503695  0.35664326 0.3929873 ]
 [0.26069233 0.3423219  0.39698583]
 [0.5784946  0.17042728 0.25107813]
 [0.5784946  0.17042728 0.25107813]]
[[0.25950563 0.34442514 0.39606926]
 [0.25036934 0.3566433  0.3929873 ]
 [0.2606922  0.34232193 0.39698586]
 [0.25918585 0.34475493 0.3960592 ]
 [0.25936535 0.34485146 0.39578322]]

Failure
Traceback (most recent call last):
  File ""/usr/lib/python3.7/unittest/case.py"", line 59, in testPartExecutor
    yield
  File ""/usr/lib/python3.7/unittest/case.py"", line 615, in run
    testMethod()
  File ""/home/XXX/.virtualenvs/tftest/lib/python3.7/site-packages/tensorflow/python/framework/test_util.py"", line 428, in wrapper
    return fn(*args, **kwargs)
  File ""/home/XXX/ideaprojects/tflite-lstm-text/sentiment-lite3.py"", line 271, in testDynamicRnnMultiRnnCell
    self.assertTrue(np.allclose(expected_output, result, rtol=1e-6, atol=1e-2))
  File ""/usr/lib/python3.7/unittest/case.py"", line 692, in assertTrue
    raise self.failureException(msg)
AssertionError: False is not true

"
26441,Tensorboard not opening in jupyter notebook,"I am trying to open tensorboard inside jupyter notebook with the command 
`% tensorboard --logdir keras`.
But I am getting 
**localhost refused to connect.**
Specifications:

- Jupyter notebook= 5.7.5
- Python = 3.6.7
- Windows =10
-  tensorflow = 2.0.0-alpha0

**Code**
```
from kaggle_data import load_data, preprocess_data, preprocess_labels
import numpy as np
import matplotlib.pyplot as plt

X_train, labels = load_data('../data/kaggle_ottogroup/train.csv', train=True)
X_train, scaler = preprocess_data(X_train)
Y_train, encoder = preprocess_labels(labels)

X_test, ids = load_data('../data/kaggle_ottogroup/test.csv', train=False)
X_test, _ = preprocess_data(X_test, scaler)

nb_classes = Y_train.shape[1]
print(nb_classes, 'classes')

dims = X_train.shape[1]
print(dims, 'dims')

import tensorflow as tf
from keras.layers import Dense, Activation

dims = X_train.shape[1]
print(dims, 'dims')
print(""Building model..."")
nb_classes = Y_train.shape[1]
print(nb_classes, 'classes')
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(nb_classes, input_shape=(dims,), activation='sigmoid'))
model.compile(optimizer = 'sgd', loss='categorical_crossentropy')
model.fit(X_train, Y_train,epochs=1,callbacks=[tf.keras.callbacks.TensorBoard('keras')] )

%load_ext tensorboard.notebook
%tensorboard --logdir keras
```"
26437,using HParams with TPU v2 PODs broken,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12.0
- Python version: 2.7
- CUDA/cuDNN version: 9.0 / 7
- GPU model and memory: T4, 16 GB
- TPU type: v2-32
- TPU TensorFlow version: 1.12.0

**Describe the current behavior**
The distribution to the pod fails with
1. `AttributeError: 'TPUContext' object has no attribute 'is_input_sharded_per_core'` if passing a user-provided `tf.contrib.training.HParams` instance to the `TPUEstimator`. Use `--use_hparams` argument in the demo script.
2. `AttributeError: 'dict' object has no attribute 'batch_size'` if no `HParams` are provided. Use `--nouse_hparams` argument in the demo script.

**Describe the expected behavior**
I expect `params` in the `model_fn` and `input_fn` to be an instance of `tf.contrib.training.HParams`, hence its parameters should be accessible as attributes. However in 2.) `params` is a python `dict`.

I expect to be able to use the `TPUEstimator` with my own `HParams`.

**Code to reproduce the issue**
See https://gist.github.com/ceaed6f7c2ed027da35fe5fbbe0056ae
Help:
```bash
USAGE: tpu_hparams.py [flags]
flags:

tpu_hparams.py:
  --num_cores: The number of TPU cores in the pod.
    (default: '32')
    (an integer)
  --project: The project name, the TPU is instantiated in.
    (default: '')
  --tpu: The TPU name.
    (default: '')
  --[no]use_hparams: To avoid the bug.
    (default: 'true')
  --zone: The zone name, the TPU is instantiated in.
    (default: '')
```

**Other info / logs**
What causes the issues?
1. If a user provides `params`, then the `TPUEstimator` adds the `TPUContext` to `params` by calling
https://github.com/tensorflow/tensorflow/blob/a6d8ffae097d0132989ae4688d224121ec6d8f35/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py#L1528-L1531
This triggers the `add_param` or `set_param` methods. If `set_param` is used because the key `context` was not found in `params`, then `_cast_to_type_if_compatible` is invoked before setting the parameter. Unfortunately this triggers a casting / copy operation:
https://github.com/tensorflow/tensorflow/blob/6612da89516247503f03ef76e974b51a434fb52e/tensorflow/contrib/training/python/training/hparam.py#L187
Because the syntax of the `TPUContext` only requires one positional argument
https://github.com/tensorflow/tensorflow/blob/a6d8ffae097d0132989ae4688d224121ec6d8f35/tensorflow/contrib/tpu/python/tpu/tpu_context.py#L43-L55
a new `TPUContext` is initialized, where the `_internal_ctx` is set to the `TPUContext` instance, which itself actually contains a instance of `_InternalTPUContext`.

How could this be solved?
* create a clone / copy constructor for `HParams`
* use deep cloning? In case of standard types, this would instantiate a new one as well, if I am not mistaken.
* fix the problem in the `TPUEstimator` by always using instances of `HParams` and setting `context` before, so we add the `context` by calling `add_hparam`, which does not use the type casting, instead of `set_hparam`.

2. `hparams` suddenly is a `dict`, not a instance of `HParams`.
"
26436,tf.contrib.training.HParams domain interval support,"Hi, this is my first issue/feature request. I hope it's relevant :)

I am currently working on hyperparameter optimisation and would like to use the HParams API but in my eyes there is a feature missing. Instead of having a discrete list of values for a parameter, I would like to represent a feasible interval, from which I can take values.

**Describe the feature and the current behavior/state.**
Currently, the HParams class holds a set of hyperparameters as name-value pairs, that is a discrete set of values for each of the hyperparameters. I would like to be able to specify also the feasible interval from which the values are coming. Also the TensorBoard api for reporting experiment summaries provides the possibility to report the domain interval and I think it would be handy to have this information in the HParams class, too.

The TensorBoard API let's you specify the `domain_discrete` but you could also provide `domain_interval`.
```
api_pb2.HParamInfo(name='num_units',  
                             display_name='Number of units',  
                             type=api_pb2.DATA_TYPE_FLOAT64,  
                             domain_discrete=num_units_list_val),
```

**Will this change the current api? How?**
Yes, this would require changing the API such that the User can also specify the interval of a parameter, for example:

```
add_hparam(
    name,
    value,
    start,
    end
)
```

Furthermore, the data structure would need to be changed to save the interval.
[Tensor2Tensor has something similar](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/layers/common_hparams.py) with RangedHParams.

**Who will benefit with this feature?**
Anyone working with intervals for hyperparameters compared to a fixed set of discrete values.

**System information**
- TensorFlow version (you are using): r1.12
- Are you willing to contribute it (Yes/No): Yes, I am happy to help, but this would be my first contribution and I would need guidance"
26435,[Tflite] Unable to convert model to tflite with uneven input dimension,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS High Sierra
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 1.13
- Python version: 3.6.5
- Bazel version (if compiling from source): - 
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: -
- GPU model and memory: -

Hi I have a fully convolutional CNN model (PoseNet to be precise), which is therefore able to process input of any dimension.

Converting the model to tflite works perfectly with even Input dimensions like 224x224.
But I need it to get converted with uneven Input Dimensions something like 225x225 or 289x289.

Setting these uneven input dimension in toco/ the tflite converter results in the following error:
```
RuntimeError: TOCO failed see console for info.
b'2019-03-07 10:20:29.334193: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] 
Before Removing unused ops: 213 operators, 318 arrays (0 quantized)\n2019-03-07 10:20:29.340266: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] 
Before general graph transformations: 213 operators, 318 arrays (0 quantized)\n2019-03-07 10:20:29.348270: F tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:991] 
Check failed: height_with_paddings % block_height == 0 (1 vs. 0)\n'
```

Questions:

1. Why does tflite need fixed input dimensions? (I suppose this reduces necessary computations and therefore allow faster inference)
2. Why is it not possible to convert a model to tflite with uneven Input Dimensions even though the model is fully convolutional and can therefore generally handle those dimensions?
3. Is there a workaround to fix my problem?
4. Additional: Can someone who knows posenet explain to me why it needs this +1 padding to even input dimensions: (line 109 in: https://github.com/tensorflow/tfjs-models/blob/master/posenet/src/util.ts)

Thank you very much in advance!
"
26432,ImportError: librt.so.1: No such file or directory,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): `pip install tensorflow`
- TensorFlow version: 1.13.1
- Python version: 3.7.2
- Installed using virtualenv? pip? conda?: `pip`
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: 16GB RAM



**Describe the problem**
1. If I run python with `$ python3`, then `import tensorflow`, I get an `ImporError: librt.so.1: cannot open shared object file: No such file or directory`
2. If I run python with `$ LD_LIBRARY_PATH=/home/kmonisit/.linuxbrew/lib:$LD_LIBRARY_PATH python3`, then `import tensorflow`, it succeeds.
3. I will be using `tensorflow` from within C code, i.e. C code calls Python code calls tensorflow. I want to find a way where I can use `tensorflow` without having to specify `LD_LIBRARY_PATH`.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
$ brew install binutils linux-headers
$ brew install glibc --force-bottle
$ brew install gcc --force-bottle
$ brew install python3
$ pip3 install tensorflow
```

Now I have `glibc` in `$HOME/.linuxbrew/lib`. Next step is:

```
$ python3
>>> import tensorflow
```

```
Traceback (most recent call last):
  File ""/home/kmonisit/.linuxbrew/Cellar/python/3.7.2_2/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/kmonisit/.linuxbrew/Cellar/python/3.7.2_2/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/kmonisit/.linuxbrew/Cellar/python/3.7.2_2/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/kmonisit/.linuxbrew/Cellar/python/3.7.2_2/lib/python3.7/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/kmonisit/.linuxbrew/Cellar/python/3.7.2_2/lib/python3.7/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: librt.so.1: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/kmonisit/.linuxbrew/Cellar/python/3.7.2_2/lib/python3.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/home/kmonisit/.linuxbrew/Cellar/python/3.7.2_2/lib/python3.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/kmonisit/.linuxbrew/Cellar/python/3.7.2_2/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/kmonisit/.linuxbrew/Cellar/python/3.7.2_2/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/kmonisit/.linuxbrew/Cellar/python/3.7.2_2/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/kmonisit/.linuxbrew/Cellar/python/3.7.2_2/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/kmonisit/.linuxbrew/Cellar/python/3.7.2_2/lib/python3.7/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/kmonisit/.linuxbrew/Cellar/python/3.7.2_2/lib/python3.7/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: librt.so.1: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```

**Successful case**
If I do:
```
$ LD_LIBRARY_PATH=/home/kmonisit/.linuxbrew/lib:$LD_LIBRARY_PATH python3
>>> import tensorflow
>>> # import success!
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Full traceback already included above.
"
26431,"[doc] dead link in ""Build and load a SavedModel""","**System information**
- TensorFlow version: master
- Doc Link: https://www.tensorflow.org/guide/saved_model


**Describe the documentation issue**

The link <https://www.tensorflow.org/tfx/guide/serving/serving_basic> for ""TensorFlow serving"" is no long available in the doc [Build and load a SavedModel](https://www.tensorflow.org/guide/saved_model#build_and_load_a_savedmodel)
"
26430,Keras model saving is not working when graph is finalized,"- TensorFlow installed from (source or binary): from pip
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6

TLDR) A Keras model used in a static-graph and session mode cannot save its weight when the graph is finalized.

```python
import tensorflow as tf
model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(256)
])

x = tf.zeros([10, 3])   # dummy input
model(x)

sess = tf.InteractiveSession()
sess.run(tf.global_variables_initializer())

# finalize the graph (or it's done automatically in MonitoredSession)
tf.get_default_graph().finalize()

# Error!
model.save('/tmp/keras-test')
```

The error is `RuntimeError: Graph is finalized and cannot be modified.` Stacktrace:

```
  File ""$PREFIX/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 1415, in save_weights
    saving.save_weights_to_hdf5_group(f, self.layers)
  File ""$PREFIX/lib/python3.6/site-packages/tensorflow/python/keras/engine/saving.py"", line 742, in save_weights_to_hdf5_group
    weight_values = K.batch_get_value(symbolic_weights)
  File ""$PREFIX/lib/python3.6/site-packages/tensorflow/python/keras/backend.py"", line 2819, in batch_get_value
    return get_session().run(tensors)
  File ""$PREFIX/lib/python3.6/site-packages/tensorflow/python/keras/backend.py"", line 482, in get_session
    _initialize_variables(session)
  File ""$PREFIX/lib/python3.6/site-packages/tensorflow/python/keras/backend.py"", line 758, in _initialize_variables
    [variables_module.is_variable_initialized(v) for v in candidate_vars])
  File ""$PREFIX/lib/python3.6/site-packages/tensorflow/python/keras/backend.py"", line 758, in <listcomp>
    [variables_module.is_variable_initialized(v) for v in candidate_vars])
  File ""$PREFIX/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py"", line 193, in wrapped
    return _add_should_use_warning(fn(*args, **kwargs))
  File ""$PREFIX/lib/python3.6/site-packages/tensorflow/python/ops/variables.py"", line 2924, in is_variable_initialized
    return state_ops.is_variable_initialized(variable)
  File ""$PREFIX/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py"", line 133, in is_variable_initialized
    return ref.is_initialized(name=name)
  File ""$PREFIX/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 833, in is_initialized
    return gen_resource_variable_ops.var_is_initialized_op(self.handle, name)
  File ""$PREFIX/lib/python3.6/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py"", line 1334, in var_is_initialized_op
    ""VarIsInitializedOp"", resource=resource, name=name)
  File ""$PREFIX/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""$PREFIX/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""$PREFIX/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3272, in create_op
    self._check_not_finalized()
  File ""$PREFIX/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2945, in _check_not_finalized
    raise RuntimeError(""Graph is finalized and cannot be modified."")
RuntimeError: Graph is finalized and cannot be modified.
```

The operation being created here is:

```
 <tf.Operation 'VarIsInitializedOp_1' type=VarIsInitializedOp>,
 <tf.Operation 'VarIsInitializedOp_2' type=VarIsInitializedOp>]
```

From [`keras/backend.py`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L761):

```python
 is_initialized = session.run(
     [variables_module.is_variable_initialized(v) for v in candidate_vars])
```

Well, I think we should not create and call this ""new op"" to check whether a variable is initialized. Why is it implemented in this way? Not only it doesn't work, ~but also it will result in op leak as well even though it would work without finalizing the graph?~ (UPD: it seems that the result is cached through `_keras_initialized` so it won't be the case) There is another way to check whether the variable is initialized or not without creating a new operation."
26429,Cannot import tensorflow,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution  Linux Ubuntu 18.04):
- TensorFlow installed from (source or binary): pip3 install -U tf-nightly-gpu-2.0-preview
- TensorFlow version: cannot be imported
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
Import Error : 

**Provide the exact sequence of commands / steps that you executed before running into the problem**

---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
~/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>()
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

~/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in <module>()
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

~/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

~/anaconda3/lib/python3.6/imp.py in load_module(name, file, filename, details)
    241         else:
--> 242             return load_dynamic(name, filename, file)
    243     elif type_ == PKG_DIRECTORY:

~/anaconda3/lib/python3.6/imp.py in load_dynamic(name, path, file)
    341             name=name, loader=loader, origin=path)
--> 342         return _load(spec)
    343 

ImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-1-1ceb9a7c68bc> in <module>()
      2 import time
      3 
----> 4 import tensorflow as tf
      5 from tensorflow.python.ops import lookup_ops
      6 

~/.local/lib/python3.6/site-packages/tensorflow/__init__.py in <module>()
     25 import sys as _sys
     26 
---> 27 from tensorflow._api.v2 import audio
     28 from tensorflow._api.v2 import autograph
     29 from tensorflow._api.v2 import bitwise

~/.local/lib/python3.6/site-packages/tensorflow/_api/v2/audio/__init__.py in <module>()
      6 from __future__ import print_function as _print_function
      7 
----> 8 from tensorflow.python.ops.gen_audio_ops import decode_wav
      9 from tensorflow.python.ops.gen_audio_ops import encode_wav
     10 

~/.local/lib/python3.6/site-packages/tensorflow/python/__init__.py in <module>()
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 # Protocol buffers

~/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>()
     72 for some common reasons and solutions.  Include the entire stack trace
     73 above this error message when asking for help."""""" % traceback.format_exc()
---> 74   raise ImportError(msg)
     75 
     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/ubuntu/anaconda3/lib/python3.6/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/ubuntu/anaconda3/lib/python3.6/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.
**Any other info / logs**
Installed Anaconda using : wget https://repo.anaconda.com/archive/Anaconda3-5.0.0-Linux-x86_64.sh
python3 --version gives python 3.6.2
Installed tensorflow 2 using cmd : pip3 install -U tf-nightly-gpu-2.0-preview

Opened Jupyter notebook and doing : import tensorflow as tf
"
26425,How to predict images using trained mobilenet model?,"Hi ,I somehow managed to train my mobile net model but i want to know with the model trained, how can i  use it to make predictions about images?
Kindly Suggest it will be a great help!


"
26422,using TFlite: ModuleNotFoundError: No module named '_tensorflow_wrap_interpreter_wrapper',"**System information**
- Windows 10:
- TensorFlow installed from pip:
- TensorFlow version 1.12.0:
- Python 3.6.8:

Failed when trying to run a mobilenet_v1_1.0_224.tflite model using tflite. So I copy the interpreter_test.py and run it. End up with 10 fails in 12 tests, all with the same error of '_tensorflow_wrap_interpreter_wrapper'.

Also, my D:\Python 36\Lib\site-packages\tensorflow\contrib\lite\python\interpreter_wrapper folder only has : 
/__pycache__ , tensorflow_wrap_interpreter_wrapper.py , __init__py

I rename the tensorflow_wrap_interpreter_wrapper.py and uploaded it to see if some one can help me.
[tensorflow_wrap_interpreter_wrapper.txt](https://github.com/tensorflow/tensorflow/files/2939561/tensorflow_wrap_interpreter_wrapper.txt)

**Error log**
Traceback (most recent call last):
  File ""D:\Python 36\lib\site-packages\tensorflow\contrib\lite\python\interpreter_wrapper\tensorflow
_wrap_interpreter_wrapper.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_tensorflow_wrap_interpreter_wrapper', [dirname(__f
ile__)])
  File ""D:\Python 36\lib\imp.py"", line 297, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_tensorflow_wrap_interpreter_wrapper'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""run_tflite.py"", line 52, in <module>
    tflite_output = run(model_file, image_data)
  File ""run_tflite.py"", line 16, in run
    interpreter = interpreter_wrapper.Interpreter(model_path=model_file)
  File ""D:\Python 36\lib\site-packages\tensorflow\contrib\lite\python\interpreter.py"", line 52, in _
_init__
    _interpreter_wrapper.InterpreterWrapper_CreateWrapperCPPFromFile(
  File ""D:\Python 36\lib\site-packages\tensorflow\python\util\lazy_loader.py"", line 53, in __getattr
__
    module = self._load()
  File ""D:\Python 36\lib\site-packages\tensorflow\python\util\lazy_loader.py"", line 42, in _load
    module = importlib.import_module(self.__name__)
  File ""D:\Python 36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 665, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 678, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""D:\Python 36\lib\site-packages\tensorflow\contrib\lite\python\interpreter_wrapper\tensorflow
_wrap_interpreter_wrapper.py"", line 28, in <module>
    _tensorflow_wrap_interpreter_wrapper = swig_import_helper()
  File ""D:\Python 36\lib\site-packages\tensorflow\contrib\lite\python\interpreter_wrapper\tensorflow
_wrap_interpreter_wrapper.py"", line 20, in swig_import_helper
    import _tensorflow_wrap_interpreter_wrapper
ModuleNotFoundError: No module named '_tensorflow_wrap_interpreter_wrapper'

"
26421,Syntax Error in Udacity example code,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6.8
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: 10.0
- GPU model and memory: nVIDIA GTX 1050 Ti


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
A syntax error is raised.

**Describe the expected behavior**
The example code should work without modifications.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
The first code block on:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/1_notmnist.ipynb

gives an error when it tries to execute:
```python
# Config the matplotlib backend as plotting inline in IPython
%matplotlib inline
```
`SyntaxError: invalid syntax`

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26420,/var/lib/jenkins/workspace/19_EMBEDDED_AI_SRID/Source/tensorflow/python/eager/BUILD:10:1: undeclared inclusion(s) in rule '//tensorflow/python/eager:pywrap_tfe_lib':,"System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): source
TensorFlow version: 1.13.0rc0
Python version: 3.6.5
Installed using virtualenv? pip? conda?: pip
Bazel version (if compiling from source): 0.21
GCC/Compiler version (if compiling from source):  5.4.0 20160609
CUDA/cuDNN version: 9.0
GPU model and memory: RTX 2070 and 8GB
Describe the problem

ERROR: /var/lib/jenkins/workspace/19_EMBEDDED_AI_SRID/Source/tensorflow/python/eager/BUILD:10:1: undeclared inclusion(s) in rule '//tensorflow/python/eager:pywrap_tfe_lib':
this rule is missing dependency declarations for the following files included by 'tensorflow/python/eager/pywrap_tensor.cc':
  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/arrayobject.h'
  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/ndarrayobject.h'
  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/ndarraytypes.h'
  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/npy_common.h'
  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/numpyconfig.h'
  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/_numpyconfig.h'
  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/npy_endian.h'
  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/npy_cpu.h'
  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/utils.h'
  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/_neighborhood_iterator_imp.h'
  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/__multiarray_api.h'
  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/npy_interrupt.h'
  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/ufuncobject.h'
  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/npy_math.h'
  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/npy_common.h'
  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/arrayobject.h'
  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/__ufunc_api.h'
Target //tensorflow/tools/pip_package:build_pip_package failed to build"
26419,ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1711991009] and type int32 on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc,"I tried to run a VAE code in my new desktop. It can run the tensorflow with GPU. However, when I tried some code in github, the error comes.
Code source: https://github.com/hwalsuklee/tensorflow-mnist-VAE

System information

OS Platform and Distribution: Windows 10
TensorFlow installed from: source
TensorFlow version: 1.12
Python version: 3.6.8
Installed using virtualenv? pip? conda?: pip
Bazel version (if compiling from source): 0.17
CUDA/cuDNN version: cuda 10.0 cuDNN 7.4.2
GPU model and memory: Nvidia RTX 2080ti

Error:

G:\PythonLib\tensorflow-mnist-VAE-master>python run_main.py --dim_z 2
Extracting data\train-images-idx3-ubyte.gz
Extracting data\train-labels-idx1-ubyte.gz
Extracting data\t10k-images-idx3-ubyte.gz
Extracting data\t10k-labels-idx1-ubyte.gz
2019-03-06 21:47:07.963506: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2019-03-06 21:47:08.167281: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.635
pciBusID: 0000:01:00.0
totalMemory: 11.00GiB freeMemory: 9.03GiB
2019-03-06 21:47:08.173938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2019-03-06 21:47:08.579713: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-03-06 21:47:08.584136: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0
2019-03-06 21:47:08.586602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N
2019-03-06 21:47:08.588443: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8706 MB memory) -> physica
l GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)
Traceback (most recent call last):
  File ""C:\Users\shica\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1334, in _do_call
    return fn(*args)
  File ""C:\Users\shica\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""C:\Users\shica\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Incompatible shapes: [2] vs. [0]
         [[{{node gradients/Sum_1_grad/floordiv}} = FloorDiv[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](gradients/Sum_1_grad/Shape, gradients/Sum_1_grad/Maximum)]]

         [[{{node Neg_1/_25}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_
device_incarnation=1, tensor_name=""edge_823_Neg_1"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""run_main.py"", line 323, in <module>
    main(args)
  File ""run_main.py"", line 291, in main
    feed_dict={x_hat: batch_xs_input, x: batch_xs_target, keep_prob : 0.9})
  File ""C:\Users\shica\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 929, in run
    run_metadata_ptr)
  File ""C:\Users\shica\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\Users\shica\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1328, in _do_run
    run_metadata)
  File ""C:\Users\shica\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Incompatible shapes: [2] vs. [0]
         [[node gradients/Sum_1_grad/floordiv (defined at run_main.py:228)  = FloorDiv[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](gradients/Sum_1_grad/Shape, gradi
ents/Sum_1_grad/Maximum)]]
         [[{{node Neg_1/_25}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_
device_incarnation=1, tensor_name=""edge_823_Neg_1"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

Caused by op 'gradients/Sum_1_grad/floordiv', defined at:
  File ""run_main.py"", line 323, in <module>
    main(args)
  File ""run_main.py"", line 228, in main
    train_op = tf.train.AdamOptimizer(learn_rate).minimize(loss)
  File ""C:\Users\shica\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\training\optimizer.py"", line 400, in minimize
    grad_loss=grad_loss)
  File ""C:\Users\shica\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\training\optimizer.py"", line 519, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""C:\Users\shica\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\gradients_impl.py"", line 630, in gradients
    gate_gradients, aggregation_method, stop_gradients)
  File ""C:\Users\shica\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\gradients_impl.py"", line 814, in _GradientsHelper
    lambda: grad_fn(op, *out_grads))
  File ""C:\Users\shica\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\gradients_impl.py"", line 408, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""C:\Users\shica\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\gradients_impl.py"", line 814, in <lambda>
    lambda: grad_fn(op, *out_grads))
  File ""C:\Users\shica\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\math_grad.py"", line 82, in _SumGrad
    tile_scaling = _safe_shape_div(input_shape, output_shape_kept_dims)
  File ""C:\Users\shica\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\math_grad.py"", line 35, in _safe_shape_div
    return x // math_ops.maximum(y, 1)
  File ""C:\Users\shica\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\math_ops.py"", line 866, in binary_op_wrapper
    return func(x, y, name=name)
  File ""C:\Users\shica\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\math_ops.py"", line 1111, in floordiv
    return gen_math_ops.floor_div(x, y, name=name)
  File ""C:\Users\shica\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\gen_math_ops.py"", line 3079, in floor_div
    ""FloorDiv"", x=x, y=y, name=name)
  File ""C:\Users\shica\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\Users\shica\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\util\deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""C:\Users\shica\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\ops.py"", line 3274, in create_op
    op_def=op_def)
  File ""C:\Users\shica\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\ops.py"", line 1770, in __init__
    self._traceback = tf_stack.extract_stack()

...which was originally created as op 'Sum_1', defined at:
  File ""run_main.py"", line 323, in <module>
    main(args)
  File ""run_main.py"", line 225, in main
    y, z, loss, neg_marginal_likelihood, KL_divergence = vae.autoencoder(x_hat, x, dim_img, dim_z, n_hidden, keep_prob)
  File ""G:\PythonLib\tensorflow-mnist-VAE-master\vae.py"", line 82, in autoencoder
    KL_divergence = 0.5 * tf.reduce_sum(tf.square(mu) + tf.square(sigma) - tf.log(1e-8 + tf.square(sigma)) - 1, 1)
  File ""C:\Users\shica\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\util\deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""C:\Users\shica\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\math_ops.py"", line 1345, in reduce_sum
    name=name))
  File ""C:\Users\shica\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\gen_math_ops.py"", line 8389, in _sum
    name=name)
  File ""C:\Users\shica\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\Users\shica\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\util\deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""C:\Users\shica\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\ops.py"", line 3274, in create_op
    op_def=op_def)
  File ""C:\Users\shica\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\ops.py"", line 1770, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): Incompatible shapes: [2] vs. [0]
         [[node gradients/Sum_1_grad/floordiv (defined at run_main.py:228)  = FloorDiv[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](gradients/Sum_1_grad/Shape, gradi
ents/Sum_1_grad/Maximum)]]
         [[{{node Neg_1/_25}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_
device_incarnation=1, tensor_name=""edge_823_Neg_1"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]
"
26418,How to install tensorflow2.0 in cuda9?,"Any way to specific a cuda version with tensorflow2.0 alpha version? To  reinstall cuda is really risky, cause there are many other projects based on caffe which does not support an advanced cuda"
26417,Missing warning or documentation for upcast in sparse_softmax_cross_entropy_with_logits?,"In the ""sparse_softmax_cross_entropy_with_logits"" function inside tensorflow/python/ops/nn_ops.py, logits that are in fp16 are automatically upcasted to fp32. At the end of the function, the result is cast back down to fp16. 

This functionality ought to be exposed in documentation and/or a warning. For users doing mixed precision training, the expected behavior would be that the entire loss (softmax + cross entropy) is done in fp16. 

Relevant lines:
```
logits = ops.convert_to_tensor(logits)
precise_logits = math_ops.cast(logits, dtypes.float32) if (dtypes.as_dtype(
	logits.dtype) == dtypes.float16) else logits
...
cost, _ = gen_nn_ops.sparse_softmax_cross_entropy_with_logits(
	precise_logits, labels, name=name)
```

Thanks so much.
"
26416,Installation issues with cuda10.1,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS  Linux Ubuntu 16.04:
- TensorFlow installed from (source or binary): pip install tensorflow-gpu==2.0.0-alpha0
- TensorFlow version:2.0.0-alpha0
- Python version:2.7
- Installed using virtualenv? pip? conda?:pip 
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.1
- GPU model and memory:Titan



**Describe the problem**

After the installation of tensorflow2.0 alpha package and running the python file
with 
*import tensorflow as tf*
getting below error :

""ImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory
""

**Any other info / logs**
File ""/home/deep/tf2.0/local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 27, in <module>
    from tensorflow._api.v2 import audio
  File ""/home/deep/tf2.0/local/lib/python2.7/site-packages/tensorflow/_api/v2/audio/__init__.py"", line 8, in <module>
    from tensorflow.python.ops.gen_audio_ops import decode_wav
  File ""/home/deep/tf2.0/local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/deep/tf2.0/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/deep/tf2.0/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/deep/tf2.0/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/deep/tf2.0/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory

"
26414,ResNet50 in Eager mode. Model training set to True and False,"I am using the Resnet50 code maintained under Eager Execution as is. I am using the following code for training: **_prediction = model(x, training=True)_**
When I try to validate while training, i replace **training=True** with **training=False**. This however, results in a significant change in the loss value as if the model has not been trained at all. 
Can you please explain to me what is going on, and how to correct this issue?  "
26413,AttributeError: module 'tensorflow._api.v1.lite' has no attribute 'Optimize' version 1.13,"following the Post-training quantization guide,
```
converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]
AttributeError: module 'tensorflow._api.v1.lite' has no attribute 'Optimize'
```
i want to convert the mnist model, full code here
```
from __future__ import absolute_import, division, print_function
import os
import tensorflow as tf
from tensorflow import keras

tf.__version__

(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()

train_labels = train_labels[:1000]
test_labels = test_labels[:1000]

train_images = train_images[:1000].reshape(-1, 28 * 28) / 255.0
test_images = test_images[:1000].reshape(-1, 28 * 28) / 255.0

# Returns a short sequential model
def create_model():
  model = tf.keras.models.Sequential([
    keras.layers.Dense(512, activation=tf.keras.activations.relu, input_shape=(784,)),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(10, activation=tf.keras.activations.softmax)
  ])
  
  model.compile(optimizer=tf.keras.optimizers.Adam(),
                loss=tf.keras.losses.sparse_categorical_crossentropy,
                metrics=['accuracy'])
  
  return model


# Create a basic model instance
model = create_model()
model.fit(train_images, train_labels, epochs=5)
model.summary()
model.save('my_mnist_v2.h5')

converter = tf.lite.TFLiteConverter.from_keras_model_file(""my_mnist_v2.h5"")
converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]
tflite_quant_model = converter.convert()
```
tf version 1.13"
26412,Segmentation fault when loading SSD-MobileNetV1 into TF Lite calibration tool,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Arch Linux (4.20.1-arch1-1-ARCH SMP PREEMPT)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary):
Source
- TensorFlow version (use command below):
TensorFlow Git commit ID: 12606ff846566dd842444f27f7fed4f911a58580
TensorFlow Models Git commit ID: [`338088d`](https://github.com/tensorflow/models/commit/338088df1782c0bf2dc105170730f2230edf2df7)
- Python version:
3.7.2
- Bazel version (if compiling from source):
0.21.0-1
- GCC/Compiler version (if compiling from source):
8.2.1
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
In Python, calling `tensorflow.lite.python.optimize.calibrator.Calibrator(float32_tflite_ssd_mobilenetv1_model_contents)` leads to a segmentation-fault.

**Describe the expected behavior**
No segmentation-fault when instantiating `Calibrator` with the floating-point SSD-MobileNetV1 TF Lite model.

**Code to reproduce the issue**

```bash
$ wget  http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2018_01_28.tar.gz
$ tar vxf ssd_mobilenet_v1_coco_2018_01_28.tar.gz
$ cd ssd_mobilenet_v1_coco_2018_01_28
<Configure paths in pipeline.config to point to COCO dataset and label-map (see tensorflow/models/research/object_detection/data/mscoco_label_map.pbtxt)>
$ python <Path to workspace>/tensorflow/models/research/object_detection/export_tflite_ssd_graph.py --pipeline_config_path=pipeline.config --trained_checkpoint_prefix=model.ckpt --output_directory=. --add_postprocessing
$ toco --graph_def_file=tflite_graph.pb --output_file=float32.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --inference_type=FLOAT --mean_values=128 --std_dev_values=128 --change_concat_input_ranges=false --allow_custom_ops
```
```python
# load_model_into_calibrator.py
from tensorflow.lite.python.optimize import calibrator

with open('float32.tflite', 'rb') as f:
  quantizer = calibrator.Calibrator(f.read())
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

`gdb` full backtrace attached.
[gdb-backtrace.txt](https://github.com/tensorflow/tensorflow/files/2938277/gdb-backtrace.txt)

"
26411,GPU placement of tf.nn.conv2d during tf.data.Dataset.map call causes UnimplementedError (NHWC),"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Linux 4.20.13-arch1-1-ARCH #1 SMP PREEMPT Wed Feb 27 19:10:28 UTC 2019 x86_64 GNU/Linux`
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): `community python-tensorflow-opt-cuda`
- TensorFlow version (use command below): `1.13.1`
- Python version: `3.7.2`
- Bazel version (if compiling from source): None
- GCC/Compiler version (if compiling from source): None
- CUDA/cuDNN version: `10.1 / 7.5`
- GPU model and memory: `Geforce GTX 1080 Ti 11GB`


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
Evaluation of `tf.nn.conv2d` in a `tf.data.Dataset.map` call fails during `Session.run()` call of `tf.data.iterator.get_next()` with error:
```python
tensorflow.python.framework.errors_impl.UnimplementedError: Generic conv implementation only supports NHWC tensor format for now.
```
See the full error log and the attached code below for the full example.

It seems that the graph is successfully created, but the evaluation of the `tf.nn.conv2d` call (which is implicitly placed on the GPU) is not possible.
This is somehow related to the fact that this `tf.nn.conv2d` call is wrapped in a `tf.data.Dataset.map` call.
Setting `data_format=""NHWC""` or `use_cudnn_on_gpu=False` procudes the same error.

**Describe the expected behavior**
`tf.nn.conv2d` should be evaluated, which is done iff the convolution is explicitly placed on the CPU (e.g. `with tf.device('/cpu:0'):` or `CUDA_VISIBLE_DEVICES=""""`).

**Code to reproduce the issue**
```python
import tensorflow as tf


class IteratorInitializerHook(tf.train.SessionRunHook):
    """"""Hook to initialise data iterator after Session is created.""""""

    def __init__(self, func=None):
        super(IteratorInitializerHook, self).__init__()
        self.iterator_initializer_func = func

    def after_create_session(self, session, coord):
        """"""Initialise the iterator after the session has been created.""""""
        self.iterator_initializer_func(session)


if __name__ == '__main__':

    def apply_kernel(tensor, kernel=tf.random_normal([3, 3])):
        t = tf.expand_dims(tensor, 0)
        t = tf.expand_dims(t, -1)
        k = tf.expand_dims(kernel, -1)
        k = tf.expand_dims(k, -1)

        # TODO the following line fails during Session.run
        #  call of tf.data.Iterator.get_next() (last line in this file)
        tf_conv = tf.nn.conv2d(t, k, [1, 1, 1, 1], ""SAME"")
        return tf.squeeze(tf_conv)

    def do_some_things(x, y):
        x = apply_kernel(x)
        return x, y

    n, image_shape = 100, [256, 256]

    ds = tf.data.Dataset.from_tensor_slices((
        tf.random_uniform([n] + image_shape), tf.random_uniform([n])
    ))
    ds = ds.map(do_some_things)
    iterator = tf.data.Iterator.from_structure(ds.output_types, ds.output_shapes)
    data = iterator.get_next()
    ds_init_op = iterator.make_initializer(ds)

    with tf.train.SingularMonitoredSession(
            hooks=[IteratorInitializerHook(lambda s: s.run(ds_init_op))],
            config=tf.ConfigProto(log_device_placement=True)
    ) as sess:
        _ = sess.run(data)
```

**Other info / logs**
Here is the full error log, including device placements.
[tf-conv-NHWC-issue.log](https://github.com/tensorflow/tensorflow/files/2938294/tf-conv-NHWC-issue.log)

Here is a zipped version of the python example
[tf-conv-NHWC-issue.py.zip](https://github.com/tensorflow/tensorflow/files/2938289/tf-conv-NHWC-issue.py.zip)


Not sure if this is important, but it seems that all tensorflow ops from the function which is called by `tf.data.Dataset.map` are not showing up in the device placement logs."
26410,Add Minor Versions for cuDNN & CUDA in Tested build configurations tables,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: 1.12
- Doc Link: https://www.tensorflow.org/install/source#tested_build_configurations


**Describe the documentation issue**
The tested build configurations table does not supply adequate information for the user since it does not provide minor versions for the cuDNN and CUDA tested in the build.  Please add this information.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
26409,[TF 2.0] tf.summary should be easier to use with graphs,"This feature request tracks improving the usability of tf.summary in TF 2.0 when used with graphs - specifically with `tf.function` and legacy graph mode.

Currently there are a number of interrelated limitations that make using tf.summary somewhat awkward and error-prone outside of eager mode:

- In legacy graph mode, the writer must be configured in advance
  - a writer resource handle must be created via `create_file_writer()` before any graph construction happens, or all summary-writing functions become no-ops
  - note that resource initialization itself can be deferred (by calling `writer.init()` later), but all options to initialization, in particular the logdir, still must be passed earlier to `create_file_writer()`
  - this means it's not possible to create a graph first, and then at execution time decide on the logdir it should emit summaries to; you need to define the logdir and then define the graph

- In tf.functions it's a similar story
  - the default writer is captured at graph construction time, aka the first function execution
  - if later executions of the function use the same trace, they will still reflect the default writer from the first execution, rather than picking up any new default writer that may exist
  - again, this means that you can't easily change the logdir in use at execution time; you would need to force the function to be re-traced to pick up the new default writer

- tf.function has the additional complication that it can't own any state
  - this means any writer created inside the function must be assigned a reference outside the function in order to still exist when the function is actually executed (since otherwise it's automatically deleted at the end of the trace and no longer exists at graph execution time)
  - so it's not currently possible for a tf.function to have its own internal-only summary writer (akin to how they cannot currently have function local tf.Variables)
  - furthermore, any mistakes here tend to generate opaque ""Resource not found"" errors that don't really communicate what the issue might be
  - the safest approach is to always create the writer outside the tf.function and then just ""re-enter"" it within the tf.function via `with writer.as_default()`, and make sure the writer object exists as long as the tf.function is being used

- The step and recording condition (`tf.summary.record_if()`) have milder but similar issues
  - they are also captured at graph definition time
  - however, they have a better workaround: set them to be either a `tf.Variable` or a placeholder (`tf.compat.v1.placeholder` for legacy graph mode, or a function argument for `tf.function`) and then set that value when executing the graph
"
26408,[TF 2.0] tf.summary built-in support in Estimator,"This feature request tracks making the Estimator summary-writing logic provide built-in support for the TF 2.0 tf.summary.scalar(), image(), etc. ops.

Background: the TF 1.x summary ops are supported automatically in Estimator via use of the `tf.estimator.RunConfig` option for `save_summary_steps`, which uses `tf.estimator.SummarySaverHook` under the hood. This means that Estimator model code can invoke `tf.summary.scalar()` and it will Just Work - the framework takes care of collecting the summary at the correct step or time interval and writing it out to the model output directory.

Currently, to use TF 2.0 summary ops, one needs to set up the summary writer, step tracking, etc. logic by hand."
26407,[TF 2.0] tf.summary step argument should cast to int64,"The TF 2.0 versions of tf.summary.scalar(), image(), etc now require a `step` argument at the callsite. This step argument should allow at least any integral type and probably also floating point types (at least if they have no fractional part), but it currently requires a python integer or a `tf.int64` tensor.

This results in errors like:

```python
tf.summary.scalar('foo', 22, step=tf.constant(1))
...
ValueError: Tensor conversion requested dtype int64 for Tensor with dtype int32: 'tf.Tensor(1, shape=(), dtype=int32)'
```

```python
tf.summary.scalar('foo', 22, step=1.0)
...
TypeError: Cannot convert provided value to EagerTensor. Provided value: 1.0 Requested dtype: int64
```"
26406,[TF 2.0] tf.summary centralized step management,"This feature request tracks providing a way to centrally control the notion of the ""step"" of a given summary (used e.g. as the x-axis in TensorBoard scalar plots). In TF 1.x there was the global step, but in TF 2.0 it no longer exists and global state is generally discouraged.

Background: the TF 2.0 versions of tf.summary.scalar(), image(), etc now require a `step` argument at the callsite. This is because the new ops emit summaries immediately to the default file writer, so there's no subsequent `add_summary(merged_summaries, step=step)` call at which the step can be provided. This is inconvenient when working with nested code in which summaries are written from parts of the code that don't have easy access to a step variable, or when working with frameworks where the logical ""step"" isn't easily accessible to user code.

Note that there is a provisional `tf.summary.experimental.set_step()` implementation that can be used to set a default step value or variable for all summary writing calls from that thread, but that may be changed or removed at any time because it's unclear if that's the right API for the long term."
26405,[TF 2.0] summary API migration paths,"This feature request tracks adding and improving migration paths in tf_upgrade_v2 to handle the various summary APIs.

The goal state is:
1) 1.x tf.summary migrated to tf.compat.v1.summary by default (done)
2) 1.x tf.summary migrated to tf.compat.v2.summary where possible to do correctly (not done, may not be feasible)
3) 1.x tf.contrib.summary migrated to tf.compat.v2.summary with best-effort correctness (partly done)

For (2) it's not possible to do this correctly in all cases because the summary APIs depend on non-local context in different ways (e.g. file writer creation works differently) and return different results (1.x returns a Summary protocol buffer, 2.x a boolean). It might be worth seeing if we can add an option to the upgrade script to indicate a preference for the conservative approach (stick with tf.compat.v1) or the more aggressive upgrade (migrate to tf.compat.v2, but more likely to be broken).

For (3) we can't migrate to compat.v1 because contrib is not exposed in v1, so migrating to V2 is the only path forwards. Migration rules here are only partially implemented (for `scalar`, `audio`, `image`, and `histogram`). We still need rules for migrating the actual writer and recording control symbols.

Tracked internally at b/124529441."
26404,Returned None Values when computing Gradients,"**Describe the current behavior**
I am using the **Resnet50** code maintained under **_Eager Execution_** as is. When computing gradients 
*grads = tape.gradient(loss, model.variables)
I get NONE values return in grads. I traced down the issue and realized that those none values correspond to the BatchNormalization layers. The layers are set to trainable=True, so I am not quite sure why they do not train or why they return None values. 

******Obviously, model.variables != tape.variables**** 

**Code to reproduce the issue****

_With tf.GradientTape() as tape:
      prediction = model(x, training=True)
      var_list = tape.watched.variables()
loss = compute_loss(prediction, label, operation = 'L2')
grads = tape.gradient(loss, model.variables)
optimizer.apply_gradients(zip(grads, model.variables), global_step = step))_


**Could you please explain why am I getting those None values?**

"
26403,build error with OpenMPI,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux xxxx 3.10.0-862.6.3.el7.x86_64 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 0.13 (downloaded 6/Mar/2019)
- Python version: 3.7.2
- Installed using virtualenv? pip? conda?:pip
- Bazel version (if compiling from source): 0.22.0
- GCC/Compiler version (if compiling from source): 4.8.5 
- CUDA/cuDNN version: 10.0/7.4.2
- GPU model and memory: NVIDIA V100, P100 both 16GB



**Describe the problem**  Build fails with MPI support enabled

**Provide the exact sequence of commands / steps that you executed before running into the problem**
$ bazel build --config=opt --config=noaws --config=nogcp --config=nohdfs --config=noignite --config=nokafka --config=cuda //tensorflow/tools/pip_package:build_pip_package
.....
3 errors detected in the compilation of ""/tmp/tmpxft_000633d8_00000000-7_ring.cu.compute_60.cpp1.ii"".
ERROR: /N/dc2/projects/osg-storage/tensorflow/tensorflow/contrib/mpi_collectives/BUILD:40:1: output 'tensorflow/contrib/mpi_collectives/_objs/python/ops/_mpi_ops_gpu/ring.cu.pic.o' was not created
ERROR: /N/dc2/projects/osg-storage/tensorflow/tensorflow/contrib/mpi_collectives/BUILD:40:1: not all outputs were created or valid
Target //tensorflow/tools/pip_package:build_pip_package failed to build



**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

It seems a patch for this was described here: https://github.com/tensorflow/tensorflow/issues/17437 about a year ago, it would be very useful if this could be included in the current release."
26401,Tensor::FromProto should check the type of tensor,"When deserializing a tensor from pb, there is a Tensor::FromProto function for us to use. However, this function do not do any checks on the type of the proto. This unexpected behaviour will cause core dump when the type of the proto and that of the tensor mismatch. Here is a toy example which will cause core dump when deserializing tensor proto.

```
// prepare tensor proto which have a string tensor
TensorProto proto;
Tensor tensor(DT_STRING, {1});
tensor.scalar<string>()() = ""foo"";
tensor.AsProtoField(&tensorProto);
// deserialze the tensor proto to a variant
Tensor newTensor(DT_VARIANT, {1});
if (!newTensor.FromProto(tensorProto)) {
  // FromProto is expected to failed
  return false;
}
// the software will core on getting the FooVariant
auto fooVariant = newTensor.scalar<Variant>()().get<FooVariant>();
```

The code of  Tensor::FromProto
```
bool Tensor::FromProto(Allocator* a, const TensorProto& proto) {
  CHECK_NOTNULL(a);
  TensorBuffer* p = nullptr;
  if (!TensorShape::IsValid(proto.tensor_shape())) return false;
  if (proto.dtype() == DT_INVALID) return false;
  TensorShape shape(proto.tensor_shape());
  const int64 N = shape.num_elements();
  if (N > 0 && proto.dtype()) {
    bool dtype_error = false;
    if (!proto.tensor_content().empty()) {
      const auto& content = proto.tensor_content();
      CASES_WITH_DEFAULT(proto.dtype(), p = Helper<T>::Decode(a, content, N),
                         dtype_error = true, dtype_error = true);
    } else {
      CASES_WITH_DEFAULT(proto.dtype(), p = FromProtoField<T>(a, proto, N),
                         dtype_error = true, dtype_error = true);
    }
    if (dtype_error || p == nullptr) return false;
  }
  shape_ = shape;
  set_dtype(proto.dtype());
  UnrefIfNonNull(buf_);
  buf_ = p;
  // TODO(misard) add tracking of which kernels and steps are calling
  // FromProto.
  if (buf_ != nullptr && buf_->data() != nullptr && LogMemory::IsEnabled()) {
    LogMemory::RecordTensorAllocation(""Unknown (from Proto)"",
                                      LogMemory::UNKNOWN_STEP_ID, *this);
  }
  return true;
}
```"
26399,[tensorflow.org] Devsite Code Container not working properly,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: 2.0
- Doc Link: https://www.tensorflow.org/install


**Describe the documentation issue**
[Link](https://drive.google.com/file/d/10G-wS5A_QQ1ma7YRLdCpUVmist-J3t-N/view?usp=sharing)
As seen in the .gif file in the link uploaded, the code container for ""Download a Package"" section isn't working correctly. On decreasing window size, the code container initially gets cut into half without a scroll feature and then gets completely deleted from the webpage.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
Yes"
26398,Converting tensor2tensor transformer to tf-lite graph,"I am trying to convert the frozen transformer model from tensor2tensor to a tf-lite graph:

```
$ tflite_convert --output_file=/tmp/tf-lite/mdl --graph_def_file=frozen.pb --input_arrays=wave_input --output_arrays=outputs
```

I am getting the following error 


```
Traceback (most recent call last):
  File ""/home/sfalk/miniconda3/envs/t2t/bin/tflite_convert"", line 11, in <module>
    sys.exit(main())
  File ""/home/sfalk/miniconda3/envs/t2t/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 412, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/home/sfalk/miniconda3/envs/t2t/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/home/sfalk/miniconda3/envs/t2t/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 408, in run_main
    _convert_model(tflite_flags)
  File ""/home/sfalk/miniconda3/envs/t2t/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 162, in _convert_model
    output_data = converter.convert()
  File ""/home/sfalk/miniconda3/envs/t2t/lib/python3.5/site-packages/tensorflow/contrib/lite/python/lite.py"", line 453, in convert
    **converter_kwargs)
  File ""/home/sfalk/miniconda3/envs/t2t/lib/python3.5/site-packages/tensorflow/contrib/lite/python/convert.py"", line 342, in toco_convert_impl
    input_data.SerializeToString())
  File ""/home/sfalk/miniconda3/envs/t2t/lib/python3.5/site-packages/tensorflow/contrib/lite/python/convert.py"", line 135, in toco_convert_protos
    (stdout, stderr))
RuntimeError: TOCO failed see console for info.
```

I would seem that there are a lot of unsupported operations - does this cause the error? 

The last line from the tflite_convert output says 

>  `F tensorflow/contrib/lite/toco/tooling_util.cc:968] Check failed: array->has_shape()`

I don't know if anybody can make sense out of that ...

Is it possible to fix this or is there simply no way to convert this graph at this point in time?

<details><summary><b>Click to show full tflite_convert output</b></summary>
<p>

```
2019-03-06 13:53:51.592675: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: RandomStandardNormal
2019-03-06 13:53:51.592791: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SplitV
2019-03-06 13:53:51.594222: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Cos
2019-03-06 13:53:51.594254: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: RFFT
2019-03-06 13:53:51.594260: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: stft/rfft
2019-03-06 13:53:51.594263: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ComplexAbs
2019-03-06 13:53:51.594283: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: LinSpace
2019-03-06 13:53:51.594290: I tensorflow/contrib/lite/toco/import_tensorflow.cc:189] Unsupported data type in placeholder op: 2
2019-03-06 13:53:51.594346: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: LinSpace
2019-03-06 13:53:51.594352: I tensorflow/contrib/lite/toco/import_tensorflow.cc:189] Unsupported data type in placeholder op: 2
2019-03-06 13:53:51.594399: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SplitV
2019-03-06 13:53:51.594531: I tensorflow/contrib/lite/toco/import_tensorflow.cc:189] Unsupported data type in placeholder op: 2
2019-03-06 13:53:51.594564: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.596289: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Abs
2019-03-06 13:53:51.596411: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Cos
2019-03-06 13:53:51.596466: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Where
2019-03-06 13:53:51.596473: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: transformer_ext/body/parallel_0/body/encoder/pad_reduce/get_ids/Where
2019-03-06 13:53:51.596591: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.596714: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.596835: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.597110: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.597220: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd
2019-03-06 13:53:51.597444: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.597699: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.597780: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ScatterNd
2019-03-06 13:53:51.597900: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.598021: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.598154: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.598540: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.598656: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd
2019-03-06 13:53:51.599083: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.599551: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.599635: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ScatterNd
2019-03-06 13:53:51.599755: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.599878: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.600066: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.600402: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.600518: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd
2019-03-06 13:53:51.600968: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.601443: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.601549: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ScatterNd
2019-03-06 13:53:51.601745: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.601951: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.602150: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.602506: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.602647: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd
2019-03-06 13:53:51.603160: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.603627: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.603779: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ScatterNd
2019-03-06 13:53:51.603972: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.604173: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.604374: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.604724: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.604852: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd
2019-03-06 13:53:51.605260: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.605712: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.605818: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ScatterNd
2019-03-06 13:53:51.605950: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Cos
2019-03-06 13:53:51.606029: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: MatrixBandPart
2019-03-06 13:53:51.606257: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.606486: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.606714: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.606942: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.607165: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.607388: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.607628: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.607651: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.607659: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.607666: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.607672: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.607679: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.607686: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.607693: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.607700: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.607707: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.607714: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.607720: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.607727: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.607733: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.607740: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.607747: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.607753: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.607760: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.607766: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.607802: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.607870: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: LoopCond
2019-03-06 13:53:51.607876: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: transformer_ext/while/LoopCond
2019-03-06 13:53:51.608650: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.608661: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.608668: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.608675: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.608682: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.608688: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.608694: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.608701: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.608708: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.608714: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.608720: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.608727: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.608733: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.608740: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.608746: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.608752: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.608833: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.608871: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.608910: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.608919: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.609056: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.609098: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.609238: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.609276: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.609419: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.609460: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.609746: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.609797: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.609872: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.609893: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.610025: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.610086: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.610326: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.610390: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.610453: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.610462: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.610832: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.610871: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.610896: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.611271: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.611311: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.611333: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.611369: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.611379: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.611499: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.611536: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.611675: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.611712: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.611849: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.611886: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.612169: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.612207: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.612257: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.612266: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.612391: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.612526: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.612746: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.612783: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.612833: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.612842: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.613232: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.613295: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.613330: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.613729: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.613768: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.613793: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.613841: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.613862: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.613986: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.614023: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.614161: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.614213: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.614362: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.614411: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.614710: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.614774: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.614847: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.614868: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.615001: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.615051: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.615280: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.615343: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.615419: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.615440: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.615813: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.615863: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.615899: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.616299: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff
2019-03-06 13:53:51.616350: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.616386: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.616447: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.616456: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.616582: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.616591: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.616597: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.616605: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.616611: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.616618: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.616624: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.616630: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.616636: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.616643: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.616649: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.616656: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.616662: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.616669: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.616675: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.616682: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter
2019-03-06 13:53:51.617107: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: IsFinite
2019-03-06 13:53:51.617115: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: transformer_ext/while/ReduceLogSumExp/IsFinite
2019-03-06 13:53:51.617189: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd
2019-03-06 13:53:51.617197: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd
2019-03-06 13:53:51.617203: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd
2019-03-06 13:53:51.617209: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd
2019-03-06 13:53:51.617215: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd
2019-03-06 13:53:51.617221: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd
2019-03-06 13:53:51.617227: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd
2019-03-06 13:53:51.617232: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd
2019-03-06 13:53:51.617237: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd
2019-03-06 13:53:51.617243: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd
2019-03-06 13:53:51.617249: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd
2019-03-06 13:53:51.617255: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd
2019-03-06 13:53:51.617261: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd
2019-03-06 13:53:51.617267: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd
2019-03-06 13:53:51.617310: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd
2019-03-06 13:53:51.617317: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd
2019-03-06 13:53:51.617323: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd
2019-03-06 13:53:51.617329: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd
2019-03-06 13:53:51.617335: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd
2019-03-06 13:53:51.617341: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd
2019-03-06 13:53:51.617346: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd
2019-03-06 13:53:51.617352: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd
2019-03-06 13:53:51.617357: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd
2019-03-06 13:53:51.617363: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd
2019-03-06 13:53:51.617369: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd
2019-03-06 13:53:51.617374: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd
2019-03-06 13:53:51.617380: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd
2019-03-06 13:53:51.617385: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd
2019-03-06 13:53:51.617391: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd
2019-03-06 13:53:51.617446: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd
2019-03-06 13:53:51.617453: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd
2019-03-06 13:53:51.617459: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd
2019-03-06 13:53:51.617488: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Exit
2019-03-06 13:53:51.617494: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Exit
2019-03-06 13:53:51.617499: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Exit
2019-03-06 13:53:51.956887: F tensorflow/contrib/lite/toco/tooling_util.cc:968] Check failed: array->has_shape() 
Aborted (core dumped)
```

</p>
</details><br/>

**System information**

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 1.12.0
- Python version: 3.5.6
- CUDA/cuDNN version: 9.0
- GPU model and memory: 4x GeForce 1080 GTX

```
$ uname -spomvi
Linux #48~16.04.1-Ubuntu SMP Tue Jan 29 18:03:48 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux

$ pip freeze | grep tensor
mesh-tensorflow==0.0.5
tensor2tensor==1.12.0
tensorboard==1.12.0
tensorflow-gpu==1.12.0
tensorflow-metadata==0.9.0
tensorflow-probability==0.5.0
```

"
26395,build_info is missing CUDA build information,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): b'v1.12.0-rc2-3-ga6d8ffae09' 1.12.0
- Python version: 3.6
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: CUDA 9.0 cuDNN 7.0.5
- GPU model and memory: NVIDIA Titan V

**Describe the current behavior**

The internal `build_info` module can be used to query information about the build, importing it like:

```py
from tensorflow.python.platform import build_info
```

 However, while in previous versions you could get for things like `build_info.cuda_version_number` or `build_info.cudnn_version_number`, now the only available relevant attribute is `build_info.is_cuda_build` (and in TensorFlow 1.13.1 also `build_info.msvcp_dll_name`).

**Describe the expected behavior**

The `build_info` module should also contain information about CUDA version among other things.

**Code to reproduce the issue**

```py
from tensorflow.python.platform import build_info
print(build_info.cuda_version_number)
# AttributeError: module 'tensorflow.python.platform.build_info' has no attribute 'cuda_version_number'
```

**Other info / logs**

The `build_info` module is generated during the build process by [`gen_build_info.py`](https://github.com/tensorflow/tensorflow/blob/v1.13.1/tensorflow/tools/build_info/gen_build_info.py). Looking through the source, it seems the now deprecated CMake build system would fill this information in [`tensorflow/contrib/cmake/CMakeLists.txt`](https://github.com/tensorflow/tensorflow/blob/v1.13.1/tensorflow/contrib/cmake/CMakeLists.txt#L557-L563), that would be used by the command defined in [`tensorflow/contrib/cmake/tf_python.cmake`](https://github.com/tensorflow/tensorflow/blob/v1.13.1/tensorflow/contrib/cmake/tf_python.cmake#L239-L240), which actually called [`gen_build_info.py`](https://github.com/tensorflow/tensorflow/blob/v1.13.1/tensorflow/tools/build_info/gen_build_info.py). Now however the corresponding Bazel build rule in [`tensorflow/tensorflow.bzl`](https://github.com/tensorflow/tensorflow/blob/v1.13.1/tensorflow/tensorflow.bzl#L2011-L2019) only provides the build type (`""cuda""` or `""cpu""`) and recently also `msvcp_dll_name` on Windows.

Even though this is not a public API, it seems like a step backwards to miss this information with respect to the CMake build system."
26394,Allow building TF + nvidia GPU targeting < sm35 if XLA is not enabled,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Gentoo
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.13.1
- Python version: 3.6.4
- Installed using virtualenv? pip? conda?: pip in venv
- Bazel version (if compiling from source): 0.21
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: 10.0
- GPU model and memory: GTX 650 Ti



I was able to successfully build TF from source with XLA enabled and compute capability 3.0. 
However, when a session is created the python interpreter exits (complaining about insufficient compute capability):
```
>>> import tensorflow as tf
>>> tf.Session()
2019-03-06 12:49:41.776396: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3500130000 Hz
2019-03-06 12:49:41.776727: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x556553ef5ee0 executing computations on platform Host. Devices:
2019-03-06 12:49:41.776741: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2019-03-06 12:49:41.809556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-03-06 12:49:41.810593: I tensorflow/compiler/xla/service/platform_util.cc:194] StreamExecutor cuda device (0) is of insufficient compute capability: 3.5 required, device is 3.0
2019-03-06 12:49:41.810666: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Internal: no supported devices found for platform CUDA

```

if I reconfigure TF by disabling XLA, and rebuild (again with compute capability 3.0), than TF works fine.

So I guess, a simple check if compute capability >= 3.5 when XLA is enabled, could at least prevent building non-functional TF."
26393,Kubernetes GPU Docs & k8s-compatible Docker Images,"- TensorFlow version: 1.13.1
- Doc Link: https://www.tensorflow.org/install/gpu

The existing documentation on how to setup TensorFlow with docker&GPU is great. Unfortunately it doesn't work with Kubernetes. This [Kubernetes Engine GPUs](https://cloud.google.com/kubernetes-engine/docs/how-to/gpus) guide suggest using ""nvidia/cuda:10.0-runtime-ubuntu18.04"" docker image. TensorFlow docs suggest ""tensorflow/tensorflow:latest-gpu"". 

Ideally, we could use them both as base images, but that's impossible. 

And of course this depends also on which VM image you're using for the k8s nodes.

It would be great if the docs provide some guidance on this setup. 

It would be really awesome if a docker image compatible with one of k8s VM images was available containing both the necessary NVidia support for a k8s node and complete tensorflow-gpu package environment."
26392,train with MirroredStrategy with multiple gpus has no speed up,"**System information**
OS Platform and Distribution: CentOS Linux release 7.3.1611
TensorFlow installed from: (pip install tensorflow-gpu)
TensorFlow version: Tensorflow 1.12
Bazel version: N/A
GPU model and memory: Tesla P40 24G
Exact command to reproduce: N/A
Mobile device: N/A
CUDA/cuDNN version: cuda 9.0 with cudnn7.1.4

**Describe the current behavior**
I trained with tensorflow for multi-gpu with MirroredStrategy and estimator. My model is a simple dnn. The layer number is 256->128->64.  when I set bigger batch_size (such as 1024), the training speed using multiple gpus  has no improvement compared with single gpu. As following picture shows:

![image](https://user-images.githubusercontent.com/11607954/53925412-da2d3000-40ba-11e9-994f-1130348d7b67.png)


What I want to ask is that the acceleration performance of MirroredStrategy has associated with the type of model? Is that the complex model like cnn just can have obvious acceleration performance when using multiple gpus?



"
26391,str_cat.h(267): error :expression must have a constant value,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):gentoo 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No, intel PC
- TensorFlow installed from (source or binary): source
- TensorFlow version:1.12.0 and 1.13.0-r2
- Python version:3.4.3
- Installed using virtualenv? pip? conda?:no 
- Bazel version (if compiling from source):0.20.0
- GCC/Compiler version (if compiling from source): 4.9.3
- CUDA/cuDNN version:8.0/6.0
- GPU model and memory:Geferfoce 1050Ti

external/com_google_absl/absl/strings/string_view.h(496): warning: expression has no effect
external/com_google_absl/absl/strings/str_cat.h(267): error: expression must have a constant value
external/com_google_absl/absl/strings/str_cat.h(267): error: expression must have a constant value

"
26390,"DLL load failed with error code -1073741795, Win 7","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): installation from https://www.tensorflow.org/install/pip
- TensorFlow version: 1.13
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: both pip, virtualenv
- Bazel version (if compiling from source):no
- GCC/Compiler version (if compiling from source): no
- CUDA/cuDNN version: tried with all CUDA8, CUDA7.5,CUDA9
- GPU model and memory:



**Describe the problem**
I tried different version of python and tensorflow and check the script for selecting CUDA version,  it has the same error. Also I installed airflow but desn't work!!!!!!
**Provide the exact sequence of commands / steps that you executed before running into the problem**
Traceback (most recent call last):
  File ""C:\Users\azak\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_t
ensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\azak\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_t
ensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\azak\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_t
ensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\azak\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\azak\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed with error code -1073741795

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Users\azak\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", l
ine 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\azak\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__
.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\azak\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_t
ensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\azak\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_t
ensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\azak\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_t
ensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\azak\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_t
ensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\azak\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\azak\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed with error code -1073741795


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26388,Bazel incompatible change: Get all tests passing with --incompatible_disable_legacy_cc_provider for Bazel 0.25.0,"It seems that bazel-watcher will start breaking with the next Bazel release 0.25 because of an incompatible change (https://github.com/bazelbuild/bazel/issues/7036)

It looks like Tensorflow is still using the legacy 'cc' provider for some Starlark rules. I explained in the issue about the steps you can take to migrate to using CcInfo. The same functionality is available with that new provider.

Here's the error:
```
ERROR: tensorflow/tensorflow/lite/python/optimize/BUILD:28:1: in deps attribute of _py_wrap_cc rule //tensorflow/lite/python/optimize:tensorflow_lite_wrap_calibration_wrapper_py_wrap: '//tensorflow/lite/python/optimize:calibration_wrapper_lib' does not have mandatory providers: 'cc'. Since this rule was created by the macro 'tf_py_wrap_cc', the error might have been caused by the macro implementation in tensorflow/tensorflow/tensorflow.bzl:1743:16
Analyzing [0m target //tensorflow/tools/pip_package:build_pip_package (0 packages\
 loaded, 15887 targets configured)


ERROR: tensorflow/tensorflow/lite/python/optimize/BUILD:28:1: in deps attribute of _py_wrap_cc rule //tensorflow/lite/python/optimize:tensorflow_lite_wrap_calibration_wrapper_py_wrap: '@local_config_python//:python_headers' does not have mandatory providers: 'cc'. Since this rule was created by the macro 'tf_py_wrap_cc', the error might have been caused by the macro implementation in tensorflow/tensorflow/tensorflow.bzl:1743:16
Analyzing:[0m target //tensorflow/tools/pip_package:build_pip_package (0 packages\
 loaded, 15894 targets configured)


ERROR: tensorflow/tensorflow/core/BUILD:2731:1: in deps attribute of _transitive_hdrs rule //tensorflow/core:framework_internal_headers_lib_gather: '//tensorflow/core:lib' does not have mandatory providers: 'cc'. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in tensorflow/tensorflow/tensorflow.bzl:1505:54
Analyzing:[0m target //tensorflow/tools/pip_package:build_pip_package (0 packages\
 loaded, 18659 targets configured)


ERROR: tensorflow/tensorflow/core/BUILD:2731:1: in deps attribute of _transitive_hdrs rule //tensorflow/core:framework_internal_headers_lib_gather: '//tensorflow/core:lib_internal' does not have mandatory providers: 'cc'. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in tensorflow/tensorflow/tensorflow.bzl:1505:54
Analyzing:[0m target //tensorflow/tools/pip_package:build_pip_package (0 packages\
 loaded, 18662 targets configured)


ERROR: tensorflow/tensorflow/core/BUILD:2731:1: in deps attribute of _transitive_hdrs rule //tensorflow/core:framework_internal_headers_lib_gather: '//tensorflow/core:version_lib' does not have mandatory providers: 'cc'. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in tensorflow/tensorflow/tensorflow.bzl:1505:54
Analyzing:[0m target //tensorflow/tools/pip_package:build_pip_package (0 packages\
 loaded, 18665 targets configured)

ERROR: [0mtensorflow/tensorflow/core/BUILD:2731:1: in deps attribute of _transitive_hdrs rule //tensorflow/core:framework_internal_headers_lib_gather: '//tensorflow/core:framework_bounds_check' does not have mandatory providers: 'cc'. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in tensorflow/tensorflow/tensorflow.bzl:1505:54
Analyzing:[0m target //tensorflow/tools/pip_package:build_pip_package (0 packages\
 loaded, 18666 targets configured)

ERROR: [0mtensorflow/tensorflow/core/BUILD:2731:1: in deps attribute of _transitive_hdrs rule //tensorflow/core:framework_internal_headers_lib_gather: '//tensorflow/core/platform/default/build_config:platformlib' does not have mandatory providers: 'cc'. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in tensorflow/tensorflow/tensorflow.bzl:1505:54
Analyzing:[0m target //tensorflow/tools/pip_package:build_pip_package (0 packages\
 loaded, 18669 targets configured)

ERROR: [0mtensorflow/tensorflow/core/BUILD:2842:1: in deps attribute of _transitive_hdrs rule //tensorflow/core:stream_executor_headers_lib_gather: '//tensorflow/core:stream_executor' does not have mandatory providers: 'cc'. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in tensorflow/tensorflow/tensorflow.bzl:1505:54
Analyzing:[0m target //tensorflow/tools/pip_package:build_pip_package (0 packages
```"
26386,building error on win7 with VS2015: ADD_LIBRARY for library tf_contrib_tpu_ops without any source files,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win7 X64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source 
- TensorFlow version:1.13.1
- Python version:3.6
- Installed using virtualenv? pip? conda?:conda
- Bazel version (if compiling from source):cmake
- GCC/Compiler version (if compiling from source):vs 2015
- CUDA/cuDNN version:no cuda
- GPU model and memory:no gpu



**Describe the problem**
when I use cmake GUI config TF, it show me the folowing error message:
""You have called ADD_LIBRARY for library tf_contrib_tpu_ops without any source files. This typically indicates a problem with your CMakeLists.txt file
CMake Error at tf_python.cmake:217 (message):
  Python module not found: tensorflow/contrib/tpu/ops""
It looks no ""ops"" under tensorflow/contrib/tpu, anyone have idea how to find the relate source files or how to block the tpu module in config file?
many thanks~
**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26385,TensorFlow Lite Android nightly runs inference x2 faster than latest version 1.13.1,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: OnePlus 6
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): `org.tensorflow:tensorflow-lite:0.0.0-nightly` / `org.tensorflow:tensorflow-lite:1.13.1`

**Describe the current behavior**
I am a contributor of [this library](https://github.com/the-super-toys/glimpse-android) where we use TensorFlow lite to run a custom model.
When using the `org.tensorflow:tensorflow-lite:0.0.0-nightly` verstion of tensorflow lite we achieve an inference time of 22ms (the model setup takes about 5ms).
If we just change the version to `org.tensorflow:tensorflow-lite:1.13.1` without changing anything else in our code, the inference time goes up to 40ms (the model setup now takes less than 1ms).

Why could this be happening? Is this a bug or the nightly version will always provide a faster inference?
"
26383,"tf2.0, tf.keras cannot be automatically completed in pycharm","This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
26382,Tensorflow GPU Installation Requirement Issues,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version: tensorflow-gpu 1.12.0
- Python version: 3.5.2
- Installed using virtualenv? pip? conda?: virtualenv pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): 
- CUDA/cuDNN version :CUDA Version 10.0.130/ cuDNN 7.4.1
- GPU model and memory: GeForce GTX 108 



I am trying to install TensorFlow using 
`pip install --upgrade tensorflow-gpu`

I end up with the following messages:

`tensorflow 1.12.0 has requirement tensorboard<1.13.0,>=1.12.0, but you'll have tensorboard 1.13.1` `which is incompatible.`
`symfit 0.4.6 has requirement scipy>=1.0, but you'll have scipy 0.17.0 which is incompatible.`
`scikit-image 0.14.1 has requirement pillow>=4.3.0, but you'll have pillow 3.1.2 which is incompatible.`

`Cannot uninstall 'enum34'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.`

I am not sure how to reinstall the individual components with their correct version (tensorboard, symfit, scikit-image). 
"
26378,Centos7.6+python37+tensorflow1.13.1 : ImportError: /lib64/libstdc++.so.6: version `CXXABI_1.3.8' not found,"```
Python 3.7.1 (default, Dec 14 2018, 19:28:38) 
[GCC 7.3.0] :: Anaconda, Inc. on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow


Traceback (most recent call last):
  File ""/home/zhaodachuan/zhaodachuan/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/zhaodachuan/zhaodachuan/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/zhaodachuan/zhaodachuan/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/zhaodachuan/zhaodachuan/anaconda3/lib/python3.7/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/zhaodachuan/zhaodachuan/anaconda3/lib/python3.7/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: /lib64/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by /home/zhaodachuan/zhaodachuan/anaconda3/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/zhaodachuan/zhaodachuan/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/home/zhaodachuan/zhaodachuan/anaconda3/lib/python3.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/zhaodachuan/zhaodachuan/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/zhaodachuan/zhaodachuan/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/zhaodachuan/zhaodachuan/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/zhaodachuan/zhaodachuan/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/zhaodachuan/zhaodachuan/anaconda3/lib/python3.7/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/zhaodachuan/zhaodachuan/anaconda3/lib/python3.7/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: /lib64/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by /home/zhaodachuan/zhaodachuan/anaconda3/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

```



**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
CentOS Linux release 7.6.1810 (Core)
- TensorFlow installed from (source or binary):
pip install tensorflow
- TensorFlow version:
1.13.1
- Python version:
Python 3.7.1
- Installed using virtualenv? pip? conda?:
pip install tensorflow




**Describe the problem**
I think the problem is that the version of gcc is 4.8 on Centos7.6.
What should I do to install tensorflow1.13 with Centos7+python37 ?

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
$ pip install tensorflow
$ python 
>>> import tensorflow
```

"
26376,ModuleNotFoundError: No module named 'gast',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Custom code - https://github.com/SeongokRyu/augmented-GCN

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
GNU/Linux 

- TensorFlow installed from (source or binary):
conda install -c conda-forge tensorflow 

- TensorFlow version (use command below):
1.12.0

- Python version:
Python 3.6.8 

- GPU model and memory:
Processor type - x86_64

Memory:         total - 62G,    used - 58G,   free - 4.3G

**Describe the problem**
Been utilizing tensorflow 1.10.0 for a weeks or so, and the custom code I've been running gave me no issues. After updating to 1.12.0 today, I get an error rooting from the `tf.get_variable()` function: ModuleNotFoundError: No module named 'gast'.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Before running into the problem I updated tensorflow with `conda update tensorflow=1.12.0` then proceeded to run a script that calls the function which raised the problem.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

<img width=""1047"" alt=""screen shot 2019-03-05 at 21 23 10"" src=""https://user-images.githubusercontent.com/35546511/53857913-eca94b80-3f8c-11e9-9c9f-7a23260928f0.png"">

"
26375,Train Keras model with GPU Google Colab very slow when compared with my personal cpu,"**Information environment when I run [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)**

== cat /etc/issue ===============================================
Linux dc5aaefaf4e1 4.14.79+ #1 SMP Wed Dec 19 21:19:13 PST 2018 x86_64 x86_64 x86_64 GNU/Linux

== are we in docker =============================================
Yes

== compiler =====================================================
c++ (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
Copyright (C) 2017 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux dc5aaefaf4e1 4.14.79+ #1 SMP Wed Dec 19 21:19:13 PST 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
mesh-tensorflow          0.0.5                
msgpack-numpy            0.4.3.2              
numpy                    1.14.6               
protobuf                 3.6.1                
tensorflow               1.13.1               
tensorflow-estimator     1.13.0               
tensorflow-hub           0.3.0                
tensorflow-metadata      0.12.1               
tensorflow-probability   0.6.0                

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.13.1
tf.GIT_VERSION = b'v1.13.1-2-g09e3b09e69'
tf.COMPILER_VERSION = b'v1.13.1-2-g09e3b09e69'
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH /usr/lib64-nvidia
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Wed Mar  6 04:31:12 2019       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 410.79       Driver Version: 410.79       CUDA Version: 10.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |
| N/A   36C    P0    71W / 149W |      0MiB / 11441MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/lib/python3.6/dist-packages/torch/lib/libcudart-1581fefa.so.10.0
/usr/local/lib/python2.7/dist-packages/torch/lib/libcudart-1581fefa.so.10.0
/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart.so.10.0.130
/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-10.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-10.0/doc/man/man7/libcudart.7


**Describe the current behavior**
Train time of each epoch on google colab gpu: ~60s
Train time of each epoch on my personal pc cpu: ~10s

**Code to reproduce the issue**
I train crf model use keras, keras_contrib library
Code:
```python
    input = Input(shape=(config[""input_dim""],))
    model = Embedding(input_dim=config[""vocab_size""] + 1, output_dim=config[""embedding_dim""],
                      input_length=config[""input_dim""], mask_zero=True)(input)
    model = Bidirectional(LSTM(units=config[""lstm_dim""],
                               return_sequences=True, recurrent_dropout=config[""recurrent_dropout""]))(model)
    model = TimeDistributed(Dense(config[""dense_dim""], activation=""relu""))(model)
    crf = CRF(config[""num_tags""])  # CRF layer
    output = crf(model)  # output

    model = Model(input, output)
    model.compile(optimizer=config[""optimizer""], loss=crf_loss, metrics=[crf_accuracy])
    model.summary()

```

**Logs**
2019-03-06 04:13:53.402787: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz
2019-03-06 04:13:53.403069: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x23c8dc0 executing computations on platform Host. Devices:
2019-03-06 04:13:53.403105: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2019-03-06 04:13:53.491058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-03-06 04:13:53.491582: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55374a0 executing computations on platform CUDA. Devices:
2019-03-06 04:13:53.491616: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7
2019-03-06 04:13:53.492038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:04.0
totalMemory: 11.17GiB freeMemory: 11.10GiB
2019-03-06 04:13:53.492071: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2019-03-06 04:13:53.863400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-03-06 04:13:53.863478: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2019-03-06 04:13:53.863513: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
2019-03-06 04:13:53.863768: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2019-03-06 04:13:53.863821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10754 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)
2019-03-06 04:13:55.629273: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally
"
26372,[TF 2.0 API Docs] tf.keras.activations.softmax,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: 2.0
- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/activations/softmax


**Describe the documentation issue**

* The softmax activation function is not described in detail, and there is no recommendation about when to use it.

* There is no usage example.

* The description of the returned value could be more useful.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**

Yes."
26371,[TF_Java] Enable Control Dependencies to While Loop Body from Outer Graph,"*System information*

* TensorFlow version (you are using): master
* Are you willing to contribute it (Yes/No): yes

*Describe the feature and the current behavior/state.*

We would like to be able to add control dependencies to ops within the body of a while loop from ops in the outer graph. 

Why: we want to be able to update variables in the outer graph from within the while loop body. An important use case for this is training within a while loop. We understand that in Python, training within a loop is done by adding control dependencies within the loop body from the weight/bias update operations in the outer graph. We had hoped to use the same approach in Java.

Current behavior: We aren't able to add control dependencies from anything outside of the body graph. When we do, we get the following error: 

`Exception in thread ""main"" java.lang.IllegalArgumentException: Node 'node_in_body_graph': Unknown input node '^node_in_outer_graph'`

We believe this error occurs because in Java, the loop body graph is not connected to the outer graph at the time it is constructed / the time that its control dependencies are added — it is only connected to the outer graph once FinishWhile is called at the C++ level. 

*Will this change the current api? How?*

Yes, people will be able to add control dependencies to operations within the loop body from operations in the outer graph.

*Who will benefit with this feature?*

Anyone who wants to train within their Java while loop, or update variables in the outer graph for any other reason. 

*Any Other info.*

"
26368,[TF 2.0] Add skip_empty argument to tf.strings.split so that tf.string_split can be deprecated.,"There were two split functions defined in TensorFlow 1.13:

1. [`tf.string_split`](https://www.tensorflow.org/api_docs/python/tf/string_split)
2. [`string_split_v2`](https://github.com/tensorflow/tensorflow/pull/19650)

We have consolidated these two ops into [`tf.strings.split`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/strings/split) in TF 2.0:

```
tf.strings.split(
    source,
    sep=None,
    maxsplit=-1
)
```
 and wish to deprecate `tf.string_split`. However, `tf.strings.split` is missing the `skip_empty` functionality included in `string_split`:

```
tf.string_split(
    source,
    delimiter=' ',
    skip_empty=True
)
```

This feature request would be to **include `skip_empty` functionality for `tf.strings.split`**."
