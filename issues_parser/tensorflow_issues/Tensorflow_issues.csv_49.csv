Issue Number,Issue Title,Issue Body
22528,"TfLite Quantization Failure: Array dense_prev, which is an input to the Concatenation operator producing the output array dense_input, is lacking min/max data,","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.11.0-rc1
- **Python version**: 2.7

### Describe the problem
I am trying to save Quantized version of graph in tflite format but getting the following error

F tensorflow/contrib/lite/toco/tooling_util.cc:1633] Array dense_prev, which is an input to the Concatenation operator producing the output array dense_input, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.

I am not able to understand why isn't Min/Max information stored for dense_prev operator if for dense_typed it's not giving any error.  

The code to reproduce the problem  
```
import tensorflow as tf
from tensorflow.contrib.lite.toco import types_pb2 as _types_pb2
QUANTIZED_UINT8 = _types_pb2.QUANTIZED_UINT8
import numpy as np

def train_model(vocab_typed_size,vocab_prev_size, decoder_size, vocab_output_size):
    g = tf.Graph()
    with tf.Session(graph=g) as sess:

        data_x = tf.placeholder(tf.float32, shape=(vocab_typed_size + vocab_prev_size), name=""data_x"" )
        label = tf.placeholder(tf.float32, shape=(vocab_output_size), name='labels')

        data_x_typed = data_x[:vocab_typed_size]
        data_x_prev = data_x[vocab_typed_size:]
        
        embedding_typed = tf.get_variable(name=""embedding_typed"",shape=(vocab_typed_size, 100))
        embedding_prev = tf.get_variable(name=""embedding_prev"",shape=(vocab_prev_size, 100))

        dense_typed = tf.matmul(tf.reshape(data_x_typed, (1,vocab_typed_size)),embedding_typed, name=""dense_typed"")
        dense_prev = tf.matmul(tf.reshape(data_x_prev, (1,vocab_prev_size)),embedding_prev,name=""dense_prev"")
        dense_input = tf.concat([dense_prev,dense_typed], axis = 1,name=""dense_input"")

        h2_w = tf.get_variable(shape=(200 , decoder_size),name=""h2_w"")
        h2_b = tf.get_variable(shape=(1,decoder_size),name=""h2_b"")
        h2_out = tf.add(tf.matmul(dense_input, h2_w), h2_b, name =""h2_out"")

        decoder_w = tf.get_variable(shape=[decoder_size,vocab_output_size],name=""decoder_w"")     
        decoder_b = tf.get_variable(shape=[1,vocab_output_size],name=""decoder_b"")

        logits_message = tf.add(tf.matmul(tf.nn.relu(h2_out), decoder_w),decoder_b, name='logits_message')

        probs_message = tf.nn.sigmoid(logits_message, name = 'probs_sticker')

        # Loss function and optimizer
        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits_message, labels=label), name=""loss"")

        # Call the training rewrite which rewrites the graph in-place with
        # FakeQuantization nodes and folds batchnorm for training. It is
        # often needed to fine tune a floating point model for quantization
        # with this training tool. When training from scratch, quant_delay
        # can be used to activate quantization after training to converge
        # with the float graph, effectively fine-tuning the model.

        tf.contrib.quantize.create_training_graph(quant_delay=0)

        optimizer = tf.train.AdamOptimizer().minimize(loss)

        sess.run(tf.global_variables_initializer())
        saver = tf.train.Saver()
        print ""Dumping model""
        saver.save(sess, './test/oink.ckpt')

        writer = tf.summary.FileWriter(""./test/"", sess.graph)
        writer.close()
        

def eval_model(vocab_typed_size,vocab_prev_size, decoder_size, vocab_output_size):
    g = tf.Graph()
    with tf.Session(graph=g) as sess:
        data_x = tf.placeholder(tf.float32, shape=(vocab_typed_size + vocab_prev_size), name=""data_x"" )
        data_x_typed = data_x[:vocab_typed_size]
        data_x_prev = data_x[vocab_typed_size:]

        embedding_typed = tf.get_variable(name=""embedding_typed"",shape=(vocab_typed_size, 100))
        embedding_prev = tf.get_variable(name=""embedding_prev"",shape=(vocab_prev_size, 100))

        dense_typed = tf.matmul(tf.reshape(data_x_typed, (1,vocab_typed_size)),embedding_typed, name=""dense_typed"")
        dense_prev = tf.matmul(tf.reshape(data_x_prev, (1,vocab_prev_size)),embedding_prev,name=""dense_prev"")
        
        dense_input = tf.concat([dense_prev,dense_typed], axis = 1,name=""dense_input"")

        h2_w = tf.get_variable(shape=(200 , decoder_size),name=""h2_w"")
        h2_b = tf.get_variable(shape=(1,decoder_size),name=""h2_b"")
        h2_out = tf.add(tf.matmul(dense_input, h2_w), h2_b, name =""h2_out"")

        decoder_w = tf.get_variable(shape=[decoder_size,vocab_output_size],name=""decoder_w"")     
        decoder_b = tf.get_variable(shape=[1,vocab_output_size],name=""decoder_b"")

        logits_message = tf.add(tf.matmul(tf.nn.relu(h2_out), decoder_w),decoder_b, name='logits_message')
        probs_message = tf.nn.sigmoid(logits_message, name = 'probs_sticker')

        tf.contrib.quantize.create_eval_graph()

        saver = tf.train.Saver()
        saver.restore(sess, ""./test/oink.ckpt"")

        converter = tf.contrib.lite.TocoConverter.from_session(sess, [data_x], [probs_message])
        converter.quantize_weights = True
        converter.inference_type = tf.contrib.lite.constants.QUANTIZED_UINT8
        input_arrays = converter.get_input_arrays()
        print (input_arrays)
        converter.quantized_input_stats = {input_arrays[0] : (0, 2)}
        #converter.default_ranges_stats = (-1.0,1.0)
        converter.change_concat_input_ranges = True
        tflite_model = converter.convert()
        open(""sr.tflite"", ""wb"").write(tflite_model)

if __name__ == ""__main__"":  
    train_model(50000, 50000, 100, 10000)
    eval_model(50000, 50000, 100, 10000)
 ```
"
22527,[Compile Error] Compiling tf r1.8 from source in cuda9.0_cudnn7.0 container failed.,"When I compile tf r1.8 from source in cuda9.0_cudnn7.0 container following this [doc](https://www.tensorflow.org/install/source), I encountered error, the error is:
```
./tensorflow/core/util/tensor_format.h:355:3: note: in expansion of macro 'CHECK'
   CHECK(index >= 0 && index < dimension_attribute.size())
   ^
ERROR: 
/tf_compile/tensorflow_cuda9.0_cudnn7.0/tensorflow/contrib/lite/toco/python/BUILD:22:1: Linking of rule '//tensorflow/contrib/lite/toco/python:_tensorflow_wrap_toco.so' failed (Exit 1)
gcc: error: tensorflow_wrap_toco_versionscript.lds: No such file or directory
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1417.439s, Critical Path: 262.20s
INFO: 7170 processes: 7170 local.
FAILED: Build did NOT complete successfully
```

"
22526,How to reset per_process_gpu_memory_fraction in tf using c++?,"I have develop two applications based on tf in c++ language, these applications are served as libraries. In the caller execuable program, library1 is called then library2. In library1 initialization,  gpu memory fraction is set to 0.5, run some inference, and session closed. then library2 is called, gpu memory fraction is set to 0.8, but the setting can not work, gpu memory allocation did not change. Both library have the same initialization code but differnet fraction value
```
int XXXLib::init(double per_process_gpu_memory_fraction)
{
    SessionOptions options;
    ConfigProto* config = &options.config;
    GPUOptions* gpu_options = config->mutable_gpu_options();

    // for library1, fraction = 0.5; for library2, fraction = 0.8
    gpu_options->set_per_process_gpu_memory_fraction(per_process_gpu_memory_fraction);
    Status status = NewSession(options, &_session);
}
```
It seems that when set_per_process_gpu_memory_fraction() is called, the gpu memory in this process is fixed, even new another Newsession(), the original fraction value is used.
1. Should different app(library) use different session ?
2. gpu memory fraction is related to session or to process ?
3. How  to change the fraction in different session but the same process? 

Some env info:
- Have I written custom code? NO
-  OS Platform and Distribution? Win10 Pro
-  TensorFlow installed from? Source code
-  TensorFlow version? 1.9
-  CUDA/cuDNN version? CUDA9.0, cudnn 7.05
-  GPU model and memory? GTX1080 with 8GB memory"
22525,kernel version 384.130.0 does not match DSO version 375.66.0 -- cannot find working device,">>> sess = tf.Session()
2018-09-26 16:20:01.272192: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX
2018-09-26 16:20:01.273306: E tensorflow/stream_executor/cuda/cuda_driver.cc:406] failed call to cuInit: CUDA_ERROR_NO_DEVICE
2018-09-26 16:20:01.273383: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: hito-Lenovo
2018-09-26 16:20:01.273406: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: hito-Lenovo
2018-09-26 16:20:01.273480: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 375.66.0
2018-09-26 16:20:01.273609: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:369] driver version file contents: """"""NVRM version: NVIDIA UNIX x86_64 Kernel Module  384.130  Wed Mar 21 03:37:26 PDT 2018
GCC version:  gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.10) 
""""""
2018-09-26 16:20:01.273659: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 384.130.0
2018-09-26 16:20:01.273681: E tensorflow/stream_executor/cuda/cuda_diagnostics.cc:303] kernel version 384.130.0 does not match DSO version 375.66.0 -- cannot find working devices in this configuration
"
22524,assign_add seems giving wrong and random results in broadcasting,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Darwin localhost 17.5.0 Darwin Kernel Version 17.5.0: Fri Apr 13 19:32:32 PDT 2018; root:xnu-4570.51.2~1/RELEASE_X86_64 x86_64
Mac OS X 10.13.4
- **TensorFlow installed from (source or binary)**:
binary

- **TensorFlow version (use command below)**:
v1.10.0-12-g4dcfddc5d1 1.10.1

- **Python version**:
2.7 & 3.7

- **Bazel version**:
N/A
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
See below
- **Mobile device**:
N/A


### Describe the problem

I ran the very simple code below and I've got random meaningless results. The first element of the result is always correct. So it seems that the broadcasting is not working, i.e. only the first element has been correctly processed. But I think people would assume `assign_add` works in simple case like `assign_add(_, 0.0)` (or it should complain rather than silently give wrong results). 

### Source code / logs

```
import tensorflow as tf
import tensorflow.contrib.eager as tfe
tf.enable_eager_execution()

Z = tfe.Variable([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
print(Z.numpy())
tf.assign_add(Z, 0.0)
print(Z.numpy())
```

One sample result:

```
[[1. 2. 3.]
 [4. 5. 6.]]
[[1.00000000e+00 6.55319777e+13 2.69145827e+20]
 [3.84908544e+09 1.23657954e+33 6.00000000e+00]]
```

"
22522,crosstool_wrapper_driver_is_not_gcc failed: error executing command,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.11.0
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: 0.17.2
- **GCC/Compiler version (if compiling from source)**: c++ (Ubuntu 7.3.0-16ubuntu3) 7.3.0
- **CUDA/cuDNN version**: 10.0 / 7.3.0
- **GPU model and memory**: GeForce 940MX
- **Exact command to reproduce**: bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 --config=cuda --verbose_failures //tensorflow/tools/pip_package:build_pip_package

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
When the VERBS support is enabled the tensorflow build fails with the following error message:
**~/Documents/dev/git/tensorflow/tensorflow/contrib/verbs/BUILD:90:1: C++ compilation of rule '//tensorflow/contrib/verbs:rdma_rendezvous_mgr' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command**
Despite the fact that it was reported multiple times before that the changes have been merged into the trunk and will be merged and therefore included in the next release candidates, we already have ""stable"" release and this issue is still observed.
This issue was observed in the 1.11.0rc1 and 1.11.0rc2 versions of tensorflow as well.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
ERROR: ~/Documents/dev/git/tensorflow/tensorflow/contrib/verbs/BUILD:90:1: C++ compilation of rule '//tensorflow/contrib/verbs:rdma_rendezvous_mgr' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd ~/.cache/bazel/_bazel_vyepishov/cf67b2b2e967476eb2b1ee98e33ab5bd/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu \
    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \
    NCCL_INSTALL_PATH=/usr/local/nccl_2.3.4-1+cuda10.0_x86_64 \
    PATH=~/bin:/usr/local/sbin:/usr/local/lib:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/linuxbrew/.linuxbrew/bin:/home/linuxbrew/.linuxbrew/sbin:/home/linuxbrew/.linuxbrew/opt/coreutils/libexec/gnubin:/usr/local/cuda/bin:/usr/local/share/apache/hadoop/sbin:/usr/local/share/apache/hadoop/bin:/usr/local/share/apache/spark/sbin:/usr/local/share/apache/spark/bin:/usr/games:/usr/local/games:~/bin:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CUDA_CLANG=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=5.0 \
    TF_CUDA_VERSION=10.0 \
    TF_CUDNN_VERSION=7 \
    TF_NCCL_VERSION=2 \
    TF_NEED_CUDA=1 \
    TF_NEED_OPENCL_SYCL=0 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/contrib/verbs/_objs/rdma_rendezvous_mgr/rdma_rendezvous_mgr.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/contrib/verbs/_objs/rdma_rendezvous_mgr/rdma_rendezvous_mgr.pic.o' '-DGRPC_ARES=0' '-DPB_FIELD_16BIT=1' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -DTENSORFLOW_USE_JEMALLOC -DTF_USE_SNAPPY -DTENSORFLOW_USE_VERBS -DTENSORFLOW_USE_GDR -DCURL_STATICLIB -DPLATFORM_LINUX -DENABLE_CURL_CLIENT -DENABLE_NO_ENCRYPTION -iquote . -iquote bazel-out/k8-opt/genfiles -iquote bazel-out/k8-opt/bin -iquote external/protobuf_archive -iquote bazel-out/k8-opt/genfiles/external/protobuf_archive -iquote bazel-out/k8-opt/bin/external/protobuf_archive -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -iquote bazel-out/k8-opt/bin/external/bazel_tools -iquote external/grpc -iquote bazel-out/k8-opt/genfiles/external/grpc -iquote bazel-out/k8-opt/bin/external/grpc -iquote external/zlib_archive -iquote bazel-out/k8-opt/genfiles/external/zlib_archive -iquote bazel-out/k8-opt/bin/external/zlib_archive -iquote external/boringssl -iquote bazel-out/k8-opt/genfiles/external/boringssl -iquote bazel-out/k8-opt/bin/external/boringssl -iquote external/com_google_absl -iquote bazel-out/k8-opt/genfiles/external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote bazel-out/k8-opt/bin/external/nsync -iquote external/jemalloc -iquote bazel-out/k8-opt/genfiles/external/jemalloc -iquote bazel-out/k8-opt/bin/external/jemalloc -iquote external/eigen_archive -iquote bazel-out/k8-opt/genfiles/external/eigen_archive -iquote bazel-out/k8-opt/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/k8-opt/genfiles/external/local_config_sycl -iquote bazel-out/k8-opt/bin/external/local_config_sycl -iquote external/gif_archive -iquote bazel-out/k8-opt/genfiles/external/gif_archive -iquote bazel-out/k8-opt/bin/external/gif_archive -iquote external/jpeg -iquote bazel-out/k8-opt/genfiles/external/jpeg -iquote bazel-out/k8-opt/bin/external/jpeg -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/genfiles/external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/k8-opt/genfiles/external/farmhash_archive -iquote bazel-out/k8-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-opt/genfiles/external/fft2d -iquote bazel-out/k8-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/k8-opt/genfiles/external/highwayhash -iquote bazel-out/k8-opt/bin/external/highwayhash -iquote external/local_config_cuda -iquote bazel-out/k8-opt/genfiles/external/local_config_cuda -iquote bazel-out/k8-opt/bin/external/local_config_cuda -iquote external/double_conversion -iquote bazel-out/k8-opt/genfiles/external/double_conversion -iquote bazel-out/k8-opt/bin/external/double_conversion -iquote external/curl -iquote bazel-out/k8-opt/genfiles/external/curl -iquote bazel-out/k8-opt/bin/external/curl -iquote external/jsoncpp_git -iquote bazel-out/k8-opt/genfiles/external/jsoncpp_git -iquote bazel-out/k8-opt/bin/external/jsoncpp_git -iquote external/aws -iquote bazel-out/k8-opt/genfiles/external/aws -iquote bazel-out/k8-opt/bin/external/aws -isystem external/protobuf_archive/src -isystem bazel-out/k8-opt/genfiles/external/protobuf_archive/src -isystem bazel-out/k8-opt/bin/external/protobuf_archive/src -isystem external/grpc/include -isystem bazel-out/k8-opt/genfiles/external/grpc/include -isystem bazel-out/k8-opt/bin/external/grpc/include -isystem external/zlib_archive -isystem bazel-out/k8-opt/genfiles/external/zlib_archive -isystem bazel-out/k8-opt/bin/external/zlib_archive -isystem external/grpc/third_party/address_sorting/include -isystem bazel-out/k8-opt/genfiles/external/grpc/third_party/address_sorting/include -isystem bazel-out/k8-opt/bin/external/grpc/third_party/address_sorting/include -isystem external/boringssl/src/include -isystem bazel-out/k8-opt/genfiles/external/boringssl/src/include -isystem bazel-out/k8-opt/bin/external/boringssl/src/include -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem external/jemalloc/include -isystem bazel-out/k8-opt/genfiles/external/jemalloc/include -isystem bazel-out/k8-opt/bin/external/jemalloc/include -isystem external/eigen_archive -isystem bazel-out/k8-opt/genfiles/external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/gif_archive/lib -isystem bazel-out/k8-opt/genfiles/external/gif_archive/lib -isystem bazel-out/k8-opt/bin/external/gif_archive/lib -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/genfiles/external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/com_google_absl -isystem bazel-out/k8-opt/genfiles/external/com_google_absl -isystem bazel-out/k8-opt/bin/external/com_google_absl -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include/crt -isystem external/double_conversion -isystem bazel-out/k8-opt/genfiles/external/double_conversion -isystem bazel-out/k8-opt/bin/external/double_conversion -isystem external/curl/include -isystem bazel-out/k8-opt/genfiles/external/curl/include -isystem bazel-out/k8-opt/bin/external/curl/include -isystem external/jsoncpp_git/include -isystem bazel-out/k8-opt/genfiles/external/jsoncpp_git/include -isystem bazel-out/k8-opt/bin/external/jsoncpp_git/include -isystem external/aws/aws-cpp-sdk-core/include -isystem bazel-out/k8-opt/genfiles/external/aws/aws-cpp-sdk-core/include -isystem bazel-out/k8-opt/bin/external/aws/aws-cpp-sdk-core/include -isystem external/aws/aws-cpp-sdk-kinesis/include -isystem bazel-out/k8-opt/genfiles/external/aws/aws-cpp-sdk-kinesis/include -isystem bazel-out/k8-opt/bin/external/aws/aws-cpp-sdk-kinesis/include -isystem external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/k8-opt/genfiles/external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/k8-opt/bin/external/aws/aws-cpp-sdk-s3/include '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -mavx -mavx2 -mfma '-mfpmath=both' -msse4.2 -c tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc -o bazel-out/k8-opt/bin/tensorflow/contrib/verbs/_objs/rdma_rendezvous_mgr/rdma_rendezvous_mgr.pic.o)
In file included from ./tensorflow/core/framework/common_shape_fns.h:22:0,
                 from ./tensorflow/core/framework/resource_mgr.h:24,
                 from ./tensorflow/core/common_runtime/device.h:43,
                 from ./tensorflow/core/common_runtime/device_mgr.h:24,
                 from ./tensorflow/core/distributed_runtime/worker_session.h:21,
                 from ./tensorflow/core/distributed_runtime/base_rendezvous_mgr.h:24,
                 from ./tensorflow/contrib/verbs/rdma_rendezvous_mgr.h:22,
                 from tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc:18:
./tensorflow/core/util/tensor_format.h: In function 'tensorflow::TensorShape tensorflow::ShapeFromFormat(tensorflow::TensorFormat, tensorflow::int64, tensorflow::gtl::ArraySlice<long long int>, tensorflow::int64)':
./tensorflow/core/util/tensor_format.h:501:45: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (format == FORMAT_NHWC_VECT_W && dim == spatial.size() - 1) {
                                         ~~~~^~~~~~~~~~~~~~~~~~~~~
tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc: In member function 'virtual void tensorflow::RdmaRemoteRendezvous::RecvFromRemoteAsync(const tensorflow::Rendezvous::ParsedKey&, const tensorflow::Rendezvous::Args&, tensorflow::Rendezvous::DoneCallback)':
tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc:66:41: error: 'using StringPiece = class absl::string_view {aka class absl::string_view}' has no member named 'ToString'
   string key(std::move(parsed.FullKey().ToString()));
                                         ^~~~~~~~
In file included from ./tensorflow/core/platform/default/logging.h:24:0,
                 from ./tensorflow/core/platform/logging.h:25,
                 from ./tensorflow/core/lib/core/refcount.h:22,
                 from ./tensorflow/core/platform/tensor_coding.h:21,
                 from ./tensorflow/core/framework/resource_handle.h:19,
                 from ./tensorflow/core/framework/types.h:31,
                 from ./tensorflow/contrib/verbs/verbs_util.h:21,
                 from ./tensorflow/contrib/verbs/rdma.h:30,
                 from ./tensorflow/contrib/verbs/rdma_mgr.h:24,
                 from ./tensorflow/contrib/verbs/rdma_rendezvous_mgr.h:21,
                 from tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc:18:
./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetTensorDim(tensorflow::gtl::ArraySlice<T>, tensorflow::TensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':
./tensorflow/core/util/tensor_format.h:452:47:   required from here
./tensorflow/core/util/tensor_format.h:420:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   CHECK(index >= 0 && index < dimension_attributes.size())
./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
./tensorflow/core/util/tensor_format.h:420:3: note: in expansion of macro 'CHECK'
   CHECK(index >= 0 && index < dimension_attributes.size())
   ^
./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetFilterDim(tensorflow::gtl::ArraySlice<T>, tensorflow::FilterTensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':
./tensorflow/core/util/tensor_format.h:461:54:   required from here
./tensorflow/core/util/tensor_format.h:435:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   CHECK(index >= 0 && index < dimension_attribute.size())
./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
./tensorflow/core/util/tensor_format.h:435:3: note: in expansion of macro 'CHECK'
   CHECK(index >= 0 && index < dimension_attribute.size())
   ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 6111.101s, Critical Path: 179.37s
INFO: 9166 processes: 9166 local.
FAILED: Build did NOT complete successfully
```"
22520,Issue in running tensorflow,"Traceback (most recent call last):
  File ""C:\Users\zuha\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\zuha\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 658, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 571, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\zuha\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\zuha\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\zuha\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\zuha\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Users\zuha\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\zuha\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\zuha\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\zuha\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\zuha\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 658, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 571, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\zuha\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\zuha\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\zuha\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\zuha\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
22518,Support for NVIDIA Video Loader (nvvl),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: GTX 980M 4G
- **Exact command to reproduce**: N/A

Could you add the support for [NVIDIA Video Loader](https://github.com/NVIDIA/nvvl)?"
22516,TF Lite QuantizationUtilTest failure/bug,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MIPS32 Linux
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: SHA: 8c2159a10e53e5301ae26c739a3d09fa53d3352e
- **Python version**: N/A
- **Bazel version (if compiling from source)**: 0.16.1
- **GCC/Compiler version (if compiling from source)**: mips-mti-linux-gnu-g++ (Codescape GNU Tools 2017.10-05 for MIPS MTI Linux) 6.3.0
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: Too complicated

### Describe the problem
TensorFlow Lite test quantization_util_test failure:
~~~
[==========] Running 17 tests from 1 test case.
[----------] Global test environment set-up.
[----------] 17 tests from QuantizationUtilTest
[ RUN      ] QuantizationUtilTest.SafeCast
[       OK ] QuantizationUtilTest.SafeCast (12 ms)
[ RUN      ] QuantizationUtilTest.ChooseQuantizationParams
[       OK ] QuantizationUtilTest.ChooseQuantizationParams (1 ms)
[ RUN      ] QuantizationUtilTest.ChooseQuantizationParamsZeroPointOnMinBoundary
[       OK ] QuantizationUtilTest.ChooseQuantizationParamsZeroPointOnMinBoundary (0 ms)
[ RUN      ] QuantizationUtilTest.ChooseQuantizationParamsZeroNotInRange
[       OK ] QuantizationUtilTest.ChooseQuantizationParamsZeroNotInRange (179 ms)
[ RUN      ] QuantizationUtilTest.ChooseQuantizationParamsEmptyRangePositive
[       OK ] QuantizationUtilTest.ChooseQuantizationParamsEmptyRangePositive (112 ms)
[ RUN      ] QuantizationUtilTest.ChooseQuantizationParamsEmptyRangeZero
[       OK ] QuantizationUtilTest.ChooseQuantizationParamsEmptyRangeZero (0 ms)
[ RUN      ] QuantizationUtilTest.ChooseQuantizationParamsZeroPointOnMaxBoundary
[       OK ] QuantizationUtilTest.ChooseQuantizationParamsZeroPointOnMaxBoundary (0 ms)
[ RUN      ] QuantizationUtilTest.IntegerFrExp
[       OK ] QuantizationUtilTest.IntegerFrExp (1 ms)
[ RUN      ] QuantizationUtilTest.IntegerFrExpVersusDouble
tensorflow/contrib/lite/kernels/internal/quantization_util_test.cc:262: Failure
The difference between result and (0.964453 * (1L << 31)) is 4142294361.764544, which exceeds 1000, where
result evaluates to 2071147315,
(0.964453 * (1L << 31)) evaluates to -2071147046.764544, and
1000 evaluates to 1000.
[  FAILED  ] QuantizationUtilTest.IntegerFrExpVersusDouble (4 ms)
[ RUN      ] QuantizationUtilTest.DoubleFromFractionAndShift
[       OK ] QuantizationUtilTest.DoubleFromFractionAndShift (1 ms)
[ RUN      ] QuantizationUtilTest.IntegerDoubleMultiply
[       OK ] QuantizationUtilTest.IntegerDoubleMultiply (0 ms)
[ RUN      ] QuantizationUtilTest.IntegerDoubleCompare
[       OK ] QuantizationUtilTest.IntegerDoubleCompare (1 ms)
[ RUN      ] QuantizationUtilTest.ChooseQuantizationParamsInvalidRange
[       OK ] QuantizationUtilTest.ChooseQuantizationParamsInvalidRange (110 ms)
[ RUN      ] QuantizationUtilTest.QuantizeMultiplierSmallerThanOneExp
[       OK ] QuantizationUtilTest.QuantizeMultiplierSmallerThanOneExp (581 ms)
[ RUN      ] QuantizationUtilTest.QuantizeMultiplierGreaterThanOne
[       OK ] QuantizationUtilTest.QuantizeMultiplierGreaterThanOne (103 ms)
[ RUN      ] QuantizationUtilTest.PreprocessSoftmaxScaling
[       OK ] QuantizationUtilTest.PreprocessSoftmaxScaling (1 ms)
[ RUN      ] QuantizationUtilTest.CalculateInputRadius
[       OK ] QuantizationUtilTest.CalculateInputRadius (0 ms)
[----------] 17 tests from QuantizationUtilTest (1106 ms total)

[----------] Global test environment tear-down
[==========] 17 tests from 1 test case ran. (1109 ms total)
[  PASSED  ] 16 tests.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] QuantizationUtilTest.IntegerFrExpVersusDouble

 1 FAILED TEST
Test quantization_util_test failed!
~~~
Looks like there's undefined behavior in the test. The test expects a 64-bit long, whereas it's 32-bit on MIPS32. Changing `1L` to `1LL` fixes the failure.

### Source code / logs
See above.
"
22514,Assertion `d.nbDims >= 3' failed with int8 for Tensorflow TensorRT integration,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:n/a
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a
- **TensorFlow installed from (source or binary)**:source 
- **TensorFlow version (use command below)**: r1.11
- **Python version**: 3.5
- **Bazel version (if compiling from source)**:0.16.1
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 9/7.1
- **GPU model and memory**: 1080ti/11gb
- **Exact command to reproduce**:

```
import tensorflow as tf
from tensorflow.contrib import tensorrt as trt

def load_graph(frozen_graph_filename):
    # We load the protobuf file from the disk and parse it to retrieve the
    # unserialized graph_def
    with tf.gfile.GFile(frozen_graph_filename, ""rb"") as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())

    # Then, we can use again a convenient built-in function to import a graph_def into the
    # current default Graph
    with tf.Graph().as_default() as graph:
        tf.import_graph_def(
            graph_def,
            name='', #DEBUG
        )
    return graph

fid = ""model.pb""
output_nodenames = 'out1,out2,out3'
output_node = list(output_nodenames.split("",""))
g = load_graph(fid)
with tf.Session(graph=g) as sess:
    trt_graph = trt.create_inference_graph(
    input_graph_def=tf.get_default_graph().as_graph_def(),
    outputs=output_node,
    max_batch_size=99999,
    max_workspace_size_bytes=1 << 25,
    precision_mode=""INT8"",  # TRT Engine precision ""FP32"",""FP16"" or ""INT8""
    minimum_segment_size=2  # minimum number of nodes in an engine
    )
    with tf.gfile.GFile(""trt.pb"", ""wb"") as f:
        f.write(trt_graph.SerializeToString())

g2 = load_graph(""trt.pb"")
with tf.Session(graph=g2) as sess:
    """"""Run given calibration graph multiple times.""""""
    num_samples = 10
    np.random.seed(0)
    ip1_data = np.random.rand(num_samples,700,800,6).astype(np.float32)
    ip1 = g2.get_tensor_by_name(""ip1:0"")

    ip2_data = np.random.rand(4).astype(np.float32)
    ip2 = g2.get_tensor_by_name(""ip2:0"")

    ip3_data = np.random.rand(20000,6).astype(np.float32)
    ip3 = g2.get_tensor_by_name(""ip3:0"")

    ip4_data = np.random.rand(20000,4).astype(np.float32)
    ip4 = g2.get_tensor_by_name(""ip4:0"")

    out1 = g2.get_tensor_by_name(""out1:0"")
    out2 = g2.get_tensor_by_name(""out2:0"")
    out3 = g2.get_tensor_by_name(""out3:0"")
# run over real calibration data here, we are mimicking a calibration set of
# 30 different batches. Use as much calibration data as you want
    for i in range(num_samples):
        val = sess.run([out1, out2, out3], feed_dict={ip1:ip1_data[i], ip2:ip2_data, ip3:ip3_data, ip4:ip4_data})
```

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
2018-09-25 17:20:14.600365: I tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:577] Starting calibration thread on device 0, Calibration Resource @ 0x7f7fb4001730
2018-09-25 17:20:14.600539: I tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:577] Starting calibration thread on device 0, Calibration Resource @ 0x7f7fac071a40
2018-09-25 17:20:14.973555: I tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:577] Starting calibration thread on device 0, Calibration Resource @ 0x7f7fa4004fe0
python: helpers.cpp:56: nvinfer1::DimsCHW nvinfer1::getCHW(const nvinfer1::Dims&): Assertion `d.nbDims >= 3' failed.
Aborted (core dumped)
```
"
22512,"Installation issue with Tensorflow-cpu, no module named '_pwyrap_tensorflow_internal'","### System information

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 server in desktop mode
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: tensorflow-gpu 1.10.0
- **Python version**: Anaconda x64 3.6
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: 9.0/7.0
- **GPU model and memory**: Q4000
- **Exact command to reproduce**: N/A

### Describe the problem

1. Create a python application that imports tensorflow-gpu
2. Package the application with pyinstaller
3. Start the generated executable, that crashes while trying to import tensorflow.

The error trace is this:

```
Traceback (most recent call last):
  File ""site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
  File ""c:\users\paperspace\.virtualenvs\***-ecouzwts\lib\site-packages\PyInstaller\loader\pyimod03_importers.py"", line 627, in exec_module
    exec(bytecode, module.__dict__)
  File ""site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in <module>
  File ""site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in swig_import_helper
  File ""importlib\__init__.py"", line 126, in import_module
ModuleNotFoundError: No module named 'tensorflow.python._pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```

On the other hand, **doing the same process with tensorflow(-cpu) works fine**. 

I ruled out CUDA/cuDNN installation issue, because if I run my python application using python, it works fine with the GPU version.

I have tried checking at the .exe with dependency_walker, but appart from false-positive missing `API-MS-*` and `EXT-MS-*` DLLs, I don't see anything obvious.

So I'm stuck here.
What discrepancy could create an import issue with tensorflow-gpu and not with tensorflow-cpu ?"
22511,[Feature Request] DistributionStrategy example not using Keras or estimator API,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
Hi guys,

I was curious if there are any resources to using a DistributionStrategy if some of us to choose to create a model using the low level tensorflow API and manage our own session. I see that with estimators we can define a tf.estimator.RunConfig and pass the strategies there, and with keras you can pass the strategy when you compile the model, but in general where are we supposed to pass the DistributionStrategy? Is it somehow supposed to be passed to tf.ConfigProto or tf.train.MonitoredTrainingSession? If there aren't any resources, I think it would be super helpful to have an example like https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distribute/python/examples/keras_mnist.py 

Thanks!"
22510,"Tensorflow lite aborts due to ""pure virtual method called"" on Raspberry Pi","Hello.
I compiled the library as described in
https://www.tensorflow.org/lite/rpi

When running
`./benchmark_model --graph=mobilenet_v1_1.0_224.tflite`
or
`./minimal mobilenet_v1_1.0_224.tflite`


it crashes with the following output:
```

STARTING!
Num runs: [50]
Inter-run delay (seconds): [-1]
Num threads: [1]
Benchmark name: []
Output prefix: []
Warmup runs: [1]
Graph: [mobilenet_v1_1.0_224.tflite]
Input layers: []
Input shapes: []
Use nnapi : [0]
Loaded model mobilenet_v1_1.0_224.tflite
resolved reporter
pure virtual method called
terminate called without an active exception


```

I tried to debug the problem myself: it occurs in a secondary thread.
Some function that is called from 
`EigenForTFLite::NonBlockingThreadPoolTempl<EigenForTFLite::StlThreadEnvironment>::WorkerLoop(int)
` 
gets the this parameter equal to NULL.

The host is a raspberry pi 3B+, with raspbian.
I tried both to cross-compile the code and to compile natively.
I tried more versions: one is r1.11
Gcc version: 4.9.2

Does anyone else have the same problem or is it just me?
Thank you.
"
22506,"toco fails to convert to tflite ""TypeError: 'NoneType' object has no attribute '__getitem__'""","`toco --graph_def_file=frozen_inference_graph.pb --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --output_file=oren.tflite --inference_type=QUANTIZED_UINT8 --input_type=FLOAT --input_arrays=ImageTensor --output_arrays=SemanticPredictions --input_shapes=1,321,321,3`


2018-09-25 19:48:04.857732: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Traceback (most recent call last):
  File ""/home/obg1/.local/bin/toco"", line 11, in <module>
    sys.exit(main())
  File ""/home/obg1/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 370, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/home/obg1/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/home/obg1/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 366, in run_main
    _convert_model(tflite_flags)
  File ""/home/obg1/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 143, in _convert_model
    output_data = converter.convert()
  File ""/home/obg1/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/lite.py"", line 374, in convert
    dump_graphviz_video=self.dump_graphviz_video)
  File ""/home/obg1/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/convert.py"", line 243, in toco_convert
    *args, **kwargs)
  File ""/home/obg1/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/convert.py"", line 210, in build_toco_convert_protos
    input_array.mean_value, input_array.std_value = quantized_input_stats[idx]
TypeError: 'NoneType' object has no attribute '__getitem__'
"
22505,Lookup Error : Gradient Registry has no entry for  StatefulPartitionedCall,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.10.1
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: v9.0 (Cuda)
- **GPU model and memory**: gtx 930m, 2GB
- **Exact command to reproduce**:

### Describe the problem
I was implementing the gradient penalty for improved stability for wgan in tensorflow eager

### Source code / logs
``` python
with tf.GradientTape() as critic_tape:
    generated_images = generator(tf.random_normal([16, 100]), training=True)
    a = tf.convert_to_tensor(images)
    real_output = critic(a, training=True)
    generated_output = critic(generated_images, training=True)               
    with tf.GradientTape() as gtape:
        epsilon = tf.random_uniform([], 0, 1)
        xhat = epsilon*a + (1-epsilon)*generated_images
        dhat = critic(xhat, training=True)
        gtape.watch(xhat)
    dhat2 = gtape.gradient(dhat, xhat)
    slopes = tf.sqrt(tf.reduce_sum(tf.square(dhat2), reduction_indices=[1]))
    gradient_penalty = 10*tf.reduce_mean((slopes-1.0)**2)
    critic_loss = get_critic_loss(real_output, generated_output)
    critic_loss+= gradient_penalty                
gradients_of_critic = critic_tape.gradient(critic_loss, critic.variables)
print(gradients_of_critic) 
```

and the error stack i recieve is 
```--------------------------------------------------------------------------
LookupError                               Traceback (most recent call last)
<ipython-input-512-cbc8ebf905ac> in <module>()
     16     critic_loss = get_critic_loss(real_output, generated_output)
     17     critic_loss+= gradient_penalty
---> 18 gradients_of_critic = critic_tape.gradient(critic_loss, critic.variables)
     19 print(gradients_of_critic)

c:\users\vibhu\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\eager\backprop.py in gradient(self, target, sources, output_gradients)
    856     flat_grad = imperative_grad.imperative_grad(
    857         _default_vspace, self._tape, nest.flatten(target), flat_sources,
--> 858         output_gradients=output_gradients)
    859 
    860     if not self._persistent:

c:\users\vibhu\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\eager\imperative_grad.py in imperative_grad(vspace, tape, target, sources, output_gradients)
     61   """"""
     62   return pywrap_tensorflow.TFE_Py_TapeGradient(
---> 63       tape._tape, vspace, target, sources, output_gradients)  # pylint: disable=protected-access

c:\users\vibhu\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\eager\backprop.py in _gradient_function(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads)
    110   """"""
    111   mock_op = _MockOp(attr_tuple, inputs, outputs, op_name)
--> 112   grad_fn = ops._gradient_registry.lookup(op_name)  # pylint: disable=protected-access
    113   if grad_fn is None:
    114     return [None] * num_inputs

c:\users\vibhu\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\registry.py in lookup(self, name)
     91     else:
     92       raise LookupError(
---> 93           ""%s registry has no entry for: %s"" % (self._name, name))

LookupError: gradient registry has no entry for: StatefulPartitionedCall

"
22504,Weird behaviour of back propagation in a while_loop in Tensorflow,"- **Have I written custom code: Yes
- **OS Platform and Distribution: Windows 10 (primary), Ubunto 16.04.3 LTS (secondary)
- **TensorFlow installed from (source or binary): Binary
- **TensorFlow version (use command below): b'v1.10.0-rc1-19-g656e7a2b34' 1.10.0
- **Python version: Python 3.6.6 :: Anaconda, Inc.
- **CUDA/cuDNN version**: N/A (not relevant)
- **GPU model and memory**: N/A (not relevant) 
- **Exact command to reproduce**:


### Describe the problem

I am trying to see if Tensorflow can backpropagate through a while_loop without a pre-defined number of loops. Something weird caught my attention: the following script results in the error 

> InvalidArgumentError: Retval[0] does not have value

However, when I replaced `tf.add(X,X)` by `tf.multiply(X,2)`, the code can be executed and return the correct answer graVal=64. 


Could anyone help me to understand where the error came from when I was using `tf.add(X,X)`? Thanks in advance!

### Source code / logs
```
import tensorflow as tf
from numpy import *

tf.reset_default_graph()

X = tf.Variable(5.0, dtype = float32)
Y = tf.Variable(4.0, dtype = float32)

def cond(iterN, Y):
    return iterN<6

def body(iterN, X):
    return iterN+1, tf.add(X,X)
    # return iterN+1, tf.multiply(X,2)

finalOutX = tf.while_loop(cond, body, [0,X], parallel_iterations=1, back_prop=True, swap_memory=True)

gra = tf.gradients(finalOutX, X)

init = tf.global_variables_initializer()

with tf.Session() as sess:

    init.run()
    X_val, graVal, = sess.run([finalOutX, gra])    

print(X_val)
print(graVal)
```"
22503,Training tfgan to restore VMP,I want to use this to restore VMP.What information can I refer to?
22502,LookupError when computing tf.gradients of DepthwiseConv2dNativeBackpropInput,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Anaconda
- **TensorFlow version:  v1.10.0-rc0-0-g586a7f1d2c 1.10.0-rc0
- **Python version : 3.6
- **CUDA/cuDNN version**: 9.0 / 7.3
- **GPU model and memory**: Titan X


### Describe the problem
I got a LookupError when computing tf.gradients()

LookupError: No gradient defined for operation 'gradients/discriminator_3/separable_conv2d/depthwise_grad/DepthwiseConv2dNativeBackpropInput' (op type: DepthwiseConv2dNativeBackpropInput)

Because DepthwiseConv2dNativeBackpropInput does not have a registered gradient function.

### Source code / logs
tf.gradients(d_hat, x_hat)[0] where tf.nn.separable_conv2d is used in the intermediate layers.
"
22501,Quantization Error:   Converting unsupported operation: Elu,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)** : pip install
- **TensorFlow version (use command below)**: 1.10.1
- **Python version**:3.6
- **GPU model and memory**:NVIDIA 1080
- **Exact command to reproduce**:N/A
- **Bazel version**: N/A
- **CUDA/cuDNN version**: use CPU
- **Mobile device**:N/A

```
import tensorflow as tf
print(tf.__version__)
tf.enable_eager_execution()
converter = tf.contrib.lite.TocoConverter.from_saved_model(""/media/user/60EA4260EA423298/Nikishyn/model"")
#converter.post_training_quantize = True
tflite_quantized_model = converter.convert()
```

I'm trying to quantify my model, but I get an error.
console logs:

RuntimeError: TOCO failed see console for info.
b'2018-09-25 08:17:37.600466: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1096] Converting unsupported operation: Elu\n2018-09-25 08:17:37.600610: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1096] Converting unsupported operation: Elu\n2018-09-25 08:17:37.600833: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1096] Converting unsupported operation: Elu\n2018-09-25 08:17:37.601051: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1096] Converting unsupported operation: Elu\n2018-09-25 08:17:37.601221: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1096] Converting unsupported operation: Elu\n2018-09-25 08:17:37.601441: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1096] Converting unsupported operation: Elu\n2018-09-25 08:17:37.601664: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1096] Converting unsupported operation: Elu\n2018-09-25 08:17:37.601835: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1096] Converting unsupported operation: Elu\n2018-09-25 08:17:37.602063: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1096] Converting unsupported operation: Elu\n2018-09-25 08:17:37.602291: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1096] Converting unsupported operation: Elu\n2018-09-25 08:17:37.603091: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1096] Converting unsupported operation: Elu\n2018-09-25 08:17:37.603218: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1096] Converting unsupported operation: Elu\n2018-09-25 08:17:37.603283: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1096] Converting unsupported operation: Elu\n2018-09-25 08:17:37.606771: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 373 operators, 615 arrays (0 quantized)\n2018-09-25 08:17:37.610480: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 347 operators, 576 arrays (0 quantized)\n2018-09-25 08:17:37.615480: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 347 operators, 576 arrays (0 quantized)\n2018-09-25 08:17:37.615675: F tensorflow/contrib/lite/toco/graph_transformations/resolve_batch_normalization.cc:42] Check failed: IsConstantParameterArray(*model, bn_op->inputs[1]) && IsConstantParameterArray(*model, bn_op->inputs[2]) && IsConstantParameterArray(*model, bn_op->inputs[3]) Batch normalization resolution requires that mean, multiplier and offset arrays be constant.\nAborted (core dumped)\n'
None
"
22500,"I get ""Session was not created with a graph before Run()"" while running official TFLite Model Benchmark Tool on Android!","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
android and Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
Xiaomi Max2
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.14
- **Python version**:
3.5
- **Bazel version**:
1.7
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
N/A

### Describe the problem
I am doing the ""build/install/run"" steps on Android phone following https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/tools/benchmark

However, when I run the benchmark using the following command I got error""Session was not created with a graph before Run()"":

adb shell /data/local/tmp/benchmark_model \
  --graph=/data/local/tmp/mobilenet_quant_v1_224.tflite \
  --num_threads=4

I did not modify the source code and just follow the official document. Does anyone encounter the same problem?

![_20180925211348](https://user-images.githubusercontent.com/14329360/46016662-f12d2680-c107-11e8-8d8e-1e1f47b895cb.jpg)

"
22499,"Can not run benchmark on android using ""tensorflow/tensorflow/tools/benchmark""","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
android and Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
Xiaomi Max2
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.14
- **Python version**:
3.5
- **Bazel version**:
1.7
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
N/A

### Describe the problem
I am trying to run benchmark on android phone following the instructions here:
https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/benchmark/

I have succeffully build the benchmark and everything is right until step (4) Run the benchmark.
When I run the command below, I got no response:
adb shell ""/data/local/tmp/benchmark_model \
  --graph=/data/local/tmp/tensorflow_inception_graph.pb \
  --input_layer=""input:0"" \
  --input_layer_shape=""1,224,224,3"" \
  --input_layer_type=""float"" \
  --output_layer=""output:0""

There is only blinking arrow:
![_20180925202142](https://user-images.githubusercontent.com/14329360/46014065-acea5800-c100-11e8-8975-db7b06c88d10.png)

So can someone tell me what's wrong?"
22498,Writing a SavedModel on Windows with long file path fails,"### System information
- **Have I written custom code**: Yes, see below.
- **OS Platform and Distribution**: Windows 10 Enterprise 64Bit (10.0.16299)
- **Mobile device**: N/A
- **TensorFlow installed from**: Binary
- **TensorFlow version**: b'v1.10.0-rc1-19-g656e7a2b34' 1.10.0
- **Python version**: 3.5.5
- **Bazel version**: N/A
- **GCC/Compiler version**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: Run the provided script on Windows


### Describe the problem
When exporting a SavedModel to a directory with a rather long path the maximum file path length of Windows (260 chars) gets exceeded while creating temporary variable files and the process fails because it can't find the file (See attachments for the full log):
```
NotFoundError (see above for traceback): Failed to create a NewWriteableFile: C:\reallylongpath\barfoobarfoobarfoobarfoobarfoobarfoo\barfoobarfoobarfoobarfoobarfoobarfoo\barfoobarfoobarfoobarfoobarfoobarfoo\barfoobarfoo\variables\variables_temp_cbd5fe79f07a4790a08e7c2adf1ed95b/part-00000-of-00001.data-00000-of-00001.tempstate12448200021531902965 : The system cannot find the path specified.
```
The usual workaround to prepend `'\\\\?\\'` to the path (like described [here](https://stackoverflow.com/a/3557977)) doesn't work (See attachments).

### Source code / logs

__Code to reproduce:__
```python
import tensorflow as tf

# Long export path (should be at least 140 chars long)
export_dir = r'C:\reallylongpath\barfoobarfoobarfoobarfoobarfoobarfoo\barfoobarfoobarfoobarfoobarfoobarfoo\barfoobarfoobarfoobarfoobarfoobarfoo\barfoobarfoo'
print('Export dir lenght: {}'.format(len(export_dir)))

# Build an easy graph with variables
x = tf.constant(2)
w = tf.Variable(3)
y = tf.multiply(x, w)

with tf.Session() as sess:
    # Initialize the variable
    sess.run(w.initializer)

    # Save as a SavedModel
    tf.saved_model.simple_save(sess,
                               export_dir,
                               inputs={'x': x },
                               outputs={'y': y })
```

__Logs:__
Full log:
[log.txt](https://github.com/tensorflow/tensorflow/files/2414582/log.txt)

Full log with `'\\\\?\\'` prefix:
[log_prefix_0.txt](https://github.com/tensorflow/tensorflow/files/2414583/log_prefix_0.txt)

Full log with `'\\?\'` prefix:
[log_prefix_1.txt](https://github.com/tensorflow/tensorflow/files/2414584/log_prefix_1.txt)
"
22497,DataLoss error on TFRecords - happens on one machine doesn't on other,"### System information
#### System 1 (Bug DOESN'T occur)
All details are from inside the docker which the codes run in
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: True
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.1, 4.15.0-29-generic
- **TensorFlow installed from (source or binary)**: pip install tensorflow_gpu
- **TensorFlow version (use command below)**: v1.10.0-0-g656e7a2b34 1.10.0
- **Mobile device**: N/A
- **Bazel version (if compiling from source)**: N/A
- **Python version**: 3.6.6
- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
- **CUDA/cuDNN version**: CUDA: V9.0.176, cuDNN 7.1.4  
- **GPU model and memory**: GTX 1080 8GB + GTX 980 Ti 6GB
- **Docker details**: 
  NVIDIA Docker: 2.0.2
    Client:
    Version:           17.12.0-ce
    API version:       1.35
    Go version:        go1.9.2
    Git commit:        c97c6d6
    Built:             Wed Dec 27 20:11:19 2017
    OS/Arch:          linux/amd64
    Experimental:      false
  Server:
    Engine:
    Version:          17.12.0-ce
    API version:      1.35 (minimum version 1.12)
    Go version:       go1.9.2
    Git commit:       c97c6d6
    Built:            Wed Dec 27 20:09:53 2017
    OS/Arch:          linux/amd64
    Experimental:     false

- **Exact command to reproduce**: See below

#### System 2 (Bug DOES occur)
All details are from inside the docker which the codes run in
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: True
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.1, 4.15.0-34-generic
- **TensorFlow installed from (source or binary)**: pip install tensorflow_gpu
- **TensorFlow version (use command below)**: v1.10.1-0-g4dcfddc5d1 1.10.1
- **Mobile device**: N/A
- **Bazel version (if compiling from source)**: N/A
- **Python version**: 3.6.6
- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
- **CUDA/cuDNN version**: CUDA: V9.0.176, cuDNN 7.2.1  
- **GPU model and memory**: GTX 1080 Ti 12GB x 2
- **Docker details**:
  NVIDIA Docker: 2.0.3
    Client:
    Version:           18.06.0-ce
    API version:       1.38
    Go version:        go1.10.3
    Git commit:        0ffa825
    Built:             Wed Jul 18 19:11:02 2018
    OS/Arch:           linux/amd64
    Experimental:      false
  Server:
    Engine:
    Version:          18.06.0-ce
    API version:      1.38 (minimum version 1.12)
    Go version:       go1.10.3
    Git commit:       0ffa825
    Built:            Wed Jul 18 19:09:05 2018
    OS/Arch:          linux/amd64
    Experimental:     false
- **Exact command to reproduce**: See below

### Describe the problem
Using the same docker image, which is an altered version of [this image](https://github.com/ufoym/deepo/blob/master/docker/Dockerfile.tensorflow-py36-cu90) running the same code over the same TFRecords on one machine (System 2) I get a DataLoss exception (corrupted record) while on the other system (System 1) I don't.
I have verified using checksum that the files were transferred safely. I have also tried to retransfer the files at least two times. I have also have tried using other sets of TFRecords I created. The problem is the same in all cases, it happens on System 2, but does not happen on System 1.

The TFRecords were written on System 1 and are over 150GB in size.
I have also tried rebuilding the image and restarting containers.
I have also tried restarting the host machine.
The code is running on one GPU, I tried using each, all fail.

### Source code / logs
Due to sensitivity of the code I can't share all of it, but I'll paste the relevant lines.

In the training code I have 3 places where I iterate over the dataset, once when I count the number of records
`train_size = sum(1 for _ in tf.python_io.tf_record_iterator(meta['train_tfr_path']))`. 
Second time when I feed the training loop

```
while True:
    try:
        ...
        _, batch_summary = sess.run([opt, merged], feed_dict={handle: train_handle})
        ...
    except tf.errors.OutOfRangeError:
        ...
```
And third time very similar code for the validation set.

After I recreate image + containers and restarting the machine, I will successfully finish the calculation of `train_size` (full loop over TFRecords), then I would start training, and somewhere in the training loop I would fail with the traceback attached below. If then I run the code again, I will fail on the first call to the TFRecords, with the same traceback - meaning this time it would happen on the `train_size` calculation. It will now forever fail on this call, until I restart the machine and the scenario repeats.

As I said, same code on different machine (System 1) never fails.


Traceback:

> Traceback (most recent call last):
>   File ""train.py"", line 884, in <module>
>     main()
>   File ""train.py"", line 878, in main
>     train(graph, model_dir, tensorboard_dir, meta, is_recovering=recovering)
>   File ""train.py"", line 846, in train
>     'sequential']})
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 877, in run
>     run_metadata_ptr)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1100, in _run
>     feed_dict_tensor, options, run_metadata)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1272, in _do_run
>     run_metadata)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1291, in _do_call
>     raise type(e)(node_def, op, message)
> tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 6720637495
> 	 [[Node: data/IteratorGetNext = IteratorGetNext[output_shapes=[[?,?], [?,1], [?,?,?], [?,?,?], [?,?], ..., [?,?], [?,1], [?,?,?], [?,?,?], [?,?]], output_types=[DT_FLOAT, DT_INT64, DT_INT64, DT_FLOAT, DT_INT64, ..., DT_INT64, DT_INT64, DT_FLOAT, DT_INT64, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](data/IteratorFromStringHandle)]]
> 
> Caused by op 'data/IteratorGetNext', defined at:
>   File ""train.py"", line 884, in <module>
>     main()
>   File ""train.py"", line 874, in main
>     graph, meta = build_model(available_gpus)
>   File ""train.py"", line 502, in build_model
>     return build_classifier(available_gpus)
>   File ""train.py"", line 572, in build_classifier
>     n_gpus=FLAGS.n_gpus)
>   File ""/opt/code/utils/architecture/data/v4.py"", line 309, in data_prep
>     <deleted line for sensitivity issues>
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 410, in get_next
>     name=name)), self._output_types,
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 2069, in iterator_get_next
>     output_shapes=output_shapes, name=name)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
>     op_def=op_def)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py"", line 454, in new_func
>     return func(*args, **kwargs)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3155, in create_op
>     op_def=op_def)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1717, in __init__
>     self._traceback = tf_stack.extract_stack()
> 
> DataLossError (see above for traceback): corrupted record at 6720637495
> 	 [[Node: data/IteratorGetNext = IteratorGetNext[output_shapes=[[?,?], [?,1], [?,?,?], [?,?,?], [?,?], ..., [?,?], [?,1], [?,?,?], [?,?,?], [?,?]], output_types=[DT_FLOAT, DT_INT64, DT_INT64, DT_FLOAT, DT_INT64, ..., DT_INT64, DT_INT64, DT_FLOAT, DT_INT64, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](data/IteratorFromStringHandle)]]
"
22496,I met the error:__init__() missing 2 required positional arguments: 'message' and 'code' when I tried to run the exact same code on image_captioning_with_attention ,"The details are as follows:
code:
for img, path in image_dataset:
    batch_features = image_features_extract_model(img)
    batch_features = tf.reshape(batch_features, 
                              (batch_features.shape[0], -1, batch_features.shape[3]))

    for bf, p in zip(batch_features, path):
        path_of_feature = p.numpy().decode(""utf-8"")
        np.save(path_of_feature, bf.numpy())
error:
_FallbackException                        Traceback (most recent call last)
E:\anaconda\lib\site-packages\tensorflow\python\ops\gen_dataset_ops.py in iterator_get_next_sync(iterator, output_types, output_shapes, name)
   1600         ""IteratorGetNextSync"", name, _ctx._post_execution_callbacks, iterator,
-> 1601         ""output_types"", output_types, ""output_shapes"", output_shapes)
   1602       return _result

_FallbackException: Expecting int64_t value for attr output_shapes, got NoneType

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
<ipython-input-17-266d7ad17ef7> in <module>()
----> 1 for img, path in image_dataset:
      2     batch_features = image_features_extract_model(img)
      3     batch_features = tf.reshape(batch_features, 
      4                               (batch_features.shape[0], -1, batch_features.shape[3]))
      5 

E:\anaconda\lib\site-packages\tensorflow\python\data\ops\iterator_ops.py in __next__(self)
    484 
    485   def __next__(self):  # For Python 3 compatibility
--> 486     return self.next()
    487 
    488   def _next_internal(self):

E:\anaconda\lib\site-packages\tensorflow\python\data\ops\iterator_ops.py in next(self)
    515     """"""
    516     try:
--> 517       return self._next_internal()
    518     except errors.OutOfRangeError:
    519       raise StopIteration

E:\anaconda\lib\site-packages\tensorflow\python\data\ops\iterator_ops.py in _next_internal(self)
    505             self._resource,
    506             output_types=self._flat_output_types,
--> 507             output_shapes=self._flat_output_shapes)
    508 
    509       return sparse.deserialize_sparse_tensors(

E:\anaconda\lib\site-packages\tensorflow\python\ops\gen_dataset_ops.py in iterator_get_next_sync(iterator, output_types, output_shapes, name)
   1604       return iterator_get_next_sync_eager_fallback(
   1605           iterator, output_types=output_types, output_shapes=output_shapes,
-> 1606           name=name, ctx=_ctx)
   1607     except _core._NotOkStatusException as e:
   1608       if name is not None:

E:\anaconda\lib\site-packages\tensorflow\python\ops\gen_dataset_ops.py in iterator_get_next_sync_eager_fallback(iterator, output_types, output_shapes, name, ctx)
   1633   _result = _execute.execute(b""IteratorGetNextSync"", len(output_types),
   1634                              inputs=_inputs_flat, attrs=_attrs, ctx=_ctx,
-> 1635                              name=name)
   1636   _execute.record_gradient(
   1637       ""IteratorGetNextSync"", _inputs_flat, _attrs, _result, name)

E:\anaconda\lib\site-packages\tensorflow\python\eager\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     58     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,
     59                                                op_name, inputs, attrs,
---> 60                                                num_outputs)
     61   except core._NotOkStatusException as e:
     62     if name is not None:

TypeError: __init__() missing 2 required positional arguments: 'message' and 'code"
22495,"I have two nvidia GPU,but run ours finalAlg.soand GPU don't work. ","first,i cmake tensorflow-GPU,and get tensorflow_cc.so
then ,cmake tensorflow__cc.so and another alg codes have got finalAlg.so;
last,run finalAlg.so by tensorflow C++ API.

    string model_path = ""finalAlg.pb"";
    GraphDef graph_def;
    status = ReadBinaryProto(Env::Default(), model_path, &graph_def);
    status = session->Create(graph_def); 
    graph::SetDefaultDevice(""/gpu:0"", &graphdef);
    sess_options.config.mutable_gpu_options()->set_allow_growth(true); 
    Tensor phase_train(DT_BOOL, TensorShape());
    phase_train.scalar<bool>()() = false;   	
    std::vector<std::pair<std::string, tensorflow::Tensor>> inputs = {
    { ""input"", input_tensor },{""phase_train"",phase_train }};
    std::vector<tensorflow::Tensor> outputs;
    status = session->Run(inputs, { ""dsts""}, {}, &outputs);
    runtime appear:
    I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 1
    I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N N
    I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 1:   N N

 
Although i have two nvidia GPUs,but ours finalAlg.so cant't run in GPU,just always run in CPU.
Give me some suggesstion.thanks."
22494,Compilation v1.10 fails at //tensorflow/core/debug:debug_io_utils,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No it's a compilation issue
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
Nil
- **TensorFlow installed from (source or binary)**:
From source
- **TensorFlow version (use command below)**:
tensorflow 1.10.0
- **Python version**:
python3
- **Bazel version (if compiling from source)**:
bazel 0.16
- **GCC/Compiler version (if compiling from source)**:
gcc-6
- **CUDA/cuDNN version**:
cuda 9.1 cudnn 7.3.0
- **GPU model and memory**:
1070 8G
- **Exact command to reproduce**:
bazel --host_jvm_args=-Xms512m --host_jvm_args=-Xmx1024m   build --jobs=2 -c opt --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --config=opt --config=cuda  //tensorflow/tools/pip_package:build_pip_package


You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
The compilation of tensorflow stops during //tensorflow/core/debug:debug_io_utils with grpc_completion_queue_factory does not name a type
1. I have compiled a separate grpc installation and that went well
2. Tried compiling all the dependencies 
        ""//tensorflow:grpc++"",
        ""//tensorflow/core:core_cpu_internal"",
        ""//tensorflow/core:framework"",
        ""//tensorflow/core:graph"",
        ""//tensorflow/core:lib"",
        ""//tensorflow/core:lib_internal"",
        ""//tensorflow/core:proto_text"",
        ""//tensorflow/core:protos_all_cc"",
and these sub dependencies compiled fine


### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.


[errorfile3.txt](https://github.com/tensorflow/tensorflow/files/2414334/errorfile3.txt)

"
22492,failed to synchronize the stop event: CUDA_ERROR_ILLEGAL_ADDRESS," when I train keras_retinanet model with tf-gpu, the following errors occured. CUDA_ERROR_ILLEGAL_ADDRESS.
It's stable reproduction.

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
CentOS Linux release 7.4.1708
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
no
- **TensorFlow installed from (source or binary)**:
binary(pip install tensorflow-gpu)
- **TensorFlow version (use command below)**:
1.10.0-dev20180720
keras version = 2.2.0
- **Python version**:
Python 2.7.5
- **Bazel version (if compiling from source)**:
n/a
- **GCC/Compiler version (if compiling from source)**:
n/a
- **CUDA/cuDNN version**:
cuda version = 9.0.176
cudnn version = 7.1.4
- **GPU model and memory**:
Tesla P40 ,4 gpu , 22919MiB memory, Driver Version: 390.77
- **Exact command to reproduce**:
download open image datasets
git clone https://github.com/fizyr/keras-retinanet.git
pip install . --user
python setup.py build_ext --inplace
python keras_retinanet/bin/train.py --batch-size 16 --gpu 0,1,2,3 --multi-gpu 4 --multi-gpu-force  --steps 100 --snapshot-path oid_snapshots oid /var/oid --parent-label Person

Epoch 1/50
2018-09-25 14:57:31.436888: E tensorflow/stream_executor/cuda/cuda_driver.cc:1078] failed to synchronize the stop event: CUDA_ERROR_ILLEGAL_ADDRESS
2018-09-25 14:57:31.436888: E tensorflow/stream_executor/cuda/cuda_driver.cc:1078] failed to synchronize the stop event: CUDA_ERROR_ILLEGAL_ADDRESS
2018-09-25 14:57:31.436888: E tensorflow/stream_executor/cuda/cuda_driver.cc:1078] failed to synchronize the stop event: CUDA_ERROR_ILLEGAL_ADDRESS
2018-09-25 14:57:31.436893: E tensorflow/stream_executor/cuda/cuda_driver.cc:1078] failed to synchronize the stop event: CUDA_ERROR_ILLEGAL_ADDRESS
2018-09-25 14:57:31.436980: E tensorflow/stream_executor/cuda/cuda_timer.cc:55] Internal: error destroying CUDA event in context 0x8365350: CUDA_ERROR_ILLEGAL_ADDRESS
2018-09-25 14:57:31.437097: E tensorflow/stream_executor/cuda/cuda_timer.cc:55] Internal: error destroying CUDA event in context 0x89fa330: CUDA_ERROR_ILLEGAL_ADDRESS
2018-09-25 14:57:31.437068: E tensorflow/stream_executor/cuda/cuda_timer.cc:55] Internal: error destroying CUDA event in context 0x7cd6b60: CUDA_ERROR_ILLEGAL_ADDRESS
2018-09-25 14:57:31.437011: E tensorflow/stream_executor/cuda/cuda_timer.cc:55] Internal: error destroying CUDA event in context 0x7622e90: CUDA_ERROR_ILLEGAL_ADDRESS
2018-09-25 14:57:31.437161: E tensorflow/stream_executor/cuda/cuda_timer.cc:60] Internal: error destroying CUDA event in context 0x7cd6b60: CUDA_ERROR_ILLEGAL_ADDRESS
2018-09-25 14:57:31.437173: E tensorflow/stream_executor/cuda/cuda_timer.cc:60] Internal: error destroying CUDA event in context 0x7622e90: CUDA_ERROR_ILLEGAL_ADDRESS
2018-09-25 14:57:31.437113: E tensorflow/stream_executor/cuda/cuda_timer.cc:60] Internal: error destroying CUDA event in context 0x8365350: CUDA_ERROR_ILLEGAL_ADDRESS
2018-09-25 14:57:31.437123: E tensorflow/stream_executor/cuda/cuda_timer.cc:60] Internal: error destroying CUDA event in context 0x89fa330: CUDA_ERROR_ILLEGAL_ADDRESS
2018-09-25 14:57:31.437207: F tensorflow/stream_executor/cuda/cuda_dnn.cc:211] Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream.
2018-09-25 14:57:31.437211: F tensorflow/stream_executor/cuda/cuda_dnn.cc:211] Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream.
2018-09-25 14:57:31.437315: F tensorflow/stream_executor/cuda/cuda_dnn.cc:211] Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream.
2018-09-25 14:57:31.437303: F tensorflow/stream_executor/cuda/cuda_dnn.cc:211] Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream.



code address:https://github.com/fizyr/keras-retinanet"
22490,failed to build tensorfolow lite demo via both bazel and AndroidStudio,"I tried to build the tflite demo apk via bazel.

	INFO: SHA256 (https://github.com/bazelbuild/rules_python/archive/8b5d0683a7d878b28fffe464779c8a53659fc645.tar.gz) = 8b32d2dbb0b0dca02e0410da81499eef8ff051dad167d6931a92579e3b2a1d48
	ERROR: error loading package '': Encountered error while reading extension file 'requirements.bzl': no such package '@pip_deps//': Traceback (most recent call last):
	  File ""/home/local/SPREADTRUM/ben.shi/.cache/bazel/_bazel_spreadtrum.com/1ebbfeac35eb8d759797bb7ba5198a8a/external/io_bazel_rules_python/python/pip.bzl"", line 26
	  repository_ctx.execute([""python"", repository_ctx.path(r...("""")])
	File ""/home/local/SPREADTRUM/ben.shi/.cache/bazel/_bazel_spreadtrum.com/1ebbfeac35eb8d759797bb7ba5198a8a/external/io_bazel_rules_python/python/pip.bzl"", line 29, in repository_ctx.execute
	  repository_ctx.path(repository_ctx.attr.requirements)
	Not a regular file: /home/local/SPREADTRUM/ben.shi/Desktop/tensorflow/tensorflow/requirements.txt
	ERROR: error loading package '': Encountered error while reading extension file 'requirements.bzl': no such package '@pip_deps//': Traceback (most recent call last):
	  File ""/home/local/SPREADTRUM/ben.shi/.cache/bazel/_bazel_spreadtrum.com/1ebbfeac35eb8d759797bb7ba5198a8a/external/io_bazel_rules_python/python/pip.bzl"", line 26
	  repository_ctx.execute([""python"", repository_ctx.path(r...("""")])
	File ""/home/local/SPREADTRUM/ben.shi/.cache/bazel/_bazel_spreadtrum.com/1ebbfeac35eb8d759797bb7ba5198a8a/external/io_bazel_rules_python/python/pip.bzl"", line 29, in repository_ctx.execute
	  repository_ctx.path(repository_ctx.attr.requirements)
	Not a regular file: /home/local/SPREADTRUM/ben.shi/Desktop/tensorflow/tensorflow/requirements.txt
	INFO: Elapsed time: 96.822s
	INFO: 0 processes.
	FAILED: Build did NOT complete successfully (0 packages loaded)

My steps
1. git checkout remotes/origin/r1.9
2. bazel build --cxxopt=--std=c++11 //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo --config=android_arm64 --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a
	
I explicitly set NDK and SDK in WORKSPACE as 

	android_sdk_repository(
	    name = ""androidsdk"",
	    api_level = 26,
	    # Ensure that you have the build_tools_version below installed in the
	    # SDK manager as it updates periodically.
	    build_tools_version = ""27.0.3"",
	    # Replace with path to Android SDK on your system
	    path = ""/home/local/SPREADTRUM/ben.shi/android/sdk""
	)
	android_ndk_repository(
	    name=""androidndk"",
	    path=""/home/local/SPREADTRUM/ben.shi/android/android-ndk-r16b"",
	    # This needs to be 14 or higher to compile TensorFlow.
	    # Please specify API level to >= 21 to build for 64-bit
	    # archtectures or the Android NDK will automatically select biggest
	    # API level that it supports without notice.
	    # Note that the NDK version is not the API level.
	    api_level=28
	)
"
22489,absl/numeric/int128_have_intrinsic.inc: No such file or directory,"Current `tf-nightly-gpu` fails to compile with CUDA-aware custom ops like this:

```
    cc1plus: warning: command line option '-Wstrict-prototypes' is valid for C/ObjC but not for C++
    In file included from /usr/local/lib/python2.7/dist-packages/tensorflow/include/absl/strings/numbers.h:37:0,
                     from /usr/local/lib/python2.7/dist-packages/tensorflow/include/absl/strings/str_cat.h:62,
                     from /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/stream_executor/lib/strcat.h:21,
                     from /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/stream_executor/launch_dim.h:40,
                     from /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/stream_executor/stream.h:36,
                     from horovod/tensorflow/mpi_ops.cc:29:
    /usr/local/lib/python2.7/dist-packages/tensorflow/include/absl/numeric/int128.h:698:50: fatal error: absl/numeric/int128_have_intrinsic.inc: No such file or directory
     #include ""absl/numeric/int128_have_intrinsic.inc""
                                                      ^
    compilation terminated.
```

cc @yuefengz "
22488,"What's the version of squeezenet, 1.1 or 1.0 in performance part of TFLITE","link here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/performance.md

Besides, where can I find the squeezenetV1.1 model of TFLITE with FP32 and no quant. I only found the squeezenetv1.0 [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md#image-classification-float-models), no squeezenetv1.1.

Thanks in advance.  "
22487,Installing Kerras on R/Windows,"Hi All,

I hope somebody can help.

I had a well running Keras version running on R, which took me 30 minutes to install. Then I needed to reinstall after more than 20 hours and several workarounds I got everything running except the install_keras() command. See below:

> install_keras()
Using r-tensorflow conda environment for TensorFlow installation
Determining latest release of TensorFlow...done
Installing TensorFlow...
Collecting tensorflow==1.10.1 from https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.10.1-cp36-cp36m-win_amd64.whl
  HTTP error 404 while getting https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.10.1-cp36-cp36m-win_amd64.whl
  Could not install requirement tensorflow==1.10.1 from https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.10.1-cp36-cp36m-win_amd64.whl because of error 404 Client Error: Not Found for url: https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.10.1-cp36-cp36m-win_amd64.whl
Could not install requirement tensorflow==1.10.1 from https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.10.1-cp36-cp36m-win_amd64.whl because of HTTP error 404 Client Error: Not Found for url: https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.10.1-cp36-cp36m-win_amd64.whl for URL https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.10.1-cp36-cp36m-win_amd64.whl
You are using pip version 10.0.1, however version 18.0 is available.
You should consider upgrading via the 'python -m pip install --upgrade pip' command.
Error: Error 1 occurred installing packages into conda environment r-tensorflow
In addition: Warning message:
running command '""C:/MINICO~1/Scripts/activate"" r-tensorflow && pip install --upgrade --ignore-installed ""https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.10.1-cp36-cp36m-win_amd64.whl""' had status 1 

It seems that version 10.10.1 is not available for windows (I use Windows 10), while it is for all other platforms. The file tensorflow-1.10.1-cp36-cp36m-win_amd64.whl seems not to be available anywhere on the Internet (same seems to be true for the 86 version)

I tested  in a browser ""https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.10.1-cp36-cp36m-win_amd64.whl"" does not work.
Older version 1.10.0: ""https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.10.0-cp36-cp36m-win_amd64.whl"" downloads file.

Also: 

install_keras(tensorflow=""1.10.1)  gives an HTTP error 404 error

install_keras(tensorflow=""1.10.0) installs fine, but does not work. 
E.g. (below first command works, while second one does not; Flags were set correctly, code used to work in my old installation, I also downgraded numpy and setup tools as requested => no success)
modelNn <- keras_model_sequential() 
modelNn %>% 
  layer_dense(units = FLAGS$dense1Neurons , activation = 'relu', input_shape = c(numInputs)) %>% 
  layer_dropout(rate = FLAGS$dense1Dropout)%>%
  layer_dense(units = 1, activation = 'linear')

I really rely on R tensorflow/keras in my research. Please help!

Thanks

Carsten"
22486,error: Keras-Preprocessing 1.0.3 is installed but keras-preprocessing==1.0.2 is required by {'keras'},"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: pypi
- **TensorFlow version (use command below)**: tensorflow 1.11.0rc2
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: not using
- **GCC/Compiler version (if compiling from source)**: not using
- **CUDA/cuDNN version**: not using
- **GPU model and memory**: not using
- **Exact command to reproduce**: Trying to install keras on Travis-CI (linux-Ubuntu 16.04), report: 

### Describe

Installed /home/travis/virtualenv/python3.6.3/lib/python3.6/site-packages/PyYAML-4.2b4-py3.6-linux-x86_64.egg
error: Keras-Preprocessing 1.0.3 is installed but keras-preprocessing==1.0.2 is required by {'keras'}

After I manually downgrade keras-preprocessing to 1.0.2 in requirements.txt and setup.py, it says:

error: Keras-Preprocessing 1.0.2 is installed but keras-preprocessing==1.0.3 is required by {'tensorflow'}

### Error Logs

Situation 1: keras-preprocessing 1.0.3 installed: [log](https://travis-ci.org/NTMC-Community/MatchZoo/jobs/429012836)."
22485,Keras eager execution: multi-output model unexpected behavior,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Colab
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: Colab
- **TensorFlow version (use command below)**: 1.10.1
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: Colab
- **GCC/Compiler version (if compiling from source)**: Colab
- **CUDA/cuDNN version**: Colab
- **GPU model and memory**: Colab
- **Exact command to reproduce**:

### Describe the problem
I have a very simple multi-output model. The model is expected to be trained with respect to the first output only.  You can find the code below.  When I run it, first I get a warning message then it crashes.
The warning message is expected since I am intentionally missing considering the second output from the loss function. But the error message is completely contradicting that : `No data provided for ""out2"". Need data for each key in: ['out1', 'out2']`

`WARNING:tensorflow:Output ""out2"" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to ""out2"" during training.`

### Source code / logs

```
import tensorflow as tf
import tensorflow.keras as keras
import tensorflow.keras.backend as K
import numpy as np
from tensorflow.keras.layers import Dense, Input, Layer
from tensorflow.keras.models import Model

tf.enable_eager_execution()

input_tensor = Input(shape=(20,), name=""input"")
hidden = Dense(100, activation='relu')(input_tensor)
out1 = Dense(10, activation='relu', name=""out1"")(hidden)
out2 = Dense(5, activation='relu', name=""out2"")(hidden)
model = Model(inputs=input_tensor, outputs=[out1, out2])
model.compile(loss={""out1"": ""mse""}, optimizer=tf.train.AdamOptimizer(learning_rate=0.001))
model.summary()

np.random.seed(0)
X = np.random.random((3, 20)).astype(np.float32)
Y1 = np.random.random((3, 10)).astype(np.float32)
Y2 = np.random.random((3, 5)).astype(np.float32)
model.fit(x={'input' : X}, y={'out1' : Y1}, batch_size=1, epochs=10)

```

### logs
```
WARNING:tensorflow:Output ""out2"" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to ""out2"" during training.

---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_utils.py in standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)
    125           if data[x].__class__.__name__ == 'DataFrame' else data[x]
--> 126           for x in names
    127       ]

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_utils.py in <listcomp>(.0)
    125           if data[x].__class__.__name__ == 'DataFrame' else data[x]
--> 126           for x in names
    127       ]

KeyError: 'out2'

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-6-00a3f18ea48b> in <module>()
     19 Y1 = np.random.random((3, 5)).astype(np.float32)
     20 Y1 = np.random.random((3, 10)).astype(np.float32)
---> 21 model.fit(x={'input' : X}, y={'out1' : Y1}, batch_size=1, epochs=10)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)
   1276         steps_name='steps_per_epoch',
   1277         steps=steps_per_epoch,
-> 1278         validation_split=validation_split)
   1279 
   1280     # Prepare validation data.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)
    915           feed_output_shapes,
    916           check_batch_axis=False,  # Don't enforce the batch size.
--> 917           exception_prefix='target')
    918 
    919       # Generate sample-wise weight values given the `sample_weight` and

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_utils.py in standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)
    128     except KeyError as e:
    129       raise ValueError('No data provided for ""' + e.args[0] + '"". Need data '
--> 130                        'for each key in: ' + str(names))
    131   elif isinstance(data, list):
    132     if isinstance(data[0], list):

ValueError: No data provided for ""out2"". Need data for each key in: ['out1', 'out2']
```

"
22484,"CollectiveAllReduceStrategy ""Out of range: End of sequence"" warnings","### System information
Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 16.04.4 ppc64le
TensorFlow installed from (source or binary):
source
TensorFlow version (use command below):
1.11
Python version: Python 2.7.15
CUDA/cuDNN version: Cuda 9.2 CuDNN 7.2.1
GPU model and memory: V100 16GB
Exact command to reproduce: python keras_model_to_estimator.py /tmp/tfkeras_example

### Describe the problem
Running the [ecosystem CollectiveAllReduceStrategy estimator example ](https://github.com/tensorflow/ecosystem/blob/master/distribution_strategy/keras_model_to_estimator.py) on a box with 2GPUs, we get those warning constantly after the last step of an epoch:
```
INFO:tensorflow:loss = 0.6660614, step = 1500 (0.288 sec)
2018-09-21 19:23:01.075834: W tensorflow/core/common_runtime/base_collective_executor.cc:203] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext}} = IteratorGetNext[output_shapes=[[?,10], [?,1]], output_types=[DT_FLOAT, DT_INT64]](IteratorFromStringHandleV2)]]
	 [[{{node FunctionBufferingResourceGetNext_1}} = FunctionBufferingResourceGetNext[output_types=[DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:GPU:1""](FunctionBufferingResource_1)]]
2018-09-21 19:23:01.075843: W tensorflow/core/common_runtime/base_collective_executor.cc:203] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext}} = IteratorGetNext[output_shapes=[[?,10], [?,1]], output_types=[DT_FLOAT, DT_INT64]](IteratorFromStringHandleV2)]]
	 [[{{node FunctionBufferingResourceGetNext}} = FunctionBufferingResourceGetNext[output_types=[DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:GPU:0""](FunctionBufferingResource)]]
	 [[{{node global_step/Read/ReadVariableOp/_171}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_550_global_step/Read/ReadVariableOp"", tensor_type=DT_INT64, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]
2018-09-21 19:23:01.076090: W tensorflow/core/common_runtime/base_collective_executor.cc:203] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext}} = IteratorGetNext[output_shapes=[[?,10], [?,1]], output_types=[DT_FLOAT, DT_INT64]](IteratorFromStringHandleV2)]]
	 [[{{node FunctionBufferingResourceGetNext}} = FunctionBufferingResourceGetNext[output_types=[DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:GPU:0""](FunctionBufferingResource)]]
```
@yuefengz: Do you also see the same warnings in your test logs?
"
22482," java.lang.IllegalArgumentException: Input to reshape is a tensor with 3072 values, but the requested shape has 9437184","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.8
- **Python version**:3.6
- **Bazel version (if compiling from source)**:0.16.1
- **GCC/Compiler version (if compiling from source)**:NA 
- **CUDA/cuDNN version**:NA
- **GPU model and memory**:NA
- **Exact command to reproduce**:NA

### Describe the problem
I have built a custom classifier model using keras with tensorflow backend and I am trying to inference the model to use it in model. I have successfully created protobuf file which contains inference model. When I am trying to feed the input to the model in android. I am getting this error
```
java.lang.IllegalArgumentException: Input to reshape is a tensor with 3072 values, but the requested shape has 9437184
    	 [[{{node reshape_1/Reshape}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_reshape_1_input_0_0, reshape_1/Reshape/shape)]]
```

### Source code / logs
```
09-24 18:24:52.780 2480-3594/com.appa.iocsv2 E/TensorFlowInferenceInterface: Failed to run TensorFlow inference with inputs:[reshape_1_input], outputs:[dense_2/Softmax]
    
    --------- beginning of crash
09-24 18:24:52.780 2480-3594/com.appa.iocsv2 E/AndroidRuntime: FATAL EXCEPTION: IntentService[Prediction Service]
    Process: com.appa.iocsv2, PID: 2480
    java.lang.IllegalArgumentException: Input to reshape is a tensor with 3072 values, but the requested shape has 9437184
    	 [[{{node reshape_1/Reshape/eightbit}} = QuantizedReshape[T=DT_QUINT8, Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](reshape_1/Reshape_eightbit/reshape_1_input/quantize, reshape_1/Reshape/shape, reshape_1/Reshape_eightbit/reshape_1_input/quantize:1, reshape_1/Reshape_eightbit/reshape_1_input/quantize:2)]]
        at org.tensorflow.Session.run(Native Method)
        at org.tensorflow.Session.access$100(Session.java:48)
        at org.tensorflow.Session$Runner.runHelper(Session.java:314)
        at org.tensorflow.Session$Runner.run(Session.java:264)
        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:228)
        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:197)
        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:187)
        at com.appa.iocsv2.analyze.PredictionService.predict(PredictionService.java:151)
        at com.appa.iocsv2.analyze.PredictionService.onHandleIntent(PredictionService.java:103)
        at android.app.IntentService$ServiceHandler.handleMessage(IntentService.java:68)
        at android.os.Handler.dispatchMessage(Handler.java:102)
        at android.os.Looper.loop(Looper.java:154)
        at android.os.HandlerThread.run(HandlerThread.java:61)
```
"
22481,Inconsistent behavior in tf.contrib.distributions.percentile for NaN values,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: v1.10.0-rc1-19-g656e7a2b34 1.10.0
- **Python version**: 3.6.6 (Anaconda)
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: NA

### Describe the problem
[`tf.contrib.distributions.percentile`](https://www.tensorflow.org/api_docs/python/tf/contrib/distributions/percentile) behaves inconsistently when NaN values are given. While the documentation does not explicitly states how this case should be handled, one would normally expect to always receive NaN, or maybe ignore them in the computation (compute the percentile of non-NaN values), or even replace them with 0. But the current logic seems to somehow depend on the order of the values in the input: trying to compute the median of `[1.0, NaN]` results in `1.0`, while for `[NaN, 1.0]` it is `NaN`.

The behavior seems to be different also between CPU and GPU. I obtained the results above while running on CPU. The same test on GPU (CUDA 9.0, CUDNN 7, Titan V) produced `NaN` in both cases; however, the computed median on GPU for `[1.0, 1.0, NaN]` was again `1.0`.

### Source code / logs

```py
import tensorflow as tf
import math

x = tf.placeholder(tf.float32, [None])
m = tf.contrib.distributions.percentile(x, q=50., validate_args=True)
with tf.Session() as sess:
    print(sess.run(m, feed_dict={ x: [1, math.nan] }))  # Prints 1.0
    print(sess.run(m, feed_dict={ x: [math.nan, 1] }))  # Prints nan
```"
22480,OSError: SavedModel file does not exist at: saved_model_dir/{saved_model.pbtxt|saved_model.pb},"I want to optimized the my Tensor flow model (mars-small128.pb) 
I have saved_model_dir directory which contain mars-small128.pb file 
Here is my code 
import tensorflow as tf
converter = tf.contrib.lite.TocoConverter.from_saved_model(saved_model_dir)
converter.post_training_quantize = True
tflite_quantized_model = converter.convert()
open(""quantized_model.tflite"", ""wb"").write(tflite_quantized_model)
Error
OSError: SavedModel file does not exist at: saved_model_dir/{saved_model.pbtxt|saved_model.pb}
System information
Os:linux 16
Python:3.5
tensorflow Installation:pip install -U tf-nightly (latest versiopn)
using Cpu

Thanks



"
22479,Keras: removing layers with model.layers.pop() doesn't work,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Colab
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: Colab
- **TensorFlow version (use command below)**:  1.10.1
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: Colab
- **GCC/Compiler version (if compiling from source)**: Colab
- **CUDA/cuDNN version**: Colab
- **GPU model and memory**: Colab
- **Exact command to reproduce**:

### Describe the problem
When we delete a layer with model.layers.pop(), the deleted layer reappears. 

### Source code / logs

    import tensorflow as tf
    import tensorflow.keras as keras
    import tensorflow.keras.backend as K
    from tensorflow.keras.layers import Dense, Input, Layer
    from tensorflow.keras.models import Model
    input_tensor = Input(shape=(10,))
    hidden = Dense(100, activation='relu')(input_tensor)
    out = Dense(10, activation='relu')(hidden)
    model = Model(input_tensor, out)
    model.compile(loss=""mse"", optimizer=tf.train.AdamOptimizer(learning_rate=0.001))
    model.summary()
    model.layers.pop()
    model.layers.pop()
    model.summary()
    hidden = Dense(120, activation='relu')(model.layers[-1].output)
    out = Dense(5, activation='softmax')(hidden)
    model = Model(input_tensor, out)
    model.summary()

```
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 10)                0         
_________________________________________________________________
dense (Dense)                (None, 100)               1100      
_________________________________________________________________
dense_1 (Dense)              (None, 10)                1010      
=================================================================
Total params: 2,110
Trainable params: 2,110
Non-trainable params: 0
```
```
_________________________________________________________________
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 10)                0         
_________________________________________________________________
dense (Dense)                (None, 100)               1100      
_________________________________________________________________
dense_1 (Dense)              (None, 10)                1010      
=================================================================
Total params: 2,110
Trainable params: 2,110
Non-trainable params: 0

```

```
_________________________________________________________________
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 10)                0         
_________________________________________________________________
dense (Dense)                (None, 100)               1100      
_________________________________________________________________
dense_1 (Dense)              (None, 10)                1010      
_________________________________________________________________
dense_2 (Dense)              (None, 120)               1320      
_________________________________________________________________
dense_3 (Dense)              (None, 5)                 605       
=================================================================
Total params: 4,035
Trainable params: 4,035
Non-trainable params: 0
_________________________________________________________________
```"
22477,failed to query event: CUDA_ERROR_LAUNCH_FAILED,"

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 
Linux Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
NA
- **TensorFlow installed from (source or binary)**: 
binary
- **TensorFlow version (use command below)**:
1.10.1
- **Python version**:
3.6.5
- **Bazel version (if compiling from source)**:
NA
- **GCC/Compiler version (if compiling from source)**:
NA
- **CUDA/cuDNN version**:
9.0 cuDNN 7.2.1
- **GPU model and memory**:
GeForce GTX 1080 Ti 11GB
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Encountered this error while running a image classification training script. 
E tensorflow/stream_executor/cuda/cuda_event.cc:48] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED
2018-09-23 18:31:38.983015: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:206] Unexpected Event status: 1
Aborted (core dumped)

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22476,ArchLinux [Optimus-Bumblebee] - Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ArchLinux x64
- **TensorFlow installed from (source or binary)**: via pip
- **TensorFlow version (use command below)**: 1.10.0
- **Python version**: 3.6.5
- **CUDA/cuDNN version**: CUDA 9.2.148 - cuDNN 7.2.1
- **GPU model and memory**: NVIDIA GTX 970M 3GB

- **Exact command to reproduce**:

- Note: I'm using Nvidia Optimus with Bumblebee: https://wiki.archlinux.org/index.php/bumblebee

1. git clone https://github.com/isseu/emotion-recognition-neural-networks
* Make reqiured configurations...

2. I removed `tensorflow` and added `tensorflow-gpu 1.10.0` to `requirements.txt`

3. `sudo optirun python3 emotion_recognition.py train` also tried:
`python3 emotion_recognition.py train`

### Describe the problem

To run Train the data on my GPU, I just edited a few lines on emotion_recognation.py:

`import tensorflow as tf`

Added under `def __init__(self):`
`tf.ConfigProto(allow_soft_placement=True)`

Added under `def start_training(self):`
```
        with tf.device('/gpu:0'):
            self.model.fit(bla bla bla)
```

### Source code / logs

```
hdf5 is not supported on this machine (please install/reinstall h5py for optimal experience)
[+] Dataset found and loaded
[+] Building CNN
WARNING:tensorflow:From /home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tflearn/initializations.py:119: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.
WARNING:tensorflow:From /home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tflearn/objectives.py:66: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
2018-09-23 21:31:13.883618: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-09-23 21:31:14.001876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-09-23 21:31:14.002292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: 
name: GeForce GTX 970M major: 5 minor: 2 memoryClockRate(GHz): 1.038
pciBusID: 0000:01:00.0
totalMemory: 2.95GiB freeMemory: 2.89GiB
2018-09-23 21:31:14.002318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2018-09-23 21:31:14.260311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-09-23 21:31:14.260358: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 
2018-09-23 21:31:14.260364: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N 
2018-09-23 21:31:14.260525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2595 MB memory) -> physical GPU (device: 0, name: GeForce GTX 970M, pci bus id: 0000:01:00.0, compute capability: 5.2)
2018-09-23 21:31:14.554680: W tensorflow/core/framework/allocator.cc:108] Allocation of 226492416 exceeds 10% of system memory.
2018-09-23 21:31:14.644729: W tensorflow/core/framework/allocator.cc:108] Allocation of 226492416 exceeds 10% of system memory.
2018-09-23 21:31:14.733250: W tensorflow/core/framework/allocator.cc:108] Allocation of 226492416 exceeds 10% of system memory.
2018-09-23 21:31:14.822594: W tensorflow/core/framework/allocator.cc:108] Allocation of 226492416 exceeds 10% of system memory.
2018-09-23 21:31:14.911661: W tensorflow/core/framework/allocator.cc:108] Allocation of 226492416 exceeds 10% of system memory.
2018-09-23 21:31:15.880266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2018-09-23 21:31:15.880304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-09-23 21:31:15.880313: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 
2018-09-23 21:31:15.880321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N 
2018-09-23 21:31:15.880412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2595 MB memory) -> physical GPU (device: 0, name: GeForce GTX 970M, pci bus id: 0000:01:00.0, compute capability: 5.2)
[+] Training network
---------------------------------
Run id: emotion_recognition
Log directory: /tmp/tflearn_logs/
---------------------------------
Training samples: 11214
Validation samples: 2804
--
Traceback (most recent call last):
  File ""/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1278, in _do_call
    return fn(*args)
  File ""/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1261, in _run_fn
    self._extend_graph()
  File ""/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1295, in _extend_graph
    tf_session.ExtendSession(self._session)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'Accuracy/__raw_': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
Registered kernels:
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_BFLOAT16]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_INT8]
  device='CPU'; T in [DT_UINT8]
  device='CPU'; T in [DT_INT16]
  device='CPU'; T in [DT_UINT16]
  device='CPU'; T in [DT_INT32]
  device='CPU'; T in [DT_INT64]

	 [[Node: Accuracy/__raw_ = ScalarSummary[T=DT_FLOAT, _device=""/device:GPU:0""](Accuracy/__raw_/tags, Accuracy/Mean)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""emotion_recognition.py"", line 110, in <module>
    network.start_training()
  File ""emotion_recognition.py"", line 78, in start_training
    run_id='emotion_recognition'
  File ""/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tflearn/models/dnn.py"", line 216, in fit
    callbacks=callbacks)
  File ""/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tflearn/helpers/trainer.py"", line 339, in fit
    show_metric)
  File ""/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tflearn/helpers/trainer.py"", line 816, in _train
    tflearn.is_training(True, session=self.session)
  File ""/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tflearn/config.py"", line 95, in is_training
    tf.get_collection('is_training_ops')[0].eval(session=session)
  File ""/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 680, in eval
    return _eval_using_default_session(self, feed_dict, self.graph, session)
  File ""/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 4951, in _eval_using_default_session
    return session.run(tensors, feed_dict)
  File ""/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 877, in run
    run_metadata_ptr)
  File ""/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1272, in _do_run
    run_metadata)
  File ""/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'Accuracy/__raw_': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
Registered kernels:
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_BFLOAT16]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_INT8]
  device='CPU'; T in [DT_UINT8]
  device='CPU'; T in [DT_INT16]
  device='CPU'; T in [DT_UINT16]
  device='CPU'; T in [DT_INT32]
  device='CPU'; T in [DT_INT64]

	 [[Node: Accuracy/__raw_ = ScalarSummary[T=DT_FLOAT, _device=""/device:GPU:0""](Accuracy/__raw_/tags, Accuracy/Mean)]]

Caused by op 'Accuracy/__raw_', defined at:
  File ""emotion_recognition.py"", line 110, in <module>
    network.start_training()
  File ""emotion_recognition.py"", line 78, in start_training
    run_id='emotion_recognition'
  File ""/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tflearn/models/dnn.py"", line 216, in fit
    callbacks=callbacks)
  File ""/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tflearn/helpers/trainer.py"", line 288, in fit
    self.summ_writer, self.coord)
  File ""/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tflearn/helpers/trainer.py"", line 794, in initialize_fit
    val_feed_dict)
  File ""/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tflearn/helpers/trainer.py"", line 937, in create_testing_summaries
    summarize(self.metric, ""scalar"", sname, tr_summ_collection)
  File ""/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tflearn/helpers/summarizer.py"", line 98, in summarize
    summaries.get_summary(type, name, value, summary_collection)
  File ""/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tflearn/summaries.py"", line 46, in get_summary
    summ = tf.summary.scalar(tag, value)
  File ""/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tensorflow/python/summary/summary.py"", line 90, in scalar
    val = _gen_logging_ops.scalar_summary(tags=tag, values=tensor, name=scope)
  File ""/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tensorflow/python/ops/gen_logging_ops.py"", line 623, in scalar_summary
    ""ScalarSummary"", tags=tags, values=values, name=name)
  File ""/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 454, in new_func
    return func(*args, **kwargs)
  File ""/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3155, in create_op
    op_def=op_def)
  File ""/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'Accuracy/__raw_': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
Registered kernels:
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_BFLOAT16]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_INT8]
  device='CPU'; T in [DT_UINT8]
  device='CPU'; T in [DT_INT16]
  device='CPU'; T in [DT_UINT16]
  device='CPU'; T in [DT_INT32]
  device='CPU'; T in [DT_INT64]

	 [[Node: Accuracy/__raw_ = ScalarSummary[T=DT_FLOAT, _device=""/device:GPU:0""](Accuracy/__raw_/tags, Accuracy/Mean)]]
```
"
22475,The tensorflow tested builds advices wrong bazel version for tensorflow-gpu r1.8,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
No
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:
r1.8
- **Python version**:
2.7
- **Bazel version (if compiling from source)**:
0.9.0
- **GCC/Compiler version (if compiling from source)**:
4.8.5
- **CUDA/cuDNN version**:
9.0/7.1
- **GPU model and memory**:
GeForce GTX 960M
- **Exact command to reproduce**:
./configure 
The output is:
You have bazel 0.9.0 installed.
Please upgrade your bazel installation to version 0.10.0 or higher to build TensorFlow!
Configuration finished

### Describe the problem
I tried to build tf r1.8 with gpu with bazel 0.17 but got errors then I checked [here](https://www.tensorflow.org/install/source) to make sure I got the right version, but saw that I need bazel 0.9.0. When trying it with bazel 0.9.0 I get the error above.
I did not find where I could edit the tf website itself to make a PR, so I just opened this issue. 

### Source code / logs
None
"
22472,Why my tensorflow-gpu runs only on cpu?,"OS Platform and Distribution: windows10
**TensorFlow installed from **: pip install tensorflow-gpu
TensorFlow version: (tensorflow-gpu            1.10.0
Python version: Python 3.6.5
CUDA/cuDNN version: Cuda compilation tools, release 9.0/cudnn-9.0-windows10-x64-v7.3.0.29.zip

when I run nvidia-smi and nvcc-V,
The output is following:
(python36) C:\Windows\System32>nvidia-smi
Sun Sep 23 20:26:38 2018
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 398.82                 Driver Version: 398.82                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 1080   WDDM  | 00000000:01:00.0  On |                  N/A |
| 42%   30C    P8    11W / 200W |    488MiB /  8192MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      1176    C+G   Insufficient Permissions                   N/A      |
|    0      2388    C+G   ...oftEdge_8wekyb3d8bbwe\MicrosoftEdge.exe N/A      |
|    0      6712    C+G   ...tEdge_8wekyb3d8bbwe\MicrosoftEdgeCP.exe N/A      |
|    0      7036    C+G   ...t_cw5n1h2txyewy\ShellExperienceHost.exe N/A      |
|    0      7220    C+G   ...dows.Cortana_cw5n1h2txyewy\SearchUI.exe N/A      |
|    0      7656    C+G   ...tEdge_8wekyb3d8bbwe\MicrosoftEdgeCP.exe N/A      |
|    0      8260    C+G   ...hell.Experiences.TextInput.InputApp.exe N/A      |
|    0      9852    C+G   ...tEdge_8wekyb3d8bbwe\MicrosoftEdgeCP.exe N/A      |
|    0     10308    C+G   ...tEdge_8wekyb3d8bbwe\MicrosoftEdgeCP.exe N/A      |
|    0     13544    C+G   ...Chrome\Chrome\Application\360chrome.exe N/A      |
|    0     16656    C+G   ...es (x86)\Internet Explorer\iexplore.exe N/A      |
+-----------------------------------------------------------------------------+

(python36) C:\Windows\System32>nvcc -V
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2017 NVIDIA Corporation
Built on Fri_Sep__1_21:08:32_Central_Daylight_Time_2017
Cuda compilation tools, release 9.0, V9.0.176

When I use import tensorflow as tf,it's work well 
Then I tried to use this code:
>>> sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
The output is following:
Device mapping: no known devices.
2018-09-23 20:12:06.377265: I T:\src\github\tensorflow\tensorflow\core\common_runtime\direct_session.cc:288] Device mapping:

there is nothing show in the map.Why is this happening? How can I fix it?"
22471,How to use runStats() and view more detail of debug information in the android image classify demo?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
android and Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
Xiaomi Max2
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
the latest
- **Python version**:
3.5
- **Bazel version**:
N/A
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
N/A

### Describe the problem
I am doing a research of tensorflow on mobile devices and I get started in the android image classify demo.
I want to fetch the detail debug information while running pre-trianed model on android devices, and I notice the debug indormation can be fetched through pressing volume button while running the demo as the following:
![20180923190653](https://user-images.githubusercontent.com/14329360/45927296-e46fcd80-bf63-11e8-922c-9c6d2d9695a1.jpg)

However I can not find where to fetch the original debug data listed in the figure. Can someone tell me where to fetch these debug datas? Also the meaning of some of the datas are vague, for example [start], [first], what does these parameters mean? It is important of my research and hope someone could explain these, thx!


### Source code / logs
  private void renderDebug(final Canvas canvas) {
    if (!isDebug()) {
      return;
    }
    final Bitmap copy = cropCopyBitmap;
    if (copy != null) {
      final Matrix matrix = new Matrix();
      final float scaleFactor = 2;
      matrix.postScale(scaleFactor, scaleFactor);
      matrix.postTranslate(
          canvas.getWidth() - copy.getWidth() * scaleFactor,
          canvas.getHeight() - copy.getHeight() * scaleFactor);
      canvas.drawBitmap(copy, matrix, new Paint());

      final Vector<String> lines = new Vector<String>();
      if (classifier != null) {
        String statString = classifier.getStatString();// Here is where the core debug information come from, 
                                                                             //but I can not get more information!
        String[] statLines = statString.split(""\n"");
        for (String line : statLines) {
          lines.add(line);
        }
      }

      lines.add(""Frame: "" + previewWidth + ""x"" + previewHeight);
      lines.add(""Crop: "" + copy.getWidth() + ""x"" + copy.getHeight());
      lines.add(""View: "" + canvas.getWidth() + ""x"" + canvas.getHeight());
      lines.add(""Rotation: "" + sensorOrientation);
      lines.add(""Inference time: "" + lastProcessingTimeMs + ""ms"");

      borderedText.drawLines(canvas, 10, canvas.getHeight() - 10, lines);
    }
  }



"
22470,[keras compatibility] tf.keras.regularizers.l2() cannot be used in tf.get_variable(),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: custom code
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10.1
- **Python version**: 3.6.4
- OS Platform and Distribution: N/A
- Bazel version: N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A
- Mobile device: N/A
- Exact command to reproduce: Source code below.

### Describe the problem
`tf.keras.regularizers.l2()` cannot be used in `tf.get_variable()`. Currently I have to use `tf.contrib.layers.l2_regularizer()` in `tf.get_variable()`.  
Note that , `tf.keras.regularizers.l2()` seems to work well in `tf.layers.Dense()`.

In general, I expect that all `keras` functions like `regularizers`, `activations`, `initializers` could be used for `tensorflow`.

### Source code / logs
```
x = tf.get_variable(shape = [dim_in, dim_out],
    initializer=tf.keras.initializers.glorot_uniform(),
    regularizer=tf.keras.regularizers.l2(lambda))
```
Traceback:
>   File ""/Users/mac/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 1467, in get_variable
>     aggregation=aggregation)
>   File ""/Users/mac/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 1217, in get_variable
>     aggregation=aggregation)
>   File ""/Users/mac/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 527, in get_variable
>     aggregation=aggregation)
>   File ""/Users/mac/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 481, in _true_getter
>     aggregation=aggregation)
>   File ""/Users/mac/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 930, in _get_single_variable
>     loss_name = loss.name
> AttributeError: 'float' object has no attribute 'name'
"
22466,Unable to use TFOptimizer from keras in tensorflow,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: NA
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA
- **TensorFlow installed from (source or binary)**: Running in colab, chrome
- **TensorFlow version (use command below)**: 1.10.1
- **Python version**: 2
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: NA

### Describe the problem
Using TFOptimizer from keras inside tensorflow gives error - 
```
---> 22 opt = keras.optimizers.TFOptimizer(tf_opt)
     23 opt.lr = lr
     24 

AttributeError: 'module' object has no attribute 'TFOptimizer'
```

I manually imported it to make the code work-
```
from tensorflow.python.keras.optimizers import TFOptimizer
```

Could TFOptimizer class be exported from tensorflow in the following file-
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/optimizers.py#L691

### Source code / logs

"
22465,[Feature Request]: tf.data.Dataset.map parallelism autotune enhancement,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Cent OS
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 2.7.5
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: NA


### Describe the problem
Recently this [PR](https://github.com/tensorflow/tensorflow/commit/c8a0dfc741736a59f8fd1776b71f38619d66da56#diff-df634c8243713c0afd2e05c1689412e2) which leverages the underlying  performance model to find the optimal values for the parallelism knobs, this seems intuitive and really useful for me. While after carefully going through the relevant codebase, there are two problems that have been bothering me.

(1) Currently dataset.map's num_parallel_calls autotune (as well as [dataset.prefetch buffer size autotune](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/prefetch_autotuner.cc#L36-L55)) only works in the scenario when `kAutotune` guard is set and then start a one-way increment. On the one hand, in actual production environment, careless setting of parameters like `map.num_parllel_calls` or `prefetch.buffer_size` is somewhat common, which may result in inefficient  or even wore resource utilization.  On the other hand, even a carefully-tuned parameter which leverages underlying system info and works good in environment may turn out to be incompatible when we transfer into another environment. Therefore it's nature that our AUTOTUNE logic can support our auto_tune params's autotune based on runtime system info **as well as user's initial params passed in**, in which case the autotune_params' adjustment should be two-way.

(2) At the specific code implementation level, this PR leverages `port::NumSchedulableCPUs()` to get the number of cores available in the process, and calculate the idealized acceleration brought by parallelism's increment(e.g. [PARALLEL_MAP](https://github.com/tensorflow/tensorflow/blob/c8a0dfc741736a59f8fd1776b71f38619d66da56/tensorflow/core/framework/model.cc#L214-L221)). This should be tuned carefully as in complicated distributed environment the increase of map parallelism perhaps results in invalid scheduling of resources, and eventually **brings in negative optimization**.

For the above two points I have figured out some workaround, and the first draft of the code is also being developed. Any comments from you side is hight welcome and much appreciated. @jsimsa 

Thanks


"
22464,memory leak cased by function tf.dynamic_partition,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:_no_
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  _Linux Ubuntu 16.04.4 LTS_
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:_no_
- **TensorFlow installed from (source or binary)**:_binary_
- **TensorFlow version (use command below)**:_1.8.0_
- **Python version**:_Python 3.5.2_
- **Bazel version (if compiling from source)**:_None_
- **GCC/Compiler version (if compiling from source)**:_None_
- **CUDA/cuDNN version**:_CUDA-9.0/cuDNN-7.0.5_
- **GPU model and memory**: _GTX 1080Ti 11GB, 2 same cards_
- **Exact command to reproduce**:_below_

### Supplementary material
I just pulled the tensorflow's offical docker image and RUN some `pip install ...` instructions.
The tag of the iamge is `1.8.0-devel-gpu-py3`.

### Describe the problem
I want to calculate a focal loss. 
![image](https://user-images.githubusercontent.com/22476764/45917573-bb403600-beac-11e8-8262-56be63acd1c3.png)

It just added weight factor to the original cross entropy loss.

#### `tf_leak` code (shown in Source code / logs) 

When I used the `tf_leak` code as shown below, there will be memory leak.
I used `top` command to see the process's memory usage.

- **pic01 occupy about 3G memory after about 1min**
![image](https://user-images.githubusercontent.com/22476764/45917603-212cbd80-bead-11e8-8b2b-e784196405b2.png)

- **pic02 occupy about 11G memory after about 33min**
![image](https://user-images.githubusercontent.com/22476764/45917609-3ace0500-bead-11e8-9d6e-b58d748f6fbc.png)

- **pic03 occupy about 14G memory after about 191min**
![image](https://user-images.githubusercontent.com/22476764/45917619-45889a00-bead-11e8-8370-082b341c4907.png)

**I use the `tf.estimator` and the graph is finazied. So I do not add ops to the graph when the seesion is running.**

- **pic04 graph is finazied**
![image](https://user-images.githubusercontent.com/22476764/45917706-16bef380-beae-11e8-8af9-a0a8fd46c52a.png)


#### `tf_no_leak` code (shown in Source code / logs) 
 
I replace the ` tf.dynamic_partition` function with `tf.boolean_mask` and the memory leak disappered.

- **pic05 occupy about 3G memory after about 9min**
![image](https://user-images.githubusercontent.com/22476764/45917758-ec216a80-beae-11e8-9d49-115f2598fa50.png)

- **pic06 occupy about 3G memory after about 133min**
![image](https://user-images.githubusercontent.com/22476764/45917782-2be85200-beaf-11e8-9427-021f2f5fcd51.png)

#### more information

I tested the `tf_leak` and `tf_no_leak` code with a small dataset, when I use a bigger dataset, the process will occupy a large memory and the system will kill the process. This is why I found the problem.


### Source code / logs
```
params:
		valid_labels	:	the labels, a Tensor with shape [N * w * h, ]
		valid_logits	:	the logits, a Tensor with shape [N * w * h, num_classes]
		num_classes 	: 	the number of classes, a scalar
                ohem_prob_g  :       a hyperparameter, a scalar, default=1.5
```

1. `tf_leak` code

```	
valid_probs = tf.clip_by_value(tf.nn.softmax(valid_logits, axis=-1), \
					clip_value_min=1e-6, clip_value_max=1.0
valid_one_hot = tf.one_hot(valid_labels, depth=num_classes, dtype=tf.int32)
weight_matrix = ohem_prob_g - tf.dynamic_partition(valid_probs, valid_one_hot, num_partitions=2)[1]
## calculate the final cross-entropy
cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=valid_labels, 
						logits=valid_logits, weights=weight_matrix)
return cross_entropy
```

2. `tf_no_leak` code

same as the `tf_leak` code except I replace the `tf.dynamic_partition` with `tf.boolean_mask`.

```
valid_probs = tf.clip_by_value(tf.nn.softmax(valid_logits, axis=-1), \
					clip_value_min=1e-6, clip_value_max=1.0)
valid_one_hot = tf.one_hot(valid_labels, depth=num_classes, dtype=tf.int32)
## calculate the final cross-entropy
weight_matrix = ohem_prob_g - tf.boolean_mask(valid_probs, valid_one_hot)
cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=valid_labels, 
						logits=valid_logits, weights=weight_matrix)
return cross_entropy
```

3. input code used for the `tf.estimator`

```
def input_fn(is_training, data_dir, batch_size, num_epochs=1):
  """"""Input_fn using the tf.data input pipeline for CIFAR-10 dataset.
  Args:
    is_training: A boolean denoting whether the input is for training.
    data_dir: The directory containing the input data.
    batch_size: The number of samples per batch.
    num_epochs: The number of epochs to repeat the dataset.
  Returns:
    A tuple of images and labels.
  """"""
  dataset = tf.data.Dataset.from_tensor_slices(get_filenames(is_training, data_dir))
  dataset = dataset.flat_map(tf.data.TFRecordDataset)
  if is_training:
    dataset = dataset.shuffle(buffer_size=_NUM_IMAGES['train'])

  dataset = dataset.map(parse_record)
  dataset = dataset.map(
      lambda image, label, idx: preprocess_image(image, label, idx, is_training),
      num_parallel_calls=1)

  # We call repeat after shuffling, rather than before, to prevent separate
  # epochs from blending together.
  dataset = dataset.repeat(num_epochs)
  dataset = dataset.batch(batch_size)
  dataset = dataset.prefetch(buffer_size=FLAGS.prefetch_buffer_size)

  iterator = dataset.make_one_shot_iterator()
  images, labels, idx = iterator.get_next()

  return images, labels

```
"
22463,Tensorflow mystique bug with MirroredStrategy freeze on particular local GPU combo,"https://stackoverflow.com/questions/52425485/tensorflow-mirroredstrategy-freeze-on-particular-gpu-combo

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04 host Linux Ubuntu 16.04 inside Docker container
- **TensorFlow installed from (source or binary)**: source and binary
- **TensorFlow version (use command below)**: 1.10.1 - 1.11.rc1
- **Python version**: 3.5 - 3.6
- **Bazel version (if compiling from source)**: 0.17.1
- **GCC/Compiler version (if compiling from source)**: 5
- **CUDA/cuDNN version**: 9.0, 9,2/7.2
- **GPU model and memory**: 
    (0) Nvidia Titan x 980 12gb
    (1) Nvidia Titan x 980 12gb (same)
    (2) Nvidia 1080ti 11gb
    (3) Nvidia 1080ti 11gb (same)
- **Exact command to reproduce**: example source below


### Describe the problem
Four GPUs on the same desktop:

(0) Nvidia Titan x 980 12gb
(1) Nvidia Titan x 980 12gb (same)
(2) Nvidia 1080ti 11gb
(3) Nvidia 1080ti 11gb (same)

MirroredStrategy (say from the obvious example code below) goes ok in combinations of the above (1,2), (1,3) but freezes on (2,3), (1,2,3). 
(0 - not used at distribution as causes another sort of CUDA_WAIT_TIMEOUT error due to guessed kernel timeouts for being used to render display)

Have no idea where to go further. Checked with original binary docker images both latest and nightly. Checked with compiled from source TensorFlow r1.10-r1.11 within original nvidia/cuda binary docker image. All the same behaviour.


### Source code / logs
```
import os
os.environ[""CUDA_DEVICE_ORDER""]=""PCI_BUS_ID""
os.environ['CUDA_VISIBLE_DEVICES']=""2,3""

import tensorflow as tf 

def model_fn(features, labels, mode):
  layer = tf.layers.Dense(1)
  logits = layer(features)

  if mode == tf.estimator.ModeKeys.PREDICT:
    predictions = {""logits"": logits}
    return tf.estimator.EstimatorSpec(mode, predictions=predictions)

  loss = tf.losses.mean_squared_error(
               labels=labels, predictions=tf.reshape(logits, []))

  if mode == tf.estimator.ModeKeys.EVAL:
    return tf.estimator.EstimatorSpec(mode, loss=loss)

  if mode == tf.estimator.ModeKeys.TRAIN:
    train_op = tf.train.GradientDescentOptimizer(0.2).minimize(loss)
    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)

def input_fn():
  features = tf.data.Dataset.from_tensors([[1.]]).repeat(100)
  labels = tf.data.Dataset.from_tensors(1.).repeat(100)
  return tf.data.Dataset.zip((features, labels))

distribution = tf.contrib.distribute.MirroredStrategy()
config = tf.estimator.RunConfig(train_distribute=distribution)
classifier = tf.estimator.Estimator(model_fn=model_fn, config=config)

classifier.train(input_fn=input_fn)

classifier.evaluate(input_fn=input_fn)
```

[freeze_output_r1.10.txt](https://github.com/tensorflow/tensorflow/files/2407611/freeze_output_r1.10.txt)
[freeze_output_r1.11.txt](https://github.com/tensorflow/tensorflow/files/2407612/freeze_output_r1.11.txt)
[normal_completion_output_r1.10.1.txt](https://github.com/tensorflow/tensorflow/files/2407613/normal_completion_output_r1.10.1.txt)
[normal_completion_output_r1.11.txt](https://github.com/tensorflow/tensorflow/files/2407614/normal_completion_output_r1.11.txt)
[tf_env_r1.10.1.txt](https://github.com/tensorflow/tensorflow/files/2407615/tf_env_r1.10.1.txt)
[tf_env_r1.11.txt](https://github.com/tensorflow/tensorflow/files/2407616/tf_env_r1.11.txt)"
22462,Different behaviors when using  relu activation inside conv2d layer and a standalone ReLU() layer,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10.1
- **Python version**: 3,6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0/7.0
- **GPU model and memory**: GTX 1080ti 11gb
- **Exact command to reproduce**: run the code on mnist dataset

```
from tensorflow.python.keras.layers import Conv2D, ReLU, BatchNormalization, Dense, Input, Conv2DTranspose, UpSampling2D, Flatten, Reshape
from tensorflow.python.keras.models import Model
from tensorflow.python.keras.utils import multi_gpu_model
from tensorflow.python.keras.optimizers import Nadam
from tensorflow.python.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
import tensorflow as tf
import numpy as np
from tensorflow.python.keras import backend as K
from tensorflow.python.keras.datasets import mnist


img_height, img_width = 28, 28
(x_train, y_train), (x_test, y_test) = mnist.load_data()
X = np.expand_dims(x_train, axis=-1)
print(X.shape)

input_image = Input(shape=(img_height, img_width, 1), name='image_imput')
x = Conv2D(filters=32, kernel_size=(5, 5), strides=(2, 2), padding='same', name ='encoder_conv1', activation='relu')(input_image)
x = Conv2D(filters=64, kernel_size=(5, 5), strides=(2, 2), padding='same', name ='encoder_conv2', activation='relu')(x)
x = Conv2D(filters=128, kernel_size=(3, 3), strides=(2, 2), padding='valid', name ='encoder_conv3', activation='relu')(x)
x = Flatten()(x)
encoded = Dense(units=10)(x)

y = Dense(units=1152, activation='relu')(encoded)
y = Reshape((3, 3, 128))(y)
y = Conv2DTranspose(filters=64, kernel_size=(3, 3), strides=(2, 2), padding='valid', name ='decoder_deconv1', activation='relu')(y)
y = Conv2DTranspose(filters=32, kernel_size=(5, 5), strides=(2, 2), padding='same', name ='decoder_deconv2', activation='relu')(y)
decoded_image = Conv2DTranspose(filters=1, kernel_size=(5, 5), strides=(2, 2), padding='same', name ='decoder_deconv3', activation='relu')(y)


CAE = Model(inputs = input_image, outputs = decoded_image, name = 'CAE')


tb = TensorBoard(log_dir='logs', write_graph=True)
mc = ModelCheckpoint(filepath='models/top_weights.h5', monitor='acc', save_best_only='True', save_weights_only='True', verbose=1)
es = EarlyStopping(monitor='loss', patience=15, verbose=1)
rlr = ReduceLROnPlateau(monitor='loss')
callbacks = [tb, mc, es, rlr]
CAE.compile(optimizer='adam', loss='mse', metrics=['accuracy'])


# CAE.load_weights('models/top_weights.h5')
# CAE.save('CAE.h5')
CAE.fit(X, X, epochs=1000, batch_size=256, callbacks=callbacks)
 ```

in the above code if i do something like this that is  use a standalone activation layer, the model behaves differently 
```
x = Conv2D(filters=32, kernel_size=(5, 5), strides=(2, 2), padding='same', name ='encoder_conv1')(input_image)
x = ReLU()(x)
```
If i include the activation in the conv2d layer, the model converges at 80% accuracy and when i use a standalone activation layer the model is stuck at 11% accuracy.

I want to know the reason for different behavior "
22460,Feature Request: GRUBlockFusedCell in tf.contrib.rnn,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: /  
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10.0
- **Python version**: 3.6.0
- **Bazel version (if compiling from source)**: /
- **GCC/Compiler version (if compiling from source)**: /
- **CUDA/cuDNN version**: /  
- **GPU model and memory**: /  
- **Exact command to reproduce**: /  

### Describe the problem
Fused RNN cells provide much better performance. Currently only tf.contrib.rnn.LSTMBlockFusedCell is provided. More, tf.contrib.cudnn_rnn.CudnnGRU, which has similar performance with typical FusedCell, is incompatible with normal GRUCell due to differences in computing mechanisms, hence pretrained GRU parameters cannot be used by CudnnGRU. So I wonder if a fused version of GRUCell could be implemented."
22459,[bazel] Unrecoverable error while evaluating node '//tensorflow/cc:ops/io_ops_gen_cc,"I am running bazel  0.17.2 on Windows 7, with the following command:
bazel build tensorflow/examples/wav_to_spectrogram/...

The output:

c:\tensorflow-master>bazel build tensorflow/examples/wav_to_spectrogram/...
Loading:
Loading: 0 packages loaded
DEBUG: C:/users/me/_bazel_me/cxy76ymx/external/bazel_tools/tools/cpp/lib_cc_conf
igure.bzl:115:5:
Auto-Configuration Warning: 'BAZEL_VC' is not set, start looking for the latest
Visual C++ installed.
DEBUG: C:/users/me/_bazel_me/cxy76ymx/external/bazel_tools/tools/cpp/lib_cc_conf
igure.bzl:115:5:
Auto-Configuration Warning: Looking for VS%VERSION%COMNTOOLS environment variabl
es, eg. VS140COMNTOOLS
DEBUG: C:/users/me/_bazel_me/cxy76ymx/external/bazel_tools/tools/cpp/lib_cc_conf
igure.bzl:115:5:
Auto-Configuration Warning: Visual C++ build tools found at C:\Program Files (x8
6)\Microsoft Visual Studio 12.0\VC\
Analyzing: 3 targets (0 packages loaded)
Analyzing: 3 targets (0 packages loaded)
Analyzing: 3 targets (1 packages loaded)
WARNING: C:/tensorflow-master/tensorflow/core/BUILD:2501:1: in includes attribut
e of cc_library rule //tensorflow/core:framework_internal_headers_lib: '../../ex
ternal/com_google_absl' resolves to 'external/com_google_absl' not below the rel
ative path of its package 'tensorflow/core'. This will be an error in the future
. Since this rule was created by the macro 'cc_header_only_library', the error m
ight have been caused by the macro implementation in C:/tensorflow-master/tensor
flow/tensorflow.bzl:1376:20
WARNING: C:/tensorflow-master/tensorflow/core/BUILD:2587:1: in includes attribut
e of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/co
m_google_absl' resolves to 'external/com_google_absl' not below the relative pat
h of its package 'tensorflow/core'. This will be an error in the future. Since t
his rule was created by the macro 'cc_header_only_library', the error might have
 been caused by the macro implementation in C:/tensorflow-master/tensorflow/tens
orflow.bzl:1376:20
Unhandled exception thrown during build; message: Unrecoverable error while eval
uating node '//tensorflow/cc:ops/io_ops_gen_cc BuildConfigurationValue.Key[ac98f
5975123fd6eeb07a7415b6161fb] true' (requested by nodes '//tensorflow/cc:io_ops_g
enrule BuildConfigurationValue.Key[6a75becf8194103d260d4e5a345be2c2] false')
INFO: Elapsed time: 31.631s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (2 packages loaded)
java.lang.RuntimeException: Unrecoverable error while evaluating node '//tensorf
low/cc:ops/io_ops_gen_cc BuildConfigurationValue.Key[ac98f5975123fd6eeb07a7415b6
161fb] true' (requested by nodes '//tensorflow/cc:io_ops_genrule BuildConfigurat
ionValue.Key[6a75becf8194103d260d4e5a345be2c2] false')
        at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate
.run(AbstractParallelEvaluator.java:497)
        at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$Wrapped
Runnable.run(AbstractQueueVisitor.java:368)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown S
ource)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown
Source)
        at java.base/java.lang.Thread.run(Unknown Source)
Caused by: java.lang.IllegalStateException
        at com.google.common.base.Preconditions.checkState(Preconditions.java:49
1)
        at com.google.devtools.build.lib.rules.cpp.LibrariesToLinkCollector.addD
ynamicInputLinkOptions(LibrariesToLinkCollector.java:290)
        at com.google.devtools.build.lib.rules.cpp.LibrariesToLinkCollector.addL
inkerInputs(LibrariesToLinkCollector.java:258)
        at com.google.devtools.build.lib.rules.cpp.LibrariesToLinkCollector.coll
ectLibrariesToLink(LibrariesToLinkCollector.java:203)
        at com.google.devtools.build.lib.rules.cpp.CppLinkActionBuilder.build(Cp
pLinkActionBuilder.java:916)
        at com.google.devtools.build.lib.rules.cpp.CcBinary.init(CcBinary.java:4
13)
        at com.google.devtools.build.lib.rules.cpp.CcBinary.create(CcBinary.java
:179)
        at com.google.devtools.build.lib.rules.cpp.CcBinary.create(CcBinary.java
:71)
        at com.google.devtools.build.lib.analysis.ConfiguredTargetFactory.create
Rule(ConfiguredTargetFactory.java:320)
        at com.google.devtools.build.lib.analysis.ConfiguredTargetFactory.create
ConfiguredTarget(ConfiguredTargetFactory.java:205)
        at com.google.devtools.build.lib.skyframe.SkyframeBuildView.createConfig
uredTarget(SkyframeBuildView.java:631)
        at com.google.devtools.build.lib.skyframe.ConfiguredTargetFunction.creat
eConfiguredTarget(ConfiguredTargetFunction.java:770)
        at com.google.devtools.build.lib.skyframe.ConfiguredTargetFunction.compu
te(ConfiguredTargetFunction.java:320)
        at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate
.run(AbstractParallelEvaluator.java:420)
        ... 4 more
java.lang.RuntimeException: Unrecoverable error while evaluating node '//tensorf
low/cc:ops/io_ops_gen_cc BuildConfigurationValue.Key[ac98f5975123fd6eeb07a7415b6
161fb] true' (requested by nodes '//tensorflow/cc:io_ops_genrule BuildConfigurat
ionValue.Key[6a75becf8194103d260d4e5a345be2c2] false')
        at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate
.run(AbstractParallelEvaluator.java:497)
        at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$Wrapped
Runnable.run(AbstractQueueVisitor.java:368)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown S
ource)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown
Source)
        at java.base/java.lang.Thread.run(Unknown Source)
Caused by: java.lang.IllegalStateException
        at com.google.common.base.Preconditions.checkState(Preconditions.java:49
1)
        at com.google.devtools.build.lib.rules.cpp.LibrariesToLinkCollector.addD
ynamicInputLinkOptions(LibrariesToLinkCollector.java:290)
        at com.google.devtools.build.lib.rules.cpp.LibrariesToLinkCollector.addL
inkerInputs(LibrariesToLinkCollector.java:258)
        at com.google.devtools.build.lib.rules.cpp.LibrariesToLinkCollector.coll
ectLibrariesToLink(LibrariesToLinkCollector.java:203)
        at com.google.devtools.build.lib.rules.cpp.CppLinkActionBuilder.build(Cp
pLinkActionBuilder.java:916)
        at com.google.devtools.build.lib.rules.cpp.CcBinary.init(CcBinary.java:4
13)
        at com.google.devtools.build.lib.rules.cpp.CcBinary.create(CcBinary.java
:179)
        at com.google.devtools.build.lib.rules.cpp.CcBinary.create(CcBinary.java
:71)
        at com.google.devtools.build.lib.analysis.ConfiguredTargetFactory.create
Rule(ConfiguredTargetFactory.java:320)
        at com.google.devtools.build.lib.analysis.ConfiguredTargetFactory.create
ConfiguredTarget(ConfiguredTargetFactory.java:205)
        at com.google.devtools.build.lib.skyframe.SkyframeBuildView.createConfig
uredTarget(SkyframeBuildView.java:631)
        at com.google.devtools.build.lib.skyframe.ConfiguredTargetFunction.creat
eConfiguredTarget(ConfiguredTargetFunction.java:770)
        at com.google.devtools.build.lib.skyframe.ConfiguredTargetFunction.compu
te(ConfiguredTargetFunction.java:320)
        at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate
.run(AbstractParallelEvaluator.java:420)
        ... 4 more
FAILED: Build did NOT complete successfully (2 packages loaded)

Server terminated abruptly (error code: 14, error message: '', log file: 'c:\use
rs\me\_bazel_me\cxy76ymx/server/jvm.out')


c:\tensorflow-master>"
22458,ImportError: _pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow20UnaryDatasetOpKernel11MakeDatasetEPNS_15OpKernelContextEPPNS_11DatasetBaseE,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22455,V1.11.0-rc2 still build fail with VERBS,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 and CentOS 7.5
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: build from source
- **TensorFlow version (use command below)**: Master and v1.11-rc2
- **Python version**: 2.7 and 3.5
- **Bazel version (if compiling from source)**: 0.16
- **GCC/Compiler version (if compiling from source)**: 4.8.5
- **CUDA/cuDNN version**: CUDA 9.1 and 9.2, cuDNN 7.2
- **GPU model and memory**: P100, Titan V
- **Exact command to reproduce**: Enable VERBS in config
Then, bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
In latest master (9/26), used following to compile:
bazel build --config=opt --config=cuda --config=verbs  //tensorflow/tools/pip_package:build_pip_package
Same error message as described blow.

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
Same environment can build 1.10 successfully, but failed on 1.11

It seems this static member functions (static void tensorflow::RdmaMgr::RegMemVisitors()) try to directly access non-static member (RdmaAdapter* rdma_adapter_;)

> ERROR: /home/ai/tensorflow/tensorflow/contrib/verbs/BUILD:105:1: C++ compilation of rule '//tensorflow/contrib/verbs:rdma_mgr' failed (Exit 1)
In file included from tensorflow/contrib/verbs/rdma_mgr.cc:18:0:
./tensorflow/contrib/verbs/rdma_mgr.h: In static member function 'static void tensorflow::RdmaMgr::RegMemVisitors()':
./tensorflow/contrib/verbs/rdma_mgr.h:50:16: error: invalid use of member 'tensorflow::RdmaMgr::rdma_adapter_' in static member function
   RdmaAdapter* rdma_adapter_;
                ^
tensorflow/contrib/verbs/rdma_mgr.cc:282:40: error: from this location
     int32_t bus_id = TryToReadNumaNode(rdma_adapter_->context_->device) + 1;

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem."
22454,Dynamic loading of CUDA libraries,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: N/A
- **TensorFlow version (use command below)**: N/A
- **Python version**: N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem

Currently, the TensorFlow Python library is released in two flavours: CPU-only and CPU/GPU. The latter is dynamically linked with CUDA libraries and fails to load if they are not available in the linker path.

```python
ImportError: dlopen([...]/tensorflow/python/_pywrap_tensorflow_internal.so, 10): Library not loaded: @rpath/libcublas.8.0.dylib
  Referenced from: [...]/tensorflow/python/_pywrap_tensorflow_internal.so
  Reason: image not found
```

I suspect that it might be possible to load CUDA libraries dynamically at runtime. This would allow having a single TensorFlow build which attempts to load CUDA, and if it is not available, falls back to CPU-only ops. 

It hard for me to estimate the feasibility of the proposal for the current TensorFlow runtime implementation, so it might as well be the case, that the proposed change is too big/intrusive to be practical. "
22452,Documentation for tf.train.init_from_checkpoint doesn't say what it does when,"Base on the code, [`tf.train.init_from_checkpoint`](https://www.tensorflow.org/api_docs/python/tf/train/init_from_checkpoint) seems to overwrite the initializers of variables to load from the given checkpoint.  The actual initialization will then occur when `tf.global_variables_initializer()` or similar is executed.

However, the documentation makes no mention of this.  It's written as if the initialization happens *now*.  I suppose it does happen now for eager mode, but for Graph mode the initialization is delayed.

This is particularly confusing in a TPU context where code is executing in all sorts of places, so it took me a while reading through the underlying source code before I understood what was going on."
22450, Intel  GPU Support ,"Support Intel GPU alongside with NVIDIA, not sure if that is possible but it may help achieve better performance
check intel deep leaning kit
https://software.intel.com/en-us/openvino-toolkit/"
22449,[bazel] Update bazel to 0.17.1,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: PR #19461
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.17.1/0.17.2
- **GCC/Compiler version (if compiling from source)**: 5.4.0 20160609
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: build of PR #19461

### Describe the problem

This is a placeholder for updating bazel to 0.17.1 so that PR #19461 could pass CI tests. Before bazel 0.17.1, bazel had some issues with fetching http_archive (See https://github.com/bazelbuild/bazel/issues/5932). The issue has been fixed in 0.17.1.

The update of bazel 0.17.1 requires additional efforts than bumping versions in the repo (See https://github.com/tensorflow/tensorflow/pull/22281#issuecomment-421414201). Pushing bleeding edge bazel immediately caused some issues before, so it is preferred to wait until a full release cycle before making the change.

At the moment, bazel 0.17.2 has been released (See https://github.com/bazelbuild/bazel/issues/6164#issuecomment-423524449) which is a minor release increment to 0.17.1.

/cc @gunan 

Note The bazel 0.18.0 release is not far away (See https://github.com/bazelbuild/bazel/issues/5963), as far as I could see. so wait until 0.18.0 is rebased, then update to 0.17.1 is also reasonable I think."
22448,Issue with gradients computation when using stop_gradient on a map_fn output,"
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOs
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.10.0-rc1-19-g656e7a2b34
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem

When training the model, the error with the stack trace below occurs randomly.
Original exception is thrown from there:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/tensor_array.h#L571

The error disappears if I remove the `stop_gradient`.

### Code

Unfortunately, I haven't managed to replicate the issue with a small piece of code.
Basically, I have a model where a layer forward pass is based on `map_fn`. Inside the function called by `map_fn`, there is a `tf.cond`.
Downstream this layer, there is another layer with a `stop_gradient` on one of the outputs of the previous layer.


### Error stack

```
2018-09-21 20:54:20.894525: W tensorflow/core/framework/op_kernel.cc:1275] OP_REQUIRES failed at tensor_array_ops.cc:527 : Invalid argument: TensorArray scene_model/roi_filter_layer/map/TensorArray_3_8140@gradients: Could not read from TensorArray index 0.  Furthermore, the element shape is not fully defined: <unknown>.  It is possible you are working with a resizeable TensorArray and stop_gradients is not allowing the gradients to be written.  If you set the full element_shape property on the forward TensorArray, the proper all-zeros tensor will be returned instead of incurring this error.
Traceback (most recent call last):
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1278, in _do_call
    return fn(*args)
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1263, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1350, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: TensorArray scene_model/roi_filter_layer/map/TensorArray_3_8140@gradients: Could not read from TensorArray index 0.  Furthermore, the element shape is not fully defined: <unknown>.  It is possible you are working with a resizeable TensorArray and stop_gradients is not allowing the gradients to be written.  If you set the full element_shape property on the forward TensorArray, the proper all-zeros tensor will be returned instead of incurring this error.
	 [[Node: gradients/scene_model/roi_filter_layer/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3 = TensorArrayReadV3[dtype=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](gradients/scene_model/roi_filter_layer/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayGrad/TensorArrayGradV3, gradients/scene_model/roi_filter_layer/map/while/TensorArrayWrite_2/TensorArrayWriteV3_grad/TensorArrayReadV3/StackPopV2, gradients/scene_model/roi_filter_layer/map/while/Merge_2_grad/Switch:1)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/jnd/dev/mask_rcnn/scripts/train_eval_export/new_train.py"", line 55, in <module>
    train(config_path)
  File ""/Users/jnd/dev/mask_rcnn/scripts/train_eval_export/new_train.py"", line 48, in train
    training_schedule=training_program, epoch_offset=0)
  File ""/Users/jnd/dev/mask_rcnn/libs/models/scene/helpers/model_trainer.py"", line 91, in train
    estimator.train(input_fn=input_fn_train, steps=self.config.STEPS_PER_EPOCH)
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 376, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1145, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1173, in _train_model_default
    saving_listeners)
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1451, in _train_with_estimator_spec
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 583, in run
    run_metadata=run_metadata)
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1059, in run
    run_metadata=run_metadata)
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1150, in run
    raise six.reraise(*original_exc_info)
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/six.py"", line 693, in reraise
    raise value
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1135, in run
    return self._sess.run(*args, **kwargs)
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1207, in run
    run_metadata=run_metadata)
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 987, in run
    return self._sess.run(*args, **kwargs)
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 877, in run
    run_metadata_ptr)
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1272, in _do_run
    run_metadata)
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: TensorArray scene_model/roi_filter_layer/map/TensorArray_3_8140@gradients: Could not read from TensorArray index 0.  Furthermore, the element shape is not fully defined: <unknown>.  It is possible you are working with a resizeable TensorArray and stop_gradients is not allowing the gradients to be written.  If you set the full element_shape property on the forward TensorArray, the proper all-zeros tensor will be returned instead of incurring this error.
	 [[Node: gradients/scene_model/roi_filter_layer/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3 = TensorArrayReadV3[dtype=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](gradients/scene_model/roi_filter_layer/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayGrad/TensorArrayGradV3, gradients/scene_model/roi_filter_layer/map/while/TensorArrayWrite_2/TensorArrayWriteV3_grad/TensorArrayReadV3/StackPopV2, gradients/scene_model/roi_filter_layer/map/while/Merge_2_grad/Switch:1)]]

Caused by op 'gradients/scene_model/roi_filter_layer/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3', defined at:
  File ""/Users/jnd/dev/mask_rcnn/scripts/train_eval_export/new_train.py"", line 55, in <module>
    train(config_path)
  File ""/Users/jnd/dev/mask_rcnn/scripts/train_eval_export/new_train.py"", line 48, in train
    training_schedule=training_program, epoch_offset=0)
  File ""/Users/jnd/dev/mask_rcnn/libs/models/scene/helpers/model_trainer.py"", line 91, in train
    estimator.train(input_fn=input_fn_train, steps=self.config.STEPS_PER_EPOCH)
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 376, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1145, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1170, in _train_model_default
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1133, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/Users/jnd/dev/mask_rcnn/libs/models/scene/helpers/estimator.py"", line 84, in model_fn
    grad_vars = optimizer.compute_gradients(loss, var_list=trainable_variables)
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 514, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 596, in gradients
    gate_gradients, aggregation_method, stop_gradients)
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 779, in _GradientsHelper
    lambda: grad_fn(op, *out_grads))
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 398, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 779, in <lambda>
    lambda: grad_fn(op, *out_grads))
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_grad.py"", line 132, in _TensorArrayWriteGrad
    grad = g.read(index)
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py"", line 824, in read
    return self._implementation.read(index, name=name)
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py"", line 258, in read
    name=name)
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py"", line 6515, in tensor_array_read_v3
    dtype=dtype, name=name)
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 454, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3155, in create_op
    op_def=op_def)
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

...which was originally created as op 'scene_model/roi_filter_layer/map/while/TensorArrayWrite/TensorArrayWriteV3', defined at:
  File ""/Users/jnd/dev/mask_rcnn/scripts/train_eval_export/new_train.py"", line 55, in <module>
    train(config_path)
[elided 5 identical lines from previous traceback]
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1133, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/Users/jnd/dev/mask_rcnn/libs/models/scene/helpers/estimator.py"", line 56, in model_fn
    outputs = model(features, training=(mode == tf.estimator.ModeKeys.TRAIN), train_bn=train_bn)
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 736, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/Users/jnd/dev/mask_rcnn/libs/models/scene/model.py"", line 197, in call
    rcnn_outputs = self.object_rcnn_model.call(inputs, **kwargs)
  File ""/Users/jnd/dev/mask_rcnn/libs/models/scene/head/rcnn/object_rcnn.py"", line 160, in call
    filter_outputs = self.roi_filter_layer(filter_inputs, **kwargs)
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 736, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/Users/jnd/dev/mask_rcnn/libs/models/scene/head/rcnn/roi_filter_layer.py"", line 77, in call
    (tf.float32, tf.int32, tf.float32))
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/ops/functional_ops.py"", line 459, in map_fn
    maximum_iterations=n)
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 3232, in while_loop
    return_same_structure)
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2952, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2887, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 3201, in <lambda>
    body = lambda i, lv: (i + 1, orig_body(*lv))
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/ops/functional_ops.py"", line 451, in compute
    tas = [ta.write(i, value) for (ta, value) in zip(tas, flat_fn_values)]
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/ops/functional_ops.py"", line 451, in <listcomp>
    tas = [ta.write(i, value) for (ta, value) in zip(tas, flat_fn_values)]
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py"", line 118, in wrapped
    return _add_should_use_warning(fn(*args, **kwargs))

InvalidArgumentError (see above for traceback): TensorArray scene_model/roi_filter_layer/map/TensorArray_3_8140@gradients: Could not read from TensorArray index 0.  Furthermore, the element shape is not fully defined: <unknown>.  It is possible you are working with a resizeable TensorArray and stop_gradients is not allowing the gradients to be written.  If you set the full element_shape property on the forward TensorArray, the proper all-zeros tensor will be returned instead of incurring this error.
	 [[Node: gradients/scene_model/roi_filter_layer/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3 = TensorArrayReadV3[dtype=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](gradients/scene_model/roi_filter_layer/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayGrad/TensorArrayGradV3, gradients/scene_model/roi_filter_layer/map/while/TensorArrayWrite_2/TensorArrayWriteV3_grad/TensorArrayReadV3/StackPopV2, gradients/scene_model/roi_filter_layer/map/while/Merge_2_grad/Switch:1)]]

```"
22447,tf.keras.utils.multi_gpu_model  just ingore loaded weights from template model,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: no
- **TensorFlow installed from (source or binary)**: binary prebuilt
- **TensorFlow version (use command below)**: 1.9.0
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0/7.2.1
- **GPU model and memory**: Tesla K80 12GB
- **Exact command to reproduce**:

actually, when i call tf.keras.utils.multi_gpu_model on a template model that load_weights,  but result show that the multi_gpu_model just ingore the loaded weights on the template model.
"
22446,tf.data.Dataset run after loading  whole data without using shuffle method,"Does have any method to load sub of data then running model, without using `shuffle`,  I want the data order not changed."
22445,No functions or submodules are installed in 1.11.0rc1 when compiled from source,"### System information
- **OS**: `uname -a`: Linux pnode1.cluster 3.10.0-514.el7.ppc64le #1 SMP Sat Nov 5 15:08:14 GMT 2016 ppc64le ppc64le ppc64le GNU/Linux
- **TensorFlow installed from**: source
- **TensorFlow version**: 1.11.0rc1
- **Python version**: 3.5.6
- **Bazel version**: 0.17.1
- **GCC/Compiler version**: gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11)
- **CUDA/cuDNN version**: 9.1
- **GPU model and memory**: NVIDIA Tesla P100
- **Exact command to reproduce**: `python3.5 -c ""import tensorflow.keras""`

### Reproduction steps
Compile and install Tensorflow from source:
```
git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
./configure
# Leave everything as default except for cuda options which I chose 9.1 and the correct cuda path
bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
cp /tmp/tensorflow_pkg/tensorflow-1.11.0rc1-cp35-cp35m-linux_ppc64le.whl ../
cd ..
pip3.5 install tensorflow-1.11.0rc1-cp35-cp35m-linux_ppc64le.whl
```
Test `tensorflow.keras`:
```
python3.5 -c ""import tensorflow.keras""
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ImportError: No module named 'tensorflow.keras'
```
Test other functions:
```
$ python3.5
>>> x = tf.constant([[-2.25 + 4.75j], [-3.25 + 5.75j]])
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: module 'tensorflow' has no attribute 'constant'
```

### Pip packages
`pip3.5 list`:
```
absl-py                  0.5.0
alabaster                0.7.11
appdirs                  1.4.3
astor                    0.7.1
atomicwrites             1.2.1
attrs                    18.2.0
Babel                    2.6.0
certifi                  2018.8.24
chardet                  3.0.4
Cython                   0.28.5
decorator                4.3.0
docutils                 0.14
gast                     0.2.0
grpcio                   1.15.0
h5py                     2.8.0
idna                     2.7
imagesize                1.1.0
Jinja2                   2.10
Keras                    2.2.2
Keras-Applications       1.0.5
Keras-Preprocessing      1.0.3
Mako                     1.0.7
Markdown                 2.6.11
MarkupSafe               1.0
mock                     2.0.0
more-itertools           4.3.0
nose                     1.3.7
numpy                    1.15.1
packaging                17.1
pathlib2                 2.3.2
pbr                      4.2.0
pip                      18.0
pluggy                   0.7.1
protobuf                 3.6.1
py                       1.6.0
pydot                    1.2.4
pydot-ng                 1.0.0
Pygments                 2.2.0
PyGraph                  0.2.1
pygraphviz               1.5
pyparsing                2.2.1
pytest                   3.8.0
pytools                  2018.5.2
pytz                     2018.5
PyYAML                   3.13
requests                 2.19.1
scipy                    1.1.0
setuptools               28.8.0
six                      1.11.0
snowballstemmer          1.2.1
Sphinx                   1.8.0
sphinxcontrib-websupport 1.1.0
tensorboard              1.10.0
tensorflow               1.11.0rc1
termcolor                1.1.0
urllib3                  1.23
Werkzeug                 0.14.1
wheel                    0.31.1
```"
22444,tflite interpreter get different output for same input,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Debian 4.5.5
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:  v1.10.1
- **Python version**: 3.5.5
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
give data to a tflite model
data shape = (2, 28, 28, 1) and data[0] == data[1]
use tf.contrib.lite.Interpreter to run the model
get the output, but output[0] != output[1]

### Source code / [logs](url)
```
import tensorflow as tf
import numpy as np

def test_same():
    data = np.random.random((1,28,28,1)).astype(np.float32)
    data = np.concatenate([data, data])
    print('input[0] == input[1]: ', equal(data[0], data[1]))
    model = tf.contrib.lite.Interpreter(
        model_path='resnet18_finetuned_batch2.tflite')
    model.allocate_tensors()
    model.set_tensor(0, data)
    model.invoke()
    output = model.get_tensor(model.get_output_details()[0]['index'])
    print('output[0] == output[1]: ', equal(output[0], output[1]))

def equal(tensor1, tensor2):
    for i, j in zip(tensor1.reshape(-1), tensor2.reshape(-1)):
        if abs(i - j) > 0.001:
            return False
    return True

if __name__ == ""__main__"":
    test_same()
```
OUTPUT:
```
input[0] == input[1]:  True
output[0] == output[1]:  False
```"
22443,ValueError: The passed save_path is not a valid checkpoint: modeltest.ckpt,"
I run this code
```
tf.reset_default_graph()
v1 = tf.Variable(tf.constant(0.1, shape = [2]), name=""v1"")
v2 = tf.Variable(tf.constant(0.2, shape = [2]), name=""v2"")
saver = tf.train.Saver()
with tf.Session() as sess:
    saver.restore(sess, ""/tmp/model/model.ckpt"")
```
and then this error is occured:ValueError: The passed save_path is not a valid checkpoint: modeltest.ckpt

this is my env:

tensorflow (1.10.1)

tensorboard (1.10.0)

h5py (2.8.0)

Python 3.6.5

Could you please advice how to solve out this error

Thank you"
22442,get error run simple_estimator_example.py,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Just example from tensorflow.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:macOS High Sierra 10.13.6
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:None
- **TensorFlow installed from (source or binary)**:Ananconda3
- **TensorFlow version (use command below)**:Tensorflow 1.10
- **Python version**: Python 3.65
- **Bazel version (if compiling from source)**:NaN
- **GCC/Compiler version (if compiling from source)**:NaN
- **CUDA/cuDNN version**:NaN
- **GPU model and memory**:NaN
- **Exact command to reproduce**:  config = tf.estimator.RunConfig(train_distribute=distribution,
 eval_distribute=distribution)

### Describe the problem
I just download the official example-""simple_estimator_example.py"" and run it in my own computer. But I just get this error `  config = tf.estimator.RunConfig(train_distribute=distribution,eval_distribute=distribution)`. 
### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22441,"[Request] Please make version-compatible table among them: ""python/tensorflow/tensorflow-gpu/tflearn/cuDNN/CUDA""","Request: Version-Compatible Table

There are a lot of people using Tensorflow, and I'm sure that those people at least have tried to use tflearn, tensorflow-gpu, cuDNN CUDA, etc. And again, I'm very sure we have had the problem of version incompatibility many many many... times. We've done a lot of research to resolve this incompatibility. We tried all kinds of version combinations. And something came to mind:
""Why there wasn't a Version-Compatible table?""

Like: https://www.tensorflow.org/install/source_windows (Tested build configurations)

Example:

Which version tensorflow-gpu (X version) want at least (also MAX.) Python version?
Which version tensorflow-gpu (X version) want at least (also MAX.) cuDNN version?
Which version tensorflow-gpu (X version) want at least (also MAX.) CUDA version?

Which tflearn versions are compatible with which tensorflow (X) versions?
Which tflearn versions are compatible with which tensorflow-gpu (X) versions?

Which CUDA versions are compatible (at least and MAX) with which tensorflow-gpu versions?

etc... It could be an Exel table that clears all of these questions from our minds. Wouldn't that be good? You need to enter individual wiki pages and investigate version compatible each time. (I mean that waste of time)

Thanks in advance...
"
22440,"Import tensorflow ,Kernel died,restarting ","### System information
- **OS Platform and Distribution (Windows 10)**:
- **TensorFlow installed from :pip3
- **TensorFlow version :1.5
- **Python version: 2.7.14
- **GPU model and memory: Nvidia GT540M 1GB

### Describe the problem
On importing tensorflow i get this error. Downgraded to 1.5 still the same error as on the lastest version.
1) Spyder and Jupyter both result in the same error
"
22439,"Import tensorflow Kernel died,restarting ","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
On imp

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22438,InvalidArgumentError when SparseTensorValue is not ordered by row then col,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS 10.13.6
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: no
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.10.0-rc1-19-g656e7a2b34 1.10.0
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: no
- **GCC/Compiler version (if compiling from source)**: no
- **CUDA/cuDNN version**: no
- **GPU model and memory**: no
- **Exact command to reproduce**: `python bug.py`

### Describe the problem

If we feed a `SparseTensorValue` to `tf.data.Dataset.from_tensor_slices` such that its indices are not lexicographically sorted by row then col, then we will get a `InvalidArgumentError`.

Maybe it could be said in the docs, or the error should provide a clearer message. It was hard to guess that `indices[2] = [1,0] is out of order` meant the indices were not provided in lexicographic order.

I finally saw that it was explained in the [`SparseTensor` docs](https://www.tensorflow.org/api_docs/python/tf/SparseTensor), but I feel it should be said in the [`SparseTensorValue` docs](https://www.tensorflow.org/api_docs/python/tf/SparseTensorValue) as well.

### Source code / logs

```python
from scipy.sparse import csr_matrix
import tensorflow as tf
import numpy as np


M = np.array([[0, 1, 1], [1, 0, 0], [1, 0, 2], [0, 1, 1]])

# First observation: these two slicing operations provide different orderings
S = csr_matrix(M)[1:3].tocoo()
print('1:3', S.row, S.col)
S = csr_matrix(M)[[1, 2]].tocoo()
print('1,2', S.row, S.col)

entries = np.column_stack((S.row, S.col, S.data))
ordering = np.arange(len(S.data))

# Uncomment the following line to fix the error
# ordering = np.lexsort((S.col, S.row))  # Sort by row then col

X_train = tf.SparseTensorValue(indices=entries[ordering, :2],
                               values=entries[ordering, 2],
                               dense_shape=S.shape)

dataset = tf.data.Dataset.from_tensor_slices(X_train)
iterator = dataset.make_initializable_iterator()
X_sample = iterator.get_next()

with tf.Session() as sess:
    sess.run(iterator.initializer)
    print(sess.run(X_sample))
```

    Traceback (most recent call last):
      File ""/Users/jilljenn/code/vae/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1278, in _do_call
        return fn(*args)
      File ""/Users/jilljenn/code/vae/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1263, in _run_fn
        options, feed_dict, fetch_list, target_list, run_metadata)
      File ""/Users/jilljenn/code/vae/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1350, in _call_tf_sessionrun
        run_metadata)
    tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[2] = [1,0] is out of order
         [[Node: SerializeManySparse = SerializeManySparse[T=DT_INT64, out_type=DT_VARIANT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](tensors/SparseTensor/indices, tensors/SparseTensor/values, tensors/SparseTensor/dense_shape)]]

    During handling of the above exception, another exception occurred:

    Traceback (most recent call last):
      File ""bug.py"", line 27, in <module>
        sess.run(iterator.initializer)
      File ""/Users/jilljenn/code/vae/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 877, in run
        run_metadata_ptr)
      File ""/Users/jilljenn/code/vae/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1100, in _run
        feed_dict_tensor, options, run_metadata)
      File ""/Users/jilljenn/code/vae/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1272, in _do_run
        run_metadata)
      File ""/Users/jilljenn/code/vae/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1291, in _do_call
        raise type(e)(node_def, op, message)
    tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[2] = [1,0] is out of order
         [[Node: SerializeManySparse = SerializeManySparse[T=DT_INT64, out_type=DT_VARIANT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](tensors/SparseTensor/indices, tensors/SparseTensor/values, tensors/SparseTensor/dense_shape)]]

    Caused by op 'SerializeManySparse', defined at:
      File ""bug.py"", line 22, in <module>
        dataset = tf.data.Dataset.from_tensor_slices(X_train)
      File ""/Users/jilljenn/code/vae/venv/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 254, in from_tensor_slices
        return TensorSliceDataset(tensors)
      File ""/Users/jilljenn/code/vae/venv/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1173, in __init__
        self._tensors = sparse.serialize_many_sparse_tensors(tensors)
      File ""/Users/jilljenn/code/vae/venv/lib/python3.6/site-packages/tensorflow/python/data/util/sparse.py"", line 132, in serialize_many_sparse_tensors
        for tensor in nest.flatten(tensors)
      File ""/Users/jilljenn/code/vae/venv/lib/python3.6/site-packages/tensorflow/python/data/util/sparse.py"", line 132, in <listcomp>
        for tensor in nest.flatten(tensors)
      File ""/Users/jilljenn/code/vae/venv/lib/python3.6/site-packages/tensorflow/python/ops/sparse_ops.py"", line 1469, in serialize_many_sparse
        out_type=out_type)
      File ""/Users/jilljenn/code/vae/venv/lib/python3.6/site-packages/tensorflow/python/ops/gen_sparse_ops.py"", line 502, in serialize_many_sparse
        out_type=out_type, name=name)
      File ""/Users/jilljenn/code/vae/venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
        op_def=op_def)
      File ""/Users/jilljenn/code/vae/venv/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 454, in new_func
        return func(*args, **kwargs)
      File ""/Users/jilljenn/code/vae/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3155, in create_op
        op_def=op_def)
      File ""/Users/jilljenn/code/vae/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1717, in __init__
        self._traceback = tf_stack.extract_stack()

    InvalidArgumentError (see above for traceback): indices[2] = [1,0] is out of order
         [[Node: SerializeManySparse = SerializeManySparse[T=DT_INT64, out_type=DT_VARIANT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](tensors/SparseTensor/indices, tensors/SparseTensor/values, tensors/SparseTensor/dense_shape)]]"
22437,tensorflow-1.11.0-rc1 fails to build,"
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04.5 LTS

- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: 
No

- **TensorFlow installed from (source or binary)**:
source

- **TensorFlow version (use command below)**:
1.11.0-rc1

- **Python version**:
3.5

- **Bazel version (if compiling from source)**:
0.17.1

- **GCC/Compiler version (if compiling from source)**:cc 
5.4.0

- **CUDA/cuDNN version**:
CUDA10.0 
cuDNN 7.2

- **GPU model and memory**:
GTX1070

Tried to build tensorflow-1.11.0-rc1 but it failed with

> ERROR: /home/bernard/opt/cuda_test/cuda10/tensorflow-1.11.0-rc1/tensorflow/contrib/verbs/BUILD:90:1: C++ compilation of rule '//tensorflow/contrib/verbs:rdma_rendezvous_mgr' failed (Exit 1)

To be sure that it is not  because cuda is too new I tried also build tensorflow-1.10.1 and it built without problem

I tried googling and dug up this old bug https://github.com/tensorflow/tensorflow/issues/9752"
22436,freeze.py OP_REQUIRES failed at save_restore_tensor.cc,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 7 
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.5
- **Python version**:
3.6
- **Bazel version (if compiling from source)**:
NA
- **GCC/Compiler version (if compiling from source)**:
NA
- **CUDA/cuDNN version**:
NA
- **GPU model and memory**:
NA
- **Exact command to reproduce**:
(tensorflow) C:\tensorflow-master> python tensorflow/examples/speech_commands/freeze.py \
--start_checkpoint=C:/tmp/speech_commands_train/conv.ckpt-1400 \
--output_file=c:/tmp/my_frozen_graph.pb

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I ran the command from https://www.tensorflow.org/tutorials/sequences/audio_recognition and got the following error

### Source code / logs
(tensorflow) C:\tensorflow-master>python tensorflow/examples/speech_commands/fre
eze.py \
2018-09-20 17:13:42.678960: W tensorflow/core/framework/op_kernel.cc:1318] OP_RE
QUIRES failed at save_restore_tensor.cc:170 : Not found: Unsuccessful TensorSlic
eReader constructor: Failed to find any matching files for
Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\py
thon\client\session.py"", line 1322, in _do_call
    return fn(*args)
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\py
thon\client\session.py"", line 1307, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\py
thon\client\session.py"", line 1409, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.NotFoundError: Unsuccessful TensorSliceR
eader constructor: Failed to find any matching files for
         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT
, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device
:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_
and_slices)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""tensorflow/examples/speech_commands/freeze.py"", line 208, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\py
thon\platform\app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""tensorflow/examples/speech_commands/freeze.py"", line 134, in main
    models.load_variables_from_checkpoint(sess, FLAGS.start_checkpoint)
  File ""C:\tensorflow-master\tensorflow\examples\speech_commands\models.py"", lin
e 155, in load_variables_from_checkpoint
    saver.restore(sess, start_checkpoint)
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\py
thon\training\saver.py"", line 1802, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\py
thon\client\session.py"", line 900, in run
    run_metadata_ptr)
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\py
thon\client\session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\py
thon\client\session.py"", line 1316, in _do_run
    run_metadata)
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\py
thon\client\session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: Unsuccessful TensorSliceR
eader constructor: Failed to find any matching files for
         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT
, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device
:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_
and_slices)]]

Caused by op 'save/RestoreV2', defined at:
  File ""tensorflow/examples/speech_commands/freeze.py"", line 208, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\py
thon\platform\app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""tensorflow/examples/speech_commands/freeze.py"", line 134, in main
    models.load_variables_from_checkpoint(sess, FLAGS.start_checkpoint)
  File ""C:\tensorflow-master\tensorflow\examples\speech_commands\models.py"", lin
e 154, in load_variables_from_checkpoint
    saver = tf.train.Saver(tf.global_variables())
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\py
thon\training\saver.py"", line 1338, in __init__
    self.build()
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\py
thon\training\saver.py"", line 1347, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\py
thon\training\saver.py"", line 1384, in _build
    build_save=build_save, build_restore=build_restore)
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\py
thon\training\saver.py"", line 835, in _build_internal
    restore_sequentially, reshape)
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\py
thon\training\saver.py"", line 472, in _AddRestoreOps
    restore_sequentially)
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\py
thon\training\saver.py"", line 886, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\py
thon\ops\gen_io_ops.py"", line 1462, in restore_v2
    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\py
thon\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\py
thon\framework\ops.py"", line 3392, in create_op
    op_def=op_def)
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\py
thon\framework\ops.py"", line 1718, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-
access

NotFoundError (see above for traceback): Unsuccessful TensorSliceReader construc
tor: Failed to find any matching files for
         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT
, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device
:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_
and_slices)]]


(tensorflow) C:\tensorflow-master>--start_checkpoint=C:/tmp/speech_commands_trai
n/conv.ckpt-1400 \
'--start_checkpoint' is not recognized as an internal or external command,
operable program or batch file.

(tensorflow) C:\tensorflow-master>--output_file=c:/tmp/my_frozen_graph.pb
'--output_file' is not recognized as an internal or external command,
operable program or batch file.

(tensorflow) C:\tensorflow-master>1a

(tensorflow) C:\tensorflow-master>python tensorflow/examples/speech_commands/fre
eze.py \
2018-09-20 17:28:18.766069: W tensorflow/core/framework/op_kernel.cc:1318] OP_RE
QUIRES failed at save_restore_tensor.cc:170 : Not found: Unsuccessful TensorSlic
eReader constructor: Failed to find any matching files for
Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\py
thon\client\session.py"", line 1322, in _do_call
    return fn(*args)
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\py
thon\client\session.py"", line 1307, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\py
thon\client\session.py"", line 1409, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.NotFoundError: Unsuccessful TensorSliceR
eader constructor: Failed to find any matching files for
         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT
, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device
:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_
and_slices)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""tensorflow/examples/speech_commands/freeze.py"", line 208, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\py
thon\platform\app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""tensorflow/examples/speech_commands/freeze.py"", line 134, in main
    models.load_variables_from_checkpoint(sess, FLAGS.start_checkpoint)
  File ""C:\tensorflow-master\tensorflow\examples\speech_commands\models.py"", lin
e 155, in load_variables_from_checkpoint
    saver.restore(sess, start_checkpoint)
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\py
thon\training\saver.py"", line 1802, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\py
thon\client\session.py"", line 900, in run
    run_metadata_ptr)
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\py
thon\client\session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\py
thon\client\session.py"", line 1316, in _do_run
    run_metadata)
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\py
thon\client\session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: Unsuccessful TensorSliceR
eader constructor: Failed to find any matching files for
         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT
, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device
:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_
and_slices)]]

Caused by op 'save/RestoreV2', defined at:
  File ""tensorflow/examples/speech_commands/freeze.py"", line 208, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\py
thon\platform\app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""tensorflow/examples/speech_commands/freeze.py"", line 134, in main
    models.load_variables_from_checkpoint(sess, FLAGS.start_checkpoint)
  File ""C:\tensorflow-master\tensorflow\examples\speech_commands\models.py"", lin
e 154, in load_variables_from_checkpoint
    saver = tf.train.Saver(tf.global_variables())
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\py
thon\training\saver.py"", line 1338, in __init__
    self.build()
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\py
thon\training\saver.py"", line 1347, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\py
thon\training\saver.py"", line 1384, in _build
    build_save=build_save, build_restore=build_restore)
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\py
thon\training\saver.py"", line 835, in _build_internal
    restore_sequentially, reshape)
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\py
thon\training\saver.py"", line 472, in _AddRestoreOps
    restore_sequentially)
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\py
thon\training\saver.py"", line 886, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\py
thon\ops\gen_io_ops.py"", line 1462, in restore_v2
    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\py
thon\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\py
thon\framework\ops.py"", line 3392, in create_op
    op_def=op_def)
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\py
thon\framework\ops.py"", line 1718, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-
access

NotFoundError (see above for traceback): Unsuccessful TensorSliceReader construc
tor: Failed to find any matching files for
         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT
, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device
:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_
and_slices)]]


(tensorflow) C:\tensorflow-master>--start_checkpoint=C:\\tmp\\speech_commands_tr
ain\\conv.ckpt-1400 \
'--start_checkpoint' is not recognized as an internal or external command,
operable program or batch file.

(tensorflow) C:\tensorflow-master>--output_file=c:\\tmp\\my_frozen_graph.pb
'--output_file' is not recognized as an internal or external command,
operable program or batch file.

(tensorflow) C:\tensorflow-master>"
22434,"kernel dying, after import.","Python 3.6.6 (default, Jul 19 2018, 14:25:17) 
[GCC 8.1.1 20180712 (Red Hat 8.1.1-5)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Illegal instruction (core dumped)
"
22433,train_op given in estimator spec raising error while given with distribution strategy,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.9
- **Python version**: 3.5.2
- **CUDA/cuDNN version**: CuDA 9 and CuDNN 7
- **GPU model and memory**: K1100M Quadro, 2 GB

### Describe the problem
While training simple mnist model with estimator, if the loss function is given to the optimiser for minimisation, the following error is reproduced.

### Source code / logs
File ""D:/Userdata/Software/FullModelSeparate/train.py"", line 62, in <module>
    tf.estimator.train_and_evaluate(mnist_est,train_spec=train_spec,eval_spec=eval_spec)
  File ""C:\Program Files\Python 3.5\lib\site-packages\tensorflow\python\estimator\training.py"", line 447, in train_and_evaluate
    return executor.run()
  File ""C:\Program Files\Python 3.5\lib\site-packages\tensorflow\python\estimator\training.py"", line 531, in run
    return self.run_local()
  File ""C:\Program Files\Python 3.5\lib\site-packages\tensorflow\python\estimator\training.py"", line 669, in run_local
    hooks=train_hooks)
  File ""C:\Program Files\Python 3.5\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 366, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""C:\Program Files\Python 3.5\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 1117, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File ""C:\Program Files\Python 3.5\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 1160, in _train_model_distributed
    self.config)
  File ""C:\Program Files\Python 3.5\lib\site-packages\tensorflow\python\training\distribute.py"", line 794, in call_for_each_tower
    return self._call_for_each_tower(fn, *args, **kwargs)
  File ""C:\Program Files\Python 3.5\lib\site-packages\tensorflow\contrib\distribute\python\one_device_strategy.py"", line 77, in _call_for_each_tower
    return fn(*args, **kwargs)
  File ""C:\Program Files\Python 3.5\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 1107, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""D:\Userdata\Software\FullModelSeparate\ModelToEstimator.py"", line 110, in convert
    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=apply_gradient_op)
  File ""C:\Program Files\Python 3.5\lib\site-packages\tensorflow\python\estimator\model_fn.py"", line 184, in __new__
    _check_is_tensor_or_operation(train_op, 'train_op')
  File ""C:\Program Files\Python 3.5\lib\site-packages\tensorflow\python\estimator\model_fn.py"", line 390, in _check_is_tensor_or_operation
    raise TypeError('{} must be Operation or Tensor, given: {}'.format(name, x))
TypeError: train_op must be Operation or Tensor, given: <tf.Variable 'global_step:0' shape=() dtype=int64>
"
22432,tensorflow.python.framework.errors_impl.NotFoundError: /home/selena/pcn-master/pc_distance/tf_nndistance_so.so: undefined symbol: _ZN10tensorflow15OpKernelContext10CtxFailureEPKciRKNS_6StatusE,"Error Message:
Traceback (most recent call last):
  File ""demo.py"", line 29, in <module>
    model_module = importlib.import_module('.%s' % args.model_type, 'models')
  File ""/home/selena/miniconda2/envs/tensorflow/lib/python3.5/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 985, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 968, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 957, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 673, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 697, in exec_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
  File ""/home/selena/pcn-master/models/pcn_cd.py"", line 2, in <module>
    from tf_util import mlp, mlp_conv, chamfer, add_train_summary, add_valid_summary
  File ""/home/selena/pcn-master/tf_util.py"", line 2, in <module>
    from pc_distance import tf_nndistance, tf_approxmatch
  File ""/home/selena/pcn-master/pc_distance/tf_nndistance.py"", line 5, in <module>
    nn_distance_module=tf.load_op_library(os.path.join(BASE_DIR, 'tf_nndistance_so.so'))
  File ""/home/selena/.local/lib/python3.5/site-packages/tensorflow/python/framework/load_library.py"", line 56, in load_op_library
    lib_handle = py_tf.TF_LoadLibrary(library_filename, status)
  File ""/home/selena/.local/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: /home/selena/pcn-master/pc_distance/tf_nndistance_so.so: undefined symbol: _ZN10tensorflow15OpKernelContext10CtxFailureEPKciRKNS_6StatusE


MakeFile:
 cuda_inc = /usr/local/cuda-9.0/include/
cuda_lib = /usr/local/cuda-9.0/lib64/
nsync = /home/selena/miniconda2/envs/tensorflow/lib/python3.5/site-packages/external/nsync/public
nvcc = /usr/local/cuda-9.0/bin/nvcc
tf_inc = /home/selena/miniconda2/envs/tensorflow/lib/python3.5/site-packages/tensorflow/include
tf_lib = /home/selena/miniconda2/envs/tensorflow/lib/python3.5/site-packages/tensorflow

all: tf_nndistance_so.so tf_approxmatch_so.so

tf_nndistance.cu.o: tf_nndistance.cu
	$(nvcc) tf_nndistance.cu -o tf_nndistance.cu.o -c -O2 -DGOOGLE_CUDA=1 -x cu -Xcompiler -fPIC

tf_nndistance_so.so: tf_nndistance.cpp tf_nndistance.cu.o
	g++ tf_nndistance.cpp tf_nndistance.cu.o -o tf_nndistance_so.so \
	-I $(cuda_inc) -I $(tf_inc) -I $(nsync) \
	-L $(cuda_lib) -lcudart -L $(tf_lib) -ltensorflow_framework \
    -shared -std=c++11 -fPIC -O2

tf_approxmatch.cu.o: tf_approxmatch.cu
	$(nvcc) tf_approxmatch.cu -o tf_approxmatch.cu.o -c -O2 -DGOOGLE_CUDA=1 -x cu -Xcompiler -fPIC

tf_approxmatch_so.so: tf_approxmatch.cpp tf_approxmatch.cu.o
	g++ -shared $(CPPFLAGS) tf_approxmatch.cpp tf_approxmatch.cu.o -o tf_approxmatch_so.so \
	-I $(cuda_inc) -I $(tf_inc) -I $(nsync) \
	-L $(cuda_lib) -lcudart -L $(tf_lib) -ltensorflow_framework \
    -shared -std=c++11 -fPIC -O2

clean:
	rm -rf *.o *.so


Cuda:9.0
TensorFlow: 1.5.0"
22431,pip official package compiled differently with package in docker image?,"I am working on filesystem plugin, a .so file, which works fine with official tensorflow pip package, but with tensorflow in tensorflow/tensorflow:1.7.0, i got error  `_ZN10tensorflow10FileSystem10FilesExistERKSt6vectorISsSaISsEEPS1_INS_6StatusESaIS6_EE`


Seems tensorflow from pypi package is different from package in docker image?
I checked docker hub image history `tensorflow/tensorflow:1.7.0` with docker history
```
/bin/sh -c pip --no-cache-dir install /tensorflow-1.7.0-cp27-cp27mu-manylinux1_x86_64.whl
``` 
seems tensorflow is install from local wal, and tensorflow size is `16533200` and symbol i want to find missing

```
-rwxr-xr-x  1 root staff 16533200 Mar 29 17:04 libtensorflow_framework.so*

nm libtensorflow_framework.so|grep _ZN10tensorflow10FileSystem10FilesExistERKSt6vectorISsSaISsEEPS1_INS_6StatusESaIS6_EE
```
But if i uninstall tensorflow an reinstall in again
```
pip uninstall -y tensorflow
pip install tensorflow==1.7.0
```

i get different tensorflow library with size `16337456`, and symbol i want to find  appears
````
16337456 Sep 20 02:45 libtensorflow_framework.so*

 nm libtensorflow_framework.so|grep _ZN10tensorflow10FileSystem10FilesExistERKSt6vectorISsSaISsEEPS1_INS_6StatusESaIS6_EE
0000000000553240 T _ZN10tensorflow10FileSystem10FilesExistERKSt6vectorISsSaISsEEPS1_INS_6StatusESaIS6_EE
```

So why and how to solve this? How can i compile a filesystem plugin compatible to tensorflow/tensorflow:1.7.0
"
22430,TensorFlow C++ API on ARM processor issue,"System information
***Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
   No
***OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
   Linux Kernel 4.4.103, Linaro 5.4.0
***TensorFlow installed from (source or binary):
   Cross compilation in Ubuntu 16.04 and install the C++ library (libtensorflow.so + protobuf + nsync) in 
   the ARM development board
***TensorFlow version (use command below):
   1.10.1
***Python version:
  3.5
***CUDA/cuDNN version:
  Only CPU no GPU
***Exact command to reproduce:
  Session->Run(feed_dict, {""num_detections:0"", ""detection_boxes:0"", 
              ""detection_scores:0"", ""detection_classes:0""}, {}, &outputs)

Describe the problem
   Trained Detection API mobileNetSSD model and saved as a 'pb' file.  Cross compiled the TensorFlow on the PC and installed the binary  (libtensorflow.so + protobuf + nsync) to the ARM board.  The code can be successfully built on the ARM side.  The pb file can be successfully read and loaded into a graph in a session.  However, when trying to run the session by feeding a tensor, the following error message came up:

2018-09-20 18:28:38.852657: F tensorflow/core/framework/tensor.cc:657] Check failed: in_size != 0 (0 vs. 0)

It refers to Tensor::UnsafeCopyFromInternal function Line 3:  CHECK_NE(in_size, 0), and in_size =DataTypeSize(other.dtype()), which is unlikely to be zero.  

BTW, these message also came up when loading the 'pb' file:
2018-09-20 18:28:37.864057: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: ""DecodeProtoV2"" device_type: ""CPU""') for unknown op: DecodeProtoV2
2018-09-20 18:28:37.865515: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: ""EncodeProto"" device_type: ""CPU""') for unknown op: EncodeProto"
22427,tf.image.resize_image running on CPU when I use tf.float16,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
     yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
    OpenSUSE 12.1
- **TensorFlow installed from (source or binary)**:
    Source
- **TensorFlow version (use command below)**:
    1.1 
- **Python version**:
    3.5
- **CUDA/cuDNN version**:
    8.0
- **GPU model and memory**:
    Tesla M40, 11GB

- **Exact command to reproduce**:
    tf.image.resize_image(input with float16, shape)

### Describe the problem
Tried to run inference using float16 but I saw a significant inference slow down comparing with float32. I did some further study and found it's running on CPU, which blocks the speed up of using float16.  
"
22426,TensorFlow installs open source Keras,"Installing TensorFlow (1.11.rc1) installs open source Keras. I don't think that's intentional, and things might get confusing.

In my bazel build system, that's a problem as there's a circular dependency as my keras depends on tensorflow, and tensorflow now depends on keras. (although it's true that open source keras does not explicitly list TF as a dependency)

This happens because of the new dependncy on keras-applications and keras-preprocessing that both depend on `keras` (open source version).

pip dependency tree of `tensorflow-gpu==1.11.0rc1`:

```
tensorflow-gpu==1.11.0rc1
  (...)
  - keras-applications [required: >=1.0.5, installed: 1.0.5]
    (...)
    - keras [required: >=2.1.6, installed: 2.2.0]
  (...)
  - keras-preprocessing [required: >=1.0.3, installed: 1.0.3]
    - keras [required: >=2.1.6, installed: 2.2.0]
  (...)
(...)
```"
22425,Inference slowdown of 3x with TF-TensorRT integration with C API ,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: r1.11
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.16.1
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**:9/7.1
- **GPU model and memory**: 1080ti/11gb
- **Exact command to reproduce**:
1. Clone tensorflow repo, checkout r1.11 branch
2. Build from source as directed from documentation by disabling everything except cuda, tensorrt(4.0.1.6)
3. Create a tensorrt .pb file using following:
```
    trt_graph = trt.create_inference_graph(
    input_graph_def=tf.get_default_graph().as_graph_def(),
    outputs=output_node,
    max_batch_size=1,
    max_workspace_size_bytes=1 << 25,
    precision_mode=""FP32"",  # TRT Engine precision ""FP32"",""FP16"" or ""INT8""
    minimum_segment_size=2  # minimum number of nodes in an engine
    )
    f = open(""trt.pb"", 'w')
    f.write(trt_graph.SerializeToString())
    f.close()

```
4. Use Tensorflow C API to run infernce on the protobuf file

Issue: I see a slowdown of 3x for a model with C API, but this is not reproducible with Python API with the same model.

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

![trt_1](https://user-images.githubusercontent.com/4759327/45907556-e4e35980-bde7-11e8-8d78-bcf84fd0b366.png)
![trt_2](https://user-images.githubusercontent.com/4759327/45907558-e876e080-bde7-11e8-9e8d-863f6b3918d9.png)
"
22424,Native compilation of Tensorflow on ARMv8 platforms fail,"------------------------

### System information
== cat /etc/issue ===============================================
Linux ad96b38fdef35570b0c421cb8b68608f.lsdk.generic.ls1046ardb.nxp 4.14.47 #1 SMP PREEMPT Sat Jun 23 08:17:00 CST 2018 aarch64 aarch64 aarch64 GNU/Linux
VERSION=""18.04 LTS (Bionic Beaver)""                                                                                                                     
VERSION_ID=""18.04""                                                                                                                                      
VERSION_CODENAME=bionic                                                                                                                                 
== are we in docker =============================================
No                                                               

== compiler =====================================================
c++ (Ubuntu/Linaro 7.3.0-16ubuntu3) 7.3.0                        
Copyright (C) 2017 Free Software Foundation, Inc.                
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux ad96b38fdef35570b0c421cb8b68608f.lsdk.generic.ls1046ardb.nxp 4.14.47 #1 SMP PREEMPT Sat Jun 23 08:17:00 CST 2018 aarch64 aarch64 aarch64 GNU/Linux

== check pips ===================================================
numpy (1.13.3)
protobuf (3.6.0)
tensorflow (1.10.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.10.0
tf.GIT_VERSION = v1.10.0-0-g656e7a2b34
tf.COMPILER_VERSION = v1.10.0-0-g656e7a2b34
Sanity check: array([1], dtype=int32)
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""tensorflow/python/pywrap_tensorflow.py"", line 25, in <module>
    from tensorflow.python.platform import self_check
ImportError: No module named platform

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tools/tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================

### Describe the problem
Native compilation on ARMv8 platform fails, due to tensorflow-lite 
Patch is copied below. Please incorporate them in the next release. thanks.
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+ Patch file is copied below
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++
diff --git a/tensorflow/contrib/lite/build_def.bzl b/tensorflow/contrib/lite/build_def.bzl
index b735d08b4b..4f333f079d 100644                                                       
--- a/tensorflow/contrib/lite/build_def.bzl                                               
+++ b/tensorflow/contrib/lite/build_def.bzl                                               
@@ -14,8 +14,8 @@ def tflite_copts():                                                     
               ""-O3"",                                                                     
           ],                                                                             
           str(Label(""//tensorflow:android_arm"")): [
-              ""-mfpu=neon"",
-              ""-mfloat-abi=softfp"",
+              #""-mfpu=neon"",
+              #""-mfloat-abi=softfp"",
               ""-std=c++11"",
               ""-O3"",
           ],
diff --git a/tensorflow/contrib/lite/kernels/internal/BUILD b/tensorflow/contrib/lite/kernels/internal/BUILD
index 3a855fe3dd..c013313512 100644
--- a/tensorflow/contrib/lite/kernels/internal/BUILD
+++ b/tensorflow/contrib/lite/kernels/internal/BUILD
@@ -21,7 +21,7 @@ HARD_FP_FLAGS_IF_APPLICABLE = select({
 NEON_FLAGS_IF_APPLICABLE = select({
     "":arm"": [
         ""-O3"",
-        ""-mfpu=neon"",
+        #""-mfpu=neon"",
     ],
     "":armeabi-v7a"": [
         ""-O3"",
diff --git a/third_party/png.BUILD b/third_party/png.BUILD
index 17c5449cc0..7b095cbc36 100644
--- a/third_party/png.BUILD
+++ b/third_party/png.BUILD
@@ -42,6 +42,7 @@ cc_library(
     ],
     includes = ["".""],
     linkopts = [""-lm""],
+    copts = [""-DPNG_ARM_NEON_OPT=0""],
     visibility = [""//visibility:public""],
     deps = [""@zlib_archive//:zlib""],
 )


"
22422,tensorflow issue on python3.5 running at raspbian 9 stretch,"tensorflow library was downloaded from python wheels to raspberry pi 3 running raspbian 9. Thonny shell log reports a runtime warning while executing ""import tensorflow"" instruction: compile version 3.4 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5"
22417,tf.estimator.train_and_evaluate() return None when trained with only one master node,"### System information
- **Hosting Environment**: GCP ML-Engine
- **TensorFlow version (use command below)**: r1.10
- **Python version**: 3.5
- **GPU model and memory**: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
- **Exact command to reproduce**: eval_metrics = tf.estimator.train_and_evaluate(my_estimator, my_train_spec, my_eval_spec

### Describe the problem
According to document of Tensorflow r1.10, the function tf.estimator.train_and_evaluate() should return a tuple of the result of the evaluate call to the Estimator and the export results using the specified ExportStrategy. 

This function returns the correct tuple if trained locally; when using this function on ml-engine (i.e. submit as a job run on Cloud ML Engine runtime version 1.10 using only one node in the cluster), it will return **None**. The corresponding TF_CONFIG environment variable is as follows: 
```
{'cluster': {'master': ['127.0.0.1:2222']}, 'job': {'scale_tier': 'BASIC_GPU', 'region': 'europe-west1', 'python_version': '3.5', 'python_module': 'task.train_eval', 'job_dir': 'gs://my_bucket_name/my_job_folder', 'runtime_version': '1.10', 'run_on_raw_vm': True, 'package_uris': ['gs://my_bucket_name/kYspuihk/packages/my_package.tar.gz']}, 'environment': 'cloud', 'task': {'index': 0, 'cloud': 'bdb8f39da23bcb702-ml', 'type': 'master'}}
```

### Source code / logs
I looked a bit into the training logic, and [this line](https://github.com/tensorflow/tensorflow/blob/ca94990804cf5326c0f6f46d75c96e0f0e240366/tensorflow/python/estimator/training.py#L674) seems suspicious, since it may not return any evaluation metrics. I might also be wrong in spotting the location because I did not too deep into it; and I made it to do an estimator.evaluate() again instead.

"
22414,CUDNN_STATUS_ALLOC_FAILED,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no, it's on examples/tutorial/mnist,  mnist_deep.py
- **Bazel version**: N/A
- **Mobile device**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: windows 10 x64 latest build
- **TensorFlow installed from (source or binary)**: binary prebuilt 
- **TensorFlow version (use command below)**: 1.11.0rc1
- **Python version**: 3.6.6
- **CUDA/cuDNN version**: 9.0/7.2.1
- **GPU model and memory**: GTX1050Ti GDDR5 4GB
- **Exact command to reproduce**: 
1. python mnist_deep.py
E tensorflow/stream_executor/cuda/cuda_dnn.cc:353] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED

2. other codes are fine-working as before

3. no problem with my laptop, which has geforce mx150 GDDR5 2GB


all this happens after I installed geforce driver 411.63

### Source code / logs
PS D:\VSWorkspace\tensorflow\tensorflow\examples\tutorials\mnist> python .\mnist_deep.py
WARNING:tensorflow:From .\mnist_deep.py:130: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
WARNING:tensorflow:From C:\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future versi
on.
Instructions for updating:
Please write your own downloading logic.
WARNING:tensorflow:From C:\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future vers
ion.
Instructions for updating:
Please use tf.data to implement this functionality.
Extracting /tmp/tensorflow/mnist/input_data\train-images-idx3-ubyte.gz
WARNING:tensorflow:From C:\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future vers
ion.
Instructions for updating:
Please use tf.data to implement this functionality.
Extracting /tmp/tensorflow/mnist/input_data\train-labels-idx1-ubyte.gz
Extracting /tmp/tensorflow/mnist/input_data\t10k-images-idx3-ubyte.gz
Extracting /tmp/tensorflow/mnist/input_data\t10k-labels-idx1-ubyte.gz
WARNING:tensorflow:From C:\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future ve
rsion.
Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
Saving graph to: C:\Users\alanp\AppData\Local\Temp\tmpxpinc32v
2018-09-20 22:10:32.455130: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2018-09-20 22:10:32.641187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties:
name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.4175
pciBusID: 0000:01:00.0
totalMemory: 4.00GiB freeMemory: 3.30GiB
2018-09-20 22:10:32.659090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2018-09-20 22:10:33.556406: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-09-20 22:10:33.565294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0
2018-09-20 22:10:33.571150: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N
2018-09-20 22:10:33.578487: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3013 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus
id: 0000:01:00.0, compute capability: 6.1)
2018-09-20 22:10:36.376573: E tensorflow/stream_executor/cuda/cuda_dnn.cc:353] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED"
22413,tf_inspect.py is not future secure due to use of inspect.getargspec(),"### System information
Python 3.6.5
Pytest version 3.8.0.

Issue valid for master of tensorflow on github

### Describe the problem
The module `tf_inspect.py` uses deprecated functions of the `inspect` module.

### Source code / logs
Easiest to see the warning is make a pytest:

```python3
import tensorflow

def test_me():
    return
```
and run `pytest` selecting that test-file producing the warnings:

```
../python3.6/site-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  return _inspect.getargspec(target)
```
It should be as easy as replacing `getargspec` with `getfullargspec` as the latter is meant as a dropin replacement of be previous according to the docs.

I would make a PR, but I'm not comfortable, to put it mildly, with the legalisms required."
22412,Tensor2Tensor Intro tensorflow.python.framework.errors_impl.OutOfRangeError: Read fewer bytes than requested,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
windows 10, x64
- **TensorFlow installed from (source or binary)**:
 pip (pip install --upgrade tensorflow)
- **TensorFlow version (use command below)**:
1.10.0
- **Python version**:
3.6
- **GPU model and memory**:
not using GPU, 12GB memory
Mobile device: N/A
Bazel version : N/A
CUDA/cuDNN version : N/A

### Describe the problem 
I'm trying to make the Tensor2Tensor intro: English to German translation with a pre-trained model work locally. I have downloaded the checkpoints to the right directory, however it's returning this error:
tensorflow.python.framework.errors_impl.OutOfRangeError: Read fewer bytes than requested

As you can see the checkpoints are in the proper directory:
![image](https://user-images.githubusercontent.com/24248699/45819470-ac0e8c00-bce4-11e8-85bf-fccc4e2e7c22.png)

I have tried redownloading the checkpoints many times, I can't seem to get the English to German translation working with the provided checkpoints.
@lukaszkaiser 

- **Exact command to reproduce**:

```
# Imports we need.
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
import os
import collections

from tensor2tensor import models
from tensor2tensor import problems
from tensor2tensor.layers import common_layers
from tensor2tensor.utils import trainer_lib
from tensor2tensor.utils import t2t_model
from tensor2tensor.utils import registry
from tensor2tensor.utils import metrics

# Enable TF Eager execution
tfe = tf.contrib.eager
tfe.enable_eager_execution()

# Other setup
Modes = tf.estimator.ModeKeys

# Setup some directories
data_dir = os.path.expanduser(""~/t2t-ende/data"")
tmp_dir = os.path.expanduser(""~/t2t-ende/tmp"")
train_dir = os.path.expanduser(""~/t2t-ende/train"")
checkpoint_dir = os.path.expanduser(""~/t2t-ende/checkpoints"")
tf.gfile.MakeDirs(data_dir)
tf.gfile.MakeDirs(tmp_dir)
tf.gfile.MakeDirs(train_dir)
tf.gfile.MakeDirs(checkpoint_dir)
gs_data_dir = ""gs://tensor2tensor-data""
gs_ckpt_dir = ""gs://tensor2tensor-checkpoints/""

# Fetch the problem
ende_problem = problems.problem(""translate_ende_wmt32k"")

# Copy the vocab file locally so we can encode inputs and decode model outputs
# All vocabs are stored on GCS
vocab_name = ""vocab.translate_ende_wmt32k.32768.subwords""
vocab_file = os.path.join(gs_data_dir, vocab_name)

# Get the encoders from the problem
encoders = ende_problem.feature_encoders(data_dir)

# Setup helper functions for encoding and decoding


def encode(input_str, output_str=None):
    """"""Input str to features dict, ready for inference""""""
    inputs = encoders[""inputs""].encode(input_str) + [1]  # add EOS id
    batch_inputs = tf.reshape(inputs, [1, -1, 1])  # Make it 3D.
    return {""inputs"": batch_inputs}


def decode(integers):
    """"""List of ints to str""""""
    integers = list(np.squeeze(integers))
    if 1 in integers:
        integers = integers[:integers.index(1)]
    return encoders[""inputs""].decode(np.squeeze(integers))

# Generate and view the data
# This cell is commented out because WMT data generation can take hours


ende_problem.generate_data(data_dir, tmp_dir)
example = tfe.Iterator(ende_problem.dataset(Modes.TRAIN, data_dir)).next()
inputs = [int(x) for x in example[""inputs""].numpy()]  # Cast to ints.
targets = [int(x) for x in example[""targets""].numpy()]  # Cast to ints.

# Example inputs as int-tensor.
print(""Inputs, encoded:"")
print(inputs)
print(""Inputs, decoded:"")
# Example inputs as a sentence.
print(decode(inputs))
# Example targets as int-tensor.
print(""Targets, encoded:"")
print(targets)
# Example targets as a sentence.
print(""Targets, decoded:"")
print(decode(targets))

# Create hparams and the model
model_name = ""transformer""
hparams_set = ""transformer_base""

hparams = trainer_lib.create_hparams(
    hparams_set, data_dir=data_dir, problem_name=""translate_ende_wmt32k"")

# NOTE: Only create the model once when restoring from a checkpoint; it's a
# Layer and so subsequent instantiations will have different variable scopes
# that will not match the checkpoint.
translate_model = registry.model(model_name)(hparams, Modes.EVAL)

# Copy the pretrained checkpoint locally
ckpt_name = ""transformer_ende_test""
gs_ckpt = os.path.join(gs_ckpt_dir, ckpt_name)
ckpt_path = tf.train.latest_checkpoint(os.path.join(checkpoint_dir, ckpt_name))

# Restore and translate!
def translate(inputs):
    encoded_inputs = encode(inputs)
    print(encoded_inputs)
    with tfe.restore_variables_on_create(ckpt_path):
        model_output = translate_model.infer(encoded_inputs)[""outputs""]
        print(model_output)
    return decode(model_output)


inputs = ""The animal didn't cross the street because it was too tired""
outputs = translate(inputs)

print(""Inputs: %s"" % inputs)
print(""Outputs: %s"" % outputs)`
```

### Source code / logs
>>> def translate(inputs):
...     encoded_inputs = encode(inputs)
...     print(encoded_inputs)
...     with tfe.restore_variables_on_create(ckpt_path):
...         model_output = translate_model.infer(encoded_inputs)[""outputs""]
...         print(model_output)
...     return decode(model_output)
...
>>> inputs = ""The animal didn't cross the street because it was too tired""
>>> outputs = translate(inputs)
{'inputs': <tf.Tensor: id=202, shape=(1, 15, 1), dtype=int32, numpy=
array([[[   28],
        [ 4705],
        [ 6253],
        [   83],
        [   62],
        [ 3444],
        [    4],
        [ 3825],
        [  244],
        [   40],
        [   53],
        [  362],
        [19285],
        [   85],
        [    1]]])>}
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<stdin>"", line 4, in translate
  File ""C:\Users\david.salsbach\AppData\Local\Programs\Python\Python36\lib\contextlib.py"", line 81, in __enter__
    return next(self.gen)
  File ""C:\Users\david.salsbach\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\contrib\eager\python\saver.py"", line 91, in restore_variables_on_create
    ckpt_var_cache[k] = reader.get_tensor(k)
  File ""C:\Users\david.salsbach\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 348, in get_tensor
    status)
  File ""C:\Users\david.salsbach\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 519, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.OutOfRangeError: Read fewer bytes than requested
>>> print(""Inputs: %s"" % inputs)
Inputs: The animal didn't cross the street because it was too tired
>>> print(""Outputs: %s"" % outputs)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
NameError: name 'outputs' is not defined
"
22411,"the session->close() is ok,but the error is  stack smashing detected ","### OS platform:ubuntu:16.04
### ensorflow install from source code
### tensorflow version:1.11(master branch)
### Bazel version:0.17.1
### CPU
## 1comand:
bazel build --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 --config=monolithic //tensorflow:libtensorflow_cc.so
bazel build --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 --config=monolithic //tensorflow:libtensorflow_framework.so

sudo mkdir /usr/local/include/tf
sudo mkdir /usr/local/include/tf/tensorflow
sudo cp -r bazel-genfiles/ /usr/include/tf
sudo cp -r tensorflow/cc /usr/include/tf/tensorflow
sudo cp -r tensorflow/core /usr/include/tf/tensorflow
sudo cp -r third_party /usr/include/tf
sudo cp -r bazel-bin/tensorflow/libtensorflow_cc.so /usr/local/lib
sudo cp -r bazel-bin/tensorflow/libtensorflow_framework.so /usr/local/lib
## 2code:
#include ""tensorflow/core/public/session.h""
#include ""tensorflow/core/platform/env.h""

using namespace std;
using namespace chrono;
using namespace tensorflow;
int main(int argc, char* argv[]) {
// SessionOptions options;
// std::unique_ptrtensorflow::Session session(tensorflow::NewSession(options));
Session *session = NewSession(SessionOptions());
if(session == nullptr)
{
throw runtime_error(""Could not create tensorflow session"");
}
Status status = session->Close();
return 0;
}
## 3the status is status.ok() is right,but the error is ""*** stack smashing detected ***: terminated  ()"""
22410,"GAN train will hand if not use default GANTrainSteps(1, 1)","When I test WGAN on tensorflow,  I set the generator_train_steps = 1 and discriminator_train_steps = 5 following the paper, like this:
tensorflow/contrib/gan/python/train.py:815
`def get_sequential_train_hooks(train_steps=namedtuples.GANTrainSteps(1, 5))`
The train process always hand after the first step. 
And the default value works well.
`def get_sequential_train_hooks(train_steps=namedtuples.GANTrainSteps(1, 1))`

Test:
just run 
`https://github.com/tensorflow/models/blob/master/research/gan/cifar/train.py `
with cifar10 dataset.

"
22409,ValueError: Cannot set tensor: Got tensor of type 4 but expected type 2 for input 25,"I am trying to run a tensorflow model on device, using tensorflow lite. I was able to convert my .pb graph to a .lite graph. Using python interpreter as below

`interpreter = interpreter_wrapper.Interpreter(model_path=model_file)
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
interpreter.set_tensor(input_details[0]['index'], candidates)`

the above code throws following error : ValueError: Cannot set tensor: Got tensor of type 4 but expected type 2 for input 25

I have no clue what tensor of type 4 and tensor of type 2 are, I am not sure if they are same as tensor ranks, in my case, the tensor ranks and types mentioned above don't seem to be the same."
22408,Tried to read from index 3 but array size is: 3,"### System information
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N/A
OS Platform and Distribution : Linux Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): N/A
TensorFlow version (use command below): N/A
Python version: 3.6
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: 8.0
GPU model and memory: N/A
Exact command to reproduce: N/A

### Describe the problem

Hi, 
when I am using the tf.data.dataset api, I meet the error, and i am confused. 

the error is :
`InvalidArgumentError (see above for traceback): Tried to read from index 3 but array size is: 3`

and my related code is:

```python
def train_input(sess, vocab, rcmodel, train_or_valid=True):
    train_dataset = get_dataset(vocab.word2idx, FLAGS.train_tfrecord_file, FLAGS.train_batch_size,
                                repeat_num=FLAGS.num_epochs, shuffle_bufer=500, prefetch=-1)
    valid_dataset = get_dataset(vocab.word2idx, FLAGS.valid_tfrecord_file, FLAGS.valid_batch_size,
                                repeat_num=-1, shuffle_bufer=500)
    train_iterator = train_dataset.make_initializable_iterator()
    valid_iterator = valid_dataset.make_initializable_iterator()
    if train_or_valid:
        data_handle = sess.run(train_iterator.string_handle())
        sess.run(train_iterator.initializer)
    else:
        data_handle = sess.run(valid_iterator.string_handle())
        sess.run(valid_iterator.initializer)
        
    data_iter = tf.data.Iterator.from_string_handle(string_handle=data_handle,
                                                    output_types=train_dataset.output_types,
                                                    output_shapes=train_dataset.output_shapes)

    batch_data = data_iter.get_next()
    feed_data = {rcmodel.passage: batch_data[""passage""].eval(),
                 rcmodel.passage_len: batch_data[""passage_len""].eval(),
                 rcmodel.query: batch_data[""query""].eval(),
                 rcmodel.query_len: batch_data[""query_len""].eval(),
                 # rcmodel.query_id: batch_data[""query_id""],
                 rcmodel.answer: batch_data['answer'].eval(),
                 rcmodel.answer_len: batch_data[""answer_len""].eval(),
                 rcmodel.alter0: batch_data[""alter0""].eval(),
                 rcmodel.alter1: batch_data[""alter1""].eval(),
                 rcmodel.alter2: batch_data[""alter2""].eval()}
    return feed_data
```

```python
train_feed = train_input(sess, vocab, rcmodel, True)
train_feed[rcmodel.dropout_keep_prob] = FLAGS.dropout_keep_prob
train_summary, _, train_loss, train_ppl_loss, train_acc, step, predict, logits, answer, embedding = sess.run(
                   [merged, rcmodel.train_op, rcmodel.total_loss, rcmodel.ppl_loss,
                    rcmodel.accuracy, rcmodel.global_step, rcmodel.predict, rcmodel.logits,
                    rcmodel.answer, rcmodel.word_embedding],
                   feed_dict=train_feed)
```

I know the error is caused by that: when I try to use 
```sess.run(fetches, feed_dict={passage:data_iter[""passage}, query:data_iter['passage_len'])```

```passage:data_iter[""passage""]``` consumed a batch, and ```query:data_iter[""passage_len""]``` consumed an other batch. So the data pasage and passage_len is not match.

So the reason is that when I sess.run one of the content of batch_iter, it will consume one batch. And I know using the string_handle can solve this problem. 

But when I need to do this:
```python
self.passage = tf.placeholder(tf.int32, [None, None], name=""passage"")
self.query = tf.placeholder(tf.int32, [None, None], name=""query"")
self.answer = tf.placeholder(tf.int32, [None, None], name=""answer"")
self.passage_len = tf.placeholder(tf.int32, [None], name=""passage_len"")
self.query_len = tf.placeholder(tf.int32, [None], name=""query_len"")
self.query_id = tf.placeholder(tf.int32, [None], name=""query_id"")
self.answer_len = tf.placeholder(tf.int32, [None], name=""answer_len"")
self.alter0 = tf.placeholder(tf.int32, [None, None], name=""alter0"")
self.alter1 = tf.placeholder(tf.int32, [None, None], name=""alter1"")
self.alter2 = tf.placeholder(tf.int32, [None, None], name=""alter2"")
```

What can I do to feed this?

"
22407,Distributed training hangs up when I use CollectiveAllReduceStrategy (Python 2),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
I slightly updated keras_model_to_estimator_client.py example so that it doesn't require Kubernetes. Updated version is [here](https://github.com/dmitrievanthony/ecosystem/blob/ignite-3/distribution_strategy/keras_model_to_estimator_client.py).

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 17.04

- **TensorFlow installed from (source or binary)**:
Binary

- **TensorFlow version (use command below)**:
1.12.0-dev20180919

- **Python version**:
2.7.13

- **Exact command to reproduce**:
Start [worker1.py](https://github.com/dmitrievanthony/ecosystem/blob/ignite-3/ignite/worker1.py).
Start [worker2.py](https://github.com/dmitrievanthony/ecosystem/blob/ignite-3/ignite/worker2.py).
Start [keras_model_to_estimator_client.py](https://github.com/dmitrievanthony/ecosystem/blob/ignite-3/distribution_strategy/keras_model_to_estimator_client.py).

As result I would expect distributed training started in standalone mode with the following cluster config: `{""worker"": [""localhost:1111"", ""localhost:1112""], ""chief"": [""localhost:1113""]}`. After initialization I would expect to see the same output as in local (non-distributed) mode.

### Describe the problem

Hi, 

It looks like the process hangs up on initialization step. See logs of processes below.

### Source code / logs

Logs of `worker1.py`.
```
2018-09-20 12:23:37.784529: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-09-20 12:23:37.785450: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:222] Initialize GrpcChannelCache for job chief -> {0 -> localhost:1113}
2018-09-20 12:23:37.785469: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:222] Initialize GrpcChannelCache for job worker -> {0 -> localhost:1111, 1 -> localhost:1112}
2018-09-20 12:23:37.786408: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:381] Started server with target: grpc://localhost:1111
2018-09-20 12:23:37.786477: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:385] Server already started (target: grpc://localhost:1111)
2018-09-20 12:23:37.786586: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:385] Server already started (target: grpc://localhost:1111)
2018-09-20 12:24:02.453219: I tensorflow/core/distributed_runtime/master_session.cc:1161] Start master session 10324091b7c40099 with config: device_filters: ""/job:worker/task:0"" device_filters: ""/job:worker/task:0"" allow_soft_placement: true graph_options { rewrite_options { meta_optimizer_iterations: ONE scoped_allocator_optimization: ON scoped_allocator_opts { enable_op: ""CollectiveReduce"" enable_op: ""CollectiveReduce"" } } } isolate_session_state: true experimental { collective_group_leader: ""/job:chief/replica:0/task:0"" }
2018-09-20 12:24:02.471993: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:355] Starting optimization for grappler item: tf_graph
```

Logs of `worker2.py`.
```
2018-09-20 12:23:43.680413: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-09-20 12:23:43.681625: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:222] Initialize GrpcChannelCache for job chief -> {0 -> localhost:1113}
2018-09-20 12:23:43.681664: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:222] Initialize GrpcChannelCache for job worker -> {0 -> localhost:1111, 1 -> localhost:1112}
2018-09-20 12:23:43.682404: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:381] Started server with target: grpc://localhost:1112
2018-09-20 12:23:43.682480: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:385] Server already started (target: grpc://localhost:1112)
2018-09-20 12:23:43.682577: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:385] Server already started (target: grpc://localhost:1112)
2018-09-20 12:24:02.426685: I tensorflow/core/distributed_runtime/master_session.cc:1161] Start master session 438db1d6d8b24351 with config: device_filters: ""/job:worker/task:1"" device_filters: ""/job:worker/task:1"" allow_soft_placement: true graph_options { rewrite_options { meta_optimizer_iterations: ONE scoped_allocator_optimization: ON scoped_allocator_opts { enable_op: ""CollectiveReduce"" enable_op: ""CollectiveReduce"" } } } isolate_session_state: true experimental { collective_group_leader: ""/job:chief/replica:0/task:0"" }
2018-09-20 12:24:02.441035: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:355] Starting optimization for grappler item: tf_graph
```

Logs of `keras_model_to_estimator_client.py`.
```
Using /tmp/asdasdasd to store checkpoints.
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense (Dense)                (None, 16)                176       
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 17        
=================================================================
Total params: 193
Trainable params: 193
Non-trainable params: 0
_________________________________________________________________
INFO:tensorflow:CollectiveAllReduceStrategy with local_devices = ['/device:CPU:0']
2018-09-20 12:30:36.241065: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
INFO:tensorflow:Initializing RunConfig with distribution strategies.
INFO:tensorflow:RunConfig initialized for Distribute Coordinator with STANDALONE_CLIENT mode
INFO:tensorflow:Using the Keras model provided.
INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f971668a110>, '_model_dir': '/tmp/asdasdasd', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': DistributeConfig(train_distribute=<tensorflow.contrib.distribute.python.collective_all_reduce_strategy.CollectiveAllReduceStrategy object at 0x7f97166f2ed0>, eval_distribute=<tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x7f971668a190>, remote_cluster=<tensorflow.python.training.server_lib.ClusterSpec object at 0x7f971668a110>), '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': <tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x7f971668a190>, '_train_distribute': <tensorflow.contrib.distribute.python.collective_all_reduce_strategy.CollectiveAllReduceStrategy object at 0x7f97166f2ed0>, '_master': '', '_distribute_coordinator_mode': 'standalone_client'}
INFO:tensorflow:Running `train_and_evaluate` with Distribute Coordinator.
INFO:tensorflow:Running Distribute Coordinator with mode = 'standalone_client', cluster_spec = {'chief': ['localhost:1113'], 'worker': ['localhost:1111', 'localhost:1112']}, task_type = None, task_id = None, environment = None, rpc_layer = 'grpc'
INFO:tensorflow:Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'chief': ['localhost:1113'], 'worker': ['localhost:1111', 'localhost:1112']}, task_type = 'chief', task_id = 0, num_workers = 3, local_devices = ['/job:chief/task:0']
INFO:tensorflow:Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'chief': ['localhost:1113'], 'worker': ['localhost:1111', 'localhost:1112']}, task_type = 'worker', task_id = 0, num_workers = 3, local_devices = ['/job:worker/task:0']
INFO:tensorflow:Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'chief': ['localhost:1113'], 'worker': ['localhost:1111', 'localhost:1112']}, task_type = 'worker', task_id = 1, num_workers = 3, local_devices = ['/job:worker/task:1']
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Collective All-reduce invoked with batches size = 4, num_workers = 3
INFO:tensorflow:Collective All-reduce invoked with batches size = 4, num_workers = 3
INFO:tensorflow:Collective All-reduce invoked with batches size = 4, num_workers = 3
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from=u'/tmp/asdasdasd/keras/keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})
INFO:tensorflow:Warm-starting from: (u'/tmp/asdasdasd/keras/keras_model.ckpt',)
INFO:tensorflow:Warm-starting variable: dense/bias; prev_var_name: Unchanged
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Warm-starting variable: dense_1/bias; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: dense_1/kernel; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: dense/kernel; prev_var_name: Unchanged
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Creating chief session creator with config: device_filters: ""/job:worker/task:0""
allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
    scoped_allocator_optimization: ON
    scoped_allocator_opts {
      enable_op: ""CollectiveReduce""
    }
  }
}
isolate_session_state: true
experimental {
  collective_group_leader: ""/job:chief/replica:0/task:0""
}

INFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from=u'/tmp/asdasdasd/keras/keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})
INFO:tensorflow:Warm-starting from: (u'/tmp/asdasdasd/keras/keras_model.ckpt',)
INFO:tensorflow:Warm-starting variable: dense/bias; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: dense_1/bias; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: dense_1/kernel; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: dense/kernel; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from=u'/tmp/asdasdasd/keras/keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})
INFO:tensorflow:Warm-starting from: (u'/tmp/asdasdasd/keras/keras_model.ckpt',)
INFO:tensorflow:Warm-starting variable: dense/bias; prev_var_name: Unchanged
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Creating chief session creator with config: device_filters: ""/job:worker/task:1""
allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
    scoped_allocator_optimization: ON
    scoped_allocator_opts {
      enable_op: ""CollectiveReduce""
    }
  }
}
isolate_session_state: true
experimental {
  collective_group_leader: ""/job:chief/replica:0/task:0""
}

INFO:tensorflow:Warm-starting variable: dense_1/bias; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: dense_1/kernel; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: dense/kernel; prev_var_name: Unchanged
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Creating chief session creator with config: device_filters: ""/job:chief/task:0""
allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
    scoped_allocator_optimization: ON
    scoped_allocator_opts {
      enable_op: ""CollectiveReduce""
    }
  }
}
isolate_session_state: true
experimental {
  collective_group_leader: ""/job:chief/replica:0/task:0""
}

INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Graph was finalized.
```"
22406,Exporting trained TensorFlow models to C++,"
### System information
- **Have I written custom code **: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: win7 X64    Visual Studio 2015
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: source 
- **TensorFlow version (use command below)**: r 1.10
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.15
- **CUDA/cuDNN version**: CUDA 9.0  /  CUDNN 7.2.1
- **GPU model and memory**: 1080Ti / 11G
- **Exact command to reproduce**:

I trained a `graph.pb` file with python  i want to Exporting trained TensorFlow models to C++
I refer to some code on github, but still can't successfully import trained models using C++.Using the MNIST data.

```
TF_Graph* LoadGraphDef( const char* file )
	{
		if (file == nullptr)
		{
			return nullptr;
		}

		TF_Buffer* buffer = ReadBufferFromFile( file );
		if (buffer == nullptr)
		{
			return nullptr;
		}

		TF_Graph* graph = TF_NewGraph();
		TF_Status* status = TF_NewStatus();
		TF_ImportGraphDefOptions* opts = TF_NewImportGraphDefOptions();

		TF_GraphImportGraphDef( graph, buffer, opts, status );
		TF_DeleteImportGraphDefOptions( opts );
		TF_DeleteBuffer( buffer );

		if (TF_GetCode( status ) != TF_OK)
		{
			TF_DeleteGraph( graph );
			graph = nullptr;
		}

		TF_DeleteStatus( status );

		return graph;
	}

TF_Tensor* CreateTensor( TF_DataType data_type,
		const std::int64_t* dims, std::size_t num_dims,
		const void* data, std::size_t len )
	{
		if (dims == nullptr || data == nullptr)
		{
			return nullptr;
		}
		TF_Tensor* tensor = TF_AllocateTensor( data_type, dims, static_cast<int>(num_dims), len );
		if (tensor == nullptr)
		{
			return nullptr;
		}
		void* tensor_data = TF_TensorData( tensor );
		if (tensor_data == nullptr)
		{
			TF_DeleteTensor( tensor );
			return nullptr;
		}
		std::memcpy( tensor_data, data, std::min( len, TF_TensorByteSize( tensor ) ) );

		return tensor;
	}
```

```
	TF_Graph* graph = tf_utils::LoadGraphDef( ""graph.pb"" );
	if (graph == nullptr)
	{
		std::cout << ""Can't load graph"" << std::endl;
		return 1;
	}
	TF_Output input_op = { TF_GraphOperationByName( graph, ""input"" ), 0 };
	if (input_op.oper == nullptr)
	{
		std::cout << ""Can't init input_op"" << std::endl;
		return 2;
	}
	const std::vector<std::int64_t> input_dims = { 1, 784 };
	const std::vector<float> input_vals = {
		0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 96, 223, 255, 255, 223, 96, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 72, 223, 255, 255, 255, 255, 255, 247, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 231, 255, 255, 255, 255, 255, 255, 151, 0, 72, 223, 223, 96, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 255, 255, 255, 167, 8, 0, 0, 0, 96, 239, 255, 255, 247, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 72, 223, 255, 255, 255, 255, 0, 0, 0, 0, 104, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 231, 255, 255, 255, 255, 183, 0, 0, 72, 223, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 223, 96, 0, 0, 0, 231, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0,0, 0, 0, 255, 255, 255, 0, 0, 0, 0, 72, 255, 255, 255, 255, 151, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 104, 255, 255, 255, 0, 0, 0, 96, 239, 255, 255, 255, 0, 0, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 231, 255, 255, 167, 0, 0, 128, 255, 255, 255, 255, 151, 0, 0, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 8, 0, 104, 255, 255, 255, 255, 151, 0, 0, 0, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 0, 0, 231, 255, 255, 183, 8, 0, 0, 0, 0, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 0, 0, 255, 255, 255, 8, 0, 0, 0, 0, 0, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 247, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 120, 255, 255, 255, 255, 255, 255, 151, 0, 0, 0, 0, 0, 0, 247, 255, 255, 24, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 183, 255, 255, 104, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 96, 255, 255, 191, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 0, 0, 0, 0, 0, 0
	};

	TF_Tensor* input_tensor = tf_utils::CreateTensor( TF_FLOAT,
		input_dims.data(), input_dims.size(),
		input_vals.data(), input_vals.size() * sizeof( float ) );

	TF_Output out_op = { TF_GraphOperationByName( graph, ""output"" ), 0 };
	if (out_op.oper == nullptr)
	{
		std::cout << ""Can't init out_op"" << std::endl;
		return 3;
	}
```
input_vals is the array corresponding to the minist picture, I want to know how `TF_AllocateTensor( data_type, dims, static_cast<int>(num_dims), len )` should be used and  what `dims` and `static_cast<int>(num_dims)` stand for?


[This link](https://github.com/taotaolin/tensorflow_test/) Detailed partial code"
22403,Where is the pywrap_tensorflow_internal.lib after building TensorFlow1.11 On Win10?,"I have build TensorFlow1.11GPU version successfully in Win10 with using bazel, and generate tensorflow-1.11.0rc1-cp36-cp36m-win_amd64.whl file seccessfully. But I can not find the C++ lib(pywrap_tensorflow_internal.lib), where is the lib?  I want to use C++ not  Python.

![qq 20180920145146](https://user-images.githubusercontent.com/39480728/45800850-bdf02f00-bce4-11e8-9cda-e233ac413387.png)


"
22401,CUDA 10.0 and python 3.7 support,"this is not an issue

more of a question, request

can tensorflowers give roadmap for CUDA 10.0(and of course, cuDNN 7.3 for CUDA 10.0) and python 3.7 support? and of course Ubuntu 18.04

CUDA 10.0 and python 3.7 already support Ubuntu 18.04


thank you always
"
22400,[tensorflow ppc64le] Building tensorflow from source fails on power8 architecture machine,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Red Hat Enterprise Linux Server 7.4 (Maipo)
- **Mobile devices**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 
- **Python version**: 3.5.6
- **Bazel version (if compiling from source)**: 0.14.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0 (tried with 4.8.x as well)
- **CUDA/cuDNN version**: 8.0/ 7.1.4
- **GPU model and memory**: NVIDIA P100
- **Exact command to reproduce**: bazel build //tensorflow/tools/pip_package:build_pip_package

**Output of the tf_collect_env.sh**

== cat /etc/issue ===============================================
Linux summitdev-login1 3.10.0-693.21.1.el7.ppc64le #1 SMP Fri Feb 23 14:02:56 EST 2018 ppc64le ppc64le ppc64le GNU/Linux
VERSION=""7.4 (Maipo)""
VERSION_ID=""7.4""
REDHAT_BUGZILLA_PRODUCT_VERSION=7.4
REDHAT_SUPPORT_PRODUCT_VERSION=""7.4""

== are we in docker =============================================
No

== compiler =====================================================
c++ (GCC) 5.4.0
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux summitdev-login1 3.10.0-693.21.1.el7.ppc64le #1 SMP Fri Feb 23 14:02:56 EST 2018 ppc64le ppc64le ppc64le GNU/Linux

== check pips ===================================================
numpy      1.15.1   

== check for virtualenv =========================================
False    **(this is not true I created an environment and ran this script from inside that env)**

== tensorflow import ============================================

== env ==========================================================
LD_LIBRARY_PATH /autofs/nccs-svm1_sw/summitdev/.swci/1-compute/opt/spack/20180406/linux-rhel7-ppc64le/gcc-5.4.0/spectrum-mpi-10.2.0.0-20180110-6keaqyzn74d6t7ilr5wcmd4ovxjkxue7/lib:/sw/summitdev/gcc/5.4.0new/lib64:/sw/summitdev/cuda/8.0.61-1/lib64:/autofs/nccs-svm1_home/shubhankar/magma_install/lib:/autofs/nccs-svm1_home1/shubhankar/magma_install/lib:/autofs/nccs-svm1_home/shubhankar/magma_install/lib:/autofs/nccs-svm1_home1/shubhankar/magma_install/lib:/opt/ibm/spectrumcomputing/lsf/10.1/linux3.10-glibc2.17-ppc64le-csm/lib:/autofs/nccs-svm1_home1/shubhankar/miniconda3/envs/pytorch/lib:/autofs/nccs-svm1_home/shubhankar/miniconda3/envs/pytorch/lib:/autofs/nccs-svm1_home/shubhankar/pytorch/build/lib:/autofs/nccs-svm1_home1/shubhankar/pytorch/build/lib:/autofs/nccs-svm1_home/shubhankar/libcudnn7_7.0.3.11-1+cuda8.0_ppc64el/lib/powerpc64le-linux-gnu:/sw/summitdev/cuda/8.0.61-1/extras/CUPTI/lib64:/autofs/nccs-svm1_home1/shubhankar/miniconda3/envs/pytorch/lib:/autofs/nccs-svm1_home/shubhankar/miniconda3/envs/pytorch/lib:/autofs/nccs-svm1_home/shubhankar/pytorch/build/lib:/autofs/nccs-svm1_home1/shubhankar/pytorch/build/lib:/sw/summitdev/cuda/9.0.69/extras/CUPTI/lib64
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Wed Sep 19 23:25:19 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 390.46                 Driver Version: 390.46                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla P100-SXM2...  Off  | 00000002:01:00.0 Off |                    0 |
| N/A   32C    P0    29W / 300W |     18MiB / 16280MiB |      0%   E. Process |
+-------------------------------+----------------------+----------------------+
|   1  Tesla P100-SXM2...  Off  | 00000003:01:00.0 Off |                    0 |
| N/A   28C    P0    30W / 300W |     18MiB / 16280MiB |      0%   E. Process |
+-------------------------------+----------------------+----------------------+
|   2  Tesla P100-SXM2...  Off  | 00000006:01:00.0 Off |                    0 |
| N/A   32C    P0    28W / 300W |     18MiB / 16280MiB |      0%   E. Process |
+-------------------------------+----------------------+----------------------+
|   3  Tesla P100-SXM2...  Off  | 00000007:01:00.0 Off |                    0 |
| N/A   29C    P0    30W / 300W |     18MiB / 16280MiB |      0%   E. Process |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================

### Describe the problem
Building tensorflow from source fails on power architecture machine. I am following the instructions given [here](https://developer.ibm.com/tutorials/install-tensorflow-on-power/)

### Source code / logs
```
WARNING: Output base '/autofs/nccs-svm1_home1/shubhankar/.cache/bazel/_bazel_shubhankar/e74c0b81a4afde0b86e7c039eda6fc5e' is on NFS. This may lead to surprising failures and undetermined behavior.
ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
    File ""/autofs/nccs-svm1_home1/shubhankar/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1166
        _create_local_cuda_repository(repository_ctx)
    File ""/autofs/nccs-svm1_home1/shubhankar/tensorflow/third_party/gpus/cuda_configure.bzl"", line 995, in _create_local_cuda_repository
        _get_cuda_config(repository_ctx)
    File ""/autofs/nccs-svm1_home1/shubhankar/tensorflow/third_party/gpus/cuda_configure.bzl"", line 750, in _get_cuda_config
        _cudnn_version(repository_ctx, cudnn_install_base..., ...)
    File ""/autofs/nccs-svm1_home1/shubhankar/tensorflow/third_party/gpus/cuda_configure.bzl"", line 464, in _cudnn_version
        _find_cudnn_header_dir(repository_ctx, cudnn_install_base...)
    File ""/autofs/nccs-svm1_home1/shubhankar/tensorflow/third_party/gpus/cuda_configure.bzl"", line 707, in _find_cudnn_header_dir
        auto_configure_fail((""Cannot find cudnn.h under %s"" ...))
    File ""/autofs/nccs-svm1_home1/shubhankar/tensorflow/third_party/gpus/cuda_configure.bzl"", line 210, in auto_configure_fail
        fail((""\n%sCuda Configuration Error:%...)))

Cuda Configuration Error: Cannot find cudnn.h under /autofs/nccs-svm1_home1/shubhankar/libcudnn7_7.0.3.11-1+cuda8.0_ppc64el/lib/powerpc64le-linux-gnu
WARNING: Target pattern parsing failed.
ERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
    File ""/autofs/nccs-svm1_home1/shubhankar/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1166
        _create_local_cuda_repository(repository_ctx)
    File ""/autofs/nccs-svm1_home1/shubhankar/tensorflow/third_party/gpus/cuda_configure.bzl"", line 995, in _create_local_cuda_repository
        _get_cuda_config(repository_ctx)
    File ""/autofs/nccs-svm1_home1/shubhankar/tensorflow/third_party/gpus/cuda_configure.bzl"", line 750, in _get_cuda_config
        _cudnn_version(repository_ctx, cudnn_install_base..., ...)
    File ""/autofs/nccs-svm1_home1/shubhankar/tensorflow/third_party/gpus/cuda_configure.bzl"", line 464, in _cudnn_version
        _find_cudnn_header_dir(repository_ctx, cudnn_install_base...)
    File ""/autofs/nccs-svm1_home1/shubhankar/tensorflow/third_party/gpus/cuda_configure.bzl"", line 707, in _find_cudnn_header_dir
        auto_configure_fail((""Cannot find cudnn.h under %s"" ...))
    File ""/autofs/nccs-svm1_home1/shubhankar/tensorflow/third_party/gpus/cuda_configure.bzl"", line 210, in auto_configure_fail
        fail((""\n%sCuda Configuration Error:%...)))

Cuda Configuration Error: Cannot find cudnn.h under /autofs/nccs-svm1_home1/shubhankar/libcudnn7_7.0.3.11-1+cuda8.0_ppc64el/lib/powerpc64le-linux-gnu
INFO: Elapsed time: 0.197s
FAILED: Build did NOT complete successfully (0 packages loaded)
    currently loading: tensorflow/tools/pip_package
```


**NOTE:** Don't have sudo access and can't modify /usr/local/* but have all dependencies installed."
22399,MirroredStrategy AssertionError in 1.11.0-rc0,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04; Not on mobile device
- **TensorFlow installed from (source or binary)**:
source 
models's commit = 17fa52864bfc7a7444a8b921d8a8eb1669e14ebd and tensorflow's commit = 1e438195399650604fb3aa3a53c67339f1167882
- **TensorFlow version (use command below)**:
git checkout -b v1.11.0-rc0 v1.11.0-rc0
- **Python version**:
Python 3.5.2 (default, Nov 23 2017, 16:37:01)
- **Bazel version (if compiling from source)**:
Build label: 0.17.1
- **GCC/Compiler version (if compiling from source)**:
gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
- **CUDA/cuDNN version**:
- **GPU model and memory**:
GeForce GTX 1070 Ti 8G*4
- **Exact command to reproduce**:
# python object_detection/model_main.py --pipeline_config_path=object_detection/samples/configs/ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync.config --logtostderr --model_dir=/data/checkpoint/del2 --num_gpus=4
and meet AssertionError:
File ""/home/soft/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/values.py"", line 414, in _tensor_conversion_mirrored
    assert not as_ref
AssertionError
You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

### Describe the problem
Similar issue: https://github.com/tensorflow/tensorflow/issues/21968. 
Reply: Actually, this issue is likely fixed in release 1.11, could you try with that and see if that fixes the issue? But I meet the issue in 1.11.0-rc0.

### Source code / logs
 1
git diff object_detection/model_main.py
-config = tf.estimator.RunConfig(model_dir=FLAGS.model_dir)
+distribution = tf.contrib.distribute.MirroredStrategy()
+config = tf.estimator.RunConfig(model_dir=FLAGS.model_dir, train_distribute=distribution, eval_distribute=distribution)
2
cd /models/research
# python object_detection/model_main.py --pipeline_config_path=object_detection/samples/configs/ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync.config --logtostderr --model_dir=/data/sisi.wu/checkpoint/del2 --num_gpus=4
3
[multi-GPU.log](https://github.com/tensorflow/tensorflow/files/2399199/multi-GPU.log)


Thanks."
22398,CUDA implementation of BiasAddGrad op is non-determinstic,"I'm running TensorFlow 1.5.0 on a K80 GPU on Python 2.7

Failing test case:
```python
from __future__ import print_function
import hashlib
import numpy as np
import tensorflow as tf

np.random.randn(2018)
tf.set_random_seed(2018)

X = np.random.randn(1024, 50).astype(np.float32)
b = tf.get_variable('bias', [50])
z = tf.nn.bias_add(X, b)

grad = tf.gradients(z*z, b)[0]

init_op = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init_op)

    run1 = sess.run(grad)
    run2 = sess.run(grad)

    print(np.all(run1 == run2))
    print(np.max(np.abs(run1 - run2)))

    dohash = lambda X: hashlib.md5(X.tostring()).hexdigest()
    print(dohash(run1))
    print(dohash(run2))
```

Outputs
```
False
6.10352e-05
b489a1d659518b2ae9213f5a21e35df2
187a57d563468e59ba5f9d9cf51ca5cb
```

This bug still exists in master, because the code in master uses the unsafe CUDA atomic floating point add in several places. See https://github.com/tensorflow/tensorflow/blob/abc55107eb7a03fe3d83f95fd5e1b8e4def90826/tensorflow/core/kernels/bias_op_gpu.cu.cc 
If TensorFlow will be ever be fully determinstic, atomic floating point add should never be used (it is inherently non-determinstic due to non-associativity of floating point).

Notably, Keras's `Dense` layer uses `bias_add`, so all networks that use this layer are non-reproducible. This is relevant to https://github.com/keras-team/keras/issues/2280 .

 This can currently be avoided by using `tf.add` instead of `tf.nn.bias_add` at a slightly performance hit. The correct fix would be refactor the `BiasAddGrad` op to use a (deterministic) reduction tree.

edit with issue template fields:
Have I written custom code: Python test case, see above
OS Platform and Distribution: RHEL 7.5 (Linux)
TensorFlow installed from: source
TensorFlow version: 1.5.0
Bazel version: unknown
CUDA/cuDNN version: CUDA 8.0.61, cuDNN v6
GPU model and memory: Nvidia K80, 12GB memory
Exact command to reproduce: run the above script
Mobile device: NA"
22396,[Feature Request]:Assign the name to SaprseTensor when build_tensor_info of it,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N.A.
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.10.1
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.16.1
- **GCC/Compiler version (if compiling from source)**: gcc version 5.4.0 20160609
- **CUDA/cuDNN version**: 9.0/7.0
- **GPU model and memory**: Tesla P100 16GB
- **Exact command to reproduce**:
A minimum reproduce example:
```
a = tf.constant([[[1.0]]])
b = tf.constant([1])
output,prob = tf.nn.ctc_beam_search_decoder(a,b)
a_tensor_proto = tf.saved_model.utils.build_tensor_info(a)
output_tensor_proto = tf.saved_model.utils.build_tensor_info(output[0]) 
```
### Describe the problem
A normal Tensor has a name:
```
a_tensor_proto
name: ""Const_15:0""
dtype: DT_FLOAT
tensor_shape {
  dim {
    size: 1
  }
  dim {
    size: 1
  }
  dim {
    size: 1
  }
}

```
while the SparseTensor from the ctc Beam_Search_Decoder does not has a name:
```
output_tensor_proto
dtype: DT_INT64
tensor_shape {
  dim {
    size: -1
  }
  dim {
    size: -1
  }
}
coo_sparse {
  values_tensor_name: ""CTCBeamSearchDecoder_12:1""
  indices_tensor_name: ""CTCBeamSearchDecoder_12:0""
  dense_shape_tensor_name: ""CTCBeamSearchDecoder_12:2""
}
```
Which cause the error when predict:

```
import tensorflow as tf
from tensorflow.contrib import predictor
from tensorflow.contrib.saved_model.python.saved_model import reader
from tensorflow.contrib.saved_model.python.saved_model import signature_def_utils
from tensorflow.python.tools import saved_model_utils

exp_dir = MY_EXPORT_DIRECTORY
metagraph_def = saved_model_utils.get_meta_graph_def(exp_dir,'serve')
signature_def = signature_def_utils.get_signature_def_by_key(
        metagraph_def,
        'predicted_sequences')
output_names = {k: v.name for k, v in signature_def.outputs.items()}
print(output_names)
predict_fn = predictor.from_saved_model(exp_dir)
seq_len = np.asarray([[4]])
combined_input = np.concatenate((batch_x,seq_len),axis = 1)
predictions = predict_fn({'inputs':combined_input})
```
ValueError: The name '' looks like an (invalid) Operation name, not a Tensor. Tensor names must be of the form ""<op_name>:<output_index>"".
and  also the above error, when export it into a saved_model and request it.

The reason is when building a TensorProto for the SaprseTensor, no name is assigned to it:
https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/python/saved_model/utils_impl.py#L46

"
22395,numpy not found during python_api generation,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS-7.4
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: None
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.10.1, earlier versions
- **Python version**: 2.7, 3.6 (separate rebuilds same issue)
- **Bazel version (if compiling from source)**: 0.17.1
- **GCC/Compiler version (if compiling from source)**: 4.8.5, 6.3.1 (separate rebuilds same issue)
- **CUDA/cuDNN version**: CPU only build
- **GPU model and memory**: Quadro K4000
- **Exact command to reproduce**: Not reproducible with single command

### Describe the problem
Problem manifests as ""No module named numpy"", during python api generation as detailed below in log section.

If placing a simple `print(sys.path)` or `print(os.environ['PYTHONPATH'])` in the listed `__init__.py` (in below log), none of the resulting paths listed include the path that numpy is included on via the PYTHONPATH, which is being passed along to the build environ included via bazels CLI opts, `--action_env` and `--test_env` arguments to the build call.

I believe this issue will be reproducible only under a state where someone has installed numpy to a nonstandard location, and is including it via PYTHONPATH and `--action_env` such as is listed. However, all python modules in our environment are in separate paths, as managed by an environment manager, so I cannot install them to a common system path for testing if the issue resolves itself. What is strange is that earlier steps that require numpy have built just fine and not errored from what I can tell.

Apologies if my details included are omitting some crucial detail. The specific set of config values seems unnecessary for detail here, but let me know if they are needed. (Every optional component driven by `TF_NEED_{var}` is turned off except for `TF_NEED_JEMALLOC` which does not appear to be part of the problem)

### Source code / logs

build command, while prefaced by many environment variable changes and the ./configure step, is of the form:
`bazel --batch build --action_env=PYTHONPATH --action_env=LD_LIBRARY_PATH --test_env=PYTHONPATH --test_env=LD_LIBRARY_PATH -c opt --copt=-mfpmath=both --copt=-msse4.2 //tensorflow/tools/pip_package:build_pip_package`

`ERROR: ${build_root}/tensorflow/BUILD:581:1: Executing genrule //tensorflow:tensorflow_python_api_gen failed (Exit 1)
Traceback (most recent call last):
  File ""/home/${user}/.cache/bazel/_bazel_${user}/ff38b9a62494579437724e08cde1b695/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 27, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""/home/${user}/.cache/bazel/_bazel_${user}/ff38b9a62494579437724e08cde1b695/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 49, in <module>
    import numpy as np
ImportError: No module named numpy`"
22394,"Tensorflow == operator is inconsistent with NumPy, other TF comparison operators","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Y
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.7.0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**: 
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: not relevant
- **Exact command to reproduce**: A == B

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Hello,

I recently ran into a bug in my code where I was trying to use the == operator to compare arrays element-wise. I found out that == was comparing the entire array, rather than comparing element-wise, so the whole expression evaluated to false. The false scalar was then broadcast across other arrays in the program, making the bug difficult to detect. I was able to fix the bug by using the function tf.equal().

My question is, why does == have this behavior in Tensorflow? In Numpy, == compares arrays element-wise. Even in Tensorflow, the other comparison operators (>, >=. <, <=) operate element-wise. Is there a specific reason for breaking this convention only for the == operator?

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

Numpy code:

>>> import numpy as np
>>> A = np.eye(3)
>>> A == 1
array([[ True, False, False],
       [False,  True, False],
       [False, False,  True]])
"
22393,ERROR: /home/developer/tensorflow/tensorflow/BUILD:652,":1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1)
Traceback (most recent call last):
  File ""/home/developer/.cache/bazel/_bazel_developer/6acbce31a4e8077fc84a239074e8eda3/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 27, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""/home/developer/.cache/bazel/_bazel_developer/6acbce31a4e8077fc84a239074e8eda3/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 70, in <module>
    from tensorflow.python.framework.framework_lib import *  # pylint: disable=redefined-builtin
  File ""/home/developer/.cache/bazel/_bazel_developer/6acbce31a4e8077fc84a239074e8eda3/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/framework/framework_lib.py"", line 52, in <module>
    from tensorflow.python.framework.importer import import_graph_def
  File ""/home/developer/.cache/bazel/_bazel_developer/6acbce31a4e8077fc84a239074e8eda3/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/framework/importer.py"", line 27, in <module>
    from tensorflow.python.framework import function
  File ""/home/developer/.cache/bazel/_bazel_developer/6acbce31a4e8077fc84a239074e8eda3/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/framework/function.py"", line 36, in <module>
    from tensorflow.python.ops import resource_variable_ops
  File ""/home/developer/.cache/bazel/_bazel_developer/6acbce31a4e8077fc84a239074e8eda3/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/ops/resource_variable_ops.py"", line 38, in <module>
    from tensorflow.python.ops import variables
  File ""/home/developer/.cache/bazel/_bazel_developer/6acbce31a4e8077fc84a239074e8eda3/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/ops/variables.py"", line 20, in <module>
    import enum  # pylint: disable=g-bad-import-order
ImportError: No module named enum
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1418.730s, Critical Path: 243.25s
INFO: 9922 processes: 9922 local.
FAILED: Build did NOT complete successfully


https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22390,bazel absence of ZIP64 support cause tensorflow build fail,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 1803
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: v1.11.0-rc1
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: 0.15.2
- **GCC/Compiler version (if compiling from source)**: vs2017 15.8 / cl.exe 19.15.26726
- **CUDA/cuDNN version**: 9.2.148.1/7.2.1
- **GPU model and memory**: 1080ti 11GB
- **Exact command to reproduce**: 
```
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
bazel-bin\tensorflow\tools\pip_package\build_pip_package C:/tmp/tensorflow_pkg
```
### Describe the problem

If build artifact large than 4GB, it can't generate the python package.

A easy way to reproduce the issue
```
Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,7.0]: 3.0,3.2,3.5,5.0,5.2,5.3
```

related link:
https://github.com/tensorflow/tensorflow/issues/20332#issuecomment-415974623
https://github.com/tensorflow/tensorflow/issues/22382
https://github.com/bazelbuild/bazel/blob/0.17.1/third_party/ijar/zip.cc#L74

### Source code / logs
```
Uncompressed input jar has size ???, which exceeds the maximum supported output size 4294967295.
Assuming that ijar will be smaller and hoping for the best.

Unzipping simple_console_for_windows.zip to create runfiles tree...
[./bazel-bin/tensorflow/tools/pip_package/simple_console_for_windows.zip]
  End-of-central-directory signature not found.  Either this file is not
  a zipfile, or it constitutes one disk of a multi-part archive.  In the
  latter case the central directory and zipfile comment will be found on
  the last disk(s) of this archive.
unzip:  cannot find zipfile directory in one of ./bazel-bin/tensorflow/tools/pip_package/simple_console_for_windows.zip or
        ./bazel-bin/tensorflow/tools/pip_package/simple_console_for_windows.zip.zip, and cannot find ./bazel-bin/tensorflow/tools/pip_package/simple_console_for_windows.zip.ZIP, period.

```

"
22389,tensorflow.python.framework.errors_impl.FailedPreconditionError: temp/G2D19_P2OF_ResHB_1LSTM_dataAug_expLR; Not a directory ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 

Yes. I was trying out the code given in https://github.com/JoshuaPiinRueyPan/ViolenceDetection

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:

Ubuntu 18.04

- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:

NA

- **TensorFlow (CPU version) installed from (source or binary)**:

Installed using pip3 as shown in https://www.python36.com/install-tensorflow-using-official-pip-pacakage/

- **TensorFlow version (use command below)**:

1.9

- **Python version**:

3.6.5

- Machine details

Architecture:        x86_64
CPU op-mode(s):      32-bit, 64-bit
Byte Order:          Little Endian
CPU(s):              4
On-line CPU(s) list: 0-3
Thread(s) per core:  2
Core(s) per socket:  2
Socket(s):           1
NUMA node(s):        1
Vendor ID:           GenuineIntel
CPU family:          6
Model:               142
Model name:          Intel(R) Core(TM) i7-7600U CPU @ 2.80GHz

- **Exact command to reproduce**:

>> python Train.py 

This is from the code given in https://github.com/JoshuaPiinRueyPan/ViolenceDetection

### Describe the problem

The code contains an LSTM+CNN network and when training with a set of videos, it fails with a tensorflow error. I checked and found that the 'temp/G2D19_P2OF_ResHB_1LSTM_dataAug_expLR' mentioned in the error log is indeed a directory. However Tensorflow complains that it is not and throws an error. I am unable to understand why.

### Source code / logs

Traceback (most recent call last):
File ""Train.py"", line 199, in
main = Main()
File ""Train.py"", line 19, in init
self.trainer = Trainer(classifier)
File ""/home/vandana/Documents/Code/ViolenceDetection-master/src/Trainer.py"", line 36, in init
self._summaryWriter = tf.summary.FileWriter(trainSettings.PATH_TO_SAVE_MODEL+""/train"")
File ""/home/vandana/anaconda3/lib/python3.6/site-packages/tensorflow/python/summary/writer/writer.py"", line 366, in init
filename_suffix)
File ""/home/vandana/anaconda3/lib/python3.6/site-packages/tensorflow/python/summary/writer/event_file_writer.py"", line 67, in init
gfile.MakeDirs(self._logdir)
File ""/home/vandana/anaconda3/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py"", line 379, in recursive_create_dir
pywrap_tensorflow.RecursivelyCreateDir(compat.as_bytes(dirname), status)
File ""/home/vandana/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 519, in exit
c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.FailedPreconditionError: temp/G2D19_P2OF_ResHB_1LSTM_dataAug_expLR; Not a directory
Send Stop singal to Loading threads...
The Loading threads will Stop in about 100 (s).
Send Stop singal to Loading threads...
The Loading threads will Stop in about 100 (s).
TrainDataManager.thread.join() successfully.
Exception ignored in: <bound method Main.del of <main.Main object at 0x7f91467b57f0>>
Traceback (most recent call last):
File ""Train.py"", line 55, in del
self.session.close()
AttributeError: 'Main' object has no attribute 'session'
"
22387,Error compiling optional_ops.cc when building from source with MSVC 2017,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Microsoft Windows [Version 10.0.17134.228]
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: trunk
- **Python version**: Python 3.7.0
- **Bazel version (if compiling from source)**: Build label: 0.17.1
- **GCC/Compiler version (if compiling from source)**: Microsoft (R) C/C++ Optimizing Compiler Version 19.15.26729 for x64
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: 
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
Unable to build from sources with Microsoft Build Tools 2017. Error when compiling **tensorflow/core/kernels/data/optional_ops.cc**

### Source code / logs
```
C:/Program Files (x86)/Microsoft Visual Studio/2017/Professional/VC/Tools/MSVC/14.15.26726/bin/HostX64/x64/cl.exe /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /bigobj /Zm500 /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Ibazel-out/x64_windows-opt/bin /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/genfiles/external/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/bazel_tools /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/genfiles/external/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/nsync /Ibazel-out/x64_windows-opt/genfiles/external/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/gif_archive /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive /Ibazel-out/x64_windows-opt/bin/external/gif_archive /Iexternal/jpeg /Ibazel-out/x64_windows-opt/genfiles/external/jpeg /Ibazel-out/x64_windows-opt/bin/external/jpeg /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/genfiles/external/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/genfiles/external/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/genfiles/external/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/genfiles/external/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/genfiles/external/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/genfiles/external/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/gif_archive/lib /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/lib /Ibazel-out/x64_windows-opt/bin/external/gif_archive/lib /Iexternal/gif_archive/windows /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/windows /Ibazel-out/x64_windows-opt/bin/external/gif_archive/windows /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DTF_USE_SNAPPY /showIncludes /MD /O2 /Oy- /DNDEBUG /wd4117 -D__DATE__=""redacted"" -D__TIMESTAMP__=""redacted"" -D__TIME__=""redacted"" /Gy /Gw -w /arch:AVX -DTENSORFLOW_MONOLITHIC_BUILD /DPLATFORM_WINDOWS /DEIGEN_HAS_C99_MATH /DTENSORFLOW_USE_EIGEN_THREADPOOL /DEIGEN_AVOID_STL_ARRAY /Iexternal/gemmlowp /wd4018 /wd4577 /DNOGDI /DTF_COMPILE_LIBRARY /Fobazel-out/x64_windows-opt/bin/tensorflow/core/kernels/data/_objs/optional_ops/optional_ops.obj /c tensorflow/core/kernels/data/optional_ops.cc
C:\Program Files (x86)\Microsoft Visual Studio\2017\Professional\VC\Tools\MSVC\14.15.26726\include\xmemory(217): error C2678: binary '*': no operator found which takes a left-hand operand of type 'const _Iter' (or there is no acceptable conversion)
        with
        [
            _Iter=tensorflow::OpInputList::Iterator
        ]
.\tensorflow/core/framework/op_kernel.h(404): note: could be 'const tensorflow::Tensor &tensorflow::OpArgIterator<tensorflow::OpInputList,const tensorflow::Tensor>::operator *(void)'
C:\Program Files (x86)\Microsoft Visual Studio\2017\Professional\VC\Tools\MSVC\14.15.26726\include\xmemory(217): note: while trying to match the argument list '(const _Iter)'
        with
        [
            _Iter=tensorflow::OpInputList::Iterator
        ]
C:\Program Files (x86)\Microsoft Visual Studio\2017\Professional\VC\Tools\MSVC\14.15.26726\include\vector(1823): note: see reference to function template instantiation '_FwdIt *std::_Uninitialized_copy<_Iter,tensorflow::Tensor*,std::allocator<_Ty>>(const _InIt,const _InIt,_FwdIt,_Alloc &)' being compiled
        with
        [
            _FwdIt=tensorflow::Tensor *,
            _Iter=tensorflow::OpInputList::Iterator,
            _Ty=tensorflow::Tensor,
            _InIt=tensorflow::OpInputList::Iterator,
            _Alloc=std::allocator<tensorflow::Tensor>
        ]
C:\Program Files (x86)\Microsoft Visual Studio\2017\Professional\VC\Tools\MSVC\14.15.26726\include\vector(708): note: see reference to function template instantiation 'tensorflow::Tensor *std::vector<tensorflow::Tensor,std::allocator<_Ty>>::_Ucopy<_Iter>(_Iter,_Iter,tensorflow::Tensor *)' being compiled
        with
        [
            _Ty=tensorflow::Tensor,
            _Iter=tensorflow::OpInputList::Iterator
        ]
C:\Program Files (x86)\Microsoft Visual Studio\2017\Professional\VC\Tools\MSVC\14.15.26726\include\vector(708): note: see reference to function template instantiation 'tensorflow::Tensor *std::vector<tensorflow::Tensor,std::allocator<_Ty>>::_Ucopy<_Iter>(_Iter,_Iter,tensorflow::Tensor *)' being compiled
        with
        [
            _Ty=tensorflow::Tensor,
            _Iter=tensorflow::OpInputList::Iterator
        ]
C:\Program Files (x86)\Microsoft Visual Studio\2017\Professional\VC\Tools\MSVC\14.15.26726\include\vector(723): note: see reference to function template instantiation 'void std::vector<tensorflow::Tensor,std::allocator<_Ty>>::_Range_construct_or_tidy<_Iter>(_Iter,_Iter,std::forward_iterator_tag)' being compiled
        with
        [
            _Ty=tensorflow::Tensor,
            _Iter=tensorflow::OpInputList::Iterator
        ]
C:\Program Files (x86)\Microsoft Visual Studio\2017\Professional\VC\Tools\MSVC\14.15.26726\include\vector(723): note: see reference to function template instantiation 'void std::vector<tensorflow::Tensor,std::allocator<_Ty>>::_Range_construct_or_tidy<_Iter>(_Iter,_Iter,std::forward_iterator_tag)' being compiled
        with
        [
            _Ty=tensorflow::Tensor,
            _Iter=tensorflow::OpInputList::Iterator
        ]
tensorflow/core/kernels/data/optional_ops.cc(112): note: see reference to function template instantiation 'std::vector<tensorflow::Tensor,std::allocator<_Ty>>::vector<tensorflow::OpInputList::Iterator,void>(_Iter,_Iter,const _Alloc &)' being compiled
        with
        [
            _Ty=tensorflow::Tensor,
            _Iter=tensorflow::OpInputList::Iterator,
            _Alloc=std::allocator<tensorflow::Tensor>
        ]
tensorflow/core/kernels/data/optional_ops.cc(111): note: see reference to function template instantiation 'std::vector<tensorflow::Tensor,std::allocator<_Ty>>::vector<tensorflow::OpInputList::Iterator,void>(_Iter,_Iter,const _Alloc &)' being compiled
        with
        [
            _Ty=tensorflow::Tensor,
            _Iter=tensorflow::OpInputList::Iterator,
            _Alloc=std::allocator<tensorflow::Tensor>
        ]
external/com_google_absl\absl/container/inlined_vector.h(73): note: see reference to class template instantiation 'std::allocator<T>' being compiled
        with
        [
            T=tensorflow::MemoryType
        ]
.\tensorflow/core/framework/op_kernel.h(176): note: see reference to class template instantiation 'absl::InlinedVector<tensorflow::MemoryType,4,std::allocator<T>>' being compiled
        with
        [
            T=tensorflow::MemoryType
        ]
external/com_google_absl\absl/container/inlined_vector.h(73): note: see reference to class template instantiation 'std::allocator<T>' being compiled
        with
        [
            T=tensorflow::TensorReference
        ]
.\tensorflow/core/framework/unique_tensor_references.h(66): note: see reference to class template instantiation 'absl::InlinedVector<tensorflow::TensorReference,4,std::allocator<T>>' being compiled
        with
        [
            T=tensorflow::TensorReference
        ]
C:\Program Files (x86)\Microsoft Visual Studio\2017\Professional\VC\Tools\MSVC\14.15.26726\include\xmemory(217): error C2100: illegal indirection
C:\Program Files (x86)\Microsoft Visual Studio\2017\Professional\VC\Tools\MSVC\14.15.26726\include\xmemory(217): error C2062: type 'unknown-type' unexpected
Target //tensorflow/tools/pip_package:build_pip_package failed to build

```"
22385,session = tf.Session(),"Can someone please tell me how to solve this. Though I'm able to run forward in tf.

```
session = tf.Session()

2018-09-19 21:16:27.452934: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-09-19 21:16:27.547097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-09-19 21:16:27.547742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: 
name: GeForce 920M major: 3 minor: 5 memoryClockRate(GHz): 0.954
pciBusID: 0000:08:00.0
totalMemory: 3.95GiB freeMemory: 3.70GiB
2018-09-19 21:16:27.547773: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2018-09-19 21:16:27.838759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-09-19 21:16:27.838808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 
2018-09-19 21:16:27.838817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N 
2018-09-19 21:16:27.839009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3439 MB memory) -> physical GPU (device: 0, name: GeForce 920M, pci bus id: 0000:08:00.0, compute capability: 3.5)

```"
22384,BUG: reciprocal GPU kernel for complex 1/1 division not found,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.2/7.2.1
- **GPU model and memory**: GeForce GTX 1050 mobile, 4096MiB
- **Exact command to reproduce**:
```
import tensorflow as tf
import numpy as np

# create a lot of points
# CHANGE NUMBER BELOW (higher -> fails)
n_points = 670000  # fails around > 650'000 (e.g. 1'000'000) for np.linspace generation


# points creation
points = np.ones(n_points)
# alternate points creation
# low = 100.  # irrelevant, only > 0 -> no 0 (division)
# high = 200.  # irrelevant
# points = tf.random_uniform(shape=[n_points], minval=low, maxval=high)
# points = np.linspace(low, high, n_points)

with tf.Session() as sess:
    # just do complex division: 1.0+0j / points
    complex_1_0 = tf.cast(1., dtype=tf.complex128)
    denom = tf.cast(points, dtype=tf.complex128)
    division = complex_1_0 / denom
    result = sess.run(division)
    print(result)
```

### Describe the problem
For a certain amount of points, the complex division kernel (or rather reciprocal kernel) ""seems to not be available on the GPU"".
**It works for**
- a lower number of points
- for floats
- on CPU (tested with ""tf.devices"")

**It seems to be independent of**
- the generation of the points (see commented lines for alternate creation)
- GPU (also tested on Tesla K80) with lower TF/Cuda version

No idea where to start...

### Source code / logs
relevant error:
NotFoundError (see above for traceback): No registered 'Reciprocal' OpKernel for GPU devices compatible with node truediv = Reciprocal[T=DT_COMPLEX128, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Cast_1)
	 (OpKernel was found, but attributes didn't match)
	.  Registered:  device='GPU'; T in [DT_INT64]
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_COMPLEX128]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_FLOAT]
	 [[Node: truediv = Reciprocal[T=DT_COMPLEX128, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Cast_1)]]

Full stacktrace of failure: 
2018-09-19 17:18:40.965160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3010 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-09-19 17:18:41.104747: E tensorflow/core/common_runtime/executor.cc:697] Executor failed to create kernel. Not found: No registered 'Reciprocal' OpKernel for GPU devices compatible with node truediv = Reciprocal[T=DT_COMPLEX128, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Cast_1)
	 (OpKernel was found, but attributes didn't match)
	.  Registered:  device='GPU'; T in [DT_INT64]
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_COMPLEX128]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_FLOAT]
	 [[Node: truediv = Reciprocal[T=DT_COMPLEX128, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Cast_1)]]
Traceback (most recent call last):
  File ""/home/jonas/anaconda3/envs/zfit36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1278, in _do_call
    return fn(*args)
  File ""/home/jonas/anaconda3/envs/zfit36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1263, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/home/jonas/anaconda3/envs/zfit36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1350, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.NotFoundError: No registered 'Reciprocal' OpKernel for GPU devices compatible with node truediv = Reciprocal[T=DT_COMPLEX128, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Cast_1)
	 (OpKernel was found, but attributes didn't match)
	.  Registered:  device='GPU'; T in [DT_INT64]
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_COMPLEX128]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_FLOAT]
	 [[Node: truediv = Reciprocal[T=DT_COMPLEX128, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Cast_1)]]
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""/home/jonas/anaconda3/envs/zfit36/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2961, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-2-0ba75500be23>"", line 1, in <module>
    runfile('/home/jonas/Documents/uni/Master_thesis/code/test/tf_playground/test/gpu_div_bug.py', wdir='/home/jonas/Documents/uni/Master_thesis/code/test/tf_playground/test')
  File ""/usr/share/pycharm/helpers/pydev/_pydev_bundle/pydev_umd.py"", line 197, in runfile
    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script
  File ""/usr/share/pycharm/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""/home/jonas/Documents/uni/Master_thesis/code/test/tf_playground/test/gpu_div_bug.py"", line 20, in <module>
    result = sess.run(division)
  File ""/home/jonas/anaconda3/envs/zfit36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 877, in run
    run_metadata_ptr)
  File ""/home/jonas/anaconda3/envs/zfit36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/jonas/anaconda3/envs/zfit36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1272, in _do_run
    run_metadata)
  File ""/home/jonas/anaconda3/envs/zfit36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: No registered 'Reciprocal' OpKernel for GPU devices compatible with node truediv = Reciprocal[T=DT_COMPLEX128, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Cast_1)
	 (OpKernel was found, but attributes didn't match)
	.  Registered:  device='GPU'; T in [DT_INT64]
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_COMPLEX128]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_FLOAT]
	 [[Node: truediv = Reciprocal[T=DT_COMPLEX128, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Cast_1)]]
Caused by op 'truediv', defined at:
  File ""/usr/share/pycharm/helpers/pydev/pydevconsole.py"", line 511, in <module>
    pydevconsole.start_server(host, int(port), int(client_port), client_host)
  File ""/usr/share/pycharm/helpers/pydev/pydevconsole.py"", line 336, in start_server
    process_exec_queue(interpreter)
  File ""/usr/share/pycharm/helpers/pydev/pydevconsole.py"", line 192, in process_exec_queue
    more = interpreter.add_exec(code_fragment)
  File ""/usr/share/pycharm/helpers/pydev/_pydev_bundle/pydev_console_utils.py"", line 281, in add_exec
    more = self.do_add_exec(code_fragment)
  File ""/usr/share/pycharm/helpers/pydev/_pydev_bundle/pydev_ipython_console.py"", line 41, in do_add_exec
    res = bool(self.interpreter.add_exec(code_fragment.text))
  File ""/usr/share/pycharm/helpers/pydev/_pydev_bundle/pydev_ipython_console_011.py"", line 442, in add_exec
    self.ipython.run_cell(line, store_history=True)
  File ""/home/jonas/anaconda3/envs/zfit36/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2662, in run_cell
    raw_cell, store_history, silent, shell_futures)
  File ""/home/jonas/anaconda3/envs/zfit36/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2785, in _run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/home/jonas/anaconda3/envs/zfit36/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2907, in run_ast_nodes
    if self.run_code(code, result):
  File ""/home/jonas/anaconda3/envs/zfit36/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2961, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-2-0ba75500be23>"", line 1, in <module>
    runfile('/home/jonas/Documents/uni/Master_thesis/code/test/tf_playground/test/gpu_div_bug.py', wdir='/home/jonas/Documents/uni/Master_thesis/code/test/tf_playground/test')
  File ""/usr/share/pycharm/helpers/pydev/_pydev_bundle/pydev_umd.py"", line 197, in runfile
    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script
  File ""/usr/share/pycharm/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""/home/jonas/Documents/uni/Master_thesis/code/test/tf_playground/test/gpu_div_bug.py"", line 19, in <module>
    division = complex_1_0 / denom
  File ""/home/jonas/anaconda3/envs/zfit36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py"", line 850, in binary_op_wrapper
    return func(x, y, name=name)
  File ""/home/jonas/anaconda3/envs/zfit36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py"", line 958, in _truediv_python3
    return gen_math_ops.real_div(x, y, name=name)
  File ""/home/jonas/anaconda3/envs/zfit36/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 5881, in real_div
    ""RealDiv"", x=x, y=y, name=name)
  File ""/home/jonas/anaconda3/envs/zfit36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/jonas/anaconda3/envs/zfit36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 454, in new_func
    return func(*args, **kwargs)
  File ""/home/jonas/anaconda3/envs/zfit36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3155, in create_op
    op_def=op_def)
  File ""/home/jonas/anaconda3/envs/zfit36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()
NotFoundError (see above for traceback): No registered 'Reciprocal' OpKernel for GPU devices compatible with node truediv = Reciprocal[T=DT_COMPLEX128, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Cast_1)
	 (OpKernel was found, but attributes didn't match)
	.  Registered:  device='GPU'; T in [DT_INT64]
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_COMPLEX128]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_FLOAT]
	 [[Node: truediv = Reciprocal[T=DT_COMPLEX128, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Cast_1)]]
"
22383,Inconsistency in tf.nn.top_k when using sorted=False and tf.float32 tensor and GPU placement,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA.
- **TensorFlow installed from (source or binary)**: Binary.
- **TensorFlow version (use command below)**: 'v1.10.1-0-g4dcfddc5d1', '1.10.1'
- **Python version**: Python 3.6.5 :: Anaconda, Inc.
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: CUDAToolkit 9.0, CuDNN 7.1.4
- **GPU model and memory**: NVIDIA Quadro P4000, 8GB (driver version 396.37)
- **Exact command to reproduce**: See source code below.

### Describe the problem
The indices and values returned by top_k(sorted=False) seem to be shuffled with respect to each other when tf.float32 tensors are used, but not when tf.int32 tensors are used. I understand that we can not expect the values to be returned in ascending/descending order since we've set sorted=False, but shouldn't the values and indices at least be consistently ordered with respect to each other? Hope this makes sense. See my minimum working example below.

Edit: This seems to be a GPU-only problem.

### Source code / logs

```
import tensorflow as tf
import numpy as np
tf.set_random_seed(21)

def demo_bug(np_dtype, tf_dtype):
    np_arr = np.array([3, 2, 8, 1], np_dtype)
    arr = tf.convert_to_tensor(np_arr, tf_dtype)
    top_k = tf.nn.top_k(arr, k=3, sorted=False)

    with tf.Session() as sess:
        vals, inds = sess.run(top_k)

    print('dtype:   ', tf_dtype.__repr__())
    print('arr:     ', np_arr)
    print('inds:    ', inds)
    print('tf_vals: ', vals)
    print('np_vals: ', np_arr[inds])

demo_bug(np.int32, tf.int32)
demo_bug(np.float32, tf.float32)

>> dtype:    tf.int32
>> arr:      [3 2 8 1]
>> inds:     [2 0 1]
>> tf_vals:  [8 3 2]
>> np_vals:  [8 3 2]

>> dtype:    tf.float32
>> arr:      [3. 2. 8. 1.]
>> inds:     [2 0 1]
>> tf_vals:  [2. 3. 8.]
>> np_vals:  [8. 3. 2.]
```"
22382,Creating pip package for TensorFlow with GPU support results in 0 byte simple_console_for_windows.zip,"Here is the stackoverflow question: https://stackoverflow.com/questions/52394305/creating-pip-package-for-tensorflow-with-gpu-support-results-in-0-byte-simple-co

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.11
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: 0.16.1
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.2/7.2.1
- **GPU model and memory**: Nvidia M1000M
- **Exact command to reproduce**: bazel-bin\tensorflow\tools\pip_package\build_pip_package C:/tmp/tensorflow_pkg

### Describe the problem
After successfully building TensorFlow with GPU support, I'm trying to build the pip package and I'm getting an error saying it can't read the simple_console_for_windows.zip file.  

I've confirmed that the file is in C:\tensorflow\bazel-bin\tensorflow\tools\pip_package folder, but it is 0 bytes.

This is my pip build command:
    bazel-bin\tensorflow\tools\pip_package\build_pip_package C:/tmp/tensorflow_pkg

My build command was:
    bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

### Source code / logs
This is the full error:

    Unzipping simple_console_for_windows.zip to create runfiles tree...
    [./bazel-bin/tensorflow/tools/pip_package/simple_console_for_windows.zip]
      End-of-central-directory signature not found.  Either this file is not
      a zipfile, or it constitutes one disk of a multi-part archive.  In the
      latter case the central directory and zipfile comment will be found on
      the last disk(s) of this archive.
    unzip:  cannot find zipfile directory in one of ./bazel-bin/tensorflow/tools/pip_package/simple_console_for_windows.zip or
            ./bazel-bin/tensorflow/tools/pip_package/simple_console_for_windows.zip.zip, and cannot find ./bazel-bin/tensorflow/tools/pip_package/simple_console_for_windows.zip.ZIP, period.
"
22380,ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory,"Hi! I have CUDA 9.2 and CUDNN 7.2.1
I have seen this error in the issues page before but didn't get a nice answer as to solve it. Should I downgarde the CUDA version, if yes, which one to? Thanks in advance!

```
Python 3.6.5 |Anaconda custom (64-bit)| (default, Apr 29 2018, 16:14:56) 
Type 'copyright', 'credits' or 'license' for more information
IPython 6.4.0 -- An enhanced Interactive Python. Type '?' for help.

In [1]: import tensorflow as tf

---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
~/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>()
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in <module>()
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

~/anaconda3/lib/python3.6/imp.py in load_module(name, file, filename, details)
    242         else:
--> 243             return load_dynamic(name, filename, file)
    244     elif type_ == PKG_DIRECTORY:

~/anaconda3/lib/python3.6/imp.py in load_dynamic(name, path, file)
    342             name=name, loader=loader, origin=path)
--> 343         return _load(spec)
    344 

ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-1-64156d691fe5> in <module>()
----> 1 import tensorflow as tf

~/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py in <module>()
     20 
     21 # pylint: disable=g-bad-import-order
---> 22 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
     23 
     24 try:

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py in <module>()
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 # Protocol buffers

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>()
     72 for some common reasons and solutions.  Include the entire stack trace
     73 above this error message when asking for help."""""" % traceback.format_exc()
---> 74   raise ImportError(msg)
     75 
     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""/home/dhananjai/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/dhananjai/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/dhananjai/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/dhananjai/anaconda3/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/dhananjai/anaconda3/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```"
22379,"[TF1.10] Use the method ""experimental_create_eval_graph()"" of quantize_graph.py to quantize a frozen_model, succeeded but failed at serializing by graph_util","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS 10.13.6
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: pip install tensorflow-gpu==1.10.0
- **TensorFlow version (use command below)**: 1.10.0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: no
- **GCC/Compiler version (if compiling from source)**: clang 9.1.0
- **CUDA/cuDNN version**: no
- **GPU model and memory**: no
- **Exact command to reproduce**: Please refer to my test script below:

Hi, 
I tried to use the tool named quantize_graph.py to quantize a frozen_graph from FP32 to INT8.
After import the frozen model, I called the method named ""experimental_create_eval_graph"" and seems succeeded.
Later when I try to use graph_util to freeze variables to constant, error occurred.
Script:
```
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import tensorflow as tf
from tensorflow.contrib.layers.python.layers import layers
from tensorflow.contrib.quantize.python import quantize_graph
from tensorflow.python.framework import ops
from tensorflow.python.framework import test_util
from tensorflow.python.ops import array_ops
from tensorflow.python.ops import init_ops
from tensorflow.python.ops import nn_ops

from tensorflow.python.framework import importer as importer
from tensorflow.python.client import session as csess
from tensorflow.core.protobuf import config_pb2 as cpb2
from tensorflow.python.framework import ops as ops
from tensorflow.python.framework import graph_util
from tensorflow.python.platform import gfile

#tf.enable_eager_execution()

output_node_name = 'InceptionV3/Logits/SpatialSqueeze'
input_node_name = 'Placeholder'
output_path = 'test.pb'
g = tf.Graph()
with g.as_default():
    output_graph_def = tf.GraphDef()
    with open('frozen_graph.pb', 'rb') as f:
        output_graph_def.ParseFromString(f.read())
        _ = tf.import_graph_def(output_graph_def, name='')
    print([n.name for n in output_graph_def.node])
    quantize_graph.experimental_create_eval_graph(
        input_graph = g,
        weight_bits = 8,
        activation_bits = 8,
        quant_delay=None,
        scope=None)

    print('=========================')
    print([n.name for n in g.as_graph_def().node])


#saver = tf.train.Saver(write_version=tf.train.SaverDef.V1)
with csess.Session(config=cpb2.ConfigProto(), graph=g) as sess:
    inp = tf.get_default_graph().get_tensor_by_name('Placeholder:0')
    out = tf.get_default_graph().get_tensor_by_name('InceptionV3/Logits/SpatialSqueeze:0')
    g = graph_util.convert_variables_to_constants(sess, g.as_graph_def(), out)
    with tf.gfile.GFile(output_path, ""wb"") as f:
        f.write(g.SerializeToString())
#    saver_path = saver.save(sess, ""model.ckpt"")
```
The Error log:
```
Traceback (most recent call last):
  File ""test.py"", line 48, in <module>
    g = graph_util.convert_variables_to_constants(sess, g.as_graph_def(), out)
  File ""/Library/Python/2.7/site-packages/tensorflow/python/framework/graph_util_impl.py"", line 232, in convert_variables_to_constants
    inference_graph = extract_sub_graph(input_graph_def, output_node_names)
  File ""/Library/Python/2.7/site-packages/tensorflow/python/framework/graph_util_impl.py"", line 174, in extract_sub_graph
    _assert_nodes_are_present(name_to_node, dest_nodes)
  File ""/Library/Python/2.7/site-packages/tensorflow/python/framework/graph_util_impl.py"", line 132, in _assert_nodes_are_present
    for d in nodes:
  File ""/Library/Python/2.7/site-packages/tensorflow/python/framework/ops.py"", line 431, in __iter__
    ""Tensor objects are not iterable when eager execution is not ""
TypeError: Tensor objects are not iterable when eager execution is not enabled. To iterate over this tensor use tf.map_fn.

```
After enabling the eager execution mode, the error remains the same.
Not sure whether my script is correct and whether there is a way to serialize the quantized graph to a file? Any idea will be welcome.

Thanks,

--------------------------

I wrote a script to try to evaluate the accuracy of the quantized model over a test dataset.
But a ""FailedPreconditionError"" error raised. Note that I'm using the official inceptionv3 model, and the output node is named ""InceptionV3/Logits/SpatialSqueeze"".
I wonder whether there will be a demo about how to use the quantized graph to do the prediction?
Let me update the full log
```

2018-09-25 10:51:49.521178: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-09-25 10:51:49.700182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: 
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:84:00.0
totalMemory: 11.91GiB freeMemory: 11.75GiB
2018-09-25 10:51:49.700336: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2018-09-25 10:51:50.023026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-09-25 10:51:50.023106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 
2018-09-25 10:51:50.023120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N 
2018-09-25 10:51:50.023437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11364 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:84:00.0, compute capability: 6.1)
Traceback (most recent call last):
  File ""test.py"", line 61, in <module>
    val = sess.run(out, {inp: preProcessImage(img_path)})
  File ""/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 877, in run
    run_metadata_ptr)
  File ""/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1272, in _do_run
    run_metadata)
  File ""/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value InceptionV3/InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/weights_quant/min
	 [[Node: InceptionV3/InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/weights_quant/min/read = Identity[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](InceptionV3/InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/weights_quant/min)]]
	 [[Node: InceptionV3/Logits/SpatialSqueeze/_755 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_2389_InceptionV3/Logits/SpatialSqueeze"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

Caused by op u'InceptionV3/InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/weights_quant/min/read', defined at:
  File ""test.py"", line 50, in <module>
    quantize_graph.experimental_create_eval_graph(input_graph = g,weight_bits = 8,activation_bits = 8,quant_delay=None,scope=None)
  File ""/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/contrib/quantize/python/quantize_graph.py"", line 228, in experimental_create_eval_graph
    scope=scope)
  File ""/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/contrib/quantize/python/quantize_graph.py"", line 73, in _create_graph
    scope=scope)
  File ""/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/contrib/quantize/python/quantize.py"", line 94, in Quantize
    consumer_scope=scope)
  File ""/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/contrib/quantize/python/quantize.py"", line 545, in _InsertQuantOp
    name_prefix=name_prefix))
  File ""/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 183, in func_with_args
    return func(*args, **current_args)
  File ""/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/contrib/quantize/python/quant_ops.py"", line 101, in LastValueQuantize
    trainable=False)
  File ""/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 183, in func_with_args
    return func(*args, **current_args)
  File ""/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/variables.py"", line 297, in model_variable
    use_resource=use_resource)
  File ""/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 183, in func_with_args
    return func(*args, **current_args)
  File ""/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/variables.py"", line 252, in variable
    use_resource=use_resource)
  File ""/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 1467, in get_variable
    aggregation=aggregation)
  File ""/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 1217, in get_variable
    aggregation=aggregation)
  File ""/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 527, in get_variable
    aggregation=aggregation)
  File ""/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 481, in _true_getter
    aggregation=aggregation)
  File ""/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 903, in _get_single_variable
    aggregation=aggregation)
  File ""/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 2443, in variable
    aggregation=aggregation)
  File ""/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 2425, in <lambda>
    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)
  File ""/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 2406, in default_variable_creator
    constraint=constraint)
  File ""/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/python/ops/variables.py"", line 259, in __init__
    constraint=constraint)
  File ""/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/python/ops/variables.py"", line 422, in _init_from_args
    self._snapshot = array_ops.identity(self._variable, name=""read"")
  File ""/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py"", line 80, in identity
    return gen_array_ops.identity(input, name=name)
  File ""/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 3264, in identity
    ""Identity"", input=input, name=name)
  File ""/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 454, in new_func
    return func(*args, **kwargs)
  File ""/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3155, in create_op
    op_def=op_def)
  File ""/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

FailedPreconditionError (see above for traceback): Attempting to use uninitialized value InceptionV3/InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/weights_quant/min
	 [[Node: InceptionV3/InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/weights_quant/min/read = Identity[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](InceptionV3/InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/weights_quant/min)]]
	 [[Node: InceptionV3/Logits/SpatialSqueeze/_755 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_2389_InceptionV3/Logits/SpatialSqueeze"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

```
"
22378,Stop the gradient computation for Keras eager execution mode,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Colab
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: 
- **TensorFlow version (use command below)**: 1.10.1
- **Python version**: python 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
I  am facing problems when implementing some custom loss function in Keras based on the eager execution mode. The problem is that some loss functions require to stop the gradient computation for some specific variables. When executed in a graph, we can use the op tf.stop_gradient. In fact, for Keras the GradientTape is internally handled by the function [`_process_single_batch` ](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training_eager.py)

### Source code / logs
Here is an example : 
   
```
    def virtual_adversarial_loss(X, DAE_encoder):
        r_vadv = generate_virtual_adversarial_perturbation(X, DAE_encoder)
        tape.reset()
        tape.watch(X)
        tape.watch(r_vadv)
        p =  DAE_encoder(X)
        p = tape.watch(p)
        q = DAE_encoder(x+r_vadv)
        loss = kl(p, q)
        return tf.identity(loss, name=""vat_loss"")
```

```
    def virtual_adversarial_loss(X, DAE_encoder):
        r_vadv = generate_virtual_adversarial_perturbation(X, DAE_encoder)
        tf.stop_gradient(X)
        tf.stop_gradient(r_vadv)
        p =  DAE_encoder(X)
        p = tf.stop_gradient(p)
        q = DAE_encoder(x+r_vadv)
        loss = kl(p, q)
        return tf.identity(loss, name=""vat_loss"")
```

Both of these functions do not work. The problem with the first one is that I can't use the tape variable created by the internal Keras wrapper function [`_process_single_batch` ](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training_eager.py). The problem with the second one is that I can't use tf.stop_gradient for the eager execution mode.

Is there a way to stop the gradient computation in Keras eager execution mode?"
22377,tflite can't ResizeInputTensor size,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:16.04.1-Ubuntu
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:N/A
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.11.0rc0
- **Python version**:3.5
- **Bazel version (if compiling from source)**:0.16.1
- **GCC/Compiler version (if compiling from source)**: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
- **CUDA/cuDNN version**:N/A
- **GPU model and memory**:N/A
- **Exact command to reproduce**:See below
### Describe the problem
When I use tflite,I set ResizeInputTensor different from model, error show:
```
tensorflow/contrib/lite/kernels/reshape.cc:58 num_input_elements != num_output_elements (880 != 440)
Node number 0 (RESHAPE) failed to prepare.
```
fisrt,my net input is [?,440], I convert it by set intput to [1,440]. I want to use it in forward with different size [n,440].Maybe I conert it to tflite wrong?
### Source code / logs
*my code:
```
int main() {

    // load model
    std::unique_ptr<tflite::FlatBufferModel> model;
    std::unique_ptr<tflite::Interpreter> interpreter;
    model = tflite::FlatBufferModel::BuildFromFile(""./moble.tflite"");
    if (!model) {
	cout << ""load model err."" << endl;
	exit(-1);
    }

    // build op
    tflite::ops::builtin::BuiltinOpResolver resolver;

    tflite::InterpreterBuilder (*model, resolver)(&interpreter);
    if (!interpreter) {
	cout << ""Failed to construct interpreter\n"" << endl;
	exit(-1);
    }

    interpreter->UseNNAPI(0);
    interpreter->SetNumThreads(1);

    // allocate tensors space
    int input = interpreter->inputs()[0];
    interpreter->ResizeInputTensor(input, {2,440});
    if (interpreter->AllocateTensors() != kTfLiteOk) {
	cout << ""Failed to allocate tensors.\n"";
	exit(-1);
    }

    // set input
    float *in = interpreter->typed_tensor<float>(input);
    for (int i = 0; i < 440 * 2; i++) {
	in[i] = i + 1.0f;
    }

    // inference
    if (interpreter->Invoke() != kTfLiteOk) {
	cout << ""Failed to invoke!\n"";
	exit(-1);
    }

    // get result
    float *output = interpreter->typed_output_tensor<float>(0);
    for (int i = 0; i < 200; i++) {
	cout << output[i] << "" "";
    }
    cout << endl;
    return 0;
}
```
* python convert script
** my net
```
bazel run tensorflow/tools/graph_transforms:summarize_graph -- --in_graph=/home/aisp/work_mirror/tensorflow/model/cnn/vocal_print_model/graph_vprint.pb
```
result
```
Found 1 possible inputs: (name=ac_input, type=float(1), shape=[?,440]) 
No variables spotted.
Found 1 possible outputs: (name=Inference/final_output/output, op=Identity) 
Found 382867 (382.87k) const parameters, 0 (0) variable parameters, and 0 control_edges
Op types used: 50 Const, 13 Reshape, 13 Identity, 12 Transpose, 6 BiasAdd, 4 Conv2D, 4 Relu, 3 ConcatV2, 3 Mul, 2 GatherV2, 2 Add, 2 Cast, 2 Prod, 2 Mean, 2 MaxPool, 2 MatMul, 1 Pack, 1 Placeholder, 1 Range, 1 ListDiff, 1 Less, 1 Shape, 1 Sqrt, 1 Sub, 1 GreaterEqual
```
** transform net
```
bazel run tensorflow/tools/graph_transforms:transform_graph -- --in_graph=/home/aisp/work_mirror/tensorflow/model/cnn/vocal_print_model/graph_vprint.pb --out_graph=/home/aisp/work_mirror/tensorflow/model/cnn/vocal_print_model/transformed_graph_simple.pb --inputs='ac_input' --outputs='Inference/final_output/output' --transforms='strip_unused_nodes(type=float,shape=""1,440"") fold_constants(ingore_errors=true) fold_batch_norms fold_old_batch_norms'
```
** convert to lite
```
tflite_convert   --output_file=$(pwd)/model/cnn/vocal_print_model/moble.tflite  --graph_def_file=$(pwd)/model/cnn/vocal_print_model/transformed_graph_simple.pb   --input_arrays=ac_input   --output_arrays=Inference/final_output/output --input_shapes=1,440
```"
22376,Failed to build TF from source with MPI support,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04 LTS
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.11.0-rc0
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: 0.17.1
- **GCC/Compiler version (if compiling from source)**: 7.3.0
- **CUDA/cuDNN version**: 9.2/7.2
- **GPU model and memory**: Asus GTX 1080Ti
- **Exact command to reproduce**: See below

### Describe the problem
I canno't seem to compile tensorflow from source when I try to enable MPI support. The build always fails.
```
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by com.google.protobuf.UnsafeUtil (file:/home/stefan/.cache/bazel/_bazel_stefan/install/28c1d2ace0add449e21862ae9f2d2289/_embedded_binaries/A-server.jar) to field java.lang.String.value
WARNING: Please consider reporting this to the maintainers of com.google.protobuf.UnsafeUtil
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.17.1 installed.
Please specify the location of python. [Default is /home/stefan/.tmp/venv3/bin/python]: 


Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
AttributeError: module 'site' has no attribute 'getsitepackages'
Found possible Python library paths:
  /home/stefan/.tmp/venv3/lib/python3.6/site-packages
Please input the desired Python library path to use.  Default is [/home/stefan/.tmp/venv3/lib/python3.6/site-packages]

Do you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: 
jemalloc as malloc support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: 
Google Cloud Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Hadoop File System support? [Y/n]: 
Hadoop File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Amazon AWS Platform support? [Y/n]: 
Amazon AWS Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Apache Kafka Platform support? [Y/n]: 
Apache Kafka Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with XLA JIT support? [y/N]: 
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with GDR support? [y/N]: 
No GDR support will be enabled for TensorFlow.

Do you wish to build TensorFlow with VERBS support? [y/N]: 
No VERBS support will be enabled for TensorFlow.

Do you wish to build TensorFlow with nGraph support? [y/N]: 
No nGraph support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: 
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]: 9.2


Please specify the location where CUDA 9.2 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 


Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: 7.2


Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 


Do you wish to build TensorFlow with TensorRT support? [y/N]: 
No TensorRT support will be enabled for TensorFlow.

Please specify the NCCL version you want to use. If NCCL 2.2 is not installed, then you can use version 1.3 that can be fetched automatically but it may have worse performance with multiple GPUs. [Default is 2.2]: 


Please specify the location where NCCL 2 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:/usr/local/cuda/nccl


Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 6.1]: 


Do you want to use clang as CUDA compiler? [y/N]: 
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 


Do you wish to build TensorFlow with MPI support? [y/N]: y
MPI support will be enabled for TensorFlow.

Please specify the MPI toolkit folder. [Default is /usr/local]: 


Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: 
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See tools/bazel.rc for more details.
	--config=mkl         	# Build with MKL support.
	--config=monolithic  	# Config for mostly static monolithic build.
Configuration finished
```
This is my configuration. Note that the build succeeds with MPI set to no.
Error message when build fails:
```
ERROR: /home/stefan/.tmp/tensorflow/tensorflow/contrib/mpi/BUILD:60:1: C++ compilation of rule '//tensorflow/contrib/mpi:mpi_rendezvous_mgr' failed (Exit 1)
In file included from ./tensorflow/core/framework/common_shape_fns.h:22:0,
                 from ./tensorflow/core/framework/resource_mgr.h:24,
                 from ./tensorflow/core/common_runtime/device.h:43,
                 from ./tensorflow/core/common_runtime/device_mgr.h:24,
                 from ./tensorflow/core/distributed_runtime/worker_session.h:21,
                 from ./tensorflow/core/distributed_runtime/base_rendezvous_mgr.h:24,
                 from ./tensorflow/contrib/mpi/mpi_rendezvous_mgr.h:35,
                 from tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc:18:
./tensorflow/core/util/tensor_format.h: In function 'tensorflow::TensorShape tensorflow::ShapeFromFormat(tensorflow::TensorFormat, tensorflow::int64, tensorflow::gtl::ArraySlice<long long int>, tensorflow::int64)':
./tensorflow/core/util/tensor_format.h:501:45: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (format == FORMAT_NHWC_VECT_W && dim == spatial.size() - 1) {
                                         ~~~~^~~~~~~~~~~~~~~~~~~~~
In file included from tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc:18:0:
./tensorflow/contrib/mpi/mpi_rendezvous_mgr.h: In member function 'void tensorflow::MPISendTensorCall::Init(const tensorflow::Rendezvous::ParsedKey&, tensorflow::int64, bool)':
./tensorflow/contrib/mpi/mpi_rendezvous_mgr.h:74:36: error: 'using StringPiece = class absl::string_view {aka class absl::string_view}' has no member named 'ToString'
     mRes_.set_key(parsed.FullKey().ToString());
                                    ^~~~~~~~
tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc: In member function 'virtual void tensorflow::MPIRemoteRendezvous::RecvFromRemoteAsync(const tensorflow::Rendezvous::ParsedKey&, const tensorflow::Rendezvous::Args&, tensorflow::Rendezvous::DoneCallback)':
tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc:139:38: error: 'using StringPiece = class absl::string_view {aka class absl::string_view}' has no member named 'ToString'
   mgr->QueueRequest(parsed.FullKey().ToString(), step_id_,
                                      ^~~~~~~~
tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc: In lambda function:
tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc:261:45: error: 'using StringPiece = class absl::string_view {aka class absl::string_view}' has no member named 'ToString'
         SendQueueEntry req(parsed.FullKey().ToString().c_str(), std::move(res));
                                             ^~~~~~~~
In file included from ./tensorflow/core/platform/default/logging.h:24:0,
                 from ./tensorflow/core/platform/logging.h:25,
                 from ./tensorflow/contrib/mpi/mpi_utils.h:25,
                 from ./tensorflow/contrib/mpi/mpi_rendezvous_mgr.h:34,
                 from tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc:18:
./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetTensorDim(tensorflow::gtl::ArraySlice<T>, tensorflow::TensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':
./tensorflow/core/util/tensor_format.h:452:47:   required from here
./tensorflow/core/util/tensor_format.h:420:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   CHECK(index >= 0 && index < dimension_attributes.size())
./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
./tensorflow/core/util/tensor_format.h:420:3: note: in expansion of macro 'CHECK'
   CHECK(index >= 0 && index < dimension_attributes.size())
   ^
./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetFilterDim(tensorflow::gtl::ArraySlice<T>, tensorflow::FilterTensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':
./tensorflow/core/util/tensor_format.h:461:54:   required from here
./tensorflow/core/util/tensor_format.h:435:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   CHECK(index >= 0 && index < dimension_attribute.size())
./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
./tensorflow/core/util/tensor_format.h:435:3: note: in expansion of macro 'CHECK'
   CHECK(index >= 0 && index < dimension_attribute.size())
   ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 863.199s, Critical Path: 65.99s
INFO: 3852 processes: 3852 local.
FAILED: Build did NOT complete successfully
```
It says that StringPiece has no member ToString.
I installed MPI with:
```
wget https://www.open-mpi.org/software/ompi/v3.0/downloads/openmpi-3.1.2.tar.gz
tar -xvf openmpi-3.1.2.tar.gz
cd openmpi-3.1.2
./configure --disable-mpi-fortran --with-cuda=/usr/local/cuda/ --prefix /usr/local/
make
make install
```
"
22375,Tensorflow Object Detection - Car is wrongly detected as N/A,"In Google-colab I am trying to detect car using Tensorflow Object-Detection API with SSD_mobilenet_v1_pets.config, it detect humans as car and car as N/A. The following are the  size config and image dimensions: 

anchor_generator {
      ssd_anchor_generator {
        num_layers: 6
        min_scale: 0.2
        max_scale: 0.95
        aspect_ratios: 1.0
        aspect_ratios: 2.0
        aspect_ratios: 0.5
        aspect_ratios: 3.0
        aspect_ratios: 0.3333
      }
    }
    image_resizer {
      fixed_shape_resizer {
        height: 300
        width: 300
      }


I have 1160 images with various dimensions (ex: 73 x 63, 118 x 62, 62 x 56, 71 x 56, 276 x 183, 259 x 184, 318 x 159, 700 x 420, 647 x 407, 897 x 554) 

This is the output I got:[enter image description here][1][enter image description here][2]


Please clarify, Is the problem of the wrong detection of car is because of the image dimensions or anything else?


  [1]: https://i.stack.imgur.com/eeau4.png
  [2]: https://i.stack.imgur.com/flkbA.png
![download](https://user-images.githubusercontent.com/39641762/45741779-186e8a00-bc16-11e8-92bf-462d46c20acb.png)
![download 1](https://user-images.githubusercontent.com/39641762/45741856-3dfb9380-bc16-11e8-9845-ec459818ebea.png)
"
22374,*** stack smashing detected ***: <unknown> terminated  (),"## describe problems
1. ubuntu 18.04 cmake
2. c++ with libtensorflow_cc.so
3. I can compile my code,but when I run it,the error is ""*** stack smashing detected ***: <unknown> terminated  ()""

## code
#include ""tensorflow/core/public/session.h""
#include ""tensorflow/core/platform/env.h""

using namespace std;
using namespace chrono;
using namespace tensorflow;
int main(int argc, char* argv[]) {
  // SessionOptions options;
  // std::unique_ptr<tensorflow::Session> session(tensorflow::NewSession(options));
  Session *session = NewSession(SessionOptions());
  if(session == nullptr)
  {
    throw runtime_error(""Could not create tensorflow session"");
  }
  session->Close();
  return 0;
}
## So, what should I do?Thank you!"
22373,AttributeError: 'TocoConverter' object has no attribute 'get_input_arrays',"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NO
- **TensorFlow installed from (source or binary)**: pip install-gpu==1.9
- **TensorFlow version (use command below)**: 1.9 GPU
- **Python version**: python27
- **Bazel version (if compiling from source)**: NO
- **CUDA/cuDNN version**: CUDA9.0 cuDNN7.0
- **GPU model and memory**: GTX1070 8G

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I want to conver the pb file to tflite file, 
according to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/python_api.md, 
I run the example codes, but failed
I run the following code
### Source code 

`import tensorflow as tf
img = tf.placeholder(name=""img"", dtype=tf.float32, shape=(1, 64, 64, 3))
const = tf.constant([1., 2., 3.]) + tf.constant([1., 4., 4.])
val = img + const
out = tf.fake_quant_with_min_max_args(val, min=0., max=1., name=""output"")

with tf.Session() as sess:
  converter = tf.contrib.lite.TocoConverter.from_session(sess, [img], [out])
  converter.inference_type = tf.contrib.lite.constants.QUANTIZED_UINT8
  input_arrays = converter.get_input_arrays()
  converter.quantized_input_stats = {input_arrays[0] : (0., 1.)}  # mean, std_dev
  tflite_model = converter.convert()
  open(""converted_model.tflite"", ""wb"").write(tflite_model)`



error occurs, logs are below
###  logs
2018-09-19 15:22:25.875027: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-09-19 15:22:25.964421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-09-19 15:22:25.964839: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.8225
pciBusID: 0000:01:00.0
totalMemory: 7.92GiB freeMemory: 7.77GiB
2018-09-19 15:22:25.964854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2018-09-19 15:22:26.147714: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-09-19 15:22:26.147746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2018-09-19 15:22:26.147752: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2018-09-19 15:22:26.147938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7502 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
Traceback (most recent call last):
  File ""test.py"", line 10, in <module>
    input_arrays = converter.get_input_arrays()
AttributeError: 'TocoConverter' object has no attribute 'get_input_arrays'


what should I do?


"
22372,crosstool_wrapper_driver_is_not_gcc failed: error executing command,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.11.0rc1
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: 0.17.1
- **GCC/Compiler version (if compiling from source)**: c++ (Ubuntu 7.3.0-16ubuntu3) 7.3.0
- **CUDA/cuDNN version**: 9.2 / 7.2.1
- **GPU model and memory**: GeForce 940MX
- **Exact command to reproduce**: bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 --config=cuda --verbose_failures //tensorflow/tools/pip_package:build_pip_package

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
**When the VERBS support is enabled the tensorflow build fails with the following error message:
~/tensorflow/tensorflow/contrib/verbs/BUILD:90:1: C++ compilation of rule '//tensorflow/contrib/verbs:rdma_rendezvous_mgr' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command**

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
ERROR: ~/Documents/dev/git/tensorflow/tensorflow/contrib/verbs/BUILD:90:1: C++ compilation of rule '//tensorflow/contrib/verbs:rdma_rendezvous_mgr' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd ~/.cache/bazel/_bazel_blablabla/cf67b2b2e967476eb2b1ee98e33ab5bd/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu \
    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \
    NCCL_INSTALL_PATH=/usr/local/nccl_2.2.13-1+cuda9.2_x86_64 \
    PATH=~/bin:/usr/local/sbin:/usr/local/lib:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/linuxbrew/.linuxbrew/bin:/home/linuxbrew/.linuxbrew/sbin:/home/linuxbrew/.linuxbrew/opt/coreutils/libexec/gnubin:/usr/local/cuda/bin:/usr/local/share/apache/hadoop/sbin:/usr/local/share/apache/hadoop/bin:/usr/local/share/apache/spark/sbin:/usr/local/share/apache/spark/bin:/usr/games:/usr/local/games:~/bin:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CUDA_CLANG=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=5.0 \
    TF_CUDA_VERSION=9.2 \
    TF_CUDNN_VERSION=7 \
    TF_NCCL_VERSION=2 \
    TF_NEED_CUDA=1 \
    TF_NEED_OPENCL_SYCL=0 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/contrib/verbs/_objs/rdma_rendezvous_mgr/rdma_rendezvous_mgr.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/contrib/verbs/_objs/rdma_rendezvous_mgr/rdma_rendezvous_mgr.pic.o' '-DGRPC_ARES=0' '-DPB_FIELD_16BIT=1' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -DTENSORFLOW_USE_JEMALLOC -DTF_USE_SNAPPY -DTENSORFLOW_USE_VERBS -DTENSORFLOW_USE_GDR -DCURL_STATICLIB -DPLATFORM_LINUX -DENABLE_CURL_CLIENT -DENABLE_NO_ENCRYPTION -iquote . -iquote bazel-out/k8-opt/genfiles -iquote bazel-out/k8-opt/bin -iquote external/protobuf_archive -iquote bazel-out/k8-opt/genfiles/external/protobuf_archive -iquote bazel-out/k8-opt/bin/external/protobuf_archive -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -iquote bazel-out/k8-opt/bin/external/bazel_tools -iquote external/grpc -iquote bazel-out/k8-opt/genfiles/external/grpc -iquote bazel-out/k8-opt/bin/external/grpc -iquote external/zlib_archive -iquote bazel-out/k8-opt/genfiles/external/zlib_archive -iquote bazel-out/k8-opt/bin/external/zlib_archive -iquote external/boringssl -iquote bazel-out/k8-opt/genfiles/external/boringssl -iquote bazel-out/k8-opt/bin/external/boringssl -iquote external/com_google_absl -iquote bazel-out/k8-opt/genfiles/external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote bazel-out/k8-opt/bin/external/nsync -iquote external/jemalloc -iquote bazel-out/k8-opt/genfiles/external/jemalloc -iquote bazel-out/k8-opt/bin/external/jemalloc -iquote external/eigen_archive -iquote bazel-out/k8-opt/genfiles/external/eigen_archive -iquote bazel-out/k8-opt/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/k8-opt/genfiles/external/local_config_sycl -iquote bazel-out/k8-opt/bin/external/local_config_sycl -iquote external/gif_archive -iquote bazel-out/k8-opt/genfiles/external/gif_archive -iquote bazel-out/k8-opt/bin/external/gif_archive -iquote external/jpeg -iquote bazel-out/k8-opt/genfiles/external/jpeg -iquote bazel-out/k8-opt/bin/external/jpeg -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/genfiles/external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/k8-opt/genfiles/external/farmhash_archive -iquote bazel-out/k8-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-opt/genfiles/external/fft2d -iquote bazel-out/k8-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/k8-opt/genfiles/external/highwayhash -iquote bazel-out/k8-opt/bin/external/highwayhash -iquote external/local_config_cuda -iquote bazel-out/k8-opt/genfiles/external/local_config_cuda -iquote bazel-out/k8-opt/bin/external/local_config_cuda -iquote external/double_conversion -iquote bazel-out/k8-opt/genfiles/external/double_conversion -iquote bazel-out/k8-opt/bin/external/double_conversion -iquote external/curl -iquote bazel-out/k8-opt/genfiles/external/curl -iquote bazel-out/k8-opt/bin/external/curl -iquote external/jsoncpp_git -iquote bazel-out/k8-opt/genfiles/external/jsoncpp_git -iquote bazel-out/k8-opt/bin/external/jsoncpp_git -iquote external/aws -iquote bazel-out/k8-opt/genfiles/external/aws -iquote bazel-out/k8-opt/bin/external/aws -isystem external/protobuf_archive/src -isystem bazel-out/k8-opt/genfiles/external/protobuf_archive/src -isystem bazel-out/k8-opt/bin/external/protobuf_archive/src -isystem external/grpc/include -isystem bazel-out/k8-opt/genfiles/external/grpc/include -isystem bazel-out/k8-opt/bin/external/grpc/include -isystem external/zlib_archive -isystem bazel-out/k8-opt/genfiles/external/zlib_archive -isystem bazel-out/k8-opt/bin/external/zlib_archive -isystem external/grpc/third_party/address_sorting/include -isystem bazel-out/k8-opt/genfiles/external/grpc/third_party/address_sorting/include -isystem bazel-out/k8-opt/bin/external/grpc/third_party/address_sorting/include -isystem external/boringssl/src/include -isystem bazel-out/k8-opt/genfiles/external/boringssl/src/include -isystem bazel-out/k8-opt/bin/external/boringssl/src/include -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem external/jemalloc/include -isystem bazel-out/k8-opt/genfiles/external/jemalloc/include -isystem bazel-out/k8-opt/bin/external/jemalloc/include -isystem external/eigen_archive -isystem bazel-out/k8-opt/genfiles/external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/gif_archive/lib -isystem bazel-out/k8-opt/genfiles/external/gif_archive/lib -isystem bazel-out/k8-opt/bin/external/gif_archive/lib -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/genfiles/external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/com_google_absl -isystem bazel-out/k8-opt/genfiles/external/com_google_absl -isystem bazel-out/k8-opt/bin/external/com_google_absl -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include/crt -isystem external/double_conversion -isystem bazel-out/k8-opt/genfiles/external/double_conversion -isystem bazel-out/k8-opt/bin/external/double_conversion -isystem external/curl/include -isystem bazel-out/k8-opt/genfiles/external/curl/include -isystem bazel-out/k8-opt/bin/external/curl/include -isystem external/jsoncpp_git/include -isystem bazel-out/k8-opt/genfiles/external/jsoncpp_git/include -isystem bazel-out/k8-opt/bin/external/jsoncpp_git/include -isystem external/aws/aws-cpp-sdk-core/include -isystem bazel-out/k8-opt/genfiles/external/aws/aws-cpp-sdk-core/include -isystem bazel-out/k8-opt/bin/external/aws/aws-cpp-sdk-core/include -isystem external/aws/aws-cpp-sdk-kinesis/include -isystem bazel-out/k8-opt/genfiles/external/aws/aws-cpp-sdk-kinesis/include -isystem bazel-out/k8-opt/bin/external/aws/aws-cpp-sdk-kinesis/include -isystem external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/k8-opt/genfiles/external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/k8-opt/bin/external/aws/aws-cpp-sdk-s3/include '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -mavx -mavx2 -mfma '-mfpmath=both' -msse4.2 -c tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc -o bazel-out/k8-opt/bin/tensorflow/contrib/verbs/_objs/rdma_rendezvous_mgr/rdma_rendezvous_mgr.pic.o)
In file included from ./tensorflow/core/framework/common_shape_fns.h:22:0,
                 from ./tensorflow/core/framework/resource_mgr.h:24,
                 from ./tensorflow/core/common_runtime/device.h:43,
                 from ./tensorflow/core/common_runtime/device_mgr.h:24,
                 from ./tensorflow/core/distributed_runtime/worker_session.h:21,
                 from ./tensorflow/core/distributed_runtime/base_rendezvous_mgr.h:24,
                 from ./tensorflow/contrib/verbs/rdma_rendezvous_mgr.h:22,
                 from tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc:18:
./tensorflow/core/util/tensor_format.h: In function 'tensorflow::TensorShape tensorflow::ShapeFromFormat(tensorflow::TensorFormat, tensorflow::int64, tensorflow::gtl::ArraySlice<long long int>, tensorflow::int64)':
./tensorflow/core/util/tensor_format.h:501:45: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (format == FORMAT_NHWC_VECT_W && dim == spatial.size() - 1) {
                                         ~~~~^~~~~~~~~~~~~~~~~~~~~
tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc: In member function 'virtual void tensorflow::RdmaRemoteRendezvous::RecvFromRemoteAsync(const tensorflow::Rendezvous::ParsedKey&, const tensorflow::Rendezvous::Args&, tensorflow::Rendezvous::DoneCallback)':
tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc:66:41: error: 'using StringPiece = class absl::string_view {aka class absl::string_view}' has no member named 'ToString'
   string key(std::move(parsed.FullKey().ToString()));
                                         ^~~~~~~~
In file included from ./tensorflow/core/platform/default/logging.h:24:0,
                 from ./tensorflow/core/platform/logging.h:25,
                 from ./tensorflow/core/lib/core/refcount.h:22,
                 from ./tensorflow/core/platform/tensor_coding.h:21,
                 from ./tensorflow/core/framework/resource_handle.h:19,
                 from ./tensorflow/core/framework/types.h:31,
                 from ./tensorflow/contrib/verbs/verbs_util.h:21,
                 from ./tensorflow/contrib/verbs/rdma.h:30,
                 from ./tensorflow/contrib/verbs/rdma_mgr.h:24,
                 from ./tensorflow/contrib/verbs/rdma_rendezvous_mgr.h:21,
                 from tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc:18:
./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetTensorDim(tensorflow::gtl::ArraySlice<T>, tensorflow::TensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':
./tensorflow/core/util/tensor_format.h:452:47:   required from here
./tensorflow/core/util/tensor_format.h:420:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   CHECK(index >= 0 && index < dimension_attributes.size())
./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
./tensorflow/core/util/tensor_format.h:420:3: note: in expansion of macro 'CHECK'
   CHECK(index >= 0 && index < dimension_attributes.size())
   ^
./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetFilterDim(tensorflow::gtl::ArraySlice<T>, tensorflow::FilterTensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':
./tensorflow/core/util/tensor_format.h:461:54:   required from here
./tensorflow/core/util/tensor_format.h:435:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   CHECK(index >= 0 && index < dimension_attribute.size())
./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
./tensorflow/core/util/tensor_format.h:435:3: note: in expansion of macro 'CHECK'
   CHECK(index >= 0 && index < dimension_attribute.size())
   ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 7400.912s, Critical Path: 180.41s
INFO: 10108 processes: 10108 local.
FAILED: Build did NOT complete successfully
```"
22370,model_to_estimator not working on float16,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
   custom code
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
    Linux
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
   N/A
- **TensorFlow installed from (source or binary)**:
   Binary
- **TensorFlow version (use command below)**:
    Codalab
- **Python version**:
   Codalb
- **Bazel version (if compiling from source)**:
  N/A
- **GCC/Compiler version (if compiling from source)**:
  N/A
- **CUDA/cuDNN version**:
  N/A
- **GPU model and memory**:
  N/a
- **Exact command to reproduce**:
  You can run exactly this code on colab

### Describe the problem
Using float16 crash `model_to_estimator`

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
from tensorflow import keras as ks
import numpy as np
import tensorflow as tf
from tensorflow.python.estimator import keras as keras_lib

ks.backend.clear_session()
ks.backend.set_floatx('float16')

tf.logging.set_verbosity(tf.logging.INFO)

my_input_fn = tf.estimator.inputs.numpy_input_fn(
    x={""digit_a"": np.array(np.random.rand(1, 27, 27, 1).astype(np.float32))},
    y={""mydense"": np.array(np.random.rand(1,1).astype(np.float32))},
      batch_size=500,
      num_epochs=1,
      shuffle=False)

# First, define the modules
digit_input = ks.Input(shape=(299, 299, 3), name=""digit_input"")
vision_model= ks.applications.xception.Xception(input_tensor=digit_input, include_top=False, weights='imagenet',classes=1)


# Then define the tell-digits-apart model
digit_a = ks.Input(shape=(299, 299, 3), name=""digit_a"")

# The vision model will be shared, weights and all
out_a = vision_model(digit_a)


out = ks.layers.Dense(1, activation='sigmoid', name=""mydense"")(out_a)

m2 = ks.Model([digit_a], out, name=""Xception_enc"")

m2.compile(loss={ 'mydense': 'binary_crossentropy'},optimizer=tf.keras.optimizers.Adam())


est = keras_lib.model_to_estimator(
            keras_model=m2,
            config=tf.estimator.RunConfig(session_config=tf.ConfigProto(log_device_placement=True)))
````

LOG:

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-9-03750f8e4923> in <module>()
     37 est = keras_lib.model_to_estimator(
     38             keras_model=m2,
---> 39             config=tf.estimator.RunConfig(session_config=tf.ConfigProto(log_device_placement=True)))
     40 
     41 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/keras.py in model_to_estimator(keras_model, keras_model_path, custom_objects, model_dir, config)
    546                            estimator,
    547                            custom_objects,
--> 548                            keras_weights)
    549   elif keras_model.built:
    550     logging.warning('You are creating an Estimator from a Keras model '

/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/keras.py in _save_first_checkpoint(keras_model, estimator, custom_objects, keras_weights)
    455         if not model.train_function:
    456           # pylint: disable=protected-access
--> 457           model._make_train_function()
    458           K._initialize_variables(sess)
    459           # pylint: enable=protected-access

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _make_train_function(self)
    596           # Training updates
    597           updates = self.optimizer.get_updates(
--> 598               params=self._collected_trainable_weights, loss=self.total_loss)
    599         # Unconditional updates
    600         updates += self.get_updates_for(None)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizers.py in get_updates(self, loss, params)
    478 
    479     for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):
--> 480       m_t = (self.beta_1 * m) + (1. - self.beta_1) * g
    481       v_t = (self.beta_2 * v) + (1. - self.beta_2) * math_ops.square(g)
    482       if self.amsgrad:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py in _run_op(a, *args)
    856       # pylint: disable=protected-access
    857       value = a._AsTensor()
--> 858       return tensor_oper(value, *args)
    859 
    860     # Propagate __doc__ to wrapper

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py in binary_op_wrapper(x, y)
    851       elif not isinstance(y, sparse_tensor.SparseTensor):
    852         try:
--> 853           y = ops.convert_to_tensor(y, dtype=x.dtype.base_dtype, name=""y"")
    854         except TypeError:
    855           # If the RHS is not a tensor, it might be a tensor aware object

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype)
    996       name=name,
    997       preferred_dtype=preferred_dtype,
--> 998       as_ref=False)
    999 
   1000 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)
   1092 
   1093     if ret is None:
-> 1094       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1095 
   1096     if ret is NotImplemented:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py in _TensorConversionFunction(v, dtype, name, as_ref)
    796       raise ValueError(
    797           ""Incompatible type conversion requested to type '%s' for variable ""
--> 798           ""of type '%s'"" % (dtype.name, v.dtype.name))
    799     if as_ref:
    800       return v._ref()  # pylint: disable=protected-access

ValueError: Incompatible type conversion requested to type 'float16' for variable of type 'float32'
```
/cc @tanzhenyu "
22369,"when use tensorflow to predict images with different resolution,the forward's cost  unstable","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.10.1
- **Python version**:3.5
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:9.0/7.1
- **GPU model and memory**:Tesla P4 ,8GB
- **Exact command to reproduce**:no

### Describe the problem
when use tensorflow to predict images with different resolution,the forward's cost  unstable. my image long side is less than 1280 , the resize code below:
```
 def resize_im(self, im, scale=600, max_scale=1280): 
      f = 1.0
      if scale > min(im.shape[0], im.shape[1]): 
        f = float(scale) / min(im.shape[0], im.shape[1]) 
      if max_scale != None and f * max(im.shape[0], im.shape[1]) > max_scale: 
        f = float(max_scale) / max(im.shape[0], im.shape[1]) 
      return cv2.resize(im, None, None, fx=f, fy=f, interpolation=cv2.INTER_LINEAR), f 
```

[use different resolution.txt](https://github.com/tensorflow/tensorflow/files/2395362/use.different.resolution.txt)
see the net_fw

when padding image to 1280*1280, the forward's cost stable,between 0.25 and 0.3,but the small image will cost more.

I think it is BFCs problem.
How can I solve this problem?I think if tensorflow can lock the chunk's memory (use input image with resolution 1280*1280) for all layers , this problem could be sovled
"
22367,a problem about using the tflite_convert,"### System information
- **Have I written custom code**:  NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NO
- **TensorFlow installed from (source or binary)**: pip install tensorflow-gpu==1.9
- **TensorFlow version (use command below)**: 1.9 GPU
- **Python version**: python 27
- **Bazel version (if compiling from source)**: NO
- **GCC/Compiler version (if compiling from source)**: 
- **CUDA/cuDNN version**: CUDA9.0 cuDNN 7.0
- **GPU model and memory**: GTX1070 8G

### Describe the problem
   I want to conver the inception_v3 from .ckpt to .tflite, but I meet a problem when I conver the .pb to .tflite,    I use the following method

 1)  download the inception_v3_2016_08_28.tar.gz from  https://github.com/tensorflow/models/tree/master/research/slim
decompression it and move to /tmp

2)  Export nception_v3_inf_graph.pb file that without parameters
` cd /mypath/models-master/research/slim`
`python export_inference_graph.py --model_name=inception_v3 --output_file=/tmp/inception_v3_inf_graph.pb`

3)  freezon the pb file
`cd /mypath/tensorflow-master/tensorflow/python/training`
`python freeze_graph.py --input_graph=/tmp/inception_v3_inf_graph.pb --input_checkpoint=/tmp/inception_v3.ckpt --output_graph=/tmp/inception_v3_frozen_graph.pb  --input_binary=true --output_node_names=InceptionV3/Predictions/Reshape_1`

4)  quantized pb file
`cd /mypath/tensorflow-master/tensorflow/tools/quantization`
`python quantize_graph.py --input=/tmp/inception_v3_frozen_graph.pb --output=/tmp/inception_v3_quantized.pb --output_node_names=""InceptionV3/Predictions/Reshape_1"" --mode=eightbit`

above steps all successed, but

5)  .pb-->. tflite
`tflite_convert  --output_file=/tmp/inception_v3_quantized.tflite  --graph_def_file=/tmp/inception_v3_quantized.pb --inference_type=QUANTIZED_UINT8 --input_arrays=input --output_arrays=InceptionV3/Predictions/Reshape_1 --mean_values=128  --std_dev_values=127`

### Source code
`tflite_convert  --output_file=/tmp/inception_v3_quantized.tflite  --graph_def_file=/tmp/inception_v3_quantized.pb --inference_type=QUANTIZED_UINT8 --input_arrays=input --output_arrays=InceptionV3/Predictions/Reshape_1 --mean_values=128  --std_dev_values=127`

### Source  logs

2018-09-19 10:54:02.077094: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-09-19 10:54:02.163219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-09-19 10:54:02.163667: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.8225
pciBusID: 0000:01:00.0
totalMemory: 7.92GiB freeMemory: 7.77GiB
2018-09-19 10:54:02.163682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2018-09-19 10:54:02.346515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-09-19 10:54:02.346548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2018-09-19 10:54:02.346555: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2018-09-19 10:54:02.346742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7502 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
Traceback (most recent call last):
  File ""/home/icare/.local/bin/tflite_convert"", line 11, in <module>
    sys.exit(main())
  File ""/home/icare/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 320, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/home/icare/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/home/icare/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 316, in run_main
    _convert_model(tflite_flags)
  File ""/home/icare/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 121, in _convert_model
    output_data = converter.convert()
  File ""/home/icare/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/lite.py"", line 309, in convert
    allow_custom_ops=self.allow_custom_ops)
  File ""/home/icare/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/convert.py"", line 225, in toco_convert
    input_data.SerializeToString())
  File ""/home/icare/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/convert.py"", line 107, in toco_convert_protos
    (stdout, stderr))
RuntimeError: TOCO failed see console for info.
2018-09-19 10:54:03.713531: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.713589: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.713603: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.713612: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.713620: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.713657: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.713668: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.713687: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.713711: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.713719: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.713741: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.713787: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.713797: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.713829: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.713841: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.713862: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.713884: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.713929: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.713939: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.713962: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedMaxPool
2018-09-19 10:54:03.713980: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.714003: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.714012: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.714034: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.714066: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.714077: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.714108: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.714120: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.714129: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.714137: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.714173: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.714183: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.714193: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedMaxPool
2018-09-19 10:54:03.714212: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.714224: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.714232: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.714241: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.714275: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.714286: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.714295: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.714314: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.714325: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.714334: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.714343: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.714376: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.714387: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.714418: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.714431: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.714440: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.714448: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.714482: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.714492: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.714501: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.714519: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.714530: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.714539: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.714548: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.714581: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.714591: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.714614: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.714626: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.714635: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.714643: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.714681: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.714692: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.714717: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.714729: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.714738: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.714746: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.714779: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.714790: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.714798: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.714808: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedAvgPool
2018-09-19 10:54:03.714827: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.714839: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.714847: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.714856: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.714889: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.714900: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.714909: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.714941: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.714953: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.714964: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.714973: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.714981: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.715015: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.715025: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.715034: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.715061: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.715073: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.715084: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.715093: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.715102: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.715135: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.715146: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.715171: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.715183: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.715192: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.715201: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.715235: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.715245: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.715254: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.715281: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.715293: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.715304: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.715312: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.715321: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.715353: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.715364: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.715387: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.715399: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.715408: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.715417: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.715451: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.715462: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.715488: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.715500: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.715508: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.715517: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.715551: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.715562: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.715570: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.715589: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.715600: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedAvgPool
2018-09-19 10:54:03.715620: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.715631: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.715640: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.715649: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.715686: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.715696: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.715705: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.715738: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.715750: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.715760: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.715769: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.715778: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.715811: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.715821: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.715829: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.715856: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.715868: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.715879: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.715888: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.715896: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.715929: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.715940: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.715965: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.715977: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.715985: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.715994: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.716027: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.716037: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.716046: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.716074: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.716085: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.716096: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.716105: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.716114: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.716148: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.716158: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.716180: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.716192: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.716201: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.716209: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.716242: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.716253: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.716278: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.716290: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.716299: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.716308: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.716341: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.716351: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.716360: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.716379: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.716390: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedAvgPool
2018-09-19 10:54:03.716410: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.716422: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.716430: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.716439: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.716473: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.716483: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.716492: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.716875: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.716890: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.716901: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.716910: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.716918: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.716955: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.716966: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.716974: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.717004: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.717016: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.717027: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.717036: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.717044: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.717077: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.717087: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.717111: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.717122: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.717132: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.717140: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.717176: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.717187: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.717213: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.717224: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.717233: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.717242: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.717276: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.717286: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.717295: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.717314: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.717324: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedMaxPool
2018-09-19 10:54:03.717335: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.717381: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.717394: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.717405: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.717414: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.717422: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.717456: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.717466: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.717474: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.717512: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.717524: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.717535: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.717543: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.717552: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.717587: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.717597: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.717627: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.717639: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.717648: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.717656: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.717690: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.717701: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.717735: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.717748: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.717759: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.717768: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.717804: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.717814: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.717823: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.717860: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.717872: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.717883: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.717892: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.717901: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.717934: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.717945: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.717975: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.717987: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.717995: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.718004: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.718039: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.718049: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.718079: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.718091: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.718100: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.718109: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.718145: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.718155: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.718185: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.718197: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.718206: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.718215: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.718262: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.718273: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.718309: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.718321: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.718330: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.718339: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.718373: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.718383: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.718392: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.718418: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.718429: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedAvgPool
2018-09-19 10:54:03.718465: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.718477: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.718486: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.718495: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.718530: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.718540: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.718549: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.718597: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.718610: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.718621: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.718629: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.718639: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.718674: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.718684: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.718693: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.718734: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.718747: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.718758: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.718767: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.718775: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.718810: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.718821: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.718857: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.718869: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.718878: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.718887: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.718923: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.718933: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.718974: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.718987: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.718996: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.719004: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.719040: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.719050: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.719059: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.719101: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.719114: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.719125: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.719134: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.719143: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.719179: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.719189: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.719225: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.719237: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.719247: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.719256: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.719291: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.719302: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.719339: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.719351: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.719360: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.719369: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.719405: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.719416: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.719452: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.719465: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.719474: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.719483: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.719518: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.719529: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.719570: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.719582: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.719592: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.719600: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.719637: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.719648: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.719656: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.719677: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.719688: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedAvgPool
2018-09-19 10:54:03.719724: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.719736: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.719745: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.719754: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.719790: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.719801: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.719809: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.719857: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.719870: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.719881: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.719890: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.719899: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.719936: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.719947: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.719955: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.719996: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.720009: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.720020: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.720028: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.720037: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.720073: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.720083: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.720119: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.720132: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.720141: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.720150: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.720187: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.720198: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.720239: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.720251: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.720260: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.720269: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.720305: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.720316: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.720325: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.720364: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.720378: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.720388: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.720397: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.720406: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.720441: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.720452: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.720488: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.720500: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.720509: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.720518: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.720554: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.720564: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.720601: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.720614: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.720623: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.720631: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.720667: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.720678: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.720715: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.720727: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.720736: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.720745: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.720780: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.720791: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.720831: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.720844: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.720852: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.720861: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.720897: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.720908: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.720917: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.720937: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.720948: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedAvgPool
2018-09-19 10:54:03.720983: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.720996: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.721005: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.721014: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.721050: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.721061: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.721070: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.721118: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.721131: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.721142: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.721151: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.721160: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.721195: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.721205: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.721214: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.721257: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.721270: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.721281: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.721289: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.721298: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.721334: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.721344: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.721390: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.721402: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.721412: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.721420: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.721456: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.721467: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.721515: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.721528: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.721537: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.721546: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.721582: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.721593: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.721602: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.721646: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.721659: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.721670: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.721679: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.721687: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.721723: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.721734: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.721779: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.721791: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.721800: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.721808: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.721845: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.721855: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.721899: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.721912: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.721921: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.721930: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.721966: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.721977: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.722021: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.722037: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.722046: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.722056: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.722091: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.722102: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.722146: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.722158: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.722167: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.722175: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.722212: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.722223: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.722232: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.722252: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.722263: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedAvgPool
2018-09-19 10:54:03.722298: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.722310: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.722319: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.722328: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.722365: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.722376: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.722385: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.722442: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.722456: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.722467: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.722475: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.722484: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.722520: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.722530: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.722611: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.722623: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.722632: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.722641: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.722680: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.722690: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.722699: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.722741: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.722754: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.722765: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.722774: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.722783: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.722819: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.722829: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.722875: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.722887: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.722896: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.722905: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.722943: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.722953: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.722998: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.723010: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.723020: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.723028: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.723065: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.723076: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.723128: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.723140: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.723149: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.723158: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.723245: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.723257: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.723266: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.723287: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.723298: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedMaxPool
2018-09-19 10:54:03.723308: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.723382: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.723395: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.723406: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.723415: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.723423: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.723460: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.723471: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.723480: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.723556: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.723570: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.723581: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.723590: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.723598: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.723634: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.723645: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.723708: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.723720: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.723729: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.723738: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.723774: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.723784: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.723793: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.723860: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.723872: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.723882: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.723890: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.723928: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.723938: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.723948: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.724042: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.724055: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.724066: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.724075: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.724084: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.724121: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.724132: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.724701: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.724719: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.724728: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.724737: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.724775: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.724786: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.724851: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.724864: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.724873: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.724881: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.724918: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.724928: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.724937: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.725000: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.725013: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.725022: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.725030: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.725068: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.725079: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.725087: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.725113: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.725124: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedAvgPool
2018-09-19 10:54:03.725171: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.725183: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.725192: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.725201: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.725237: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.725248: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.725256: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.725357: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.725371: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.725382: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.725390: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.725399: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.725435: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.725446: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.725455: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.725562: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.725576: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.725587: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.725596: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.725604: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.725643: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.725654: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.725718: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.725730: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.725739: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.725748: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.725786: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.725796: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.725805: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.725870: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.725882: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.725891: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.725899: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.725936: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.725947: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.725955: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.726322: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.726337: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.726348: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.726356: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.726364: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.726408: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.726420: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.726984: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.727001: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.727010: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.727019: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.727058: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.727069: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.727132: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.727145: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.727154: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.727163: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.727200: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.727211: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.727219: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.727282: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.727294: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.727303: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.727312: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.727348: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.727359: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.727367: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.727393: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.727404: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedAvgPool
2018-09-19 10:54:03.727464: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.727476: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.727484: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.727493: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.727529: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.727539: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.727548: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.727574: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.727585: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedAvgPool
2018-09-19 10:54:03.728329: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.728346: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.728356: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.728374: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedBiasAdd
2018-09-19 10:54:03.728384: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.728393: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.728402: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.728429: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.728439: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedReshape
2018-09-19 10:54:03.728448: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.728475: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.728485: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedReshape
2018-09-19 10:54:03.728494: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.794554: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1479 operators, 3498 arrays (0 quantized)
2018-09-19 10:54:03.893920: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1479 operators, 3498 arrays (0 quantized)
2018-09-19 10:54:04.023420: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 1292 operators, 3014 arrays (1 quantized)
2018-09-19 10:54:04.147459: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 1292 operators, 3014 arrays (1 quantized)
2018-09-19 10:54:04.224980: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 1292 operators, 3014 arrays (1 quantized)
2018-09-19 10:54:04.322794: F tensorflow/contrib/lite/toco/tooling_util.cc:1589] Array InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D_eightbit_min_input, which is an input to the (Unsupported TensorFlow op: QuantizeV2) operator producing the output array InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D_eightbit_quantize_input, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.
Aborted (core dumped)

None



I also try
### Source code
`tflite_convert  --output_file=/tmp/inception_v3_quantized.tflite  --graph_def_file=/tmp/inception_v3_quantized.pb --inference_type=QUANTIZED_UINT8 --input_arrays=input --output_arrays=InceptionV3/Predictions/Reshape_1 --mean_values=128  --std_dev_values=127 --default_ranges_min=0 --default_ranges_max=6`


` tflite_convert  --output_file=/tmp/inception_v3_quantized.tflite  --graph_def_file=/tmp/inception_v3_frozen_graph.pb --inference_type=QUANTIZED_UINT8 --input_arrays=input --output_arrays=InceptionV3/Predictions/Reshape_1 --mean_values=128  --std_dev_values=127 --default_ranges_min=0 --default_ranges_max=6`

### Source log

usage: tflite_convert [-h] --output_file OUTPUT_FILE
                      (--graph_def_file GRAPH_DEF_FILE | --saved_model_dir SAVED_MODEL_DIR)
                      [--output_format {TFLITE,GRAPHVIZ_DOT}]
                      [--inference_type {FLOAT,QUANTIZED_UINT8}]
                      [--inference_input_type {FLOAT,QUANTIZED_UINT8}]
                      [--input_arrays INPUT_ARRAYS]
                      [--input_shapes INPUT_SHAPES]
                      [--output_arrays OUTPUT_ARRAYS]
                      [--saved_model_tag_set SAVED_MODEL_TAG_SET]
                      [--saved_model_signature_key SAVED_MODEL_SIGNATURE_KEY]
                      [--std_dev_values STD_DEV_VALUES]
                      [--mean_values MEAN_VALUES]
                      [--default_ranges_min DEFAULT_RANGES_MIN]
                      [--default_ranges_max DEFAULT_RANGES_MAX]
                      [--drop_control_dependency DROP_CONTROL_DEPENDENCY]
                      [--reorder_across_fake_quant REORDER_ACROSS_FAKE_QUANT]
                      [--change_concat_input_ranges CHANGE_CONCAT_INPUT_RANGES]
                      [--allow_custom_ops ALLOW_CUSTOM_OPS]
tflite_convert: error: --default_ranges_min and --default_ranges_max must be used together


How should I solve the problem ?
"
22366,Example problem in tf.reduce_sum function documentation,"I'm not sure about registering an issue for an documentation error here, I found that the name of argument ""keep_dims"" on example of tf.reduce_sum is wrong.

ref: https://www.tensorflow.org/versions/r1.11/api_docs/python/tf/reduce_sum

x = tf.constant([[1, 1, 1], [1, 1, 1]])
tf.reduce_sum(x)  # 6
tf.reduce_sum(x, 0)  # [2, 2, 2]
tf.reduce_sum(x, 1)  # [3, 3]
tf.reduce_sum(x, 1, **keepdims**=True)  # [[3], [3]]
tf.reduce_sum(x, [0, 1])  # 6

==>

x = tf.constant([[1, 1, 1], [1, 1, 1]])
tf.reduce_sum(x)  # 6
tf.reduce_sum(x, 0)  # [2, 2, 2]
tf.reduce_sum(x, 1)  # [3, 3]
tf.reduce_sum(x, 1, **keep_dims**=True)  # [[3], [3]]
tf.reduce_sum(x, [0, 1])  # 6

"
22364,Estimator with MirroredStrategy on multiple GPUs seems to start with non-initialized weights on one of the GPUs,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: slightly modified example code
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a
- **TensorFlow installed from (source or binary)**: binary (pip)
- **TensorFlow version (use command below)**: ('v1.10.1-0-g4dcfddc5d1', '1.10.1')
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: 9.0/7.1.4
- **GPU model and memory**: GeForce GTX 1080
- **Exact command to reproduce**:

Slightly modified version of https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute#example-with-estimator-api to add some print nodes.

```

    import tensorflow as tf
    
    def build_model_fn_optimizer():

        def model_fn(features, labels, mode):
            features = tf.Print(features, [features], message=""features = "")
            layer = tf.layers.Dense(1, use_bias=True)
            logits = layer(features)
            logits = tf.Print(logits, [logits], message=""logits = "")

            if mode == tf.estimator.ModeKeys.PREDICT:
                predictions = {""logits"": logits}
                return tf.estimator.EstimatorSpec(mode, predictions=predictions)

            loss = tf.losses.mean_squared_error(
                labels=labels, predictions=tf.reshape(logits, []))

            if mode == tf.estimator.ModeKeys.EVAL:
                return tf.estimator.EstimatorSpec(mode, loss=loss)

            if mode == tf.estimator.ModeKeys.TRAIN:
                train_op = tf.train.GradientDescentOptimizer(0.2).minimize(loss)
                return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)

        return model_fn
    
    def main(_):
        #distribution = tf.contrib.distribute.OneDeviceStrategy(""/device:GPU:0"")
        distribution = tf.contrib.distribute.MirroredStrategy(
            [""/device:GPU:0"", ""/device:GPU:1""]
        )
        config = tf.estimator.RunConfig(train_distribute=distribution)
    
        def input_fn():
            features = tf.data.Dataset.from_tensors([[1.]]).repeat(10)
            labels = tf.data.Dataset.from_tensors(1.).repeat(10)
            return tf.data.Dataset.zip((features, labels))
    
        estimator = tf.estimator.Estimator(
            model_fn=build_model_fn_optimizer(), config=config)
        estimator.train(input_fn=input_fn, steps=10)
    
        eval_result = estimator.evaluate(input_fn=input_fn, steps=10)
        print(""Eval result: {}"".format(eval_result))
    
        def predict_input_fn():
            predict_features = tf.data.Dataset.from_tensors([[1.]]).repeat(10)
            return predict_features
    
        predictions = estimator.predict(input_fn=predict_input_fn)
        predictions = list(predictions)
        print(""Prediction results: {}"".format(predictions))
    
    
    if __name__ == ""__main__"":
        tf.app.run()

```

The output I get is the following:

    WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpygBKCI
    INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3bd6649b90>, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_device_fn': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/tmp/tmpygBKCI', '_train_distribute': <tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x7f3bd6649b10>, '_save_summary_steps': 100}
    2018-09-18 23:44:42.051140: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
    2018-09-18 23:44:42.244608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties:
    name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
    pciBusID: 0000:08:00.0
    totalMemory: 10.92GiB freeMemory: 9.93GiB
    2018-09-18 23:44:42.392404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
    2018-09-18 23:44:42.394060: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 1 with properties:
    name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
    pciBusID: 0000:42:00.0
    totalMemory: 10.92GiB freeMemory: 10.76GiB
    2018-09-18 23:44:42.398325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0, 1
    2018-09-18 23:44:42.808254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
    2018-09-18 23:44:42.808317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 1
    2018-09-18 23:44:42.808325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N Y
    2018-09-18 23:44:42.808331: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 1:   Y N
    2018-09-18 23:44:42.808726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/device:GPU:0 with 9603 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:08:00.0, compute capability: 6.1)
    2018-09-18 23:44:42.887416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/device:GPU:1 with 10403 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:42:00.0, compute capability: 6.1)
    INFO:tensorflow:Device is available but not used by distribute strategy: /device:CPU:0
    INFO:tensorflow:Configured nccl all-reduce.
    2018-09-18 23:44:42.999232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0, 1
    2018-09-18 23:44:42.999351: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
    2018-09-18 23:44:42.999364: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 1
    2018-09-18 23:44:42.999371: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N Y
    2018-09-18 23:44:42.999379: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 1:   Y N
    2018-09-18 23:44:42.999591: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9603 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:08:00.0, compute capability: 6.1)
    2018-09-18 23:44:42.999726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10403 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:42:00.0, compute capability: 6.1)
    INFO:tensorflow:Calling model_fn.
    INFO:tensorflow:Calling model_fn.
    INFO:tensorflow:batch_all_reduce invoked for batches size = 2 with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10
    INFO:tensorflow:Done calling model_fn.
    INFO:tensorflow:Done calling model_fn.
    INFO:tensorflow:Create CheckpointSaverHook.
    INFO:tensorflow:Graph was finalized.
    2018-09-18 23:44:43.215770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0, 1
    2018-09-18 23:44:43.215877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
    2018-09-18 23:44:43.215889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 1
    2018-09-18 23:44:43.215897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N Y
    2018-09-18 23:44:43.215904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 1:   Y N
    2018-09-18 23:44:43.216104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9603 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:08:00.0, compute capability: 6.1)
    2018-09-18 23:44:43.216251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10403 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:42:00.0, compute capability: 6.1)
    INFO:tensorflow:Running local_init_op.
    INFO:tensorflow:Done running local_init_op.
    INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpygBKCI/model.ckpt.
    features = [[1]]
    logits = [[0]]
    features = [[1]]
    logits = [[-0.225562453]]
    INFO:tensorflow:loss = 1.2510016, step = 0
    features = [[1]]
    logits = [[0]]
    features = [[1]]
    logits = [[0.6646626]]
    features = [[1]]
    features = [[1]]
    logits = [[-5.94375372]]
    logits = [[1.19879758]]
    features = [[1]]
    logits = [[-23.9573383]]
    features = [[1]]
    logits = [[3.89678]]
    features = [[1]]
    features = [[1]]
    logits = [[-82.8739166]]
    logits = [[12.7210035]]
    INFO:tensorflow:Saving checkpoints for 5 into /tmp/tmpygBKCI/model.ckpt.
    INFO:tensorflow:Loss for final step: 3586.108.
    INFO:tensorflow:Calling model_fn.
    INFO:tensorflow:Done calling model_fn.
    INFO:tensorflow:Starting evaluation at 2018-09-18-23:44:45
    INFO:tensorflow:Graph was finalized.
    2018-09-18 23:44:45.322872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0, 1
    2018-09-18 23:44:45.322972: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
    2018-09-18 23:44:45.322984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 1
    2018-09-18 23:44:45.322992: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N Y
    2018-09-18 23:44:45.323000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 1:   Y N
    2018-09-18 23:44:45.323190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9603 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:08:00.0, compute capability: 6.1)
    2018-09-18 23:44:45.323319: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10403 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:42:00.0, compute capability: 6.1)
    INFO:tensorflow:Restoring parameters from /tmp/tmpygBKCI/model.ckpt-5
    INFO:tensorflow:Running local_init_op.
    INFO:tensorflow:Done running local_init_op.

Notice that the logits comes out printed as zero the first and third time, which suggests the weights for at least one of the GPU towers is all zeroes at the start (or both are all zeros initially but the prints are showing up interwoven between the two towers).

If I run the code with just a single GPU using OneDeviceStrategy, this does not happen (first logit value printed out is non-zero due to random initial weights).

This affects training for more complicated scenarios since it could lead to incorrect loss computation.

Still happens if I upgrade to TensorFlow 1.11.0-rc1 ('v1.11.0-rc1-0-ge4c4b20805').

I posted on [StackOverflow](https://stackoverflow.com/questions/52396116/tensorflow-estimator-with-mirroredstrategy-on-multiple-gpus-seems-to-start-with) but also decided to open this issue since it seems somewhat similar to the variable initialization issue reported in #19069 which is supposedly fixed. If this is more appropriate on StackOverflow, I can wait for responses there. Thanks!
"
22362,"Impossible to initialize Variable, restored from a model, in Java API?","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
N/A
- **TensorFlow installed from (source or binary)**:
binary via pom.xml:
```
<dependency>
     <groupId>org.tensorflow</groupId>
     <artifactId>libtensorflow</artifactId>
     <version>1.10.0</version>
 </dependency>
 <dependency>
     <groupId>org.tensorflow</groupId>
     <artifactId>libtensorflow_jni_gpu</artifactId>
     <version>1.10.0</version>
 </dependency>
```

- **TensorFlow version (use command below)**:
1.10.0
- **Python version**:
Using Java
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
9.0/7.0.5
- **GPU model and memory**:
GeForce GTX 750 Ti
- **Exact command to reproduce**:
N/A

### Describe the problem

Trying to run inference in Java from a restored model, getting:

```java.lang.IllegalStateException: Attempting to use uninitialized value```

Same thing works in Python.

The  model is not trivial - 2 stages with intermediate data stored in that uninitialized variable in GPU.

To workaround would like to initialize it. Looked through Operation and OperationBuilder classes - nothing.

Able to get the variable using:
```
Operation ooyoo = bundle.graph().operation(""rv_stage1_out"");
System.out.println(ooyoo.toString());
System.out.println(ooyoo.type());
```
getting:
```
<VariableV2 'rv_stage1_out'>
VariableV2
```

How to assign?

### Source code / logs
[Code](https://git.elphel.com/Elphel/imagej-elphel/blob/gpu/src/main/java/TensorflowExamplePlugin.java)

More information as well as code and logs are posted on [Stackoverflow](https://stackoverflow.com/questions/52394799/tensorflow-1-10-0-java-api-java-lang-illegalstateexception-attempting-to-use-u).
"
22361,Conv1d ops don't get quantized,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OS X
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.10.1
- **Python version**: 2.7.10
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: See below.

### Describe the problem

Currently `tf.contrib.quantize.create_eval_graph` and `tf.contrib.quantize.create_training_graph` will create a quantized version of the graph if the graph contains 2d convolutions.  The pattern matching that these functions use breaks in the case of 1d convolutions, however.  This is because 1d convolutions are transformed to 2d convolutions under the hood with an `ExpandDims` Op, and this Op breaks the expected pattern of Ops used by `_FindLayersToQuantize`.

It would be great if the pattern in `_FindLayersToQuantize` could be changed from

```
weight|folded_weight --> conv|fc --> [batch_to_space_nd] --> ...
```

to

```
weight|folded_weight --> [expand_dims] --> conv|fc --> [batch_to_space_nd] --> ...
```

This should allow conv1d ops to quantize exactly like conv2d ops.

### Source code / logs

```
# Create and quantize a graph with 2d convolution.
X = tf.placeholder(tf.float32, [None, 28, 28, 1])
conv = tf.layers.conv2d(X, 64, 3, padding='same', activation=tf.nn.relu)
tf.contrib.quantize.create_eval_graph()
len([n.name for n in tf.get_default_graph().as_graph_def().node if 'quant' in n.name]) > 0
```

This should be `True`.

```
# Create and quantize a graph with 1d convolution.
X = tf.placeholder(tf.float32, [None, 28, 1])
conv = tf.layers.conv1d(X, 64, 3, padding='same', activation=tf.nn.relu)
tf.contrib.quantize.create_eval_graph()
len([n.name for n in tf.get_default_graph().as_graph_def().node if 'quant' in n.name]) > 0
```

This should be `False`."
22360,Op type not registered 'TRTEngineOp' in binary running with C API,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: r1.11
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.16.1
- **GCC/Compiler version (if compiling from source)**: 5.4
- **CUDA/cuDNN version**: 9/7.1
- **GPU model and memory**: 1080ti/11gb
- **Exact command to reproduce**: 
1. Clone tensorflow repo, checkout r1.11 branch
2. Build from source as directed from documentation by disabling everything except cuda, tensorrt(4.0.1.6)
3. Create a tensorrt .pb file using following:
```
    trt_graph = trt.create_inference_graph(
    input_graph_def=tf.get_default_graph().as_graph_def(),
    outputs=output_node,
    max_batch_size=1,
    max_workspace_size_bytes=1 << 25,
    precision_mode=""FP32"",  # TRT Engine precision ""FP32"",""FP16"" or ""INT8""
    minimum_segment_size=2  # minimum number of nodes in an engine
    )
    f = open(""trt.pb"", 'w')
    f.write(trt_graph.SerializeToString())
    f.close()

```
4. Use Tensorflow C API to run infernce on the protobuf file

Full Error:
2018-09-18 14:21:41.085891: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: ""TRTEngineOp"" device_type: ""GPU""') for unknown op: TRTEngineOp
check_status: Caused by: Add graph to TF session: Not found: Op type not registered 'TRTEngineOp' in binary running on dhingratul-Workstation. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.


Related issue: #22005 , except I am using the latest version of tensorflow and tensorRT. I have also tried the workarounds mentioned [here](https://stackoverflow.com/questions/50125889/c-tensorflow-api-with-tensorrt), but don't work.
 
You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22359,Bazel Build all_protos proto_library Support,"I would like to build a `proto_library` for all the protobuf files in (core) tensorflow.

Specifically, I need to build the GraphDef protobuf message (and later target it for Golang) and include it in a custom proto definition. However, it seems only cc and py are visible/supported. It does not seem like the general `proto_library` is exposed by the tensorflow repo. Is there a simple way to do this? Ideally there would be a `proto_library` definition that is available in the [core/BUILD file](https://github.com/tensorflow/tensorflow/blob/89e172f6ae5a52f8ceca6b4690331e1dce89d456/tensorflow/core/BUILD).

When I try to execute ""tf_additional_all_protos"" after successfully loading it from build_config.bzl, I get an error saying `no such package 'tensorflow/core'`. 

Any suggestions?

Have I written custom code
### Bazel Build
```
# bazel build :my_tf_proto
ERROR: /Users/zane/dev0/piran/test/BUILD:4:1: no such package 'tensorflow/core': BUILD file not found on package path and referenced by '//:_tensorflow_all_protos'
ERROR: Analysis of target '//:my_tf_proto' failed; build aborted: Analysis failed
INFO: Elapsed time: 0.346s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (1 packages loaded)
```

### Environment
OS Platform and Distribution
```
# bazel version
Build label: 0.15.2
Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Tue Jul 17 12:22:40 2018 (1531830160)
Build timestamp: 1531830160
Build timestamp as int: 1531830160
#
# lsb_release -a
No LSB modules are available.
Distributor ID:	Ubuntu
Description:	Ubuntu 16.04.5 LTS
Release:	16.04
Codename:	xenial
```

tf.proto
```
syntax = ""proto3"";

package mytf;

import ""tensorflow/core/framework/graph.proto"";

message MyMessage {
  tensorflow.GraphDef myGraph = 1;
}
```

WORKSPACE
```

git_repository(
    name = ""io_bazel_rules_closure"",
    remote = ""https://github.com/bazelbuild/rules_closure.git"",
    tag = ""0.8.0"",
)

git_repository(
    name = ""com_github_tensorflow_tensorflow"",
    remote = ""https://github.com/tensorflow/tensorflow.git"",
    commit = ""0c8a8289da120ee353c4fba5decb0bea9014e0a7""  # Sep 18, 2018
)
load(""@com_github_tensorflow_tensorflow//tensorflow:workspace.bzl"", ""tf_workspace"")
tf_workspace()

```

BUILD
```
package(default_visibility = [""//visibility:public""])
load(""@com_github_tensorflow_tensorflow//tensorflow/core:platform/default/build_config.bzl"", ""tf_additional_all_protos"")

proto_library(
  name = '_tensorflow_all_protos',
  srcs = tf_additional_all_protos(),
)

proto_library(
	name = 'my_tf_proto',
	srcs = ['tf.proto'],
	deps = [':_tensorflow_all_protos'],
)
```

TensorFlow installed from: N/A
TensorFlow version: N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: `bazel build :my_tf_proto`
Mobile device: N/A"
22358,Document how to train with large batch sizes,"This request was a result of the TensorFlow Fall Symposium. Document tips and methods to use for large batch scaling with TensorFlow.  Possibly mention where this is proven and unproven to work.  This would include adding optimizers or wrapper to simplify usage.  Researchers are large labs want to scale but do not have the knowledge to do it quickly.
"
22357,Make NVIDIA library versions to TF Version matrix more visible.,This request was a product of the TensorFlow Fall Symposium. Consider talking to the documentation team as well as maybe linking it to the top or near the top of all the release documents.  
22356,Document DistributionStrategies including how it can be extended.,"This request was a product of the TensorFlow Fall Symposium.  It might be nice to start with a simple block diagram and then go deeper with the objective to show companies, individual, and academics how they can extend DistributionStrategies at various points.

Picking a random person from the tf-dist-strat team to start."
22355,Bazel build fails for r1.11,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra 10.13.4
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.11
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**: 0.15.2-homebrew\
- **CUDA/cuDNN version**:  I use CPU-only tensorflow version
- **GPU model and memory**: Intel Iris Graphics 6100 / 1536 MB
- **Exact command to reproduce**: bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

Hello everyone!
I've updated tensorflow today, run ```./configure``` command succesfully, but then ```bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package``` command and it failed with error
```
ERROR: /private/var/tmp/_bazel_kate/8be913d8cdea6dd8174f0dac0a48c277/external/double_conversion/BUILD.bazel:12:1: undeclared inclusion(s) in rule '@double_conversion//:double-conversion':
this rule is missing dependency declarations for the following files included by 'external/double_conversion/double-conversion/fast-dtoa.cc':
  '/Library/Developer/CommandLineTools/usr/lib/clang/10.0.0/include/stdint.h'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 3.297s, Critical Path: 1.92s
INFO: 0 processes.
FAILED: Build did NOT complete successfully
```
This text was rendered after this command with ```--verbose_failures```  option"
22354,ig -v,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22353,[Feature Request] Making gradient boosted trees available outside tf.Estimator,"**Feature Request**

Is there any ongoing discussion on making gradient boosted trees available outside of `tf.estimators` ?

Doing this will allow a wider range of experimentation than what is possible with `tf.estimators`.
"
22349,Error in tf.contrib.distributions.fill_triangular_inverse,"### System information
** TF Env Info: **
[tf_env.txt](https://github.com/tensorflow/tensorflow/files/2393567/tf_env.txt)

** TF Version: **
```
$ python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
/home/mab/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype
from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
v1.10.1-0-g4dcfddc5d1 1.10.1
```

### Describe the problem
The function ```fill_triangular_inverse``` actually uses the upper triangular portion of the matrices when constructing the transformation.  This can be seen with the above code where a vector is converted to a lower triangular matrix with ```fill_triangular``` and then back to a vector with ```fill_triangular_inverse```.  This works fine and reproduces the original vector unless there are non-zero entries added to the upper triangular portion of the intervening matrix.

The fix for this should be relatively easy and include a call to ```tf.matrix_band_part``` at the beginning of ```fill_triangular_inverse```.

### Source code / logs
Source to reproduce the issue:
```
import numpy as np
import tensorflow as tf
import tensorflow.contrib.distributions as tfdist

d = 5
par_d = d*(d+1)//2
vec = tf.constant(np.random.randn(1,par_d))
M_orig = tfdist.fill_triangular(vec)
M = M_orig + tf.constant(np.triu(np.random.randn(d,d),1).reshape([1,d,d]))
vec2 = tfdist.fill_triangular_inverse(M)
vec3 = tfdist.fill_triangular_inverse(M_orig)

with tf.Session() as sess:
    [vec_val,vec2_val,vec3_val] = sess.run([vec,vec2,vec3])

    print(""d = {}, w/upper error = {}, w/o upper error = {}"".format(d,np.linalg.norm(vec_val-vec2_val),np.linalg.norm(vec_val-vec3_val)))
```

Output on local installation:
```
d = 5, w/upper error = 2.6202726540863606, w/o upper error = 0.0
```"
22348,Try to install TF from source got error,"The environment is Ubuntu 18, nvidia driver 396(K80), cuda 9.2, cudnn 7.2.1, nccl 2.2, TF 1.10, python 3.6, bazel 0.17.1, gcc 7.3.0
 
the ./configure is ok, but command below got error
bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

ERROR: /home/eliyart/tensorflow/tensorflow/python/BUILD:1598:1: Linking of rule '//tensorflow/python:gen_linalg_ops_py_wrappers_cc' failed (Exit 1)
/usr/bin/x86_64-linux-gnu-ld: warning: libcublas.so.9.2, needed by bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)
/usr/bin/x86_64-linux-gnu-ld: warning: libcudnn.so.7, needed by bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)
/usr/bin/x86_64-linux-gnu-ld: warning: libcufft.so.9.2, needed by bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)
/usr/bin/x86_64-linux-gnu-ld: warning: libcurand.so.9.2, needed by bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftMakePlanMany@libcufft.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDtrsm_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasIzamax_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZhpr2_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnCreate@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnCreatePoolingDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDzasum_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `curandDestroyGenerator@libcurand.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnCreateConvolutionDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnConvolutionBiasActivationForward@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasIsamax_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnDestroyConvolutionDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSsbmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSgemm_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZherk_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSspr_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDsymm_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasIdamax_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnDestroyFilterDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftPlan3d@libcufft.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSetStream_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasGemmEx@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZhpmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCsyrk_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasIsamin_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCscal_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftExecD2Z@libcufft.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSrotg_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnLRNCrossChannelForward@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSsyrk_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnCreateRNNDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnDestroyRNNDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnConvolutionBackwardData@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftSetWorkArea@libcufft.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnCreateFilterDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZher2k_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDrotm_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCsyr2k_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnLRNCrossChannelBackward@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftMakePlan2d@libcufft.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDgbmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSsyr2k_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasStpsv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasStrmm_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnDestroyLRNDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnDestroy@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDznrm2_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCtrsv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZgemmBatched@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZdscal_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDdot_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnDropoutGetStatesSize@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnGetConvolutionForwardWorkspaceSize@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSgemv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnSetActivationDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDsbmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnSetPoolingNdDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnGetRNNWorkspaceSize@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasChbmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasStpmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCgemv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnGetConvolutionForwardAlgorithm@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCtrsm_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnSetConvolutionGroupCount@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnConvolutionBackwardBias@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftMakePlan1d@libcufft.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCsymm_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDtrmm_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSgemmBatched@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSgbmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCtbmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDasum_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasScopy_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDrotmg_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnSetTensorNdDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZgemv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCaxpy_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasChpr2_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnAddTensor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZgemm_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZtrmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnDestroyActivationDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDrotg_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZgerc_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZhemv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDsyrk_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnSetConvolutionNdDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZtbmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSrotmg_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDgemmBatched@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnPoolingBackward@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDspmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnRNNBackwardData@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnGetFilterNdDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCrotg_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCher_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftExecR2C@libcufft.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSdot_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZrotg_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCsscal_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnGetProperty@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnGetConvolutionBackwardFilterWorkspaceSize@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasChemm_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDtpmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDscal_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDger_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasStrsm_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnGetRNNTrainingReserveSize@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDsyr_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSsyr2_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `curandSetStream@libcurand.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasChemv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDrot_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSaxpy_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `curandGenerateUniformDouble@libcurand.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `curandSetPseudoRandomGeneratorSeed@libcurand.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `curandGenerateUniform@libcurand.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnSetLRNDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnBatchNormalizationForwardTraining@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDnrm2_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZtrsm_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnDestroyTensorDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnDestroyDropoutDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSsymv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDtpsv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSspmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnCreateTensorDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCgeru_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSrot_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnSetStream@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZtrmm_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCtpsv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasIdamin_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasIcamin_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDsymv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSrotm_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZhpr_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCherk_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasChpmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnBatchNormalizationForwardInference@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCdotu_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnCreateDropoutDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZtpmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnCreatePersistentRNNPlan@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZsyr2k_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCgerc_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZscal_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCtpmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `curandGenerateNormal@libcurand.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSsyr_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftDestroy@libcufft.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZhemm_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftCreate@libcufft.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZgbmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSetMathMode@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDtrmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnDestroyPersistentRNNPlan@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnDestroyPoolingDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDtbsv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZhbmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnCreateActivationDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSspr2_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSger_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasGemmBatchedEx@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSasum_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSsymm_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDcopy_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftPlanMany@libcufft.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDspr_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDswap_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnRNNForwardInference@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasStrmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `curandGenerateNormalDouble@libcurand.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCreate_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasIzamin_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftPlan1d@libcufft.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftSetStream@libcufft.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftExecZ2Z@libcufft.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnGetConvolutionBackwardDataAlgorithm@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnGetConvolutionNdDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnActivationForward@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnGetRNNLinLayerMatrixParams@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnSetRNNDescriptor_v6@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnGetConvolutionBackwardDataWorkspaceSize@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasChpr_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `curandSetGeneratorOffset@libcurand.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnBatchNormalizationBackward@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasStbsv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSetPointerMode_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnSetDropoutDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZgeru_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasStrsv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCswap_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDgemv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDspr2_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCher2_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCdotc_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDestroy_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftSetAutoAllocation@libcufft.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDsyr2k_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCcopy_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnConvolutionForward@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnConvolutionBackwardFilter@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCgemm_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCgbmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZher2_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnGetConvolutionBackwardFilterAlgorithm@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZswap_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnGetRNNLinLayerBiasParams@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCtrmm_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnSetFilterNdDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnSetRNNMatrixMathType@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSscal_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZsyrk_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnGetRNNParamsSize@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasScnrm2_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZsymm_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZher_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasIcamax_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnGetConvolutionNdForwardOutputDim@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftMakePlan3d@libcufft.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSnrm2_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasScasum_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnSetPersistentRNNPlan@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCher2k_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDtbmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSgemmEx@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnSetTensor4dDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZtpsv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSswap_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnRNNBackwardWeights@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZtrsv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZtbsv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCtbsv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZdotc_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasStbmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZdrot_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftExecC2R@libcufft.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnRNNForwardTraining@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDgemm_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnTransformTensor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCtrmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftExecC2C@libcufft.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCsrot_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZdotu_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDtrsv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnSetConvolutionMathType@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnPoolingForward@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDaxpy_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftPlan2d@libcufft.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftExecZ2D@libcufft.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `curandCreateGenerator@libcurand.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZaxpy_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnCreateLRNDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasGetMathMode@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDsyr2_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZcopy_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCgemmBatched@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ulinalg_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasGetPointerMode_v2@libcublas.so.9.2'
collect2: error: ld returned 1 exit status
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 3430.624s, Critical Path: 213.00s
INFO: 2894 processes: 2894 local.
FAILED: Build did NOT complete successfully"
22347,Bug in tf.import_graph_def for Graph Editor,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: see example below
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.8
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: Titan X
- **Exact command to reproduce**: see example below

### Describe the problem
I'm trying to export and then later import a subset of my graph after using the graph editor for network pruning (such that I get only the pruned network left at the end). I've written two toy examples to illustrate the problem as simply as possible: example 1 (below) works, I can define two subgraphs that share only their input, grab the graphdef for one, clear the graph and restore only the subgraph I want; example 2 (below) tries to do the same thing but uses the graph editor to replace the weights (as in the pruning case) and grab the new subgraph.

### Source code / logs
**Example 1:**
```
import tensorflow as tf

x = tf.placeholder(tf.float32, [], 'x')
w = tf.Variable([3.], dtype=tf.float32, name='w')
y = tf.multiply(x, w, name='y')

w_1 = tf.Variable([1.], dtype=tf.float32, name='w_1')
y_1 = tf.multiply(x, w_1, name='y_1')

subgraph = tf.graph_util.extract_sub_graph(tf.get_default_graph().as_graph_def(), [y_1.op.name])
print(type(subgraph))
tf.reset_default_graph()
_ = tf.import_graph_def(subgraph)

print(tf.contrib.graph_editor.get_tensors(tf.get_default_graph()))
```
this prints, as expected:
> <class 'tensorflow.core.framework.graph_pb2.GraphDef'>
> [<tf.Tensor 'import/w_1:0' shape=(1,) dtype=float32_ref>, <tf.Tensor 'import/w_1/read:0' shape=(1,) dtype=float32>, <tf.Tensor 'import/x_1:0' shape=() dtype=float32>, <tf.Tensor 'import/y_1:0' shape=(1,) dtype=float32>]

**Example 2:**
```
import tensorflow as tf

x = tf.placeholder(tf.float32, [], 'x')
w = tf.Variable([3.], dtype=tf.float32, name='w')
y = tf.multiply(x, w, 'y')

w_1 = tf.Variable([2.], dtype=tf.float32, name='w_1')
y_1 = tf.contrib.graph_editor.graph_replace(y, {w.op.outputs[0]: w_1.op.outputs[0]})

subgraph = tf.graph_util.extract_sub_graph(tf.get_default_graph().as_graph_def(), [y_1.op.name])
print(type(subgraph))
tf.reset_default_graph()
_ = tf.import_graph_def(subgraph)

print(tf.contrib.graph_editor.get_tensors(tf.get_default_graph()))
```
Edit: fixed typo in example 2, error is now:

> ValueError: Node 'w/read_1' expects to be colocated with unknown node 'w'

The notable difference in the subgraph graphdefs is the line `s: ""loc:@w""` under node `w/read_1`, which indicates the root cause is likely in the graph editor (considering that's the only difference between the two examples). Tagging @purpledog for input on the graph editor setting the attribute of that node."
22346," NotFoundError (see above for traceback): Key Variable not found in checkpoint     	 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_INT32, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], ","When I restore a saved model using:

    checkpoint = tf.train.get_checkpoint_state(config.pre_model_dir)
    if checkpoint and checkpoint.model_checkpoint_path:
     saver.restore(session, checkpoint.model_checkpoint_path)
, I am getting this error: 


    INFO:tensorflow:Restoring parameters from ./saved_model/10_zones/10/network--1685000
    ---------------------------------------------------------------------------
    NotFoundError                             Traceback (most recent call last)
    /usr/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
       1321     try:
    -> 1322       return fn(*args)
       1323     except errors.OpError as e:
    
    /usr/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)
       1306       return self._call_tf_sessionrun(
    -> 1307           options, feed_dict, fetch_list, target_list, run_metadata)
       1308 
    
    /usr/lib/python3.6/site-packages/tensorflow/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)
       1408           self._session, options, feed_dict, fetch_list, target_list,
    -> 1409           run_metadata)
       1410     else:
    
    NotFoundError: Key Variable not found in checkpoint
    	 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_INT32, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
    	 [[Node: save/RestoreV2/_21 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_18_save/RestoreV2"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
    
    During handling of the above exception, another exception occurred:
    
    NotFoundError                             Traceback (most recent call last)
    <ipython-input-97-0cbd09927b40> in <module>()
         42 checkpoint = tf.train.get_checkpoint_state(config.pre_model_dir)
         43 if checkpoint and checkpoint.model_checkpoint_path:
    ---> 44     saver.restore(session, checkpoint.model_checkpoint_path)
         45     print(""loaded the model"")
         46 else:
    
    /usr/lib/python3.6/site-packages/tensorflow/python/training/saver.py in restore(self, sess, save_path)
       1800     else:
       1801       sess.run(self.saver_def.restore_op_name,
    -> 1802                {self.saver_def.filename_tensor_name: save_path})
       1803 
       1804   @staticmethod
    
    /usr/lib/python3.6/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
        898     try:
        899       result = self._run(None, fetches, feed_dict, options_ptr,
    --> 900                          run_metadata_ptr)
        901       if run_metadata:
        902         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)
    
    /usr/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
       1133     if final_fetches or final_targets or (handle and feed_dict_tensor):
       1134       results = self._do_run(handle, final_targets, final_fetches,
    -> 1135                              feed_dict_tensor, options, run_metadata)
       1136     else:
       1137       results = []
    
    /usr/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
       1314     if handle is None:
       1315       return self._do_call(_run_fn, feeds, fetches, targets, options,
    -> 1316                            run_metadata)
       1317     else:
       1318       return self._do_call(_prun_fn, handle, feeds, fetches)
    
    /usr/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
       1333         except KeyError:
       1334           pass
    -> 1335       raise type(e)(node_def, op, message)
       1336 
       1337   def _extend_graph(self):
    
    NotFoundError: Key Variable not found in checkpoint
    	 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_INT32, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
    	 [[Node: save/RestoreV2/_21 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_18_save/RestoreV2"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
    
    Caused by op 'save/RestoreV2', defined at:
      File ""/usr/lib64/python3.6/runpy.py"", line 193, in _run_module_as_main
        ""__main__"", mod_spec)
      File ""/usr/lib64/python3.6/runpy.py"", line 85, in _run_code
        exec(code, run_globals)
      File ""/usr/lib/python3.6/site-packages/ipykernel_launcher.py"", line 16, in <module>
        app.launch_new_instance()
      File ""/usr/lib/python3.6/site-packages/traitlets/config/application.py"", line 658, in launch_instance
        app.start()
      File ""/usr/lib/python3.6/site-packages/ipykernel/kernelapp.py"", line 486, in start
        self.io_loop.start()
      File ""/usr/lib64/python3.6/site-packages/tornado/platform/asyncio.py"", line 127, in start
        self.asyncio_loop.run_forever()
      File ""/usr/lib64/python3.6/asyncio/base_events.py"", line 422, in run_forever
        self._run_once()
      File ""/usr/lib64/python3.6/asyncio/base_events.py"", line 1432, in _run_once
        handle._run()
      File ""/usr/lib64/python3.6/asyncio/events.py"", line 145, in _run
        self._callback(*self._args)
      File ""/usr/lib64/python3.6/site-packages/tornado/platform/asyncio.py"", line 117, in _handle_events
        handler_func(fileobj, events)
      File ""/usr/lib64/python3.6/site-packages/tornado/stack_context.py"", line 276, in null_wrapper
        return fn(*args, **kwargs)
      File ""/usr/lib64/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 450, in _handle_events
        self._handle_recv()
      File ""/usr/lib64/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 480, in _handle_recv
        self._run_callback(callback, msg)
      File ""/usr/lib64/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 432, in _run_callback
        callback(*args, **kwargs)
      File ""/usr/lib64/python3.6/site-packages/tornado/stack_context.py"", line 276, in null_wrapper
        return fn(*args, **kwargs)
      File ""/usr/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 283, in dispatcher
        return self.dispatch_shell(stream, msg)
      File ""/usr/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 233, in dispatch_shell
        handler(stream, idents, msg)
      File ""/usr/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 399, in execute_request
        user_expressions, allow_stdin)
      File ""/usr/lib/python3.6/site-packages/ipykernel/ipkernel.py"", line 208, in do_execute
        res = shell.run_cell(code, store_history=store_history, silent=silent)
      File ""/usr/lib/python3.6/site-packages/ipykernel/zmqshell.py"", line 537, in run_cell
        return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
      File ""/usr/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2662, in run_cell
        raw_cell, store_history, silent, shell_futures)
      File ""/usr/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2785, in _run_cell
        interactivity=interactivity, compiler=compiler, result=result)
      File ""/usr/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2903, in run_ast_nodes
        if self.run_code(code, result):
      File ""/usr/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2963, in run_code
        exec(code_obj, self.user_global_ns, self.user_ns)
      File ""<ipython-input-97-0cbd09927b40>"", line 26, in <module>
        saver = tf.train.Saver()
      File ""/usr/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1338, in __init__
        self.build()
      File ""/usr/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1347, in build
        self._build(self._filename, build_save=True, build_restore=True)
      File ""/usr/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1384, in _build
        build_save=build_save, build_restore=build_restore)
      File ""/usr/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 835, in _build_internal
        restore_sequentially, reshape)
      File ""/usr/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 472, in _AddRestoreOps
        restore_sequentially)
      File ""/usr/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 886, in bulk_restore
        return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
      File ""/usr/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1463, in restore_v2
        shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)
      File ""/usr/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
        op_def=op_def)
      File ""/usr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3392, in create_op
        op_def=op_def)
      File ""/usr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1718, in __init__
        self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access
    
    NotFoundError (see above for traceback): Key Variable not found in checkpoint
    	 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_INT32, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
    	 [[Node: save/RestoreV2/_21 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_18_save/RestoreV2"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]


I searched about this error, and there was a tf bug which requires to call the model using a full relative path, and I followed that path and tried values: `'./saved_model/10_zones/10'` and `os.path.abspath(config.pre_model_dir+'./../saved_model/10_zones/10')`
for `config.pre_model_dir`. Both resulted in a same error. 

I also checked the name of the saved variables using 
from tensorflow.contrib.framework.python.framework import checkpoint_utils

    var_list = checkpoint_utils.list_variables(config.pre_model_dir)
    for v in var_list:
        print(v)
which is:

    ('actor/main_net/layer1/biases/Variable', [90])
    ('actor/main_net/layer1/biases/Variable/Adam', [90])
    ('actor/main_net/layer1/biases/Variable/Adam_1', [90])
    ('actor/main_net/layer1/weights/Variable', [30, 90])
    ('actor/main_net/layer1/weights/Variable/Adam', [30, 90])
    ('actor/main_net/layer1/weights/Variable/Adam_1', [30, 90])
    ('actor/main_net/layer2/biases/Variable', [60])
    ('actor/main_net/layer2/biases/Variable/Adam', [60])
    ('actor/main_net/layer2/biases/Variable/Adam_1', [60])
    ('actor/main_net/layer2/weights/Variable', [90, 60])
    ('actor/main_net/layer2/weights/Variable/Adam', [90, 60])
    ('actor/main_net/layer2/weights/Variable/Adam_1', [90, 60])
    ('actor/main_net/layer3/biases/Variable', [30])
    ('actor/main_net/layer3/biases/Variable/Adam', [30])
    ('actor/main_net/layer3/biases/Variable/Adam_1', [30])
    ('actor/main_net/layer3/weights/Variable', [60, 30])
    ('actor/main_net/layer3/weights/Variable/Adam', [60, 30])
    ('actor/main_net/layer3/weights/Variable/Adam_1', [60, 30])
    ('actor/main_net/layer4/biases/Variable', [10])
    ('actor/main_net/layer4/biases/Variable/Adam', [10])
    ('actor/main_net/layer4/biases/Variable/Adam_1', [10])
    ('actor/main_net/layer4/weights/Variable', [30, 10])
    ('actor/main_net/layer4/weights/Variable/Adam', [30, 10])
    ('actor/main_net/layer4/weights/Variable/Adam_1', [30, 10])
    ('actor/target_net/layer1/biases/Variable', [90])
    ('actor/target_net/layer1/weights/Variable', [30, 90])
    ('actor/target_net/layer2/biases/Variable', [60])
    ('actor/target_net/layer2/weights/Variable', [90, 60])
    ('actor/target_net/layer3/biases/Variable', [30])
    ('actor/target_net/layer3/weights/Variable', [60, 30])
    ('actor/target_net/layer4/biases/Variable', [10])
    ('actor/target_net/layer4/weights/Variable', [30, 10])
    ('beta1_power', [])
    ('beta1_power_1', [])
    ('beta2_power', [])
    ('beta2_power_1', [])
    ('critic/main_net/l1/biases', [90])
    ('critic/main_net/l1/biases/Adam', [90])
    ('critic/main_net/l1/biases/Adam_1', [90])
    ('critic/main_net/l1/weights', [40, 90])
    ('critic/main_net/l1/weights/Adam', [40, 90])
    ('critic/main_net/l1/weights/Adam_1', [40, 90])
    ('critic/main_net/l2/biases', [60])
    ('critic/main_net/l2/biases/Adam', [60])
    ('critic/main_net/l2/biases/Adam_1', [60])
    ('critic/main_net/l2/weights', [90, 60])
    ('critic/main_net/l2/weights/Adam', [90, 60])
    ('critic/main_net/l2/weights/Adam_1', [90, 60])
    ('critic/main_net/l3/biases', [30])
    ('critic/main_net/l3/biases/Adam', [30])
    ('critic/main_net/l3/biases/Adam_1', [30])
    ('critic/main_net/l3/weights', [60, 30])
    ('critic/main_net/l3/weights/Adam', [60, 30])
    ('critic/main_net/l3/weights/Adam_1', [60, 30])
    ('critic/main_net/l4/bias', [1])
    ('critic/main_net/l4/bias/Adam', [1])
    ('critic/main_net/l4/bias/Adam_1', [1])
    ('critic/main_net/l4/kernel', [30, 1])
    ('critic/main_net/l4/kernel/Adam', [30, 1])
    ('critic/main_net/l4/kernel/Adam_1', [30, 1])
    ('critic/target_net/l1/biases', [90])
    ('critic/target_net/l1/weights', [40, 90])
    ('critic/target_net/l2/biases', [60])
    ('critic/target_net/l2/weights', [90, 60])
    ('critic/target_net/l3/biases', [30])
    ('critic/target_net/l3/weights', [60, 30])
    ('critic/target_net/l4/bias', [1])
    ('critic/target_net/l4/kernel', [30, 1])

with what `tf.global_variables()` in my current model results in, and they are both similar:

    <tf.Variable 'actor/main_net/layer1/weights/Variable:0' shape=(30, 90) dtype=float32_ref>,
     <tf.Variable 'actor/main_net/layer1/biases/Variable:0' shape=(90,) dtype=float32_ref>,
     <tf.Variable 'actor/main_net/layer2/weights/Variable:0' shape=(90, 60) dtype=float32_ref>,
     <tf.Variable 'actor/main_net/layer2/biases/Variable:0' shape=(60,) dtype=float32_ref>,
     <tf.Variable 'actor/main_net/layer3/weights/Variable:0' shape=(60, 30) dtype=float32_ref>,
     <tf.Variable 'actor/main_net/layer3/biases/Variable:0' shape=(30,) dtype=float32_ref>,
     <tf.Variable 'actor/main_net/layer4/weights/Variable:0' shape=(30, 10) dtype=float32_ref>,
     <tf.Variable 'actor/main_net/layer4/biases/Variable:0' shape=(10,) dtype=float32_ref>,
     <tf.Variable 'actor/target_net/layer1/weights/Variable:0' shape=(30, 90) dtype=float32_ref>,
     <tf.Variable 'actor/target_net/layer1/biases/Variable:0' shape=(90,) dtype=float32_ref>,
     <tf.Variable 'actor/target_net/layer2/weights/Variable:0' shape=(90, 60) dtype=float32_ref>,
     <tf.Variable 'actor/target_net/layer2/biases/Variable:0' shape=(60,) dtype=float32_ref>,
     <tf.Variable 'actor/target_net/layer3/weights/Variable:0' shape=(60, 30) dtype=float32_ref>,
     <tf.Variable 'actor/target_net/layer3/biases/Variable:0' shape=(30,) dtype=float32_ref>,
     <tf.Variable 'actor/target_net/layer4/weights/Variable:0' shape=(30, 10) dtype=float32_ref>,
     <tf.Variable 'actor/target_net/layer4/biases/Variable:0' shape=(10,) dtype=float32_ref>,
     <tf.Variable 'Variable:0' shape=() dtype=int32_ref>,
     <tf.Variable 'beta1_power:0' shape=() dtype=float32_ref>,
     <tf.Variable 'beta2_power:0' shape=() dtype=float32_ref>,
     <tf.Variable 'actor/main_net/layer1/weights/Variable/Adam:0' shape=(30, 90) dtype=float32_ref>,
     <tf.Variable 'actor/main_net/layer1/weights/Variable/Adam_1:0' shape=(30, 90) dtype=float32_ref>,
     <tf.Variable 'actor/main_net/layer1/biases/Variable/Adam:0' shape=(90,) dtype=float32_ref>,
     <tf.Variable 'actor/main_net/layer1/biases/Variable/Adam_1:0' shape=(90,) dtype=float32_ref>,
     <tf.Variable 'actor/main_net/layer2/weights/Variable/Adam:0' shape=(90, 60) dtype=float32_ref>,
     <tf.Variable 'actor/main_net/layer2/weights/Variable/Adam_1:0' shape=(90, 60) dtype=float32_ref>,
     <tf.Variable 'actor/main_net/layer2/biases/Variable/Adam:0' shape=(60,) dtype=float32_ref>,
     <tf.Variable 'actor/main_net/layer2/biases/Variable/Adam_1:0' shape=(60,) dtype=float32_ref>,
     <tf.Variable 'actor/main_net/layer3/weights/Variable/Adam:0' shape=(60, 30) dtype=float32_ref>,
     <tf.Variable 'actor/main_net/layer3/weights/Variable/Adam_1:0' shape=(60, 30) dtype=float32_ref>,
     <tf.Variable 'actor/main_net/layer3/biases/Variable/Adam:0' shape=(30,) dtype=float32_ref>,
     <tf.Variable 'actor/main_net/layer3/biases/Variable/Adam_1:0' shape=(30,) dtype=float32_ref>,
     <tf.Variable 'actor/main_net/layer4/weights/Variable/Adam:0' shape=(30, 10) dtype=float32_ref>,
     <tf.Variable 'actor/main_net/layer4/weights/Variable/Adam_1:0' shape=(30, 10) dtype=float32_ref>,
     <tf.Variable 'actor/main_net/layer4/biases/Variable/Adam:0' shape=(10,) dtype=float32_ref>,
     <tf.Variable 'actor/main_net/layer4/biases/Variable/Adam_1:0' shape=(10,) dtype=float32_ref>,
     <tf.Variable 'critic/main_net/l1/weights:0' shape=(40, 90) dtype=float32_ref>,
     <tf.Variable 'critic/main_net/l1/biases:0' shape=(90,) dtype=float32_ref>,
     <tf.Variable 'critic/main_net/l2/weights:0' shape=(90, 60) dtype=float32_ref>,
     <tf.Variable 'critic/main_net/l2/biases:0' shape=(60,) dtype=float32_ref>,
     <tf.Variable 'critic/main_net/l3/weights:0' shape=(60, 30) dtype=float32_ref>,
     <tf.Variable 'critic/main_net/l3/biases:0' shape=(30,) dtype=float32_ref>,
     <tf.Variable 'critic/main_net/l4/kernel:0' shape=(30, 1) dtype=float32_ref>,
     <tf.Variable 'critic/main_net/l4/bias:0' shape=(1,) dtype=float32_ref>,
     <tf.Variable 'critic/target_net/l1/weights:0' shape=(40, 90) dtype=float32_ref>,
     <tf.Variable 'critic/target_net/l1/biases:0' shape=(90,) dtype=float32_ref>,
     <tf.Variable 'critic/target_net/l2/weights:0' shape=(90, 60) dtype=float32_ref>,
     <tf.Variable 'critic/target_net/l2/biases:0' shape=(60,) dtype=float32_ref>,
     <tf.Variable 'critic/target_net/l3/weights:0' shape=(60, 30) dtype=float32_ref>,
     <tf.Variable 'critic/target_net/l3/biases:0' shape=(30,) dtype=float32_ref>,
     <tf.Variable 'critic/target_net/l4/kernel:0' shape=(30, 1) dtype=float32_ref>,
     <tf.Variable 'critic/target_net/l4/bias:0' shape=(1,) dtype=float32_ref>,
     <tf.Variable 'beta1_power_1:0' shape=() dtype=float32_ref>,
     <tf.Variable 'beta2_power_1:0' shape=() dtype=float32_ref>,
     <tf.Variable 'critic/main_net/l1/weights/Adam:0' shape=(40, 90) dtype=float32_ref>,
     <tf.Variable 'critic/main_net/l1/weights/Adam_1:0' shape=(40, 90) dtype=float32_ref>,
     <tf.Variable 'critic/main_net/l1/biases/Adam:0' shape=(90,) dtype=float32_ref>,
     <tf.Variable 'critic/main_net/l1/biases/Adam_1:0' shape=(90,) dtype=float32_ref>,
     <tf.Variable 'critic/main_net/l2/weights/Adam:0' shape=(90, 60) dtype=float32_ref>,
     <tf.Variable 'critic/main_net/l2/weights/Adam_1:0' shape=(90, 60) dtype=float32_ref>,
     <tf.Variable 'critic/main_net/l2/biases/Adam:0' shape=(60,) dtype=float32_ref>,
     <tf.Variable 'critic/main_net/l2/biases/Adam_1:0' shape=(60,) dtype=float32_ref>,
     <tf.Variable 'critic/main_net/l3/weights/Adam:0' shape=(60, 30) dtype=float32_ref>,
     <tf.Variable 'critic/main_net/l3/weights/Adam_1:0' shape=(60, 30) dtype=float32_ref>,
     <tf.Variable 'critic/main_net/l3/biases/Adam:0' shape=(30,) dtype=float32_ref>,
     <tf.Variable 'critic/main_net/l3/biases/Adam_1:0' shape=(30,) dtype=float32_ref>,
     <tf.Variable 'critic/main_net/l4/kernel/Adam:0' shape=(30, 1) dtype=float32_ref>,
     <tf.Variable 'critic/main_net/l4/kernel/Adam_1:0' shape=(30, 1) dtype=float32_ref>,
     <tf.Variable 'critic/main_net/l4/bias/Adam:0' shape=(1,) dtype=float32_ref>,
     <tf.Variable 'critic/main_net/l4/bias/Adam_1:0' shape=(1,) dtype=float32_ref>


The only difference in these two lists, is ` <tf.Variable 'Variable:0' shape=() dtype=int32_ref>`, which I do not know what is this for and how it is generated. But, I do not think if it is the problem, since any of my models that can be restored also has it. 

I appreciate any help and comment to resolve this error. 
  

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes. 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: P100, 24 GB
- **Exact command to reproduce**: 
"
22345,bazel build Error: use of undeclared identifier 'strtold',"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NA
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04 LTS
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.9
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.16.1
- **GCC/Compiler version (if compiling from source)**: clang version 6.0.0-1ubuntu2 (tags/RELEASE_600/final), gcc 7.3
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: 2 Tesla K80 with 12 GB Memory
- **Exact command to reproduce**: bazel build --cxxopt='-std=c++11'  -c opt //tensorflow/contrib/android:libtensorflow_inference.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a --verbose_failures --copt=-Wno-c++11-narrowing

### Description of the problem / feature request:

> I am trying to build tensorflow for android deployment. But the build process is failing with error message
```
external/llvm/include/llvm/ADT/StringExtras.h:220:35: error: use of undeclared identifier 'strtold'
  return detail::to_float(T, Num, strtold);
```
### Operating system?

> Distributor ID: Ubuntu
Description:    Ubuntu 18.04 LTS
Release:        18.04
Codename:       bionic


### output of `bazel info release

> WARNING: detected http_proxy set in env, setting no_proxy for localhost.
> Starting local Bazel server and connecting to it...
> release 0.16.1- (@non-git)

> I downloaded bazel-0.16.1-dist.zip and followed the instructions given in https://docs.bazel.build/versions/master/install-compile-source.html

I could just find the solutions for 'use of undeclared identifier' in c or cpp perspective but nothing particularly wrt to bazel. I am not able to understand why only 'strtold' is causing the error when there are 'strtof' and 'strtod' in the same scope which are not declared in the file StringExtras.h

### Error Log
```


ERROR: /home/gtornd/.cache/bazel/_bazel_gtornd/fff5ece2010c235c3b6f64ee1f972ad4/external/llvm/BUILD.bazel:1175:1: C++ compilation of rule '@llvm//:debug_info_code_view' failed (Exit 1): clang failed: error executing command
  (cd /home/gtornd/.cache/bazel/_bazel_gtornd/fff5ece2010c235c3b6f64ee1f972ad4/execroot/org_tensorflow && \
  exec env - \
    ANDROID_BUILD_TOOLS_VERSION=debian \
    ANDROID_NDK_API_LEVEL=15 \
    ANDROID_NDK_HOME=/usr/lib/android-sdk/android-ndk-r15c \
    ANDROID_SDK_API_LEVEL=23 \
    ANDROID_SDK_HOME=/usr/lib/android-sdk \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu \
    HOST_CXX_COMPILER=/usr/bin/g++ \
    HOST_C_COMPILER=/usr/bin/gcc \
    LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64:/usr/local/cuda/extras/CUPTI/lib64 \
    PATH=/usr/local/cuda-9.0/bin:/data/java/jdk1.8.0_161/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CUDA_CLANG=1 \
    TF_CUDA_COMPUTE_CAPABILITIES=3.7,3.7 \
    TF_CUDA_VERSION=9.0 \
    TF_CUDNN_VERSION=7 \
    TF_DOWNLOAD_CLANG=1 \
    TF_NCCL_VERSION=1 \
    TF_NEED_COMPUTECPP=0 \
    TF_NEED_CUDA=1 \
    TF_NEED_OPENCL_SYCL=1 \
    TRISYCL_INCLUDE_DIR=/usr/local/triSYCL/include \
  external/androidndk/ndk/toolchains/llvm/prebuilt/linux-x86_64/bin/clang '-D__ANDROID_API__=14' -isystemexternal/androidndk/ndk/sysroot/usr/include/arm-linux-androideabi -gcc-toolchain external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64 -fpic -ffunction-sections -funwind-tables -fstack-protector-strong -Wno-invalid-command-line-argument -Wno-unused-command-line-argument -no-canonical-prefixes -fno-integrated-as -target armv7-none-linux-androideabi '-march=armv7-a' '-mfloat-abi=softfp' '-mfpu=vfpv3-d16' -mthumb -Os -g -DNDEBUG -MD -MF bazel-out/armeabi-v7a-opt/bin/external/llvm/_objs/debug_info_code_view/CodeViewRecordIO.d '-frandom-seed=bazel-out/armeabi-v7a-opt/bin/external/llvm/_objs/debug_info_code_view/CodeViewRecordIO.o' -DLLVM_ENABLE_STATS -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -D_DEBUG -DLLVM_BUILD_GLOBAL_ISEL -iquote external/llvm -iquote bazel-out/armeabi-v7a-opt/genfiles/external/llvm -iquote bazel-out/armeabi-v7a-opt/bin/external/llvm -iquote external/bazel_tools -iquote bazel-out/armeabi-v7a-opt/genfiles/external/bazel_tools -iquote bazel-out/armeabi-v7a-opt/bin/external/bazel_tools -iquote external/zlib_archive -iquote bazel-out/armeabi-v7a-opt/genfiles/external/zlib_archive -iquote bazel-out/armeabi-v7a-opt/bin/external/zlib_archive -isystem external/llvm/include -isystem bazel-out/armeabi-v7a-opt/genfiles/external/llvm/include -isystem bazel-out/armeabi-v7a-opt/bin/external/llvm/include -isystem external/zlib_archive -isystem bazel-out/armeabi-v7a-opt/genfiles/external/zlib_archive -isystem bazel-out/armeabi-v7a-opt/bin/external/zlib_archive -Wno-c++11-narrowing '-std=c++11' '--sysroot=external/androidndk/ndk/platforms/android-14/arch-arm' -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/backward -isystemexternal/androidndk/ndk/sysroot/usr/include -c external/llvm/lib/DebugInfo/CodeView/CodeViewRecordIO.cpp -o bazel-out/armeabi-v7a-opt/bin/external/llvm/_objs/debug_info_code_view/CodeViewRecordIO.o)
In file included from external/llvm/lib/DebugInfo/CodeView/CodeViewRecordIO.cpp:10:
In file included from external/llvm/include/llvm/DebugInfo/CodeView/CodeViewRecordIO.h:18:
In file included from external/llvm/include/llvm/DebugInfo/CodeView/CodeViewError.h:13:
In file included from external/llvm/include/llvm/Support/Error.h:19:
external/llvm/include/llvm/ADT/StringExtras.h:220:35: error: use of undeclared identifier 'strtold'
  return detail::to_float(T, Num, strtold);
                                  ^
1 error generated.
Target //tensorflow/contrib/android:libtensorflow_inference.so failed to build
INFO: Elapsed time: 2.545s, Critical Path: 1.89s
INFO: 10 processes: 10 local.
FAILED: Build did NOT complete successfully
gtornd@ubuntu:/data/Maitreya/tensorflow$


```"
22344,tensorflow.contrib.mpi import fails and cannot run with mpi even though tensorflow is compiled with mpi,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes, I follow [this](https://github.com/tensorflow/tensorflow/issues/17758) to add ""#define TENSORFLOW_USE_MPI"" to each *.cc files and *.h files in mpi_collectives folder.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.5 LTS
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.4.2
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.5.4
- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
- **CUDA/cuDNN version**:cuda8.0 cudnn6
- **GPU model and memory**:Tesla K80 11439MB
- **Exact command to reproduce**: import tensorflow.contrib.mpi as mpi

### Describe the problem
I have built a docker image with openpai and tensorflow according to the [official tutorial](https://www.tensorflow.org/install/install_sources) and [this document](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/mpi). During the docker building process, nothing wrong. But, I cannot use mpi in the built docker. When I import mpi, it throws **ImportError: No module named mpi**. And when I run a tensorflow job `python code/benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py --local_parameter_device=cpu --batch_size=32 --model=resnet20 --variable_update=parameter_server --data_dir=$PAI_DATA_DIR --data_name=cifar10 --train_dir=$PAI_OUTPUT_DIR --ps_hosts=$PAI_TASK_ROLE_ps_server_HOST_LIST --worker_hosts=$PAI_TASK_ROLE_worker_HOST_LIST --job_name=ps --task_index=$PAI_CURRENT_TASK_ROLE_CURRENT_TASK_INDEX --server_protocol=grpc+mpi `, it throw error too. See [this issue](https://github.com/Microsoft/pai/issues/1336) in openpai to find the detail.

### Source code / logs
Here is the [base docker file](https://github.com/Microsoft/pai/blob/master/examples/Dockerfiles/cuda8.0-cudnn6/Dockerfile.build.base)
Here is the [mpi docker file](https://github.com/Microsoft/pai/blob/master/examples/Dockerfiles/cuda8.0-cudnn6/Dockerfile.build.mpi)
And here is my [tensorflow docker file](https://github.com/Microsoft/pai/blob/77715f758dcdb9fb6c68b3f931703d4cd20db822/examples/mpi/Dockerfile.example.tensorflow-mpi)
"
22343,How to penalize the loss of one class more than the other in tensorflow for a multi class problem?,"Let's say my model has two classes Class 1 and Class 2. Both Class 1 and Class 2 has a equal amount of training and testing data. But I want to penalize the loss of the Class 1 more than Class 2, so that one class has a fewer number of False Positives than the other (I want the model to perform better for one class than the other). "
22342,tensorflow import error(tf.estimator package not installed),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Centos 7
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:1.10.0
- **Python version**:anaconda python 3.6.5
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None
- **Exact command to reproduce**: import tensorflow

I have installed anaconda 3 on Centos 7 and instlall tensorflow with pip.
Just import tensorflow and the error says ""tf.estimator package not installed."" I am not able to import tensorflow.estimator().
I have figured out one way to avoid this is to import msgpack or distributed before import tensorflow and it works just fine. But I am still looking for the right fix.
"
22341,Feature Request: Add quadratic weighted kappa,"### System information
- **Have I written custom code**: No
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **Mobile device if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.10.0
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: GTX 1070 8G
- **Exact command to reproduce**: N/A

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem

Quadratic weighted kappa (qwk) is a frequently used evaluation criteria, and is used in many competitions, such as the Kaggle Diabetic Retinopathy Detection, Prudential Life Insurance Assessment, and Crowdflower Search Results Relevance, etc. 

It would be very helpful for those who actively take part in those competitions if a quadratic_weighted_kappa function can be added into tf.contrib.metrics.

Also, we already have cohen's kappa (tf.contrib.metrics.cohen_kappa) and confusion matrix (tf.contrib.metrics.confusion_matrix -> tf.confusion_matrix) . Adding qwk is consistent with the existing codes. 

Again, please add quadratic weighted kappa into tensorflow.


"
22339,pip official package compiled differently with docker package?,"I found in docker container, tensorflow is installed from `http://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.7.0-cp27-none-linux_x86_64.whl`
not official pip source `https://pypi.org/project/tensorflow/`. 

And i am working on filesystem plugin, a .so file, which works fine with official tensorflow pip package, but with tensorflow from `http://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.7.0-cp27-none-linux_x86_64.whl`, i got error `_ZN10tensorflow10FileSystem10FilesExistERKSt6vectorISsSaISsEEPS1_INS_6StatusESaIS6_EE`.

Seems tensorflow from official source compiled differently with `http://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.0.0-cp27-none-linux_x86_64.whl` ?

Why and how to solve this? How can i  compile a filesystem plugin compatible to `http://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.0.0-cp27-none-linux_x86_64.whl`?"
22338,using bazel to build tensorflow.dll,"i try using bazel to build on windows, but tips:

**Have I written custom code** No
**OS Platform and Distribution** WIN7 Visual Studio 2015
**TensorFlow installed from** source master
**Bazel version** 0.17.1
**CUDA/cuDNN version** CUDA9.0 / CUDNN 7.2.1
**GPU model and memory** 1080Ti / 11G
**Mobile device** N/A
**Exact command to reproduce**
1. python ./configure.py
```
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by com.google.protobuf.UnsafeUtil (file:/C:/U
sers/tao/_bazel_tao/install/360a69cc0fef41d83a4dd948c0474ca5/_embedded_binaries/
A-server.jar) to field java.lang.String.value
WARNING: Please consider reporting this to the maintainers of com.google.protobu
f.UnsafeUtil
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflect
ive access operations
WARNING: All illegal access operations will be denied in a future release
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Ba
zel server using the command ""bazel shutdown"".
You have bazel 0.17.1 installed.
Please specify the location of python. [Default is C:\Users\tao\AppData\Local\Pr
ograms\Python\Python35\python.exe]:

Found possible Python library paths:
  C:\Users\tao\AppData\Local\Programs\Python\Python35\lib\site-packages
Please input the desired Python library path to use.  Default is [C:\Users\tao\A
ppData\Local\Programs\Python\Python35\lib\site-packages]

Do you wish to build TensorFlow with nGraph support? [y/N]:
No nGraph support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the CUDA SDK version you want to use. [Leave empty to default to
CUDA 9.0]:

Please specify the location where CUDA 9.0 toolkit is installed. Refer to README
.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/
CUDA/v9.0]:


Please specify the cuDNN version you want to use. [Leave empty to default to cuD
NN 7.0]:


Please specify the location where cuDNN 7 library is installed. Refer to README.
md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/C
UDA/v9.0]:

Please specify a list of comma-separated Cuda compute capabilities you want to b
uild with.
You can find the compute capability of your device at: https://developer.nvidia.
com/cuda-gpus.
Please note that each additional compute capability significantly increases your
 build time and binary size. [Default is: 3.5,7.0]:

Please specify optimization flags to use during compilation when bazel option ""-
-config=opt"" is specified [Default is /arch:AVX]:

Would you like to override eigen strong inline for some C++ compilation to reduc
e the compilation time? [Y/n]:
Eigen strong inline overridden.
```


2.bazel build --config=opt --config=cuda //tensorflow:libtensorflow.so
```
bazel build --config=opt --config=cuda //tensorflow:libtensorflow.so will be generated tensorflow.so and tensorflow.lib ?
```
Then rename the libtensorflow.so to tensorflow.dll, it can be work? @meteorcloudy"
22336,ssd_mobilenet_v1_quantized_300x300_coco14_sync's mAP=0,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform Ubuntu 16.04
- **TensorFlow installed from source: models's commit = 55d55abc71483723743c0273b9c1fd8e0c7d8391 and tensorflow's commit = 6c3e1f4bc803f2dc8a804f4f15ada0eda6c90a18
- **TensorFlow version **:1.9.0
- **Python version**:Python 3.5.2 (default, Nov 23 2017, 16:37:01)
- **Bazel version (if compiling from source)**: 0.11.1
- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
- **CUDA/cuDNN version**: Cuda compilation tools, release 9.0, V9.0.176;cuDNN7.0
- **GPU model and memory**: GeForce GTX 1070 Ti  * 4
- **Exact command to reproduce**:
python object_detection/model_main.py --pipeline_config_path=./object_detection/samples/configs/ssd_mobilenet_v1_quantized_300x300_coco14_sync.config  --model_dir=/checkpoint/ssd_mobilenet_v1_quantized_300x300_coco14_sync --checkpoint_dir=/ssd/ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18 --run_once=True

### Describe the problem
0.
train		118287
wget -c http://images.cocodataset.org/zips/train2017.zip 
wget -c http://images.cocodataset.org/annotations/annotations_trainval2017.zip
eval		5000
wget -c http://images.cocodataset.org/zips/val2017.zip 
wget -c http://images.cocodataset.org/annotations/stuff_annotations_trainval2017.zip
1
Download checkpoint in http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18.tar.gz
2
Amend file's path in models/object_detection/samples/configs/ssd_mobilenet_v1_quantized_300x300_coco14_sync.config
3.
from models/research,
python object_detection/model_main.py --pipeline_config_path=./object_detection/samples/configs/ssd_mobilenet_v1_quantized_300x300_coco14_sync.config  --model_dir=/checkpoint/ssd_mobilenet_v1_quantized_300x300_coco14_sync --checkpoint_dir=/ssd/ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18 --run_once=True

### Source code / logs
1
[ssd_mobilenet_v1_quantized_300x300_coco14_sync.zip](https://github.com/tensorflow/tensorflow/files/2391202/ssd_mobilenet_v1_quantized_300x300_coco14_sync.zip)
2
[ssd_mobilenet_v1_quantized_300x300_coco14_sync_log.txt](https://github.com/tensorflow/tensorflow/files/2391211/ssd_mobilenet_v1_quantized_300x300_coco14_sync_log.txt)





"
22333,TensorFlowlite   it is probably compressed,"android5.1
Windows
androidstudio3.0.1
 yolo_v2_7_448.tflite
AssetFileDescriptor fileDescriptor = assets.openFd(modelFilename)This file can not be opened as a file descriptor; it is probably compressed"
22332,Alternative to http://www.image-net.org/challenges/LSVRC/ for tflite accuracy tool?,"I was looking into using the tflite accuracy tool provided here: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/tools/accuracy/ilsvrc. The documentation mentions that we need to download the devkit from http://www.image-net.org/challenges/LSVRC/. Although, it seems like image-net.org has been down for several days.

Is there any alternative repository for the same data which I can use to run the tflite accuracy tool?
"
22331,broken link in documentation for autograph,"This page from docs links to the limitations, but in the wrong subfolder:

https://www.tensorflow.org/guide/autograph

WRONG PATH:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/autograph/LIMITATIONS.md

> AutoGraph helps you write complicated graph code using normal Python. Behind the scenes, AutoGraph automatically transforms your code into the equivalent TensorFlow graph code. AutoGraph already supports much of the Python language, and that coverage continues to grow. For a list of supported Python language features, see the Autograph capabilities and limitations.

But the original Jupyter Notebook refers to the markdown file in the correct subfolder:

https://github.com/tensorflow/docs/blob/master/site/en/guide/autograph.ipynb
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/LIMITATIONS.md

"
22330,Eigenvalue decomposition of asymmetric matrices,"There is no general eigenvalue decomposition available in TensorFlow.  ```tf.self_adjoint_eig``` currently only supports self-adjoint matrices.  A full implementation of the eigenvalue decomposition for more general (diagonalizable) matrices could be very useful.  Even a version without gradients would be helpful.
"
22329,Error when loading model from InputStream in Tensorflow Lite,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Samsung Galaxy, android 6.0 API 23 
- **TensorFlow installed from (source or binary)**: gradle
- **TensorFlow version (use command below)**: tensorflow-android:1.9.0
- **Python version**: no python
- **Bazel version (if compiling from source)**: no bazel
- **GCC/Compiler version (if compiling from source)**: no compiler
- **CUDA/cuDNN version**: no version
- **GPU model and memory**: Mobile
- **Exact command to reproduce**: initialize tensorflowInferenceInterface with a InputBuffer > 17 MO

### Describe the problem
I use Tensorflow Lite since Tensorflow mobile is not ready for production yet.

When I tried to directly load Inception V3 from a file from External storage, it returns a OOM error

By looking at the code, I saw : 
      `byte[] buf = new byte[16384];` 
at the line 143 of TensorflowInferenceInterface

but my network uses more than 16384
I think it should use baosInitSize
      `int baosInitSize = is.available() > 16384 ? is.available() : 16384;` at line 140

### Source code / logs
java.lang.OutOfMemoryError: Failed to allocate a 87514870 byte allocation with 16773184 free bytes and 29MB until OOM
"
22326,CollectiveAllReduceStrategy errors with official models,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
I modified the tensorflow/models repo utility function to use CollectiveAllReduceStrategy instead of MirroredStrategy. See the one-line change at: https://gist.github.com/nvcastet/60b8c0c66da4cf2949e38fc790208a1c#file-distribution_utils-py-L47
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04.4 ppc64le
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
1.11
- **Python version**: Python 2.7.15
- **CUDA/cuDNN version**: Cuda 9.2 CuDNN 7.2.1
- **GPU model and memory**: V100 16GB 
- **Exact command to reproduce**: ~/models/official/mnist$ python mnist.py --num_gpus 2
### Describe the problem
Using the MNIST model script from the tensorflow/models repo after modifying the distribution_utils.py utility file to use CollectiveAllReduceStrategy (see modification above).
`python mnist.py --num_gpus 2` crashes with
```
2018-09-17 19:28:36.648966: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at collective_ops.cc:210 : Internal: Second consumer arrived for key 10003:0:0:0:0:1
         [[{{node conv2d_1/bias/replica_1/Initializer/CollectiveBcastRecv}} = CollectiveBcastRecv[T=DT_FLOAT, _class=[""loc:@conv2d_1/bias/replica_1/Assign""], group_key=1, group_size=2, instance_key=10003, shape=[64], _device=""/job:localhost/replica:0/task:0/device:GPU:1""]()]]
         [[{{node GroupCrossDeviceControlEdges_0/group_deps_27/_2}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:1"", send_device_incarnation=1, tensor_name=""edge_238_GroupCrossDeviceControlEdges_0/group_deps_27"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
```
Full log of the run with stack trace: [run.log](https://github.com/tensorflow/tensorflow/files/2390053/run.log)"
22325,bazel build tensorflow on windows 10 getting cudnn.h- system cannot find the file specified,"From stackoverflow:

https://stackoverflow.com/questions/52335703/bazel-build-tensorflow-on-windows-10-getting-cudnn-h-system-cannot-find-the-fil

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: 1.11.0
- **TensorFlow version (use command below)**:  
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: 0.16.0
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:  9.2/7.2.1
- **GPU model and memory**: Nvidia M1000M
- **Exact command to reproduce**:

Additional information:
I've added these to my path (and rebooted):
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.2\bin
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.2\libnvvp
C:\tools\msys64
C:\tools\bazel
C:\tools\bazel\bazel.exe
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.2\extras\CUPTI\libx64

These are the system variables I've set:
BAZEL_SH  C:\tools\msys64\usr\bin\bash.exe
BAZEL_VC  C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC
BAZEL_VS  C:\Program Files (x86)\Microsoft Visual Studio 14.0

### Describe the problem
I keep getting this error when trying to build the tensorflow-gpu using bazel and python in Windows 10:

Cuda Configuration Error: Error reading C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.2/cuda/include/cudnn.h: ja
va.io.IOException: ERROR: src/main/native/windows/processes-jni.cc(239): CreateProcessW(""grep"" --color=never -A1 -E ""#de
fine CUDNN_MAJOR"" ""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.2/cuda/include/cudnn.h""): The system cannot fin
d the file specified.
This is the command I'm trying to run:
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

I've confirmed the C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.2/cuda/include/cudnn.h is there.

I've tried running it from VS2015 x64 Native Tools Command Prompt, cmd, and powershell and get the same error.

I'm using bazel 0.16.1, CUDA 9.2, Anaconda3 (Python 3.6.5), and CUDNN 7.2.1. I ""installed"" the CUDDNN files by unzipping its cuda folder into my C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.2 folder (i.e. the whole ""cuda"" folder). I specified the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.2\cuda path in the cudnn path question when I ran the configure.py. The configure.py completes without error.

I also tried putting the CUDNN files directly in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.2 folder (instead of a cuda folder in there) and specified the default location and still get basically the same error: ""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.2/include/cudnn.h"": The system cannot find the file specified.

This is the full error:

ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package'
: Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Trac
eback (most recent call last):
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1458
                _create_local_cuda_repository(repository_ctx)
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1185, in _create_local_cuda_repository
                _get_cuda_config(repository_ctx)
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 909, in _get_cuda_config
                _cudnn_version(repository_ctx, cudnn_install_base..., ...)
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 584, in _cudnn_version
                find_cuda_define(repository_ctx, cudnn_header_dir, ""c..."", ...)
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 534, in find_cuda_define
                auto_configure_fail((""Error reading %s: %s"" % (str(h...)))
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 315, in auto_configure_fail
                fail((""\n%sCuda Configuration Error:%...)))

Cuda Configuration Error: Error reading C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.2/cuda/include/cudnn.h: ja
va.io.IOException: ERROR: src/main/native/windows/processes-jni.cc(239): CreateProcessW(""grep"" --color=never -A1 -E ""#de
fine CUDNN_MAJOR"" ""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.2/cuda/include/cudnn.h""): The system cannot fin
d the file specified.

WARNING: Target pattern parsing failed.
ERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_
defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1458
                _create_local_cuda_repository(repository_ctx)
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1185, in _create_local_cuda_repository
                _get_cuda_config(repository_ctx)
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 909, in _get_cuda_config
                _cudnn_version(repository_ctx, cudnn_install_base..., ...)
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 584, in _cudnn_version
                find_cuda_define(repository_ctx, cudnn_header_dir, ""c..."", ...)
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 534, in find_cuda_define
                auto_configure_fail((""Error reading %s: %s"" % (str(h...)))
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 315, in auto_configure_fail
                fail((""\n%sCuda Configuration Error:%...)))

Cuda Configuration Error: Error reading C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.2/cuda/include/cudnn.h: ja
va.io.IOException: ERROR: src/main/native/windows/processes-jni.cc(239): CreateProcessW(""grep"" --color=never -A1 -E ""#de
fine CUDNN_MAJOR"" ""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.2/cuda/include/cudnn.h""): The system cannot fin
d the file specified.


### Source code / logs
Solution/workaround is:

The problem is a bug in cuda_configure.bzl: it uses ctx.execute instead of ctx.action.run_shell.
To work around it: add c:\tools\msys64\usr\bin to your PATH. That's where grep.exe is so ctx.execute can find grep on the PATH.
"
22322,Error compiling/importing a custom (GPU) op on Windows w/ Bazel and TF r1.11,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Pro x64 17134.x
- **TensorFlow installed from (source or binary)**: binary, w/ Bazel
- **TensorFlow version (use command below)**: r1.11
- **Python version**: 3.6.5 x64
- **Bazel version (if compiling from source)**: 17.1
- **GCC/Compiler version (if compiling from source)**: VS 2015 C++ tools
- **CUDA/cuDNN version**: CUDA 9.0, cuDNN 7
- **GPU model and memory**: GTX1080
- **Exact command to reproduce**: see below

### Describe the problem
I saw that the support for building TF on Windows with Bazel has been added recently, so I followed the [official docs](https://www.tensorflow.org/install/install_sources_windows) and proceeded to setup my environment, clone TF r1.11 and build the library (with GPU support) to make sure everything was compiling fine. This worked without issues, so that's something.

Then, I tried to compile a custom GPU op following the [docs here](https://www.tensorflow.org/extend/adding_an_op#compile_the_op_using_bazel_tensorflow_source_installation) and hit a roadblock. I'm not sure if this is officially supported yet, but since I saw you guys added the ability to load user ops on Windows too a while back (plus the fact that building TF on Windows now works fine), it seemed plausible at this point.

I wrote a dummy GPU op (code [here](https://gist.github.com/Sergio0694/2b463571e6f82519137e9785caf90b08) and kernel [here](https://gist.github.com/Sergio0694/48a5bb39ea92494371b3c847d23d09f6)) and used the `BUILD` script as explained in the docs (replacing `.so` with `.dll`), but the build failed with the [this output log](https://gist.github.com/Sergio0694/3f379c9007fa2abd588dac5be0fe996e).

I also tried to build the included `fact.cc` op (replacing `.so` again with `.dll`), which compiled fine (as that op is CPU only), but when I tried to import the module into TF it failed with the usual ""can't find entry point in dynamic library..."" error.

So here are my two questions:
1) Is there an official statement about the ability to build GPU ops on Windows, now that TF supports building the rest of the library on Windows?
2) If so (I mean, the whole TF library builds just fine, so I guess I'm just doing something wrong here), what's the right procedure to build a custom GPU op on Windows with Bazel? Would it be possible to update the docs so that every TF user on Windows would be able to do the same as well?

Thanks! 
"
22321,Distributed training fails when I use CollectiveAllReduceStrategy,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
I slightly updated mnist.py example so that it uses CollectiveAllReduceStrategy. Updated version is [here](https://github.com/dmitrievanthony/models/blob/ignite/official/mnist/mnist.py#L208).

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Mac OS 10.13.3

- **TensorFlow installed from (source or binary)**:
Binary

- **TensorFlow version (use command below)**:
1.11.0-dev20180913

- **Python version**:
3.6.3

- **Exact command to reproduce**:
See [updated example](https://github.com/dmitrievanthony/models/blob/ignite/official/mnist/mnist.py#L208).

### Describe the problem

Hi, 

I'm trying to update `mnist` model from [official repository](https://github.com/tensorflow/models/tree/master/official/) so that it uses CollectiveAllReduceStrategy as it's shown in [keras_model_to_estimator_client.py](https://github.com/tensorflow/ecosystem/blob/master/distribution_strategy/keras_model_to_estimator_client.py). Updated example you can find [here](https://github.com/dmitrievanthony/models/blob/ignite/official/mnist/mnist.py#L208). Unfortunately, it fails on `deepcopy` of run config.

### Source code / logs
```
Traceback (most recent call last):
  File ""mnist.py"", line 286, in <module>
    absl_app.run(main)
  File ""/Users/antondmitriev/anaconda3/lib/python3.6/site-packages/absl/app.py"", line 274, in run
    _run_main(main, args)
  File ""/Users/antondmitriev/anaconda3/lib/python3.6/site-packages/absl/app.py"", line 238, in _run_main
    sys.exit(main(argv))
  File ""mnist.py"", line 280, in main
    run_mnist(flags.FLAGS)
  File ""mnist.py"", line 226, in run_mnist
    'data_format': data_format,
  File ""/Users/antondmitriev/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 190, in __init__
    model_dir)
  File ""/Users/antondmitriev/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1591, in maybe_overwrite_model_dir_and_session_config
    config = run_config.RunConfig.replace(config, session_config=session_config)
  File ""/Users/antondmitriev/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/run_config.py"", line 849, in replace
    copy.deepcopy(self),
  File ""/Users/antondmitriev/anaconda3/lib/python3.6/copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""/Users/antondmitriev/anaconda3/lib/python3.6/copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""/Users/antondmitriev/anaconda3/lib/python3.6/copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""/Users/antondmitriev/anaconda3/lib/python3.6/copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""/Users/antondmitriev/anaconda3/lib/python3.6/copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""/Users/antondmitriev/anaconda3/lib/python3.6/copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""/Users/antondmitriev/anaconda3/lib/python3.6/copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""/Users/antondmitriev/anaconda3/lib/python3.6/copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""/Users/antondmitriev/anaconda3/lib/python3.6/copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""/Users/antondmitriev/anaconda3/lib/python3.6/copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""/Users/antondmitriev/anaconda3/lib/python3.6/copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""/Users/antondmitriev/anaconda3/lib/python3.6/copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""/Users/antondmitriev/anaconda3/lib/python3.6/copy.py"", line 169, in deepcopy
    rv = reductor(4)
TypeError: can't pickle _thread._local objects
```
"
22320,fatal error: google/protobuf/inlined_string_field.h: No such file or directory  #include <google/protobuf/inlined_string_field.h>,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: Commit cac963862be3faa421c559f39033c9bfb3b27a51
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Follow up to issue #22240. I was able to fix the absl issue by pulling and copying over the absl folder to the external folder but I think there is an issue with the protobufs.

After following this tutorial: https://tuatini.me/building-tensorflow-as-a-standalone-project/ and running the following command:
g++ -std=c++11 -Wl,-rpath='$ORIGIN/lib' -Iinclude -Llib main.cpp -ltensorflow_cc -o exec

I'm getting the error ""This file was generated by a newer version of protoc which is incompatible with your Protocol Buffer headers. Please update"" and ""google/protobuf/inlined_string_field.h: No such file or directory  #include <google/protobuf/inlined_string_field.h>"" 

After looking through the commits I think the protobuf url is outdated.

![screenshot from 2018-09-17 12 39 06](https://user-images.githubusercontent.com/14967965/45640040-2cc85080-ba77-11e8-90e6-95fa13d909cd.png)


### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22319,Distributed training fails when I use FixedLengthRecordDataset,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
I slightly updated `keras_model_to_estimator_client.py` example so that it uses FixedLengthRecordDataset. Updated version is [here](https://github.com/dmitrievanthony/ecosystem/blob/ignite/distribution_strategy/keras_model_to_estimator_client.py).

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 17.04

- **TensorFlow installed from (source or binary)**:
Binary

- **TensorFlow version (use command below)**:
1.11.0-dev20180917

- **Python version**:
3.6.3

- **Exact command to reproduce**:
See [updated example](https://github.com/dmitrievanthony/ecosystem/blob/ignite/distribution_strategy/keras_model_to_estimator_client.py).

### Describe the problem
Hi,

I'm investigating TensorFlow distributed training approaches with help of @yuefengz who kindly helps me. So far I updated [keras_model_to_estimator_client.py](https://github.com/tensorflow/ecosystem/blob/master/distribution_strategy/keras_model_to_estimator_client.py) example so that it uses FixedLengthRecordDataset, but as result the whole code fails.

### Source code / logs

```
Traceback (most recent call last):
  File ""/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 2667, in gather
    return params.sparse_read(indices, name=name)
AttributeError: 'Tensor' object has no attribute 'sparse_read'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1628, in _create_c_op
    c_op = c_api.TF_FinishOperation(op_desc)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Shape must be at least rank 1 but is rank 0 for 'GatherV2' (op: 'GatherV2') with input shapes: [], [1], [].

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""keras_model_to_estimator_client.py"", line 134, in <module>
    tf.app.run(argv=sys.argv)
  File ""/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""keras_model_to_estimator_client.py"", line 129, in main
    eval_spec=tf.estimator.EvalSpec(input_fn=input_fn))
  File ""/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 462, in train_and_evaluate
    estimator, train_spec, eval_spec, _TrainingExecutor)
  File ""/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/python/distribute/estimator_training.py"", line 264, in train_and_evaluate
    session_config=run_config.session_config)
  File ""/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_coordinator.py"", line 798, in run_distribute_coordinator
    session_config, rpc_layer)
  File ""/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_coordinator.py"", line 344, in _run_single_worker
    worker_fn(strategy)
  File ""/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/python/distribute/estimator_training.py"", line 232, in _worker_fn
    hooks=list(train_spec.hooks))
  File ""/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 355, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1178, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File ""/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1246, in _train_model_distributed
    input_fn, model_fn_lib.ModeKeys.TRAIN, self._train_distribution)
  File ""/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1037, in _get_iterator_from_input_fn
    lambda: self._call_input_fn(input_fn, mode))
  File ""/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 480, in distribute_dataset
    self._prefetch_on_device)
  File ""/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/values.py"", line 814, in __init__
    worker_input, len(worker_device_map), i)
  File ""/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/input_ops.py"", line 140, in auto_shard_dataset
    return _auto_shard_impl(dataset=dataset, found_reader_op=False)
  File ""/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/input_ops.py"", line 107, in _auto_shard_impl
    dataset._input_dataset, found_reader_op)
  File ""/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/input_ops.py"", line 107, in _auto_shard_impl
    dataset._input_dataset, found_reader_op)
  File ""/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/input_ops.py"", line 107, in _auto_shard_impl
    dataset._input_dataset, found_reader_op)
  File ""/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/input_ops.py"", line 67, in _auto_shard_impl
    filenames_tensor, math_ops.range(index, num_files, num_shards))
  File ""/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 2669, in gather
    return gen_array_ops.gather_v2(params, indices, axis, name=name)
  File ""/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 3232, in gather_v2
    ""GatherV2"", params=params, indices=indices, axis=axis, name=name)
  File ""/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3274, in create_op
    op_def=op_def)
  File ""/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1792, in __init__
    control_input_ops)
  File ""/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1631, in _create_c_op
    raise ValueError(str(e))
ValueError: Shape must be at least rank 1 but is rank 0 for 'GatherV2' (op: 'GatherV2') with input shapes: [], [1], [].
```
It looks like the exception happens in `input_ops.py` in `_auto_shard_impl` method when TensorFlow tries to shard file names. Unfortunately, I don't clearly understand this command to be honest: 

```
 sharded_filenames_tensor = array_ops.gather(
            filenames_tensor, math_ops.range(index, num_files, num_shards))
```
"
22318,"Hi when i run the this code[ input_layer = tf.reshape(features[""x""], [-1, 28, 28, 1])] then it give this error     [   1 input_layer = tf.reshape(features[""x""], [-1, 28, 28, 1])  NameError: name 'features' is not defined ]] can u tell me that how i fix it thanks","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22317,"Hi when i run the this code[ input_layer = tf.reshape(features[""x""], [-1, 28, 28, 1])]","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22316,Hi ,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22315,Ji,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22314,Problems after installing tensorflow with pip,"Hi I encounter following issue, when I run ""import tensorflow as tf"" with python

install command I used:
`pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.12.0-py3-none-any.whl`

python version
`Python 3.7.0`

```
Traceback (most recent call last):
  File ""C:\Users\victor\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])
  File ""C:\Users\victor\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 297, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\victor\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\victor\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\victor\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow_internal
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\victor\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\victor\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\victor\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 72, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\victor\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])
  File ""C:\Users\victor\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 297, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\victor\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\victor\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\victor\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow_internal
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```

- 
What's the problem here? Thank you for your help!"
22311,ERROR when reload  shared_embedding_columns with partitioner ,"- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: 
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:1.10.1
- **Python version**:3.5.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:NO
- **GPU model and memory**:NO
- **Exact command to reproduce**:

### Describe the problem
When use shared_embedding_columns without partitioner  to save model ,then import_meta_graph from check_point is success. But use shared_embedding_columns with partitioner, then import_meta_graph from check_point is failed.   
Report error: KeyError: 
The name 'run_2/input_from_feature_columns/a1_shared_embedding/a1_a1_shared_embedding' refers to an Operation not in the graph

### Source code / logs

code for model
first, mkdir mypath/model
````
from tensorflow.contrib.layers.python.layers import feature_column as fc
from tensorflow.python.ops import variable_scope
from tensorflow.python.framework import sparse_tensor as sparse_tensor_lib
from tensorflow.contrib.layers.python.layers import feature_column_ops
from tensorflow.python.training import saver
import tensorflow as tf

def mySharedEmbeddingColumn():
    a1 = fc.sparse_column_with_keys(""a1"", [""marlo"", ""omar"", ""stringer""])
    b = fc.shared_embedding_columns([a1, a1], dimension=4, combiner=""mean"")
    input_tensor_c1 = sparse_tensor_lib.SparseTensor(
        indices=[[0, 0], [1, 1], [2, 2]], values=[0, 1, 2], dense_shape=[3, 3])
    dnn_partitioner = (
        tf.min_max_variable_partitioner(
        max_partitions=1))
    with variable_scope.variable_scope(""run_2"",partitioner=dnn_partitioner):
      b2 = feature_column_ops.input_from_feature_columns({
          b[1]: input_tensor_c1
      }, [b[1]])
    saver = tf.train.Saver()
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        variable_names = [v.name for v in tf.trainable_variables()]
        values = sess.run(variable_names)
        for k,v in zip(variable_names, values):
            print(""Variable: "", k)
            print(""Shape: "", v.shape)
            print(v)
        saver.save(sess,'model/model-ckt')

mySharedEmbeddingColumn()
````
code for restore
````
import tensorflow as tf

graph = tf.Graph()
sess = tf.Session(graph=graph)
with graph.as_default():
    del tf.get_collection_ref(tf.GraphKeys.TRAIN_OP)[:]
    check_point_path = 'model' 
    saver = tf.train.import_meta_graph('model/model-ckt.meta')
    print(graph.get_operations())

````

"
22310,Tflite label_image resize bug,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
NA
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:
1.10
- **Python version**:
3.5
- **Bazel version (if compiling from source)**:
0.15
- **GCC/Compiler version (if compiling from source)**:
NA
- **CUDA/cuDNN version**:
NA
- **GPU model and memory**:
NA
- **Exact command to reproduce**:
NA

### Describe the problem
In the `label_image.cc` example of TFlite, a resize function present in `bitmap_helpers_impl.h` is being called in line [171](https://github.com/tensorflow/tensorflow/blob/4dcfddc5d12018a5a0fdca652b9221ed95e9eb23/tensorflow/contrib/lite/examples/label_image/label_image.cc#L171) - 
`   resize<float>(interpreter->typed_tensor<float>(input), in.data(),`

In the implementation of the function in `bitmap_helpers_impl.h`, the following [line](https://github.com/tensorflow/tensorflow/blob/4dcfddc5d12018a5a0fdca652b9221ed95e9eb23/tensorflow/contrib/lite/examples/label_image/bitmap_helpers_impl.h#L90) 
```
  auto output_number_of_pixels =
      wanted_height * wanted_height * wanted_channels;
```
assumes that for the input image, the height and width must be the same. And this works for the mobilenet example since the input size is 224 x 224. But, this would cause issues for any network where the expected input size does not have the same width and height. It should instead be - 
```
   auto output_number_of_pixels =
      wanted_width * wanted_height * wanted_channels;
```

### Source code / logs
NA
"
22309,About partitioned variable is Disabled in DistributionStrategy,"### System information
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): master
Python version: 2.7
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:
Exact command to reproduce:

### Describe the problem

I noticed that the partitioned variable is disabled in base class `DistributionStrategy` in current master branch. However, some subclasses like ParameterServerStrategy is okay to use partitioner in current architecture.  So I want to know the reason for  disabling the partitioner in base class? Does MirroredValue can use that partitioner?

And I want to do some code change in base class `DistributionStrategy` to make a pull request: To rename the  function `disable_partitioned_variables` to `distributed_custom_getter` in base class and move them out of  function `scope` to be a member function of `DistributionStrategy`. The subclasses should override this function to control the custom getter. Do you think this is a reasonable design ? If it is Okay, I will make a code review."
22307,Tensorflow library loading order causes segmentation fault ,"My application depends on both RocksDB and tensorflow. If I load rocksDB shared library before loading the tensorflow model in my application then it results in SIGSEGV. However, if the loading order is reversed then there're no exceptions(and it works fine as expected). 

Here're the external dependencies of my application:
Tensorflow  version: 1.6
RocksDB version: 5.7.3
Java version: JDK-8
OS platform and distribution: RedHat-7(RHEL-7.2)

The following code uses both RocksDB and tensorflow results in segmentation fault:

```java
public class LibLoad{
    public static String absPath(String relativePath) {
	return System.getProperty(""user.dir"") + ""/"" + relativePath;
    }
    public static void main(String[] args) throws InterruptedException {
      System.load(LibLoad.absPath(""libs/librocksdbjni6775043347215777920.so""));
      System.load(LibLoad.absPath(""libs/tensorflow_native_libraries-1534022257987-0/libtensorflow_framework.so""));
    }
}
```

Actual exception:
```
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x0000000000000000, pid=3773, tid=140566680151808
#
# JRE version: Java(TM) SE Runtime Environment (8.0_05-b13) (build 1.8.0_05-b13)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.5-b02 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  0x0000000000000000
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ulimit -c unlimited before starting Java again
#
# An error report file with more information is saved as:
# /export/content/lid/apps/careers-banzai-jobs-embedding-samza/dev-i001/hs_err_pid3773.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.sun.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
  
6  <signal handler called>
#7  0x0000000000000000 in ?? ()
#8  0x00007f17b8dbfbb0 in pthread_once () from /lib64/libpthread.so.0
#9  0x00007f16dd60545a in void std::call_once<void (&)()>(std::once_flag&, void (&)()) ()
   from libtensorflow_framework.so
#10 0x00007f16dd60549e in tensorflow::port::TestCPUFeature(tensorflow::port::CPUFeature) ()
 
```

Similar issues had been reported previously in the community and the suggestion solution was to load tensorflow shared library before the RocksDB shared library. 

1. Just curious about the actual root cause of this segmentation fault. Can someone help me on how to get to the bottom of it.
2. What is the recommended long-term solution for this problem? Does the same codeflow that caused segmentation fault when loading the tensorflow model get triggered in other scenarios as well?

Any help for solving this problem is greatly appreciated."
22306,why options in session_config like intra_op_parallelism_threads/inter_op_parallelism_threads doesn't work in tensorflow estimator distribution mode,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:just tried to import it.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:CentOS Linux release 7.2
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.7 to 1.10
- **Python version**:2.7
- **Bazel version (if compiling from source)**:N/A
- **GCC/Compiler version (if compiling from source)**:N/A
- **CUDA/cuDNN version**:N/A
- **CPU model and memory**:Intel(R) Xeon(R) CPU E5-2670 v3 @ 2.30GHz,128G
- **Exact command to reproduce**:N/A

### Describe the problem
when i set intra_op_parallelism_threads/inter_op_parallelism_threads for estimator dist mode like this:
`config = tf.ConfigProto()
config.intra_op_parallelism_threads = 4
config.inter_op_parallelism_threads = 4
run_config = tf.estimator.RunConfig().replace(session_config=config)
model = tf.estimator.DNNLinearCombinedClassifier(
            model_dir=model_dir,
            linear_feature_columns=wide_columns,
            dnn_feature_columns=deep_columns,
            dnn_hidden_units=hidden_units,
            config=run_config)`
it doesn't work, i find the options isn't passed to kernals:

tensorflow/python/estimator/training.py
`if config.session_config is None:
  session_config = config_pb2.ConfigProto(log_device_placement=False)
else:
  session_config = config_pb2.ConfigProto(
	  log_device_placement=False,
	  gpu_options=config.session_config.gpu_options)`
only a few options are set, the code above indicate intra_op_parallelism_threads/inter_op_parallelism_threads have no effectis a bugwhy
"
22305,Use own model on android demo(TF_OD) make app crash,"My stack overflow question: https://stackoverflow.com/questions/52309707/use-own-model-on-android-demo-tf-od-but-app-was-crash-after-open-camera

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
    I had change camera from back to front.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
    Windows10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
    Zenfone 5z
- **TensorFlow version (use command below)**:
    v1.10.0
- **Python version**:
    Python 3.5.4    
- **GPU model and memory**:
    GTX 950M,  Memory 8GB
- **TensorFlow installed from**:
    CMD
- **CUDA/cuDNN version**:
    CUDA v9.0  /  cuDNN v7.0
- **Bazel version**:
    0.11.1
- **Exact command to reproduce**:
    N/A


-----------------------------------------------------------------------------------------------------------------
I trained my model with this tutorial: https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10

When I use the default model(ssd_mobilenet_v1_android_export.pb), the app can run correctly, but when I use my own model, the app was crash without detecting anything.

I had change TF_OD_API_MODEL_FILE and TF_OD_API_LABELS_FILE to my file,  also I I created a labelmap.txt , that contains all the label what i trained, and I reserve the first line with ""???""

What I confused is the app can open the camera and display a little time, but before detecting anything, the app was crash. So I think the problem maybe is my model wrong, but when I used my model with python code, it can run well and detect correctly.

So why my own model use on android demo(TF_OD) make app crash?


"
22304,C++   set_allow_soft_placement does not work,"

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution: Mac  CPU
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 2.7

### Describe the problem

I have set set_allow_soft_placement(true), but it does not work.



### Source code / logs


c++ source code
```c++
    const string pathToGraph = tf/checkpoint/model-1.meta"";

    tensorflow::SessionOptions options;
    options.config.set_allow_soft_placement(true);  // why this does not work?
    auto session = NewSession(options);

    Status status = session->Create(graph_def.graph_def());

```

It get errors in my mac(cpu),  no error in GPU.

```sh
./eval
libc++abi.dylib: terminating with uncaught exception of type std::runtime_error: 
Error creating graph: Invalid argument: Cannot assign a device for 
operation 'ExponentialMovingAverage/AssignMovingAvg_5/sub': 
Operation was explicitly assigned to /device:GPU:0 but available devices are 
[ /job:localhost/replica:0/task:0/device:CPU:0 ]. 
Make sure the device specification refers to a valid device.
```
"
22300,Can't import tensorflow,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: just tried to import it.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 x64
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: tried 1.10 down to 1.4, none worked, only cpu versions.
- **Python version**: 3.7 (also tried with 3.6)
- **CPU model and memory**: Intel core-i3 6100 - Skylake architecture [Tried on another system with the same OS and even more advanced cpu, still didn't work.]
- **Exact command to reproduce**: import tensorflow

### Describe the problem
Can't import. I double checked everything. From github and stackoverflow issues to whether or not my cpu supports avx instructions (it does) and even enabled intel virutalization option from boot. **(Please consider these lines before tagging this as a duplicate.)**

### Source code / logs
Traceback (most recent call last):
  File ""FlowTest.py"", line 15, in <module>
    import tensorflow
  File ""C:\Users\there\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\there\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\there\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\there\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])
  File ""C:\Users\there\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 297, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\there\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\there\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\there\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow_internal
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
"
22299,Contributing to tf from windows environment,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:1.8
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:9.0/7.1
- **GPU model and memory**: Nvidia 1050 Ti, 4GB
- **Exact command to reproduce**:

### Describe the problem
I would like to contribute to tensorflow, however, I have not found any information on how to do it
on windows in Visual Studio 2017? Just building the code(apparently cmake is depricated?) and running it in windows? I have installed TF with GPU support ,CUDA, cuDNN as explained here: https://www.tensorflow.org/install/install_windows
But this is only for running the library? What if I, for instance, want to port a gradient from python to C++, rebuild the code and then run it and see if that particular gradient works, on Windows? I would just like to contribute to tf in the easiest way possible, from Windows.
"
22298,Can't initialize keras.Model based network with tf.train.init_from_chekpoint,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**: git master
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem

We are trying use `tf.keras.Model` as a base class for our neural network. So far, with `slim` based network we have used `tf.train.init_from_checkpoint` to warm start the training.
But, it seems that the current version of `tf.train.init_from_checkpoint` doesn't play well with `keras.Model`. Probably because under the hood, `tf.train.init_from_checkpoint` calls `variable_scope._get_defautl_variable_store()` which doesn't know variables created with keras objects.

Below a snippet of code to illustrate this problem

### Source code / logs

```
import tensorflow as tf

from tensorflow.python.ops import variable_scope as vs

net = tf.keras.Sequential([
    tf.keras.layers.Dense(10, name='fc1')
])

x = tf.random_uniform([10, 3])
y = net(x)

# all variable are created 
vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)
for v in vars:
    print(v.name)

# but variable scope doesn't see them
# then no variable to initialize for tf.train.init_from_checkpoint
var_store = vs._get_default_variable_store()._vars
print(var_store)
```"
22297,shared_embedding_columns  with partitioner  is  ERROR,"- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: 
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:1.10.1
- **Python version**:3.5.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:NO
- **GPU model and memory**:NO
- **Exact command to reproduce**:

### Describe the problem
When use shared_embedding_columns without partitioner  to save model ,then import_meta_graph from check_point is success. But use shared_embedding_columns with partitioner, then import_meta_graph from check_point is failed.   
Report error: KeyError: 
The name 'run_2/input_from_feature_columns/a1_shared_embedding/a1_a1_shared_embedding' refers to an Operation not in the graph

### Source code / logs

code for model
first, mkdir mypath/model
````
from tensorflow.contrib.layers.python.layers import feature_column as fc
from tensorflow.python.ops import variable_scope
from tensorflow.python.framework import sparse_tensor as sparse_tensor_lib
from tensorflow.contrib.layers.python.layers import feature_column_ops
from tensorflow.python.training import saver
import tensorflow as tf

def mySharedEmbeddingColumn():
    a1 = fc.sparse_column_with_keys(""a1"", [""marlo"", ""omar"", ""stringer""])
    b = fc.shared_embedding_columns([a1, a1], dimension=4, combiner=""mean"")
    c = fc.embedding_column(a1, dimension=4, combiner=""mean"")
    input_tensor_c1 = sparse_tensor_lib.SparseTensor(
        indices=[[0, 0], [1, 1], [2, 2]], values=[0, 1, 2], dense_shape=[3, 3])
    dnn_partitioner = (
        tf.min_max_variable_partitioner(
        max_partitions=1))
    with variable_scope.variable_scope(""run_2"",partitioner=dnn_partitioner):
      b2 = feature_column_ops.input_from_feature_columns({
          b[1]: input_tensor_c1
      }, [b[1]])
    with variable_scope.variable_scope(""run_3"", partitioner=dnn_partitioner):
      c1 = feature_column_ops.input_from_feature_columns({
          c: input_tensor_c1
      }, [c])
    saver = tf.train.Saver()
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        variable_names = [v.name for v in tf.trainable_variables()]
        values = sess.run(variable_names)
        for k,v in zip(variable_names, values):
            print(""Variable: "", k)
            print(""Shape: "", v.shape)
            print(v)
        saver.save(sess,'model/model-ckt')

mySharedEmbeddingColumn()
````
code for restore
````
import tensorflow as tf

graph = tf.Graph()
sess = tf.Session(graph=graph)
with graph.as_default():
    del tf.get_collection_ref(tf.GraphKeys.TRAIN_OP)[:]
    check_point_path = 'model' 
    saver = tf.train.import_meta_graph('model/model-ckt.meta')
    print(graph.get_operations())

````

"
22295,Allow grads to be scaled by tensors in tf.contrib.layers.optimize_loss(),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: 
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 
- **GPU model and memory**: TitanX Pascal
- **Exact command to reproduce**:

### Describe the problem

`tf.contrib.layers.optimize_loss()` currently allows gradient multipliers by passing a `gradient_multipliers` dictionary of `{variable_name: multiplier}` where `multiplier` is a python float. It would be useful to allow `multiplier` to be tensor so that gradient scaling could be, for example, tied to global_step.

It seems like this could easily be implemented  by not pushing the multiplier to a constant in `_multiply_gradients` function at https://github.com/tensorflow/tensorflow/blob/e37baff90ed84faef679bbf1e785e6790fd2739d/tensorflow/contrib/layers/python/layers/optimizers.py#L436-L437

Is there a reason why the dict shouldn't contain tensors that I'm overlooking?"
22294,Golang: could not determine kind of name for C.TF_UINT64,"I do have a issue with building my golang app. I'm getting this error while compiling my application. 

github.com/donutloop/imagedetection/vendor/github.com/tensorflow/tensorflow/tensorflow/go
vendor/github.com/tensorflow/tensorflow/tensorflow/go/tensor.go:42:24: could not determine kind of name for C.TF_UINT32
vendor/github.com/tensorflow/tensorflow/tensorflow/go/tensor.go:50:24: could not determine kind of name for C.TF_UINT64

go env
GOARCH=""amd64""
GOBIN=""""
GOCACHE=""/root/.cache/go-build""
GOEXE=""""
GOFLAGS=""""
GOHOSTARCH=""amd64""
GOHOSTOS=""linux""
GOOS=""linux""
GOPATH=""/go""
GOPROXY=""""
GORACE=""""
GOROOT=""/usr/local/go""
GOTMPDIR=""""
GOTOOLDIR=""/usr/local/go/pkg/tool/linux_amd64""
GCCGO=""gccgo""
CC=""gcc""
CXX=""g++""
CGO_ENABLED=""1""
GOMOD=""""
CGO_CFLAGS=""-g -O2""
CGO_CPPFLAGS=""""
CGO_CXXFLAGS=""-g -O2""
CGO_FFLAGS=""-g -O2""
CGO_LDFLAGS=""-g -O2""
PKG_CONFIG=""pkg-config""
GOGCCFLAGS=""-fPIC -m64 -pthread -fmessage-length=0 -fdebug-prefix-map=/tmp/go-build290707876=/tmp/go-build -gno-record-gcc-switches""

golang dep
[[constraint]]
  name = ""github.com/tensorflow/tensorflow""
  version = ""1.10.1""

"
22293,'_TensorLike',"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22292,Docker with python 3.6,"Please update your [nvidia docker file](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dockerfiles/dockerfiles/nvidia.Dockerfile) to support python 3.6.


### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: tried to take relevant parts from docker file, but I get timeouts.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 (yes has weird python 3.5.3 dependency, but doesn't mean you can't make python3.6 docker image)
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/a
- **TensorFlow installed from (source or binary)**: trying to use tensorflow-gpu>=1.10
- **TensorFlow version (use command below)**: 1.10+
- **Python version**: 3.6+ 
- **Bazel version (if compiling from source)**: N/a
- **GCC/Compiler version (if compiling from source)**: N/a
- **CUDA/cuDNN version**: nvidia/cuda:9.0-base-ubuntu16.04
- **GPU model and memory**: Titan
- **Exact command to reproduce**:

```
FROM nvidia/cuda:9.0-base-ubuntu16.04
FROM python:3.6-slim
RUN apt-get update && apt-get install \
        cuda-command-line-tools-9-0 \
        libcudnn7=7.2.1.38-1+cuda9.0 \
        libnccl2=2.2.13-1+cuda9.0
...
RUN pip install tensorflow-gpu==1.10.0
```



### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Python 3.6 introduces some non-backwards compatible features - most notably f-strings. Any code with an f-string anywhere will not work with any existing official nvidia or tensorflow docker image

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22291,"vgg16  transfer learning will be error ""TypeError: provided list of inputs contains objects other than 'EagerTensor'""","
### System information
Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 1.11.0 (use tf-night-gpu)
Python version:2.7
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 9.0 /7.1
GPU model and memory:
Exact command to reproduce:

### Describe the problem
vgg16  used to `transfer learning ` and data `cocodataset 2014`  and it will be error `TypeError: provided list of inputs contains objects other than 'EagerTensor'. Item 0 is Tensor`  but use` InceptionV3` it is can work. 

### Source code / logs
this is my code.  the cocodataset need to download 3 hours,  
if vgg16  change to InceptionV3, it is can work
```
import tensorflow as tf
tf.enable_eager_execution()

# We'll generate plots of attention in order to see which parts of an image
# our model focuses on during captioning
import matplotlib.pyplot as plt
#import inception_v4 
# Scikit-learn includes many helpful utilities
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
import time
import re
import numpy as np
import os
import time
import json
from glob import glob
from PIL import Image
import pickle

annotation_zip = tf.keras.utils.get_file('captions.zip', 
                                          cache_subdir=os.path.abspath('.'),
                                          origin = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip',
                                          extract = True)
annotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'

name_of_zip = 'train2014.zip'
if not os.path.exists(os.path.abspath('.') + '/' + name_of_zip):
    image_zip = tf.keras.utils.get_file(name_of_zip, 
                                      cache_subdir=os.path.abspath('.'),
                                      origin = 'http://images.cocodataset.org/zips/train2014.zip',
                                      extract = True)
    PATH = os.path.dirname(image_zip)+'/train2014/'
else:
    PATH = os.path.abspath('.')+'/train2014/'


# read the json file# read  
with open(annotation_file, 'r') as f:
    annotations = json.load(f)

# storing the captions and the image name in vectors
all_captions = []
all_img_name_vector = []

for annot in annotations['annotations']:
    caption = '<start> ' + annot['caption'] + ' <end>'
    image_id = annot['image_id']
    full_coco_image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (image_id)
    
    all_img_name_vector.append(full_coco_image_path)
    all_captions.append(caption)

train_captions, img_name_vector = shuffle(all_captions,
                                          all_img_name_vector,
                                          random_state=1)

# selecting the first 30000 captions from the shuffled set
num_examples = 30000

def load_image(image_path):
    img = tf.read_file(image_path)
    img = tf.image.decode_jpeg(img, channels=3)
    img = tf.image.resize_images(img, (224, 224))
    img = tf.keras.applications.vgg16.preprocess_input(x = img)
    return img, image_path

image_model = tf.keras.applications.vgg16.VGG16(include_top=False, weights='imagenet')
new_input = image_model.input
hidden_layer = image_model.layers[-1].output
image_features_extract_model = tf.keras.Model(new_input, hidden_layer)

startTime=time.time()
# getting the unique images
encode_train = sorted(set(img_name_vector))

# feel free to change the batch_size according to your system configuration
image_dataset = tf.data.Dataset.from_tensor_slices(
                                encode_train).map(load_image).batch(16)

for img, path in image_dataset:
    batch_features_0 = image_features_extract_model(img)
  
    batch_features = tf.reshape(batch_features_0, (batch_features_0.shape[0], -1, batch_features_0.shape[3]))
    Nan = np.any(np.isnan(batch_features))

    for bf, p in zip(batch_features, path):

        path_of_feature = p.numpy().decode(""utf-8"")
        np.save(path_of_feature, bf.numpy())

### test image
image = './COCO_val2014_000000000042.jpg'
print(load_image(image))
```
the error output
```
_FallbackException                        Traceback (most recent call last)
/home/ailab/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py in reshape(tensor, shape, name)
   6211         _ctx._context_handle, _ctx._eager_context.device_name, ""Reshape"",
-> 6212         name, _ctx._post_execution_callbacks, tensor, shape)
   6213       return _result

_FallbackException: This function does not handle the case of the path where all inputs are not already EagerTensors.

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
<ipython-input-7-90810626227a> in <module>()
      1 image = './COCO_val2014_000000000042.jpg'
----> 2 print(type(load_image(image)))
      3 print(len(load_image(image)))
      4 print(load_image(image))

<ipython-input-4-34a6a48d497c> in load_image(image_path)
      8     #img = tf.keras.applications.inception_v3.preprocess_input(img)
      9     #img = inception_v4.preprocess_input(img)
---> 10     img = tf.keras.applications.vgg16.preprocess_input(x = img)
     11     return img, image_path

/home/ailab/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/applications/imagenet_utils.py in preprocess_input(x, data_format, mode)
    197     return _preprocess_numpy_input(x, data_format=data_format, mode=mode)
    198   else:
--> 199     return _preprocess_symbolic_input(x, data_format=data_format, mode=mode)
    200 
    201 

/home/ailab/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/applications/imagenet_utils.py in _preprocess_symbolic_input(x, data_format, mode)
    160     x = K.bias_add(x, math_ops.cast(_IMAGENET_MEAN, K.dtype(x)), data_format)
    161   else:
--> 162     x = K.bias_add(x, _IMAGENET_MEAN, data_format)
    163   if std is not None:
    164     x /= std

/home/ailab/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py in bias_add(x, bias, data_format)
   4480     elif data_format == 'channels_last':
   4481       if len(bias_shape) == 1:
-> 4482         x = x + reshape(bias, (1, 1, bias_shape[0]))
   4483       else:
   4484         x = x + reshape(bias, (1,) + bias_shape)

/home/ailab/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py in reshape(x, shape)
   2214       A tensor.
   2215   """"""
-> 2216   return array_ops.reshape(x, shape)
   2217 
   2218 

/home/ailab/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py in reshape(tensor, shape, name)
   6214     except _core._FallbackException:
   6215       return reshape_eager_fallback(
-> 6216           tensor, shape, name=name, ctx=_ctx)
   6217     except _core._NotOkStatusException as e:
   6218       if name is not None:

/home/ailab/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py in reshape_eager_fallback(tensor, shape, name, ctx)
   6233   _attrs = (""T"", _attr_T, ""Tshape"", _attr_Tshape)
   6234   _result = _execute.execute(b""Reshape"", 1, inputs=_inputs_flat, attrs=_attrs,
-> 6235                              ctx=_ctx, name=name)
   6236   _execute.record_gradient(
   6237       ""Reshape"", _inputs_flat, _attrs, _result, name)

/home/ailab/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     58     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,
     59                                                op_name, inputs, attrs,
---> 60                                                num_outputs)
     61   except core._NotOkStatusException as e:
     62     if name is not None:

TypeError: provided list of inputs contains objects other than 'EagerTensor'. Item 0 is Tensor
```
"
22290,can tf.FFT deal with images,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22287,ConvertGraphDefToGraph in graph_construction.cc has a bug,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu1~16.04.10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: 1.10 from source, 1.8 from source
- **TensorFlow version (use command below)**:
- **Python version**: Python 2.7.15 |Anaconda, Inc.| (default, May  1 2018, 23:32:55)
- **Bazel version (if compiling from source)**: Build label: 0.15.2
- **GCC/Compiler version (if compiling from source)**: gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.10)
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

```
/* Adding a Test to tensorflow/core/graph/graph_constructor_test.cc
Add includes at top */
#include ""tensorflow/cc/saved_model/loader.h""
#include ""tensorflow/cc/tools/freeze_saved_model.h""
/*This test should download the faster_rcnn_resnet101_kitti, and fail when converting a frozen_graph_def to a graph*/
TEST_F(GraphConstructorTest, FillOp_faster_rcnn) {
 FILE *file = fopen(""/tmp/resnet_example/1/faster_rcnn_resnet101_kitti_2018_01_28/frozen_inference_graph.pb"", ""r"");
 if (!file){
    printf("" Downloading faster_rcnn_resnet101_kitti_2018_01_28.tar.gz ....."");
    system(""wget http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet101_kitti_2018_01_28.tar.gz"");
    system(""mkdir -p /tmp/resnet_example/1/"");
    system(""tar -xf faster_rcnn_resnet101_kitti_2018_01_28.tar.gz -C /tmp/resnet_example/1/"");
    system(""rm faster_rcnn_resnet101_kitti_2018_01_28.tar.gz"");
    }
  const string export_dir = ""/tmp/resnet_example/1/faster_rcnn_resnet101_kitti_2018_01_28/saved_model"";
  const std::unordered_set<std::string> tags = {""serve""};
  SessionOptions opts;
  RunOptions r_opts;
  // Loading Saved Model
  SavedModelBundle bundle;
  std::unordered_set<std::string> inputs, outputs;
  TF_CHECK_OK(LoadSavedModel(opts, r_opts, export_dir, tags, &bundle));
  GraphDef gd2 = (bundle).meta_graph_def.graph_def();
  Graph *g = new Graph(tensorflow::OpRegistry::Global());
  TF_CHECK_OK(ConvertGraphDefToGraph(tensorflow::GraphConstructorOptions(), gd2, g));
}
```

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

There is a bug in ConvertGraphDefToGraph, I can currently run this model through the Python API but I am having problems getting the GraphDef, and then converting it back to a Graph in C++.
This seems to stem from the fact that its an old graphdef being loaded into a new version of tensorflow. The index type attribute

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

2018-09-15 00:25:13.325147: I tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /tmp/resnet_example/1/faster_rcnn_resnet101_kitti_2018_01_28/saved_model
2018-09-15 00:25:13.440161: I tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }
2018-09-15 00:25:13.831448: I tensorflow/cc/saved_model/loader.cc:162] Restoring SavedModel bundle.
2018-09-15 00:25:13.831492: I tensorflow/cc/saved_model/loader.cc:172] The specified SavedModel has no variables; no checkpoints were restored. File does not exist: /tmp/resnet_example/1/faster_rcnn_resnet101_kitti_2018_01_28/saved_model/variables/variables.index
2018-09-15 00:25:13.831499: I tensorflow/cc/saved_model/loader.cc:138] Running MainOp with key legacy_init_op on SavedModel bundle.
2018-09-15 00:25:13.831525: I tensorflow/cc/saved_model/loader.cc:259] SavedModel load for tags { serve }; Status: success. Took 506360 microseconds.
2018-09-15 00:25:14.044682: F tensorflow/core/graph/graph_constructor_test.cc:3248] Non-OK-status: ConvertGraphDefToGraph(tensorflow::GraphConstructorOptions(), gd2, g) status: Not found: No attr named 'index_type' in NodeDef:
   [[{{node GridAnchorGenerator/Meshgrid/ExpandedShape/ones}} = Fill[T=DT_INT32, _output_shapes=[[?]]](GridAnchorGenerator/Meshgrid/ExpandedShape/Reshape, GridAnchorGenerator/Meshgrid/ExpandedShape/ones/Const)]]
   [[{{node GridAnchorGenerator/Meshgrid/ExpandedShape/ones}} = Fill[T=DT_INT32, _output_shapes=[[?]]](GridAnchorGenerator/Meshgrid/ExpandedShape/Reshape, GridAnchorGenerator/Meshgrid/ExpandedShape/ones/Const)]]
*** Received signal 6 ***
*** BEGIN MANGLED STACK TRACE ***
/home/ubuntu/.cache/bazel/_bazel_ubuntu/0e110d1a1e25aaa081a6bc08469ac91f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/../../_solib_k8/_U_S_Stensorflow_Score_Cgraph_Ugraph_Uconstructor_Utest___Utensorflow/libtensorflow_framework.so(+0x6cb53d)[0x7fd9d02fc53d]
/lib/x86_64-linux-gnu/libpthread.so.0(+0x11390)[0x7fd9cee9d390]
/lib/x86_64-linux-gnu/libc.so.6(gsignal+0x38)[0x7fd9ce050428]
/lib/x86_64-linux-gnu/libc.so.6(abort+0x16a)[0x7fd9ce05202a]
/home/ubuntu/.cache/bazel/_bazel_ubuntu/0e110d1a1e25aaa081a6bc08469ac91f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/../../_solib_k8/_U_S_Stensorflow_Score_Cgraph_Ugraph_Uconstructor_Utest___Utensorflow/libtensorflow_framework.so(+0x6d2837)[0x7fd9d0303837]
/home/ubuntu/.cache/bazel/_bazel_ubuntu/0e110d1a1e25aaa081a6bc08469ac91f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/graph_graph_constructor_test.runfiles/org_tensorflow/tensorflow/core/graph_graph_constructor_test[0x54e091]
/home/ubuntu/.cache/bazel/_bazel_ubuntu/0e110d1a1e25aaa081a6bc08469ac91f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/../../_solib_k8/libexternal_Scom_Ugoogle_Ugoogletest_Slibgtest.so(_ZN7testing8internal35HandleExceptionsInMethodIfSupportedINS_4TestEvEET0_PT_MS4_FS3_vEPKc+0x47)[0x7fd9d0ae95d7]
/home/ubuntu/.cache/bazel/_bazel_ubuntu/0e110d1a1e25aaa081a6bc08469ac91f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/../../_solib_k8/libexternal_Scom_Ugoogle_Ugoogletest_Slibgtest.so(_ZN7testing4Test3RunEv+0xd2)[0x7fd9d0ae9822]
/home/ubuntu/.cache/bazel/_bazel_ubuntu/0e110d1a1e25aaa081a6bc08469ac91f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/../../_solib_k8/libexternal_Scom_Ugoogle_Ugoogletest_Slibgtest.so(_ZN7testing8TestInfo3RunEv+0x118)[0x7fd9d0ae9b28]
/home/ubuntu/.cache/bazel/_bazel_ubuntu/0e110d1a1e25aaa081a6bc08469ac91f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/../../_solib_k8/libexternal_Scom_Ugoogle_Ugoogletest_Slibgtest.so(_ZN7testing8TestCase3RunEv+0xb5)[0x7fd9d0ae9dc5]
/home/ubuntu/.cache/bazel/_bazel_ubuntu/0e110d1a1e25aaa081a6bc08469ac91f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/../../_solib_k8/libexternal_Scom_Ugoogle_Ugoogletest_Slibgtest.so(_ZN7testing8internal12UnitTestImpl11RunAllTestsEv+0x218)[0x7fd9d0aea068]
/home/ubuntu/.cache/bazel/_bazel_ubuntu/0e110d1a1e25aaa081a6bc08469ac91f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/../../_solib_k8/libexternal_Scom_Ugoogle_Ugoogletest_Slibgtest.so(_ZN7testing8internal35HandleExceptionsInMethodIfSupportedINS0_12UnitTestImplEbEET0_PT_MS4_FS3_vEPKc+0x47)[0x7fd9d0aea387]
/home/ubuntu/.cache/bazel/_bazel_ubuntu/0e110d1a1e25aaa081a6bc08469ac91f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/../../_solib_k8/libexternal_Scom_Ugoogle_Ugoogletest_Slibgtest.so(_ZN7testing8UnitTest3RunEv+0x92)[0x7fd9d0aea592]
/home/ubuntu/.cache/bazel/_bazel_ubuntu/0e110d1a1e25aaa081a6bc08469ac91f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/../../_solib_k8/libtensorflow_Score_Slibtest_Umain.so(main+0x9b)[0x7fd9d0b5db6b]
/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf0)[0x7fd9ce03b830]
/home/ubuntu/.cache/bazel/_bazel_ubuntu/0e110d1a1e25aaa081a6bc08469ac91f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/graph_graph_constructor_test.runfiles/org_tensorflow/tensorflow/core/graph_graph_constructor_test[0x515239]
*** END MANGLED STACK TRACE ***

*** Begin stack trace ***
  tensorflow::CurrentStackTrace[abi:cxx11]()


  gsignal
  abort


  void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*)
  testing::Test::Run()
  testing::TestInfo::Run()
  testing::TestCase::Run()
  testing::internal::UnitTestImpl::RunAllTests()
  bool testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*)
  testing::UnitTest::Run()
  main
  __libc_start_main

*** End stack trace ***
"
22285,"Executor failed to create kernel. Not found: No registered 'DataFormatDimMap' OpKernel for GPU devices compatible with node ReverseV2_1-1-DimMapNHWCToNCHW-LayoutOptimizer = DataFormatDimMap[T=DT_INT32, _kernel=""host"", dst_format=""NCHW"", src_format=""NHWC"", _device=""/job:localhost/replica:0/task:0/device:GPU:0""](concat)","Note: I'm noob to tensorflow. I can provide more info if needed.

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No. I trained https://github.com/lightvector/GoNN on my own data (`data.h5`) without code modification
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: gentoo (kernel 4.17.14-gentoo)  uptodate
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: no mobile device
- **TensorFlow installed from (source or binary)**: source (from gentoo repository)
- **TensorFlow version (use command below)**: 'unknown' 1.10.0
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: 0.16.0
- **GCC/Compiler version (if compiling from source)**: 7.3.0-r3 p1.4
- **CUDA/cuDNN version**: cuda 9.2.88 / cudnn 7.1.4
- **GPU model and memory**: GeForce GTX 1070 Ti 7.93GiB
- **Exact command to reproduce**: `./train.py -traindir models -fast-factor 50 -gamesh5 data.h5`

**Extra:** 
- ebuild from tensorflow I used: https://gitweb.gentoo.org/repo/gentoo.git/tree/sci-libs/tensorflow/tensorflow-1.10.0.ebuild
- use flag : 
`sci-libs/tensorflow cuda -jemalloc -system-libs -mpi PYTHON_TARGETS: -* python3_6`

### Describe the problem

When I run tensorflow on GPU, it crashed.

When I run on CPU, it works:

```
CUDA_VISIBLE_DEVICES="""" ./train.py  -traindir models -fast-factor 50 -gamesh5 data.h5
```

### Source code / logs

Code:

https://github.com/lightvector/GoNN/tree/2d74b1300fef6700e75af7f2344d852798b713fb

Logs:

```
 ~/c/GoNN: ./train.py -traindir models -fast-factor 50 -gamesh5 data.h5                                                                                                                                      1058ms  ven. 14 sept. 2018 20:43:51 CEST
Building model
WARNING:tensorflow:From /home/tychota/code/GoNN/model.py:912: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See @{tf.nn.softmax_cross_entropy_with_logits_v2}.

Adjusting gradient for rconv1/w1:0 by Tensor(""PadV2:0"", shape=(3, 3, 1, 1), dtype=float32)
Adjusting gradient for rconv1/w2:0 by Tensor(""PadV2_1:0"", shape=(3, 3, 1, 1), dtype=float32)
Adjusting gradient for rconv2/w1a:0 by Tensor(""PadV2_2:0"", shape=(3, 3, 1, 1), dtype=float32)
Adjusting gradient for rconv2/w1b:0 by Tensor(""PadV2_3:0"", shape=(3, 3, 1, 1), dtype=float32)
Adjusting gradient for rconv2/w2:0 by Tensor(""PadV2_4:0"", shape=(3, 3, 1, 1), dtype=float32)
Adjusting gradient for rconv3/w1a:0 by Tensor(""PadV2_5:0"", shape=(3, 3, 1, 1), dtype=float32)
Adjusting gradient for rconv3/w1b:0 by Tensor(""PadV2_6:0"", shape=(3, 3, 1, 1), dtype=float32)
Adjusting gradient for rconv3/w2:0 by Tensor(""PadV2_7:0"", shape=(3, 3, 1, 1), dtype=float32)
Adjusting gradient for rconv4/w1a:0 by Tensor(""PadV2_8:0"", shape=(3, 3, 1, 1), dtype=float32)
Adjusting gradient for rconv4/w1b:0 by Tensor(""PadV2_9:0"", shape=(3, 3, 1, 1), dtype=float32)
Adjusting gradient for rconv4/w2:0 by Tensor(""PadV2_10:0"", shape=(3, 3, 1, 1), dtype=float32)
Adjusting gradient for rconv5/w1a:0 by Tensor(""PadV2_11:0"", shape=(3, 3, 1, 1), dtype=float32)
Adjusting gradient for rconv5/w1b:0 by Tensor(""PadV2_12:0"", shape=(3, 3, 1, 1), dtype=float32)
Adjusting gradient for rconv5/w2:0 by Tensor(""PadV2_13:0"", shape=(3, 3, 1, 1), dtype=float32)
Adjusting gradient for rconv7/w1a:0 by Tensor(""PadV2_14:0"", shape=(3, 3, 1, 1), dtype=float32)
Adjusting gradient for rconv7/w1b:0 by Tensor(""PadV2_15:0"", shape=(3, 3, 1, 1), dtype=float32)
Adjusting gradient for rconv7/w2:0 by Tensor(""PadV2_16:0"", shape=(3, 3, 1, 1), dtype=float32)
Adjusting gradient for rconv8/w1a:0 by Tensor(""PadV2_17:0"", shape=(3, 3, 1, 1), dtype=float32)
Adjusting gradient for rconv8/w1b:0 by Tensor(""PadV2_18:0"", shape=(3, 3, 1, 1), dtype=float32)
Adjusting gradient for rconv8/w2:0 by Tensor(""PadV2_19:0"", shape=(3, 3, 1, 1), dtype=float32)
Adjusting gradient for rconv9/w1a:0 by Tensor(""PadV2_20:0"", shape=(3, 3, 1, 1), dtype=float32)
Adjusting gradient for rconv9/w1b:0 by Tensor(""PadV2_21:0"", shape=(3, 3, 1, 1), dtype=float32)
Adjusting gradient for rconv9/w2:0 by Tensor(""PadV2_22:0"", shape=(3, 3, 1, 1), dtype=float32)
Adjusting gradient for rconv10/w1a:0 by Tensor(""PadV2_23:0"", shape=(3, 3, 1, 1), dtype=float32)
Adjusting gradient for rconv10/w1b:0 by Tensor(""PadV2_24:0"", shape=(3, 3, 1, 1), dtype=float32)
Adjusting gradient for rconv10/w2:0 by Tensor(""PadV2_25:0"", shape=(3, 3, 1, 1), dtype=float32)
Adjusting gradient for rconv11/w1a:0 by Tensor(""PadV2_26:0"", shape=(3, 3, 1, 1), dtype=float32)
Adjusting gradient for rconv11/w1b:0 by Tensor(""PadV2_27:0"", shape=(3, 3, 1, 1), dtype=float32)
Adjusting gradient for rconv11/w2:0 by Tensor(""PadV2_28:0"", shape=(3, 3, 1, 1), dtype=float32)
Adjusting gradient for rconv13/w1a:0 by Tensor(""PadV2_29:0"", shape=(3, 3, 1, 1), dtype=float32)
Adjusting gradient for rconv13/w1b:0 by Tensor(""PadV2_30:0"", shape=(3, 3, 1, 1), dtype=float32)
Adjusting gradient for rconv13/w2:0 by Tensor(""PadV2_31:0"", shape=(3, 3, 1, 1), dtype=float32)
Adjusting gradient for rconv14/w1a:0 by Tensor(""PadV2_32:0"", shape=(3, 3, 1, 1), dtype=float32)
Adjusting gradient for rconv14/w1b:0 by Tensor(""PadV2_33:0"", shape=(3, 3, 1, 1), dtype=float32)
Adjusting gradient for rconv14/w2:0 by Tensor(""PadV2_34:0"", shape=(3, 3, 1, 1), dtype=float32)
Adjusting gradient for p1/norm/beta:0 by 0.25
Adjusting gradient for p2/w:0 by 0.25
Model variable conv1/wcenter:0, 4256 parameters
Model variable conv1/w:0, 106400 parameters
Model variable rconv1/norm1/beta:0, 224 parameters
Model variable rconv1/w1:0, 451584 parameters
Model variable rconv1/norm2/beta:0, 224 parameters
Model variable rconv1/w2:0, 451584 parameters
Model variable rconv2/norm1/beta:0, 224 parameters
Model variable rconv2/w1a:0, 322560 parameters
Model variable rconv2/w1b:0, 129024 parameters
Model variable rconv2/norm2/beta:0, 224 parameters
Model variable rconv2/w2:0, 451584 parameters
Model variable rconv3/norm1/beta:0, 224 parameters
Model variable rconv3/w1a:0, 322560 parameters
Model variable rconv3/w1b:0, 129024 parameters
Model variable rconv3/norm2/beta:0, 224 parameters
Model variable rconv3/w2:0, 451584 parameters
Model variable rconv4/norm1/beta:0, 224 parameters
Model variable rconv4/w1a:0, 322560 parameters
Model variable rconv4/w1b:0, 129024 parameters
Model variable rconv4/norm2/beta:0, 224 parameters
Model variable rconv4/w2:0, 451584 parameters
Model variable rconv5/norm1/beta:0, 224 parameters
Model variable rconv5/w1a:0, 322560 parameters
Model variable rconv5/w1b:0, 129024 parameters
Model variable rconv5/norm2/beta:0, 224 parameters
Model variable rconv5/w2:0, 451584 parameters
Model variable rconv7/norm1/beta:0, 224 parameters
Model variable rconv7/w1a:0, 322560 parameters
Model variable rconv7/w1b:0, 129024 parameters
Model variable rconv7/norm1b/beta:0, 64 parameters
Model variable rconv7/w1r:0, 20480 parameters
Model variable rconv7/norm2/beta:0, 160 parameters
Model variable rconv7/w2:0, 322560 parameters
Model variable rconv8/norm1/beta:0, 224 parameters
Model variable rconv8/w1a:0, 322560 parameters
Model variable rconv8/w1b:0, 129024 parameters
Model variable rconv8/norm2/beta:0, 224 parameters
Model variable rconv8/w2:0, 451584 parameters
Model variable rconv9/norm1/beta:0, 224 parameters
Model variable rconv9/w1a:0, 322560 parameters
Model variable rconv9/w1b:0, 129024 parameters
Model variable rconv9/norm2/beta:0, 224 parameters
Model variable rconv9/w2:0, 451584 parameters
Model variable rconv10/norm1/beta:0, 224 parameters
Model variable rconv10/w1a:0, 322560 parameters
Model variable rconv10/w1b:0, 129024 parameters
Model variable rconv10/norm2/beta:0, 224 parameters
Model variable rconv10/w2:0, 451584 parameters
Model variable rconv11/norm1/beta:0, 224 parameters
Model variable rconv11/w1a:0, 322560 parameters
Model variable rconv11/w1b:0, 129024 parameters
Model variable rconv11/norm1b/beta:0, 64 parameters
Model variable rconv11/w1r:0, 20480 parameters
Model variable rconv11/norm2/beta:0, 160 parameters
Model variable rconv11/w2:0, 322560 parameters
Model variable rconv13/norm1/beta:0, 224 parameters
Model variable rconv13/w1a:0, 322560 parameters
Model variable rconv13/w1b:0, 129024 parameters
Model variable rconv13/norm2/beta:0, 224 parameters
Model variable rconv13/w2:0, 451584 parameters
Model variable rconv14/norm1/beta:0, 224 parameters
Model variable rconv14/w1a:0, 322560 parameters
Model variable rconv14/w1b:0, 129024 parameters
Model variable rconv14/norm2/beta:0, 224 parameters
Model variable rconv14/w2:0, 451584 parameters
Model variable trunk/norm/beta:0, 224 parameters
Model variable p1/intermediate_conv/w:0, 96768 parameters
Model variable g1/w:0, 64512 parameters
Model variable g1/norm/beta:0, 32 parameters
Model variable matmulg2w:0, 3072 parameters
Model variable p1/norm/beta:0, 48 parameters
Model variable p2/w:0, 48 parameters
Built model, 10901664 total parameters
Additional update op on train step: rconv1/norm1/AssignMovingAvg
Additional update op on train step: rconv1/norm1/AssignMovingAvg_1
Additional update op on train step: rconv1/norm2/AssignMovingAvg
Additional update op on train step: rconv1/norm2/AssignMovingAvg_1
Additional update op on train step: rconv2/norm1/AssignMovingAvg
Additional update op on train step: rconv2/norm1/AssignMovingAvg_1
Additional update op on train step: rconv2/norm2/AssignMovingAvg
Additional update op on train step: rconv2/norm2/AssignMovingAvg_1
Additional update op on train step: rconv3/norm1/AssignMovingAvg
Additional update op on train step: rconv3/norm1/AssignMovingAvg_1
Additional update op on train step: rconv3/norm2/AssignMovingAvg
Additional update op on train step: rconv3/norm2/AssignMovingAvg_1
Additional update op on train step: rconv4/norm1/AssignMovingAvg
Additional update op on train step: rconv4/norm1/AssignMovingAvg_1
Additional update op on train step: rconv4/norm2/AssignMovingAvg
Additional update op on train step: rconv4/norm2/AssignMovingAvg_1
Additional update op on train step: rconv5/norm1/AssignMovingAvg
Additional update op on train step: rconv5/norm1/AssignMovingAvg_1
Additional update op on train step: rconv5/norm2/AssignMovingAvg
Additional update op on train step: rconv5/norm2/AssignMovingAvg_1
Additional update op on train step: rconv7/norm1/AssignMovingAvg
Additional update op on train step: rconv7/norm1/AssignMovingAvg_1
Additional update op on train step: rconv7/norm1b/AssignMovingAvg
Additional update op on train step: rconv7/norm1b/AssignMovingAvg_1
Additional update op on train step: rconv7/norm2/AssignMovingAvg
Additional update op on train step: rconv7/norm2/AssignMovingAvg_1
Additional update op on train step: rconv8/norm1/AssignMovingAvg
Additional update op on train step: rconv8/norm1/AssignMovingAvg_1
Additional update op on train step: rconv8/norm2/AssignMovingAvg
Additional update op on train step: rconv8/norm2/AssignMovingAvg_1
Additional update op on train step: rconv9/norm1/AssignMovingAvg
Additional update op on train step: rconv9/norm1/AssignMovingAvg_1
Additional update op on train step: rconv9/norm2/AssignMovingAvg
Additional update op on train step: rconv9/norm2/AssignMovingAvg_1
Additional update op on train step: rconv10/norm1/AssignMovingAvg
Additional update op on train step: rconv10/norm1/AssignMovingAvg_1
Additional update op on train step: rconv10/norm2/AssignMovingAvg
Additional update op on train step: rconv10/norm2/AssignMovingAvg_1
Additional update op on train step: rconv11/norm1/AssignMovingAvg
Additional update op on train step: rconv11/norm1/AssignMovingAvg_1
Additional update op on train step: rconv11/norm1b/AssignMovingAvg
Additional update op on train step: rconv11/norm1b/AssignMovingAvg_1
Additional update op on train step: rconv11/norm2/AssignMovingAvg
Additional update op on train step: rconv11/norm2/AssignMovingAvg_1
Additional update op on train step: rconv13/norm1/AssignMovingAvg
Additional update op on train step: rconv13/norm1/AssignMovingAvg_1
Additional update op on train step: rconv13/norm2/AssignMovingAvg
Additional update op on train step: rconv13/norm2/AssignMovingAvg_1
Additional update op on train step: rconv14/norm1/AssignMovingAvg
Additional update op on train step: rconv14/norm1/AssignMovingAvg_1
Additional update op on train step: rconv14/norm2/AssignMovingAvg
Additional update op on train step: rconv14/norm2/AssignMovingAvg_1
Additional update op on train step: trunk/norm/AssignMovingAvg
Additional update op on train step: trunk/norm/AssignMovingAvg_1
Additional update op on train step: g1/norm/AssignMovingAvg
Additional update op on train step: g1/norm/AssignMovingAvg_1
Additional update op on train step: p1/norm/AssignMovingAvg
Additional update op on train step: p1/norm/AssignMovingAvg_1
Opening H5 file: data.h5
Adjusting H5 cache settings to: [0, 521, 134217728, 0.75]
Training
2018-09-14 20:44:04.209964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-09-14 20:44:04.210288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: 
name: GeForce GTX 1070 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683
pciBusID: 0000:01:00.0
totalMemory: 7.93GiB freeMemory: 7.34GiB
2018-09-14 20:44:04.210299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2018-09-14 20:44:04.581780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-09-14 20:44:04.581800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 
2018-09-14 20:44:04.581804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N 
2018-09-14 20:44:04.581914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7077 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
Began session
Training on 13234987 rows, validating on 688542/688542 rows
Epoch size = 20000
h5_chunk_size = 6000
Batch size = 200
L2 coeff value = 3e-05
use_ranks = False
predict_pass = False
2018-09-14 20:44:05.383592: E tensorflow/core/common_runtime/executor.cc:697] Executor failed to create kernel. Not found: No registered 'DataFormatDimMap' OpKernel for GPU devices compatible with node ReverseV2_1-1-DimMapNHWCToNCHW-LayoutOptimizer = DataFormatDimMap[T=DT_INT32, _kernel=""host"", dst_format=""NCHW"", src_format=""NHWC"", _device=""/job:localhost/replica:0/task:0/device:GPU:0""](concat)
	.  Registered:  device='CPU'; T in [DT_INT64]
  device='CPU'; T in [DT_INT32]
  device='GPU'; T in [DT_INT64]
  device='GPU'; T in [DT_INT32]

	 [[Node: ReverseV2_1-1-DimMapNHWCToNCHW-LayoutOptimizer = DataFormatDimMap[T=DT_INT32, _kernel=""host"", dst_format=""NCHW"", src_format=""NHWC"", _device=""/job:localhost/replica:0/task:0/device:GPU:0""](concat)]]
Traceback (most recent call last):
  File ""/usr/lib64/python3.6/site-packages/tensorflow/python/client/session.py"", line 1278, in _do_call
    return fn(*args)
  File ""/usr/lib64/python3.6/site-packages/tensorflow/python/client/session.py"", line 1263, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/usr/lib64/python3.6/site-packages/tensorflow/python/client/session.py"", line 1350, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.NotFoundError: No registered 'DataFormatDimMap' OpKernel for GPU devices compatible with node ReverseV2_1-1-DimMapNHWCToNCHW-LayoutOptimizer = DataFormatDimMap[T=DT_INT32, _kernel=""host"", dst_format=""NCHW"", src_format=""NHWC"", _device=""/job:localhost/replica:0/task:0/device:GPU:0""](concat)
	.  Registered:  device='CPU'; T in [DT_INT64]
  device='CPU'; T in [DT_INT32]
  device='GPU'; T in [DT_INT64]
  device='GPU'; T in [DT_INT32]

	 [[Node: ReverseV2_1-1-DimMapNHWCToNCHW-LayoutOptimizer = DataFormatDimMap[T=DT_INT32, _kernel=""host"", dst_format=""NCHW"", src_format=""NHWC"", _device=""/job:localhost/replica:0/task:0/device:GPU:0""](concat)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""./train.py"", line 455, in <module>
    vmetrics_evaled = merge_dicts(run_validation_in_batches(vmetrics), np.sum)
  File ""./train.py"", line 325, in run_validation_in_batches
    result = run(fetches, rows, symmetries=[False,False,False], training=False)
  File ""./train.py"", line 309, in run
    model.is_training: training
  File ""/usr/lib64/python3.6/site-packages/tensorflow/python/client/session.py"", line 877, in run
    run_metadata_ptr)
  File ""/usr/lib64/python3.6/site-packages/tensorflow/python/client/session.py"", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/lib64/python3.6/site-packages/tensorflow/python/client/session.py"", line 1272, in _do_run
    run_metadata)
  File ""/usr/lib64/python3.6/site-packages/tensorflow/python/client/session.py"", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: No registered 'DataFormatDimMap' OpKernel for GPU devices compatible with node ReverseV2_1-1-DimMapNHWCToNCHW-LayoutOptimizer = DataFormatDimMap[T=DT_INT32, _kernel=""host"", dst_format=""NCHW"", src_format=""NHWC"", _device=""/job:localhost/replica:0/task:0/device:GPU:0""](concat)
	.  Registered:  device='CPU'; T in [DT_INT64]
  device='CPU'; T in [DT_INT32]
  device='GPU'; T in [DT_INT64]
  device='GPU'; T in [DT_INT32]

	 [[Node: ReverseV2_1-1-DimMapNHWCToNCHW-LayoutOptimizer = DataFormatDimMap[T=DT_INT32, _kernel=""host"", dst_format=""NCHW"", src_format=""NHWC"", _device=""/job:localhost/replica:0/task:0/device:GPU:0""](concat)]]

```
"
22280,"Broken link to ""capabilities and limitations"" in AutoGraph guide","https://www.tensorflow.org/guide/autograph has a link to `Autograph capabilities and limitations`.  The URL is https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/autograph/LIMITATIONS.md, but this is a dead link.  The corrected link is https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/LIMITATIONS.md."
22279,tflite flatbuffer conversion to json by upgrade_schema.py doesn't transfer uint8 bias vals,"the upgrade_schema conversion to json does not export the int32 bias values of the quantized tflite mobilenet v1 example, MobileNet_v1_1.0_224_quant.  For example, the first Conv2D has 32 int32 bias values, but these are not exported to a json output.  The bias values are visible  in the the tflite file using Netron.

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/schema/upgrade_schema.py
https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md
MobileNet_v1_1.0_224_quant

I'm working in Ubuntu 18.04 with the anaconda installation of tensorflow v 1.9.0"
22278,"TFLite Android ArrayIndex outofBound Exception length=40, index=-7","- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Partially modified TFLite Android code

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I converted mobilenet_v2_ssd trained using my data to tflite following the instructions in the blog post:
https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193

I did not quantize the model since I didn't train it using quantized training. To run on mobile I set the 
IS_QUANTIZED variable in Detector.Activity to false. When I run the app, i get the error 
java.lang.ArrayIndexOutOfBoundsException: length=40; index=-8
at java.util.Vector.elementAt(Vector.java:326)
at java.util.Vector.get(Vector.java:442)
at org.tensorflow.demo.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:214)

I thought the issue is due to the length of the output vector. So I changed the NUM_DETECTIONS in TFLiteObjectDetectionAPIModel.java to 40, then  I get another error saying
Cannot copy between a TensorFlowLite tensor with shape [1, 10] and a Java object with shape [1, 40].

Is the length of output vector fixed? Please help me with the issue.Thanks!"
22277,How to compile the tensorflow-lite to get a smaller and faster library?,"### System information
- **Mobile device**: 
    * HUAWEI Mate 9
    * Xiaomi Mi 6

### Describe the problem
During Arm AI Developer Global Summit, the Google engineer said that TensorFlow-Lite can be compiled into an about 400KB library. However, when I compile the latest TensorFlow-Lite, I can only get a  1.2MB dynamic link library. The following script is my compiled command:
```bash
bazel build -c opt --cxxopt='--std=c++11' --cxxopt='-O3' --cxxopt='-march=armv7-a' --cxxopt='-mfpu=neon' --cxxopt='-mfloat-abi=softfp' //tensorflow/contrib/lite/java:tensorflowlite --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a
```
What's wrong with this compiled command?

Besides,  in order to test the performance of tensorflow-lite, I also compiled its benchmarks according to [this official doc](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/tools/benchmark) .The following script is my compiled command (offered by the official doc):
```
bazel build -c opt \
  --config=android_arm \
  --cxxopt='--std=c++11' \
  tensorflow/contrib/lite/tools/benchmark:benchmark_model
```
When running MobileNet_v1_1.0_224, its result is shown as follow:
![image](https://user-images.githubusercontent.com/17102274/45552548-d9ca7f80-b863-11e8-8bd1-db14bf5c354f.png)
I want to know how to get a faster result?
"
22276,How to incorporate custom functions into tf.data pipe-lining process for maximum efficiency,"So tf.image for example has some elementary image processing methods already implemented which i'd assumed are optimized. The question is as I'm iterating through a large dataset of images what/how is the recommended way of implementing a more complex function on every image, in batches of course, (for example a a patch 2-D DCT) for it to go as best as possible with the whole tf.data framework.

Thanks in advance.  

p.s. of course I could use the ""Map"" method but i'm asking beyond that. like if I'm passing a pure numpy written function to pass to ""map"", it wouldn't help as much."
22275,Class_weight with tf.dataset as input to model.fit will throw an error,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No Mobile device
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.10
- **Python version**:2.7
- **Bazel version (if compiling from source)**: ---
- **GCC/Compiler version (if compiling from source)**:---
- **CUDA/cuDNN version**: cuda-8.0
- **GPU model and memory**: (Titan X and GeForce GTX 1080 )
- **Exact command to reproduce**:

### Describe the problem
I am using` tf.keras `in order to be able to feed the train_data using` tf.dataset` API through model.fit directly. It works fine whenever you didn't pass `class_weight`, but if you pass dict of class_weights, it will throw the following error :

`AxisError: axis 1 is out of bounds for array of dimension 1
`

When I debug the error , I found the error happened exactly at line 531 : 
```
> /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training_utils.py(530)standardize_weights()
    528       raise ValueError('`class_weight` not supported for '
    529                        '3+ dimensional targets.')
    530     if y.shape[1] > 1:
-->  531       y_classes = np.argmax(y, axis=1)
    532     elif y.shape[1] == 1:

```
The dimension of y is `TensorShape([Dimension(None), Dimension(8)])
`
So` y.shape[1] > 1` , but the problem y is a **tensor** now not an **numpy array,** that is why it throws the previous error.

So is there any solution to this situation? 

### **EDIT**: Adding sample code to reproduce the error.

```
import os, sys, logging
import numpy as np
import tensorflow as tf
import itertools as itt
logging.basicConfig(level=logging.INFO)


def _int64_feature(value):
  return tf.train.Feature(int64_list= tf.train.Int64List(value= [value]))

def _bytes_feature(value):
  return tf.train.Feature(bytes_list= tf.train.BytesList(value= [value]))

def _float32_feature(value):
  return tf.train.Feature(float_list= tf.train.FloatList(value= [value]))

def tf_records_creating(tfrecord_file):
    logging.info('Creating random tfrecord files for 100 sample')

    labels = np.random.uniform(0, num_classes, total_train).astype(np.int32)
    data = np.random.uniform(0, 255, total_train*224*224*3).reshape(total_train, 224, 224, 3).astype(np.int32)

    writer = tf.python_io.TFRecordWriter(tfrecord_file)

    for idx, (image, label) in enumerate(itt.izip(data, labels)):
        image = image.tostring()
        example = tf.train.Example(features=tf.train.Features(feature={
            'label': _int64_feature(int(label)),
            'image': _bytes_feature(image),
        }))
        writer.write(example.SerializeToString())
    writer.close()
    return

def decode(serialized_example):
  features = tf.parse_single_example(
      serialized_example,
      features={
          'image': tf.FixedLenFeature([], tf.string),
          'label': tf.FixedLenFeature([], tf.int64),
      })

  image = tf.decode_raw(features['image'], tf.float32)
  image.set_shape([224*224*3])

  image=tf.reshape(image, (224,224,3))

  label = tf.cast(features['label'], tf.int32)
  label_categorical = tf.one_hot(label,depth= num_classes, on_value=1,off_value=0,dtype=tf.int32,)
  label_categorical = tf.reshape(label_categorical, [num_classes])
  label_categorical.set_shape([num_classes])

  return image, label_categorical

def data_preparing(tfrecord_file):

    logging.info('Preparing the Training tf.dataset ')
    training_files = [tfrecord_file]
    dataset_train = tf.data.TFRecordDataset(training_files, num_parallel_reads=1)
    dataset_train = dataset_train.apply(tf.contrib.data.shuffle_and_repeat(buffer_size=4 * batch_size))
    dataset_train = dataset_train.map(decode, num_parallel_calls=1)  
    dataset_train = dataset_train.batch(batch_size)
    dataset_train = dataset_train.prefetch(tf.contrib.data.AUTOTUNE)
    return dataset_train

def train_model(tfrecord_file):

    base_model = tf.keras.applications.VGG16(include_top=False, weights='imagenet',
                                                 input_shape=(224, 224, 3), pooling='avg')

    for layer in base_model.layers:
        layer.trainable = False

    logging.info('Building Our Classifier')
    x = base_model.output
    x = tf.keras.layers.Flatten()(x)
    x = tf.keras.layers.Dense(512, activation='relu')(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Dropout(0.5)(x)
    x = tf.keras.layers.Dense(num_classes, activation='sigmoid')(x)

    model = tf.keras.models.Model(inputs=base_model.input, outputs=x)
    model.summary()
    opt = tf.keras.optimizers.Adam(lr=0.001)
    model.compile(loss='categorical_crossentropy',
                  optimizer=opt, metrics=['accuracy'])
    dataset_train = data_preparing(tfrecord_file=tfrecord_file)

    weighted_array_train= np.array([0.01557266,0.00867447,0.04579864,0.08275284,0.18281397,
       0.30659676, 0.04686068, 0.31092999])
    class_weight_dict = dict(enumerate(weighted_array_train))

    if using_class_weight == True:
        model.fit(x=dataset_train, epochs=epochs, verbose=1,class_weight = class_weight_dict,
              steps_per_epoch=int(np.ceil(total_train / batch_size)))
    else:
        model.fit(x=dataset_train, epochs=epochs, verbose=1,
              steps_per_epoch=int(np.ceil(total_train / batch_size)))
    return

if __name__== '__main__':

    total_train = 100.
    num_classes= 8
    batch_size = 10
    epochs = 100

    #TODO (1) :Set the path to tfrecord file that we will create it.
    tfrecord_file = '~/train.tfrecords'
    tf_records_creating(tfrecord_file)                # Implement this only one time

    using_class_weight= False                         # if you set this to True, you will produce the error
    train_model(tfrecord_file)
```
"
22274,CollectiveAllReduceStrategy fails with CPU-only workers,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: tf-nightly
- **TensorFlow version (use command below)**: ('v1.9.0-rc2-4081-g626bc997c2', '1.11.0-dev20180913')
- **Python version**: Python 2.7.15
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: https://gist.github.com/df3df82f7ae8f47b6288fc42eb8c8b17

### Describe the problem
Invoke `tf.estimator.train_and_evaluate` with `CollectiveAllReduceStrategy` fails on CPU-only worker nodes, with the following message:

```
InternalError: ScopedAllocatorMgr not supported on device /job:worker/replica:0/task:0/device:CPU:0
```

### Source code / logs

```python
from tensorflow.contrib.distribute import CollectiveAllReduceStrategy
from tensorflow.contrib.distribute import DistributeConfig

distribution = CollectiveAllReduceStrategy(num_gpus_per_worker=0)

config = tf.estimator.RunConfig(
    experimental_distribute=DistributeConfig(
        train_distribute=distribution,
        remote_cluster={
            'worker': ['localhost:5000', 'localhost:5001'],
        },
    )
)

estimator = tf.estimator.Estimator(model_fn=model_fn, config=config)

tf.estimator.train_and_evaluate(estimator=estimator, train_spec=train_spec, eval_spec=eval_spec)
```
```
INFO:tensorflow:CollectiveAllReduceStrategy with local_devices = ['/device:CPU:0']
INFO:tensorflow:Initializing RunConfig with distribution strategies.
INFO:tensorflow:RunConfig initialized for Distribute Coordinator with STANDALONE_CLIENT mode
WARNING:tensorflow:Using temporary folder as model directory: /var/folders/gn/sjntndrs1fs22kfr302697mr0000gn/T/tmpeIE_x6
INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': {'worker': ['localhost:5000', 'localhost:5001']}, '_model_dir': '/var/folders/gn/sjntndrs1fs22kfr302697mr0000gn/T/tmpeIE_x6', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': DistributeConfig(train_distribute=<tensorflow.contrib.distribute.python.collective_all_reduce_strategy.CollectiveAllReduceStrategy object at 0x123f09810>, eval_distribute=None, remote_cluster={'worker': ['localhost:5000', 'localhost:5001']}), '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': <tensorflow.contrib.distribute.python.collective_all_reduce_strategy.CollectiveAllReduceStrategy object at 0x123f09810>, '_master': '', '_distribute_coordinator_mode': 'standalone_client'}
INFO:tensorflow:Running `train_and_evaluate` with Distribute Coordinator.
INFO:tensorflow:Running Distribute Coordinator with mode = 'standalone_client', cluster_spec = {'worker': ['localhost:5000', 'localhost:5001']}, task_type = None, task_id = None, environment = None, rpc_layer = 'grpc'
WARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.
INFO:tensorflow:Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'worker': ['localhost:5000', 'localhost:5001']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ['/job:worker/task:0']
INFO:tensorflow:Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'worker': ['localhost:5000', 'localhost:5001']}, task_type = 'worker', task_id = 1, num_workers = 2, local_devices = ['/job:worker/task:1']
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Collective All-reduce invoked with batches size = 2, num_workers = 2
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Creating chief session creator with config: device_filters: ""/job:worker/task:0""
allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
    scoped_allocator_optimization: ON
    scoped_allocator_opts {
      enable_op: ""CollectiveReduce""
    }
  }
}
isolate_session_state: true
experimental {
  collective_group_leader: ""/job:worker/replica:0/task:0""
}

INFO:tensorflow:Collective All-reduce invoked with batches size = 2, num_workers = 2
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Creating chief session creator with config: device_filters: ""/job:worker/task:1""
allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
    scoped_allocator_optimization: ON
    scoped_allocator_opts {
      enable_op: ""CollectiveReduce""
    }
  }
}
isolate_session_state: true
experimental {
  collective_group_leader: ""/job:worker/replica:0/task:0""
}

INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Initialize system
INFO:tensorflow:Initialize system
INFO:tensorflow:Saving checkpoints for 0 into /var/folders/gn/sjntndrs1fs22kfr302697mr0000gn/T/tmpeIE_x6/model.ckpt.

Exception in thread Thread-5:
Traceback (most recent call last):
  File ""/usr/local/Cellar/python@2/2.7.15_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/threading.py"", line 801, in __bootstrap_inner
    self.run()
  File ""/usr/local/Cellar/python@2/2.7.15_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/threading.py"", line 754, in run
    self.__target(*self.__args, **self.__kwargs)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/distribute/distribute_coordinator.py"", line 344, in _run_single_worker
    worker_fn(strategy)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/distribute/estimator_training.py"", line 232, in _worker_fn
    hooks=list(train_spec.hooks))
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 355, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1178, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1325, in _train_model_distributed
    saving_listeners)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1408, in _train_with_estimator_spec
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 671, in run
    run_metadata=run_metadata)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1148, in run
    run_metadata=run_metadata)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1239, in run
    raise six.reraise(*original_exc_info)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1224, in run
    return self._sess.run(*args, **kwargs)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1296, in run
    run_metadata=run_metadata)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1076, in run
    return self._sess.run(*args, **kwargs)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 887, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1110, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1286, in _do_run
    run_metadata)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1306, in _do_call
    raise type(e)(node_def, op, message)
InternalError: ScopedAllocatorMgr not supported on device /job:worker/replica:0/task:1/device:CPU:0
	 [[{{node scoped_allocator_1}} = _ScopedAllocator[T=DT_FLOAT, expected_call_count=2, id=1, sa_name=""scoped_allocator_1"", shape=[17], shapes=[[1,1], [1]], _device=""/job:worker/replica:0/task:1/device:CPU:0""]()]]

Exception in thread Thread-4:
Traceback (most recent call last):
  File ""/usr/local/Cellar/python@2/2.7.15_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/threading.py"", line 801, in __bootstrap_inner
    self.run()
  File ""/usr/local/Cellar/python@2/2.7.15_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/threading.py"", line 754, in run
    self.__target(*self.__args, **self.__kwargs)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/distribute/distribute_coordinator.py"", line 344, in _run_single_worker
    worker_fn(strategy)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/distribute/estimator_training.py"", line 232, in _worker_fn
    hooks=list(train_spec.hooks))
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 355, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1178, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1325, in _train_model_distributed
    saving_listeners)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1408, in _train_with_estimator_spec
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 671, in run
    run_metadata=run_metadata)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1148, in run
    run_metadata=run_metadata)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1239, in run
    raise six.reraise(*original_exc_info)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1224, in run
    return self._sess.run(*args, **kwargs)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1296, in run
    run_metadata=run_metadata)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1076, in run
    return self._sess.run(*args, **kwargs)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 887, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1110, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1286, in _do_run
    run_metadata)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1306, in _do_call
    raise type(e)(node_def, op, message)
InternalError: ScopedAllocatorMgr not supported on device /job:worker/replica:0/task:0/device:CPU:0
	 [[{{node scoped_allocator_1}} = _ScopedAllocator[T=DT_FLOAT, expected_call_count=2, id=1, sa_name=""scoped_allocator_1"", shape=[17], shapes=[[1,1], [1]], _device=""/job:worker/replica:0/task:0/device:CPU:0""]()]]
```

PS: thanks @yuefengz for today's introduction of multi-node distribution strategy in TF Roadshow 2018@Beijing "
22273,failed to build tensorflow android,"Using : 
---------
OS: Ubuntu 16.04
Bazel : 0.8.1
tensorflow : 1.5.0
NDK : 12b -> sdk level 14
sdk : 23
build tools: 25.2.0

The full build log of error:
**https://justpaste.it/621im**
tried the follwing solutions but still getting the error:
**[](**
https://github.com/tensorflow/tensorflow/issues/8641)
**[](**
https://github.com/tensorflow/tensorflow/issues/6356)

Please help me out.

Below is the shot form of Error:

<command-line>:0:0: note: this is the location of the previous definition
ERROR: /home/kv/Desktop/Work/AR/TensorFlow/Source/Tensorflow_1.3/tensorflow-master/tensorflow/contrib/android/BUILD:29:1: C++ compilation of rule '//tensorflow/contrib/android:android_tensorflow_inference_jni' failed (Exit 1): arm-linux-androideabi-gcc failed: error executing command 
  (cd /home/kv/.cache/bazel/_bazel_kv/9644bdd546e0ad6ab4fab8f0ed146de9/execroot/org_tensorflow && \
  exec env - \
    PATH=/home/kv/anaconda3/bin:/home/kv/bin:/home/kv/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/home/kv/anaconda3/bin/python \
    PYTHON_LIB_PATH=/home/kv/anaconda3/lib/python3.6/site-packages \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL_SYCL=0 \
  external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-gcc -fstack-protector-strong -fpic -ffunction-sections -funwind-tables -no-canonical-prefixes -fno-canonical-system-headers '-march=armv7-a' '-mfpu=vfpv3-d16' '-mfloat-abi=softfp' -mthumb -Os -g -DNDEBUG -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -MD -MF bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/contrib/android/_objs/android_tensorflow_inference_jni/tensorflow/contrib/android/asset_manager_filesystem.d '-frandom-seed=bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/contrib/android/_objs/android_tensorflow_inference_jni/tensorflow/contrib/android/asset_manager_filesystem.o' -DEIGEN_MPL2_ONLY -iquote . -iquote bazel-out/armeabi-v7a-py3-opt/genfiles -iquote external/protobuf_archive -iquote bazel-out/armeabi-v7a-py3-opt/genfiles/external/protobuf_archive -iquote external/bazel_tools -iquote bazel-out/armeabi-v7a-py3-opt/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/armeabi-v7a-py3-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/armeabi-v7a-py3-opt/genfiles/external/local_config_sycl -iquote external/nsync -iquote bazel-out/armeabi-v7a-py3-opt/genfiles/external/nsync -iquote external/fft2d -iquote bazel-out/armeabi-v7a-py3-opt/genfiles/external/fft2d -iquote external/gemmlowp -iquote bazel-out/armeabi-v7a-py3-opt/genfiles/external/gemmlowp -isystem external/protobuf_archive/src -isystem bazel-out/armeabi-v7a-py3-opt/genfiles/external/protobuf_archive/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/eigen_archive -isystem bazel-out/armeabi-v7a-py3-opt/genfiles/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/armeabi-v7a-py3-opt/genfiles/external/nsync/public -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' '-mfpu=neon' '-std=c++11' -DTF_LEAN_BINARY -Wno-narrowing -fomit-frame-pointer -O2 '--sysroot=external/androidndk/ndk/platforms/android-12/arch-arm' -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/backward -c tensorflow/contrib/android/asset_manager_filesystem.cc -o bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/contrib/android/_objs/android_tensorflow_inference_jni/tensorflow/contrib/android/asset_manager_filesystem.o)
tensorflow/contrib/android/asset_manager_filesystem.cc: In member function 'virtual tensorflow::Status tensorflow::{anonymous}::RandomAccessFileFromAsset::Read(tensorflow::uint64, size_t, tensorflow::StringPiece*, char*) const':
tensorflow/contrib/android/asset_manager_filesystem.cc:97:69: error: 'AAsset_seek64' was not declared in this scope
     off64_t new_offset = AAsset_seek64(asset.get(), offset, SEEK_SET);
                                                                     ^
tensorflow/contrib/android/asset_manager_filesystem.cc:98:52: error: 'AAsset_getLength64' was not declared in this scope
     off64_t length = AAsset_getLength64(asset.get());
                                                    ^
tensorflow/contrib/android/asset_manager_filesystem.cc: In member function 'virtual tensorflow::Status tensorflow::AssetManagerFileSystem::NewReadOnlyMemoryRegionFromFile(const string&, std::unique_ptr<tensorflow::ReadOnlyMemoryRegion>*)':
tensorflow/contrib/android/asset_manager_filesystem.cc:158:68: error: 'AAsset_openFileDescriptor64' was not declared in this scope
   int fd = AAsset_openFileDescriptor64(asset.get(), &start, &length);
                                                                    ^
tensorflow/contrib/android/asset_manager_filesystem.cc:173:44: error: 'AAsset_getLength64' was not declared in this scope
     length = AAsset_getLength64(asset.get());
                                            ^
tensorflow/contrib/android/asset_manager_filesystem.cc: In member function 'virtual tensorflow::Status tensorflow::AssetManagerFileSystem::GetFileSize(const string&, tensorflow::uint64*)':
tensorflow/contrib/android/asset_manager_filesystem.cc:214:38: error: 'AAsset_getLength64' was not declared in this scope
   *s = AAsset_getLength64(asset.get());
                                      ^
Target //tensorflow/contrib/android:libtensorflow_inference.so failed to build
INFO: Elapsed time: 358.381s, Critical Path: 76.20s
FAILED: Build did NOT complete successfully















Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22272,failed to build tensorflow wheel file on jetson TX2 board ,"ERROR: /home/nvidia/Desktop/tensorflow/tensorflow/cc/BUILD:422:1: Linking of rule '//tensorflow/cc:ops/math_ops_gen_cc' failed (Exit 1)
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scc_Cops_Smath_Uops_Ugen_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `png_init_filter_functions_neon'
collect2: error: ld returned 1 exit status
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1648.891s, Critical Path: 115.03s
FAILED: Build did NOT complete successfully


Bazel version: 10.0
CUDA: 9.0
CUDNN:7.0

Request you to kindly help me find a solution to the problem."
22271,make schema_generated.h source file  turn error,"### source  code
```
bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_MIN) &&
           verifier.Verify(min()) &&
           VerifyOffset(verifier, VT_MAX) &&
           verifier.Verify(max()) &&
           VerifyOffset(verifier, VT_SCALE) &&
           verifier.Verify(scale()) &&
           VerifyOffset(verifier, VT_ZERO_POINT) &&
           verifier.Verify(zero_point()) &&
           verifier.EndTable();
  }
```

### error info
```
erroe :   flatbuffers::Verifier::Verify: No overload function takes 1 
parameters.tensorflow\contrib\lite\schema\schema_generated.h	
````



The same error occurs in every structure inside this method. 

"
22270,tf.nn.softmax_cross_entropy_with_logits_v2() not warning incorrect input shapes when placeholders are provided,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.10.0
- **Python version**: 3.5.2
- **GPU model and memory**: GTX1070

### Describe the problem
Following is a simple snippet to reproduce the problem. If I'm understanding this correctly, the shape of _labels_ is clearly wrong here but tensorflow doesn't warn you and when the same problem is present in my actual code, not even a runtime error will show up. Appreciate it if you guys can help me out on this especially if I mistook this. Otherwise this looks like a devastating issue.

### Source code / logs
a = tf.placeholder(tf.int32, (None))
b = tf.constant([[1.0,2,3],[4,5,6]])
c = tf.nn.softmax_cross_entropy_with_logits_v2(labels=a, logits=b)
"
22269,TensorFlow compile failure with error tensorflow/tensorflow/compiler/xla/service/gpu/BUILD:649:1: C++ compilation of rule '//tensorflow/compiler/xla/service/gpu:infeed_manager' failed (Exit 1),"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04.1 (GNU/Linux 4.15.0-34-generic x86_64)
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.10.1
- **Python version**: Anaconda 5.2.0 (Python 3.6.5)
- **Bazel version (if compiling from source)**: 0.16.1
- **GCC/Compiler version (if compiling from source)**:7.3.0
- **CUDA/cuDNN version** : 9.2 / 7.2.1
- **GPU model and memory**: NVIDIA 1080Ti x 4 with driver 396.54 installed from ubuntu ppa.
- **Exact command to reproduce**: bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
Hi. I have a problem while compiling TensorFlow 1.10. I have always compiled pip package for my computer since back in TF versions like 0.12. But from TF 1.10 I started getting compile error related to XLA (I enabled XLA in ./configure before compiling). I did not get any XLA related error while compiling until TF version 1.9 but the error stated occurring from TF 1.10.

The error lines are the following:
TensorFlow fails to compile with error tensorflow/tensorflow/compiler/xla/service/gpu/BUILD:649:1: C++ compilation of rule '//tensorflow/compiler/xla/service/gpu:infeed_manager' failed (Exit 1)"
22268,Tensorflow compile error with win7,"**Have I written custom code** No
**OS Platform and Distribution** WIN7  Visual Studio 2015
**TensorFlow installed from**  source  r1.10
**Bazel version** N/A
**CUDA/cuDNN version** CUDA9.0  /   CUDNN  7.0
**GPU model and memory**   1080Ti / 11G 
**Mobile device**  N/A
**Exact command to reproduce** 
**1, cmake** 
```
cmake .. -A x64 -DCMAKE_BUILD_TYPE=Debug 
-T host=x64 -DSWIG_EXECUTABLE=D:/lib/swigwin-3.0.12/swig.exe 
-DPYTHON_EXECUTABLE=C:/Users/tao/AppData/Local/Programs/Python/python35/python.exe 
-DPYTHON_LIBRARIES=C:/Users/tao/AppData/Local/Programs/Python/python35/libs/python35.lib 
-Dtensorflow_ENABLE_GPU=ON 
-Dtensorflow_ENABLE_GRPC_SUPPORT=OFF 
-Dtensorflow_BUILD_SHARED_LIB=ON
```
**2. build**
`MSBuild /p:Configuration=Debug /p:Platform=x64 ALL_BUILD.vcxproj`

### Describe the problem
When I compiled the Tensorflow with source code, there exist some errors:
```
						
	LNK2019	 ""void __cdecl tensorflow::NewRemoteDevices(class tensorflow::Env *,class tensorflow::WorkerCacheInterface *,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class std::function<void __cdecl(class tensorflow::Status const &,class std::vector<class tensorflow::Device *,class std::allocator<class tensorflow::Device *> > *)>)"" (?NewRemoteDevices@tensorflow@@YAXPEAVEnv@1@PEAVWorkerCacheInterface@1@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$function@$$A6AXAEBVStatus@tensorflow@@PEAV?$vector@PEAVDevice@tensorflow@@V?$allocator@PEAVDevice@tensorflow@@@std@@@std@@@Z@5@@Z) ""class tensorflow::Status __cdecl `anonymous namespace'::GetAllRemoteDevices(class std::vector<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class std::allocator<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > > > const &,class tensorflow::WorkerCacheInterface *,class std::unique_ptr<class tensorflow::DeviceMgr,struct std::default_delete<class tensorflow::DeviceMgr> > *)"" (?GetAllRemoteDevices@?A0x02eb28b0@@YA?AVStatus@tensorflow@@AEBV?$vector@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$allocator@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@@std@@PEAVWorkerCacheInterface@3@PEAV?$unique_ptr@VDeviceMgr@tensorflow@@U?$default_delete@VDeviceMgr@tensorflow@@@std@@@5@@Z) 	tensorflow	G:\tensorflow\tensorflow\contrib\cmake\build\c_api.cc.obj	1	
	LNK2019	 ""public: class tensorflow::Status __cdecl tensorflow::SessionMgr::CreateSession(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class tensorflow::ServerDef const &,bool)"" (?CreateSession@SessionMgr@tensorflow@@QEAA?AVStatus@2@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBVServerDef@2@_N@Z) ""class tensorflow::Status __cdecl `anonymous namespace'::NewRemoteAwareTFE_Context(struct TFE_ContextOptions const *,struct TFE_Context * *)"" (?NewRemoteAwareTFE_Context@?A0x02eb28b0@@YA?AVStatus@tensorflow@@PEBUTFE_ContextOptions@@PEAPEAUTFE_Context@@@Z) 	tensorflow	G:\tensorflow\tensorflow\contrib\cmake\build\c_api.cc.obj	1	
	LNK2019	 ""public: class tensorflow::Status __cdecl tensorflow::SessionMgr::WorkerSessionForSession(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class std::shared_ptr<struct tensorflow::WorkerSession> *)"" (?WorkerSessionForSession@SessionMgr@tensorflow@@QEAA?AVStatus@2@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAV?$shared_ptr@UWorkerSession@tensorflow@@@5@@Z) ""class tensorflow::Status __cdecl `anonymous namespace'::NewRemoteAwareTFE_Context(struct TFE_ContextOptions const *,struct TFE_Context * *)"" (?NewRemoteAwareTFE_Context@?A0x02eb28b0@@YA?AVStatus@tensorflow@@PEBUTFE_ContextOptions@@PEAPEAUTFE_Context@@@Z) 	tensorflow	G:\tensorflow\tensorflow\contrib\cmake\build\c_api.cc.obj	1	
	LNK2019	 ""class tensorflow::eager::EagerClientCache * __cdecl tensorflow::eager::NewGrpcEagerClientCache(class std::shared_ptr<class tensorflow::GrpcChannelCache>)"" (?NewGrpcEagerClientCache@eager@tensorflow@@YAPEAVEagerClientCache@12@V?$shared_ptr@VGrpcChannelCache@tensorflow@@@std@@@Z) ""class tensorflow::Status __cdecl `anonymous namespace'::NewRemoteAwareTFE_Context(struct TFE_ContextOptions const *,struct TFE_Context * *)"" (?NewRemoteAwareTFE_Context@?A0x02eb28b0@@YA?AVStatus@tensorflow@@PEBUTFE_ContextOptions@@PEAPEAUTFE_Context@@@Z) 	tensorflow	G:\tensorflow\tensorflow\contrib\cmake\build\c_api.cc.obj	1	
	LNK1120	4 	tensorflow	G:\tensorflow\tensorflow\contrib\cmake\build\Debug\tensorflow.dll	1	
```
can anyone help me , thx"
22267,"Docs/tutorial link to 'Eager execution', item 1 of 'Research and experimentation' 404 err.","Documentation; tutorials.
Link to Eager execution tutorial (item 1 of 'Research and experimentation') give a 404 error.

The link is currently pointing to:
[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/notebooks/eager_intro.ipynb](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/notebooks/eager_intro.ipynb)

I believe that it should be pointing to:
[https://github.com/tensorflow/docs/blob/master/site/en/tutorials/eager/eager_basics.ipynb](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/eager/eager_basics.ipynb)"
22266,How to use the function FusedBatchNorm() in C++?,"The question is that : https://stackoverflow.com/questions/52308509/tensorflow-c-how-to-convert-tensorflowoutput-to-tensor

Python has the function `covert_to_tensor()`but C++ does not have this function. The `FusedBatchNorm()` need a `Tensor ` input, but my input type is `tensorflow::Input`. So how to solve this problemHow to convert `tensorflow::Input` to `Tensor` "
22260,y,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22259,`tf.nn.softmax` gives an error at execution time for certain empty inputs,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra (10.13.6)
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.10.0-12-g4dcfddc5d1 1.10.1
- **Python version**: 3.6.4 (Anaconda)
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: n/a

### Describe the problem
When the softmax axis has length 0 and rank of the tensor is not 2, `tf.nn.softmax` gives an error at execution time:
```
  File ""/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 1612, in _flatten_outer_dims
    output = array_ops.reshape(logits, array_ops.concat([[-1], last_dim_size], 0))
  File ""/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 6222, in reshape
    _six.raise_from(_core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: Reshape cannot infer the missing input size for an empty tensor unless all specified input sizes are non-zero [Op:Reshape]
```
I'd expect it to instead just give back a tensor of the same shape (also empty).

For example, `tf.nn.softmax(tf.zeros([0]))` and `tf.nn.softmax(tf.zeros([16,16,0]))` fail, but `tf.nn.softmax(tf.zeros([16,0]))` works (since no reshape needs to be performed).

This seems to happen because `_flatten_outer_dims` in `nn_ops.py` uses `-1` as a dimension in a `reshape`. 

### Source code / logs
Here's an interactive session with eager execution that demonstrates the problem. It happens the same way without eager execution.
```
Python 3.6.4 |Anaconda custom (64-bit)| (default, Jan 16 2018, 12:04:33)
[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> tf.enable_eager_execution()
>>> tf.nn.softmax(tf.zeros([0]))
2018-09-13 13:08:55.015409: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 454, in new_func
    return func(*args, **kwargs)
  File ""/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 1738, in softmax
    return _softmax(logits, gen_nn_ops.softmax, axis, name)
  File ""/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 1679, in _softmax
    logits = _flatten_outer_dims(logits)
  File ""/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 1612, in _flatten_outer_dims
    output = array_ops.reshape(logits, array_ops.concat([[-1], last_dim_size], 0))
  File ""/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 6222, in reshape
    _six.raise_from(_core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: Reshape cannot infer the missing input size for an empty tensor unless all specified input sizes are non-zero [Op:Reshape]
>>> tf.nn.softmax(tf.zeros([16,0]))
<tf.Tensor: id=17, shape=(16, 0), dtype=float32, numpy=array([], shape=(16, 0), dtype=float32)>
>>> tf.nn.softmax(tf.zeros([16,16,0]))
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 454, in new_func
    return func(*args, **kwargs)
  File ""/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 1738, in softmax
    return _softmax(logits, gen_nn_ops.softmax, axis, name)
  File ""/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 1679, in _softmax
    logits = _flatten_outer_dims(logits)
  File ""/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 1612, in _flatten_outer_dims
    output = array_ops.reshape(logits, array_ops.concat([[-1], last_dim_size], 0))
  File ""/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 6222, in reshape
    _six.raise_from(_core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: Reshape cannot infer the missing input size for an empty tensor unless all specified input sizes are non-zero [Op:Reshape]
>>>
```"
22257,"I found that even after following the tensorflow documentation for installing tf gpu, tensorflow and keras was still using CPU only for computation.","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22254,Feature Request: Permit Changes to LSTMStateTuple to be compatible with autodiff,"I would like to make changes to an LSTMStateTuple that (in theory) should be compatible with auto differentiation (I'm taking a vector output by one LSTM and adding it to the cell state `c` of another LSTM at each ""time step""). However, as far as I can tell, LSTMStateTuple is a named tupled, so making a change to `c` requires creating a new LSTMStateTuple, preventing autodiff from working. I confirmed with Tensorboard summaries that no gradients are flowing through the kernels or biases of my second LSTM. Does a work-around exist or is this capability missing?"
22252,Is tensorflow-gpu version same with just tensorflow for cpu?,"I've developed some Keras code using tensorflow with cpu on Windows7 on my pc, and trained it with GPU on linux.

After complete training, I have moved the trained model files to my pc and used them without any problems. But I don't know what happened, it doesn't work on my pc. I guess it is from Keras and tensorflow version issue.

The linux's Keras and tensorflow version are Keras (2.1.5), tensorflow-gpu (1.4.1), and my pc's version are Keras (2.1.5), tensorflow (1.4.0.).

Below is the error msg.

```
Traceback (most recent call last):
  File ""C:\WinPython-64bit-3.5.4.1Qt5\python-3.5.4.amd64\lib\site-packages\tensorflow\python\client\session.py"", line 1323, in _do_call
    return fn(*args)
  File ""C:\WinPython-64bit-3.5.4.1Qt5\python-3.5.4.amd64\lib\site-packages\tensorflow\python\client\session.py"", line 1302, in _run_fn
    status, run_metadata)
  File ""C:\WinPython-64bit-3.5.4.1Qt5\python-3.5.4.amd64\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 473, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[22,3] = 13257 is not in [0, 13004)
         [[Node: embedding_1/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=""/job:localhost/replica:0/task:0/devi
ce:CPU:0""](embedding_1/embeddings/read, embedding_1/Cast)]]


```
and 
```
File ""C:\WinPython-64bit-3.5.4.1Qt5\python-3.5.4.amd64\lib\site-packages\tensorflow\python\client\session.py"", line 1336, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[22,3] = 13257 is not in [0, 13004)
         [[Node: embedding_1/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=""/job:localhost/replica:0/task:0/devi
ce:CPU:0""](embedding_1/embeddings/read, embedding_1/Cast)]]
```

and
```
File ""C:\WinPython-64bit-3.5.4.1Qt5\python-3.5.4.amd64\lib\site-packages\tensorflow\python\framework\ops.py"", line 2956, in create_op
    op_def=op_def)
  File ""C:\WinPython-64bit-3.5.4.1Qt5\python-3.5.4.amd64\lib\site-packages\tensorflow\python\framework\ops.py"", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): indices[22,3] = 13257 is not in [0, 13004)
         [[Node: embedding_1/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=""/job:localhost/replica:0/task:0/devi
ce:CPU:0""](embedding_1/embeddings/read, embedding_1/Cast)]]

Exception ignored in: <bound method BaseSession.__del__ of <tensorflow.python.client.session.Session object at 0x000000001A8DE198>>
Traceback (most recent call last):
  File ""C:\WinPython-64bit-3.5.4.1Qt5\python-3.5.4.amd64\lib\site-packages\tensorflow\python\client\session.py"", line 696, in __del__
TypeError: 'NoneType' object is not callable

```

There were several similar questions, but anything were helpful. To use trained model file from Keras (2.1.5) and tensorflow-gpu (1.4.1), which Keras and tensorflow(cpu) version should I use? Of course I use same python version with the linux which is 3.5 on 64bit.
"
22251,error when adding library ,
22250,"Documentation: ""Creating Custom Estimators"" says that it uses same train_input_fn as ""pre-made Estimator implementation"", but replaces the dataset return value by an iterator.get_next() without explaining","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:na
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:na
- **TensorFlow installed from (source or binary)**:na
- **TensorFlow version (use command below)**:na
- **Python version**:na
- **Bazel version (if compiling from source)**:na
- **GCC/Compiler version (if compiling from source)**:na
- **CUDA/cuDNN version**:na
- **GPU model and memory**:na
- **Exact command to reproduce**:na

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

In the documentation ""Creating Custom Estimators"" (https://www.tensorflow.org/guide/custom_estimators), under ""Write an Input function"", it is claimed that the function is the same as in ""pre-made Estimator implementation"" (https://www.tensorflow.org/guide/premade_estimators) and in iris_data.py (https://github.com/tensorflow/models/blob/master/samples/core/get_started/iris_data.py). But in both these sources the function is returning the created dataset, while in ""Creating Custom Estimators"" the function returns dataset.make_one_shot_iterator().get_next() instead.

I haven't been able to understand if both approaches are equivalent, or if both are correct but yield different results, or if one of them is wrong. But in any of these situations, I think the documentation should state why it is using the get next operations instead of the original dataset, since it refers to sources where dataset is used instead.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

""Creating Custom Estimators"": https://www.tensorflow.org/guide/custom_estimators
Claim: ""_Our custom Estimator implementation uses the same input function as our pre-made Estimator implementation, from iris_data.py. Namely:_""
Function:
```
def train_input_fn(features, labels, batch_size):
  """"""An input function for training""""""
  # Convert the inputs to a Dataset.
  dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))

  # Shuffle, repeat, and batch the examples.
  dataset = dataset.shuffle(1000).repeat().batch(batch_size)

  # Return the read end of the pipeline.
  return dataset.make_one_shot_iterator().get_next()
```
""Premade Estimators"": https://www.tensorflow.org/guide/premade_estimators
Function:
```
def train_input_fn(features, labels, batch_size):
  """"""An input function for training""""""
  # Convert the inputs to a Dataset.
  dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))

  # Shuffle, repeat, and batch the examples.
  return dataset.shuffle(1000).repeat().batch(batch_size)
```
iris_data.py: https://github.com/tensorflow/models/blob/master/samples/core/get_started/iris_data.py
Function:
```
def train_input_fn(features, labels, batch_size):
    """"""An input function for training""""""
    # Convert the inputs to a Dataset.
    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))
    # Shuffle, repeat, and batch the examples.
    dataset = dataset.shuffle(1000).repeat().batch(batch_size)
    # Return the dataset.
    return dataset
```"
22249,Inconsistency in supported integer types on GPU,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
not relevant
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux 4.9.0-8-amd64 #1 SMP Debian 4.9.110-3+deb9u4 (2018-08-21) x86_64 GNU/Linux
VERSION_ID=""9""
VERSION=""9 (stretch)""
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
1.10.1 (from pip binary)
- **TensorFlow version (use command below)**:
tf.VERSION = 1.10.1
tf.GIT_VERSION = v1.10.1-0-g4dcfddc5d1
tf.COMPILER_VERSION = v1.10.1-0-g4dcfddc5d1
- **Python version**:
Python 3.5.3
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2017 NVIDIA Corporation
Built on Fri_Nov__3_21:07:56_CDT_2017
Cuda compilation tools, release 9.1, V9.1.85

- **GPU model and memory**:
Tesla P100-PCIE-16GB

- **Exact command to reproduce**:


### Describe the problem
It appears that the kernel of `tf.reduce_sum` is not registerd for GPU's if the type of the tensor to be summed is `int64` (only registered for `tf.int32`)
Moreover the kernel of `tf.tile` is not registered for the case where the tensor to be tiled is of type `tf.int32` (only registered for `tf.int64`)

Why is there this inconsistency?
"
22248,Calculating custom metrics with tf.estimator.DNNRegressor in TensorFlow 1.10,"How to configure a `tf.estimator.DNNRegressor` to report different metrics like **RMSE** and **MAE** while evaluating?

(One can ask the same question for `tf.estimator.DNNClassifier` and **AUC** metric)

> **Note:** I know that it must be done in `tf.estimator.EstimatorSpec` of `model_fn()` for a custom `tf.estimator.Estimator`, but I don't know how to apply it for a `tf.estimator.DNNRegressor`.

[https://stackoverflow.com/q/52300519/2737801](https://stackoverflow.com/q/52300519/2737801)"
22246,CPU nodes are swapped out using /grappler/optimizers/gpu_swapping_kernel,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.5
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: Issue occurs on both pip binary and compiled from source
- **TensorFlow version (use command below)**: 'v1.8.0-8-g23c2187' 1.8.0
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: 0.16.1
- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
- **CUDA/cuDNN version**: CUDA 9.0, cuDNN 7.2.1.38
- **GPU model and memory**: GeForce GTX 1080, 11178MiB
- **Exact command to reproduce**: Cannot share the exact codebase


### Describe the problem

Tensorflow attempts to swap-out a node to the CPU to save memory, even when the node is already on CPU.

This happens in `core/grappler/optimizers/memory_optimizers.cc` with the op `_CopyFromGpuToHost`.
But since this swapping is intended to save GPU memory by swapping temporarily nodes from GPU to the the CPU, the kernel for that op in `core/grappler/optimizers/gpu_swapping_kernels.cc` is only for GPU devices.

A suggested fix will be to simply check if the node to be swapped out is already in CPU before making it eligible for swap_out/swap_in. Note that on reducing the model size this error disappears as more memory becomes free.

#### Some more information confirming the issue - 
The op is correctly registered in OpRegistry as shown by `tensorflow::OpRegistry::Global()->DebugString(false)`.
The OpKernel `CopyFromGpuToHostKernel` is built according to bazel logs.
The OpKernel is loaded on importing tensorflow in python, verified by adding a print in that file using `__attribute__((constructor)) )`

### Source code / logs

The relevant error is -

tensorflow.python.framework.errors_impl.NotFoundError: No registered '_CopyFromGpuToHost' OpKernel for CPU devices compatible with node swap_out_context_zoom/GatherNd_2_0 = _CopyFromGpuToHost[T=DT_INT32, _class=[""loc@context_zoom/GatherNd_2_0""], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_context_sents_len_0_10)
	.  Registered:  device='GPU'

	 [[Node: swap_out_context_zoom/GatherNd_2_0 = _CopyFromGpuToHost[T=DT_INT32, _class=[""loc@context_zoom/GatherNd_2_0""], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_context_sents_len_0_10)

I have attached the full log here - 
https://gist.github.com/akhilkedia/c13ff191e08b4462a3870a0b3a091e93

"
22245,Could not open .\linear_regression.pbtxt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):,"Please go to Stack Overflow for help and support:


If you open a GitHub issue, here is our policy:

------------------------

### System information
TensorFlow 1.9
Windows 10
Python 3.6 64

You can collect some of this information using our environment capture script:

```
2018-09-12 19:55:49.880076: I C:\tf_jenkins\workspace\rel-win\M\windows\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2018-09-12 19:55:49.898198: W C:\tf_jenkins\workspace\rel-win\M\windows\PY\36\tensorflow\core\util\tensor_slice_reader.cc:95] Could not open .\linear_regression.pbtxt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?
Traceback (most recent call last):
  File ""C:/Users/vlad/Downloads/Linear_Regression_Model/Freeze_Graph.py"", line 7, in <module>
    initializer_nodes='', variable_names_blacklist=''
  File ""C:\Python36\lib\site-packages\tensorflow\python\tools\freeze_graph.py"", line 244, in freeze_graph
    saved_model_tags.split("",""), checkpoint_version=checkpoint_version)
  File ""C:\Python36\lib\site-packages\tensorflow\python\tools\freeze_graph.py"", line 119, in freeze_graph_with_def_protos
    reader = pywrap_tensorflow.NewCheckpointReader(input_checkpoint)
  File ""C:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 254, in NewCheckpointReader
    return CheckpointReader(compat.as_bytes(filepattern), status)
  File ""C:\Python36\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 516, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file .\linear_regression.pbtxt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?

```




### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
```
import tensorflow as tf
from tensorflow.python.tools import freeze_graph, optimize_for_inference_lib

freeze_graph.freeze_graph(input_graph='linear_regression.pbtxt', input_saver='', input_binary=True, input_checkpoint='linear_regression.pbtxt', output_node_names='y_output',
                          restore_op_name='save/restore_all',
                          filename_tensor_name='save/Const:0', output_graph='frozen_linear_regression.pb', clear_devices= True,
                          initializer_nodes='', variable_names_blacklist=''
                          )
input_graph_def = tf.GraphDef()
with tf.gfile.Open('frozen_linear_regression.pb', 'rb') as f:
    data = f.read()
    input_graph_def.ParseFromString(data)

output_graph_def = optimize_for_inference_lib.optimize_for_inference(input_graph_def=input_graph_def, input_node_names=['x'],
                                                                     output_node_names=['y_output'],
                                                                     placeholder_type_enum=tf.float32.as_datatype_enum)

f = tf.gfile.FastGFile(name='optimized_frozen_linear_regression.pb', mode='w')

f.write(file_content=output_graph_def.SerializeToString())


```"
22243,bazel build branch r1.10 on windows ERROR _api_implementation.so,"ERROR: C:/users/thinkpad/_bazel_thinkpad/26orbg4z/external/protobuf_archive/BUILD:645:1: C++ compilation of rule '@protobuf_archive//:python/google/protobuf/internal/_api_implementation.so' failed (Exit 1): cl.exe failed: error executing command

C:\users\thinkpad\_bazel_thinkpad\26orbg4z\execroot\org_tensorflow\external\protobuf_archive\python\google\protobuf\internal\api_implementation.cc : fatal error C1083: : : Invalid argument
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 22363.015s, Critical Path: 4574.16s
INFO: 6185 processes: 6185 local.
FAILED: Build did NOT complete successfully

How to solve compilation errors?
Thank you"
22242,Using TF-TRT doubles the size of frozen protobuf file,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/a
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: master @ 3e137b24b06a81772402b86392dbd158653d487b
- **Python version**:2.7
- **Bazel version (if compiling from source)**: 0.16.1
- **GCC/Compiler version (if compiling from source)**: 5.4.0 20160609
- **CUDA/cuDNN version**: 9.0/7.1
- **GPU model and memory**: 1080ti/11gb dual
- **Exact command to reproduce**: 

```
import tensorflow as tf
from tensorflow.contrib import tensorrt as trt
import numpy as np

from keras import backend as K
cfg = K.tf.ConfigProto()
cfg.gpu_options.allow_growth = True
K.set_session(K.tf.Session(config=cfg))

def load_graph(frozen_graph_filename):
    # We load the protobuf file from the disk and parse it to retrieve the
    # unserialized graph_def
    with tf.gfile.GFile(frozen_graph_filename, ""rb"") as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())

    # Then, we can use again a convenient built-in function to import a graph_def into the
    # current default Graph
    with tf.Graph().as_default() as graph:
        tf.import_graph_def(
            graph_def,
            name='', #DEBUG
        )
    return graph

fid = ""model.pb""
output_nodenames = 'output1,output2,output3'
output_node = list(output_nodenames.split("",""))
g = load_graph(fid)
with tf.Session(graph=g) as sess:
    nodes = [n.name for n in tf.get_default_graph().as_graph_def().node]
    with open(""original_graph.txt"", ""w"") as fid:
        for item in nodes:
            fid.write(""%s\n"" % item)
    writer = tf.summary.FileWriter(""logs_viz_orig"", tf.get_default_graph().as_graph_def())
    trt_graph = trt.create_inference_graph(
    input_graph_def=tf.get_default_graph().as_graph_def(),
    outputs=output_node,
    max_batch_size=1,
    max_workspace_size_bytes=1 << 25,
    precision_mode=""FP32"",  # TRT Engine precision ""FP32"",""FP16"" or ""INT8""
    minimum_segment_size=2  # minimum number of nodes in an engine
    )
    f = open(""trt.pb"", 'w')
    f.write(trt_graph.SerializeToString())
    f.close()
```
  
The original graph as ~1100 ops in total, and trt graph has ~900, even then the original model was ~60mb, whereas the exported trt graph is ~120 mb. 

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22240,absl/strings/string_view.h: No such file or directory,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
N/A
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:
Latest 
- **Python version**: 2.7
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I am trying to build tensorflow as a standalone project and have been following this tutorial:
https://tuatini.me/building-tensorflow-as-a-standalone-project/

Everything is working till I try to run the script. I get the following error:

g++ -std=c++11 -Wl,-rpath='$ORIGIN/lib' -Iinclude -Llib main.cpp -ltensorflow_cc -o exec
In file included from include/tensorflow/core/platform/tensor_coding.h:22:0,
                 from include/tensorflow/core/framework/resource_handle.h:19,
                 from include/tensorflow/core/framework/allocator.h:24,
                 from include/tensorflow/core/framework/tensor.h:20,
                 from include/tensorflow/cc/framework/ops.h:21,
                 from include/tensorflow/cc/client/client_session.h:24,
                 from main.cpp:9:
include/tensorflow/core/lib/core/stringpiece.h:34:38: fatal error: absl/strings/string_view.h: No such file or directory
 #include ""absl/strings/string_view.h""
                                      ^
Any work around for this? Thanks!



"
22239,configure.gpu_options.per_process_gpu_memory_fraction = 1case OOM,"### System information

- **Have I written custom code**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS Linux release 7.3.1611 (Core), but that does not matter
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: ('v1.10.1-0-g4dcfddc5d1', '1.10.1')
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0/7.0
- **GPU model and memory**: N/A
- **Exact command to reproduce**: docu issue

### Describe the problem

when use the code below to config the tensorflow session, will case an `CUDA_ERROR_OUT_OF_MEMORY` error.

`configure.gpu_options.per_process_gpu_memory_fraction = 1`

```
2018-09-12 20:59:57.154299: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-09-12 20:59:57.409106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties:
name: Tesla P40 major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:84:00.0
totalMemory: 22.38GiB freeMemory: 22.21GiB
2018-09-12 20:59:57.409748: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2018-09-12 20:59:59.004788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-09-12 20:59:59.005075: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0
2018-09-12 20:59:59.005126: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N
2018-09-12 20:59:59.008029: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22912 MB memory) -> physical GPU (device: 0, name: Tesla P40, pci bus id: 0000:84:00.0, compute capability: 6.1)
2018-09-12 20:59:59.015888: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 22.38G (24025956352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
```

### Source code 

`cuda_driver.cc`

```
if (per_process_gpu_memory_fraction == 0) {
    allocated_memory = available_memory;
    const int64 min_system_memory = MinSystemMemory(available_memory);
    if (min_system_memory < allocated_memory) {
      allocated_memory -= min_system_memory;
    }
  } else {
    allocated_memory = total_memory * per_process_gpu_memory_fraction;
  }
  *memory_limit = allocated_memory;
```

tensorflow set memory_limit by `total_memory * per_process_gpu_memory_fraction;` , this case the OOM error, I think, may be `free_memory * per_process_gpu_memory_fraction;` is better.

"
22238,TENSORFLOW quantize_graph.py throws error : Graph_def is invalid at node error,"- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:NA
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:14.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:NA
- **TensorFlow installed from (source or binary)**:PIP and BAZEL
- **TensorFlow version (use command below)**: 1.10
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.15
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: NVIDIA 12GB

ERROR ::: Graph_def is invalid at node u'ExpandDims': Input tensor 'image_ph:0' Cannot convert a tensor of type float32 to an input of type int32

I have trained a model , froze it and performed optimization (https://petewarden.com/2016/09/27/tensorflow-for-mobile-poets/) , graph transformation (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms) on tensorflow 1.4.0 till the point which there is no error but when i quantize the model i run into the above mentioned error.

As per https://github.com/tensorflow/tensorflow/issues/4044 i have tried maintaining version consistency which has failed to solve the issue."
22237,Graph_def is invalid at node u'ExpandDims': Input tensor 'image_ph:0' Cannot convert a tensor of type float32 to an input of type int32,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22236,Error restoring checkpoint: BeamSearchDecoder,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows7, CPU 
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: tf.VERSION = 1.10.0|tf.GIT_VERSION = b'v1.10.0-rc1-19-g656e7a2b34'| tf.COMPILER_VERSION = b'v1.10.0-rc1-19-g656e7a2b34'
- **Python version**: Python 3.5.5
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: NA

### Describe the problem
I am trying to load a model trained using basic decoder with train_ce=True and loading it with beamsearch with train_ws=True (like inference) but it fails during restore:

```
Traceback (most recent call last):
  File ""train_ws.py"", line 502, in <module>
    tf.app.run()
  File ""C:\ProgramData\Miniconda3\envs\tensorflow\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""train_ws.py"", line 337, in main
    saver.restore(sess, FLAGS.restore_path)
  File ""C:\ProgramData\Miniconda3\envs\tensorflow\lib\site-packages\tensorflow\python\training\saver.py"", line 1759, in restore
    err, ""a mismatch between the current graph and the graph"")
tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

Input 1 of node decoder/dynamic_decoder_out/decoder/while/Merge_7_1 was passed float from decoder/dynamic_decoder_out/decoder/while/NextIteration_7:0 incompatible with expected int32.
```

### Source code 
```
def inference_s2s_ce(self, encoder_inputs, decoder_inputs, encoder_inputs_lenghts, decoder_inputs_lenghts, feed_previous):
    true_batch_size = tf.size(encoder_inputs_lenghts)
    encoder_outputs, encoder_state = self.input_encoder(encoder_inputs)
    print('encoder_state ', encoder_state)
    attention_inputs = encoder_outputs # Not time major
    #encoder_outputs = tf.transpose(encoder_outputs, [1, 0, 2]) # time major
    if self.mode == 'train_ws':
        attention_inputs = tf.contrib.seq2seq.tile_batch(encoder_outputs, multiplier=self.beam_width)
        encoder_state = tf.contrib.seq2seq.tile_batch(encoder_state, multiplier=self.beam_width)
        encoder_inputs_lenghts = tf.contrib.seq2seq.tile_batch(encoder_inputs_lenghts, multiplier=self.beam_width)  
    with tf.variable_scope('decoder', reuse=tf.AUTO_REUSE) as scope:
        decoder_cell = tf.contrib.rnn.LSTMCell(self.rnn_size,state_is_tuple=True)
        embedding_decoder = variable_scope.get_variable(""embedding_decoder"", [self.tgt_vocab_size, self.decoder_embedding_size])
        decoder_emb_inp = tf.nn.embedding_lookup(embedding_decoder, decoder_inputs)
        input_layer = Dense(self.rnn_size, dtype=self.dtype, name='input_projection')
        decoder_emb_inp = input_layer(decoder_emb_inp)
        attention_mechanism = tf.contrib.seq2seq.BahdanauAttention( num_units=self.rnn_size, memory=attention_inputs, 
        memory_sequence_length=encoder_inputs_lenghts)
        attn_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism)#, attention_layer_size= self.num_units / 2)
        projection_layer = tf.layers.Dense(self.tgt_vocab_size, use_bias=False)
        if self.mode == 'train_ce':
            helper = tf.contrib.seq2seq.TrainingHelper(decoder_emb_inp, decoder_inputs_lenghts)#,  time_major=False)
            decoder_initial_state = attn_cell.zero_state(true_batch_size, dtype=tf.float32).clone(cell_state=encoder_state)
            decoder = tf.contrib.seq2seq.BasicDecoder(
                cell=attn_cell, helper=helper,
                initial_state=decoder_initial_state, 
                output_layer=projection_layer)
            inference_decoder = decoder
        elif self.mode == 'train_ws':
            tgt_sos_id = 3
            tgt_eos_id = 1
            start_tokens = tf.tile(tf.constant([tgt_sos_id], dtype=tf.int32), [true_batch_size])
            batch_size = true_batch_size * self.beam_width#tf.cast(true_batch_size * self.beam_width, dtype=tf.int32)
            decoder_initial_state = attn_cell.zero_state(batch_size, tf.float32).clone(cell_state=encoder_state)
            print('decoder_initial_state shape ', decoder_initial_state)
            decoder = tf.contrib.seq2seq.BeamSearchDecoder(cell = attn_cell, embedding=embedding_decoder, start_tokens=start_tokens,
            end_token=tgt_eos_id, initial_state=decoder_initial_state, beam_width=int(self.beam_width), output_layer=projection_layer)
            inference_decoder = decoder
        with tf.variable_scope('dynamic_decoder_out', reuse=tf.AUTO_REUSE) as scope:
            outputs, _, _ = seq2seq.dynamic_decode(decoder=inference_decoder,output_time_major=False,maximum_iterations=self.output_max_length) 
            if self.mode == 'train_ce':
                logits = outputs.rnn_output
                print('Logits ', logits.get_shape())
                return logits
            elif self.mode == 'train_ws':
                return outputs.beam_search_decoder_output.scores
        
```
I have tried restoring basicdecoder, that works.
"
22235,TF 1.10 needs CUDA 9.2 - put info in release notes,"Hi,
TF 1.10 needs CUDA 9.2 now. I am missing this info in the RELEASE.md file.
I suggest to add that. Did cost me a lot of time installing the different CUDA versions...

What do you think?

Even this documentation is ""wrong"" about this:
https://www.tensorflow.org/install/install_linux#tensorflow_gpu_support

Thanks
Philip

------------------------

Added informations (that do not matter) as requested:

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: newest Xubuntu but that does not matter
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: no
- **TensorFlow installed from (source or binary)**: bin
- **TensorFlow version (use command below)**: 1.10.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: no
- **GCC/Compiler version (if compiling from source)**: no
- **CUDA/cuDNN version**: 9.2
- **GPU model and memory**: does not matter
- **Exact command to reproduce**: docu issue
"
22234,[Bug] Keras TimeDistributed Concatenation,"### System information
- **Code**:
```
import tensorflow.keras as k
import numpy as np

input1 = k.layers.Input(shape=(None, 10))
input2 = k.layers.Input(shape=(10,))

concat_layer = k.layers.Lambda(lambda x: k.layers.concatenate([x, input2]))
results = k.layers.TimeDistributed(concat_layer)(input1)

model = k.models.Model([input1, input2], results)

data1 = np.zeros((3,7,10))
data2 = np.zeros((3,10))

model.predict([data1,data2])
```
- **OS Platform and Distribution**: Windows server 2016
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 'v1.10.0-rc1-19-g656e7a2b34' 1.10.0
- **Python version**: 3.6.6
- **CUDA/cuDNN version**: CUDA 9.0, cuDNN 7.0
- **Have I written custom code**: all the same as above code
- **Bazel version** : N/A
- **GPU model and memory**: Quadro P600, 2GiB
- **Exact command to reproduce**: python test.py
- **Mobile device**: N/A

### Describe the problem
I try to run the code mentioned above.
My goal is to concatenate input2 to each timestep of input1.
But the log shows TF reshaped first two dimensions of input1 when concatenating.

Line 235 in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/wrappers.py is wrong.
Even the batch size is None, the time-distributed layer's behavior may relate to actual batch size.

```
if input_shape[0]:
      # batch size matters, use rnn-based implementation
      ...................
else:
      # No batch size specified, therefore the layer will be able
      # to process batches of any size. (TRUE)
      # We can go with reshape-based implementation for performance.
      (FALSE! BEHAVIOR MAY RELATE TO ACTUAL BATCH SIZE)
```

### Source code / logs
> Traceback (most recent call last):
>   File ""<stdin>"", line 1, in <module>
>   File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 1493, in predict
>     self, x, batch_size=batch_size, verbose=verbose, steps=steps)
>   File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\keras\engine\training_arrays.py"", line 374, in predict_loop
>     batch_outs = f(ins_batch)
>   File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\keras\backend.py"", line 2914, in __call__
>     fetched = self._callable_fn(*array_vals)
>   File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1382, in __call__
>     run_metadata_ptr)
>   File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 519, in __exit__
>     c_api.TF_GetCode(self.status.status))
> tensorflow.python.framework.errors_impl.InvalidArgumentError: ConcatOp : Dimensions of inputs should match: shape[0] = [21,10] vs. shape[1] = [3,10]
>          [[Node: time_distributed_2/concatenate_1/concat = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](time_distributed_2/Reshape, _arg_input_3_0_1/_3, time_distributed_2/concatenate_1
> /concat/axis)]]
>          [[Node: time_distributed_2/Reshape_1/_5 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_n
> ame=""edge_27_time_distributed_2/Reshape_1"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]"
22233,ssd_mobilenet_v2 frozen model size much larger than ssd_mobilenet_v1,"Hi, comparing the pre-trained models published on tensorflow object detection model zoo, why the frozen_model/check_point/saved_model of ssd_mobilenet_v2_coco is much larger than the ones of ssd_mobilenet_v1_coco? mobilenet v2 should be more lite than mobilenet v1."
22232, encrypt the tflite model,"  I want to encrypt the tflite model, what should I do?"
22229,intra_op_parallelism_threads will be invalid if call any API initialized device,"### System information

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow):** Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04):** (Red Hat 4.8.5-16)
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile devic:** n/a
- **TensorFlow installed from (source or binary):** source
- **TensorFlow version (use command below):** 1.10.0
- **python version:** 3.4.5
- **Bazel version (if compiling from source):** 0.15.1
- **GCC/Compiler version (if compiling from source):** gcc version 6.3.1
- **CUDA/cuDNN version:** n/a
- **GPU model and memory:** n/a
- **Exact command to reproduce:** Run included script

### Describe the problem

I'm trying to run [NCF model ](https://github.com/tensorflow/models/tree/master/official/recommendation) on X86 CPU, and add the **intra_op_parallelism_threads** to tune the performance. Then I found that if we call any API including **list_devices()**, TF would initialize a global intra thread pool and overwrite the configuration of **intra_op_parallelism_threads**.

NCF will call **is_gpu_available()** before training, and its implementation includes **list_devices()**, so the **intra_op_parallelism_threads** option is invalid.

My question is, dose this logic makes sense? If we want to run a model on CPU, the config may be invalid when we call some 'harmless' API and get no feedback.

### Source code / logs

here's the modify of NCF to enable **intra_op_parallelism_threads**:
[ncf_intra.txt](https://github.com/tensorflow/tensorflow/files/2373942/ncf_intra.txt)

I also add some code to print the intra thread pool status in TF, this patch has changed the global setting to false to solve the issue, revert it to true will reproduce the question:
[tf_intra.txt](https://github.com/tensorflow/tensorflow/files/2373947/tf_intra.txt)

With the print code, you can see that TF will try to initialize a device with a global intra thread pool when call **list_devices()**. Once if the global intra thread pool was initialized, other device couldn't change the configuration any more.


"
22226,Orthogonal initialization gives inaccurate result when running on GPU,"### System information
- **Have I written custom code**: see example below
- **OS Platform and Distribution**:Google Colabratory (also in compiled code on Linux)
- **Mobile device**: N/A
- **TensorFlow installed from (source or binary)**: Google Colabratory
- **TensorFlow version (use command below)**: 1.10.1
- **Python version**: 2.7.14 
- **Bazel version (if compiling from source)**:  N/A
- **GCC/Compiler version (if compiling from source)**:  N/A
- **CUDA/cuDNN version**:  N/A
- **GPU model and memory**: Issue occurred both on Telsa V100 and Tesla K80 with at least 11GB of available memory (but not when using CPU runtime)
- **Exact command to reproduce**: see example below

### Describe the problem
When generating an orthogonal matrix and running on a GPU, if the matrix is 1000x1000 or larger the resulting matrix is not orthogonal. The deviations are several orders of magnitude larger than expected from finite precision errors.
The issue doesn't occur for smaller n (say 100) or when running on a CPU.

In the example below, the final result has entry-wise deviations from the identity matrix of order 1e-2.

### Source code / logs
 

```
import tensorflow as tf
tf.enable_eager_execution()
tf.set_random_seed(0)
init = tf.orthogonal_initializer(1,seed=0)
a = init([1000,1000])
tf.matmul(a,tf.transpose(a))
```
"
22224,Eeager Mode:sigmoid_cross_entropy_with_logits expects int32 but not float tensor,"### System information
OS: ubuntu 1604
Virtual Env:Conda
TF Version:1.10
Python:3.6.6

```python
tfe = tf.contrib.eager
def loss(inputs, labels):
    print(labels.dtype)
    logits = model(inputs, training=True)
    print(logits.dtype)
    return tf.reduce_mean(
      tf.nn.sigmoid_cross_entropy_with_logits(
        logits=logits,
        labels=labels)
    )

val_grad_fn = tfe.implicit_value_and_gradients(loss)
```
Error: here can be seen the type of logits is float32, same as the requirement of  [sigmoid_cross_entropy_with_logits](https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits).   
So I'm curious where's wrong.
```
<dtype: 'int32'>
<dtype: 'float32'>
Traceback (most recent call last):
  File ""model.py"", line 384, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/home/ubuntu/hdd1/zip_data/Paul/miniconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""model.py"", line 335, in main
    train_one_epoch(epoch_i=i, train_data=ds_train, log_interval=FLAGS.log_interval, **model_objects)
  File ""model.py"", line 264, in train_one_epoch
    value, grads_and_vars = val_grad_fn((u, i, i_c, ts, hist_c, sl), y)
  File ""/home/ubuntu/hdd1/zip_data/Paul/miniconda3/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py"", line 203, in grad_fn
    end_node = f(*args, **kwds)
  File ""model.py"", line 249, in loss
    labels=labels)
  File ""/home/ubuntu/hdd1/zip_data/Paul/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py"", line 181, in sigmoid_cross_entropy_with_logits
    relu_logits - logits * labels,
  File ""/home/ubuntu/hdd1/zip_data/Paul/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py"", line 850, in binary_op_wrapper
    return func(x, y, name=name)
  File ""/home/ubuntu/hdd1/zip_data/Paul/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py"", line 1094, in _mul_dispatch
    return gen_math_ops.mul(x, y, name=name)
  File ""/home/ubuntu/hdd1/zip_data/Paul/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 4959, in mul
    _six.raise_from(_core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: cannot compute Mul as input #0 was expected to be a int32 tensor but is a float tensor [Op:Mul] name: logistic_loss/mul/
```
"
22223,tf.unstack did not work with tf 1.8 CudnnGRU tensors,"- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
```
$uname -r
3.10.0-327.el7.x86_64
```
- **Mobile device**
```
Not mobile
```

- **TensorFlow installed from (source or binary)**:
anaconda tf 1.8

- **TensorFlow version (use command below)**:
```
$conda list|grep tensor
tensorboard               1.8.0            py36hf484d3e_0
tensorflow                1.8.0                hb381393_0
tensorflow-base           1.8.0            py36h4df133c_0
tensorflow-gpu            1.8.0                h7b35bdc_0
```
- **Python version**:
```
$python3.6 -V
Python 3.6.2 :: Continuum Analytics, Inc.
```
- **Bazel version (if compiling from source)**:
```
$bazel version
Build label: 0.4.5
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Mar 16 12:19:38 2017 (1489666778)
Build timestamp: 1489666778
Build timestamp as int: 1489666778
```

- **CUDA/cuDNN version**:
```
$conda list|grep -i cuda
cudatoolkit               8.0                           3    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free
cudnn                     7.0.5                 cuda8.0_0
```
- **GPU model and memory**:
```

== cat /etc/issue ===============================================
Linux rvab01298.sqa.ztt 3.10.0-327.el7.x86_64 #1 SMP Thu Nov 19 22:10:57 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""7.2 (Paladin)""
VERSION_ID=""7.2""
Qihoo360_BUGZILLA_PRODUCT_VERSION=7.2
Qihoo360_SUPPORT_PRODUCT_VERSION=7.2

== are we in docker =============================================
No

== compiler =====================================================
c++ (GCC) 4.9.2
Copyright (C) 2014 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux rvab01298.sqa.ztt 3.10.0-327.el7.x86_64 #1 SMP Thu Nov 19 22:10:57 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.3)
protobuf (3.5.1)
tensorflow (1.8.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.8.0
tf.GIT_VERSION = b'unknown'
tf.COMPILER_VERSION = b'unknown'
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH :/usr/local/mpc-0.8.1/lib:/usr/local/gmp-4.3.2/lib:/usr/local/mpfr-2.4.2/lib:/gruntdata/qihoo360/cuda/lib64
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Wed Sep 12 13:34:30 2018
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.26                 Driver Version: 375.26                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K40m          On   | 0000:02:00.0     Off |                    0 |
| N/A   36C    P0    67W / 235W |   1161MiB / 11439MiB |     39%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla K40m          On   | 0000:03:00.0     Off |                    0 |
| N/A   35C    P0    60W / 235W |     73MiB / 11439MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0     13950    C   bin/arks                                       868MiB |
|    0     27880    C   python3.6                                      288MiB |
|    1     27880    C   python3.6                                       71MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/lib64/libcudart_static.a
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61
/usr/local/cuda-7.5/doc/man/man7/libcudart.7
/usr/local/cuda-7.5/doc/man/man7/libcudart.so.7
/usr/local/cuda-7.5/lib64/libcudart.so.7.5.18
/usr/local/cuda-7.5/lib64/libcudart_static.a
/usr/local/cuda-7.5/lib/libcudart.so.7.5.18
/usr/local/cuda-7.5/lib/libcudart_static.a
```

### Describe the problem
`tf.unstack` did not work as expected. It did not reduce `R` rank tensor to `R-1` rank tensor

### Source code / logs
code:
```
#! /usr/bin/env python
# -*- coding: utf-8 -*-

import sys
import tensorflow as tf
rnn_model = tf.contrib.cudnn_rnn.CudnnGRU(
        num_layers=1,
        num_units=64,
        direction='unidirectional')
rnn_model.build([3, 1, 3])
inputs=[[[1,1,1],[1,1,1],[1,1,1]]]
inputs_tensor= tf.convert_to_tensor(inputs, dtype=tf.float32)
print(tf.shape(inputs_tensor))
rnn_out, rnn_state = rnn_model(inputs_tensor)
print(""rnn_state: "", rnn_state)
rnn_layers = tf.unstack(rnn_state)
print(""rnn_layers"", rnn_layers)
```

paste the code to file `demo.py`, then run from linux command line:
```
$ python3.6 demo.py
```

output:
```
Tensor(""Shape:0"", shape=(3,), dtype=int32)
rnn_state:  (<tf.Tensor 'cudnn_gru/CudnnRNN:1' shape=(1, ?, 64) dtype=float32>,)
rnn_layers [<tf.Tensor 'unstack:0' shape=(1, ?, 64) dtype=float32>]
```

the `rnn_layers` should be ` rnn_layers [<tf.Tensor 'unstack:0' shape=(?, 64) dtype=float32>]`
"
22222,The latest process of building tensorflow serving for windows,"**Have I written custom code**  in /tensorflow/contrib/cmake/CMakeLists.txt
```
iftensorflow_OPTIMIZE_FOR_NATIVE_ARCH
  includeCheckCXXCompilerFlag
  CHECK_CXX_COMPILER_FLAG - march = native COMPILER_OPT_ARCH_NATIVE_SUPPORTED
  ifCOMPILER_OPT_ARCH_NATIVE_SUPPORTED
    setCMAKE_CXX_FLAGS  $ {CMAKE_CXX_FLAGS} -march = native
  endif
endif
```

changed to

```
if (tensorflow_OPTIMIZE_FOR_NATIVE_ARCH)
  include(CheckCXXCompilerFlag)
  CHECK_CXX_COMPILER_FLAG(""-march=native"" COMPILER_OPT_ARCH_NATIVE_SUPPORTED)
  if (COMPILER_OPT_ARCH_NATIVE_SUPPORTED)
    set(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -march=native"")
  else()
    CHECK_CXX_COMPILER_FLAG(""/arch:AVX"" COMPILER_OPT_ARCH_AVX_SUPPORTED)
    if(COMPILER_OPT_ARCH_AVX_SUPPORTED)
      set(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} /arch:AVX"")
    endif()
  endif()
endif()
```

**OS Platform and Distribution**    windows7 x64
**TensorFlow installed from**    sourse
**TensorFlow version**  r1.10
**Bazel version** N/A
**CUDA/cuDNN** version 9.0/Cudnn 7.0
**GPU model and memory** 1080Ti / 11G
**Exact command to reproduce**

**1. Cmake build:**
```
cmake .. -A x64 -DCMAKE_BUILD_TYPE=Debug
-T host=x64
-DSWIG_EXECUTABLE=D:/lib/swigwin-3.0.12/swig.exe
-DPYTHON_EXECUTABLE=C:/Users/tao/AppData/Local/Programs/Python/Python35-32/python.exe 
-DPYTHON_LIBRARIES=C:/Users/tao/AppData/Local/Programs/Python/Python35-32/libs/python35.lib 
-Dtensorflow_ENABLE_GPU=ON 
-Dtensorflow_ENABLE_GRPC_SUPPORT=OFF
-Dtensorflow_BUILD_SHARED_LIB=ON  
```

**2. Visual studio 2015:**

`MSBuild /p:Configuration=Debug /p:Platform=x64 ALL_BUILD.vcxproj
`
### Describe the problem

I want to use Tensorflow in my Windows C++ application. Therefore I'm trying to build in Visual studio on a Windows system.

Whether to support compiling DEBUG version.
### logs
```
fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\tensorflow\tensorflow\core\kernels\captured_function.cc)
```"
22221,Threading data out of one while loop into another interferes with gradients,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS 10.13.6
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: `b'v1.9.0-rc2-3217-g8e5c118ce8' 1.10.0`
- **Python version**: 3.6.2
- **Bazel version (if compiling from source)**: `0.15.2-homebrew`
- **GCC/Compiler version (if compiling from source)**: `Apple LLVM version 9.1.0 (clang-902.0.39.2)`
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: Run included script
- **Mobile device**: n/a

### Describe the problem
I have some code which

1. Generates some data using an inference-only `tf.while_loop`.
2. Uses a second while loop to run several minibatches of Adam, using the data from the first loop.

Both while loops have `back_prop=False`.  The second loop computes gradients, but these are used only inside the second loop (which includes running the `train_op`).  I use `tf.stop_gradients` to prevent gradients from flowing backwards from the second loop to the first...but to no avail:

```
Traceback (most recent call last):
  File ""./while-bug"", line 26, in <module>
    back_prop=False)
  File ""/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 3281, in while_loop
    return_same_structure)
  File ""/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 3001, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File ""/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2936, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File ""/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 3245, in <lambda>
    body = lambda i, lv: (i + 1, orig_body(lv))
  File ""./while-bug"", line 16, in body
    train = tf.train.AdamOptimizer(learning_rate=1e-2).minimize(loss)
  File ""/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 401, in minimize
    grad_loss=grad_loss)
  File ""/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 517, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 610, in gradients
    gate_gradients, aggregation_method, stop_gradients)
  File ""/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 674, in _GradientsHelper
    to_ops, from_ops, colocate_gradients_with_ops, func_graphs, xs)
  File ""/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 204, in _PendingCount
    between_op_list, between_ops, colocate_gradients_with_ops)
  File ""/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1442, in MaybeCreateControlFlowState
    loop_state.AddWhileContext(op, between_op_list, between_ops)
  File ""/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1247, in AddWhileContext
    grad_state = GradLoopState(forward_ctxt, outer_grad_state)
  File ""/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 824, in __init__
    cnt, forward_index = forward_ctxt.AddForwardLoopCounter(outer_grad_state)
  File ""/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2580, in AddForwardLoopCounter
    name=""f_count"")
  File ""/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 249, in _Enter
    data, frame_name, is_constant, parallel_iterations, name=name)
  File ""/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/ops/gen_control_flow_ops.py"", line 179, in enter
    parallel_iterations=parallel_iterations, name=name)
  File ""/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3254, in create_op
    op_def=op_def)
  File ""/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1787, in __init__
    self._control_flow_post_processing()
  File ""/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1796, in _control_flow_post_processing
    control_flow_util.CheckInputFromValidContext(self, input_tensor.op)
  File ""/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_util.py"", line 322, in CheckInputFromValidContext
    raise ValueError(error_msg + "" See info log for more details."")
ValueError: Cannot use 'while_1/gradients/f_count_1' as input to 'while_1/gradients/f_count' because they are in different while loops. See info log for more details.
```

(Caveat: line numbers for the above stacktrace may be slightly wrong, since my TF has some debug print statements.)

I've traced the problem to `ops/gradients_impl.py:_PendingCount` or the surrounding code.  `_PendingCount` seems to trace right through `tf.stop_gradients`.  I'm not sure where the right fix is, though, so could use some control flow expert help.

### Source code / logs

Here's a minimized test case:

```
import tensorflow as tf

# First while loop
rollouts = tf.while_loop(
    cond=lambda _: True,
    body=lambda _: tf.get_variable('b', []),
    loop_vars=[tf.zeros([])],
    maximum_iterations=1,
    back_prop=False)
rollouts = tf.stop_gradient(rollouts)

def body(i):
    loss = tf.stop_gradient(rollouts)
    train = tf.train.AdamOptimizer(learning_rate=1e-2).minimize(loss)
    with tf.control_dependencies([train]):
        return i + 1

# Second while loop.  Crashes since it thinks the gradients depend on
# something from the previous while loop.
tf.while_loop(
    cond=lambda _: True,
    body=body, maximum_iterations=1, parallel_iterations=1,
    loop_vars=(tf.zeros((), dtype=tf.int32),),
    back_prop=False)
```"
22218,Tensorflow hangs when running `broadcast_to` op,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: ---
- **TensorFlow installed from (source or binary)**: binary via pip install tensorflow-gpu
- **TensorFlow version (use command below)**: 1.10.0
- **Python version**: 3.6.6
- **Bazel version (if compiling from source)**: ---
- **GCC/Compiler version (if compiling from source)**: ---
- **CUDA/cuDNN version**: CUDA-9.0 cuDNN-x64-v7.1
- **GPU model and memory**: NVIDIA GeForce GTX 1080, 8GB
- **Exact command to reproduce**:

```
import tensorflow as tf

print(""Version"", tf.GIT_VERSION, tf.VERSION)

with tf.Graph().as_default(), tf.Session() as sess:
    
    zero = tf.constant(0.0, dtype=tf.float32)
    
    dynamic_tensor = tf.placeholder(name=""dynamic_tensor"", shape=[None], dtype=tf.bool)
    
    dynamic_shape = tf.shape(dynamic_tensor)
    
    broadcast_zeroes = tf.broadcast_to(input=zero, shape=dynamic_shape)
    
    example_feed = {dynamic_tensor: [True, False]}
    
    print(""Inputs:"", sess.run([zero, dynamic_tensor, dynamic_shape], feed_dict=example_feed))
    
    print(""Broadcast:"", sess.run(broadcast_zeroes, feed_dict=example_feed))
    
    print(""Never reached"")
```
Outputs:
```
Version 1.10.0 b'v1.10.0-rc1-19-g656e7a2b34' 1.10.0
Inputs: [0.0, array([ True, False]), array([2])]
```

### Describe the problem
Tensowflow is getting stuck (hangs without any error) trying to broadcast a dynamic shape. Changing to a static `shape=[2]` produces the expected result:

```
Version b'v1.10.0-rc1-19-g656e7a2b34' 1.10.0
Inputs: [0.0, array([ True, False]), array([2])]
Broadcast: [0. 0.]
Never reached
```"
22217,Session run options for configuring the timeout of a run is not working as expected,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: no
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.9.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.2/
- **GPU model and memory**: V100 and 16GB
- **Exact command to reproduce**:

== cat /etc/issue ===============================================
Linux ip-172-31-35-59 4.4.0-1062-aws #71-Ubuntu SMP Fri Jun 15 10:07:39 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.4 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux ip-172-31-35-59 4.4.0-1062-aws #71-Ubuntu SMP Fri Jun 15 10:07:39 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy               1.14.5
protobuf            3.6.0
tensorflow          1.9.0

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.9.0
tf.GIT_VERSION = v1.9.0-0-g25c197e023
tf.COMPILER_VERSION = v1.9.0-0-g25c197e023
Sanity check: array([1], dtype=int32)
/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters

== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda-9.0/lib64:/usr/local/cuda-9.0/extras/CUPTI/lib64:/lib/nccl/cuda-9.0/lib:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Tue Sep 11 19:14:44 2018
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 396.37                 Driver Version: 396.37                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |
| N/A   39C    P0    48W / 300W |     40MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      7354      C   nvidia-cuda-mps-server                        30MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-9.1/doc/man/man7/libcudart.7
/usr/local/cuda-9.1/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.1/lib64/libcudart.so.9.1.85
/usr/local/cuda-9.1/lib64/libcudart_static.a
/usr/local/cuda-9.2/doc/man/man7/libcudart.7
/usr/local/cuda-9.2/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.2/lib64/libcudart.so.9.2.88
/usr/local/cuda-9.2/lib64/libcudart_static.a
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/lib64/libcudart_static.a
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61
/usr/local/cuda-9.0/doc/man/man7/libcudart.7
/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.0/lib64/libcudart.so.9.0.176
/usr/local/cuda-9.0/lib64/libcudart_static.a

--------

### Describe the problem
The timeout_in_ms for session run_options seems to be not working as expected.

### Source code / logs
```python
import tensorflow as tf

i = tf.constant(0)
c = lambda i: True
b = lambda i: tf.add(i, 1)

print(""Testing timeout"")

r = tf.while_loop(c, b, [i])
with tf.Session() as sess:
        sess.run(r, options=tf.RunOptions(timeout_in_ms=3000))

```

The above code goes into an infinite loop and the timeout_in_ms is not taking effect."
22216,tensorflow.python.framework.errors_impl.NotFoundError: No registered 'Empty' OpKernel for XLA_CPU devices compatible with node Empty,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Centos 7.4.1708
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.10
- **Python version**: 3.4.5
- **Bazel version (if compiling from source)**: 0.16
- **GCC/Compiler version (if compiling from source)**: 4.8.5
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: NA


### Describe the problem
I am trying to run a char_rnn model on a TPU. To do so I am using 
tf.contrib.recurrent.functional_rnn. It runs successfully on CPU. When I change it to XLA_CPU I am running into the error mentioned in the title. I built from source, and the XLA_CPU works well for my other models. 
```
tensorflow.python.framework.errors_impl.NotFoundError: No registered 'Empty' OpKernel for XLA_CPU devices compatible with node Empty = Empty[dtype=DT_FLOAT, init=true, _device=""/job:localhost/replica:0/task:0/device:XLA_CPU:0""](concat)
	.  Registered:  device='CPU'; dtype in [DT_UINT8]
  device='CPU'; dtype in [DT_BOOL]
  device='CPU'; dtype in [DT_INT64]
  device='CPU'; dtype in [DT_INT32]
  device='CPU'; dtype in [DT_STRING]
  device='CPU'; dtype in [DT_HALF]
  device='CPU'; dtype in [DT_DOUBLE]
  device='CPU'; dtype in [DT_FLOAT]

	 [[Node: Empty = Empty[dtype=DT_FLOAT, init=true, _device=""/job:localhost/replica:0/task:0/device:XLA_CPU:0""](concat)]]
	 [[Node: test_conv/Forward_eNPkQVpjczA = Forward_eNPkQVpjczA[_device=""/job:localhost/replica:0/task:0/device:XLA_CPU:0""](cluster_30/_6/_7, cluster_31/_2/_3, cluster_28/_10/_11, cluster_28/_10/_11, cluster_26/_14/_15, cluster_29/_8/_9/_69)]]
	 [[Node: cluster_27/_12/_13/_74 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:XLA_CPU:0"", send_device_incarnation=1, tensor_name=""edge_68_cluster_27/_12/_13"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]
````
"
22215,How to get max and min x and y of bounding boxes from Object_detection_image.py?,"this is the link of the image:
https://ibb.co/nbLjc9

After I print the ""boxes"" variable, it gives an array of numbers.

When I change the values inside, the bounding box changes too.

So, I am pretty sure those are the values will give different size and location of bounding boxes.

However, how can I get the max and min bounding box from this array?

like max y = 1920, max x = 1080, min y = 0, min x = 0.

Thank you. "
22214,C++ API: tensorflow::ops::Mul causes read access violation when second attribute is int constant,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, see below
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: C++ API compiled from source
- **TensorFlow version (use command below)**: 1.10.0
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: Compiled via CMake
- **GCC/Compiler version (if compiling from source)**: VS 2015
- **CUDA/cuDNN version**: 9/7.0.5
- **GPU model and memory**: GTX 1080, 8GBs
- **Exact command to reproduce**: run the script

### Describe the problem
It seems `tensorflow::ops::Mul` accepts arguments of different type ad then crashes during `Run()`.

Take this sample program:

```
// dunno if all are needed, some may be redundant for this small sample
#include <tensorflow/cc/ops/const_op.h>
#include <tensorflow/cc/ops/image_ops.h>
#include <tensorflow/cc/ops/standard_ops.h>
#include <tensorflow/core/public/session.h>
#include <tensorflow/core/platform/init_main.h>
#include <tensorflow/cc/client/client_session.h>

int main(int argc, char* argv[])
{
    tensorflow::port::InitMain(argv[0], &argc, &argv);
    auto root = tensorflow::Scope::NewRootScope();
    auto input = tensorflow::ops::Placeholder(root, tensorflow::DataType::DT_UINT8);
    auto id = tensorflow::ops::Identity(root, input); // just to check we get this far before the error occurs
    auto to_float = tensorflow::ops::Cast(root, id, tensorflow::DataType::DT_FLOAT);
    auto mul = tensorflow::ops::Mul(root, to_float, { -1 }); // <<<<<<<<<<<<<<<<<

    tensorflow::ClientSession session(root);

    // Run net
    std::vector<tensorflow::Tensor> outputs;
    tensorflow::Status run_status = session.Run({ {input, {(uint8_t)42, (uint8_t)35}} }, { id, mul }, &outputs);
    std::cout << ""Run status: "" << run_status << std::endl;
    std::cout << outputs[0].DebugString() << std::endl;
    std::cout << outputs[1].DebugString() << std::endl;

    return 0;
}
```

It compiles fine but it will throw an ""Access Violation reading location 0x0000000000000050"" when executing `Run()`.

Changing the marked line to
`auto mul = tensorflow::ops::Mul(root, to_float, { -1.0f });`
(i.e., just changing the numerical constant in the second argument from int to float) fixes the error and produces the proper output.

I'm not sure whether this is a bug or my error for passing two tensors with different types, but either way IMHO this should probably emit a warning at graph construction time (or prevent compilation altogether, which would be the better solution).
"
22213,"how to fix  ""tf.nightly-gpu"" caused ""nan"" problem","
### System information
Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 1.11.0 (use tf-night-gpu)
Python version:2.7
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 9.0 /7.1
GPU model and memory:
Exact command to reproduce:


### Describe the problem
I used `tf-night-gpu` in my project code, but it will be made ""nan"" at the next batch train.
it only the  `tf-night-gpu` will happen, it is can work in tensorflow1.10.


### Source code / logs
the minimal code at here , it is just part of my project code. the input is word_index.
the batch size = 64
use tf-nightly-gpu.
platform = jupyter notebook.
in first batch size was worked, but in next batch train will be ""nan"". but this problem only happen in ""tf-nightly-gpu""
```

class  RNN_Decoder(tf.keras.Model):
    def __init__(self, embedding_dim, units, vocab_size):
        super(RNN_Decoder, self).__init__()
        self.units = units
        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
    def call(self, x, features, hidden):
        print('embedding_input:  ', x)
        x = self.embedding(x)
        print('embedding_output:  ', x)

decoder = RNN_Decoder(embedding_dim, units, vocab_size)

for epoch in range(20):
    hidden = decoder.reset_state(batch_size=64)
    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)
        with tf.GradientTape() as tape:
            features = encoder(img_tensor)
            for i in range(1, target.shape[1])
            predictions, hidden, _ = decoder(dec_input, features, hidden)
```

---- first batch train input and output------
```
('embedding_input ', <tf.Tensor: id=4586198, shape=(64, 1), dtype=int32, numpy=
array([[3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3]], dtype=int32)>)
('Embedding_output:  ', <tf.Tensor: id=4586379, shape=(64, 1, 256), dtype=float32, numpy=
array([[[ 0.01097491, -0.02041122,  0.04823625, ..., -0.0155656 ,
          0.01521399, -0.0496642 ]],

       [[ 0.01097491, -0.02041122,  0.04823625, ..., -0.0155656 ,
          0.01521399, -0.0496642 ]],

       [[ 0.01097491, -0.02041122,  0.04823625, ..., -0.0155656 ,
          0.01521399, -0.0496642 ]],

       ...,

       [[ 0.01097491, -0.02041122,  0.04823625, ..., -0.0155656 ,
          0.01521399, -0.0496642 ]],

       [[ 0.01097491, -0.02041122,  0.04823625, ..., -0.0155656 ,
          0.01521399, -0.0496642 ]],

       [[ 0.01097491, -0.02041122,  0.04823625, ..., -0.0155656 ,
          0.01521399, -0.0496642 ]]], dtype=float32)>)
```

the next batch train , have same keras.layers.Embedding_input:

```
`Epoch 1 Batch 0 Loss 2.0415
(''keras.layers.Embedding_input:: ', <tf.Tensor: id=4607289, shape=(64, 1), dtype=int32, numpy=
array([[3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3]], dtype=int32)>)
(''keras.layers.Embedding_output::  ', <tf.Tensor: id=4607374, shape=(64, 1, 256), dtype=float32, numpy=
array([[[nan, nan, nan, ..., nan, nan, nan]],

       [[nan, nan, nan, ..., nan, nan, nan]],

       [[nan, nan, nan, ..., nan, nan, nan]],


       [[nan, nan, nan, ..., nan, nan, nan]],

       [[nan, nan, nan, ..., nan, nan, nan]],

       [[nan, nan, nan, ..., nan, nan, nan]]], dtype=float32)>)`
```"
22212,Tensorflow with MKL Very Slow,"I am trying to reduce the time taken by an LSTM model using MKL. I use Tensorflow 1.10.0. I have give information about configuration I am using below. It will be great, if you can help me improve the speed of my model. Please let me know what more information you need. Thanks a lot!

CPU make and model (try lscpu; if your lscpu does not list CPU flags,
try running cat /proc/cpuinfo | grep flags | sort -u)
flags	: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq monitor est ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single pti fsgsbase bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx xsaveopt ida

OS version (uname -a)
Linux ip-172-31-24-72 4.14.67-66.56.amzn1.x86_64 #1 SMP Tue Sep 4 22:03:21 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

Compiler version (gcc --version)
gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11)

os.environ['OMP_NUM_THREADS'] = '32'
os.environ['KMP_BLOCKTIME'] = '0'
os.environ['KMP_AFFINITY'] = 'granularity=fine,verbose,compact,1,0'
K.set_image_data_format('channels_first')
config = tf.ConfigProto()
config.inter_op_parallelism_threads = 1
config.intra_op_parallelism_threads = 32
session = tf.Session(config=config)
K.set_session(session)

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes

- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
No

- **TensorFlow installed from (source or binary)**:
Source

- **TensorFlow version (use command below)**:
1.10.0

- **Python version**:
2.7

- **Bazel version (if compiling from source)**:
https://copr.fedorainfracloud.org/coprs/vbatts/bazel/repo/epel-7/vbatts-bazel-epel-7.repo

- **CUDA/cuDNN version**:
None

- **GPU model and memory**:
None"
22211,The latest process of building tensorflow serving,"
Hi, friends, can someone tell me the latest process of building tensorflow serving? I have tried the methods provided by the official documentation, but it failed, I need your help."
22209,Does it support Android Neural Networks API?,"Have I written custom code N/A
OS Platform and Distribution Ubuntu 18.04
TensorFlow installed from anaconda
TensorFlow version 1.3.0
Bazel version N/A
CUDA/cuDNN version N/A
GPU model and memory N/A
Exact command to reproduce N/A
Mobile devic Pixel 2"
22208,LookupError: No gradient defined for operation type: ResizeNearestNeighborGrad or ResizeBilinearGrad,"### System information:
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version**:1.10.0
- **Python version**:2.7.x
- **CUDA/cuDNN version**:9.0.x
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Nope
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:N/
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**:N/A
- **GPU model and memory**: Geforce 940mx
- **Exact command to reproduce**: N/A

### Describe the problem
I want to the gradient of any of [these image resizing operations](https://www.tensorflow.org/api_docs/python/tf/image/ResizeMethod) inside the neural network. But seems to be any of these does not define the gradient ops for them. I see there is a [similar issue ](https://github.com/tensorflow/tensorflow/issues/7641) which has not been answered yet. I also asked in the [SO](https://stackoverflow.com/questions/52264891/lookuperror-no-gradient-defined-for-operation-type-resizenearestneighborgrad) but still, I did not get an answer for this. 

### Source code / logs

      
		differences = tf.subtract(images_fake, images_real)
		alpha_shape = [params.batch_size] + [1] * (differences.shape.ndims - 1)
		alpha = tf.random_uniform(shape=alpha_shape, minval=0., maxval=1.)
		interpolates = images_real + (alpha * differences)
		d_model = Model(params, args.mode, interpolates, reuse_variables, images_fake, 1)
		gradients = tf.gradients(d_model.logistic_linear, [interpolates])[0]
		slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[1]))
		gradient_penalty = tf.reduce_mean((slopes - 1.) ** 2)
		_gradient_penalty = 10 * gradient_penalty

Inside my model, it uses [ tf.image.resize_bicubic()](https://www.tensorflow.org/api_docs/python/tf/image/resize_bicubic). But I see there are some [exisiting implementation](https://github.com/tensorflow/tensorflow/commit/3331c574bcfd85787d7a4f3d1b1b139239a6595b) for this. But not sure how those things get it them working. Please let me know how this can be achieved. These image resizing cannot be done as preprocessing it needs to be done inside a model. Any help would be really appreciated. "
22207,Example in Keras guide does not work when training with tf.data datasets,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:na
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.11.0-dev20180907
- **Python version**:v3.6.3:2c5fed8
- **Bazel version (if compiling from source)**:na
- **GCC/Compiler version (if compiling from source)**:na
- **CUDA/cuDNN version**:na
- **GPU model and memory**:na
- **Exact command to reproduce**:na

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

When going through the tutorial in https://www.tensorflow.org/guide/keras, training with numpy arrays works, but training with tf.data datasets only works if you first perform training with numpy arrays. I expected that you could train with tf.data datasets without first going through numpy arrays. This issue was first identified in #20827 but it was closed as solved at the moment, for some nightly build over tensorflow v1.9.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

The code below works if one uncomments the line `#model.fit(data, labels, epochs=10, batch_size=32)`. If it runs as it is, it will give an error, showed below.

Code:

```
import tensorflow as tf
from tensorflow import keras

model = keras.Sequential()
# Adds a densely-connected layer with 64 units to the model:
model.add(keras.layers.Dense(64, activation='relu'))
# Add another:
model.add(keras.layers.Dense(64, activation='relu'))
# Add a softmax layer with 10 output units:
model.add(keras.layers.Dense(10, activation='softmax'))

model.compile(optimizer=tf.train.AdamOptimizer(0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Configure a model for categorical classification.
model.compile(optimizer=tf.train.RMSPropOptimizer(0.01),
              loss=keras.losses.categorical_crossentropy,
              metrics=[keras.metrics.categorical_accuracy])

import numpy as np

data = np.random.random((1000, 32))
labels = np.random.random((1000, 10))

#model.fit(data, labels, epochs=10, batch_size=32)

# Instantiates a toy dataset instance:
dataset = tf.data.Dataset.from_tensor_slices((data, labels))
dataset = dataset.batch(32)
dataset = dataset.repeat()

# Don't forget to specify `steps_per_epoch` when calling `fit` on a dataset.
model.fit(dataset, epochs=10, steps_per_epoch=30)
```

Error:

```
Traceback (most recent call last):
  File ""...\tensorflow\python\framework\op_def_library.py"", line 510, in _apply_op_helper
    preferred_dtype=default_dtype)
  File ""...\tensorflow\python\framework\ops.py"", line 1145, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""...\tensorflow\python\ops\variables.py"", line 799, in _TensorConversionFunction
    ""of type '%s'"" % (dtype.name, v.dtype.name))
ValueError: Incompatible type conversion requested to type 'float64' for variable of type 'float32'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""...\temp.py"", line 34, in <module>
    model.fit(dataset, epochs=10, steps_per_epoch=30)
  File ""...\tensorflow\python\keras\engine\training.py"", line 1440, in fit
    validation_split=validation_split)
  File ""...\tensorflow\python\keras\engine\training.py"", line 944, in _standardize_user_data
    class_weight, batch_size)
  File ""...\tensorflow\python\keras\engine\training.py"", line 984, in _standardize_weights
    self._set_inputs(x)
  File ""...\tensorflow\python\training\checkpointable\base.py"", line 426, in _method_wrapper
    method(self, *args, **kwargs)
  File ""...\tensorflow\python\keras\engine\training.py"", line 1198, in _set_inputs
    self._symbolic_set_inputs(inputs, training=training)
  File ""...\tensorflow\python\training\checkpointable\base.py"", line 426, in _method_wrapper
    method(self, *args, **kwargs)
  File ""...\tensorflow\python\keras\engine\training.py"", line 1282, in _symbolic_set_inputs
    outputs = self.call(dummy_input_values, training=training)
  File ""...\tensorflow\python\keras\engine\sequential.py"", line 232, in call
    inputs, training=training, mask=mask)
  File ""...\tensorflow\python\keras\engine\sequential.py"", line 250, in _call_and_compute_mask
    x = layer.call(x, **kwargs)
  File ""...\tensorflow\python\keras\layers\core.py"", line 947, in call
    outputs = gen_math_ops.mat_mul(inputs, self.kernel)
  File ""...\tensorflow\python\ops\gen_math_ops.py"", line 4856, in mat_mul
    name=name)
  File ""...\tensorflow\python\framework\op_def_library.py"", line 546, in _apply_op_helper
    inferred_from[input_arg.type_attr]))
TypeError: Input 'b' of 'MatMul' Op has type float32 that does not match type float64 of argument 'a'.

Process finished with exit code 1
```
"
22205,Python Tensorflow1.10.0 Pooling Invalid,"  W1 = parameters[""W1""]
    W2 = parameters[""W2""]

    # 
    Z1 = tf.nn.conv2d(X, W1, strides=[1, 1, 1, 1], padding='SAME')
    A1 = tf.nn.relu(Z1)
    # 
    P1 = tf.nn.max_pool(A1, ksize=[1, 8, 8, 1], strides=[1, 8, 8, 1], padding='VALID')  # ,padding

    Z2 = tf.nn.conv2d(P1, W2, strides=[1, 1, 1, 1], padding=""SAME"")
    A2 = tf.nn.relu(Z2)
    P2 = tf.nn.max_pool(A2, ksize=[1, 4, 4, 1], strides=[1, 4, 4, 1], padding='SAME')


##################
tf.nn.max_pool() padding specify SAME is valid,I think this is a very important  problem"
22204,Java API support from Gradle,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Only specific dependency  in vanila Gradle**W** project from maven central:
```
plugins {
    id 'java'
}

group 'tf-test'
version '1.0-SNAPSHOT'

sourceCompatibility = 1.8

repositories {
    mavenCentral()
}

dependencies {
    compile group: 'org.tensorflow', name: 'tensorflow', version: '1.10.1'
    testCompile group: 'junit', name: 'junit', version: '4.12'
}
```
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Darwin Kernel Version 17.7.0

- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
N/A

- **TensorFlow installed from (source or binary)**:
binary

- **TensorFlow version (use command below)**:
1.10.1

- **Python version**:
Python 3.6.5 :: Anaconda, Inc.

- **Bazel version (if compiling from source)**:
N/A

- **GCC/Compiler version (if compiling from source)**:
N/A

- **CUDA/cuDNN version**:
N/A

- **GPU model and memory**:
N/A

- **Exact command to reproduce**:
```
./gradlew clean build
```

You can collect some of this information using our environment capture script:
N/A

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
Artifact described in the documentation not available and/or not retrievable using Gradle wrapper v4.8.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
Could not resolve all files for configuration ':compileClasspath'.
> Could not find org.tensorflow:tensorflow:1.10.1.
  Searched in the following locations:
    - https://repo.maven.apache.org/maven2/org/tensorflow/tensorflow/1.10.1/tensorflow-1.10.1.pom
    - https://repo.maven.apache.org/maven2/org/tensorflow/tensorflow/1.10.1/tensorflow-1.10.1.jar
  Required by:
      project :

* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.
```
"
22202,Java version for GPU at Maven Central is built for CUDA 0.9 (==a bit too old),"I have **libcublas.so.9.2** installed.

Please, update. And make it somewhat usable if possible.
Many thanks.

pom.xml:
```
    <dependency>
        <groupId>org.tensorflow</groupId>
        <artifactId>libtensorflow</artifactId>
        <version>1.10.0</version>
    </dependency>
    <dependency>
         <groupId>org.tensorflow</groupId>
         <artifactId>libtensorflow_jni_gpu</artifactId>
         <version>1.10.0</version>
    </dependency>
```
```
Exception in thread ""main"" java.lang.UnsatisfiedLinkError: /tmp/tensorflow_native_libraries-1536621740227-0/libtensorflow_jni.so: libcublas.so.9.0: cannot open shared object file: No such file or directory
	at java.lang.ClassLoader$NativeLibrary.load(Native Method)
	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941)
	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824)
	at java.lang.Runtime.load0(Runtime.java:809)
	at java.lang.System.load(System.java:1086)
	at org.tensorflow.NativeLibrary.load(NativeLibrary.java:101)
	at org.tensorflow.TensorFlow.init(TensorFlow.java:66)
	at org.tensorflow.TensorFlow.<clinit>(TensorFlow.java:70)
	at org.tensorflow.Graph.<clinit>(Graph.java:337)
	at TensorflowExamplePlugin.main(TensorflowExamplePlugin.java:25)
	at TensorflowExamplePlugin.run(TensorflowExamplePlugin.java:17)
```"
22201,Fall 2018 Symposium Performance Talk Notes,"There were additional topics discussed that could not be turned into coherent notes after the fact.

## Action items
   * TensorFlow team to create a block diagram and then go deeper.  Goal is to help people who want to contribute plugins or extensions. [#22356](https://github.com/tensorflow/tensorflow/issues/22356)
   * Make the NVIDIA Library to TF Version matrix more visible and consider better error messages. [#22357](https://github.com/tensorflow/tensorflow/issues/22357)
   * Document tips and methods to use for large batch scaling with TensorFlow.  Possibly mention where this is proven and unproven to work.  This would include adding optimizers or wrapper to simplify usage.  Researchers are large labs want to scale but do not have the knowledge to do it quickly. [#22358](https://github.com/tensorflow/tensorflow/issues/22358)

## General Questions and discussion results

   * Should TensorFlow default builds move forward more aggressively with newer versions of CUDA/cuDNN?
      * Internally Google moves slowly to new versions of CUDA as the verification process is long and often includes multiple patches with NVIDIA until all edge cases are covered.  The bar is extremely high for obvious reasons.
      * Group thought was it doesnt matter. Situation would be improved with better error messages indicating what version is needed and making the matrix showing what NVIDIA libs are needed for each TensorFlow version.
   * Provide more models with good performance and accuracy in github.com/tensorflow/models.  Too many different versions and unsure which one to use.  Example:  There are 3+ ResNet models.
      * Contact tfboyd@ or comment in this thread if there is a specific model of interest and why.  MLPerf may improve this situation.
   * Auto regressive models with long dependency chains are hard to optimize by doing fusion by hand.
      * Consider trying XLA and providing feedback. An objective of XLA is to reduce the need for custom fusion.
   * [Ask] Distributing the input pipeline across servers.  Some work has been done and rumored to have occured in production but not trivial to setup.  Consider an RFP and/or reach out to the tf.data team.
   * TensorCores are only used if you are using tf.16.  You can do Pseudo FP16 with an envar that should be in TF 1.11 from NVIDIA as an experimental feature.  You still need to do loss scaling.

## Debugging and performance investigations
   * Hard to get timelines and profiler results.
      * New guide is on the way, not positive this will make it as easy as desired.
   * [Ask] Provide an interface to autotuning parameters such as intra and inter thread pools.
      * In hindsight, more information is needed to make this actionable.  Please add info to the comments.  

## Distributed compute (multi-gpu and multi-node)
   * All reduce API for multi-node and support MPI.
      * MPI would be useful for supercomputers where you need to use their MPI library to get good communication.
      * NCCL currently used by MirroredStrategy but only for multi-GPU.  TensorFlows own `ops` are used for multi-node all-reduce.

   * Document on how the distribution strategies is setup with the goal of showing how others can add their own solution and collective ops.  The API is the best place right now to figure this out.
   * [AI] TensorFlow team to create a block diagram and then go deeper.  Goal is to help people who want to contribute plugins or extensions.
   * Having a hard time with MPI.  People are using a newer versions of MPI
   * How do we plan to support fault tolerance? Will it allow dynamic addition and removal of machines to the collective. E.g. preemption but keep training with a smaller pool. Also increasing machines & batch size as training progresses. Need to adjust the learning rate, so maybe get a callback, especially when you are going to lose or have just lost a machine. Response to call back is starting with revised set of machines an new learning rate.
   *[AI] Document strategies for large batch training.  LARS looking to automatic learning rate, warmup rate and stuff.  

## Model parallelism and Data parallelism
   * Model parallelism is currently manual.  Work to automate it is on going and it can be phrased as an RL problem.  
   * Data parallelism supported via MirroredStrategy discussed today
"
22200,Image retraining tutorial (label_image.py issue),"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:pip
- **TensorFlow version (use command below)**:1.5.0
- **Python version**:2.7
- **Bazel version (if compiling from source)**:-
- **GCC/Compiler version (if compiling from source)**:-
- **CUDA/cuDNN version**:-
- **GPU model and memory**:no dedicated GPU
- **Exact command to reproduce**: python label_images.py --graph=/tmp/output_graph.pb --labels=/tmp/output_labels.txt --input_layer=Placeholder --output_layer=final_result --input_height=128 --input_width=128 --image=flower.png

### Describe the problem


Running the tutorial script label_image.py results in crashing (Container localhost does not exist)
I used mobilenet_v2 instead of inception_v3 during retraining


### Source code / log
Caused by op 

'import/module_apply_default/MobilenetV2/expanded_conv_13/depthwise/depthwise/ReadVariableOp', defined at:
  File ""label_images.py"", line 118, in <module>
    graph = load_graph(model_file)
  File ""label_images.py"", line 33, in load_graph
    tf.import_graph_def(graph_def)
  File ""/home/john/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 316, in new_func
    return func(*args, **kwargs)
  File ""/home/john/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 554, in import_graph_def
    op_def=op_def)
  File ""/home/john/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3160, in create_op
    op_def=op_def)
  File ""/home/john/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access


NotFoundError (see above for traceback): Error while reading resource variable module/MobilenetV2/expanded_conv_13/depthwise/depthwise_weights from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/module/MobilenetV2/expanded_conv_13/depthwise/depthwise_weights)
	 [[Node: import/module_apply_default/MobilenetV2/expanded_conv_13/depthwise/depthwise/ReadVariableOp = ReadVariableOp[dtype=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](import/module/MobilenetV2/expanded_conv_13/depthwise/depthwise_weights)]]

"
22197,KeyError u'ImageProjectiveTransform' when loading Tensorflow model,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA
- **TensorFlow installed from (source or binary)**: using pip
- **TensorFlow version (use command below)**:1.10.0
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: 
- **GPU model and memory**: Tesla K80 on Google Colab
- **Exact command to reproduce**: 

### Describe the problem

I trained a ConvNet on MNIST, and saved the model using simple_save.

When loading the saved model, using 
`tf.saved_model.loader.load(sess,[tag_constants.SERVING],'/save_folder')`

I get this error `KeyError: u'ImageProjectiveTransform'`

Code for saving my ConvNet model

```
graph = tf.get_default_graph()

from tensorflow.python.saved_model import tag_constants
with graph.as_default():
  input_dict = {""x"":x,""y"":y,""keep_prob"":keep_prob,""aug_img"":aug_img}
  output_dict = {""logits"":fc3}
  tf.saved_model.simple_save(sess,'/saved_model/',input_dict,output_dict)
```

I used tf.contrib.image.rotate as a augmentation technique, but didn't add that to any of the dicts while saving the model.

### Source code / logs

This is the code for the model and the training part
```
x = tf.placeholder(tf.float32,shape=[None,28,28])
y = tf.placeholder(tf.int64,shape=[None])
keep_prob = tf.placeholder(tf.float32,shape=())
lr = tf.placeholder(tf.float32,shape=())

aug_img = tf.placeholder(tf.float32,shape=[28,28])

img_rotate = tf.contrib.image.rotate(aug_img,-0.5+np.random.random(),interpolation='BILINEAR')
img_affine = tf.contrib.image.transform(aug_img,[np.random.random() for i in xrange(8)])

with tf.device('/gpu:0'):
  conv1 = tf.layers.conv2d(tf.reshape(x,shape=[-1,28,28,1]),filters=64,kernel_size=3,strides=1,padding='same',activation=tf.nn.leaky_relu)
  conv2 = tf.layers.conv2d(conv1,filters=64,kernel_size=3,strides=2,padding='same',activation=tf.nn.leaky_relu) #NX14X14
  conv3 = tf.layers.conv2d(conv2,filters=128,kernel_size=3,strides=1,padding='same',activation=tf.nn.leaky_relu)
  conv4 = tf.layers.conv2d(conv3,filters=64,kernel_size=1,strides=1,padding='same',activation=tf.nn.leaky_relu)
  conv5 = tf.layers.conv2d(conv4,filters=128,kernel_size=3,strides=2,padding='same',activation=tf.nn.leaky_relu) #NX7X7
  conv6 = tf.layers.conv2d(conv5,filters=64,kernel_size=1,strides=1,padding='same',activation=tf.nn.leaky_relu)
  conv7 = tf.layers.conv2d(conv6,filters=128,kernel_size=3,strides=2,padding='same',activation=tf.nn.leaky_relu) #NX3X3
  conv8 = tf.layers.conv2d(conv7,filters=128,kernel_size=3,strides=1,padding='same',activation=tf.nn.leaky_relu) #NX1X1
  flat = tf.contrib.layers.flatten(conv7)
  fc1 = tf.layers.dense(flat,units=256,activation=tf.nn.leaky_relu)
  do1 = tf.nn.dropout(fc1,keep_prob)
  fc2 = tf.layers.dense(do1,units=256,activation=tf.nn.leaky_relu)
  do2 = tf.nn.dropout(fc2,keep_prob)
  fc3 = tf.layers.dense(do2,units=10)
  
  cost = loss(fc3,y)
  opt = tf.train.AdamOptimizer(learning_rate=0.0001)
  opt_op = opt.minimize(cost)
  
  top1_acc = accuracy(fc3,y)

epochs = 25
ne=0
batchsize = 50
numiter = 400
while(ne<epochs):
    print 'Epoch:: ',ne+1,'-->'
    stime = time.time()
    if ne != 0:
       np.random.shuffle(index)
       images = images[index]
       labels = labels[index]
    for niter in xrange(numiter):
      offset = niter*batch_size
      x_iter, y_iter = np.array(images[offset:offset+batch_size,:,:]), np.array(labels[offset:offset+batch_size])
      for n in xrange(batch_size):
        augs = np.random.choice([True,False])
        if augs==True:
          x_iter[n] = sess.run(img_rotate,feed_dict={aug_img:x_iter[n]})
      feed_trdict={x:x_iter,y:y_iter,keep_prob:0.6}
      sess.run(opt_op,feed_dict=feed_trdict)
    ne+=1

```"
22193,AttributeError: module 'tensorflow.python.framework.ops' has no attribute '_TensorLike',"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: RaspberryPi stretch
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: https://www.tensorflow.org/install/install_linux
- **TensorFlow version (use command below)**: 0.11.0
- **Python version**: Python 3.5.3
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I was trying to run facenet module on raspberrypi. and in facenet.py I am getting this error.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
[facenet.zip](https://github.com/tensorflow/tensorflow/files/2367106/facenet.zip)

"
22192,Unknown shape on placeholder caused by tf.graph_util.convert_variables_to_constants,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Mac OS X 10.13.6
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
N/A
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
v1.10.0-rc1-19-g656e7a2b34 1.10.0
- **Python version**:
Python 3.6.5
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
N/A

### Describe the problem
Placeholders with shape `()` gets shape `<unknown>` after `tf.graph_util.convert_variables_to_constants` has been run on a graph def.

The expected result is that the shape is kept, as it is for tensors with higher rank.

### Source code / logs
```python
import tensorflow as tf

with tf.Graph().as_default(), tf.Session() as sess:
    placeholder = tf.placeholder(tf.float32, shape=(), name='placeholder')
    output = tf.identity(placeholder)

    graph_def = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def, [output.op.name])
    print('Before:', placeholder.shape)


with tf.Graph().as_default() as g, tf.Session() as sess:
    tf.import_graph_def(graph_def, name='')
    placeholder = g.get_tensor_by_name('placeholder:0')
    print('After:', placeholder.shape)
```
Output

    Before: ()
    After: <unknown>
"
22191,toco: error: one of the arguments --graph_def_file --saved_model_dir --keras_model_file is required,"I'm trying to convert my tensorflow model to .tflite following the devguide on the official tensorflowlite site. I got the following error: toco: error: one of the arguments --graph_def_file --saved_model_dir --keras_model_file is required
Thanks in advance for your help.

Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: binary (pip install through anaconda)
- **TensorFlow version (use command below)**: 1.8
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22190,metrics=['accuracy'] seems to be calculated differently if one uses tf.data inputs instead of numpy arrays for keras model,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: **YES**
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: **Windows 10**
- **Mobile device**:na
- **TensorFlow installed from (source or binary)**: **binary**
- **TensorFlow version (use command below)**:**1.11.0-dev20180907**
- **Python version**:**3.6.3**
- **Bazel version (if compiling from source)**:na
- **GCC/Compiler version (if compiling from source)**:na
- **CUDA/cuDNN version**:na
- **GPU model and memory**:na
- **Exact command to reproduce**:na

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Given the same piece of code for loading mnist data and training a keras model in tensorflow, the metric ""accuracy"" given as argument to keras_model.compile(metrics=[...]) generates very different values (order of 0.10 versus order of 0.90) depending on if you use numpy arrays or tf.data datasets as training inputs. Note that the values of the loss in each case are very close. I suspect that ""accuracy"" is being calculated differently depending on the type of input (numpy or tf.data), or that it is being calculated wrong in one of the cases.
In particular, as an example, using numpy arrays as input, one can get the pair loss: 0.2086 - acc: 0.9389 in one of the steps, while the same loss in with tf.data gives the pair loss: 0.2086 - acc: 0.1024.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

The code below as it is can be run and training with tf.data datasets will be performed. If you comment the block between `#Train with tf.data datasets` and `########################` and uncomment the block between `#Train with numpy arrays` and `########################`, training with numpy arrays as inputs will be performed.

```
import tensorflow as tf
import numpy as np

np.random.seed(1)
tf.set_random_seed(1)
BATCH_SIZE = 32

#Import mnist dataset as numpy arrays
(x_train, y_train), (_, _) = tf.keras.datasets.mnist.load_data()#Import
x_train = x_train / 255.0 #normalizing
y_train = y_train.astype(dtype='float32')
x_train = x_train.astype(dtype='float32')

x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1]*x_train.shape[2]))#Reshaping the 2D picture

##############################################################################################
#THIS BLOCK CREATES A DATASET FROM THE NUMPY ARRAYS. IT WILL BE USED FOR THE CASE OF TF.DATA DATASET INPUTS
tfdata_dataset_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))
tfdata_dataset_train = tfdata_dataset_train.batch(BATCH_SIZE).repeat()
##############################################################################################

#Create model
keras_model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(512, activation=tf.nn.relu),
    tf.keras.layers.Dropout(0.2, seed=1),
    tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])

#Compile the model
keras_model.compile(optimizer=tf.keras.optimizers.Adam(),
                    loss=tf.keras.losses.sparse_categorical_crossentropy,
                    metrics=['accuracy'])

#Train with numpy arrays
#keras_training_history = keras_model.fit(x_train,
#                y_train,
#                epochs=1
#                )
########################

#Train with tf.data datasets
keras_training_history = keras_model.fit(tfdata_dataset_train,
                epochs=1,
                steps_per_epoch=60000//BATCH_SIZE
                )
########################
```


"
22189,Speech recognition ios example not found,"Hi TFTeam,
I want to make an application in iOS which will convert the speech to text. But unfortunately there are no examples for iOS are included in the github where there exist example for android. How could i implement the same feature in iOS platform"
22187,keras.layers.Conv3DTranspose error when reused,"Version 1.10

file: tensorflow/python/keras/layers/ convolution.py

line: 1000

problem: 

input_spec is modified every time the module is called; 
input_spec uses dynamic shape causes error in shape checker when the layer is reused
"
22186,Incorrect .runfiles folder name  building from source on Windows,"### System information

Have I written custom code: No
OS Platform and Distribution: Windows 7
TensorFlow installed from: Building from source
TensorFlow version: 1.10.1
Bazel version: 0.16.1
CUDA/cuDNN version: 9.0 / 7.2.1
GPU model and memory: nVidia GeForce GTX 660 Ti, 2 GB
Exact command to reproduce: `bazel-bin\tensorflow\tools\pip_package\build_pip_package D:\temp\tensorflow_pkg`
Mobile device: N/A

Python: 3.6

### Describe the problem
I'm building from source following these instructions: https://www.tensorflow.org/install/install_sources_windows

I've completed the step Build pip package with GPU support:
`bazel --output_user_root=D:\temp\bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`
I was running out of disk space on system drive, so I specified a different base folder via `--output_user_root=D:\temp\bazel`.

I'm stuck on the next step:
`bazel-bin\tensorflow\tools\pip_package\build_pip_package D:\temp\tensorflow_pkg`
The error reads:
`cp: cannot stat 'bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/tensorflow': No such file or directory`

Upon inspection, the folder `build_pip_package.runfiles` is called `build_pip_package.exe.runfiles` and contains single file:
![Screenshot of incorrect folder](https://i.imgur.com/x84Nifs.png ""Incorrect folder"")
![Screenshot folder with single file](https://i.imgur.com/sjMe6ZK.png ""Folder contains single file"")

My guess is that Bazel created folder based on Windows .exe file name and then could not copy the content.

### Source code / logs
Output from `bazel --output_user_root=D:\temp\bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`:
```
Target //tensorflow/tools/pip_package:build_pip_package up-to-date:
  D:/temp/bazel/ir5rkgt6/execroot/org_tensorflow/bazel-out/x64_windows-opt/bin/tensorflow/tools/pip_package/build_pip_package
  D:/temp/bazel/ir5rkgt6/execroot/org_tensorflow/bazel-out/x64_windows-opt/bin/tensorflow/tools/pip_package/build_pip_package.exe
INFO: Elapsed time: 35809,593s, Critical Path: 9300,80s
INFO: 4105 processes: 4105 local.
INFO: Build completed successfully, 4106 total actions
INFO: Build completed successfully, 4106 total actions
```

Output from `bazel-bin\tensorflow\tools\pip_package\build_pip_package D:\temp\tensorflow_pkg`:
```
Mon Sep 10 12:44:13 KST 2018 : === Preparing sources in dir: /tmp/tmp.cdKFXp2q0W

cp: cannot stat 'bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/tensorflow': No such file or directory
```"
22185,0kb,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22184,"Keras equivalent of ""class_weight"" in fit method in Tensorflow Estimator","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I want to weight down the imbalanced class documents by implementing Keras ""class_weight"" provided in fit method. Is there any equivalent of it in Tf.estimator?

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.


"
22183,""" error: no member named 'stoi' in namespace 'std' "" when bazel build //tensorflow/contrib/lite/tools/accuracy/ilsvrc:imagenet_accuracy_eval","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 
Linux Ubuntu 14.04.4
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
snapdragon SDM845
- **TensorFlow installed from (source or binary)**:
Install from binary
- **TensorFlow version (use command below)**: 
1.9.0 , CPU only
- **Python version**: 
Python 2.7.6
- **Bazel version (if compiling from source)**:
Build label: 0.16.1
- **GCC/Compiler version (if compiling from source)**: gcc version 4.8.4 
- **CUDA/cuDNN version**: 
- **GPU model and memory**:
- **Exact command to reproduce**:

**### Describe the problem**
https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/tools/accuracy  
bazel build -c opt \
  --config=android_arm \
  --config=monolithic \
  --cxxopt='--std=c++11' \
  --copt=-D__ANDROID_TYPES_FULL__ \
  --copt=-DSUPPORT_SELECTIVE_REGISTRATION \
  //tensorflow/contrib/lite/tools/accuracy/ilsvrc:imagenet_accuracy_eval 

NFO: Analysed target //tensorflow/contrib/lite/tools/accuracy/ilsvrc:imagenet_accuracy_eval (71 packages loaded).
INFO: Found 1 target...
ERROR: /local/mnt/workspace/ruipan/tf_source/tensorflow/tensorflow/contrib/lite/tools/accuracy/ilsvrc/BUILD:122:1: C++ compilation of rule '//tensorflow/contrib/lite/tools/accuracy/ilsvrc:imagenet_model_evaluator' failed (Exit 1): clang failed: error executing command
  (cd /local/mnt/workspace/ruipan/tmp/cache_bazel/_bazel_ruipan/9f5b27aadc11adc5a986b0783cdd6b20/execroot/org_tensorflow && \
  exec env - \
    ANDROID_BUILD_TOOLS_VERSION=28.0.0 \
    ANDROID_NDK_API_LEVEL=16 \
    ANDROID_NDK_HOME=/local/mnt/workspace/ruipan/android/android-ndk-r16b \
    ANDROID_SDK_API_LEVEL=28 \
    ANDROID_SDK_HOME=/local/mnt/workspace/ruipan/android/sdk \
    PATH=/local/mnt/workspace/ruipan/android/android-ndk-r16b:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/jvm/java-8-oracle/bin:/usr/lib/jvm/java-8-oracle/db/bin:/usr/lib/jvm/java-8-oracle/jre/bin:/local/mnt/workspace/ruipan/android/sdk/tools:/local/mnt/workspace/ruipan/android/sdk/platform-tools \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python \
    PYTHON_LIB_PATH=/usr/local \
    TF_DOWNLOAD_CLANG=0 \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL_SYCL=0 \
  external/androidndk/ndk/toolchains/llvm/prebuilt/linux-x86_64/bin/clang '-D__ANDROID_API__=16' -isystemexternal/androidndk/ndk/sysroot/usr/include/arm-linux-androideabi -gcc-toolchain external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64 -fpic -ffunction-sections -funwind-tables -fstack-protector-strong -Wno-invalid-command-line-argument -Wno-unused-command-line-argument -no-canonical-prefixes -fno-integrated-as -target armv7-none-linux-androideabi '-march=armv7-a' '-mfloat-abi=softfp' '-mfpu=vfpv3-d16' -mthumb -Os -g -DNDEBUG -MD -MF bazel-out/armeabi-v7a-opt/bin/tensorflow/contrib/lite/tools/accuracy/ilsvrc/_objs/imagenet_model_evaluator/imagenet_model_evaluator.d '-frandom-seed=bazel-out/armeabi-v7a-opt/bin/tensorflow/contrib/lite/tools/accuracy/ilsvrc/_objs/imagenet_model_evaluator/imagenet_model_evaluator.o' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DS_IREAD=S_IRUSR' '-DS_IWRITE=S_IWUSR' '-DS_IEXEC=S_IXUSR' -iquote . -iquote bazel-out/armeabi-v7a-opt/genfiles -iquote bazel-out/armeabi-v7a-opt/bin -iquote external/com_google_absl -iquote bazel-out/armeabi-v7a-opt/genfiles/external/com_google_absl -iquote bazel-out/armeabi-v7a-opt/bin/external/com_google_absl -iquote external/bazel_tools -iquote bazel-out/armeabi-v7a-opt/genfiles/external/bazel_tools -iquote bazel-out/armeabi-v7a-opt/bin/external/bazel_tools -iquote external/nsync -iquote bazel-out/armeabi-v7a-opt/genfiles/external/nsync -iquote bazel-out/armeabi-v7a-opt/bin/external/nsync -iquote external/protobuf_archive -iquote bazel-out/armeabi-v7a-opt/genfiles/external/protobuf_archive -iquote bazel-out/armeabi-v7a-opt/bin/external/protobuf_archive -iquote external/eigen_archive -iquote bazel-out/armeabi-v7a-opt/genfiles/external/eigen_archive -iquote bazel-out/armeabi-v7a-opt/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/armeabi-v7a-opt/genfiles/external/local_config_sycl -iquote bazel-out/armeabi-v7a-opt/bin/external/local_config_sycl -iquote external/double_conversion -iquote bazel-out/armeabi-v7a-opt/genfiles/external/double_conversion -iquote bazel-out/armeabi-v7a-opt/bin/external/double_conversion -iquote external/fft2d -iquote bazel-out/armeabi-v7a-opt/genfiles/external/fft2d -iquote bazel-out/armeabi-v7a-opt/bin/external/fft2d -iquote external/gemmlowp -iquote bazel-out/armeabi-v7a-opt/genfiles/external/gemmlowp -iquote bazel-out/armeabi-v7a-opt/bin/external/gemmlowp -iquote external/gif_archive -iquote bazel-out/armeabi-v7a-opt/genfiles/external/gif_archive -iquote bazel-out/armeabi-v7a-opt/bin/external/gif_archive -iquote external/jpeg -iquote bazel-out/armeabi-v7a-opt/genfiles/external/jpeg -iquote bazel-out/armeabi-v7a-opt/bin/external/jpeg -iquote external/png_archive -iquote bazel-out/armeabi-v7a-opt/genfiles/external/png_archive -iquote bazel-out/armeabi-v7a-opt/bin/external/png_archive -iquote external/zlib_archive -iquote bazel-out/armeabi-v7a-opt/genfiles/external/zlib_archive -iquote bazel-out/armeabi-v7a-opt/bin/external/zlib_archive -iquote external/flatbuffers -iquote bazel-out/armeabi-v7a-opt/genfiles/external/flatbuffers -iquote bazel-out/armeabi-v7a-opt/bin/external/flatbuffers -iquote external/arm_neon_2_x86_sse -iquote bazel-out/armeabi-v7a-opt/genfiles/external/arm_neon_2_x86_sse -iquote bazel-out/armeabi-v7a-opt/bin/external/arm_neon_2_x86_sse -iquote external/androidndk -iquote bazel-out/armeabi-v7a-opt/genfiles/external/androidndk -iquote bazel-out/armeabi-v7a-opt/bin/external/androidndk -iquote external/farmhash_archive -iquote bazel-out/armeabi-v7a-opt/genfiles/external/farmhash_archive -iquote bazel-out/armeabi-v7a-opt/bin/external/farmhash_archive -isystem external/nsync/public -isystem bazel-out/armeabi-v7a-opt/genfiles/external/nsync/public -isystem bazel-out/armeabi-v7a-opt/bin/external/nsync/public -isystem external/protobuf_archive/src -isystem bazel-out/armeabi-v7a-opt/genfiles/external/protobuf_archive/src -isystem bazel-out/armeabi-v7a-opt/bin/external/protobuf_archive/src -isystem external/eigen_archive -isystem bazel-out/armeabi-v7a-opt/genfiles/external/eigen_archive -isystem bazel-out/armeabi-v7a-opt/bin/external/eigen_archive -isystem external/double_conversion -isystem bazel-out/armeabi-v7a-opt/genfiles/external/double_conversion -isystem bazel-out/armeabi-v7a-opt/bin/external/double_conversion -isystem external/gif_archive/lib -isystem bazel-out/armeabi-v7a-opt/genfiles/external/gif_archive/lib -isystem bazel-out/armeabi-v7a-opt/bin/external/gif_archive/lib -isystem external/png_archive -isystem bazel-out/armeabi-v7a-opt/genfiles/external/png_archive -isystem bazel-out/armeabi-v7a-opt/bin/external/png_archive -isystem external/zlib_archive -isystem bazel-out/armeabi-v7a-opt/genfiles/external/zlib_archive -isystem bazel-out/armeabi-v7a-opt/bin/external/zlib_archive -isystem tensorflow/contrib/lite/schema -isystem bazel-out/armeabi-v7a-opt/genfiles/tensorflow/contrib/lite/schema -isystem bazel-out/armeabi-v7a-opt/bin/tensorflow/contrib/lite/schema -isystem external/flatbuffers/include -isystem bazel-out/armeabi-v7a-opt/genfiles/external/flatbuffers/include -isystem bazel-out/armeabi-v7a-opt/bin/external/flatbuffers/include -isystem external/farmhash_archive/src -isystem bazel-out/armeabi-v7a-opt/genfiles/external/farmhash_archive/src -isystem bazel-out/armeabi-v7a-opt/bin/external/farmhash_archive/src -D__ANDROID_TYPES_FULL__ -DSUPPORT_SELECTIVE_REGISTRATION '--std=c++11' -DFARMHASH_NO_CXX_STRING '-mfpu=neon' '-mfloat-abi=softfp' '-std=c++11' -O3 -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK '--sysroot=external/androidndk/ndk/platforms/android-16/arch-arm' -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/backward -isystemexternal/androidndk/ndk/sysroot/usr/include -c tensorflow/contrib/lite/tools/accuracy/ilsvrc/imagenet_model_evaluator.cc -o bazel-out/armeabi-v7a-opt/bin/tensorflow/contrib/lite/tools/accuracy/ilsvrc/_objs/imagenet_model_evaluator/imagenet_model_evaluator.o)
tensorflow/contrib/lite/tools/accuracy/ilsvrc/imagenet_model_evaluator.cc:240:56: error: no member named 'stoi' in namespace 'std'
                   [](const string& val) { return std::stoi(val) - 1; });
                                                  ~~~~~^
1 error generated.
Target //tensorflow/contrib/lite/tools/accuracy/ilsvrc:imagenet_accuracy_eval failed to build
INFO: Elapsed time: 18.066s, Critical Path: 2.62s
INFO: 0 processes.
FAILED: Build did NOT complete successfully

"
22182,"""master"" in cluster spec for distributed training ","Hi,

Given in the sample code [mnist_replica.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dist_test/python/mnist_replica.py), `tf.train.ClusterSpec()` needs to be used to define the cluster for distributed training.  The function `tf.train.ClusterSpec()` can only take `""ps""` and `""worker""` hosts, but `""master""` is not allowed. As far as I know, `""master""` is usually defined as a special worker and should be a part of the worker hosts, referred as `task_id = 0`. This makes sense to me.

However, in the context of `tf.estimator`, if `""cluster""` is set in `TF_CONFIG`, it must have one `""chief""` node, in addition to `""worker""`. For example,

```
TF_CONFIG='{
    ""cluster"": {
        ""chief"": [""host0:2222""],
        ""worker"": [""host1:2222"", ""host2:2222"", ""host3:2222""],
        ""ps"": [""host4:2222"", ""host5:2222""]
    },
    ""task"": {""type"": ""chief"", ""index"": 0}
}'
```
I am trying to use `TF_CONFIG` for all our distributed training work, but it looks like to be inconsistent. For example, we need to do some preprocessing in the above `TF_CONFIG` (i.e., combining `""chief""` and `""worker""` hosts, and re-index them), to use it for `tf.train.ClusterSpec()`. 

Is there any better standard we can follow to achieve our goal?

Thanks 

"
22180,Tensorflow java built failure on amazon linux - C++ compilation of rule '@protobuf_archive//:protobuf_lite' failed (Exit 1),"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Amazon Linux AMI 2018.03
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: no
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:  17a34ab
- **Python version**: 2.7.14
- **Bazel version (if compiling from source)**:  0.16.1
- **GCC/Compiler version (if compiling from source)**: c++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-28)
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Failed building Tensorflow Java library from source in Amazon Linux 2018.03

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

[root@ip-xxx-xx-xx-xx tensorflow-master]# ./configure
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.16.1- (@non-git) installed.
Please specify the location of python. [Default is /usr/bin/python]:


Found possible Python library paths:
  /usr/local/lib/python2.7/site-packages
  /usr/lib64/python2.7/dist-packages
  /usr/lib64/python2.7/site-packages
  /usr/local/lib64/python2.7/site-packages
  /usr/lib/python2.7/dist-packages
  /usr/lib/python2.7/site-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/site-packages]

Do you wish to build TensorFlow with jemalloc as malloc support? [Y/n]:
jemalloc as malloc support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Google Cloud Platform support? [Y/n]:
Google Cloud Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Hadoop File System support? [Y/n]:
Hadoop File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Amazon AWS Platform support? [Y/n]:
Amazon AWS Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Apache Kafka Platform support? [Y/n]:
Apache Kafka Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with XLA JIT support? [y/N]:
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with GDR support? [y/N]:
No GDR support will be enabled for TensorFlow.

Do you wish to build TensorFlow with VERBS support? [y/N]:
No VERBS support will be enabled for TensorFlow.

Do you wish to build TensorFlow with nGraph support? [y/N]:
No nGraph support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]:
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]:
No CUDA support will be enabled for TensorFlow.

Do you wish to download a fresh release of clang? (Experimental) [y/N]:
Clang will not be downloaded.

Do you wish to build TensorFlow with MPI support? [y/N]:
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]:


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]:
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See tools/bazel.rc for more details.
	--config=mkl         	# Build with MKL support.
	--config=monolithic  	# Config for mostly static monolithic build.
Configuration finished
[root@ip-xxx-xx-xx-xx tensorflow-master]# bazel build --config opt \
>   //tensorflow/java:tensorflow \
>   //tensorflow/java:libtensorflow_jni
Starting local Bazel server and connecting to it...
WARNING: /root/tensorflow-master/tensorflow/core/BUILD:2480:1: in includes attribute of cc_library rule //tensorflow/core:framework_internal_headers_lib: '../../external/com_google_absl' resolves to 'external/com_google_absl' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /root/tensorflow-master/tensorflow/tensorflow.bzl:1373:20
WARNING: /root/tensorflow-master/tensorflow/core/BUILD:2565:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/com_google_absl' resolves to 'external/com_google_absl' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /root/tensorflow-master/tensorflow/tensorflow.bzl:1373:20
INFO: Analysed 2 targets (97 packages loaded).
INFO: Found 2 targets...
ERROR: /root/.cache/bazel/_bazel_root/510cb3499e6983b92a09020af9102ff3/external/protobuf_archive/BUILD:70:1: C++ compilation of rule '@protobuf_archive//:protobuf_lite' failed (Exit 1)
cc1plus: error: unrecognized command line option '-mno-sha'
cc1plus: error: unrecognized command line option '-mno-sgx'
cc1plus: error: unrecognized command line option '-mno-avx512f'
cc1plus: error: unrecognized command line option '-mno-avx512er'
cc1plus: error: unrecognized command line option '-mno-avx512cd'
cc1plus: error: unrecognized command line option '-mno-avx512pf'
cc1plus: error: unrecognized command line option '-mno-prefetchwt1'
cc1plus: error: unrecognized command line option '-mno-clflushopt'
cc1plus: error: unrecognized command line option '-mno-xsavec'
cc1plus: error: unrecognized command line option '-mno-xsaves'
cc1plus: error: unrecognized command line option '-mno-avx512dq'
cc1plus: error: unrecognized command line option '-mno-avx512bw'
cc1plus: error: unrecognized command line option '-mno-avx512vl'
cc1plus: error: unrecognized command line option '-mno-avx512ifma'
cc1plus: error: unrecognized command line option '-mno-avx512vbmi'
cc1plus: error: unrecognized command line option '-mno-avx5124fmaps'
cc1plus: error: unrecognized command line option '-mno-avx5124vnniw'
cc1plus: error: unrecognized command line option '-mno-clwb'
cc1plus: error: unrecognized command line option '-mno-mwaitx'
cc1plus: error: unrecognized command line option '-mno-clzero'
cc1plus: error: unrecognized command line option '-mno-rdpid'
external/protobuf_archive/src/google/protobuf/stubs/structurally_valid.cc:1:0: error: bad value (haswell) for -march= switch
 // Protocol Buffers - Google's data interchange format
 ^
external/protobuf_archive/src/google/protobuf/stubs/structurally_valid.cc:1:0: error: bad value (haswell) for -mtune= switch
cc1plus: warning: unrecognized command line option ""-Wno-writable-strings"" [enabled by default]
INFO: Elapsed time: 22.577s, Critical Path: 0.07s
INFO: 0 processes.
FAILED: Build did NOT complete successfully
"
22179,TypeError: argmax() got an unexpected keyword argument 'output_type' in tensorflow1.2,"Hello, I encountered TypeError error when I was running the program.

File ""/home/kjm/Desktop/inpainting/GatedConvolution/inpaint_ops.py"", line 512, in contextual_attention
offset = tf.argmax(yi, axis=3, output_type=tf.int32)
TypeError: argmax() got an unexpected keyword argument 'output_type'

I used python3.6+tensorflow1.2.And what should I do to correct this error?

Thank you very much for your help."
22178,feature request:  A transform that maps the input to the output in tf.contrib.image,"**feature request: 
A transform that map the input to the output in tf.contrib.image**

In current tf.contrib.image.transform, it says ""The transforms are *inverted* compared to the transform mapping input points to output points"".
This is counter-intuitive to me. Why would you define a function that applies a transform converting the output to the input? Isn't input -> transform matrix -> output more straight forward?

Update: 

I believe this should be easy to implement.

You can check the the source code [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/image/python/ops/image_ops.py).

Under `def transform`, it says:

>"" it maps the *output* point `(x, y)` to a transformed *input* point""

To make the inverted transform work, people have to use it with `tf.linalg.inv`.

**My request is to change it to a normal transform, which maps the input point to the output point, so that we don't have to bother with `tf.linalg.inv`.**

### System information
- Have I written custom code: NO
- OS Platform and Distribution: Linux Ubuntu 16.04
- Mobile device: N/A
- TensorFlow installed from:
- TensorFlow version: 1.10.0
- Python version: 2.7.12
- Bazel version:N/A
- GCC/Compiler version : 5.4.0
- CUDA/cuDNN version: 9.0
- GPU model and memory: GTX 1070 8G
- Exact command to reproduce: N/A
"
22177,Wavelet transform ,Is there any plan in making tensorflow support wavelet transform?
22176,seq2seq.dynamic_decode with float16 inputs on CPU,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Manjaro Linux, testing
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: nice
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10.1
- **Python version**: 3.7.0
- **Bazel version (if compiling from source)**: 0.16.1
- **GCC/Compiler version (if compiling from source)**: 7.3.1
- **CUDA/cuDNN version**: cuda 9.2.148, cudnn 7.2.1
- **GPU model and memory**: gtx 1080
- **Exact command to reproduce**:
I am using my own setup here:
https://github.com/georgesterpu/Sigmedia-AVSR/blob/master/experiment_tcd_audio.py
If really needed, I could write a minimal example to reproduce the issue.

### Describe the problem
I have an issue with the code indicated above, when switching to 16 bit precision.
On the CPU only, the code crashes with the following error:

```
InvalidArgumentError (see above for traceback): indices[0] = 31 is not in [0, 31)
	 [[Node: Decoder/decoder/while/BasicDecoderStep/ScheduledEmbeddingTrainingHelperNextInputs/cond/embedding_lookup = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Decoder/decoder/while/BasicDecoderStep/ScheduledEmbeddingTrainingHelperNextInputs/cond/embedding_lookup/Switch, Decoder/decoder/while/BasicDecoderStep/ScheduledEmbeddingTrainingHelperNextInputs/cond/GatherNd, Decoder/decoder/while/BasicDecoderStep/ScheduledEmbeddingTrainingHelperNextInputs/cond/embedding_lookup/axis)]]
``` 
This is supposed to be the embedding lookup of the input ids to a `seq2seq.BasicDecoder` taking a `seq2seq.ScheduledEmbeddingTrainingHelper`. I first created the embedding matrix in float16, then passed it to the `embedding` argument. As a workaround, I've also created it in float32, passing instead a callable that would cast the retrieved vectors to float16, but the error persists.

The number `31` is the size of the vocabulary, and no id is above 30. However, even if I increase the size of the embedding matrix, the error would look like this:
`indices[0] = 132 is not in [0, 132)`

The same code runs fine on GPU, but I am aware that error reporting is restricted in this case. 

Could you please help me find out where is the problem and how to fix it ?
Is `seq2seq.dynamic_decode` fully supporting float16 data types ?

Thank you


### Source code / logs
```
  File ""/usr/lib/python3.7/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py"", line 322, in dynamic_decode
    swap_memory=swap_memory)
  File ""/usr/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 3232, in while_loop
    return_same_structure)
  File ""/usr/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2952, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File ""/usr/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2887, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File ""/usr/lib/python3.7/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py"", line 265, in body
    decoder_finished) = decoder.step(time, inputs, state)
  File ""/usr/lib/python3.7/site-packages/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py"", line 146, in step
    sample_ids=sample_ids)
  File ""/usr/lib/python3.7/site-packages/tensorflow/contrib/seq2seq/python/ops/helper.py"", line 351, in next_inputs
    all_finished, lambda: base_next_inputs, maybe_sample)
  File ""/usr/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py"", line 454, in new_func
    return func(*args, **kwargs)
  File ""/usr/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2057, in cond
    orig_res_f, res_f = context_f.BuildCondBranch(false_fn)
  File ""/usr/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1895, in BuildCondBranch
    original_result = fn()
  File ""/usr/lib/python3.7/site-packages/tensorflow/contrib/seq2seq/python/ops/helper.py"", line 340, in maybe_sample
    sampled_next_inputs = self._embedding_fn(sample_ids_sampling)
  File ""/run/media/john_tukey/work/phd/65.cleanavsr/Sigmedia-AVSR/avsr/decoder_unimodal.py"", line 454, in <lambda>
    embedding_fun = lambda ids: tf.cast(tf.nn.embedding_lookup(self._embedding_matrix, ids), dtype=self._hparams.dtype)
  File ""/usr/lib/python3.7/site-packages/tensorflow/python/ops/embedding_ops.py"", line 310, in embedding_lookup
    transform_fn=None)
  File ""/usr/lib/python3.7/site-packages/tensorflow/python/ops/embedding_ops.py"", line 133, in _embedding_lookup_and_transform
    result = _clip(array_ops.gather(params[0], ids, name=name),
  File ""/usr/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py"", line 2659, in gather
    return gen_array_ops.gather_v2(params, indices, axis, name=name)
  File ""/usr/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 3142, in gather_v2
    ""GatherV2"", params=params, indices=indices, axis=axis, name=name)
  File ""/usr/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py"", line 454, in new_func
    return func(*args, **kwargs)
  File ""/usr/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 3155, in create_op
    op_def=op_def)
  File ""/usr/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): indices[0] = 31 is not in [0, 31)
	 [[Node: Decoder/decoder/while/BasicDecoderStep/ScheduledEmbeddingTrainingHelperNextInputs/cond/embedding_lookup = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_HALF, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Decoder/decoder/while/BasicDecoderStep/ScheduledEmbeddingTrainingHelperNextInputs/cond/embedding_lookup/Switch, Decoder/decoder/while/BasicDecoderStep/ScheduledEmbeddingTrainingHelperNextInputs/cond/GatherNd, Decoder/decoder/while/BasicDecoderStep/ScheduledEmbeddingTrainingHelperNextInputs/cond/embedding_lookup/axis)]]
```
"
22174,tensorflow/core/framework/tensor.cc:861] Check failed: buf_ null buf_ with non-zero shape size 1,"I'm running a model Lda2vec_tensorflow on a Mac with Tensorflow 1.8 in an anaconda 3.6 virtual environment.  The model runs properly without trying to attach the tensor board debugger.
When I attach the debugger I get an abort signal.

**Have I written custom code**
No
**OS Platform and Distribution**
OS X 10.11.6

**TensorFlow installed from**
```
$ conda list tensorflow
# packages in environment at /Users/davidlaxer/anaconda/envs/ai:
#
# Name                    Version                   Build  Channel
tensorflow                1.8.0                    py36_1    condo-forge
```
**TensorFlow version**
1.8.0

**Bazel version**
[bazel release 0.14.0]

**CUDA/cuDNN version**
N/A

**GPU model and memory**
N/A

**Exact command to reproduce**
```
/Users/davidlaxer/anaconda/envs/ai/bin/python /Applications/PyCharm.app/Contents/helpers/pydev/pydevd.py --multiproc --qt-support=auto --client 127.0.0.1 --port 51376 --file /Users/davidlaxer/Lda2vec-Tensorflow/run_stories.py
pydev debugger: process 91253 is connecting

Connected to pydev debugger (build 182.4129.34)
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
/Users/davidlaxer/anaconda/envs/ai/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
95 ops no flops stats due to incomplete shapes.
```

Next, click STEP in tensor board debugger:
```
2018-09-10 10:27:19.544078: F tensorflow/core/framework/tensor.cc:861] Check failed: buf_ null buf_ with non-zero shape size 1
Process finished with exit code 134 (interrupted by signal 6: SIGABRT)
```

**Mobile device**
N/A

Here's my environment:
```
$ cat tf_env.txt 

== cat /etc/issue ===============================================
Darwin David-Laxers-MacBook-Pro.local 15.6.0 Darwin Kernel Version 15.6.0: Thu Jun 21 20:07:40 PDT 2018; root:xnu-3248.73.11~1/RELEASE_X86_64 x86_64
Mac OS X 10.11.6

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 8.0.0 (clang-800.0.42.1)
Target: x86_64-apple-darwin15.6.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

== uname -a =====================================================
Darwin David-Laxers-MacBook-Pro.local 15.6.0 Darwin Kernel Version 15.6.0: Thu Jun 21 20:07:40 PDT 2018; root:xnu-3248.73.11~1/RELEASE_X86_64 x86_64

== check pips ===================================================
msgpack-numpy       0.4.3     
numpy               1.14.5    
protobuf            3.6.0     
tensorflow          1.8.0     

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.8.0
tf.GIT_VERSION = b'v1.8.0-8-g23c2187'
tf.COMPILER_VERSION = b'v1.8.0-8-g23c2187'
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tf_env_collect.sh: line 105: nvidia-smi: command not found



== cuda libs  ===================================================
(ai) David-Laxers-MacBook-Pro:tools davidlaxer$ 

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

b'v1.8.0-8-g23c2187' 1.8.0
```

I'm trying to use the tensorboard debugger to debug the github model Lda2vec_tensorflow model.
I am able to attach the tensorboard debugger to the model, however, when I click 'step' or 'continue,
the process running the model exits with the following error message :

95 ops no flops stats due to incomplete shapes.
2018-09-09 11:07:02.150246: F tensorflow/core/framework/tensor.cc:861] Check failed: buf_ null buf_ with non-zero shape size 1

Process finished with exit code 134 (interrupted by signal 6: SIGABRT)


### Source code / logs

The code to run the Lda2vec_Tensorflow model is here:

https://github.com/nateraw/Lda2vec-Tensorflow

Here is the code that I use to invoke the tensor board debugger.

I use port 60664 to attach the debugger:
```
self.sesh = tf.Session(config=self.config)
self.sesh = tf_debug.TensorBoardDebugWrapperSession(self.sesh, 'localhost:6064')
...
run_metadata = tf.RunMetadata()
self.sesh.run(tf.global_variables_initializer(), options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE, report_tensor_allocations_upon_oom=True),
               run_metadata=run_metadata)
```
![screen shot 2018-09-09 at 11 07 05 am](https://user-images.githubusercontent.com/3105499/45267642-12fa8e00-b425-11e8-8687-abee65c7f1be.png)"
22172,seq2seq.BeamSearchDecoder incompatible with float16,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Manjaro Linux, testing
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10.1
- **Python version**: 3.7.0
- **Bazel version (if compiling from source)**: 0.16.1
- **GCC/Compiler version (if compiling from source)**: 7.3.1
- **CUDA/cuDNN version**: cuda 9.2.148, cudnn 7.2.1
- **GPU model and memory**: gtx 1080
- **Exact command to reproduce**:
I am using my own setup here:
https://github.com/georgesterpu/Sigmedia-AVSR/blob/master/experiment_tcd_audio.py
If really needed, I could write a minimal example to reproduce the issue.

### Describe the problem
I want to add support for 16 bit precision to our TensorFlow-based speech recognition system.
It looks like `seq2seq.BeamSearchDecoder` is not compatible with this data type when using the `length_penalty_weight` option. This is because the `_get_scores` function in `beam_search_decoder.py`  has to compute a division between a float16 tensor and the float32 tensor returned by the `_length_penalty` function.

### Source code / logs

>   File ""/usr/lib/python3.7/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py"", line 322, in dynamic_decode
    swap_memory=swap_memory)
  File ""/usr/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 3232, in while_loop
    return_same_structure)
  File ""/usr/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2952, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File ""/usr/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2887, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File ""/usr/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 3201, in <lambda>
    body = lambda i, lv: (i + 1, orig_body(*lv))
  File ""/usr/lib/python3.7/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py"", line 265, in body
    decoder_finished) = decoder.step(time, inputs, state)
  File ""/usr/lib/python3.7/site-packages/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py"", line 658, in step
    length_penalty_weight=length_penalty_weight)
  File ""/usr/lib/python3.7/site-packages/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py"", line 720, in _beam_search_step
    length_penalty_weight=length_penalty_weight)
  File ""/usr/lib/python3.7/site-packages/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py"", line 824, in _get_scores
    return log_probs / length_penalty_
  File ""/usr/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py"", line 850, in binary_op_wrapper
    return func(x, y, name=name)
  File ""/usr/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py"", line 950, in _truediv_python3
    (x_dtype, y_dtype))
TypeError: x and y must have the same dtype, got tf.float16 != tf.float32
"
22171,Elements of a CsvDataset should be tensors instead of tuples,"------------------------

### System information
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 18.04**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **No**
- TensorFlow installed from (source or binary): **source**
- TensorFlow version (use command below): **1.10**
- Python version: **3.6**
- Bazel version (if compiling from source): **0.16.1**
- GCC/Compiler version (if compiling from source): **7.3.0**
- CUDA/cuDNN version: **N/A**
- GPU model and memory: **N/A**
- Exact command to reproduce:

### Describe the problem
According to the [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) documentation, the elements of a generated dataset are expected to be tensors or sparse tensors. But when you create a [CsvDataset](https://www.tensorflow.org/api_docs/python/tf/contrib/data/CsvDataset), the elements are tuples. As a result, the dataset methods behave strangely.

### Source code / logs
Assume the file example.csv contains this text:
```
1.0,2.0,3.0
4.0,5.0,6.0
```

This code creates a CsvDataset and calls batch to combine elements into batches:
```
ds1 = tf.contrib.data.CsvDataset('example.csv', [tf.float32, tf.float32, tf.float32])
ds2 = ds1.batch(2)
iter = ds2.make_one_shot_iterator()
with tf.Session() as sess:
    while True:            
        try:
          print(sess.run(iter.get_next()))
        except tf.errors.OutOfRangeError:
          break
```

The application prints the following result for ds2:
```(array([1., 4.], dtype=float32), array([2., 5.], dtype=float32), array([3., 6.], dtype=float32))```

The elements don't form expected batches because the elements of ds1 are tuples instead of tensors.
"
22169,Possible bug when getting the gradient of unstack,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:

Ubuntu 18.04

- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
n/a

- **TensorFlow installed from (source or binary)**:
binary

- **TensorFlow version (use command below)**:

v1.8.0-0-g93bc2e2072 1.8.0

- **Python version**: Python 3.6.5
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**:
- **GPU model and memory**: GeForce 1060 6GB
- **Exact command to reproduce**:


### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Unable to get the gradient of unstack:

a = tf.constant([[[[0, 8], [1, 9]], [[2, 10], [3, 11]]], [[[4, 12], [5, 13]], [[6, 14], [7, 15]]]])
x, y = tf.unstack(a, axis=3)
g = tf.gradients(x, [a])
sess = tf.Session()
sess.run(g)

Expected: [[[[[1, 0], [1, 0]], [[1, 0], [1, 0]]], [[[1, 0], [1, 0]], [[1, 0], [1, 0]]]]]

Actual: TypeError: Tensors in list passed to 'values' of 'Pack' Op have types [int32, <NOT CONVERTIBLE TO TENSOR>] that don't all match.

### Source code / logs




"
22168,tensorflow.python.framework.errors_impl.NotFoundError: C:\Anaconda3\lib\site-packages\tensorflow\contrib\data\python\ops\..\..\_dataset_ops.so not found,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22167,tensorflow/contrib/lite/examples/android/BUILD/android-profile is not a directory,"Hi there. I have installed the latest version of tensorflow and trained SSD quantized model following [this article](https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193) very precisely. After the training was successfully done, I've decided to run the android tflite example project to check how my model (frozen graph) will work with it. I've installed 14b NDK version as it was recommended and I have API level 27.

Then I run 
```
bazel build -c opt --config=android_arm{,64} --cxxopt='--std=c++11' \
//tensorflow/contrib/lite/examples/android:tflite_demo
```
And it compiled successfully without errors. But when I opened the project with Android Studio, I can't sync it with Gradle because there is an error I mentioned above (I will add the screenshot). After any attempts of re-build it says: ""Sync failed. Already finished""

<img width=""830"" alt=""2018-09-09 10 48 54"" src=""https://user-images.githubusercontent.com/4061805/45262560-96b27b80-b421-11e8-91c0-ac32e95d1b38.png"">

What should I do?

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra 10.13.4
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.10
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**: 0.15.2-homebrew
- **GPU model and memory**: Intel Iris Graphics 6100 / 1536 MB
"
22166,"After convert a mobilenet_v1 from pb model to lite, the model accuracy reduction for 12%","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:None
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.9.0
- **Python version**:2.7
- **Bazel version (if compiling from source)**:1.6.1
- **GCC/Compiler version (if compiling from source)**:5.4.0
- **CUDA/cuDNN version**:9.0/7.1
- **GPU model and memory**:8GB
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I converted the pb-formatted mobilenet_v1 to lite with the following code:
python tflite_convert.py \
--output_file=/data2/LIBY0711/trained_model/quantize_0827/mobilenet_v1_quant_lr_0.005_GD_0.5_decay_int8/pb_model2/sku_classify_scale-66.tflite \
--graph_def_file=/data2/LIBY0711/trained_model/quantize_0827/mobilenet_v1_quant_lr_0.005_GD_0.5_decay_int8/pb_model2/frozen_inference_graph_skip.pb \
--inference_type=QUANTIZED_UINT8 \
--input_shape=1,224,224,3 \
--input_arrays=input \
--default_ranges_min=0 \
--default_ranges_max=6 \
--output_arrays=MobilenetV1/Predictions/Softmax \
--mean_values=0 \
--std_dev_values=1

However, the precision of lite after transformation is 12% lower than that of pb. I don't know if I'm setting the parameters incorrectly what else is going to be the problem, hopefully that's answered

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22165,TENSORFLOW ISSUES ,"I've been trying to install TensorFlow for the past few hours and keep getting this error:
# >>>>>>>>>>>>>>>>>>>>>> ERROR REPORT <<<<<<<<<<<<<<<<<<<<<<

    Traceback (most recent call last):
      File ""/anaconda2/lib/python2.7/site-packages/conda/exceptions.py"", line 812, in __call__
        return func(*args, **kwargs)
      File ""/anaconda2/lib/python2.7/site-packages/conda/cli/main.py"", line 78, in _main
        exit_code = do_call(args, p)
      File ""/anaconda2/lib/python2.7/site-packages/conda/cli/conda_argparse.py"", line 77, in do_call
        exit_code = getattr(module, func_name)(args, parser)
      File ""/anaconda2/lib/python2.7/site-packages/conda/cli/main_install.py"", line 11, in execute
        install(args, parser, 'install')
      File ""/anaconda2/lib/python2.7/site-packages/conda/cli/install.py"", line 253, in install
        handle_txn(unlink_link_transaction, prefix, args, newenv)
      File ""/anaconda2/lib/python2.7/site-packages/conda/cli/install.py"", line 269, in handle_txn
        unlink_link_transaction.print_transaction_summary()
      File ""/anaconda2/lib/python2.7/site-packages/conda/core/link.py"", line 730, in print_transaction_summary
        legacy_action_groups = self._make_legacy_action_groups()
      File ""/anaconda2/lib/python2.7/site-packages/conda/core/link.py"", line 713, in _make_legacy_action_groups
        self._pfe.prepare()
      File ""/anaconda2/lib/python2.7/site-packages/conda/common/io.py"", line 46, in decorated
        return f(*args, **kwds)
      File ""/anaconda2/lib/python2.7/site-packages/conda/core/package_cache_data.py"", line 561, in prepare
        for prec in self.link_precs)
      File ""/anaconda2/lib/python2.7/_abcoll.py"", line 571, in update
        for key, value in other:
      File ""/anaconda2/lib/python2.7/site-packages/conda/core/package_cache_data.py"", line 561, in <genexpr>
        for prec in self.link_precs)
      File ""/anaconda2/lib/python2.7/site-packages/conda/core/package_cache_data.py"", line 454, in make_actions_for_record
        ), None)
      File ""/anaconda2/lib/python2.7/site-packages/conda/core/package_cache_data.py"", line 451, in <genexpr>
        pcrec for pcrec in concat(PackageCacheData(pkgs_dir).query(pref_or_spec)
      File ""/anaconda2/lib/python2.7/site-packages/conda/core/package_cache_data.py"", line 452, in <genexpr>
        for pkgs_dir in context.pkgs_dirs)
      File ""/anaconda2/lib/python2.7/site-packages/conda/core/package_cache_data.py"", line 125, in query
        return (pcrec for pcrec in itervalues(self._package_cache_records) if pcrec == param)
      File ""/anaconda2/lib/python2.7/site-packages/conda/core/package_cache_data.py"", line 221, in _package_cache_records
        self.load()
      File ""/anaconda2/lib/python2.7/site-packages/conda/core/package_cache_data.py"", line 91, in load
        package_cache_record = self._make_single_record(base_name)
      File ""/anaconda2/lib/python2.7/site-packages/conda/core/package_cache_data.py"", line 350, in _make_single_record
        md5 = compute_md5sum(package_tarball_full_path)
      File ""/anaconda2/lib/python2.7/site-packages/conda/gateways/disk/read.py"", line 71, in compute_md5sum
        return _digest_path('md5', file_full_path)
      File ""/anaconda2/lib/python2.7/site-packages/conda/gateways/disk/read.py"", line 64, in _digest_path
        with open(path, ""rb"") as fh:
      File ""/anaconda2/lib/python2.7/site-packages/conda/common/compat.py"", line 121, in open
        errors=errors, newline=newline, closefd=closefd)
    IOError: [Errno 13] Permission denied: '/anaconda2/pkgs/pyspark-2.3.0-py27_0.tar.bz2'

`$ /anaconda2/bin/conda install tensorflow`

  environment variables:
                 CIO_TEST=<not set>
               CONDA_ROOT=/anaconda2
                     PATH=/Library/Frameworks/Python.framework/Versions/3.7/bin:/anaconda2/bin:/
                          Users/asy/Downloads/google-cloud-sdk/bin:/usr/local/bin:/usr/bin:/bin:
                          /usr/sbin:/sbin:/Library/TeX/texbin
       REQUESTS_CA_BUNDLE=<not set>
            SSL_CERT_FILE=<not set>

     active environment : None
       user config file : /Users/asy/.condarc
 populated config files : /Users/asy/.condarc
          conda version : 4.5.0
    conda-build version : 3.0.27
         python version : 2.7.14.final.0
       base environment : /anaconda2  (writable)
           channel URLs : https://repo.anaconda.com/pkgs/main/osx-64
                          https://repo.anaconda.com/pkgs/main/noarch
                          https://repo.anaconda.com/pkgs/free/osx-64
                          https://repo.anaconda.com/pkgs/free/noarch
                          https://repo.anaconda.com/pkgs/r/osx-64
                          https://repo.anaconda.com/pkgs/r/noarch
                          https://repo.anaconda.com/pkgs/pro/osx-64
                          https://repo.anaconda.com/pkgs/pro/noarch
          package cache : /anaconda2/pkgs
                          /Users/asy/.conda/pkgs
       envs directories : /anaconda2/envs
                          /Users/asy/.conda/envs
               platform : osx-64
             user-agent : conda/4.5.0 requests/2.18.4 CPython/2.7.14 Darwin/16.7.0 OSX/10.12.6
                UID:GID : 301705310:285281272
             netrc file : None
           offline mode : False

 This is what happens when I go down the conda route. 

When I go through the pip route,  this happens: 

Could not install packages due to an EnvironmentError: [Errno 13] Permission denied: u'/private/var/folders/k7/xthqkfr968g8c83fbtdm1jfc8zq_2y/T/pip-unpack-yhcLfC/tensorflow-1.10.1-cp27-cp27m-macosx_10_11_x86_64.whl'
Consider using the `--user` option or check the permissions. 

Please assist. 
"
22162,tf.map_fn causes error when importing the same graph twice and connecting them,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: binary (pip in conda environment)
- **TensorFlow version (use command below)**: v1.10.1-0-g4dcfddc5d1 1.10.1
- **Python version**: 3.6.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: GTX 1070 / 8GB
- **Exact command to reproduce**: below


### Describe the problem
I am trying to load the same graph twice (with different variable weights) to perform stage-wise object detection with one connected graph. I import them with different name scopes, but still I get an error message because there is some bug in the tf.map_fn:
```""tensorflow.python.framework.errors_impl.InvalidArgumentError: Invalid loop structure: Loop ""map/while/while_context"" has more than one LoopCond node: ""graph2/map/while/LoopCond"" and ""graph1/map/while/LoopCond"". This is an internal bug, please file a bug report with instructions on how to reproduce the error.""```

Minimal example to reproduce is below, but to me it ocurred with the faster-rcnn meta architecture from the object detection framework, where the first loop that causes this error is in the ""preprocess"" funtion of ""object_detection.meta_architectures.faster_rcnn_meta_arch.py"" at line 535. When I changed the tf.name_scope from 'Preprocessor' to something else in just one of the graphs, it threw this error from a different usage of the map_fn further down the graph.

from object_detection.meta_architectures.faster_rcnn_meta_arch.py (line 535):
```python
    if inputs.dtype is not tf.float32:
      raise ValueError('`preprocess` expects a tf.float32 tensor')
    with tf.name_scope('Preprocessor'): #when I change this in one of the graphs, the error doesn't occur here anymore
      outputs = shape_utils.static_or_dynamic_map_fn(
          self._image_resizer_fn,
          elems=inputs,
          dtype=[tf.float32, tf.int32],
          parallel_iterations=self._parallel_iterations)
      resized_inputs = outputs[0]
      true_image_shapes = outputs[1]
      return (self._feature_extractor.preprocess(resized_inputs),
              true_image_shapes)
```
This would actually be my workaround if this doesn't get fixed, to just rename all the map_fn scopes before training each subgraph.

### Script to reproduce bug
```python
import tensorflow as tf
import numpy as np

def build_map_graph(name_scope=''):
    graph = tf.Graph()
    with graph.as_default():
        input_tensor = tf.placeholder(tf.float32, shape=[None, 1], name='input')

        def mult(input):
            input = tf.multiply(input, 2)
            return input

        with tf.name_scope(name_scope):
            multiplied = tf.map_fn(mult, input_tensor)
        out_tensor = tf.identity(multiplied, name='output')

    return graph

def build_simple_graph():
    graph = tf.Graph()
    with graph.as_default():
        input_tensor = tf.placeholder(tf.float32, shape=[None, 1], name='input')

        multiplied = tf.multiply(input_tensor, 2)
        out_tensor = tf.identity(multiplied, name='output')

    return graph


def connect_graphs(both_are_map_graphs=False, rename_second=False):
    test_data = np.asarray([[1.0], [2.0]], dtype=np.float32)

    graph1 = build_map_graph()
    g1_graph_def = graph1.as_graph_def()

    g2_map_name_scope = 'graph2_map' if rename_second else ''
    graph2 = build_map_graph(g2_map_name_scope) if both_are_map_graphs else build_simple_graph()

    g2_graph_def = graph2.as_graph_def()

    connected_graph = tf.Graph()

    with connected_graph.as_default():
        tf.import_graph_def(g1_graph_def, name='graph1')

        g1_output_tensor = tf.get_default_graph().get_tensor_by_name('graph1/output:0')
        g1_input_tensor = tf.get_default_graph().get_tensor_by_name('graph1/input:0')

        tf.import_graph_def(g2_graph_def, name='graph2', input_map={'input': g1_output_tensor})
        g2_output_tensor = tf.get_default_graph().get_tensor_by_name('graph2/output:0')

        with tf.Session() as sess:
            result = sess.run(g2_output_tensor, {g1_input_tensor: test_data})
            print(result)


#runs fine
connect_graphs(both_are_map_graphs=False)
connect_graphs(both_are_map_graphs=True, rename_second=True)

#throws error
connect_graphs(both_are_map_graphs=True)
```

Edit:
changed connect_graphs(both_are_map_graphs=False, rename_second=True) to connect_graphs(both_are_map_graphs=True, rename_second=True) in the example to better demonstrate the issue."
22161,ValueError on using tf.contrib.avro with tf.keras,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: RHEL 7
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA
- **TensorFlow installed from (source or binary)**: Yes
- **TensorFlow version (use command below)**: 1.9.0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: Please see the source below

### Describe the problem
On using [tf.contrib.avro](https://github.com/tensorflow/tensorflow/pull/18224) with tf.keras, it throws a ValueError. The dataset created using tf.contrib.avro produces the results correctly but when used as training data for tf.keras.model during fit, it fails saying - `ValueError: Input 0 of layer dense0 is incompatible with the layer: : expected min_ndim=2, found ndim=1. Full shape received: [None]`

### Source code / logs
Here is the test code to reproduce the error:
```python
import tensorflow as tf
from tensorflow.keras.layers import Conv1D, MaxPool1D, Flatten, Dense, Dropout, Input, BatchNormalization, concatenate
from tensorflow.keras.optimizers import Adam

from tensorflow.contrib.avro.python.parse_avro_record import parse_avro_record
from tensorflow.contrib.avro.python.avro_record_dataset import *


from avro.io import DatumReader, DatumWriter, BinaryDecoder, BinaryEncoder
from avro.schema import parse
from StringIO import StringIO
from fastavro import writer, parse_schema
import random, json, glob, os

schema = '''{""doc"": ""Simple example."",
             ""namespace"": ""com.linkedin.demo.person"",
             ""type"": ""record"",
             ""name"": ""data_row"",
             ""fields"": [
               {""name"": ""index"", ""type"": ""long""},
               {""name"": ""first_name"", ""type"": ""string""},
               {""name"": ""age"", ""type"": ""int""},
               {""name"": ""gender"", ""type"": ""int""}
            ]}'''

data = [{'index': 0, 'first_name': ""Karl"", 'age': 22, ""gender"": 0},
        {'index': 1, 'first_name': ""Julia"", 'age': 42, ""gender"": 1},
        {'index': 2, 'first_name': ""Liberty"", 'age': 90, ""gender"": 1}]

features = {'index': tf.FixedLenFeature([], tf.int64),
            'first_name': tf.FixedLenFeature([], tf.string),
            'age': tf.FixedLenFeature([], tf.int32)}

RECORDS_PER_FILE = 1000
FEATURE_LIST = [""index"", ""age""]
exclude_features = [""first_name""]
RESPONSE_FIELD = ""gender""
DATA_SCHEMA = schema
BATCH_SIZE = 10

def data_generator(output_directory, num_files, records_per_file):
    parsed_schema = parse_schema(json.loads(DATA_SCHEMA))

    for i in range(num_files):
        print(""Generating file "" + str(i))
        records = [{}] * records_per_file
        for j in range(records_per_file):
            records[j] = {
                ""index"": int( random.random() * 100 ),
                ""first_name"": """",
                ""age"": int( random.random() * 95 ),
                ""gender"": int( random.random() * 10 ) % 2
            }
        
        with open( output_directory + str(i) + '.avro', 'wb') as out:
            writer(out, parsed_schema, records)

def create_input_fn(data_files, batch_size, num_parallel, shuffle_size, repeat_times=-1):
    """"""Create the input function for training and evaluation.""""""
    
    # Define the function for parsing avro records into TensorFlow tensors (features, label).
    def _parse_function(serialized):
        parser_spec = {f: tf.FixedLenFeature([], tf.float32, default_value=0) for f in FEATURE_LIST}
        parser_spec[RESPONSE_FIELD] = tf.FixedLenFeature([], tf.int32, default_value=0)
        parsed = parse_avro_record(serialized=serialized, schema=DATA_SCHEMA, features=parser_spec)

        features = {f: parsed[f] for f in FEATURE_LIST if f not in exclude_features}
        label = parsed.pop(RESPONSE_FIELD)
        return features, label

    # Create the dataset and apply the parse function (must apply batch before parsing).
    dataset = AvroRecordDataset(filenames=data_files)
    dataset = dataset.batch(batch_size).map(_parse_function, num_parallel_calls=num_parallel)

    if repeat_times is not None:
        dataset = dataset.cache().repeat(repeat_times)
    if shuffle_size is not None:
        dataset = dataset.shuffle(shuffle_size)
        
    dataset = dataset.prefetch(2)

    return dataset
    
def deep_model(layer_names):
    inputs = [Input(shape=(1,), name=layer_name) for layer_name in layer_names]
    x = concatenate(inputs, name=""input_layer"")
    x = Dense(1, activation='relu', name=""dense0"")(x)
    outputs = Dense(2, activation='softmax')(x)
    
    return tf.keras.Model(inputs, outputs)

def test_data_generator(output_directory):
    print(""Running data generator"")
    data_generator(output_directory, 10, RECORDS_PER_FILE)

def train_using_fit(data_dir):
    print(""Running the training loop using model.fit"")
    files = tf.gfile.Glob(os.path.join(data_dir, ""*.avro"")) # can be generated using data_generator() above
    
    model = deep_model([f for f in FEATURE_LIST if f not in exclude_features])
    optimizer = Adam(lr=0.001, clipvalue=0.5)
    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    model.summary()
    
    training_dataset = create_input_fn(data_files=files, 
                                       batch_size=BATCH_SIZE, num_parallel=5, shuffle_size=None)
    model.fit(
        training_dataset.make_one_shot_iterator(),
        steps_per_epoch=(RECORDS_PER_FILE * len(files)) // BATCH_SIZE,
        epochs=2)

def train_using_estimator(data_dir):
    print(""Running the training loop using keras estimators"")
    files = tf.gfile.Glob(os.path.join(data_dir, ""*.avro"")) # can be generated using data_generator() above
    
    model = deep_model([f for f in FEATURE_LIST if f not in exclude_features])
    optimizer = Adam(lr=0.001, clipvalue=0.5)
    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    model.summary()

    training_dataset = create_input_fn(data_files=files, 
                                       batch_size=BATCH_SIZE, num_parallel=5, shuffle_size=None)

    estimator = tf.keras.estimator.model_to_estimator(model) # can provide distributed training strategy
    estimator.train(input_fn=lambda: training_dataset.make_one_shot_iterator().get_next(), max_steps=20)


def main():
    data_dir = ""./data/mock_data/"" # make sure this folder exists
    # test_data_generator(data_dir) # need to run this only once to generate dummy data
    # train_using_fit(data_dir) # approach 1 for training
    train_using_estimator(data_dir) # approach 2 for training

if __name__ == '__main__':
    main()
```
"
22158,Unexpected behavior of per_process_gpu_memory_fraction ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: See below
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary (used pip)
- **TensorFlow version (use command below)**:  See below 
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A 
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: See below
- **GPU model and memory**: See below 
- **Exact command to reproduce**: Just run the script with python 

### System information from your tf env script:

== cat /etc/issue ===============================================
Linux ip-172-31-54-194 4.4.0-1062-aws #71-Ubuntu SMP Fri Jun 15 10:07:39 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.4 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux ip-172-31-54-194 4.4.0-1062-aws #71-Ubuntu SMP Fri Jun 15 10:07:39 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy               1.14.5
protobuf            3.6.0
tensorflow-gpu      1.10.0

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.10.0
tf.GIT_VERSION = v1.10.0-0-g656e7a2b34
tf.COMPILER_VERSION = v1.10.0-0-g656e7a2b34
Sanity check: array([1], dtype=int32)
/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters

== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda-9.0/lib64:/usr/local/cuda-9.0/extras/CUPTI/lib64:/lib/nccl/cuda-9.0/lib:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Sat Sep  8 01:19:08 2018
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 390.46                 Driver Version: 390.46                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:00:1B.0 Off |                    0 |
| N/A   45C    P0    63W / 300W |      0MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-SXM2...  On   | 00000000:00:1C.0 Off |                    0 |
| N/A   42C    P0    38W / 300W |      0MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-SXM2...  On   | 00000000:00:1D.0 Off |                    0 |
| N/A   42C    P0    43W / 300W |      0MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |
| N/A   44C    P0    42W / 300W |      0MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-9.1/doc/man/man7/libcudart.7
/usr/local/cuda-9.1/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.1/lib64/libcudart.so.9.1.85
/usr/local/cuda-9.1/lib64/libcudart_static.a
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/lib64/libcudart_static.a
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61
/usr/local/cuda-9.0/doc/man/man7/libcudart.7
/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.0/lib64/libcudart.so.9.0.176
/usr/local/cuda-9.0/lib64/libcudart_static.a

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I have written a small example that uses per_process_gpu_memory_fraction. I set it to .1 and allow_growth=false (by default). My expectation when I run nvidia-smi during program execution is that I will see a maximum of 10% of the gpu memory being used. During some trial runs of the program though, I have seen it using around 13%. So I am wondering if this is a bug or if it is expected behavior. If it's expected behavior, why is this happening?

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.


### Script to reproduce with:

```
import tensorflow as tf

from time import sleep

i = tf.constant(0)
x = tf.constant(10)
r = tf.add(i,x)

# Use at most 10% of gpu memory 
gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=.1)

# sleep is used to see what nvidia-smi says for gpu memory usage, 
# I expect that it will be at most 10% of gpu memory (which is 1616.0 mib for my gpu)
# but instead I see up to 2120 mib used by the process
with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
        sess.run(r);
        sleep(10) 

```


"
22153,Tensorflow does not work with python 3.7 when using Virtualenv,"I am not sure where it goes wrong, but after reading tons of posts online I figured the combination tensor flow, Virtualenv en python 3.7 is not valid on Mac OSX.
I use python3.6 now and it is fine.
Sorry I can not provide more info, I'm broken by the struggle and want to sleep and forget about it."
22148,"Expected dimension in the range [-1, 1), but got 1 [[Node: accuracy_5/ArgMax_1 = ArgMax[T=DT_FLOAT, Tidx=DT_INT32, output_type=DT_INT64, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_y_label_0_1, accuracy_5/ArgMax/dimension)]]","I am working on luna16 dataset, I use tensorflow CNN code to classify images (cancer/not-cancer [0 or 1]), the code has no problem and runs without any error, but in session when I want to add summary `_, c, summary = sess.run([accuracy, cost, merged_summary], feed_dict={x_img: X, y_label: Y})` I get this error: InvalidArgumentError (see above for traceback): `Expected dimension in the range [-1, 1), but got 1 [[Node: accuracy_5/ArgMax_1 = ArgMax[T=DT_FLOAT, Tidx=DT_INT32, output_type=DT_INT64, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_y_label_0_1, accuracy_5/ArgMax/dimension)]]`
if I remove the merged_summary the code is works fine?
"
22146,TOCO fails to handle Dilated Convolution properly,"### System information
Have I written custom code (as opposed to using a stock example script provided in TensorFlow):N/A
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A
TensorFlow installed from (source or binary):source
TensorFlow version (use command below):1.10.0
Python version: 2.7.12
Bazel version (if compiling from source):0.15.0
GCC/Compiler version (if compiling from source):5.4.0
CUDA/cuDNN version:N/A
GPU model and memory:N/A
Exact command to reproduce:N/A

### Problem description
I have frozen_model.pb which works well.

I build toco with command:
`bazel build -c opt //tensorflow/contrib/lite/toco:toco`
 
Then i'm trying to convert model to TfLite:
`toco --allow_custom_ops --input_file='frozen_model.pb'  --output_file='wavenet.tflite' --input_arrays='original_input' --input_shapes='1,48000,1' --output_arrays='activation_2/Softmax' --dump_graphviz=graph --dump_graphviz_video=true`


and geting error:

```
2018-09-07 17:45:53.870648: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 412 operators, 674 arrays (0 quantized)
2018-09-07 17:45:53.876846: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 412 operators, 674 arrays (0 quantized)
2018-09-07 17:45:53.877237: I tensorflow/contrib/lite/toco/graph_transformations/identify_dilated_conv.cc:161] Identified sub-network emulating dilated convolution.
2018-09-07 17:45:53.877266: I tensorflow/contrib/lite/toco/graph_transformations/identify_dilated_conv.cc:209] Replaced with Dilated Conv2D op outputting ""dilated_conv_2_tanh/convolution/Conv2D"".
2018-09-07 17:45:53.877487: I tensorflow/contrib/lite/toco/graph_transformations/identify_dilated_conv.cc:161] Identified sub-network emulating dilated convolution.
2018-09-07 17:45:53.877512: I tensorflow/contrib/lite/toco/graph_transformations/identify_dilated_conv.cc:209] Replaced with Dilated Conv2D op outputting ""dilated_conv_2_sigm/convolution/Conv2D"".
2018-09-07 17:45:53.877852: F tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:116] Check failed: dim_x == dim_y (48002 vs. 48000)Dimensions must match
```


This is Dilated Convolution block in original frozen_model.pb visualized in tensorboard, it has input and output shape [?, 48000, 40]:

[?, 48000, 40] ->Pad->[?, 48002, 40]->SpaceToBatchND -> ExpandDims -> Conv2D -> Squeeze -> BatchToSpaceND->[?, 48000, 40]

![img2](https://user-images.githubusercontent.com/673872/45226776-67d4c200-b2c8-11e8-9886-1aa498559a00.png)


TOCO tries to substitute with
[?, 48000, 40] ->Pad->[?, 48002, 40]->Reshape -> Conv1x1/S -> Squeze->[?, 48002, 40]
![img3](https://user-images.githubusercontent.com/673872/45227203-cf8b0d00-b2c8-11e8-9092-7a4a94a731c0.png)

so ouput of resulted block is [?, 48002, 40] instead of [?, 48000, 40], it causes error in subsequent operations.

I managed to avoid this issue by removing line 102:
`transformations->Add(new IdentifyDilatedConv);`
from tensorflow/contrib/lite/toco/toco_tooling.cc and rebuilding TOCO,
but unfortunately this causes new problems, because some operations like SpaceToBatchND and BatchToSpaceND don't support 3D tensors in current TfLite verison.









"
22145,Depthwise convolution inside Dataset API throws data_format error,"### System information
-  Linux Ubuntu 16.04
- TensorFlow installed using: pip
- TensorFlow version: 1.10
- Python version: 3.6
- CUDA/cuDNN version: 9.0/7.2
- GPU model and memory: GeForce GTX 1080 ti

I tried to reproduce the error in a simpler fashion but couldn't manage it. Basically I use dataset API to load some patches from tfrecords and apply a synthethic blur on it (loaded from another tfrrecords) similar to this:
```
import tensorflow as tf
import numpy as np

def apply_blur(img):
    blur = np.random.rand(3,3,1,1)
    img = tf.nn.depthwise_conv2d(img[None], blur, [1,1,1,1], 'VALID')
    return img

tf.reset_default_graph()
dataset = tf.data.Dataset.from_tensor_slices(np.ones((10, 128, 128, 1)))
dataset = dataset.map(apply_blur, 2)

iterator = dataset.make_one_shot_iterator()
batch = iterator.get_next()

config = tf.ConfigProto()
config.gpu_options.allow_growth = True
with tf.Session(config=config) as sess:
    out = sess.run(batch)
```

My code is still working with tf v1.8, but not in higher versions. It says the data_format should be ""NCHW"" for 
Depthwise convolution on CPU, while in fact the data_format is ""NCHW"".

### Source code / logs
```
Traceback (most recent call last):
  File ""cli_deblurring.py"", line 95, in <module>
    cli()
  File ""/home/user/miniconda3/lib/python3.6/site-packages/click/core.py"", line 722, in __call__
    return self.main(*args, **kwargs)
  File ""/home/user/miniconda3/lib/python3.6/site-packages/click/core.py"", line 697, in main
    rv = self.invoke(ctx)
  File ""/home/user/miniconda3/lib/python3.6/site-packages/click/core.py"", line 895, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/home/user/miniconda3/lib/python3.6/site-packages/click/core.py"", line 535, in invoke
    return callback(*args, **kwargs)
  File ""cli_deblurring.py"", line 90, in cli
    model.train()
  File ""/home/user/Project/model/__init__.py"", line 159, in train
    self._train(sess)
  File ""/home/user/Project/model/deblurring.py"", line 318, in _train
    epoch, global_step = self._train_epoch(sess)
  File ""/home/user/Project/model/deblurring.py"", line 291, in _train_epoch
    sess.run(light_fetches, feed_dict=self.train_feed_dict)
  File ""/home/user/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 900, in run
    run_metadata_ptr)
  File ""/home/user/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/user/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1316, in _do_run
    run_metadata)
  File ""/home/user/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.UnimplementedError: Depthwise convolution on CPU is only supported for NHWC format
         [[Node: depthwise_3 = DepthwiseConv2dNative[T=DT_FLOAT, data_format=""NCHW"", dilations=[1, 1, 1, 1], padding=""VALID"", strides=[1, 1, 1, 1]](depthwise-0-TransposeNHWCToNCHW-LayoutOptimizer, strided_slice_4)]]
         [[Node: load_data/IteratorGetNext = IteratorGetNext[output_shapes=[[?,8,128,128,1], [?,128,128,1]], output_types=[DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](load_data/Iterator)]]
         [[Node: load_data/IteratorGetNext/_671 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_64_load_data/IteratorGetNext""
, tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
```
Any idea why this error could happen in higher versions?
"
22143,ValueError: Op type not registered 'NcclAllReduce' in binary running on 35712d892b7a.,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: v1.10.0-0-g656e7a2b34 1.10.0
- **Python version**: 3.5
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: based on nvidia-docker tensorflow/tensorflow:1.10.0-gpu-py3 image.
- **GPU model and memory**: Titan Xp 11GB 2 pcs.
- **Exact command to reproduce**: gen_nccl_ops.nccl_all_reduce( ... )

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Hi, I'm trying to implement multi-gpu tensorflow code.
I am trying to use **gen_nccl_ops.nccl_all_reduce** to sum values from each GPU, however, the code raises ValueError written below.
`ValueError: Op type not registered 'NcclAllReduce' in binary running on 35712d892b7a. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed. while building NodeDef 'Tower_0/Network/NcclAllReduce'
`
As suggested by the error message, I tried to access **tf.contrib.nccl** at the beginning of the whole code files.
```
import tensorflow as tf
tf.contrib.nccl

# ... Code continues ... #
```
But it doesn't work! 
I checked NCCL is installed using 
`export NCCL_DEBUG=VERSION`
, and got **NCCL version 2.2.13+cuda9.2**.

I'm using nvidia-docker image tensorflow/tensorflow:1.10.0-gpu-py3.
I confirmed **nccl.all_sum()** is working.

Is there anyway to import NCCL library before graph establishment?

Thanks.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
# Gradient Penalty on real distribution
self.gradient = tf.gradients(tf.reduce_sum(self.D[real][logits]), [self.x])[0]
self.slope_square = tf.reduce_sum(tf.square(self.gradient), reduction_indices=[1, 2, 3])

if self.tower_config is None:
    penalty = tf.reduce_mean(self.slope_square)
    self.gradient_penalty = self.C_gradient * penalty
else:
    # Utilize NCCL
    shared_name = tf.get_variable_scope().name.\
        replace(self.tower_config.name, self.tower_config.prefix.format(""NCCL""))
    penalty = gen_nccl_ops.nccl_all_reduce(
        input=self.slope_square,
        reduction=""sum"",
        num_devices=self.tower_config.num_devices,
        shared_name=shared_name
    ) / (1.0 / self.tower_config.num_devices)
    self.gradient_penalty = self.C_gradient * penalty
```
"
22142,name_scope has an effect on names of Variable create by tf.keras.layers.*,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.11.0-dev20180907
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

`name_scope` has an effect on names of `Variable`s created by `tf.keras.layers.*`. The following code got different result on tf 1.8 and tf >= 1.9. 

### Source code / logs

```python
import tensorflow as tf

with tf.name_scope('name_scope'):
    tf.keras.layers.Conv2D(3, 3, name='keras')(tf.ones([1, 3, 3, 1]))
    tf.layers.Conv2D(3, 3, name='legacy')(tf.ones([1, 3, 3, 1]))
print([v.name for v in tf.global_variables()])
```

On tf 1.11.0-dev20180907 it prints
```
['name_scope/keras/kernel:0', 
 'name_scope/keras/bias:0',
 'legacy/kernel:0', 
 'legacy/bias:0']
```

On tf 1.8 it prints
```
['keras/kernel:0',
 'keras/bias:0',
 'legacy/kernel:0',
 'legacy/bias:0']
```"
22141,Predictions from keras h5 file is different than its tflite version,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04.1 LTS
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.10.1
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: See source code

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem

I am following this video to convert my ML model to TensorFLow Lite:

https://www.youtube.com/watch?v=MZx1fhbL2q4

I noticed that if I am making predictions by reading the saved h5 file, the result is correct. This is not the case when I make predictions by reading the saved tflite file. I expect the predictions from both files to be the same.

### Source code / logs
```
import numpy as np
from tensorflow import keras
from tensorflow.contrib import lite

model = keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])
model.compile(optimizer='sgd', loss='mean_squared_error')

xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=np.float32)
ys = np.array([-3.0, -1.0, 0.0, 3.0, 5.0, 7.0], dtype=np.float32)

model.fit(xs, ys, epochs=500)

# Write out to keras save file
keras_file = 'linear.h5'
keras.models.save_model(model, keras_file)

# Convert the keras file to TensorFlow Lite
converter = lite.TocoConverter.from_keras_model_file(keras_file)
tflite_model = converter.convert()
open('linear.tflite', 'wb').write(tflite_model)

# input to predict
model_input = np.array([10.0], dtype=np.float32)

# make predictions from the h5 file
h5_model = keras.models.load_model(keras_file)
h5_prediction = h5_model.predict(model_input)

# make predictions fromm the tflite file
interpreter = lite.Interpreter(model_path='linear.tflite')
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
interpreter.set_tensor(input_details[0]['index'], [model_input])
interpreter.invoke()
tflite_prediction = interpreter.get_tensor(output_details[0]['index'])

# h5_prediction will always be around 19, which is correct
print('Prediction from h5 model: {}'.format(h5_prediction))

# tflite_prediction is some random value
print('Prediction from tflite model: {}'.format(tflite_prediction))
```
"
22140,Eager mode model.save will not save layer vars if the layer is in list of list,"tf 1.10.1 
like below model.save will not save CuDNNGRU vars if you use MyLayer in a tf.keras.Model.


    class MyLayer(keras.Model):
       def __init__(self):
        super(MyLayer, self).__init__()
          self.encodes =  [(keras.layers.CuDNNGRU(units=100, 
                                        return_sequences=True, 
                                        return_state=False, 
                                        recurrent_initializer='glorot_uniform'),)]

      def call(self, x):
        return self.encodes[0][0](x)"
22138,Array mul  lacking min/max data,"
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:N/A
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.10.1
- **Python version**:2.7.3
- **Bazel version (if compiling from source)**:0.12.0
- **GCC/Compiler version (if compiling from source)**:c++11
- **CUDA/cuDNN version**:7.5.18
- **GPU model and memory**:TITAN,12GB
- **Exact command to reproduce**:
 ./bazel-bin/tensorflow/contrib/lite/python/tflite_convert --output_file=~/foo.cc   --graph_def_file!/frozen_inference_graph.pb   --inference_type=QUANTIZED_UINT8   --input_arrays=Imageholder --output_arrays=ResizeBilinear_2   --mean_values=127.5   --std_dev_values=127.5


### Describe the problem

I modified deeplabv3+(mobilenetv2) model, and train/eval a quantized model with ""tf.contrib.quantize.create_training_graph(quant_delay=0)
 tf.contrib.quantize.create_eval_graph"". The graph show as below:


Placeholder->mul->sub->mobilenetv2



When I convert this frozen model to tflite quantized model, I use input_node = Placeholer. There are error occurred. Does any one know how to solve this problem?









### Source code / logs

2018-09-07 10:59:40.172018: F tensorflow/contrib/lite/toco/tooling_util.cc:1635] Array mul, which is an input to the Sub operator producing the output array sub, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.
tf.estimator package not installed.
Aborted (core dumped)


"
22137,ps0 will be OOM using MonitoredTrainingSession when workers too many,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 2.7.5
- **Bazel version (if compiling from source)**: 1.9
- **GCC/Compiler version (if compiling from source)**: 4.9.2
- **CUDA/cuDNN version**: CPU
- **GPU model and memory**: CPU
- **Exact command to reproduce**: Using MonitoredTrainingSession and thousands of workers.


### Describe the problem
My model is large and complex. Thousands of worker&ps are needed and MonitoredTrainingSession is used. In the stage of initialization, the memory of ps0 will raise up to 100G and then OOM, but other ps work correctly. Op device placement is tf.train.replica_device_setter and no other special strategy.

I open the log_device_placement and found that all ops related with report_uninitialized_xxx are placed on ps0. I think this is the root cause.

I change the code as PR #22136 , and then it works correctly.
"
22135,CopyFromGpuToHost and other operations can't be parallelized,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: no
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.8.0
- **Python version**:3.5
- **Bazel version (if compiling from source)**:0.10.0
- **GCC/Compiler version (if compiling from source)**:c++11
- **CUDA/cuDNN version**:9/7
- **GPU model and memory**:1080ti,11G
- **Exact command to reproduce**:N/A

### Describe the problem
Following the photo, why is the ""CopyFromGpuToHost"" function not working with other operations?
The operation can execute after sending data finish. Can they be parallelized?
Is this a limitation of gpu card? or Tensorflow no support?

![screenshot from 2018-09-07 10-20-43](https://user-images.githubusercontent.com/34928698/45194964-0ab91c00-b288-11e8-9564-e8071145ef27.png)
"
22134,tensorflow.python.framework.errors_impl.NotFoundError: No registered '_CopyFromGpuToHost' OpKernel for CPU devices compatible with node swap_out_gradients,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes. Added some custom TF operators. 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**:1.10.1
- **Python version**:3
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:9.2
- **GPU model and memory**: GeForce GTX 970
- **Exact command to reproduce**:python3 train/train.py --gpu 0 --model frustum_pointnets_v2 --log_dir train/log_v2 --num_point 1024 --max_epoch 201 --batch_size 24 --decay_step 800000 --decay_rate 0.5

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Getting an error that ""No registered '_CopyFromGpuToHost' OpKernel"". Not able to find the refernce to this OpKernel anywhere. 

### Source code / logs
pid: 16638
WARNING:tensorflow:From /home/rishi/sw/frustum-pointnets/models/pointnet_util.py:126: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:From /home/rishi/sw/frustum-pointnets/models/pointnet_util.py:212: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:From /home/rishi/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:519: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.
Instructions for updating:
`NHWC` for data_format is deprecated, use `NWC` instead
2018-09-06 19:00:38.848283: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-09-06 19:00:38.920714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-09-06 19:00:38.921052: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: 
name: GeForce GTX 970 major: 5 minor: 2 memoryClockRate(GHz): 1.1775
pciBusID: 0000:01:00.0
totalMemory: 3.94GiB freeMemory: 3.56GiB
2018-09-06 19:00:38.921067: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2018-09-06 19:00:39.115930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-09-06 19:00:39.115958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 
2018-09-06 19:00:39.115967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N 
2018-09-06 19:00:39.116154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3282 MB memory) -> physical GPU (device: 0, name: GeForce GTX 970, pci bus id: 0000:01:00.0, compute capability: 5.2)
**** EPOCH 000 ****
2018-09-06 19:00:42.022472
2018-09-06 19:00:42.305938: W tensorflow/core/framework/allocator.cc:108] Allocation of 50331648 exceeds 10% of system memory.
2018-09-06 19:00:42.322253: W tensorflow/core/framework/allocator.cc:108] Allocation of 50331648 exceeds 10% of system memory.
2018-09-06 19:00:42.337831: W tensorflow/core/framework/allocator.cc:108] Allocation of 100663296 exceeds 10% of system memory.
2018-09-06 19:00:42.368540: W tensorflow/core/framework/allocator.cc:108] Allocation of 100663296 exceeds 10% of system memory.
2018-09-06 19:00:42.401274: W tensorflow/core/framework/allocator.cc:108] Allocation of 50331648 exceeds 10% of system memory.
2018-09-06 19:00:52.106763: E tensorflow/core/common_runtime/executor.cc:697] Executor failed to create kernel. Not found: No registered '_CopyFromGpuToHost' OpKernel for CPU devices compatible with node swap_out_gradients/fa_layer3/ThreeInterpolate_grad/ThreeInterpolateGrad_2 = _CopyFromGpuToHost[T=DT_FLOAT, _class=[""loc@gradients/fa_layer3/ThreeInterpolate_grad/ThreeInterpolateGrad_2""], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](fa_layer3/truediv_2/_91)
	.  Registered:  device='GPU'

	 [[Node: swap_out_gradients/fa_layer3/ThreeInterpolate_grad/ThreeInterpolateGrad_2 = _CopyFromGpuToHost[T=DT_FLOAT, _class=[""loc@gradients/fa_layer3/ThreeInterpolate_grad/ThreeInterpolateGrad_2""], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](fa_layer3/truediv_2/_91)]]
Traceback (most recent call last):
  File ""/home/rishi/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1278, in _do_call
    return fn(*args)
  File ""/home/rishi/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1263, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/home/rishi/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1350, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.NotFoundError: No registered '_CopyFromGpuToHost' OpKernel for CPU devices compatible with node swap_out_gradients/fa_layer3/ThreeInterpolate_grad/ThreeInterpolateGrad_2 = _CopyFromGpuToHost[T=DT_FLOAT, _class=[""loc@gradients/fa_layer3/ThreeInterpolate_grad/ThreeInterpolateGrad_2""], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](fa_layer3/truediv_2/_91)
	.  Registered:  device='GPU'

	 [[Node: swap_out_gradients/fa_layer3/ThreeInterpolate_grad/ThreeInterpolateGrad_2 = _CopyFromGpuToHost[T=DT_FLOAT, _class=[""loc@gradients/fa_layer3/ThreeInterpolate_grad/ThreeInterpolateGrad_2""], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](fa_layer3/truediv_2/_91)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""train/train.py"", line 376, in <module>
    train()
  File ""train/train.py"", line 201, in train
    train_one_epoch(sess, ops, train_writer)
  File ""train/train.py"", line 256, in train_one_epoch
    feed_dict=feed_dict)
  File ""/home/rishi/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 877, in run
    run_metadata_ptr)
  File ""/home/rishi/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/rishi/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1272, in _do_run
    run_metadata)
  File ""/home/rishi/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: No registered '_CopyFromGpuToHost' OpKernel for CPU devices compatible with node swap_out_gradients/fa_layer3/ThreeInterpolate_grad/ThreeInterpolateGrad_2 = _CopyFromGpuToHost[T=DT_FLOAT, _class=[""loc@gradients/fa_layer3/ThreeInterpolate_grad/ThreeInterpolateGrad_2""], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](fa_layer3/truediv_2/_91)
	.  Registered:  device='GPU'

	 [[Node: swap_out_gradients/fa_layer3/ThreeInterpolate_grad/ThreeInterpolateGrad_2 = _CopyFromGpuToHost[T=DT_FLOAT, _class=[""loc@gradients/fa_layer3/ThreeInterpolate_grad/ThreeInterpolateGrad_2""], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](fa_layer3/truediv_2/_91)]]

"
22133,_beam_search_ops.so and PyFunc don't work with TF Serving (Was: In-Graph GatherTree),"Hi, i want to implement `tensorflow/contrib/seq2seq/python/ops/beam_search_ops.py >> gather_tree()`, because of `_beam_search_ops.so` and `PyFunc` are not worked in tf-serving, so is there any implementation of GatherTree by while_loop?"
22130,HLO serialization insufficiently validated on deserialization (e.g. for xrt),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 17.10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: bc7c47ccddbf351d17b0d2d61cde3d48e2d530d6
- **Python version**: N/A
- **Bazel version (if compiling from source)**: 0.11.1
- **GCC/Compiler version (if compiling from source)**: 7.2.0
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: See below 

### Describe the problem
The new xrt ops expose serialized HLO Snapshots as an interface over the distributed TF interface. Some validation of the serialization is performed and returned as an error to the TF client. However, other format errors lead to assertions instead, killing the TF server and requiring a restart. It would be desirable to instead validate the HLO more thoroughly and return any errors to the client.

cc @michaelisard 

### Source code / logs
I have four minimal example HLO modules that trigger assertions that I encountered when using xrt. Though I encountered them using `xrt`, I'll be using `//tensorflow/compiler/xla/tools:replay_computation_cpu` for easy reproducability in these examples. For each of these examples, I'll show the textual hlo (if dump_computation_to_text was able to process the pb), the .pbtext of the HloSnapshot, and the output of `replay_computation_cpu`. To reproduce,

```
cd tensorflow
protoc --encode=xla.HloSnapshot -I=$PWD $PWD/tensorflow/compiler/xrt/xrt.proto < x.pbtext > x.pb
./bazel-bin/tensorflow/compiler/xla/tools/replay_computation_cpu x.pb
```

where x.pbtext is the textual pb from this issue.

#### Add operation with missing parameters:
```
HloModule test

ENTRY comp {
  ROOT op = f64[] add()
}
```

```
hlo {
  hlo_module {
    name: ""test""
    entry_computation_name: ""op""
    computations {
      name: ""comp""
      instructions {
        name: ""op""
        opcode: ""add""
        shape {
          element_type: F64
          layout {
            format: DENSE
          }
        }
        id: 2
      }
      root_id: 2
    }
    program_shape {
      result {
        element_type: F64
        layout {
          format: DENSE
        }
      }
    }
  }
}
```

```
bazel-bin/tensorflow/compiler/xla/tools/replay_computation_cpu ~/XLAHacks.jl/op_missing_args.pb
2018-09-06 20:18:45.853017: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz
2018-09-06 20:18:45.854635: I tensorflow/compiler/xla/service/service.cc:149] XLA service 0x556b26f45230 executing computations on platform Host. Devices:
2018-09-06 20:18:45.854662: I tensorflow/compiler/xla/service/service.cc:157]   StreamExecutor device (0): <undefined>, <undefined>
2018-09-06 20:18:45.854726: I tensorflow/compiler/xla/tools/replay_computation.cc:272] Compiling 1 modules in parallel.
2018-09-06 20:18:45.855744: F tensorflow/compiler/xla/service/hlo_instruction.cc:1931] Check failed: 2 == operand_count() (2 vs. 0)
Aborted
```

#### Parameter reference when function has no parameters
```
HloModule test

ENTRY comp {
  ROOT op = f64[] parameter(0)
}
```

```
hlo {
  hlo_module {
    name: ""test""
    entry_computation_name: ""op""
    computations {
      name: ""comp""
      instructions {
        name: ""op""
        opcode: ""parameter""
        shape {
          element_type: F64
          layout {
            format: DENSE
          }
        }
        id: 2
      }
      root_id: 2
    }
    program_shape {
      result {
        element_type: F64
        layout {
          format: DENSE
        }
      }
    }
  }
}
```

```
2018-09-06 20:20:29.529757: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz
2018-09-06 20:20:29.531397: I tensorflow/compiler/xla/service/service.cc:149] XLA service 0x562efc8c8230 executing computations on platform Host. Devices:
2018-09-06 20:20:29.531425: I tensorflow/compiler/xla/service/service.cc:157]   StreamExecutor device (0): <undefined>, <undefined>
2018-09-06 20:20:29.531492: I tensorflow/compiler/xla/tools/replay_computation.cc:272] Compiling 1 modules in parallel.
Segmentation fault
```

#### Parameter reference out of bounds
Basically an add of parameters 1, 2 for a one parameter function (i.e. I had an off by one in my parameter numbers):

```
hlo {
  hlo_module {
    name: ""test""
    entry_computation_name: ""op""
    computations {
      name: ""comp""
      instructions {
        name: ""parameter0""
        opcode: ""parameter""
        shape {
          element_type: F64
          dimensions: 1
          layout {
            minor_to_major: 0
            format: DENSE
          }
        }
        parameter_number: 1
      }
      instructions {
        name: ""parameter1""
        opcode: ""parameter""
        shape {
          element_type: F64
          dimensions: 1
          layout {
            minor_to_major: 0
            format: DENSE
          }
        }
        parameter_number: 2
        id: 1
      }
      instructions {
        name: ""add2""
        opcode: ""add""
        shape {
          element_type: F32
          layout {
            format: DENSE
          }
        }
        id: 2
        operand_ids: 0
        operand_ids: 1
      }
      root_id: 2
    }
    program_shape {
      parameters {
        element_type: F64
        dimensions: 1
        layout {
          minor_to_major: 0
          format: DENSE
        }
      }
      parameters {
        element_type: F64
        dimensions: 1
        layout {
          minor_to_major: 0
          format: DENSE
        }
      }
      result {
        element_type: F32
        layout {
          format: DENSE
        }
      }
    }
  }
}
```

```
2018-09-06 20:25:29.866942: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz
2018-09-06 20:25:29.868569: I tensorflow/compiler/xla/service/service.cc:149] XLA service 0x560a8b2b7230 executing computations on platform Host. Devices:
2018-09-06 20:25:29.868598: I tensorflow/compiler/xla/service/service.cc:157]   StreamExecutor device (0): <undefined>, <undefined>
2018-09-06 20:25:29.868667: I tensorflow/compiler/xla/tools/replay_computation.cc:272] Compiling 1 modules in parallel.
2018-09-06 20:25:29.869118: F tensorflow/compiler/xla/service/hlo_computation.cc:78] Check failed: param_no >= 0 && param_no < parameter_count
ERROR: invalid parameter number.  Expected [0, 2), got 2
Aborted
```

#### Missing dot dimension numbers
```
HloModule test

ENTRY comp {
  parameter0 = f64[2,2]{0,1} parameter(0)
  parameter1 = f64[2,2]{0,1} parameter(1)
  ROOT dot2 = f64[1]{0} dot(parameter0, parameter1)
}
```

```
hlo {
  hlo_module {
    name: ""test""
    entry_computation_name: ""op""
    computations {
      name: ""comp""
      instructions {
        name: ""parameter0""
        opcode: ""parameter""
        shape {
          element_type: F64
          dimensions: 2
          dimensions: 2
          layout {
            minor_to_major: 0
            minor_to_major: 1
            format: DENSE
          }
        }
      }
      instructions {
        name: ""parameter1""
        opcode: ""parameter""
        shape {
          element_type: F64
          dimensions: 2
          dimensions: 2
          layout {
            minor_to_major: 0
            minor_to_major: 1
            format: DENSE
          }
        }
        parameter_number: 1
        id: 1
      }
      instructions {
        name: ""dot2""
        opcode: ""dot""
        shape {
          element_type: F64
          dimensions: 1
          layout {
            minor_to_major: 0
            format: DENSE
          }
        }
        id: 2
        operand_ids: 0
        operand_ids: 1
      }
      root_id: 2
    }
    program_shape {
      parameters {
        element_type: F64
        dimensions: 2
        dimensions: 2
        layout {
          minor_to_major: 0
          minor_to_major: 1
          format: DENSE
        }
      }
      parameters {
        element_type: F64
        dimensions: 2
        dimensions: 2
        layout {
          minor_to_major: 0
          minor_to_major: 1
          format: DENSE
        }
      }
      result {
        element_type: F64
        dimensions: 1
        layout {
          minor_to_major: 0
          format: DENSE
        }
      }
    }
  }
}
```

```
2018-09-06 20:30:04.934720: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz
2018-09-06 20:30:04.936398: I tensorflow/compiler/xla/service/service.cc:149] XLA service 0x55fd5deeb230 executing computations on platform Host. Devices:
2018-09-06 20:30:04.936426: I tensorflow/compiler/xla/service/service.cc:157]   StreamExecutor device (0): <undefined>, <undefined>
2018-09-06 20:30:04.936502: I tensorflow/compiler/xla/tools/replay_computation.cc:272] Compiling 1 modules in parallel.
2018-09-06 20:30:04.937560: F ./tensorflow/compiler/xla/service/hlo_instruction.h:1105] Check failed: dot_dimension_numbers_ != nullptr
Aborted
```"
22129,[Feature Request] Exporting TF-Hub for the Best Model,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: N/A
- **TensorFlow version (use command below)**: N/A
- **Python version**: N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
I believe that it might be helpful to have `BestExporter` equivalent for `tf-hub` too. Currently, there is no way to export hub corresponding to the bet model. The easiest way to achieve this (or at least the way I did) is to trigger the `LatestHubExporter` within the `export` of `BestExporter`.

Another design proposal:
The exporters can be chained. However, I do not know of any other use cases (other than the one above)."
22128,[Feature Request] retrieving exact node names from multiple outputs (`tf.nn.top_k`),"### System information
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **TensorFlow version (use command below)**: '1.9.0-rc1'  I also believe it happens r1.10
- **Python version**: 2.7 
- **CUDA/cuDNN version**: 7.1
- **Bazel version**: 0.16.1
- **GPU model and memory** : TitanV 12 G
- **Exact command to reproduce**: I added python sample code 
- **Mobile device**:  n/a



### Describe the problem
When a tensor op returns multiple outputs such as  `tf.nn.top_k`, 
it would be great to retrieve exact individual input nodes of its output recursively. 

As addressed here, https://www.tensorflow.org/api_docs/python/tf/nn/top_k, 
`tf.nn.top_k` returns 
- values: The k largest elements along each last dimensional slice.
- indices: The indices of values within the last dimension of input.

With below sample code, 
'Namespace/TopKV2' returns two outputs: 'Namespace/probabilities', 'Namespace/indices'
(From the two output nodes, )
'Namespace/probabilities' takes 'Namespace/TopKV2' **values** as input, 
'Namespace/indices' takes  'Namespace/TopKV2' **indices** as input. 

However, only 'Namespace/TopKV2' can be retrieved from the two output nodes. 
I also checked graph structure on tensorboard; it seems that same tensor/node (i.e., 'Namespace/TopKV2') goes to 'Namespace/probabilities' and  'Namespace/indices', both, however, one is values and, the other indices. 

If differentiable names for multiple outputs are used, that would be great. 
Or, if there is workaround, please advise me. 
This issue also causes key error in TensorRT graphsurgeon module. 

```
def _map_nodes(nodes):
    return {node.name: node for node in nodes}

def main():
   # simple network 
    with tf.variable_scope(""Namespace""):
        inputs = tf.placeholder(tf.float32, [1, 3, 16, 16], name=""input"")
        convolved = tf.layers.conv2d(inputs, 4, [1, 1], data_format='channels_first')
        reshaped = tf.reshape(convolved, [1, 256, 4])
        best_probabilities, best_indices = tf.nn.top_k(reshaped, k=1)
        output1 = tf.reshape(best_probabilities, [256], name='probabilities')
        output2 = tf.reshape(best_indices, [256], name='indices')

    graph = tf.get_default_graph()
    graphdef = tf.get_default_graph().as_graph_def()

    # Freeze the graph.
    output_node_names = ['Namespace/probabilities', 'Namespace/indices']
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        frozen_graph = tf.graph_util.convert_variables_to_constants(
            sess, graphdef, output_node_names
        )
        # Remove training nodes.
        infer_graph = tf.graph_util.remove_training_nodes(frozen_graph)
        graph_io.write_graph(infer_graph, ""./"", ""test.pb"", as_text=False)
        node_map = _map_nodes(infer_graph.node)
        for node in infer_graph.node:
            for input_name in node.input:
                # Recursively check against all of this node's inputs
                input_node = node_map[input_name]

if __name__ == '__main__':
    main()

```
I also attached TF board events file. 
https://drive.google.com/file/d/1HdV_eqMloqXgLenZSZHmKwME_oSnJbFb/view?usp=sharing "
22124,"python -c ""import tensorflow as tf"" fails when run in Dockerfile, works when inside container.","

So I can import tensorflow inside the docker container that I've pulled from DockerHub, but not in the Dockerfile.
```
thomas@fw0013591:~/code/tf_docker_bug$ nvidia-docker build .
Sending build context to Docker daemon  2.048kB
Step 1/2 : FROM tensorflow/tensorflow:latest-devel-gpu
 ---> f73df9e66ebb
Step 2/2 : RUN python -c ""import tensorflow as tf""
 ---> Running in 18e621bcd3b1
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: libcuda.so.1: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
The command '/bin/sh -c python -c ""import tensorflow as tf""' returned a non-zero code: 1
thomas@fw0013591:~/code/tf_docker_bug$ nano Dockerfile
thomas@fw0013591:~/code/tf_docker_bug$ nvidia-docker build .
Sending build context to Docker daemon  2.048kB
Step 1/1 : FROM tensorflow/tensorflow:latest-devel-gpu
 ---> f73df9e66ebb
Successfully built f73df9e66ebb
thomas@fw0013591:~/code/tf_docker_bug$ nvidia-docker run -it f73df9e66ebb /bin/bash
root@68ed17426105:~# python -c ""import tensorflow as tf""
root@68ed17426105:~# 
```

The dockerfile is incredibly simple. Looks like this:
```
FROM tensorflow/tensorflow:latest-devel-gpu
RUN python -c ""import tensorflow as tf""
```

Is this behavior to be expected?"
22123,Incorrect results on GPU for reduce_* methods on large tensors,"-----------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.9.0 / 1.5.0 / 1.10.1
- **Python version**: 2.7.15 / 3.6.3
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: CUDA 9.0
- **GPU model and memory**: K10, P100
- **Exact command to reproduce**: see code below

### Describe the problem
For tensors with large amount of values (2^28 seems to be the threshold) a few of the reduce_* (tested max, sum, min) methods don't work correctly. The incorrect values appear to be read randomly from memory. The issue was only observed on GPU. Other observations are that a tensor with int types works as expected. Also no problem if the tensor is read as tf.constant. 
These posts seem to be related:
https://github.com/tensorflow/tensorflow/issues/20094
https://stackoverflow.com/questions/49520394/tf-reduce-sum-on-gpu-fails-in-combination-with-placeholder-as-input-shape
https://stackoverflow.com/questions/50806742/tensorflow-reduce-sum-returns-partially-incorrect-output-given-a-large-input

### Source code / logs
https://colab.research.google.com/drive/1GNqzUfRTRRohyjiF8HqvWzvKE6LasQ-4

```
import tensorflow as tf
import numpy as np

n = np.random.randint(1,9)
inp = np.ones(shape=(140000000, 2)) * n
tensor = tf.placeholder(tf.float32, shape=(None, None))
output = tf.reduce_max(tensor, axis=-1)

with tf.Session() as sess:
    result = sess.run(output, feed_dict={tensor: inp})
    print('output values')
    print(result)
    print('number of incorrect values')
    print(np.sum(result != inp.max(axis=-1)))
```
Output:
```
output values
[5. 5. 5. ... 0. 0. 0.]
number of incorrect values
5782272
```

Pinging:
@nuance-research"
22121,Unable to profile tf.keras Model. Am I missing something or is it a known issue?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: RHEL 7
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA
- **TensorFlow installed from (source or binary)**: No
- **TensorFlow version (use command below)**: 1.9
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**: NA
- **Exact command to reproduce**: Below code

### Describe the problem
**tf.contrib.tfprof** does not profile tf.keras Model. Am I missing something or is it a known issue?

### Source code / logs
The goal is to find what % of total time is spent in the input pipeline (mostly IO) and what % of total time is spent on model training  ideally a chart like the one presented in the [Training Performance session](https://www.youtube.com/watch?v=SxOsJPaxHME&feature=youtu.be&t=977) at tf dev summit 2018.

**Model Training:**
```python
training_set = tfdata_generator_with_interleave([f1.avro, f2.avro, ], is_training=True, batch_size=512, preprocess_fn=preprocess_fn_dense)
validation_set = tfdata_generator_with_interleave([f1_val.avro, f2_val.avro, ], is_training=False, batch_size=512, preprocess_fn=preprocess_fn_dense)
model = my_tf_keras_dense_deep_model()
model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])

with tf.contrib.tfprof.ProfileContext( './profiles/dense_model) ) as pctx:
   model.fit(
                training_set.make_one_shot_iterator(),
                steps_per_epoch=TOTAL_TRAIN_RECORDS // _BATCH_SIZE, # need this as we are feeding data using a generator
                epochs=_EPOCHS,
                validation_data= validation_set.make_one_shot_iterator(),
                validation_steps=TOTAL_TEST_RECORDS // _BATCH_SIZE,
                callbacks=[tensorboard])
```

**Input Pipeline:**
```python
def tfdata_generator_with_interleave(data_files, is_training, batch_size=128, preprocess_fn=preprocess_fn_dense):
    def wrap_generator(filename):
        return tf.data.Dataset.from_generator(read_local_avro_data_gen, (tf.float32, tf.float64),
                                              args=[filename, batch_size / 4])

    files = tf.data.Dataset.from_tensor_slices(data_files)
    dataset = files.apply(tf.contrib.data.parallel_interleave(wrap_generator, cycle_length=4, sloppy=True))
    dataset = dataset.flat_map(lambda *x: tf.data.Dataset.from_tensor_slices(x))

    if is_training:
        dataset = dataset.apply(tf.contrib.data.shuffle_and_repeat(1000, -1, 42))

    dataset = dataset.apply(tf.contrib.data.map_and_batch(preprocess_fn, batch_size,
                                                          num_parallel_batches=num_parallel_data_fetch,
                                                          drop_remainder=True if is_training else False)
                            )

    dataset = dataset.prefetch(tf.contrib.data.AUTOTUNE)
    return dataset
```

But I do not see any files to load into tensorboard or chrome in ./profiles directory. Does the profile not work with tf.keras models? Or Am I missing any setup?

Also tried contacting the email address mentioned on the [Profiler README](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/README.md) - xpan@google.com, but message sending failed.
"
22119,XLA commonly aborts with nvidia error,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.10
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: 0.15
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.2 / 7.2.1.38
- **GPU model and memory**: Titan
- **Exact command to reproduce**: N/A

### Describe the problem
When running XLA via the session config we commonly hit the following error:

`2018-09-06 17:28:38.814423: F tensorflow/stream_executor/cuda/cuda_dnn.cc:211] Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream.
`

However, sometimes we get further than this error and see significant speed up with XLA and the loss decreasing.

What could be causing this error?

The docker image is located at: https://hub.docker.com/r/joeyearsley/tf-builds/

I can't diverge the exact script, however I do have the following:
- NCCL for multi-gpu reduction like TF-Benchmark's Variable Manager
- `tf.data` APIs including the string handles to alter between train and validation datasets.

"
22118,"tf.flags.FLAGS bug for verison 1.10.0, got the message of  UnrecognizedFlagError: Unknown command line flag 'f'","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22117,Bad output for sqrt?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04

- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: no
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:
v1.10.1-0-g4dcfddc5d1 1.10.1
I used pip3 install tensorflow-gpu
- **Python version**:Python 3.5.2 (default, Nov 23 2017, 16:37:01) 
[GCC 5.4.0 20160609] on linux
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
9.0.176/7.0.5
- **GPU model and memory**:GeForce GTX 1050 Ti/PCIe/SSE2 4GB
- **Exact command to reproduce**:
see below

### Describe the problem
If i run the below script with tf 1.10.1 it erroneously prints:
[0.58032876 1.1248689  0.685996  ] [1.1606575 2.2497377 1.371992 ]
While when using tf 1.8 it outputs the mathematically correct:
[0.58032876 1.1248689  0.685996  ] [0.8615806  0.44449627 0.72886723]

### Source code / logs
```
import tensorflow as tf

def test_unary_sqrt_with_gradients():
    floats = tf.placeholder(dtype=tf.float32, shape=[3], name='floats')
    output0 = tf.sqrt(floats)
    const1 = tf.constant(value=[1.0], dtype=tf.float32, shape=[3])
    mul1 = tf.multiply(const1, y=0.5)
    grad_floats = tf.divide(mul1, output0)

    return [grad_floats, output0]

grad, out = test_unary_sqrt_with_gradients()
with tf.Session() as sess:
    [grad_, out_] = sess.run([grad, out], feed_dict={'floats:0': [0.7423212,  0.19757693, 0.53124744]})
    print(grad_, out_)

```"
22116,[1.10.1] op_level_cost_estimator.cc:404] Check failed: 0 < gflops (0 vs. 0),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: debian experimental
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: no
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.10.1
- **Python version**: not related
- **Bazel version (if compiling from source)**: not related
- **GCC/Compiler version (if compiling from source)**: gcc8
- **CUDA/cuDNN version**: not related
- **GPU model and memory**: not related
- **Exact command to reproduce**:

### Describe the problem

I built tensorflow from source with a customized build system instead of bazel. https://salsa.debian.org/science-team/tensorflow . I can successfully build libtensorflow_cc.so according to the bazel query result. However when running `benchmark_model` test I encountered this problem:

```
$ LD_LIBRARY_PATH=. ./tf_benchmark_model --graph=../tensorflow_inception_graph.pb --num_threads=1
2018-09-06 15:45:23.316799: I tensorflow/tools/benchmark/benchmark_model.cc:469] Graph: [../tensorflow_inception_graph.pb.pb]
2018-09-06 15:45:23.316843: I tensorflow/tools/benchmark/benchmark_model.cc:470] Init ops:
2018-09-06 15:45:23.316848: I tensorflow/tools/benchmark/benchmark_model.cc:471] Input layers: [input:0]
2018-09-06 15:45:23.316852: I tensorflow/tools/benchmark/benchmark_model.cc:472] Input shapes: [1,224,224,3]
2018-09-06 15:45:23.316855: I tensorflow/tools/benchmark/benchmark_model.cc:473] Input types: [float]
2018-09-06 15:45:23.316859: I tensorflow/tools/benchmark/benchmark_model.cc:474] Output layers: [output:0]
2018-09-06 15:45:23.316862: I tensorflow/tools/benchmark/benchmark_model.cc:475] Target layers: []
2018-09-06 15:45:23.316868: I tensorflow/tools/benchmark/benchmark_model.cc:476] Num runs: [1000]
2018-09-06 15:45:23.316874: I tensorflow/tools/benchmark/benchmark_model.cc:477] Inter-inference delay (seconds): [-1.0]
2018-09-06 15:45:23.316898: I tensorflow/tools/benchmark/benchmark_model.cc:478] Inter-benchmark delay (seconds): [-1.0]
2018-09-06 15:45:23.316903: I tensorflow/tools/benchmark/benchmark_model.cc:480] Num threads: [1]
2018-09-06 15:45:23.316907: I tensorflow/tools/benchmark/benchmark_model.cc:481] Benchmark name: []
2018-09-06 15:45:23.316911: I tensorflow/tools/benchmark/benchmark_model.cc:482] Output prefix: []
2018-09-06 15:45:23.316915: I tensorflow/tools/benchmark/benchmark_model.cc:483] Show sizes: [0]
2018-09-06 15:45:23.316932: I tensorflow/tools/benchmark/benchmark_model.cc:484] Warmup runs: [1]
2018-09-06 15:45:23.316937: I tensorflow/tools/benchmark/benchmark_model.cc:251] Loading TensorFlow.
2018-09-06 15:45:23.316950: I tensorflow/tools/benchmark/benchmark_model.cc:258] Got config, 0 devices
2018-09-06 15:45:23.317031: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
2018-09-06 15:45:23.380132: I tensorflow/tools/benchmark/benchmark_model.cc:496] Initialized session in 0.063179s
2018-09-06 15:45:23.380206: I tensorflow/tools/benchmark/benchmark_model.cc:327] Running benchmark for max 1 iterations, max -1 seconds without detailed stat logging, with -1s sleep between inferences
2018-09-06 15:45:23.516527: F tensorflow/core/grappler/costs/op_level_cost_estimator.cc:404] Check failed: 0 < gflops (0 vs. 0)type: ""CPU""
vendor: ""GenuineIntel""
model: ""110""
num_cores: 4
environment {
  key: ""cpu_instruction_set""
  value: ""SSE, SSE2""
}
environment {
  key: ""eigen""
  value: ""3.3.90""
}
l1_cache_size: 32768
l2_cache_size: 262144
l3_cache_size: 6291456
memory_size: 8558383104

Aborted
```

the `tensorflow_inception_graph.pb` files comes from `tensorflow/contrib/makefile/README.md`

### Source code / logs

https://salsa.debian.org/science-team/tensorflow

For error log see above.
"
22115,Feature Request: fp16 support for tf.contrib.image.transform on gpu,"### System information
- **Have I written custom code**: yes
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **Mobile device**: n/a
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: ('v1.10.0-0-g656e7a2b34', '1.10.0')
- **Python version**: Python 2.7.12
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**:  9.0  /  7.1.4.18-1
- **GPU model and memory**: GeForce GTX 1080 8097MB
- **Exact command to reproduce**:

### Describe the problem
`tf.contrib.image.transform` does not support fp16 on GPU. 

### Source code / logs
```python
import tensorflow as tf

session_config = tf.ConfigProto(
    allow_soft_placement=False,
    log_device_placement=True)

with tf.device(""/device:GPU:0""):
  images =tf.random_normal([512,512], dtype=tf.float16)
  y = tf.contrib.image.transform(images,[1]*8)
  with tf.Session(config=session_config) as sess:
    print(sess.run(y))
```
 
```
Caused by op u'transform/ImageProjectiveTransform', defined at:
  File ""./test.py"", line 11, in <module>
    y = tf.contrib.image.transform(images,[1]*8)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/image/python/ops/image_ops.py"", line 269, in transform
    images, transforms, interpolation=interpolation.upper())
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/image/ops/gen_image_ops.py"", line 235, in image_projective_transform
    interpolation=interpolation, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py"", line 454, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3155, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'transform/ImageProjectiveTransform': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
Registered kernels:
  device='GPU'; dtype in [DT_DOUBLE]
  device='GPU'; dtype in [DT_FLOAT]
  device='GPU'; dtype in [DT_INT64]
  device='GPU'; dtype in [DT_INT32]
  device='GPU'; dtype in [DT_UINT8]
  device='CPU'; dtype in [DT_DOUBLE]
  device='CPU'; dtype in [DT_FLOAT]
  device='CPU'; dtype in [DT_HALF]
  device='CPU'; dtype in [DT_INT64]
  device='CPU'; dtype in [DT_INT32]
  device='CPU'; dtype in [DT_UINT8]

	 [[Node: transform/ImageProjectiveTransform = ImageProjectiveTransform[dtype=DT_HALF, interpolation=""NEAREST"", _device=""/device:GPU:0""](transform/strided_slice, transform/strided_slice_1)]]
```
"
22113,Build of tensorflow r1.11 fails on ubuntu 18.04 (r1.10 was OK),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: r1.11
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: 0.16.1
- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 7.3.0-16ubuntu3) 7.3.0
- **CUDA/cuDNN version**: 9.2/7.2.1
- **GPU model and memory**: NVIDIA GeForce 940MX
- **Exact command to reproduce**: bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures

### Describe the problem
While building the tensorflow from the branch r1.11 with GPU support, the build fails with the following error:
ERROR: ~/Documents/dev/git/tensorflow/tensorflow/contrib/verbs/BUILD:90:1: C++ compilation of rule '//tensorflow/contrib/verbs:rdma_rendezvous_mgr' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command

### Source code / logs
The following command line is available:
INFO: From Compiling tensorflow/compiler/xla/service/gpu/outfeed_manager.cc [for host]:
In file included from ./tensorflow/core/platform/default/logging.h:24:0,
                 from ./tensorflow/core/platform/logging.h:25,
                 from ./tensorflow/core/lib/core/status.h:25,
                 from ./tensorflow/compiler/xla/status.h:19,
                 from ./tensorflow/compiler/xla/array.h:33,
                 from ./tensorflow/compiler/xla/array2d.h:29,
                 from ./tensorflow/compiler/xla/literal.h:31,
                 from ./tensorflow/compiler/xla/service/gpu/outfeed_manager.h:19,
                 from tensorflow/compiler/xla/service/gpu/outfeed_manager.cc:16:
./tensorflow/core/platform/default/logging.h: In instantiation of 'std::__cxx11::string* tensorflow::internal::Check_LEImpl(const T1&, const T2&, const char*) [with T1 = long long int; T2 = long unsigned int; std::__cxx11::string = std::__cxx11::basic_string<char>]':
./tensorflow/compiler/xla/shape_util.h:117:5:   required from here
./tensorflow/core/platform/default/logging.h:232:35: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
 TF_DEFINE_CHECK_OP_IMPL(Check_LE, <=)
./tensorflow/core/platform/macros.h:88:49: note: in definition of macro 'TF_PREDICT_TRUE'
 #define TF_PREDICT_TRUE(x) (__builtin_expect(!!(x), 1))
                                                 ^
./tensorflow/core/platform/default/logging.h:232:1: note: in expansion of macro 'TF_DEFINE_CHECK_OP_IMPL'
 TF_DEFINE_CHECK_OP_IMPL(Check_LE, <=)
 ^~~~~~~~~~~~~~~~~~~~~~~
ERROR: ~/Documents/dev/git/tensorflow/tensorflow/contrib/verbs/BUILD:90:1: C++ compilation of rule '//tensorflow/contrib/verbs:rdma_rendezvous_mgr' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd ~/.cache/bazel/_bazel_~/cf67b2b2e967476eb2b1ee98e33ab5bd/execroot/org_tensorflow && \
  exec env - \
    PATH=~/bin:/usr/local/sbin:/usr/local/lib:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/linuxbrew/.linuxbrew/bin:/home/linuxbrew/.linuxbrew/sbin:/home/linuxbrew/.linuxbrew/opt/coreutils/libexec/gnubin:/usr/local/cuda/bin:/usr/local/share/apache/hadoop/sbin:/usr/local/share/apache/hadoop/bin:/usr/local/share/apache/spark/sbin:/usr/local/share/apache/spark/bin:/usr/games:/usr/local/games:~/bin:/snap/bin \
    PWD=/proc/self/cwd \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/tensorflow/contrib/verbs/_objs/rdma_rendezvous_mgr/rdma_rendezvous_mgr.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/contrib/verbs/_objs/rdma_rendezvous_mgr/rdma_rendezvous_mgr.pic.o' '-DGRPC_ARES=0' '-DPB_FIELD_16BIT=1' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -DTENSORFLOW_USE_JEMALLOC -DTF_USE_SNAPPY -DTENSORFLOW_USE_VERBS -DTENSORFLOW_USE_GDR -DCURL_STATICLIB -DPLATFORM_LINUX -DENABLE_CURL_CLIENT -DENABLE_NO_ENCRYPTION -iquote . -iquote bazel-out/host/genfiles -iquote bazel-out/host/bin -iquote external/protobuf_archive -iquote bazel-out/host/genfiles/external/protobuf_archive -iquote bazel-out/host/bin/external/protobuf_archive -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -iquote bazel-out/host/bin/external/bazel_tools -iquote external/grpc -iquote bazel-out/host/genfiles/external/grpc -iquote bazel-out/host/bin/external/grpc -iquote external/zlib_archive -iquote bazel-out/host/genfiles/external/zlib_archive -iquote bazel-out/host/bin/external/zlib_archive -iquote external/boringssl -iquote bazel-out/host/genfiles/external/boringssl -iquote bazel-out/host/bin/external/boringssl -iquote external/com_google_absl -iquote bazel-out/host/genfiles/external/com_google_absl -iquote bazel-out/host/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/host/genfiles/external/nsync -iquote bazel-out/host/bin/external/nsync -iquote external/jemalloc -iquote bazel-out/host/genfiles/external/jemalloc -iquote bazel-out/host/bin/external/jemalloc -iquote external/eigen_archive -iquote bazel-out/host/genfiles/external/eigen_archive -iquote bazel-out/host/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/host/genfiles/external/local_config_sycl -iquote bazel-out/host/bin/external/local_config_sycl -iquote external/gif_archive -iquote bazel-out/host/genfiles/external/gif_archive -iquote bazel-out/host/bin/external/gif_archive -iquote external/jpeg -iquote bazel-out/host/genfiles/external/jpeg -iquote bazel-out/host/bin/external/jpeg -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/genfiles/external/com_googlesource_code_re2 -iquote bazel-out/host/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/genfiles/external/farmhash_archive -iquote bazel-out/host/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/genfiles/external/fft2d -iquote bazel-out/host/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/genfiles/external/highwayhash -iquote bazel-out/host/bin/external/highwayhash -iquote external/local_config_cuda -iquote bazel-out/host/genfiles/external/local_config_cuda -iquote bazel-out/host/bin/external/local_config_cuda -iquote external/double_conversion -iquote bazel-out/host/genfiles/external/double_conversion -iquote bazel-out/host/bin/external/double_conversion -iquote external/curl -iquote bazel-out/host/genfiles/external/curl -iquote bazel-out/host/bin/external/curl -iquote external/jsoncpp_git -iquote bazel-out/host/genfiles/external/jsoncpp_git -iquote bazel-out/host/bin/external/jsoncpp_git -iquote external/aws -iquote bazel-out/host/genfiles/external/aws -iquote bazel-out/host/bin/external/aws -isystem external/protobuf_archive/src -isystem bazel-out/host/genfiles/external/protobuf_archive/src -isystem bazel-out/host/bin/external/protobuf_archive/src -isystem external/grpc/include -isystem bazel-out/host/genfiles/external/grpc/include -isystem bazel-out/host/bin/external/grpc/include -isystem external/zlib_archive -isystem bazel-out/host/genfiles/external/zlib_archive -isystem bazel-out/host/bin/external/zlib_archive -isystem external/grpc/third_party/address_sorting/include -isystem bazel-out/host/genfiles/external/grpc/third_party/address_sorting/include -isystem bazel-out/host/bin/external/grpc/third_party/address_sorting/include -isystem external/boringssl/src/include -isystem bazel-out/host/genfiles/external/boringssl/src/include -isystem bazel-out/host/bin/external/boringssl/src/include -isystem external/nsync/public -isystem bazel-out/host/genfiles/external/nsync/public -isystem bazel-out/host/bin/external/nsync/public -isystem external/jemalloc/include -isystem bazel-out/host/genfiles/external/jemalloc/include -isystem bazel-out/host/bin/external/jemalloc/include -isystem external/eigen_archive -isystem bazel-out/host/genfiles/external/eigen_archive -isystem bazel-out/host/bin/external/eigen_archive -isystem external/gif_archive/lib -isystem bazel-out/host/genfiles/external/gif_archive/lib -isystem bazel-out/host/bin/external/gif_archive/lib -isystem external/farmhash_archive/src -isystem bazel-out/host/genfiles/external/farmhash_archive/src -isystem bazel-out/host/bin/external/farmhash_archive/src -isystem external/com_google_absl -isystem bazel-out/host/genfiles/external/com_google_absl -isystem bazel-out/host/bin/external/com_google_absl -isystem external/local_config_cuda/cuda -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda -isystem bazel-out/host/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include/crt -isystem external/double_conversion -isystem bazel-out/host/genfiles/external/double_conversion -isystem bazel-out/host/bin/external/double_conversion -isystem external/curl/include -isystem bazel-out/host/genfiles/external/curl/include -isystem bazel-out/host/bin/external/curl/include -isystem external/jsoncpp_git/include -isystem bazel-out/host/genfiles/external/jsoncpp_git/include -isystem bazel-out/host/bin/external/jsoncpp_git/include -isystem external/aws/aws-cpp-sdk-core/include -isystem bazel-out/host/genfiles/external/aws/aws-cpp-sdk-core/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-core/include -isystem external/aws/aws-cpp-sdk-kinesis/include -isystem bazel-out/host/genfiles/external/aws/aws-cpp-sdk-kinesis/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-kinesis/include -isystem external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/host/genfiles/external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-s3/include '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 -g0 -c tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc -o bazel-out/host/bin/tensorflow/contrib/verbs/_objs/rdma_rendezvous_mgr/rdma_rendezvous_mgr.pic.o)
In file included from ./tensorflow/core/framework/common_shape_fns.h:22:0,
                 from ./tensorflow/core/framework/resource_mgr.h:24,
                 from ./tensorflow/core/common_runtime/device.h:43,
                 from ./tensorflow/core/common_runtime/device_mgr.h:24,
                 from ./tensorflow/core/distributed_runtime/worker_session.h:21,
                 from ./tensorflow/core/distributed_runtime/base_rendezvous_mgr.h:24,
                 from ./tensorflow/contrib/verbs/rdma_rendezvous_mgr.h:22,
                 from tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc:18:
./tensorflow/core/util/tensor_format.h: In function 'tensorflow::TensorShape tensorflow::ShapeFromFormat(tensorflow::TensorFormat, tensorflow::int64, tensorflow::gtl::ArraySlice<long long int>, tensorflow::int64)':
./tensorflow/core/util/tensor_format.h:501:45: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (format == FORMAT_NHWC_VECT_W && dim == spatial.size() - 1) {
                                         ~~~~^~~~~~~~~~~~~~~~~~~~~
tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc: In member function 'virtual void tensorflow::RdmaRemoteRendezvous::RecvFromRemoteAsync(const tensorflow::Rendezvous::ParsedKey&, const tensorflow::Rendezvous::Args&, tensorflow::Rendezvous::DoneCallback)':
tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc:66:41: error: 'using StringPiece = class absl::string_view {aka class absl::string_view}' has no member named 'ToString'
   string key(std::move(parsed.FullKey().ToString()));
                                         ^~~~~~~~
In file included from ./tensorflow/core/platform/default/logging.h:24:0,
                 from ./tensorflow/core/platform/logging.h:25,
                 from ./tensorflow/core/lib/core/refcount.h:22,
                 from ./tensorflow/core/platform/tensor_coding.h:21,
                 from ./tensorflow/core/framework/resource_handle.h:19,
                 from ./tensorflow/core/framework/types.h:31,
                 from ./tensorflow/contrib/verbs/verbs_util.h:21,
                 from ./tensorflow/contrib/verbs/rdma.h:30,
                 from ./tensorflow/contrib/verbs/rdma_mgr.h:24,
                 from ./tensorflow/contrib/verbs/rdma_rendezvous_mgr.h:21,
                 from tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc:18:
./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetTensorDim(tensorflow::gtl::ArraySlice<T>, tensorflow::TensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':
./tensorflow/core/util/tensor_format.h:452:47:   required from here
./tensorflow/core/util/tensor_format.h:420:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   CHECK(index >= 0 && index < dimension_attributes.size())
./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
./tensorflow/core/util/tensor_format.h:420:3: note: in expansion of macro 'CHECK'
   CHECK(index >= 0 && index < dimension_attributes.size())
   ^
./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetFilterDim(tensorflow::gtl::ArraySlice<T>, tensorflow::FilterTensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':
./tensorflow/core/util/tensor_format.h:461:54:   required from here
./tensorflow/core/util/tensor_format.h:435:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   CHECK(index >= 0 && index < dimension_attribute.size())
./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
./tensorflow/core/util/tensor_format.h:435:3: note: in expansion of macro 'CHECK'
   CHECK(index >= 0 && index < dimension_attribute.size())
   ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 1566.447s, Critical Path: 150.61s
INFO: 1352 processes: 1352 local.
FAILED: Build did NOT complete successfully"
22111,"File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/tensorrt/python/trt_convert.py"", line 153, in create_inference_graph     int(msg[0]))"," 'tensorrt_dir', ['L2Loss'])
  File ""demo.py"", line 88, in get_trt_graph
    precision_mode=precision_mode)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/tensorrt/python/trt_convert.py"", line 153, in create_inference_graph
    int(msg[0]))
tensorflow.python.framework.errors_impl.NotFoundError: No attr named 'index_type' in NodeDef:
	 [[Node: BatchMultiClassNonMaxSuppression/ones = Fill[T=DT_INT32](BatchMultiClassNonMaxSuppression/ones/shape, BatchMultiClassNonMaxSuppression/ones/Const)]]

List of Pakages
TensorRT:4.0.1.6
Cuda:9.0
Linux:16.0.4
Python :3.5
Tensorflow:1.10"
22110,[Bug] tf.nn.depthwise_conv2d fails with AttributeError,"### System information
- **Have I written custom code**: yes
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **TensorFlow installed from**: binary
- **TensorFlow version**: v1.10.1-0-g4dcfddc5d1
- **Python version**: 2.7.12
- **CUDA/cuDNN version**: 9.0/7.1.4
- **GPU model and memory**: GTX 1070

### Describe the problem
`tf.nn.depthwise_conv2d` fails when the shape of the input is not known statically and the data format is 'NCHW' and the dilation rate is larger than 1.

### Source code / logs
Reproducible test case:
```
import tensorflow as tf
sh = tf.placeholder(dtype=tf.int32, shape=[4])
img = tf.ones(sh)
k = tf.ones([1, 1, 1, 1])
t = tf.nn.depthwise_conv2d(img, k, [1, 1, 1, 1], 'VALID', rate=[2, 1], data_format='NCHW')
```
Traceback:
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/manu/.local/lib/python2.7/site-packages/tensorflow/python/ops/nn_impl.py"", line 461, in depthwise_conv2d
    op=op)
  File ""/home/manu/.local/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py"", line 364, in with_space_to_batch
    return new_op(input, None)
  File ""/home/manu/.local/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py"", line 520, in __call__
    return self.call(inp, filter)
  File ""/home/manu/.local/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py"", line 514, in _with_space_to_batch_call
    output_shape[1] = filter.shape[-1]
AttributeError: 'NoneType' object has no attribute 'shape'
```"
22109,TocoConverter: permute layer after dim-reducing reshape / squeeze: error on conversion,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: -
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.10.1-0-g4dcfddc5d1 1.10.1
- **Python version**: 3.5.5
- **Bazel version (if compiling from source)**: -
- **GCC/Compiler version (if compiling from source)**: -
- **CUDA/cuDNN version**: V9.0.176
- **GPU model and memory**: NVIDIA TITAN Xp, 12196MiB
- **Exact command to reproduce**: see minimum example code below


### Describe the problem
The code example below containing a model with an input, ""transformation"" and permute layer compiles fine on pc and it is also possible to call `predict()`on it and getting the expected results.
The input layer expects the last dimension to be 1, the transform layer being either a reshape or a squeeze layer gets rid of the 1, and the permutation layer just pushes the first dimension after the batch size to the end.
The native Keras reshape layer (A) is the only one that doesn't work because it doesn't support dimensionality reduction. Its output shape would be `(None, None, 10, 42)`.
However, lines (B), (C) and (D) all work fine and it doesn't matter which one is commented in.
In the same way it doesn't matter whether the Keras permute layer (E) or the backend version (F) is commented in.

If all transform layers (A-D) are commented out and only one permute layer (E) or (F) is commented in, directly gets passed `input` and an input shape of `(2,3,1)` for (E) or `(0,2,3,1)` for (F), the model compiles and the `TocoConverter` produces a tflite-file.
If all permute layers (E, F) are commented out and only one transform layer (B) or (C) or (D) is commented in and the `outputs` of `Model` is set to `[transform]`, the model also compiles and the `TocoConverter` produces a tflite-file.

But for each combination of (B-C) and (E, F) being commented in so that one transform layer and one permute layer is there, the TocoConverter doesn't produce a tflite-file although the model compiles and the model summary looks as it should.

The expected result is the minimum example code running without errors and producing a tflite model file from the converted keras model file.

---------------
**Edit: For the time being and for everyone with the same problem, I wrote an updated minimum example code with an ""emulated"" permute layer as a workaround. It uses split, concatenate and reshape operations and you find it at the end of this page after the tracebacks.**

---------------

### Source code / logs
#### Minimum Example Code:
```python
from tensorflow.keras import Model
from tensorflow.keras.models import save_model

from tensorflow.keras.layers import Input, Lambda, Permute, Reshape
from tensorflow.keras.backend import permute_dimensions
from tensorflow.keras.backend import squeeze as b_squeeze
from tensorflow import reshape, squeeze

from tensorflow.contrib import lite


input = Input(shape=(10, 42, 1))

# transform = Reshape((-1, 10, 42))(input)					# (A)
transform = Lambda(lambda x: reshape(x, (-1, 10, 42)))(input)			# (B)
# transform = Lambda(lambda x: squeeze(x, axis=-1))(input)			# (C)
# transform = Lambda(lambda x: b_squeeze(x, axis=-1))(input)			# (D)

permute = Permute((2,1))(transform)						# (E)
# permute = Lambda(lambda x: permute_dimensions(x, (0,2,1)))(transform)		# (F)

keras_model = Model(inputs=[input], outputs=[permute])
keras_model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])


print(keras_model.summary())
save_model(model=keras_model, filepath=""keras_model.hdf5"",
	overwrite=True, include_optimizer=True)

converter = lite.TocoConverter.from_keras_model_file(""keras_model.hdf5"")
tflite_model = converter.convert()
with open(""tflite_model.tflite"", ""wb"") as f: f.write(tflite_model)
```

#### keras_model.summary() for each of the valid six combinations:
```
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 10, 42, 1)         0
_________________________________________________________________
lambda (Lambda)              (None, 10, 42)            0
_________________________________________________________________
permute (Permute)            (None, 42, 10)            0
=================================================================
Total params: 0
Trainable params: 0
Non-trainable params: 0
_________________________________________________________________
```

#### Traceback for (B)+(E) and (B)+(F):
```
Traceback (most recent call last):
  File ""minimal.py"", line 32, in <module>
    tflite_model = converter.convert()
  File ""/usr/stud/staab/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/lite/python/lite.py"", line 374, in convert
    dump_graphviz_video=self.dump_graphviz_video)
  File ""/usr/stud/staab/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/lite/python/convert.py"", line 246, in toco_convert
    input_data.SerializeToString())
  File ""/usr/stud/staab/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/lite/python/convert.py"", line 106, in toco_convert_protos
    (stdout, stderr))
RuntimeError: TOCO failed see console for info.
b'2018-09-06 10:40:25.228797: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 2 operators, 5 arrays (0 quantized)\n
2018-09-06 10:40:25.228949: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 2 operators, 5 arrays (0 quantized)\n
2018-09-06 10:40:25.229050: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 2 operators, 5 arrays (0 quantized)\n
2018-09-06 10:40:25.229088: F tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:1402] Check failed: axis < input_shape.dimensions_count() (1211997096 vs. 4)\n
Aborted (core dumped)\n'
None
```
The number 1211997096 is always randomly different and even can be negative.

#### Traceback for (C)+(E), (D)+(E), (C)+(F) and (D)+(F):
```
Traceback (most recent call last):
  File ""minimal.py"", line 32, in <module>
    tflite_model = converter.convert()
  File ""/usr/stud/staab/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/lite/python/lite.py"", line 374, in convert
    dump_graphviz_video=self.dump_graphviz_video)
  File ""/usr/stud/staab/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/lite/python/convert.py"", line 246, in toco_convert
    input_data.SerializeToString())
  File ""/usr/stud/staab/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/lite/python/convert.py"", line 106, in toco_convert_protos
    (stdout, stderr))
RuntimeError: TOCO failed see console for info.
b'2018-09-06 10:44:46.550688: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 2 operators, 4 arrays (0 quantized)\n
2018-09-06 10:44:46.550805: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 2 operators, 4 arrays (0 quantized)\n
2018-09-06 10:44:46.550870: F tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:1395] Check failed: perm.size() == input_shape.dimensions_count() (3 vs. 4)Transpose permutation input permute/transpose/perm must be same length as input dimensions\n
Aborted (core dumped)\n'
None
```

#### Minimum Example Code with Workaround:
```python
from tensorflow.keras import Model
from tensorflow.keras.models import save_model

from tensorflow.keras.layers import Input, Lambda, Permute, Reshape, concatenate
from tensorflow.keras.backend import permute_dimensions
from tensorflow.keras.backend import squeeze as b_squeeze
from tensorflow import reshape, squeeze, split, expand_dims

from tensorflow.contrib import lite


input = Input(shape=(10, 42, 1))

# transform = Reshape((-1, 10, 42))(input)					# (A)
transform = Lambda(lambda x: reshape(x, (-1, 10, 42)))(input)			# (B)
# transform = Lambda(lambda x: squeeze(x, axis=-1))(input)			# (C)
# transform = Lambda(lambda x: b_squeeze(x, axis=-1))(input)			# (D)

# Emulated permute to come around a bug: split, concatenate, reshape
#------------------------------------------------------------------------------
split_p = Lambda(lambda x: split(expand_dims(x, axis=-1), num_or_size_splits=10,
                                                            axis=1))(transform)
merge_p = concatenate(split_p)
squeeze_p = Lambda(lambda x: reshape(x, (-1, 42, 10)))(merge_p)
#------------------------------------------------------------------------------

keras_model = Model(inputs=[input], outputs=[squeeze_p])
keras_model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])


print(keras_model.summary())
save_model(model=keras_model, filepath=""keras_model.hdf5"",
	overwrite=True, include_optimizer=True)

converter = lite.TocoConverter.from_keras_model_file(""keras_model.hdf5"")
tflite_model = converter.convert()
with open(""tflite_model.tflite"", ""wb"") as f: f.write(tflite_model)
```"
22108,"tf.nightly-gpu will create ""nan"" in  next batch train value","
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.11.0   (use tf-night-gpu)
- **Python version**:2.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0 /7.1
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
I installed  tf-night-gpu, and my` tf.__version__` was `1.11.0-dev20180905`
it only use `tf-night-gpu` can create  the `tf.keras.applications.InceptionResNetV2` model so i can't without. my problem is the  `tf-night-gpu` will create `nan`  in  next batch model output 

### Source code / logs
in first batch train the
```
('keras.layers.Embedding_input: ', <tf.Tensor: id=4586198, shape=(64, 1), dtype=int32, numpy=
array([[3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3]], dtype=int32)>)
('Embedding_output:  ', <tf.Tensor: id=4586379, shape=(64, 1, 256), dtype=float32, numpy=
array([[[ 0.01097491, -0.02041122,  0.04823625, ..., -0.0155656 ,
          0.01521399, -0.0496642 ]],

       [[ 0.01097491, -0.02041122,  0.04823625, ..., -0.0155656 ,
          0.01521399, -0.0496642 ]],

       [[ 0.01097491, -0.02041122,  0.04823625, ..., -0.0155656 ,
          0.01521399, -0.0496642 ]],

       ...,

       [[ 0.01097491, -0.02041122,  0.04823625, ..., -0.0155656 ,
          0.01521399, -0.0496642 ]],

       [[ 0.01097491, -0.02041122,  0.04823625, ..., -0.0155656 ,
          0.01521399, -0.0496642 ]],

       [[ 0.01097491, -0.02041122,  0.04823625, ..., -0.0155656 ,
          0.01521399, -0.0496642 ]]], dtype=float32)>)
```
the next batch train , have same `keras.layers.Embedding_input: `
```
`Epoch 1 Batch 0 Loss 2.0415
(''keras.layers.Embedding_input:: ', <tf.Tensor: id=4607289, shape=(64, 1), dtype=int32, numpy=
array([[3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3],
       [3]], dtype=int32)>)
(''keras.layers.Embedding_output::  ', <tf.Tensor: id=4607374, shape=(64, 1, 256), dtype=float32, numpy=
array([[[nan, nan, nan, ..., nan, nan, nan]],

       [[nan, nan, nan, ..., nan, nan, nan]],

       [[nan, nan, nan, ..., nan, nan, nan]],


       [[nan, nan, nan, ..., nan, nan, nan]],

       [[nan, nan, nan, ..., nan, nan, nan]],

       [[nan, nan, nan, ..., nan, nan, nan]]], dtype=float32)>)`
```
and ""nan"" forever....."
22106,Toco/TFLite_Convert for TFLite Problem,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Pixel 1
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.9.0 (commit r1.9)
- **Python version**: N/A
- **Bazel version (if compiling from source)**: 0.16.1
- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:

1.
```bazel run -c opt tensorflow/python/tools/optimize_for_inference -- --input=$ORIGINAL_PB  --output=$STRIPPED_PB --frozen_graph=True --input_names=Preprocessor/sub --output_names=concat,concat_1 --alsologtostderr```

2.
```bazel run tensorflow/contrib/lite/toco:toco -- --input_file=$STRIPPED_PB --output_file=/absolute/path/to/tensorflow/tensorflow/contrib/lite/examples/android/assets/new_model.tflite  --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --input_shapes=1,300,300,3 --input_arrays=Preprocessor/sub --output_arrays=concat,concat_1 --inference_type=QUANTIZED_UINT8 --logtostderr --default_ranges_min=0 --default_ranges_max=5 --mean_values=128 --std_values=127 --allow_custom_opps```

or

```bazel run //tensorflow/contrib/lite/python:tflite_convert -- --graph_def_file=$STRIPPED_PB --output_file=/absolute/path/to/tensorflow/tensorflow/contrib/lite/examples/android/assets/tflite_convert_example.tflite --input_arrays=Preprocessor/sub --output_arrays=concat,concat_1 --output_format=TFLITE --input_shapes=1,300,300,3  --inference_type=QUANTIZED_UINT8 --default_ranges_min=0 --default_ranges_max=5 --mean_values=128 --std_dev_values=127 --allow_custom_opps```
(which fails)

or

```bazel run //tensorflow/contrib/lite/python:tflite_convert -- --graph_def_file=$STRIPPED_PB --output_file=/absolute/path/to/tensorflow/tensorflow/contrib/lite/examples/android/assets/tflite_convert_example.tflite --input_arrays=Preprocessor/sub --output_arrays=concat,concat_1 --input_shapes=1,300,300,3```
(which succeeds)

3. Change TF_OD_API_MODEL_FILE and append new file to the assets list in BUILD

4.
```bazel build -c opt --cxxopt='--std=c++11' //tensorflow/contrib/lite/examples/android:tflite_demo```

5.
```adb install -r -f bazel-bin/tensorflow/contrib/lite/examples/android/tflite_demo.apk```

6. Open TFL Detect

### Describe the problem
I'm attempting to import [ssd_mobilenet_v1](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2018_01_28.tar.gz), [ssd_mobilenet_v2](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_03_29.tar.gz) and [ssdlite](http://download.tensorflow.org/models/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz) from the (model zoo)[https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md] into the TFLite Android example. Ultimately I'm aiming to retrain either the ssdlite or ssd_mobilenet_v2 models, but for right now all models I use trigger runtime errors. All of the errors imply that the models are changed by the `optimize_for_inference` and `toco`/`tflite_convert` commands in a way that makes them incompatible with r1.9.

Now, it's most likely that my command for `toco`/`tflite_convert` are to blame, but since these commands seem to be well formed I'm elevating this to github.

### Source code / logs
Firstly, according to (the toco documentation)[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/cmdline_examples.md] we're only supposed to use `tflite_convert` once we're in r1.9. When I try to actually specify all of the fields that the command has in the help (aka the tflite_convert command I put in above) I get the following log and no file is produced:

```bazel run //tensorflow/contrib/lite/python:tflite_convert -- --graph_def_file=$STRIPPED_PB --output_file=/home/bryan/Support/tensorflow/tensorflow/contrib/lite/examples/android/assets/tflite_convert_example.tflite --input_arrays=Preprocessor/sub --output_arrays=concat,concat_1 --output_format=TFLITE --input_shapes=1,300,300,3  --inference_type=QUANTIZED_UINT8 --default_ranges_min=0 --default_ranges_max=5 --mean_values=128 --std_dev_values=127 --allow_custom_opps
 WARNING: /home/bryan/.cache/bazel/_bazel_bryan/36e77cb69a80a4f75d1ba8b192f69b6d/external/grpc/BUILD:1960:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/bryan/.cache/bazel/_bazel_bryan/36e77cb69a80a4f75d1ba8b192f69b6d/external/grpc/bazel/grpc_build_system.bzl:172:12
 WARNING: /home/bryan/.cache/bazel/_bazel_bryan/36e77cb69a80a4f75d1ba8b192f69b6d/external/grpc/BUILD:1960:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/bryan/.cache/bazel/_bazel_bryan/36e77cb69a80a4f75d1ba8b192f69b6d/external/grpc/bazel/grpc_build_system.bzl:172:12
 WARNING: /home/bryan/.cache/bazel/_bazel_bryan/36e77cb69a80a4f75d1ba8b192f69b6d/external/grpc/BUILD:1960:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/bryan/.cache/bazel/_bazel_bryan/36e77cb69a80a4f75d1ba8b192f69b6d/external/grpc/bazel/grpc_build_system.bzl:172:12
 INFO: Analysed target //tensorflow/contrib/lite/python:tflite_convert (0 packages loaded).
 INFO: Found 1 target...
 Target //tensorflow/contrib/lite/python:tflite_convert up-to-date:
   bazel-bin/tensorflow/contrib/lite/python/tflite_convert
 INFO: Elapsed time: 0.254s, Critical Path: 0.00s
 INFO: 0 processes.
 INFO: Build completed successfully, 1 total action
 INFO: Running command line: bazel-bin/tensorflow/contrib/lite/python/tflite_convert '--graph_def_file=/home/bryan/Downloads/ssd_mobilenet_v1_coco_2018_01_28/stripped' '--output_file=/home/bryan/Support/tensorflow/tensorflow/contrib/lite/examples/android/assets/tflite_convert_example.tflite' '--input_arrays=Preprocessor/sub' '--output_arrays=concat,concat_1' '--output_format=TFLITE' '--input_shapes=1,300,3INFO: Build completed successfully, 1 total action
 /home/bryan/.local/lib/python2.7/site-packages/scipy/__init__.py:114: UserWarning: Numpy 1.8.2 or above is recommended for this version of scipy (detected version 1.8.0)
   UserWarning)
 usage: tflite_convert.py [-h] --output_file OUTPUT_FILE
                          (--graph_def_file GRAPH_DEF_FILE | --saved_model_dir SAVED_MODEL_DIR)
                          [--output_format {TFLITE,GRAPHVIZ_DOT}]
                          [--inference_type {FLOAT,QUANTIZED_UINT8}]
                          [--inference_input_type {FLOAT,QUANTIZED_UINT8}]
                          [--input_arrays INPUT_ARRAYS]
                          [--input_shapes INPUT_SHAPES]
                          [--output_arrays OUTPUT_ARRAYS]
                          [--saved_model_tag_set SAVED_MODEL_TAG_SET]
                          [--saved_model_signature_key SAVED_MODEL_SIGNATURE_KEY]
                          [--std_dev_values STD_DEV_VALUES]
                          [--mean_values MEAN_VALUES]
                          [--default_ranges_min DEFAULT_RANGES_MIN]
                          [--default_ranges_max DEFAULT_RANGES_MAX]
                          [--drop_control_dependency DROP_CONTROL_DEPENDENCY]
                          [--reorder_across_fake_quant REORDER_ACROSS_FAKE_QUANT]
                          [--change_concat_input_ranges CHANGE_CONCAT_INPUT_RANGES]
                          [--allow_custom_ops ALLOW_CUSTOM_OPS]
 tflite_convert.py: error:
```

When I strip the tflite_convert params to just include the bare minimum (graph_def_file, output_file, input_arrays, output_arrays, input_shapes) it does create an unquantized tflite model. When I load an unquantized tflite model generated with either command, TFL Detect exits with the following log:

```
09-06 00:00:51.046 25024 25041 E AndroidRuntime: FATAL EXCEPTION: inference
09-06 00:00:51.046 25024 25041 E AndroidRuntime: Process: org.tensorflow.lite.demo, PID: 25024
09-06 00:00:51.046 25024 25041 E AndroidRuntime: java.lang.IllegalArgumentException: Output error: Shape of output target [1, 1917, 4] does not match with the shape of the Tensor [1, 1917, 1, 4].
09-06 00:00:51.046 25024 25041 E AndroidRuntime: 	at org.tensorflow.lite.Tensor.copyTo(Tensor.java:44)
09-06 00:00:51.046 25024 25041 E AndroidRuntime: 	at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:156)
09-06 00:00:51.046 25024 25041 E AndroidRuntime: 	at org.tensorflow.demo.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:222)
09-06 00:00:51.046 25024 25041 E AndroidRuntime: 	at org.tensorflow.demo.DetectorActivity$3.run(DetectorActivity.java:242)
09-06 00:00:51.046 25024 25041 E AndroidRuntime: 	at android.os.Handler.handleCallback(Handler.java:873)
09-06 00:00:51.046 25024 25041 E AndroidRuntime: 	at android.os.Handler.dispatchMessage(Handler.java:99)
09-06 00:00:51.046 25024 25041 E AndroidRuntime: 	at android.os.Looper.loop(Looper.java:193)
09-06 00:00:51.046 25024 25041 E AndroidRuntime: 	at android.os.HandlerThread.run(HandlerThread.java:65)
09-06 00:00:51.049   914  2995 W ActivityManager:   Force finishing activity org.tensorflow.lite.demo/org.tensorflow.demo.DetectorActivity
```

The closest I've found as a solution is in (this stackoverflow page)[https://stackoverflow.com/questions/50388330/java-lang-illegalargumentexception-output-error-shape-of-output-target-1-191] which suggests modifying the `TFLiteObjectDetectionAPIModel` itself (which runs into problems when you get similar errors on the outputClassification array).

If we use a quantized model, it crashes with this error:
```
09-05 22:54:27.413 21650 21667 E AndroidRuntime: java.lang.IllegalArgumentException: Input error: DataType (1) of input data does not match with the DataType (3) of model inputs.
09-05 22:54:27.413 21650 21667 E AndroidRuntime: 	at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)
09-05 22:54:27.413 21650 21667 E AndroidRuntime: 	at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:123)
09-05 22:54:27.413 21650 21667 E AndroidRuntime: 	at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:144)
09-05 22:54:27.413 21650 21667 E AndroidRuntime: 	at org.tensorflow.demo.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:222)
09-05 22:54:27.413 21650 21667 E AndroidRuntime: 	at org.tensorflow.demo.DetectorActivity$3.run(DetectorActivity.java:242)
```

Given similar error messages in (this test)[https://github.com/OAID/TensorFlow-HRT/blob/master/tensorflow/contrib/lite/java/src/test/java/org/tensorflow/lite/NativeInterpreterWrapperTest.java#L228] and how commits after r1.9 give the `TFLiteObjectDetectionAPIModel` class an `isQuantized` flag this makes me think that r1.9 may not support quantization. If so, is there a definitive source for this? There are several sources that are imperfect in different ways (for the most official sources (fixed point quantization)[https://www.tensorflow.org/performance/quantization] page seems geared towards classification instead of the object detection, the required output arrays in the (medium page)[https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193] are not found when we run toco, and both of them are supposedly out of date because of (the toco documentation)[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/cmdline_examples.md])."
22105,[Bug] Sometimes CUDA check failed,"### System information
- **Code**: a keras example (use tf.keras instead) https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py
- **OS Platform and Distribution**: Windows server 2016
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 'v1.10.0-rc1-19-g656e7a2b34' 1.10.0
- **Python version**: 3.6.6
- **CUDA/cuDNN version**: CUDA 9.0, cuDNN 7.0

- **Have I written custom code**: some I/O code to adapt data format, which shouldn't involved
- **Bazel version** : N/A
- **GPU model and memory**: Quadro P600, 2GiB
- **Exact command to reproduce**: python lstm_seq2seq.py
- **Mobile device**: N/A

### Describe the problem
I try to run the code mentioned above.
If I run the script successfully, the script will raise a error in next several runs.
Keep retry and I can successfully run it again.

I only know the check failed, but can't see the error code.
Could you provide some help on it?

Another problem: only changing latent_dim from 128 to 256 results in 5e-8 training loss and a bad model. But CPU training gives a good result.

### Source code / logs
Epoch 1/100
2018-09-06 10:51:48.513539: I T:\src\github\tensorflow\tensorflow\core\platform\cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2018-09-06 10:51:48.726860: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1405] Found device 0 with properties:
name: Quadro P600 major: 6 minor: 1 memoryClockRate(GHz): 1.5565
pciBusID: 0000:21:00.0
totalMemory: 2.00GiB freeMemory: 1.62GiB
2018-09-06 10:51:48.731423: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1484] Adding visible gpu devices: 0
2018-09-06 10:51:49.568323: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-09-06 10:51:49.573347: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:971]      0
2018-09-06 10:51:49.576631: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:984] 0:   N
2018-09-06 10:51:49.582844: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1377 MB memory) -> physical GPU (device: 0, name:
Quadro P600, pci bus id: 0000:21:00.0, compute capability: 6.1)
2018-09-06 10:51:49.872995: I T:\src\github\tensorflow\tensorflow\core\kernels\cuda_solvers.cc:159] Creating CudaSolver handles for stream 000001DDA2412050
**2018-09-06 10:51:50.120523: F T:\src\github\tensorflow\tensorflow\core\kernels\cuda_solvers.cc:94] Check failed: cusolverDnCreate(&cusolver_dn_handle) == CUSOLVER_STATUS_SUCCESS Failed to create cuSolverDN instance.**
"
22104,tf.keras.layers.CuDNNGRU in eager mode can cause OOM ,"tensorflow 1.10.1 
Code like below, with inputs dim 1 can be dynamic length.
  
    class Model(keras.Model):
      def __init__(self):
        super(Model, self).__init__()
        self.embedding = keras.layers.Embedding(vocab_size, FLAGS.emb_dim)
        self.encode = keras.layers.CuDNNGRU(units=FLAGS.rnn_hidden_size, 
                                          return_sequences=True, 
                                          return_state=False, 
                                          recurrent_initializer='glorot_uniform')
        self.pooling = keras.layers.GlobalMaxPool1D()
        self.logits = keras.layers.Dense(NUM_CLASSES, activation=None)

      def call(self, inputs, training=False):
        x = inputs.comment
        x = self.embedding(x)
        x = self.encode(x)
        x = self.pooling(x)
        x = self.logits(x)
      return x

    def calc_loss(model, inputs, training=False):
      y_ = model(inputs, training=training)
      y = inputs.classes
      return tf.losses.sigmoid_cross_entropy(y, y_)   


1. Using keras.layers.GRU will not OOM
2. Using keras.layers.CuDNNGRU but not in eager mode will not OOM
3. Using keras.layers.CuDNNGRU + eager mode  will OOM (complain self.embedding OOM after training a few steps)  

    optimizer.apply_gradients(zip(grads, model.variables))
  File ""/home/gezi/py3env/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 605, in apply_gradients
    update_ops.append(processor.update_op(self, grad))
  File ""/home/gezi/py3env/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 165, in update_op
    g.values, self._v, g.indices)
  File ""/home/gezi/py3env/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 958, in _resource_apply_sparse_duplicate_indices
    return self._resource_apply_sparse(summed_grad, handle, unique_indices)
  File ""/home/gezi/py3env/lib/python3.6/site-packages/tensorflow/python/training/adam.py"", line 218, in _resource_apply_sparse
    grad, var, indices, self._resource_scatter_add)
  File ""/home/gezi/py3env/lib/python3.6/site-packages/tensorflow/python/training/adam.py"", line 200, in _apply_sparse_shared
    lr * m_t / (v_sqrt + epsilon_t),
  File ""/home/gezi/py3env/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py"", line 850, in binary_op_wrapper
    return func(x, y, name=name)
  File ""/home/gezi/py3env/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py"", line 958, in _truediv_python3
    return gen_math_ops.real_div(x, y, name=name)
  File ""/home/gezi/py3env/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 5904, in real_div
    _six.raise_from(_core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[151627,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:RealDiv] name: Adam/update_/truediv/
"
22103,TensorFlow profiler timeline range,"Hello,

I'm following the tf.profiler.Profiler guide to profile my code.
When generating the timelines, all of them except the first one (for the first batch) are scaled in hundreds of years, rather than seconds.

There is something happening at year 0, then something going on after 583 years.
No, my DNN code is not that slow. 
![screenshot_2018-09-05_23-19-38](https://user-images.githubusercontent.com/6018251/45128332-5c24b680-b175-11e8-96f6-c5cc9abee2d4.png)
Running the latest stable version of everything.

Could you please recommend me a fix?
Thank you"
22102,raw_rnns not working with xla jit compile,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Centos 7.4.1708
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.10
- **Python version**: 3.4.5
- **Bazel version (if compiling from source)**: 0.16
- **GCC/Compiler version (if compiling from source)**: 4.8.5
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: NA

### Describe the problem
Trying to run char_rnn using raw_rnns with LSTMCell using TF estimators on the shakespeare dataset. It trains when I don't use the xla jit compile. When I do try to add the xla jit compile, I run into errors.  

first error:
```
Traceback (most recent call last):
  File ""/net/server5/srv/nfs/vishal-data/ws/venv_tf110/lib/python3.4/site-packages/tensorflow/python/client/session.py"", line 1278, in _do_call
    return fn(*args)
  File ""/net/server5/srv/nfs/vishal-data/ws/venv_tf110/lib/python3.4/site-packages/tensorflow/python/client/session.py"", line 1263, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/net/server5/srv/nfs/vishal-data/ws/venv_tf110/lib/python3.4/site-packages/tensorflow/python/client/session.py"", line 1350, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InternalError: Resource arguments cannot be constant (argument 3)
	EncapsulateSubgraphsPass failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""basic_rnn.py"", line 332, in <module>
    classifier.train(input_fn, steps=args.num_steps)
  File ""/net/server5/srv/nfs/vishal-data/ws/venv_tf110/lib/python3.4/site-packages/tensorflow/python/estimator/estimator.py"", line 376, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/net/server5/srv/nfs/vishal-data/ws/venv_tf110/lib/python3.4/site-packages/tensorflow/python/estimator/estimator.py"", line 1145, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/net/server5/srv/nfs/vishal-data/ws/venv_tf110/lib/python3.4/site-packages/tensorflow/python/estimator/estimator.py"", line 1173, in _train_model_default
    saving_listeners)
  File ""/net/server5/srv/nfs/vishal-data/ws/venv_tf110/lib/python3.4/site-packages/tensorflow/python/estimator/estimator.py"", line 1451, in _train_with_estimator_spec
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""/net/server5/srv/nfs/vishal-data/ws/venv_tf110/lib/python3.4/site-packages/tensorflow/python/training/monitored_session.py"", line 583, in run
    run_metadata=run_metadata)
  File ""/net/server5/srv/nfs/vishal-data/ws/venv_tf110/lib/python3.4/site-packages/tensorflow/python/training/monitored_session.py"", line 1059, in run
    run_metadata=run_metadata)
  File ""/net/server5/srv/nfs/vishal-data/ws/venv_tf110/lib/python3.4/site-packages/tensorflow/python/training/monitored_session.py"", line 1150, in run
    raise six.reraise(*original_exc_info)
  File ""/net/server5/srv/nfs/vishal-data/ws/venv_tf110/lib/python3.4/site-packages/six.py"", line 693, in reraise
    raise value
  File ""/net/server5/srv/nfs/vishal-data/ws/venv_tf110/lib/python3.4/site-packages/tensorflow/python/training/monitored_session.py"", line 1135, in run
    return self._sess.run(*args, **kwargs)
  File ""/net/server5/srv/nfs/vishal-data/ws/venv_tf110/lib/python3.4/site-packages/tensorflow/python/training/monitored_session.py"", line 1207, in run
    run_metadata=run_metadata)
  File ""/net/server5/srv/nfs/vishal-data/ws/venv_tf110/lib/python3.4/site-packages/tensorflow/python/training/monitored_session.py"", line 987, in run
    return self._sess.run(*args, **kwargs)
  File ""/net/server5/srv/nfs/vishal-data/ws/venv_tf110/lib/python3.4/site-packages/tensorflow/python/client/session.py"", line 877, in run
    run_metadata_ptr)
  File ""/net/server5/srv/nfs/vishal-data/ws/venv_tf110/lib/python3.4/site-packages/tensorflow/python/client/session.py"", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/net/server5/srv/nfs/vishal-data/ws/venv_tf110/lib/python3.4/site-packages/tensorflow/python/client/session.py"", line 1272, in _do_run
    run_metadata)
  File ""/net/server5/srv/nfs/vishal-data/ws/venv_tf110/lib/python3.4/site-packages/tensorflow/python/client/session.py"", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Resource arguments cannot be constant (argument 3)
	EncapsulateSubgraphsPass failed
```
I fixed this by modifying the tensorflow/compiler/tf2xla/const_analysis.cc with
```
Status BackwardsConstAnalysis(const Graph& g,
         int index;
         status = GetNodeAttr(node->attrs(), ""index"", &index);
         if (!status.ok()) return;
-        compile_time_const_args->at(index) = true;
+       DataType dt;
+       status = GetNodeAttr(node->attrs(), ""T"", &dt);
+       if (!status.ok()) return;
+       if (dt != DT_RESOURCE) {
+         VLOG(1) << ""HIHIH ""<< SummarizeNodeDef(node->def());
+         compile_time_const_args->at(index) = true;
+       }
         return;
       }
       for (const Edge* pred : node->in_edges()) {
``` 
I then run into this error:
```
 (No registered '_Retval' OpKernel for XLA_CPU_JIT devices compatible with node test_conv_gradients_test_conv_rnn_while_select_1_grad_select_f_acc_0_retval_RetVal = _Retval[T=DT_RESOURCE, index=0](test_conv/gradients/test_conv/rnn/while/Select_1_grad/Select/f_acc)
         (OpKernel was found, but attributes didn't match)
        .  Registered:  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_HALF, DT_UINT32, DT_UINT64]
  device='GPU'; T in [DT_STRING]
  device='GPU'; T in [DT_RESOURCE]
  device='GPU'; T in [DT_INT32]
  device='GPU'; T in [DT_BOOL]
  device='GPU'; T in [DT_COMPLEX128]
  device='GPU'; T in [DT_COMPLEX64]
  device='GPU'; T in [DT_INT8]
  device='GPU'; T in [DT_UINT8]
  device='GPU'; T in [DT_INT16]
  device='GPU'; T in [DT_UINT16]
  device='GPU'; T in [DT_INT64]
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_BFLOAT16]
  device='GPU'; T in [DT_HALF]
  device='CPU'
  device='XLA_CPU'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_HALF, DT_UINT32, DT_UINT64]
)
```

Is there a way for me to fix this? Or for the tensorflow folks to fix this? Or is there an example of using raw_rnns with the xla jit compile that I can look at? "
22099,Issue with Tensorflow softmax on gpus,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
none
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.10.0 gpu
- **Python version**:
3.5.5
- **Bazel version (if compiling from source)**:
n/a
- **GCC/Compiler version (if compiling from source)**:
n/a
- **CUDA/cuDNN version**:
CUDA 9.0/cuDNN v7.2.1.38 for CUDA 9.0 
- **GPU model and memory**:
GTX 980 4GB
- **Exact command to reproduce**:
n/a


### Describe the problem
There is still a huge issue with not included GPU kernels in the latest tensorflow python version. When you try to run 

> import tensorflow as tf
> with tf.device('/{}:{}'.format('gpu','0')):
>     x = [0., -1., 2., 3.]
>     softmax_x = tf.nn.softmax(x)
>     session = tf.Session()
>     print(session.run(softmax_x))

it's exiting with the following error:

```
Traceback (most recent call last):
  File ""C:\Users\xxx\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py"", line 1278, in _do_call
    return fn(*args)
  File ""C:\Users\xxx\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py"", line 1261, in _run_fn
    self._extend_graph()
  File ""C:\Users\xxx\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py"", line 1295, in _extend_graph
    tf_session.ExtendSession(self._session)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'Softmax': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
Registered kernels:
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_DOUBLE]

         [[Node: Softmax = Softmax[T=DT_FLOAT, _device=""/device:GPU:0""](Reshape)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""tools\check_tensorflow.py"", line 7, in <module>
    print(session.run(softmax_x))
  File ""C:\Users\xxx\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py"", line 877, in run
    run_metadata_ptr)
  File ""C:\Users\xxx\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py"", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\Users\xxx\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py"", line 1272, in _do_run
    run_metadata)
  File ""C:\Users\xxx\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py"", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'Softmax': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
Registered kernels:
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_DOUBLE]

         [[Node: Softmax = Softmax[T=DT_FLOAT, _device=""/device:GPU:0""](Reshape)]]

Caused by op 'Softmax', defined at:
  File ""tools\check_tensorflow.py"", line 5, in <module>
    softmax_x = tf.nn.softmax(x)
  File ""C:\Users\xxx\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\util\deprecation.py"", line 454, in new_func
    return func(*args, **kwargs)
  File ""C:\Users\xxx\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\nn_ops.py"", line 1738, in softmax
    return _softmax(logits, gen_nn_ops.softmax, axis, name)
  File ""C:\Users\xxx\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\nn_ops.py"", line 1680, in _softmax
    output = compute_op(logits)
  File ""C:\Users\xxx\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py"", line 7672, in softmax
    ""Softmax"", logits=logits, name=name)
  File ""C:\Users\xxx\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\Users\xxx\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\util\deprecation.py"", line 454, in new_func
    return func(*args, **kwargs)
  File ""C:\Users\xxx\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 3155, in create_op
    op_def=op_def)
  File ""C:\Users\xxx\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'Softmax': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
Registered kernels:
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_DOUBLE]

         [[Node: Softmax = Softmax[T=DT_FLOAT, _device=""/device:GPU:0""](Reshape)]]
```

The softmax cuda kernel seems not to be included in the package. This means that when you don't specify explicitly to use the gpu, it will just run softmax on cpu, which leads to cpu usages of 100% and very low gpu usage from 10 to 20%, and if you do it crashes.
Especially for beginners this can be very confusing and hard to find. Also it has a huge impact on overall performance.

So are there any solutions for this problem?
"
22098,Memory issue when inter_op_parallelism_threads > 1 on Ubuntu 16.04,"### System information

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, see below
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 
```
== cat /etc/issue ===============================================
Linux ml 4.4.0-87-generic #110-Ubuntu SMP Tue Jul 18 12:55:35 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.3 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux ml 4.4.0-87-generic #110-Ubuntu SMP Tue Jul 18 12:55:35 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.14.3)
protobuf (3.6.1)
tensorflow (1.10.0)
tensorflow-tensorboard (0.4.0)

== check for virtualenv =========================================
True

== tensorflow import ============================================
tf.VERSION = 1.10.0
tf.GIT_VERSION = v1.10.0-0-g656e7a2b34
tf.COMPILER_VERSION = v1.10.0-0-g656e7a2b34
Sanity check: array([1], dtype=int32)
/home/btabanpour/mlpy/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters

== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda/lib64:/home/anaconda3/lib/
DYLD_LIBRARY_PATH is unset

```
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10.0
- **Python version**: Python 3.5.2
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**:


Save this file as `reproduce.py`:

```python
import numpy as np
from tensorflow.keras.applications.inception_v3 import InceptionV3
from memory_profiler import profile

image = np.random.random((1, 299, 299, 3))

@profile
def predict():
    m.predict(image)


if __name__ == '__main__':
    m = InceptionV3()  # or any other large network like VGG19()
    for _ in range(100):
        predict()
```

I run `python reproduce.py` and memory increases from ~500MiB to ~1200MiB after 100 calls of `predict()`, **running on CPU**. Here is truncated output:

```
 > python reproduce.py
2018-09-05 14:41:10.229405: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Filename: reproduce.py

Line #    Mem usage    Increment   Line Contents
================================================
     7    493.0 MiB    493.0 MiB   @profile
     8                             def predict():
     9    524.8 MiB     31.8 MiB       m.predict(image)


Filename: reproduce.py

Line #    Mem usage    Increment   Line Contents
================================================
     7    524.8 MiB    524.8 MiB   @profile
     8                             def predict():
     9    557.2 MiB     32.4 MiB       m.predict(image)


...


Filename: reproduce.py

Line #    Mem usage    Increment   Line Contents
================================================
     7   1283.4 MiB   1283.4 MiB   @profile
     8                             def predict():
     9   1283.6 MiB      0.2 MiB       m.predict(image)


Filename: reproduce.py

Line #    Mem usage    Increment   Line Contents
================================================
     7   1283.6 MiB   1283.6 MiB   @profile
     8                             def predict():
     9   1285.9 MiB      2.3 MiB       m.predict(image)

```

### Describe the problem

In short, memory keeps increasing after each predict call. I tried the above script with other networks such as `tensorflow.keras.applications.vgg19` and pure tensorflow convnets and I see the same issue.

This issue seems to be resolved when I set `inter_op_parallelism_threads=1` in the `tf.ConfigProto` as such:

```python
    config = tf.ConfigProto(
        inter_op_parallelism_threads=1)
    sess = tf.Session(config=config)
```

And re-appears when I start increasing `inter_op_parallelism_threads` to 32 (I'm running on 32 CPU cores).

### Another issue:

As I mentioned previously, setting `inter_op_parallelism_threads=1` stops the memory from increasing on each predict call. Here is a script that sets the config `inter_op_parallelism_threads=1` and does not exhibit a memory leak:
```python
import numpy as np
import tensorflow as tf
import tensorflow.keras.backend as K

from tensorflow.keras.applications.inception_v3 import InceptionV3
from memory_profiler import profile


image = np.random.random((1, 299, 299, 3))


@profile
def predict():
    m.predict(image)


if __name__ == '__main__':
    config = tf.ConfigProto(
        inter_op_parallelism_threads=1)
    sess = tf.Session(config=config)

    print(sess._config)
    K.set_session(sess)

    m = InceptionV3()
    for _ in range(100):
        predict()
```

However, I see memory increasing again if I add an extra call to `tf.Session()` before calling the one with the `tf.ConfigProto`:

```python
import numpy as np
import tensorflow as tf
import tensorflow.keras.backend as K

from tensorflow.keras.applications.inception_v3 import InceptionV3
from memory_profiler import profile

image = np.random.random((1, 299, 299, 3))


@profile
def predict():
    m.predict(image)


if __name__ == '__main__':
    s = tf.Session()  # Added an extra tf.Session() call here!!!
    s.close()

    config = tf.ConfigProto(
        inter_op_parallelism_threads=1)
    sess = tf.Session(config=config)

    print(sess._config)
    K.set_session(sess)

    m = InceptionV3()

    for _ in range(100):
        predict()
```

It seems like the ConfigProto from the first `tf.Session()` is overwriting the config from the next session I create.

---

Here is a graph of the script above for 1000 predict calls:

![tf_issue](https://user-images.githubusercontent.com/7320238/45115233-3cfa3a80-b11d-11e8-9b5e-781508fc8583.png)
"
22097,"Failed to load the native TensorFlow runtime (TF 1.10.0, Windows Platform Only)","### System information
- **Have I written custom code**: No
- **OS Platform and Distribution**: Windows 10 and Windows Server 2016
- **TensorFlow installed from**: Anaconda Distribution
- **TensorFlow version**: 1.10.0
- **Python version**: 3.6.6
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
After updating TF to v1.10.0 (through conda update), it cannot be imported. Kindly note that this issue is specific to Windows platform only (tested on both 10 and Server 2016). On Linux platform and using the same setting (TF installed using Anaconda Distribution), the same code is working fine (tested on Ubuntu 18.04.1 LTS).

I also tried running a stock example but the same issue occured (TF cannot be imported).

### Source code / logs
<details>
<summary>Full Traceback (click to expand)</summary>
Traceback (most recent call last):
  File ""C:\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Anaconda3\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Anaconda3\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Anaconda3\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Anaconda3\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Anaconda3\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
</details>
<br />

Thanks!"
22096,CTCLoss does not support half precision (float16/fp16),"I was trying to use `CTCLoss` with float16, and encountered a type exception in `python.framework.op_def_library._apply_op_helper `. In the [op definition](https://github.com/tensorflow/tensorflow/blob/294442996b2aeff00b1bfdc7e7169f7cb35bbf3d/tensorflow/core/ops/ops.pbtxt#L3568), it only lists `DT_FLOAT`.  Is there a particular reason that this op wasn't updated to support half precision in #1300?

```
  File ""/nix/store/rnfj7g778synyrkr7qa8nx67sdmsr4xg-python3.6-tensorflow-1.7.0/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 533, in _apply_op_helper
    (prefix, dtypes.as_dtype(input_arg.type).name))
TypeError: Input 'inputs' of 'CTCLoss' Op has type float16 that does not match expected type of float32.
```

Have I written custom code: No
OS Platform and Distribution: Ubuntu 16.04
TensorFlow installed from: pip
TensorFlow version: 1.7
Bazel version: 0.10.1
CUDA/cuDNN version: 9.1
GPU model and memory: Tesla V100
Exact command to reproduce: N/A
Mobile device: N/A"
22095,3D convolutions on GPU with large input produces incorrect results on some GPUs,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, see reproduction script below.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.5 LTS
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: Binary via pip
- **TensorFlow version (use command below)**: 1.10.1
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: GCC 5.4.0
- **CUDA/cuDNN version**: CUDA 9.0 / cuDNN 7.2
- **GPU model and memory**: GTX 1080 Ti
- **Exact command to reproduce**: See script below.

```
== cat /etc/issue ===============================================
Linux gpu01 4.4.0-116-generic #140-Ubuntu SMP Mon Feb 12 21:23:04 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.5 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux gpu01 4.4.0-116-generic #140-Ubuntu SMP Mon Feb 12 21:23:04 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy                    1.14.2
protobuf                 3.6.1
tensorflow-gpu           1.10.1
tensorflow-tensorboard   1.5.1

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.10.1
tf.GIT_VERSION = v1.10.1-0-g4dcfddc5d1
tf.COMPILER_VERSION = v1.10.1-0-g4dcfddc5d1
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda/lib64
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Wed Sep  5 16:19:24 2018
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 387.26                 Driver Version: 387.26                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 108...  On   | 00000000:04:00.0 Off |                  N/A |
| 23%   38C    P0    64W / 250W |      1MiB / 11172MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GTX 108...  On   | 00000000:05:00.0 Off |                  N/A |
| 23%   36C    P2    65W / 250W |      1MiB / 11172MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  GeForce GTX 108...  On   | 00000000:08:00.0 Off |                  N/A |
| 23%   42C    P0    77W / 250W |      1MiB / 11172MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  GeForce GTX 108...  On   | 00000000:09:00.0 Off |                  N/A |
| 23%   42C    P0    63W / 250W |      1MiB / 11172MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   4  GeForce GTX 108...  On   | 00000000:85:00.0 Off |                  N/A |
| 23%   35C    P0    63W / 250W |      1MiB / 11172MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   5  GeForce GTX 108...  On   | 00000000:86:00.0 Off |                  N/A |
| 23%   38C    P0    73W / 250W |      1MiB / 11172MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   6  GeForce GTX 108...  On   | 00000000:89:00.0 Off |                  N/A |
| 23%   38C    P0    72W / 250W |      1MiB / 11172MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   7  GeForce GTX 108...  On   | 00000000:8A:00.0 Off |                  N/A |
| 23%   36C    P0    71W / 250W |      1MiB / 11172MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.0/doc/man/man7/libcudart.7
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.176
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-9.1/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.1/doc/man/man7/libcudart.7
/usr/local/cuda-9.1/targets/x86_64-linux/lib/libcudart.so.9.1.85
/usr/local/cuda-9.1/targets/x86_64-linux/lib/libcudart_static.a
```

### Describe the problem
3D convolutions on GPU with large batches fail to calculate the correct result for the last part of the batch. The first entries will be correct, however, after some point the resulting values will be random. This is the case for at least the GTX 1080 Ti and the GeForce GTX 980 Ti gpus. It produces the correct result however on the CPU and at least the K80 GPU.

### Source code / logs

A small reproduction script:

```
import tensorflow as tf
import numpy as np



run_on_gpu = True  # Set to false for the correct result, computed on the CPU.
batch_size = 128  # Try out a large batch, that just fits into memory. Small batches work fine.



random_state = np.random.RandomState(42)

inputs = tf.placeholder(tf.float32, (None, 29, 72, 72, 1), 'inputs')

device_name = '/gpu:0' if run_on_gpu else '/cpu:0'

with tf.device(device_name):
    weights = tf.constant(random_state.randn(3, 3, 3, 1, 32).astype(np.float32))
    outputs = tf.nn.conv3d(inputs, weights, strides=(1, 1, 1, 1, 1), padding='SAME', dilations=(1, 1, 1, 1, 1))
    weights = tf.constant(random_state.randn(1, 1, 1, 32, 16).astype(np.float32))
    outputs = tf.nn.conv3d(outputs, weights, strides=(1, 1, 1, 1, 1), padding='VALID', dilations=(1, 1, 1, 1, 1))

session = tf.Session()

random_inputs = random_state.randn(batch_size, 29, 72, 72, 1).astype(np.float32)

a = session.run(outputs, feed_dict={inputs: random_inputs})[-1]
b = session.run(outputs, feed_dict={inputs: random_inputs[-1:]})[0]

# Should print the same line, twice.
print(a.shape, a.ravel()[0])
print(b.shape, b.ravel()[0])
```

This script should print the same line twice.

Expected result:
```
(29, 72, 72, 16) -5.843975
(29, 72, 72, 16) -5.843975
```

Actual result:
```
(29, 72, 72, 16) -4.591032
(29, 72, 72, 16) -5.843975
```

Run on a GeForce GTX 1080 Ti:

```
Python 3.6.3
Tensorflow 1.10.1
Numpy 1.15.1

2018-09-05 15:55:27.172809: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-09-05 15:55:27.685907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:8a:00.0
totalMemory: 10.91GiB freeMemory: 10.75GiB
2018-09-05 15:55:27.685952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2018-09-05 15:55:28.161475: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-09-05 15:55:28.161524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 
2018-09-05 15:55:28.161532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N 
2018-09-05 15:55:28.161948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10398 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:8a:00.0, compute capability: 6.1)

(29, 72, 72, 16) -4.591032
(29, 72, 72, 16) -5.843975
```

Run on a GeForce GTX 980 Ti:

```
Python 3.6.3
Tensorflow 1.5.1
Numpy 1.14.2

2018-09-05 16:05:54.557373: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-09-05 16:05:57.096399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties:
name: GeForce GTX 980 Ti major: 5 minor: 2 memoryClockRate(GHz): 1.076
pciBusID: 0000:02:00.0
totalMemory: 5.94GiB freeMemory: 5.83GiB
2018-09-05 16:05:57.096467: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 980 Ti, pci bus id: 0000:02:00.0, compute capability: 5.2)

(29, 72, 72, 16) -4.591032
(29, 72, 72, 16) -5.843975
```

Run on a K80:

```
Python 3.6.5
Tensorflow 1.5.1
Numpy 1.15.0

2018-09-05 14:37:31.279189: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-09-05 14:37:32.131417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-09-05 14:37:32.131763: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties:
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:1e.0
totalMemory: 11.17GiB freeMemory: 11.10GiB
2018-09-05 14:37:32.131795: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)

(29, 72, 72, 16) -5.843975
(29, 72, 72, 16) -5.843975
```"
22094,"Possible bug with map_fn in Graph mode during training, works fine with Eager execution","Tensorflow 1.10 on Ubuntu 16.04

A simplified code example, see also stackoverflow: https://stackoverflow.com/questions/52187269/tensorflow-map-fn-does-not-work-in-graph-mode-during-training-in-eager-mode-it

This loss function works:
```
def discriminative_loss_working(y_true, y_pred):
    # Compute the loss for only the first image in the batch
    
    prediction = y_pred[0]
    label = y_true[0]

    # Number of clusters in ground truth
    clusters,_ = tf.unique(tf.reshape(label, [-1]))

    # Compute cluster means and variances for each cluster
    def compute_mean(c):
        mask = tf.equal(label[:,:,0], c)
        masked_pixels = tf.boolean_mask(prediction, mask)
        cluster_mean = tf.reduce_mean(masked_pixels, axis=0)

        return cluster_mean

    cluster_means = tf.map_fn(compute_mean, clusters, dtype=(tf.float32))
    return tf.reduce_mean(cluster_means)
```

However, when inserting an extra map_fn to work with batch sizes > 1 it doesnot work:
```
def discriminative_loss(y_true, y_pred):
    """"""Computes loss for a batch of images
    Args:
        y_true: (n, h, w) where each elements contains the ground truth instance id
        y_pred: (n, h, w, d) d-dimensional vector for each pixel for each image in the batch
    Returns:
        loss
    """"""
    # Compute the loss for each image in the batch
    def compute_loss(input):
        prediction = input[1]
        label = input[0]

        # Number of clusters in ground truth
        clusters,_ = tf.unique(tf.reshape(label, [-1]))

        # Compute cluster means and variances for each cluster
        def compute_mean(c):
            mask = tf.equal(label[:,:,0], c)
            masked_pixels = tf.boolean_mask(prediction, mask)
            cluster_mean = tf.reduce_mean(masked_pixels, axis=0)

            return cluster_mean

        cluster_means = tf.map_fn(compute_mean, clusters, dtype=(tf.float32))
        return tf.reduce_mean(cluster_means)
        
    # We want to know the loss for each image in the batch
    losses = tf.map_fn(compute_loss, (y_true,y_pred), dtype=(tf.float32))
    return losses
```

The error is:

> 018-09-05 16:07:24.740128: W tensorflow/core/framework/op_kernel.cc:1275] OP_REQUIRES failed at tensor_array_ops.cc:121 : Not found: Resource __per_step_6/_tensor_arraysloss/output_1_loss/map/while/map/TensorArray_1_3/N10tensorflow11TensorArrayE does not exist.
> Traceback (most recent call last):
>   File ""instancesegmenter/test.py"", line 90, in <module>
>     model.fit(train_dataset, epochs=5, steps_per_epoch=2)
>   File ""/home/derk/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py"", line 1363, in fit
>     validation_steps=validation_steps)
>   File ""/home/derk/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training_arrays.py"", line 205, in fit_loop
>     outs = f(ins)
>   File ""/home/derk/.local/lib/python3.5/site-packages/tensorflow/python/keras/backend.py"", line 2914, in __call__
>     fetched = self._callable_fn(*array_vals)
>   File ""/home/derk/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1382, in __call__
>     run_metadata_ptr)
>   File ""/home/derk/.local/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 519, in __exit__
>     c_api.TF_GetCode(self.status.status))
> tensorflow.python.framework.errors_impl.NotFoundError: Resource __per_step_6/_tensor_arraysloss/output_1_loss/map/while/map/TensorArray_1_3/N10tensorflow11TensorArrayE does not exist.
> 	 [[Node: training/SGD/gradients/loss/output_1_loss/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3 = TensorArrayGradV3[_class=[""loc:@train...ad/truediv""], source=""training/SGD/gradients"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](training/SGD/gradients/loss/output_1_loss/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3/StackPopV2, training/SGD/gradients/loss/output_1_loss/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3/StackPopV2_1)]]
> 	 [[Node: training/SGD/gradients/loss/output_1_loss/map/while/map/while/Mean_grad/truediv/_161 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_608_t...ad/truediv"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](^_clooptraining/SGD/gradients/loss/output_1_loss/map/while/map/while/boolean_mask/Reshape/Enter_grad/Switch/_36)]]

Note that this works fine in Eager mode and also when only doing a forward pass. However the backward pass when in graph mode gives the error.

The full code example to reproduce the issue:

```
import tensorflow as tf
import numpy as np

def discriminative_loss(y_true, y_pred):
    """"""Computes loss for a batch of images
    Args:
        y_true: (n, h, w) where each elements contains the ground truth instance id
        y_pred: (n, h, w, d) d-dimensional vector for each pixel for each image in the batch
    Returns:
        loss
    """"""
    # Compute the loss for each image in the batch
    def compute_loss(input):
        prediction = input[1]
        label = input[0]

        # Number of clusters in ground truth
        clusters,_ = tf.unique(tf.reshape(label, [-1]))

        # Compute cluster means and variances for each cluster
        def compute_mean(c):
            mask = tf.equal(label[:,:,0], c)
            masked_pixels = tf.boolean_mask(prediction, mask)
            cluster_mean = tf.reduce_mean(masked_pixels, axis=0)

            return cluster_mean

        cluster_means = tf.map_fn(compute_mean, clusters, dtype=(tf.float32))
        return tf.reduce_mean(cluster_means)
        
    # We want to know the loss for each image in the batch
    losses = tf.map_fn(compute_loss, (y_true,y_pred), dtype=(tf.float32))
    return losses

def discriminative_loss_working(y_true, y_pred):
    # Compute the loss for only the first image in the batch
    
    prediction = y_pred[0]
    label = y_true[0]

    # Number of clusters in ground truth
    clusters,_ = tf.unique(tf.reshape(label, [-1]))

    # Compute cluster means and variances for each cluster
    def compute_mean(c):
        mask = tf.equal(label[:,:,0], c)
        masked_pixels = tf.boolean_mask(prediction, mask)
        cluster_mean = tf.reduce_mean(masked_pixels, axis=0)

        return cluster_mean

    cluster_means = tf.map_fn(compute_mean, clusters, dtype=(tf.float32))
    return tf.reduce_mean(cluster_means)

class MyModel(tf.keras.Model):
    def __init__(self, input_shape):
        super(MyModel, self).__init__()
        self.conv = tf.keras.layers.Conv2D(filters=4, kernel_size=(1,1))

    def call(self, input):
        return self.conv(input)

input_shape = (1,128,128,3)
def my_gen():
    while True:
        x = np.random.rand(1,input_shape[1], input_shape[2],3)
        y = np.random.randint(11000, 11015, (input_shape[1], input_shape[2],1))
        yield x,y

train_dataset = tf.data.Dataset.from_generator(my_gen, (tf.float32, tf.float32))
train_dataset = train_dataset.batch(1)
train_dataset = train_dataset.repeat()

model = MyModel(input_shape=input_shape)

# This is a fix to make loading weights possible
# x = tf.zeros((1,) + input_shape)
x = tf.zeros(input_shape)
y = model(x)

optimizer = tf.keras.optimizers.SGD(lr=0.0001)
model.compile(loss=discriminative_loss,optimizer=optimizer)
model.fit(train_dataset, epochs=5, steps_per_epoch=2)
```
"
22093,AttributeError: 'SsdAnchorGenerator' object has no attribute 'height_stride',"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
I am using the stock scripts with a custom training dataset
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
n/A
- **TensorFlow installed from (source or binary)**:
Installed via Anaconda command line
- **TensorFlow version (use command below)**:
b'v1.10.0-rc1-19-g656e7a2b34' 1.10.0
- **Python version**:
anaconda                  5.1.0                    py36_2
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
python train.py --logtostderr --train_dir=training\\ --pipeline_config_path=training\\test.config

### Describe the problem
I'm trying to train various models using a custom dataset.  I am able to train using the Faster RCNN Inception V2 without issue.  However, all of the other models I try to use throw the same error:

AttributeError: 'SsdAnchorGenerator' object has no attribute 'height_stride'

I've tried with ssd_mobilenet_v1_coco,ssd_mobilenet_v2_coco, and ssd_inception_v2_coco.  All three produce the same error.

### Source code / logs
C:\ProgramData\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Traceback (most recent call last):
  File ""train.py"", line 168, in <module>
    tf.app.run()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""train.py"", line 164, in main
    worker_job_name, is_chief, FLAGS.train_dir)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\trainer.py"", line 240, in train
    detection_model = create_model_fn()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\builders\model_builder.py"", line 98, in build
    add_background_class)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\builders\model_builder.py"", line 178, in _build_ssd_model
    ssd_config.anchor_generator)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\builders\anchor_generator_builder.py"", line 59, in build
    if ssd_anchor_generator_config.height_stride:
AttributeError: 'SsdAnchorGenerator' object has no attribute 'height_stride'
PS C:\Users\timothy.molner\Desktop\tensorflow\research\object_detection> python train.py --logtostderr --train_dir=training\\ --pipeline_config_path=training\\test.config"
22092,"TF Record, Example, and SeqeunceExample inconsistency(?) / Documentation request","I have posted 2 stack overflow questions regarding TF Records and (Sequence)Examples.
I even had a bounty on one. 
Despite only receiving up-votes. Neither received any support.
Both had an associated Colab document to assist.

The questions where

1. [Store images as byte strings or per channel?]https://stackoverflow.com/questions/52035692/tensorflow-v1-10-store-images-as-byte-strings-or-per-channel)

2. [Recovering TF Records](https://stackoverflow.com/questions/52064866/tensorflow-1-10-tfrecorddataset-recovering-tfrecords)

With the corresponding colabs

[Colab 1](https://colab.research.google.com/drive/1HUGoXfgxp0A_0eSdaCzutOkFvnYZ-egv)
[Colab 2](https://colab.research.google.com/drive/1M10tbHih5eJ8LiApJSKKpNM79IconYJX)


I am creating an issue as I am struggling to see the consistency in the behavior between tf Records, Example, SequenceExample, Feature, Features, FeatureList, FeatureLists, FixLenFeature, FixLenSequenceFeature, and VarLenFeature.

In this [Colab](https://colab.research.google.com/drive/1M10tbHih5eJ8LiApJSKKpNM79IconYJX)

I survey 10 ways of encoding an array / array of arrays.

-    Example: Int64 feature (int array)
-    Example: Float feature (float array)
-    Example: Bytes feature (int array dumped to byte string)
-    SequenceExample: Int64 feature list (array of int arrays)
-    SequenceExample: Float feature list (array of float arrays)
-    SequenceExample: Bytes feature list (array of int arrays dumped to byte strings)
-    Example: Bytes feature (array of int arrays all of which is dumped to byte string)
-    SequenceExample: Bytes feature list (array of int arrays dumped to byte strings)
-    SequenceExample: Bytes feature list (array of int arrays all of which is dumped to byte string)
-    SequenceExample: Bytes feature list (array of int arrays, where each int is dumped to byte string)

In short, with the exception of 8, I was able to 'recover' (write to tf.record and read back the data). However, it should be noted that for methods 7 and 10, the retrieved array is flattened.

Further, when I state 'recover' I mean that of course that the feature is wrapped in an addition array. e.g. shape (3, ) becomes (1, 3).


As most tutorials of TF Record, Example and SequenceExample are just a rehashing of the one example provided in the TF Docs (for a movie recommender system, although code is provided for images). 

I think it would be beneficial for a ""style guide"" of sorts, given that one can their data in a myriad of ways. (e.g. are byte strings superior to float feature lists? should one break their image / sequence down per channel? why is there FixLenSeqeuenceFeature but no VarLenSequenceFeature?, if Example and SequenceExample are meant to contain a single data record, why are the only options List based features?)

I would be happy to contribute to more documentation / examples if I knew the answer to these questions. However, to my unexperienced eyes the mismatch between converting data into a (Sequence)Example and extracting it out from a Record is bewildering.



-------

TensorFlow Butler requested information:

Have I written custom code: yes, see linked colabs and S.O. posts.
OS Platform and Distribution: N/a (irrelevant, but Ubuntu 16.04LTS / macOS High Sierra)
TensorFlow installed from: (irrelevant, but conda / Colab's V.M.)
TensorFlow version: 1.10
Bazel version: N/a
CUDA/cuDNN version: N/a (irrelevant but latest / Colab's V.M.'s)
GPU model and memory: N/a (irrelevant, but NVIDIA Titan X / NVIDIA Titan V / Colab's GPU)
Exact command to reproduce: see linked to code
Mobile device: N/a  (irrelevant) 


This is more of a ""meta"" question regarding the structuring of Example / SequenceExample, the correspondingly needed Feature, Features, FeatureList, FeatureLists, etc and the parsing equivalents FixedLenFeature, VariableLenFeature, FixedLenSequenceFeature.

More specifically:

- a request for more documentation
- a request for ""best"" practices recording encoding of rank 2+ tensors

   +  e.g. if I have a sequence in an _numpy.ndarray_ with _n_ channels should I ""dump"" (call _.tostring()_) on the entire sequence and store in a BytesFeature in an Example? or split each channel up into a Float/Int Feature (or dump to BytesFeature) in an Example / SequenceExample. 

-  a request for ""best"" practices as to why ever use Example, when it seems that the ""context features"" are equivalent and then if sequence features are needed later it is easier to adapt? (i.e. the difference between context features in sequence example and normal features of example rather than just calling context features ""meta-data"" of the sequences)

- a request for ""best' practices for recording rank 0 tensors (as they have to be wrapped into a feature list, so should similarly typed features be concatenated together)

- a request for elucidating why sometimes decoding a bytes array losses the rank from which it was encoded (see linked colab)

- a request for a mid-tier api at the feature level e.g. remove the need for the user to define how a feature should be encoded as a record and then decoded  (parsed) from a serialized record. I have a conceptual prototype that I can include here or in another colab if interested. 

etc, etc
"
22091,Failed to load the native TensorFlow runtime.,"Traceback (most recent call last):
  File ""test_pixel_link.py"", line 5, in <module>
    import tensorflow as tf
  File ""/home/aashish/anaconda3/envs/pixel_link/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/aashish/anaconda3/envs/pixel_link/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 51, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/aashish/anaconda3/envs/pixel_link/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/aashish/anaconda3/envs/pixel_link/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/aashish/anaconda3/envs/pixel_link/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/aashish/anaconda3/envs/pixel_link/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: libcuda.so.1: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
22090,import tensorflow as tf,"import tensorflow as tf
Traceback (most recent call last):

  File ""<ipython-input-1-64156d691fe5>"", line 1, in <module>
    import tensorflow as tf

  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import

  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow

  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)

ImportError: Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\ProgramData\Anaconda3\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 985, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 968, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 957, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 938, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed with error code -1073741795

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\ProgramData\Anaconda3\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
22089,ResourceExhaustedError,"Hi, I have trained CNN model on overall 3021 images (train data) which belongs to 9 classes (labeled data). I have 453 test images to verify my model predictions. My training model built fine with no issues (~95% accuracy) but while I am predicting it on test data, after running on some 40-45 images(also printing their predictions), it does not able to load and process further data/images to make prediction and shows below error.
I have tried reducing the batch size, but problems still persist.


Error:
ResourceExhaustedError: OOM when allocating tensor with shape[64,64,3,3] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[Node: block1_conv2_77/convolution = Conv2D[T=DT_FLOAT, data_format=""NCHW"", dilations=[1, 1, 1, 1], padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](block1_conv1_77/Relu, block1_conv2_77/kernel/read)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

*****************************************************************


For the reference, I am attaching my source code(please check model.zip for code) and error screenshot and I am using Tensorflow  1.10.0  with Python 3.6.5


Note: My system configuration -
Intel i7, 32GB RAM, NVIDIA 1080 GPU, Windows 10.
![error](https://user-images.githubusercontent.com/24571705/45087543-e5ab9880-b123-11e8-9bae-ebaa1cda965c.png)
![gpu](https://user-images.githubusercontent.com/24571705/45087544-e6442f00-b123-11e8-9a13-11ea42f5c0b6.PNG)



[model.zip](https://github.com/tensorflow/tensorflow/files/2352317/model.zip)



-----
Regards,
Ajay Sharma
"
22088,distribute.MirroredStrategy fails with Resource exhausted: OOM when allocating tensor with shape,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:  source r1.10
- **TensorFlow version (use command below)**: 1.10
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem

We have a training code based on `tf.Estimator` that works well on single GPU with `tf.contrib.distribute.OneDeviceStrategy(""device:GPU:0"")`. But when we add another GPU and change the distribution type to `tf.contrib.distribute.MirroredStrategy(num_gpus=num_gpus)` the training code doesn't run anymore and raise an ugly memory allocation error. Even if we reduce drastically the batch size (from 128 to 64). Below you'll find a sample of the output error.

### Source code / logs

```
2018-09-05 12:06:36.826713: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fa445486000 of size 2013265920
2018-09-05 12:06:36.826719: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fa4bd486000 of size 2013265920
2018-09-05 12:06:36.826725: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0x7fa535486000 of size 2013265920
2018-09-05 12:06:36.826730: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fa5ad486000 of size 2013501696
2018-09-05 12:06:36.826735: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0x7fa6254bf900 of size 1855833856
2018-09-05 12:06:36.826741: I tensorflow/core/common_runtime/bfc_allocator.cc:671]      Summary of in-use Chunks by size:
2018-09-05 12:06:36.826748: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 17 Chunks of size 256 totalling 4.2KiB
2018-09-05 12:06:36.826754: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 1280 totalling 2.5KiB
2018-09-05 12:06:36.826760: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 3 Chunks of size 2048 totalling 6.0KiB
2018-09-05 12:06:36.826766: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 2304 totalling 2.2KiB
2018-09-05 12:06:36.826772: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 9728 totalling 19.0KiB
2018-09-05 12:06:36.826778: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 73728 totalling 144.0KiB
2018-09-05 12:06:36.826784: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 147456 totalling 288.0KiB
2018-09-05 12:06:36.826791: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 13516800 totalling 12.89MiB
2018-09-05 12:06:36.826797: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 2013265920 totalling 3.75GiB
2018-09-05 12:06:36.826803: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 2013501696 totalling 1.88GiB
2018-09-05 12:06:36.826809: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Sum Total of in-use chunks: 5.64GiB
2018-09-05 12:06:36.826818: I tensorflow/core/common_runtime/bfc_allocator.cc:680] Stats:
Limit:                 11922948096
InUse:                  6054027520
MaxInUse:               9980828416
NumAllocs:                     153
MaxAllocSize:           3507027968

2018-09-05 12:06:36.826832: W tensorflow/core/common_runtime/bfc_allocator.cc:279] *_______________***********************************________________******************_______________
2018-09-05 12:06:36.826879: W tensorflow/core/framework/op_kernel.cc:1275] OP_REQUIRES failed at nccl_ops.cc:96 : Resource exhausted: OOM when allocating tensor with shape[503375361] and type float on /job:localhost/replica:0/task:0/device:GPU:1 by allocator GPU_1_bfc

...

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[503375361] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[Node: NcclAllReduce = NcclAllReduce[T=DT_FLOAT, _class=[""loc:@Reshape_28""], num_devices=2, reduction=""sum"", shared_name=""c0"", _device=""/job:localhost/replica:0/task:0/device:GPU:0""](concat)]]
```"
22087,Provide wheels for multiple versions of CUDA officially,"Ubuntu 18.04, the latest LTS version, only supports CUDA 9.2, but there is no wheel available for that combination of CUDA and Linux.  This prevents servers where Ubuntu 16.04 are installed from upgrading to 18.04.

On the other hand, PyTorch and CuPy officially provides wheels multiple versions of CUDA.  TF may be the only deep learning framework that is available in PyPI and confines the version of CUDA to specific one.

Let Ubuntu 18.04 use TF without building from source and Ubuntu 16.04 upgrade to 18.04!"
22086,Lack of support for half precision type in linear algebra operators,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 7
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: v1.9.0-0-g25c197e023 1.9.0
- **Python version**: Python 3.6.5
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: Tesla V-100 16160MiB
- **Exact command to reproduce**:
Minimal example to reproduce the issue
```
import tensorflow as tf
from tensorflow.python.ops import gen_linalg_ops

a = tf.placeholder(tf.float16, shape=[2, 2])
gen_linalg_ops.qr(a)
```

### Problem
There seems to be no support for float16 type in [linalg_ops.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/linalg_ops.cc) at all. Is there any reason as to why that is the case? Or a way around it?

I have run into this issue when trying to use keras with float16 (by editing `.keras/keras.json`), sequential model with GRU cells. Code included for completeness:
```
import tensorflow.keras as keras

model = keras.Sequential()
model.add(keras.layers.GRU(128, recurrent_dropout=0.75, input_shape=(200, 4)))
```

### Log
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File "".../python3.6/site-packages/tensorflow/python/ops/gen_linalg_ops.py"", line 1494, in qr
    ""Qr"", input=input, full_matrices=full_matrices, name=name)
  File "".../python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 609, in _apply_op_helper
    param_name=input_name)
  File "".../python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 60, in _SatisfiesTypeConstraint
    "", "".join(dtypes.as_dtype(x).name for x in allowed_list)))
TypeError: Value passed to parameter 'input' has DataType float16 not in list of allowed values: float64, float32, complex64, complex128
```"
22085,How to convert to .pb when there are multiple sess.run in graph,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22082,Build failes,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux powerai 4.4.0-109-generic #132-Ubuntu SMP Tue Jan 9 20:00:40 UTC 2018 ppc64le ppc64le ppc64le GNU/Linux

- **TensorFlow installed from (source or binary)**:
source (R1.11)
- **TensorFlow version (use command below)**:
R1.11
- **Python version**:
3.5.2
- **Bazel version (if compiling from source)**:
1.15.2
- **GCC/Compiler version (if compiling from source)**:
5.4.0 20160609 
- **CUDA/cuDNN version**:
CUDA 9.2, cuDNN 7.1
- **GPU model and memory**:
4 x  Tesla P100-SXM2-16GB
- **Exact command to reproduce**:
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package


### Describe the problem
Build failes at the end:

cc1plus: warning: unrecognized command line option '-Wno-self-assign'
ERROR: /root/tensorflow/tensorflow/BUILD:592:1: Executing genrule //tensorflow:tensorflow_python_api_gen failed (Exit 1)
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 27, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 52, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>
    from google.protobuf import descriptor as _descriptor
  File ""/usr/local/lib/python3.5/dist-packages/google/protobuf/__init__.py"", line 37, in <module>
    __import__('pkg_resources').declare_namespace(__name__)
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 2927, in <module>
    @_call_aside
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 2913, in _call_aside
    f(*args, **kwargs)
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 2952, in _initialize_master_working_set
    add_activation_listener(lambda dist: dist.activate())
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 956, in subscribe
    callback(dist)
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 2952, in <lambda>
    add_activation_listener(lambda dist: dist.activate())
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 2515, in activate
    declare_namespace(pkg)
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 2097, in declare_namespace
    _handle_ns(packageName, path_item)
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 2047, in _handle_ns
    _rebuild_mod_path(path, packageName, module)
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 2066, in _rebuild_mod_path
    orig_path.sort(key=position_in_sys_path)
AttributeError: '_NamespacePath' object has no attribute 'sort'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 7114.719s, Critical Path: 1859.33s
INFO: 4737 processes: 4737 local.
FAILED: Build did NOT complete successfully
"
22081,GDR Cannot register memory region,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: no
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:1.10
- **Python version**: 2.7 
- **Bazel version (if compiling from source)**: 0.16.1
- **GCC/Compiler version (if compiling from source)**: 5.4
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: K40c 12G
- **Exact command to reproduce**: 
**Worker :** python benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py --local_parameter_device=gpu --num_gpus=4 --batch_size=128 --num_epochs=10 --model=alexnet --variable_update=distributed_replicated --data_dir=imagenet-data --all_reduce_spec=pscpu --job_name=worker --ps_hosts=10.10.10.6:2222 --worker_hosts=10.10.10.5:3333 --task_index=0 --server_protocol=grpc+gdr
**PS :** CUDA_VISIBLE_DEVICES='' python benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py --local_parameter_device=cpu --num_gpus=0 --batch_size=128 --num_epochs=10 --model=alexnet --variable_update=distributed_replicated --data_dir=imagenet-data --all_reduce_spec=pscpu --job_name=ps --ps_hosts=10.10.10.6:2222 --worker_hosts=10.10.10.5:3333 --task_index=0 --server_protocol=grpc+gdr

## Describe the problem
I tried to use GDR, but it seems no performance improvement compared to native RDMA, and logs are the following:

```
$ python benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py --local_parameter_device=cpu --num_gpus=4 --batch_size=128 --num_epochs=10 --model=alexnet --variable_update=distributed_replicated --data_dir=/home/shuai/imagenet-data --all_reduce_spec=pscpu --job_name=worker --ps_hosts=10.10.10.6:2222 --worker_hosts=10.10.10.5:3333 --task_index=0 --server_protocol=grpc+gdr
2018-09-05 15:05:58.062100: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties:
name: Tesla K40c major: 3 minor: 5 memoryClockRate(GHz): 0.745
pciBusID: 0000:02:00.0
totalMemory: 11.92GiB freeMemory: 11.84GiB
2018-09-05 15:05:58.211502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 1 with properties:
name: Tesla K40c major: 3 minor: 5 memoryClockRate(GHz): 0.745
pciBusID: 0000:03:00.0
totalMemory: 11.92GiB freeMemory: 11.84GiB
2018-09-05 15:05:58.374791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 2 with properties:
name: Tesla K40c major: 3 minor: 5 memoryClockRate(GHz): 0.745
pciBusID: 0000:83:00.0
totalMemory: 11.92GiB freeMemory: 11.84GiB
2018-09-05 15:05:58.553152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 3 with properties:
name: Tesla K40c major: 3 minor: 5 memoryClockRate(GHz): 0.745
pciBusID: 0000:84:00.0
totalMemory: 11.92GiB freeMemory: 11.84GiB
2018-09-05 15:05:58.553735: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0, 1, 2, 3
2018-09-05 15:05:59.918282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-09-05 15:05:59.918335: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 1 2 3
2018-09-05 15:05:59.918344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N Y N N
2018-09-05 15:05:59.918349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 1:   Y N N N
2018-09-05 15:05:59.918354: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 2:   N N N Y
2018-09-05 15:05:59.918359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 3:   N N Y N
2018-09-05 15:05:59.919609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:0 with 11473 MB memory) -> physical GPU (device: 0, name: Tesla K40c, pci bus id: 0000:02:00.0, compute capability: 3.5)
2018-09-05 15:06:00.088804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:1 with 11473 MB memory) -> physical GPU (device: 1, name: Tesla K40c, pci bus id: 0000:03:00.0, compute capability: 3.5)
2018-09-05 15:06:00.288448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:2 with 11473 MB memory) -> physical GPU (device: 2, name: Tesla K40c, pci bus id: 0000:83:00.0, compute capability: 3.5)
2018-09-05 15:06:00.487520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:3 with 11473 MB memory) -> physical GPU (device: 3, name: Tesla K40c, pci bus id: 0000:84:00.0, compute capability: 3.5)
2018-09-05 15:06:00.688704: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> 10.10.10.6:2222}
2018-09-05 15:06:00.688752: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:3333}
2018-09-05 15:06:00.697485: I tensorflow/contrib/gdr/gdr_memory_manager.cc:254] RDMA server is listening on 10.10.10.5:3333
2018-09-05 15:06:00.697568: I tensorflow/contrib/gdr/gdr_memory_manager.cc:302] Instrumenting CPU allocator cuda_host_bfc
2018-09-05 15:06:00.697588: I tensorflow/contrib/gdr/gdr_memory_manager.cc:302] Instrumenting CPU allocator cpu_pool
2018-09-05 15:06:00.697607: I tensorflow/contrib/gdr/gdr_memory_manager.cc:302] Instrumenting CPU allocator cpu_rdma_bfc
2018-09-05 15:06:00.697830: I tensorflow/contrib/gdr/gdr_memory_manager.cc:95] NUMA node for device: mlx4_0 is 1
2018-09-05 15:06:00.734616: W tensorflow/contrib/gdr/gdr_memory_manager.cc:705] Cannot register memory region
2018-09-05 15:06:00.771893: W tensorflow/contrib/gdr/gdr_memory_manager.cc:705] Cannot register memory region
2018-09-05 15:06:00.771954: I tensorflow/contrib/gdr/gdr_memory_manager.cc:314] Instrumenting GPU allocator with bus_id 2
2018-09-05 15:06:00.772783: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:375] Started server with target: grpc://localhost:3333
```
and here is GPU topology:

```
$ nvidia-smi topo -m
        GPU0    GPU1    GPU2    GPU3    mlx4_0  CPU Affinity
GPU0     X      PHB     SYS     SYS     SYS     0-7,16-23
GPU1    PHB      X      SYS     SYS     SYS     0-7,16-23
GPU2    SYS     SYS      X      PHB     PHB     8-15,24-31
GPU3    SYS     SYS     PHB      X      PHB     8-15,24-31
mlx4_0  SYS     SYS     PHB     PHB      X

Legend:

  X    = Self
  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)
  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node
  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)
  PXB  = Connection traversing multiple PCIe switches (without traversing the PCIe Host Bridge)
  PIX  = Connection traversing a single PCIe switch
  NV#  = Connection traversing a bonded set of # NVLinks
```

The logs said,"" cannot register memory region"" in GPU 2 and GPU 3. Is it a bug?"
22080,The performance of FusedBatchNormGradGrad is very low,"When I test the performance of GAN model (https://github.com/tensorflow/models/tree/master/research/gan), I find to use FusedBatchNorm can boost the performance by 20% to 30% (https://github.com/tensorflow/models/issues/5206).

However, when use gan_loss with gradient_penalty, there is no performance improvement with FusedBatchNorm.  I find the root cause is FusedBatchNormGradGrad is very low performance.

I propose to set fused_batch_norm to false on (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gan/python/losses/python/losses_impl.py#L367) to use BatchNormGradGrad to replace FusedBatchNormGradGrad. There is about 20% performance improvement with this change. 
Change
`disc_interpolates = discriminator_fn(interpolates, generator_inputs)`
to 
`disc_interpolates = discriminator_fn(interpolates, generator_inputs, fused_batch_norm=false)`
   
"
22079,bazel build tensorflow/python/tools:print_selective_registration_header,"ERROR: /Users/xihu/Desktop/tensorflow-1.10.1/tensorflow/BUILD:581:1: Executing genrule //tensorflow:tensorflow_python_api_gen failed (Exit 1)
Traceback (most recent call last):
  File ""/private/var/tmp/_bazel_xihu/07e18ff1fe4322e207df6775908e278a/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 27, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""/private/var/tmp/_bazel_xihu/07e18ff1fe4322e207df6775908e278a/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 63, in <module>
    from tensorflow.python.framework.framework_lib import *  # pylint: disable=redefined-builtin
  File ""/private/var/tmp/_bazel_xihu/07e18ff1fe4322e207df6775908e278a/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/framework/framework_lib.py"", line 52, in <module>
    from tensorflow.python.framework.importer import import_graph_def
  File ""/private/var/tmp/_bazel_xihu/07e18ff1fe4322e207df6775908e278a/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/framework/importer.py"", line 27, in <module>
    from tensorflow.python.framework import function
  File ""/private/var/tmp/_bazel_xihu/07e18ff1fe4322e207df6775908e278a/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/framework/function.py"", line 39, in <module>
    from tensorflow.python.ops import variable_scope as vs
  File ""/private/var/tmp/_bazel_xihu/07e18ff1fe4322e207df6775908e278a/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/ops/variable_scope.py"", line 24, in <module>
    import enum  # pylint: disable=g-bad-import-order
ImportError: No module named enum
Target //tensorflow/python/tools:print_selective_registration_header failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 7515.094s, Critical Path: 226.54s
INFO: 7850 processes: 7850 local.
FAILED: Build did NOT complete successfully


how can i to fix error?"
22078,Tensorflow runtime error (misaligned data) with ARM 32bit NEON.,"With a c++ application using tensorflow in armv7 (32bit arm) + NEON, we had been experiencing the following errors in run-time:
```
[ 7128.415134] Alignment trap: not handling instruction f9048a1f at [<b28ca130>]
[ 7128.420838] Unhandled fault: alignment exception (0x801) at 0xbef9d1d4
[ 7128.427333] pgd = eb6dc000
[ 7128.430014] [bef9d1d4] *pgd=6c6f5831, *pte=b2d5775f, *ppte=b2d57c7f
[ 7128.436295] audit: type=1701 audit(1469483309.425:16): auid=0 uid=0 gid=0 ses=3 subj=User pid=8294 comm=""hello-taos"" exe=""/usr/bin/7
Bus error (core dumped)
```

We are experiencing it even though we are using tensorflow 1.9.0, which has https://github.com/tensorflow/tensorflow/commit/88103d000add4ea7f8d1a34ee3c898fc79d9e3c7 included mentioned in #19158 .

However, @again4you has successfully fixed the issue with a few compiler options mandated although the current fix is created quick and dirty.

@again4you : Please send a PR when you embed the compiler options (for arm32) after rewriting the fix so that the compiler options are added in CMake script (and Bazel as well if it seems not too difficult) for ARM32.


### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Happens both with and without custom codes.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 and Tizen
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No. But same architecture with such devices (armv7)
- **TensorFlow installed from (source or binary)**: github.com source, built with cmake.
- **TensorFlow version (use command below)**: 1.9.0
- **Python version**: 2.7.3
- **Bazel version (if compiling from source)**: N/A (used cmake)
- **GCC/Compiler version (if compiling from source)**: 6.2.1
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: ARM Mali. (not used)
- **Exact command to reproduce**: Execute the C++ executable on a shell. (e.g., ```$ ./app ```

### Describe the problem
Memory misalignment error:
```
[ 7128.415134] Alignment trap: not handling instruction f9048a1f at [<b28ca130>]
[ 7128.420838] Unhandled fault: alignment exception (0x801) at 0xbef9d1d4
```
However, we have a fix, which is going to be cleaned up to be sent as a PR.
"
22077,RoCE v1 error,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: no
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.10
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.16.1
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: K40c 12G
- **Exact command to reproduce**:
 **Worker :** python benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py --local_parameter_device=gpu --num_gpus=4 --batch_size=128 --num_epochs=10 --model=alexnet --variable_update=distributed_replicated --data_dir=imagenet-data --all_reduce_spec=pscpu --job_name=worker --ps_hosts=10.10.10.6:2222 --worker_hosts=10.10.10.5:3333 --task_index=0 --server_protocol=grpc+verbs
**PS :** CUDA_VISIBLE_DEVICES='' python benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py --local_parameter_device=cpu --num_gpus=0 --batch_size=128 --num_epochs=10 --model=alexnet --variable_update=distributed_replicated --data_dir=imagenet-data --all_reduce_spec=pscpu --job_name=ps --ps_hosts=10.10.10.6:2222 --worker_hosts=10.10.10.5:3333 --task_index=0 --server_protocol=grpc+verbs

Hi

 I am trying to run TF on RoCE v1, and I have configured RoCE v1, including PFC, in my cluster. I configured PFC based on Trust L2. That is, I created a VLAN and used VLAN IP to communicate with each other. (I have tested it is correct by using ""ib_write_bw"", because I found packets are received by the pg 4). But when I used the VLAN IP to run distributed Tensorflow (source code from beachmark), I cannot find any packet in the pg 4. I looked up the source code, and found the RDMA_DEVICE_PORT is needed to be configured and the VLAN IP cannot be recognized corrcetly. Is it a bug? or I used RoCE v1 wrongly?

Thanks"
22071,MatMul flop incorrect for 3D inputs,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:N/A
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:N/A
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:v1.10.0-0-g656e7a2b34 1.10.0
- **Python version**:3.7
- **Bazel version (if compiling from source)**:N/A
- **GCC/Compiler version (if compiling from source)**:N/A
- **CUDA/cuDNN version**:N/A
- **GPU model and memory**:N/A
- **Exact command to reproduce**:see below
```python
import tensorflow as tf
a = tf.random_normal([1, 100, 100])
b = tf.random_normal([1, 100, 100])
c = tf.matmul(a, b)
tf.profiler.profile(
            tf.get_default_graph(),
            cmd='op',
            options=tf.profiler.ProfileOptionBuilder.float_operation())
```
The outputs are correct (2M flops) if `a` and `b` have shape `[100, 100]`.
But with 3D inputs, it outputs no flops for matmul.

The reason is that for 3D inputs it uses the ""BatchMatMul"" op which does not have flop statistics implemented."
22069,freeze_graph.py with --input_saved_model argument gives `IOError: SavedModel file does not exist`,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
n/a
- **TensorFlow installed from (source or binary)**:
pip
- **TensorFlow version (use command below)**:
1.10.1
- **Python version**:
2.7.15rc1
- **Bazel version (if compiling from source)**:
n/a
- **GCC/Compiler version (if compiling from source)**:
n/a
- **CUDA/cuDNN version**:
n/a
- **GPU model and memory**:
n/a
- **Exact command to reproduce**:
```
#first, train the model with
python tensorflow/models/research/deeplab/train.py \
    --logtostderr \
    --clone_on_cpu=True \
    --training_number_of_steps=900 \ # 90000 \
    --train_split=""train"" \
    --model_variant=""mobilenet_v2"" \
    --atrous_rates=6 \
    --atrous_rates=12 \
    --atrous_rates=18 \
    --output_stride=16 \
    --decoder_output_stride=4 \
    --train_crop_size=128 \
    --train_crop_size=128 \
    --train_batch_size=1 \
    --dataset=""cityscapes"" \
    --tf_initial_checkpoint=${PATH_TO_INITIAL_CHECKPOINT} \
    --train_logdir=${PATH_TO_TRAIN_DIR} \
    --dataset_dir=${PATH_TO_DATASET}

#then try to freeze the output
python tensorflow/python/tools/freeze_graph.py \
     --input_saved_model=${PATH_TO_TRAIN_DIR} \
     --output_graph=${PATH_TO_TRAIN_DIR}/frozen_graph.pb \
     --output_node_names=MobilenetV2/expanded_conv_16/output
```

### Describe the problem
The first problem is that the train.py script seems to change the structure of the graph. Specifically, the input and output nodes from the `initial_checkpoint` are missing. The normal `output_node_names` such as softmax, predictions etc do not appear in the trained graph.pbtxt, which is why I have used `MobilenetV2/expanded_conv_16/output`. The ImageTensor also does not appear as an input_node.
However, the problem with missing nodes is not the focus of this issue.

The second problem is that `freeze_graph.ph` with the `--input_saved_model` option gives 
```
Traceback (most recent call last):
  File ""/home/username/.local/lib/python2.7/site-packages/tensorflow/python/tools/freeze_graph.py"", line 382, in <module>
    run_main()
  File ""/home/username/.local/lib/python2.7/site-packages/tensorflow/python/tools/freeze_graph.py"", line 379, in run_main
    app.run(main=my_main, argv=[sys.argv[0]] + unparsed)
  File ""/home/username/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/home/username/.local/lib/python2.7/site-packages/tensorflow/python/tools/freeze_graph.py"", line 378, in <lambda>
    my_main = lambda unused_args: main(unused_args, flags)
  File ""/home/username/.local/lib/python2.7/site-packages/tensorflow/python/tools/freeze_graph.py"", line 272, in main
    flags.saved_model_tags, checkpoint_version)
  File ""/home/username/.local/lib/python2.7/site-packages/tensorflow/python/tools/freeze_graph.py"", line 229, in freeze_graph
    input_saved_model_dir, saved_model_tags).graph_def
  File ""/home/username/.local/lib/python2.7/site-packages/tensorflow/python/tools/saved_model_utils.py"", line 42, in get_meta_graph_def
    saved_model = reader.read_saved_model(saved_model_dir)
  File ""/home/username/.local/lib/python2.7/site-packages/tensorflow/contrib/saved_model/python/saved_model/reader.py"", line 55, in read_saved_model
    raise IOError(""SavedModel file does not exist at: %s"" % saved_model_dir)
IOError: SavedModel file does not exist at: /path/given/in/PATH_TO_TRAIN_DIR

$ls ${PATH_TO_TRAIN_DIR}
-rw-r--r-- 1 username username    21786 Sep  3 15:28 model.ckpt-0.index
-rw-r--r-- 1 username username 21955616 Sep  3 15:28 model.ckpt-0.data-00000-of-00001
-rw-r--r-- 1 username username  3070908 Sep  3 15:28 model.ckpt-0.meta
-rw-r--r-- 1 username username  6081043 Sep  4 10:54 graph.pbtxt
-rw-r--r-- 1 username username 21955616 Sep  4 10:54 model.ckpt-900.data-00000-of-00001
-rw-r--r-- 1 username username    21786 Sep  4 10:54 model.ckpt-900.index
-rw-r--r-- 1 username username  3070908 Sep  4 10:54 model.ckpt-900.meta
-rw-r--r-- 1 username username      541 Sep  4 10:54 checkpoint

```
"
22068,Tensorflow C++ error when using CV Mat image ,"I am trying to create a custom OCR engine using Tensorflow(v1.10) in Ubuntu 18.04, with Bazel (0.16) in C++.
------------------------
## Issues Faced
- Core Dumped Error for dtype mismatch (0 vs 1)
- Predicted value of weights is higher than 1, when Tensorflow Session is re-run in loop
-----------------------
#### Code

```
for (size_t i=0; i<filenames.size(); i++)
	{
		cv::Mat image = cv::imread(filenames[i]);

		Tensor image_tensor (tensorflow::DT_FLOAT,tensorflow::TensorShape{1,tf_height,tf_width,3});
		image.convertTo(image, CV_32FC1);
		tensorflow::StringPiece tmp_data = image_tensor.tensor_data();
		memcpy(const_cast<char*>(tmp_data.data()), (image.data), tf_height * tf_width * sizeof(float));

		// Creating a Session with the Graph
std::unique_ptr<tensorflow::Session>session(tensorflow::NewSession(tensorflow::SessionOptions()));
		//session->tensorflow::reset(tensorflow::NewSession(tensorflow::SessionOptions()));
		tensorflow::Status session_create_status = session->Create(graph_def);

		std::vector<std::pair<string, tensorflow::Tensor>> inputs = {{inputLayer, image_tensor}};
		outputs.clear();
		Status runStatus = session->Run(inputs, {outputLayer}, {}, &outputs);
		if (!runStatus.ok()) {
			LOG(ERROR) << ""Running model failed: "" << runStatus;
			return -1;
		}
		cv::imshow(""Original Image"",image);
		cv::waitKey();
		session->Close();
		image.release();
	}
```

-----------------------
"
22066,"`true_positives` returned Variable rather than Tensor or Operation as update_op in ""call_for_each_tower""","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.10.1
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.16.1
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:
```python
def fn():
  c1 = tf.constant([1,1,0,0])
  c2 = tf.constant([1,0,1,0])
  tp, op = tf.metrics.true_positives(c1, c2)
  print(tp)
  print(op)

# Without DistributionStrategy
fn()
# Outputs:
# Tensor(""true_positives/Identity:0"", shape=(), dtype=float32)
# Tensor(""true_positives/AssignAdd:0"", shape=(), dtype=float32_ref)

# With DistributionStrategy
ds = tf.contrib.distribute.OneDeviceStrategy(""/cpu:0"")
with ds.scope():
  ds.call_for_each_tower(fn)
# Outputs:
# Tensor(""true_positives_1/Identity:0"", shape=(), dtype=float32, device=/device:CPU:0)
# <tf.Variable 'true_positives_1/AssignAddVariableOp' shape=() dtype=float32>
```


You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

While using Estimator, we define the metrics in the `model_fn`, and pass it to the `EstimatorSpec` as `eval_metric_ops`. The `EstimatorSpec` raises an error regarding the `update_op` returned by `tf.metrics.true_positives` as it is neither a Tensor or an Operation when it is called through `ds.call_for_each_tower` in a `with ds.scope():` block. It appears that this originates from the `count` variable returned from `metric_variable` not having a reference dtype, which causes `state_ops.add_assign` to return `ref.assign_add` straight, without any Tensor wrapping. This makes it impossible to include `true_positives` in the `model_fn` code if a distribution strategy is to be used with Estimator. The situation holds also for `true_negatives`, `false_positives` and `false_negatives`.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22064,Can't save models trained with tf.contrib.cudnn_rnn.CudnnRNNRelu,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10.1
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0/7.0.3.11
- **GPU model and memory**: Quadro M1200
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

When I try to create a saver for a session with a graph containing a module from the `tf.contrib.cudnn_rnn` package I get `AttributeError: 'Tensor' object has no attribute 'assign'`

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
Traceback (most recent call last):
  File ""train.py"", line 154, in <module>
    update_interval=config['training']['update_interval'])
  File ""train.py"", line 66, in train
    saver = tf.train.Saver()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py"", line 1281, in __init__
    self.build()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py"", line 1293, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py"", line 1330, in _build
    build_save=build_save, build_restore=build_restore)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py"", line 778, in _build_internal
    restore_sequentially, reshape)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py"", line 419, in _AddRestoreOps
    assign_ops.append(saveable.restore(saveable_tensors, shapes))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py"", line 291, in restore
    self._variables, opaque_params, validate_shape=False)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/state_ops.py"", line 217, in assign
    return ref.assign(value, name=name)
AttributeError: 'Tensor' object has no attribute 'assign'
```
"
22063,Tensorflow segfaults in eager mode with DEVICE_PLACEMENT_EXPLICIT,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.4
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.10.1-0-g4dcfddc5d1
- **Python version**: 1.10.1
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 7.1.3
- **GPU model and memory**: GeForce GTX 1080, 8Gb
- **Exact command to reproduce**:
[1]: import tensorflow as tf
[2]: tf.enable_eager_execution(device_policy=tf.contrib.eager.DEVICE_PLACEMENT_EXPLICIT)
[3]: a = tf.zeros((2,2))




Full output:
```
In [1]: import tensorflow as tf

In [2]: tf.enable_eager_execution(device_policy=tf.contrib.eager.DEVICE_PLACEMENT_EXPLICIT)

In [3]: tf.__version__
Out[3]: '1.10.1'

In [4]: a = tf.zeros((2,2))
2018-09-04 12:39:47.441463: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-09-04 12:39:47.524855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-09-04 12:39:47.525190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335
pciBusID: 0000:01:00.0
totalMemory: 7.92GiB freeMemory: 3.93GiB
2018-09-04 12:39:47.525203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2018-09-04 12:39:47.706125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-09-04 12:39:47.706148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 
2018-09-04 12:39:47.706153: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N 
2018-09-04 12:39:47.706287: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3660 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
Segmentation fault (core dumped)
```"
22062,Keras load_model crash for custom layer using tf.gather,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Y
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.11.0-dev20180823
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0/7
- **GPU model and memory**: Nvidia Quadro 4000
- **Exact command to reproduce**: N/A

### Describe the problem
When having a custom layer using tf.gather, the model could be built successfully and run without problem. I can save that model successfully; however, when loading the model, the error occurs. If no tf.gather custom layer used, there is no problem. Is this a bug or a mis-use?

My codes are:

	def test_lambda_layer():
	  data_input = keras.Input(shape=(1,4,5), dtype=float)
	  mask = [0,1,1,0,1]
	  valid_out_idx = np.nonzero(mask)[0]
	  x = Lambda(lambda t: tf.gather(t, valid_out_idx, axis=-1))(data_input)
	  x = Conv2D(5, 1, use_bias=False, kernel_initializer='ones', trainable=False)(x)
	  model = keras.Model(inputs=data_input, outputs=x)
	  model.summary()

	  data = np.ones((1,1,4,5), dtype=float)
	  print(model.predict(data))

	  model.save('test.h5')

	  new_model = keras.models.load_model('test.h5')
	  new_model.summary()

	  print(new_model.predict(data))

The error message is:

	Traceback (most recent call last):
	  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\framework\tensor_util.py"", line 527, in make_tensor_proto
		str_values = [compat.as_bytes(x) for x in proto_values]
	  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\framework\tensor_util.py"", line 527, in <listcomp>
		str_values = [compat.as_bytes(x) for x in proto_values]
	  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\util\compat.py"", line 61, in as_bytes
		(bytes_or_text,))
	TypeError: Expected binary or unicode string, got {'type': 'ndarray', 'value': [1, 2, 4]}

	During handling of the above exception, another exception occurred:

	Traceback (most recent call last):
	  File ""test_keras.py"", line 31, in <module>
		main()
	  File ""test_keras.py"", line 28, in main
		test_lambda_layer()
	  File ""test_keras.py"", line 16, in test_lambda_layer
		new_model = keras.models.load_model('test.h5')
	  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\keras\engine\saving.py"", line 230, in load_model
		model = model_from_config(model_config, custom_objects=custom_objects)
	  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\keras\engine\saving.py"", line 310, in model_from_config
		return deserialize(config, custom_objects=custom_objects)
	  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\keras\layers\serialization.py"", line 64, in deserialize
		printable_module_name='layer')
	  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\keras\utils\generic_utils.py"", line 173, in deserialize_keras_object
		list(custom_objects.items())))
	  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\keras\engine\network.py"", line 1302, in from_config
		process_node(layer, node_data)
	  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\keras\engine\network.py"", line 1260, in process_node
		layer(input_tensors[0], **kwargs)
	  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 770, in __call__
		outputs = self.call(inputs, *args, **kwargs)
	  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\keras\layers\core.py"", line 716, in call
		return self.function(inputs, **arguments)
	  File ""test_keras.py"", line 5, in <lambda>
		x = Lambda(lambda t: tf.gather(t, valid_out_idx, axis=-1))(data_input)
	  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 2650, in gather
		return gen_array_ops.gather_v2(params, indices, axis, name=name)
	  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\ops\gen_array_ops.py"", line 3804, in gather_v2
		""GatherV2"", params=params, indices=indices, axis=axis, name=name)
	  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 513, in _apply_op_helper
		raise err
	  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 510, in _apply_op_helper
		preferred_dtype=default_dtype)
	  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\framework\ops.py"", line 1127, in internal_convert_to_tensor
		ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
	  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\framework\constant_op.py"", line 228, in _constant_tensor_conversion_function
		return constant(v, dtype=dtype, name=name)
	  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\framework\constant_op.py"", line 207, in constant
		value, dtype=dtype, shape=shape, verify_shape=verify_shape))
	  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\framework\tensor_util.py"", line 531, in make_tensor_proto
		""supported type."" % (type(values), values))
	TypeError: Failed to convert object of type <class 'dict'> to Tensor. Contents: {'type': 'ndarray', 'value': [1, 2, 4]}. Consider casting elements to a supported type."
22059,Optimizing slice of variable not possible,"Applying the gradient of a variable slice currently results in a `NotImplemented` error of tf.train.Optimizer.

**The following two examples are working:**
```python
### WORKING ###
X = tf.Variable(2, dtype=tf.float32)
y = tf.constant(10, dtype=""float32"")
loss = y - (X*X)

variables=[X]
gradient = tf.gradients(loss, variables)
gradient = [(g, v) for g, v in zip(gradient, variables)]
train_op = tf.train.AdamOptimizer().apply_gradients(gradient)
```

```python
### WORKING ###
big_X = tf.Variable([2,3,4], dtype=tf.float32)
X = big_X[0]
y = tf.constant(10, dtype=""float32"")
loss = y - (X*X)

train_op = train_op = tf.train.AdamOptimizer().minimize(loss)
```

**The following example throws an error:**
```python
### NOT WORKING ###
big_X = tf.Variable([2,3,4], dtype=tf.float32)
X = big_X[0]
y = tf.constant(10, dtype=""float32"")
loss = y - (X*X)

variables=[X]
gradient = tf.gradients(loss, variables)
gradient = [(g, v) for g, v in zip(gradient, variables)]
train_op = tf.train.AdamOptimizer().apply_gradients(gradient)
```
The error:
```
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py"", line 2963, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-22-10282dee2005>"", line 10, in <module>
    train_op = tf.train.AdamOptimizer().apply_gradients(gradient)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py"", line 605, in apply_gradients
    update_ops.append(processor.update_op(self, grad))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py"", line 189, in update_op
    raise NotImplementedError(""Trying to update a Tensor "", self._v)
NotImplementedError: ('Trying to update a Tensor ', <tf.Tensor 'strided_slice_9:0' shape=() dtype=float32>)
```
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.10.1-0-g4dcfddc5d1 1.10.1
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: NA"
22058,tf.estimator.Estimator with distribution strategy messes up TensorBoard readability,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.12.0
- **Python version**: 2.7
- **Bazel version**: N/A
- **Mobile device**: N/A
- **CUDA/cuDNN version**: cuda-9-0 / libcudnn.so.7
- **GPU model and memory**: 2x NVIDIA T4, 16GB
- **Exact command to reproduce**: example provided here: https://github.com/patzm/tf-estimator-distribute-so

### Describe the problem
Using a `tf.estimator.Estimator` with a distribution strategy like `OneDeviceStrategy` or `MirroredStrategy` creates a lot of tensors / operations that are not enclosed in a common scope.
If one views the graph in TensorBoard, it is very annoying.
TensorBoard automatically zooms out until all nodes are visible, making everything so tiny and unreadable.
Also TensorBoard becomes pretty slow.
In essence this is the
```python
    run_config = tf.estimator.RunConfig(
        model_dir='/tmp/some/dir',
        session_config=session_config,
        train_distribute=MirroredStrategy(num_gpus=2),  # or OneDeviceStrategy(device)
        save_summary_steps=1,
        save_checkpoints_steps=1e4,
    )

    estimator = tf.estimator.Estimator(
        model_fn=model_fn,
        config=run_config,
        params={},
    )
```
Here are two example images:

* `OneDeviceStrategy`:
  
  ![OneDeviceStrategy](https://raw.githubusercontent.com/patzm/tf-estimator-distribute-so/master/images/OneDeviceStrategy.png)

* `MirroredStrategy`:

  ![MirroredStrategy](https://raw.githubusercontent.com/patzm/tf-estimator-distribute-so/master/images/MirroredStrategy.png)

The row / column of `Read_<num>` / `group_deps_<num>` nodes continues way beyond the area of the screenshot.
The initial view of the TensorBoard could look like this, with the nodes almost invisible, because they are so tiny:
![TensorBoardInitialView](https://i.imgur.com/iuhxLCK.png)

### Source code / logs
example provided here: https://github.com/patzm/tf-estimator-distribute-so
I also raised the issue on StackOverflow: https://stackoverflow.com/questions/50924287/how-to-influence-the-name-scope-of-collated-variables-slots

"
22057,Tensorflow as a bazel dependency produces inconsistent builds,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: None
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: [ 1.10.1, 1.10, 1.9, 1.8]  v1.8 does not exhibit this bug.
- **Python version**: 3
- **Bazel version (if compiling from source)**: release 0.16.1
- **GCC/Compiler version (if compiling from source)**:  7.3.0
- **CUDA/cuDNN version**: 9.2 / 7.2.1
- **GPU model and memory**:  GeForce GTX 1050, 4 GB, Driver 396.54
- **Exact command to reproduce**:
  - Extract broken.zip [broken.zip](https://github.com/tensorflow/tensorflow/files/2349443/broken.zip)
  - bazel build ...
  - Add a z to changling.h::baz character array.
  - bazel build ... [ NOTE there was no rebuild of any targets ]

### Describe the problem
It appears that specifying `build --config=cuda` and `build:cuda --crosstool_top=@local_config_cuda//crosstool:toolchain` in bazel.rc are breaking bazels ability to detect changes.  Removing these two lines & bazel detects source changes & recompiles as appropriate.

I tried [master, v1.10.1, v1.10, v1.9, v1.8] this bug appears somewhere between v1.8 & v1.9 of tensorflow.  (maybe I will binary search to find what rev...).

### Source code / logs
[broken.zip](https://github.com/tensorflow/tensorflow/files/2349443/broken.zip)
"
22056,Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED,"TensorFlow: 1.10.1
cuda: 9.0
cudnn: cudnn7.1 for cuda9.0
python3
Ubuntu 18.04
nvidia-driver: 390

Try running cnns error got:

```
Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED
2018-09-04 23:13:52.571542: E tensorflow/stream_executor/cuda/cuda_dnn.cc:360] Possibly insufficient driver version: 390.77.0

```
I think the cuda version are already the newest version, and my cudnn and cuda are OK. Why still got this problem? The error message throw from cuda_dnn.cc should be more specific what's going on."
22055,TensorFlow Speech Recognition for Android,"Hello,

I am new to this exicted feature TensorFlow. I could import the Sample project and able to run it successfully. Now i just need help to how to build the Speech Recognition from TensorFlow what are the files required to add. Can someone give the steps, which would help me in my sample application

Thanks
Sravan Kumar Shetty"
22054,map_fn gives colocation errors with integer typed input tensors,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
CentOS Linux release 7.4.1708
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
binary (pip3 install tensorflow-gpu==1.9.0)
- **TensorFlow version (use command below)**:
v1.9.0-0-g25c197e023 1.9.0
- **Python version**:
3.6.4
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
9.0.176
- **GPU model and memory**:
 GeForce GTX-1080 - 8GB memory
- **Exact command to reproduce**:
`python3 map_fn_GPU_error.py`

### Describe the problem
The `tf.map_fn` function raises the following error if I provide two tensors of input one of which is `tf.float32` and the other `tf.int64`. This error doesn't appear if both of them are `tf.float32`.

```
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'map/TensorArray_1': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/device:GPU:0'
Colocation Debug Info:                                                                                                     
Colocation group had the following types and devices:
TensorArrayReadV3: CPU                                                                                                    
Enter: GPU CPU                                                                        
TensorArrayV3: CPU                         
TensorArrayScatterV3: CPU                                                                                                                                      
Placeholder: GPU CPU                                                                                   
                                                           
Colocation members and user-requested devices:                   
  RaggedLengths (Placeholder) /device:GPU:0
  map/TensorArray_1 (TensorArrayV3)                                                                                                                            
  map/TensorArrayUnstack_1/TensorArrayScatter/TensorArrayScatterV3 (TensorArrayScatterV3) /device:GPU:0
  map/while/TensorArrayReadV3_1/Enter (Enter) /device:GPU:0
  map/while/TensorArrayReadV3_1 (TensorArrayReadV3) /device:GPU:0
                                                  
         [[Node: map/TensorArray_1 = TensorArrayV3[clear_after_read=true, dtype=DT_INT64, dynamic_size=false, element_shape=<unknown>, identical_element_shapes=true, tensor_array_name=""""](map/strided_slice)]]
                                                                                                                            
Caused by op 'map/TensorArray_1', defined at:
  File ""map_fn_GPU_error.py"", line 33, in <module>                                                                              
    dtype=tf.float32)                                      
  File ""/my/python/package/path/tensorflow/python/ops/functional_ops.py"", line 420, in map_fn    
    for elem in elems_flat]                    
  File ""/my/python/package/path/tensorflow/python/ops/functional_ops.py"", line 420, in <listcomp>                               
    for elem in elems_flat]                      
  File ""/my/python/package/path/tensorflow/python/ops/tensor_array_ops.py"", line 754, in __init__
    name=name)                                                           
  File ""/my/python/package/path/tensorflow/python/ops/tensor_array_ops.py"", line 160, in __init__         
    self._handle, self._flow = create()            
  File ""/my/python/package/path/tensorflow/python/ops/tensor_array_ops.py"", line 157, in create                                 
    name=scope)   
  File ""/my/python/package/path/tensorflow/python/ops/gen_data_flow_ops.py"", line 7157, in tensor_array_v3
    tensor_array_name=tensor_array_name, name=name)                  
  File ""/my/python/package/path/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)                                                                    
  File ""/my/python/package/path/tensorflow/python/framework/ops.py"", line 3414, in create_op
    op_def=op_def)                                                                                                                                             
  File ""/my/python/package/path/tensorflow/python/framework/ops.py"", line 1740, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access    

```
This is triggered by the line
`sess.run(tf.global_variables_initializer())` in the code below. This code works perfectly on CPU however, I cannot seem to force placement on the GPU.

### Source code / logs

This is a minimal example to reproduce the above error. (This is the file named `map_fn_GPU_error.py` used in the command above)

```python
import tensorflow as tf
import numpy as np

with_gpu = True
if with_gpu:
    device_name = '/device:GPU:0'
else:
    device_name = '/device:CPU:*'

batch_size = 32
max_ragged_dim = 10
embedding_dim = 20


def ragged_function(ragged_input, length_of_input):
    # length_of_input is not used here but is in the actual function
    # The error is caused whether or not it is used
    return tf.reduce_sum(ragged_input, axis=0)


def final_loss_function(ragged_function_outputs):
    weights = tf.get_variable('LossWeights', dtype=tf.float32,
                              shape=(ragged_function_outputs.shape[1], 1))
    return tf.reduce_sum(tf.matmul(ragged_function_outputs, weights))


with tf.device(device_name):
    input_ragged_mat = tf.placeholder(name='RaggedMatrix', dtype=tf.float32,
                                      shape=(None, max_ragged_dim, embedding_dim))
    input_ragged_lengths = tf.placeholder(name='RaggedLengths', dtype=tf.int64,
                                          shape=(None,))

    ragged_function_outputs = tf.map_fn(lambda x: ragged_function(*x),
                                        [input_ragged_mat, input_ragged_lengths], 
                                        # Note that input_ragged_lengths is of type int64
                                        # This leads to an error on GPU placement
                                        dtype=tf.float32)
    final_loss = final_loss_function(ragged_function_outputs)

    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)
    train_op = optimizer.minimize(final_loss)

# Create input ragged_matrices, with zeros beyond the actual length
ragged_input_mat = np.random.random(size=(batch_size, max_ragged_dim, embedding_dim))
ragged_input_length_mat = np.random.randint(max_ragged_dim, size=(batch_size,)) + 1
for i, rlen in enumerate(ragged_input_length_mat):
    ragged_input_mat[i, rlen:max_ragged_dim] = 0

with tf.Session() as sess:
    feed_dict = {input_ragged_mat: ragged_input_mat,
                 input_ragged_lengths: ragged_input_length_mat}

    sess.run(tf.global_variables_initializer())

    print(""Calculating Final Loss"")
    sess.run([final_loss],
             feed_dict=feed_dict)
    print(""Running Train operation"")
    sess.run([train_op],
             feed_dict=feed_dict)
```"
22053,Build Tensorflow on RHEL 7 with or without GPU (CUDA) support,"Was searching high and wide for a guide that included all the steps to install TF on Rhel. Posting the solution that worked for me. Also includes CUDA install steps.

Machine Specs:
GCP Rhel 7 OS
Tested P-100 and V-100 GPU
Tensorflow 1.10.0
CUDA 9.2
cuDNN 7.2"
22052,Variables in bijectors cannot be reused.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Centos 6
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.9.0 (tested in 1.10.0 too)
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 8.0
- **GPU model and memory**: NVIDIA Tesla, 24GB
- **Exact command to reproduce**: the code within the ""Source code / logs""

### Describe the problem
I am trying to reuse the weights and biases in the neural network within the MaskedAutoregressiveFlow bijector, by placing it within a `tf.variable_scope` with `reuse=tf.AUTO_REUSE`. But found that the weights and biases are not reused in practice.

### Source code / logs
```
import tensorflow as tf
from tensorflow.contrib.distributions.python.ops import bijectors as tfb

def get_bijector(name='my_bijector', reuse=None):
  """"""Returns a MAF bijector.""""""
  with tf.variable_scope(name, reuse=reuse):
    shift_and_log_scale_fn = \
        tfb.masked_autoregressive_default_template([128])
    return tfb.MaskedAutoregressiveFlow(shift_and_log_scale_fn)
  
x = tf.placeholder(shape=[None, 64], dtype='float32', name='x')

bijector_0 = get_bijector(reuse=tf.AUTO_REUSE)
y_0 = bijector_0.forward(x)

bijector_1 = get_bijector(reuse=tf.AUTO_REUSE)
y_1 = bijector_1.forward(x)

# We were expecting that the `y_0` and `y_1` share the same dependent variables,
# since we used `tf.AUTO_REUSE` within the `tf.variable_scope`. However, the following
# will return a `False`.
print(get_dependent_variables(y_0) == get_dependent_variables(y_1))
```

wherein we have employed the function that gains all the variables a tensor depends on:

```
import collections

def get_dependent_variables(tensor):
  """"""Returns all variables that the tensor `tensor` depends on.
  
  Forked from: https://stackoverflow.com/a/42861919/1218716
  
  Args:
    tensor: Tensor.
    
  Returns:
    List of variables.
  """"""  
  # Initialize
  starting_op = tensor.op
  dependent_vars = []
  queue = collections.deque()
  queue.append(starting_op)
  op_to_var = {var.op: var for var in tf.trainable_variables()}
  visited = {starting_op}

  while queue:
    op = queue.popleft()
    try:
      dependent_vars.append(op_to_var[op])
    except KeyError:
      # `op` is not a variable, so search its inputs (if any). 
      for op_input in op.inputs:
        if op_input.op not in visited:
          queue.append(op_input.op)
          visited.add(op_input.op)
          
  return dependent_vars
```"
22051,Integrating this framework into swift causing syntax errors,"I am using this framework in a swift project and used a bridging header to implement some functionalities. I was included this framework with CocoaPods into my swift project. But while trying to build the project, there are so many syntax errors like #include not found, #include not found etc. I have been banging my head around this issue from past two days but no luck. The Xcode compiler is able to build if i am using an Objective C Project. Somebody please guide me to resolve the issue with swift project

Thanks in Advance"
22050,Can't find out how put smartreply_jni.c file in android,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22049,"After building TensorFlow from source, seeing ImportError: __longjmp_chk: symbol not found error","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Alpine
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**:  From binary https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.10.1-cp36-cp36m-linux_x86_64.whl
- **TensorFlow version (use command below)**: 1.10.1
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: No
- **GCC/Compiler version (if compiling from source)**: glibc 2.28
- **CUDA/cuDNN version**: No
- **GPU model and memory**: No
- **Exact command to reproduce**: import tensorflow

### Describe the problem
Docker image with Alpine Linux is built successfully. But the ImportError occured while running `import tensorflow`.

### Source code / logs
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/local/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/local/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: Error relocating /usr/local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: __longjmp_chk: symbol not found

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/local/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/local/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: Error relocating /usr/local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: __longjmp_chk: symbol not found


Failed to load the native TensorFlow runtime.
"
22048,[Bug] Clip by norm NaN gradients ,"```
a = tf.zeros([3], dtype=tf.float32)
b = tf.clip_by_norm(a, 1.)
c = tf.gradients(b,a)
s = tf.Session()
s.run(c)
[array([nan, nan, nan], dtype=float32)]
```

The gradient should obviously be [1,1,1] for all vectors a of norm smaller than 1, since this function should be the identity for those vectors.

Have I written custom code:
OS Platform and Distribution: Ubuntu 14.10
TensorFlow installed from: pip3
TensorFlow version: 1.10.1
Bazel version:
CUDA/cuDNN version:
GPU model and memory:
Exact command to reproduce: see above
Mobile device:"
22047,Tensorflow failed to build on x64 when build with MSVC,"System information:
*Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
NA
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows server 2016
*TensorFlow installed from (source or binary):
Source
*TensorFlow version (use command below):
https://github.com/tensorflow/tensorflow/commit/24787842adfefe35f5a520313d775b14c29f143a#diff-47cae0cad6802731af5c3d2c42011880
*Python version:
Anaconda 4.1.1 (Python 3.5 64-bit)
*Bazel version (if compiling from source):
N/A
*GCC/Compiler version (if compiling from source):
VS2017 15.7.2
*CUDA/cuDNN version:
NVidia CUDA Toolkit 8.0
NVidia CUDNN 5.1
*GPU model and memory:
N/A
*Exact command to reproduce:
N/A

Describe the problem:
Tensorflow failed to build on x64. This issue can be reproduced from the master revision commit/24787842adfefe35f5a520313d775b14c29f143a#diff-47cae0cad6802731af5c3d2c42011880. This should be tensorflow source issue, could you please help take a look at this? Thanks!

The failures like:
The whole log file please see attachment.
[log_x64_build.log](https://github.com/tensorflow/tensorflow/files/2347551/log_x64_build.log)

Repro steps:

1.git clone https://github.com/tensorflow/tensorflow D:\Tensorflow\src
2.pushd D:\Tensorflow
3.set PreferredToolArchitecture=x64
4.set rel=Release
5.set CUDNN_HOME=""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\cuda""
6.set PY=C:\ProgramData\Anaconda3
7.set CL=/FS /permissive-
8.cmake D:\Tensorflow\src\tensorflow\contrib\cmake -A x64 -DCMAKE_BUILD_TYPE=Release -DPYTHON_EXECUTABLE=C:\ProgramData\Anaconda3\python.exe -DPYTHON_LIBRARIES=C:\ProgramData\Anaconda3\libs\python36.lib -DSWIG_EXECUTABLE=D:\Tensorflow\swigwin-3.0.12\swig.exe -Dtensorflow_BUILD_PYTHON_TESTS=ON -Dtensorflow_BUILD_SHARED_LIB=ON
9.MSBuild /m /p:Configuration=Release;Platform=x64 /p:WindowsTargetPlatformVersion=10.0.17134.0 tensorflow.sln /t:Rebuild

Build Message:
C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\VC\Tools\MSVC\14.14.26428\bin\HostX64\x64\CL.exe /c /ID:\Tensorflow\src /ID:\Tensorflow\build_x64 /ID:\Tensorflow\build_x64\external\zlib_archive /I""D:\Tensorflow\build_x64\external\gif_archive\giflib-5.1.4"" /ID:\Tensorflow\build_x64\external\png_archive /ID:\Tensorflow\build_x64\external\jpeg_archive /ID:\Tensorflow\build_x64\external\lmdb /ID:\Tensorflow\build_x64\external\eigen_archive /ID:\Tensorflow\src\third_party\eigen3 /ID:\Tensorflow\build_x64\gemmlowp\src\gemmlowp /ID:\Tensorflow\build_x64\jsoncpp\src\jsoncpp /ID:\Tensorflow\build_x64\external\farmhash_archive /ID:\Tensorflow\build_x64\external\farmhash_archive\util /ID:\Tensorflow\build_x64\external\highwayhash /ID:\Tensorflow\build_x64\cub\src\cub /ID:\Tensorflow\build_x64\external\nsync\public /ID:\Tensorflow\build_x64\protobuf\src\protobuf\src /ID:\Tensorflow\build_x64\re2\install\include /ID:\Tensorflow\build_x64\external\sqlite /ID:\Tensorflow\build_x64\double_conversion\src\double_conversion /ID:\Tensorflow\build_x64\grpc\src\grpc\include /ID:\Tensorflow\build_x64\snappy\src\snappy /nologo /W3 /WX- /diagnostics:caret /MP /O2 /Ob2 /D WIN32 /D _WINDOWS /D NDEBUG /D _ITERATOR_DEBUG_LEVEL=0 /D SQLITE_OMIT_LOAD_EXTENSION /D EIGEN_AVOID_STL_ARRAY /D WIN64 /D NOMINMAX /D _WIN32_WINNT=0x0A00 /D WIN32_LEAN_AND_MEAN /D NOGDI /D PLATFORM_WINDOWS /D TENSORFLOW_USE_EIGEN_THREADPOOL /D EIGEN_HAS_C99_MATH /D TF_COMPILE_LIBRARY /D _HAS_EXCEPTIONS=0 /D GRPC_ARES=0 /D TF_USE_SNAPPY /D ""CMAKE_INTDIR=\""Release\"""" /D _MBCS /GF /Gm- /MD /GS /fp:precise /Zc:wchar_t /Zc:forScope /Zc:inline /GR /openmp /std:c++14 /Fo""tf_core_lib.dir\Release\\"" /Fd""tf_core_lib.dir\Release\tf_core_lib.pdb"" /Gd /TP /wd4267 /wd4244 /wd4800 /wd4503 /wd4554 /wd4996 /wd4348 /wd4018 /wd4099 /wd4146 /wd4305 /wd4307 /wd4715 /wd4722 /wd4723 /wd4838 /wd4309 /wd4334 /wd4003 /wd4506 /wd4577 /FC /errorReport:queue  /EHs-c- /bigobj D:\Tensorflow\src\tensorflow\core\lib\bfloat16\bfloat16.cc D:\Tensorflow\src\tensorflow\core\lib\core\arena.cc D:\Tensorflow\src\tensorflow\core\lib\core\bitmap.cc D:\Tensorflow\src\tensorflow\core\lib\core\coding.cc D:\Tensorflow\src\tensorflow\core\lib\core\status.cc D:\Tensorflow\src\tensorflow\core\lib\core\threadpool.cc D:\Tensorflow\src\tensorflow\core\lib\db\snapfn.cc D:\Tensorflow\src\tensorflow\core\lib\db\sqlite.cc D:\Tensorflow\src\tensorflow\core\lib\gif\gif_io.cc D:\Tensorflow\src\tensorflow\core\lib\hash\crc32c.cc D:\Tensorflow\src\tensorflow\core\lib\hash\crc32c_accelerate.cc D:\Tensorflow\src\tensorflow\core\lib\hash\hash.cc D:\Tensorflow\src\tensorflow\core\lib\histogram\histogram.cc D:\Tensorflow\src\tensorflow\core\lib\io\block.cc D:\Tensorflow\src\tensorflow\core\lib\io\block_builder.cc D:\Tensorflow\src\tensorflow\core\lib\io\buffered_inputstream.cc D:\Tensorflow\src\tensorflow\core\lib\io\compression.cc D:\Tensorflow\src\tensorflow\core\lib\io\format.cc D:\Tensorflow\src\tensorflow\core\lib\io\inputbuffer.cc D:\Tensorflow\src\tensorflow\core\lib\io\inputstream_interface.cc D:\Tensorflow\src\tensorflow\core\lib\io\iterator.cc D:\Tensorflow\src\tensorflow\core\lib\io\path.cc D:\Tensorflow\src\tensorflow\core\lib\io\random_inputstream.cc D:\Tensorflow\src\tensorflow\core\lib\io\record_reader.cc D:\Tensorflow\src\tensorflow\core\lib\io\record_writer.cc D:\Tensorflow\src\tensorflow\core\lib\io\snappy\snappy_inputbuffer.cc D:\Tensorflow\src\tensorflow\core\lib\io\snappy\snappy_outputbuffer.cc D:\Tensorflow\src\tensorflow\core\lib\io\table.cc D:\Tensorflow\src\tensorflow\core\lib\io\table_builder.cc D:\Tensorflow\src\tensorflow\core\lib\io\two_level_iterator.cc D:\Tensorflow\src\tensorflow\core\lib\io\zlib_compression_options.cc D:\Tensorflow\src\tensorflow\core\lib\io\zlib_inputstream.cc D:\Tensorflow\src\tensorflow\core\lib\io\zlib_outputbuffer.cc D:\Tensorflow\src\tensorflow\core\lib\jpeg\jpeg_handle.cc D:\Tensorflow\src\tensorflow\core\lib\jpeg\jpeg_mem.cc D:\Tensorflow\src\tensorflow\core\lib\monitoring\collection_registry.cc D:\Tensorflow\src\tensorflow\core\lib\monitoring\sampler.cc D:\Tensorflow\src\tensorflow\core\lib\png\png_io.cc D:\Tensorflow\src\tensorflow\core\lib\random\distribution_sampler.cc D:\Tensorflow\src\tensorflow\core\lib\random\random.cc D:\Tensorflow\src\tensorflow\core\lib\random\random_distributions.cc D:\Tensorflow\src\tensorflow\core\lib\random\simple_philox.cc D:\Tensorflow\src\tensorflow\core\lib\random\weighted_picker.cc D:\Tensorflow\src\tensorflow\core\lib\strings\base64.cc D:\Tensorflow\src\tensorflow\core\lib\strings\numbers.cc D:\Tensorflow\src\tensorflow\core\lib\strings\ordered_code.cc D:\Tensorflow\src\tensorflow\core\lib\strings\proto_serialization.cc D:\Tensorflow\src\tensorflow\core\lib\strings\proto_text_util.cc D:\Tensorflow\src\tensorflow\core\lib\strings\scanner.cc D:\Tensorflow\src\tensorflow\core\lib\strings\str_util.cc D:\Tensorflow\src\tensorflow\core\lib\strings\strcat.cc D:\Tensorflow\src\tensorflow\core\lib\strings\stringprintf.cc D:\Tensorflow\src\tensorflow\core\lib\wav\wav_io.cc D:\Tensorflow\src\tensorflow\core\platform\abi.cc D:\Tensorflow\src\tensorflow\core\platform\cpu_feature_guard.cc D:\Tensorflow\src\tensorflow\core\platform\cpu_info.cc D:\Tensorflow\src\tensorflow\core\platform\denormal.cc D:\Tensorflow\src\tensorflow\core\platform\file_system.cc D:\Tensorflow\src\tensorflow\core\platform\file_system_helper.cc D:\Tensorflow\src\tensorflow\core\platform\protobuf_util.cc D:\Tensorflow\src\tensorflow\core\platform\setround.cc D:\Tensorflow\src\tensorflow\core\platform\stacktrace_handler.cc D:\Tensorflow\src\tensorflow\core\platform\tensor_coding.cc D:\Tensorflow\src\tensorflow\core\platform\default\device_tracer.cc D:\Tensorflow\src\tensorflow\core\platform\default\human_readable_json.cc D:\Tensorflow\src\tensorflow\core\platform\default\logging.cc D:\Tensorflow\src\tensorflow\core\platform\default\mutex.cc D:\Tensorflow\src\tensorflow\core\platform\default\protobuf.cc D:\Tensorflow\src\tensorflow\core\platform\default\string_coding.cc D:\Tensorflow\src\tensorflow\core\framework\resource_handle.cc D:\Tensorflow\src\tensorflow\core\platform\windows\net.cc D:\Tensorflow\src\tensorflow\core\platform\windows\port.cc D:\Tensorflow\src\tensorflow\core\platform\windows\windows_file_system.cc
bfloat16.cc
         arena.cc
         bitmap.cc
         coding.cc
         status.cc
         threadpool.cc
         snapfn.cc
         sqlite.cc
         gif_io.cc
         crc32c.cc
         crc32c_accelerate.cc
         hash.cc
         histogram.cc
         block.cc
         block_builder.cc
         buffered_inputstream.cc
         compression.cc
         format.cc
         inputbuffer.cc
         inputstream_interface.cc
         iterator.cc
         path.cc
         random_inputstream.cc
         record_reader.cc
         record_writer.cc
         snappy_inputbuffer.cc
   297>D:\Tensorflow\src\tensorflow\core\lib\core\stringpiece.h(34,10): error C1083:  Cannot open include file: 'absl/strings/string_view.h': No such file or directory (d:\agent\_work\3\s\src\vctools\compiler\cxxfe\sl\p1\c\p0prepro.c:1711) [D:\Tensorflow\build_x64\tf_core_lib.vcxproj]"
22043,GPU: tf.Session() needs 1-3 minutes at first start,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux powerai 4.4.0-109-generic #132-Ubuntu SMP Tue Jan 9 20:00:40 UTC 2018 ppc64le ppc64le ppc64le GNU/Linux

- **TensorFlow installed from (source or binary)**:
source (R1.10)
- **TensorFlow version (use command below)**:
R1.10
- **Python version**:
3.5.2
- **Bazel version (if compiling from source)**:
1.15.2
- **GCC/Compiler version (if compiling from source)**:
5.4.0 20160609 
- **CUDA/cuDNN version**:
CUDA 9.0, cuDNN 7.1
- **GPU model and memory**:
4 x  Tesla P100-SXM2-16GB
- **Exact command to reproduce**:
import tensorflow as tf
sess = tf.Session()

### Describe the problem

Every first time I start a tensorflow session, it needs 1-3 minutes to finish. 
It freezes at the point ""2018-09-04 08:32:04.920181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0, 1, 2, 3""

After waiting and restarting the python script it works in 1-5 seconds.

### Source code / logs
There is no additional source code or loginformation."
22042,Could not use adagrad or adadelta optimizer in eager mode(gpu),"https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/generative_examples/text_generation.ipynb
Change to use tf.train.AdagradOptimizer then will get below error 
NotFoundError: No registered 'ResourceSparseApplyAdagrad' OpKernel for GPU devices compatible with node ResourceSparseApplyAdagrad = ResourceSparseApplyAdagrad[T=DT_FLOAT, Tindices=DT_INT32, update_slots=true, use_locking=false](dummy_input, dummy_input, dummy_input, dummy_input, dummy_input)
	.  Registered:  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_BFLOAT16]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_BFLOAT16]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_HALF]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_HALF]; Tindices in [DT_INT32]
 [Op:ResourceSparseApplyAdagrad]"
22041,Can't Import Keras from Tensorflow library,"I've been trying to import keras from tensorflow using the following statement (using Python in PyCharm):
```
import tensorflow as tf
from tensorflow import keras
```

It should work as far as I know but I still get the following message for some reason:
```
from tensorflow import keras
ImportError: cannot import name 'keras' from 'tensorflow'(/Users/gabork/PycharmProjects/Tester/venv/lib/python3.7/site-packages/tensorflow/__init__.py)
```

**EDIT:** As I'm using PyCharm with Tensorflow version 0.12.0 and Python 3.7, I'm suspecting version issues. I tried updating to Tensorflow 1.10.1 through PyCharm (venv) but got the following message:
```
Collecting tensorflow==1.10.1

 Could not find a version that satisfies the requirement tensorflow==1.10.1 (from versions: )
No matching distribution found for tensorflow==1.10.1
```

**EDIT2:** Downgrading to Python 2.7 seems to fix the issue. But I would still prefer someone more knowledgable to maybe give me a reason why it worked like this.



Have I written custom code: 
> No, only code written is shown above

OS Platform and Distribution: 
> OSX 10.13.6

TensorFlow version
> 0.12.0

GPU model and memory
> Intel HD Graphics 6000 1536 MB graphics
> 8 GB 1600 MHz DDR3 Memory

Exact command to reproduce
> Shown Above

Mobile device
> N/A"
22040,AttributeError: 'TPUInfeedOutfeedSessionHook' object has no attribute '_name',"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

Probably irrelevant for this bug
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux debian
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: not sure, it's baked into an image
- **TensorFlow version (use command below)**: b'v1.9.0-0-g25c197e' 1.9.0
- **Python version**: 3.6.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A


### Describe the problem

Running a TPUEstimator and passing initial_infeed_sleep_secs to the TPUConfig results in:

Exception in thread InfeedController:
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/opt/conda/lib/python3.6/threading.py"", line 864, in run
    self._target(*self._args, **self._kwargs)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py"", line 421, in _run_infeed
    logging.info('%s thread sleeping for %d seconds.', self._name,
AttributeError: 'TPUInfeedOutfeedSessionHook' object has no attribute '_name'


### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

Usage of self._name here is invalid
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py#L421
"
22039,Session.run () takes a long time,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:1.9.0
- **Python version**:3.6
- **Bazel version (if compiling from source)**:N/A
- **GCC/Compiler version (if compiling from source)**:6.4.0
- **CUDA/cuDNN version**:9.0 / 7.0
- **GPU model and memory**:GeForce GTX960
- **Exact command to reproduce**:N/A

### Describe the problem

Each time tensorflow performs session.run() for target detection, the detection time of the first image is very long (including some initialization operations, of course), while the detection time of other images is normal. Suppose I detect the image under a certain path (for example, there are ten images), the time of detecting the first image is relatively long, and the remaining nine images are relatively short (basically consistent). However, my operation in practical application is as follows: the session.run() is called every once in a while to detect a picture. I hope that the detection time after the first one is normal except for the long time. However, through my test (guess), after exiting the loop logic of detection, tensorflow redid a series of initialization operations the next time the detection was done, which puzzled me.

With other frameworks, such as mxnet, initial detection takes longer. And then the detection time is normal, as if it's not doing some initialization anymore. I thought, could tensorflow do the same thing?

### Source code / logs

2018-09-04 14:48:59.111510: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.57GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
time5627.940655ms
0.9999423027038574
time657.650471ms
0.9996843338012695
time676.565170ms
0.9722122550010681
0.6320008635520935
time667.881966ms
0.996504545211792
"
22038,Keras Save / Load with Eager,"https://github.com/tensorflow/tensorflow/blob/ec357afc00c8fb0e9c99754afd78d47fc0ab2116/tensorflow/python/keras/engine/network.py#L1357-L1361

Since TensorFlow uses more and more of Eager execution. 
Would it make sense to add a bit more verbose mention on how to save a non-graph model?


Just mentioning more explicitly the reason and linking `tfe.Checkpoint` docs would be already of great help.

Any objections?"
22037,compile issue for bazel build tensorflow/python/tools:optimize_for_inference,"optimize_for_inference.py

### System information:

## OS platform and distribution
Linux debian 4.9.0-6-amd64 #1 SMP Debian 4.9.82-1+deb9u3 (2018-03-02) x86_64 GNU/Linux

## Tensorflow version
Tensorflow version r1.9 (compiled from source for CPU)

## CUDA/cuDNN 
n/a

## gpu model and memory
n/a

## Python version
Python version 3.5

## Bazel version
Bazel version 0.16.0 

## Gcc version
gcc version 6.3.0

## No custom code used, just standard script:
optimize_for_inference.py

### Command to reproduce:

bazel build tensorflow/python/tools:optimize_for_inference

### Mobile device 

n/a

### Problem:

The result is a failure to build.  

### Source code / logs
The output is rather lengthy, but here is the relevant ""ERROR"" part:

ERROR: /home/david/tensorflow/tensorflow/contrib/coder/BUILD:144:1: C++ compilation of rule '//tensorflow/contrib/coder:python/ops/_coder_ops.so' failed (Exit 1)
In file included from /usr/include/c++/6/map:60:0,
                 from external/protobuf_archive/src/google/protobuf/stubs/common.h:40,
                 from external/protobuf_archive/src/google/protobuf/stubs/atomicops.h:59,
                 from external/protobuf_archive/src/google/protobuf/stubs/atomic_sequence_num.h:33,
                 from external/protobuf_archive/src/google/protobuf/arena_impl.h:38,
                 from external/protobuf_archive/src/google/protobuf/arena.h:54,
                 from ./tensorflow/core/platform/default/protobuf.h:22,
                 from ./tensorflow/core/platform/protobuf.h:31,
                 from ./tensorflow/core/platform/default/string_coding.h:23,
                 from ./tensorflow/core/platform/tensor_coding.h:29,
                 from ./tensorflow/core/framework/resource_handle.h:19,
                 from ./tensorflow/core/framework/allocator.h:24,
                 from ./tensorflow/core/framework/op_kernel.h:23,
                 from tensorflow/contrib/coder/kernels/pmf_to_cdf_op.cc:24:
/usr/include/c++/6/bits/stl_tree.h: In instantiation of 'std::pair<std::_Rb_tree_iterator<_Val>, std::_Rb_tree_iterator<_Val> > std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::equal_range(const _Key&) [with _Key = std::__cxx11::basic_string<char>*; _Val = std::__cxx11::basic_string<char>*; _KeyOfValue = std::_Identity<std::__cxx11::basic_string<char>*>; _Compare = google::protobuf::Map<std::__cxx11::basic_string<char>, tensorflow::AttrValue>::InnerMap::KeyCompare; _Alloc = google::protobuf::Map<std::__cxx11::basic_string<char>, tensorflow::AttrValue>::MapAllocator<std::__cxx11::basic_string<char>*>]':
/usr/include/c++/6/bits/stl_tree.h:2298:49:   required from 'std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::size_type std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::erase(const _Key&) [with _Key = std::__cxx11::basic_string<char>*; _Val = std::__cxx11::basic_string<char>*; _KeyOfValue = std::_Identity<std::__cxx11::basic_string<char>*>; _Compare = google::protobuf::Map<std::__cxx11::basic_string<char>, tensorflow::AttrValue>::InnerMap::KeyCompare; _Alloc = google::protobuf::Map<std::__cxx11::basic_string<char>, tensorflow::AttrValue>::MapAllocator<std::__cxx11::basic_string<char>*>; std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::size_type = long unsigned int]'
/usr/include/c++/6/bits/stl_set.h:602:30:   required from 'std::set<_Key, _Compare, _Alloc>::size_type std::set<_Key, _Compare, _Alloc>::erase(const key_type&) [with _Key = std::__cxx11::basic_string<char>*; _Compare = google::protobuf::Map<std::__cxx11::basic_string<char>, tensorflow::AttrValue>::InnerMap::KeyCompare; _Alloc = google::protobuf::Map<std::__cxx11::basic_string<char>, tensorflow::AttrValue>::MapAllocator<std::__cxx11::basic_string<char>*>; std::set<_Key, _Compare, _Alloc>::size_type = long unsigned int; std::set<_Key, _Compare, _Alloc>::key_type = std::__cxx11::basic_string<char>*]'
external/protobuf_archive/src/google/protobuf/map.h:619:9:   required from 'void google::protobuf::Map<Key, T>::InnerMap::erase(google::protobuf::Map<Key, T>::InnerMap::iterator) [with Key = std::__cxx11::basic_string<char>; T = tensorflow::AttrValue; google::protobuf::Map<Key, T>::InnerMap::iterator = google::protobuf::Map<std::__cxx11::basic_string<char>, tensorflow::AttrValue>::InnerMap::iterator_base<google::protobuf::Map<std::__cxx11::basic_string<char>, tensorflow::AttrValue>::KeyValuePair>]'
external/protobuf_archive/src/google/protobuf/map.h:1139:5:   required from 'google::protobuf::Map<Key, T>::iterator google::protobuf::Map<Key, T>::erase(google::protobuf::Map<Key, T>::iterator) [with Key = std::__cxx11::basic_string<char>; T = tensorflow::AttrValue]'
external/protobuf_archive/src/google/protobuf/map.h:1144:20:   required from 'void google::protobuf::Map<Key, T>::erase(google::protobuf::Map<Key, T>::iterator, google::protobuf::Map<Key, T>::iterator) [with Key = std::__cxx11::basic_string<char>; T = tensorflow::AttrValue]'
external/protobuf_archive/src/google/protobuf/map.h:1147:23:   required from 'void google::protobuf::Map<Key, T>::clear() [with Key = std::__cxx11::basic_string<char>; T = tensorflow::AttrValue]'
external/protobuf_archive/src/google/protobuf/map_field_inl.h:180:3:   required from 'void google::protobuf::internal::MapField<Derived, Key, T, key_wire_type, value_wire_type, default_enum_value>::Clear() [with Derived = tensorflow::NameAttrList_AttrEntry_DoNotUse; Key = std::__cxx11::basic_string<char>; T = tensorflow::AttrValue; google::protobuf::internal::WireFormatLite::FieldType kKeyFieldType = (google::protobuf::internal::WireFormatLite::FieldType)9u; google::protobuf::internal::WireFormatLite::FieldType kValueFieldType = (google::protobuf::internal::WireFormatLite::FieldType)11u; int default_enum_value = 0]'
bazel-out/k8-opt/genfiles/tensorflow/core/framework/attr_value.pb.h:1764:15:   required from here
/usr/include/c++/6/bits/stl_tree.h:1739:5: internal compiler error: Segmentation fault
     }
     ^


"
22036,eager mode using tf.train.Checkpoint not support setting max models to keep,"tensorflow 1.10.1
It looks tf.train.Checkpoint not support setting max models to keep.
In tensorflow/python/training/checkpointable/util.py
line 1123   def save(self, file_prefix, checkpoint_number=None, session=None):
line 1178 self._last_save_saver = saver_lib.Saver(var_list=named_variables)
looks like we need saver_lib.Saver(var_list=named_variables, max_to_keep=...) 

"
22034,ubuntu image with tensorflow preinstalled ,"Hi every one
I tried install tensorflow-gpu on my pc I failed i tried all possible ways I failed to build it from source I failed to run it from docker image
can on you share ubuntu image with tensorflow-gpu installed on it i iam despaired really need to run tensorlow-gpu
and I will be very thankful
"
22033,Accuracy oscillates between ~0% and ~70% when creating multiple models,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: PyPi (pip)
- **TensorFlow version (use command below)**: b'v1.10.0-rc1-19-g656e7a2b34' 1.10.0
- **Python version**: 3.6.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: CUDA 9.0
- **GPU model and memory**: GTX 1060 6 GB
- **Exact command to reproduce**:
  - Download bug.py and training.csv from this Gist: https://gist.github.com/ChrisSwinchatt/97304761e9f875dfd34e3339891a5475
  - Run python bug.py

### Describe the problem

**I've found that the problem doesn't occur when TensorFlow is forced to use the CPU**

Full disclosure: I also opened an issue with Keras (https://github.com/keras-team/keras/issues/11070) because I'm not sure whether the bug is with TensorFlow or Keras. I am using tf.Keras, though, rather than just Keras with TensorFlow as a backend.

I'm using a sequence-to-sequence model based on the Keras blogpost which I've wrapped into a fairly complicated object (although the issue also occurs with a simplified version linked below). When I create a new model (which I have to do for gridsearch and for clearing the TF session when the graph gets too big and slows down training) it starts with accuracy of either 0% or 70%.

Here are a pair of screenshots that show what I mean:

Good: https://i.imgur.com/7mT5Siv.png
Bad: https://i.imgur.com/MZ3NdCB.png

You can see in the first screenshot that the accuracy is low but trending upwards. In the second, the accuracy of two models starts at 70% and doesn't increase (another model starts at 3% and also doesn't increase).

This happens whether I create new, blank models or load pretrained weights into new models with model.load_weights().

### Source code / logs

Source code and screenshots are above."
22032,[1.10.1] sparse_tensor_dense_matmul_op.cc doesn't compile,"command
```
g++ -w -std=c++11 -O2 -fPIC -gsplit-dwarf -pthread -I. -I/usr/include/gemmlowp -I/usr/include/jsoncpp -I/usr/include/llvm-c-7 -I/usr/include/llvm-7 -Ithird_party/toolchains/gpus/cuda/ -c tensorflow/core/kernels/sparse_tensor_dense_matmul_op.cc -o tensorflow/core/kernels/sparse_tensor_dense_matmul_op.o -Idebian/embedded/eigen
```
where debian/embedded/eigen is the eigen source code directory whose version is the same as specified in bazel workspace.

error output
```
tensorflow/core/kernels/sparse_tensor_dense_matmul_op.cc: In instantiation of static tensorflow::Status tensorflow::functor::SparseTensorDenseMatMulFunctor<Eigen::ThreadPoolDevice, T, Tindices, ADJ_A, ADJ_B>::Compute(const CPUDevice&, typename tensorflow::TTypes<T>::Matrix, typename tensorflow::TTypes<T>::ConstMatrix, typename tensorflow::TTypes<T>::ConstVec, typename tensorflow::TTypes<T>::ConstMatrix) [with T = std::complex<double>; Tindices = int; bool ADJ_A = true; bool ADJ_B = false; tensorflow::CPUDevice = Eigen::ThreadPoolDevice; typename tensorflow::TTypes<T>::Matrix = Eigen::TensorMap<Eigen::Tensor<std::complex<double>, 2, 1, long int>, 16, Eigen::MakePointer>; typename tensorflow::TTypes<T>::ConstMatrix = Eigen::TensorMap<Eigen::Tensor<const int, 2, 1, long int>, 16, Eigen::MakePointer>; typename tensorflow::TTypes<T>::ConstVec = Eigen::TensorMap<Eigen::Tensor<const std::complex<double>, 1, 1, long int>, 16, Eigen::MakePointer>; typename tensorflow::TTypes<T>::ConstMatrix = Eigen::TensorMap<Eigen::Tensor<const std::complex<double>, 2, 1, long int>, 16, Eigen::MakePointer>]:
tensorflow/core/kernels/sparse_tensor_dense_matmul_op.cc:148:5:   required from void tensorflow::SparseTensorDenseMatMulOp<Device, T, Tindices>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = std::complex<double>; Tindices = int]
tensorflow/core/kernels/sparse_tensor_dense_matmul_op.cc:42:8:   required from here
tensorflow/core/kernels/sparse_tensor_dense_matmul_op.cc:305:30: error: no matching function for call to std::array<int, 2>::array(int, int)
         Eigen::array<int, 2> shuffle(1, 0);  // preserve dimension order
                              ^~~~~~~
In file included from ./tensorflow/core/kernels/sparse_tensor_dense_matmul_op.h:19,
                 from tensorflow/core/kernels/sparse_tensor_dense_matmul_op.cc:20:
/usr/include/c++/8/array:94:12: note: candidate: std::array<int, 2>::array()
     struct array
            ^~~~~
/usr/include/c++/8/array:94:12: note:   candidate expects 0 arguments, 2 provided
/usr/include/c++/8/array:94:12: note: candidate: constexpr std::array<int, 2>::array(const std::array<int, 2>&)
/usr/include/c++/8/array:94:12: note:   candidate expects 1 argument, 2 provided
/usr/include/c++/8/array:94:12: note: candidate: constexpr std::array<int, 2>::array(std::array<int, 2>&&)
/usr/include/c++/8/array:94:12: note:   candidate expects 1 argument, 2 provided
tensorflow/core/kernels/sparse_tensor_dense_matmul_op.cc: In instantiation of static tensorflow::Status tensorflow::functor::SparseTensorDenseMatMulFunctor<Eigen::ThreadPoolDevice, T, Tindices, ADJ_A, ADJ_B>::Compute(const CPUDevice&, typename tensorflow::TTypes<T>::Matrix, typename tensorflow::TTypes<T>::ConstMatrix, typename tensorflow::TTypes<T>::ConstVec, typename tensorflow::TTypes<T>::ConstMatrix) [with T = std::complex<double>; Tindices = int; bool ADJ_A = true; bool ADJ_B = true; tensorflow::CPUDevice = Eigen::ThreadPoolDevice; typename tensorflow::TTypes<T>::Matrix = Eigen::TensorMap<Eigen::Tensor<std::complex<double>, 2, 1, long int>, 16, Eigen::MakePointer>; typename tensorflow::TTypes<T>::ConstMatrix = Eigen::TensorMap<Eigen::Tensor<const int, 2, 1, long int>, 16, Eigen::MakePointer>; typename tensorflow::TTypes<T>::ConstVec = Eigen::TensorMap<Eigen::Tensor<const std::complex<double>, 1, 1, long int>, 16, Eigen::MakePointer>; typename tensorflow::TTypes<T>::ConstMatrix = Eigen::TensorMap<Eigen::Tensor<const std::complex<double>, 2, 1, long int>, 16, Eigen::MakePointer>]:
tensorflow/core/kernels/sparse_tensor_dense_matmul_op.cc:149:5:   required from void tensorflow::SparseTensorDenseMatMulOp<Device, T, Tindices>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = std::complex<double>; Tindices = int]
tensorflow/core/kernels/sparse_tensor_dense_matmul_op.cc:42:8:   required from here
tensorflow/core/kernels/sparse_tensor_dense_matmul_op.cc:305:30: error: no matching function for call to std::array<int, 2>::array(int, int)
         Eigen::array<int, 2> shuffle(1, 0);  // preserve dimension order
                              ^~~~~~~
In file included from ./tensorflow/core/kernels/sparse_tensor_dense_matmul_op.h:19,
                 from tensorflow/core/kernels/sparse_tensor_dense_matmul_op.cc:20:
/usr/include/c++/8/array:94:12: note: candidate: std::array<int, 2>::array()
     struct array
            ^~~~~
/usr/include/c++/8/array:94:12: note:   candidate expects 0 arguments, 2 provided
/usr/include/c++/8/array:94:12: note: candidate: constexpr std::array<int, 2>::array(const std::array<int, 2>&)
/usr/include/c++/8/array:94:12: note:   candidate expects 1 argument, 2 provided
/usr/include/c++/8/array:94:12: note: candidate: constexpr std::array<int, 2>::array(std::array<int, 2>&&)
/usr/include/c++/8/array:94:12: note:   candidate expects 1 argument, 2 provided
```


------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: debian experimental
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.10.1
- **Python version**: not related
- **Bazel version (if compiling from source)**: not related
- **GCC/Compiler version (if compiling from source)**: g++ (Debian 8.2.0-4) 8.2.0
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: above
"
22030,Inconsistent behaviour of CSVDataset with eager disabled/enabled,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.10.0-12-g4dcfddc5d1 1.10.1
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: see below

### Describe the problem

The following code snippet works without any errors in eager mode but fails when eager is disabled

```python
import tensorflow as tf
data = tf.contrib.data.CsvDataset(""foo.csv"", [tf.constant(0, dtype=tf.int64)])
data.make_one_shot_iterator().get_next()
```

wheer `foo.csv` is generated by `echo 42 > foo.csv`. The issue is fixed by making 42 a single-element list following the example in the `CsvDataset` [documentation](https://www.tensorflow.org/api_docs/python/tf/contrib/data/CsvDataset).

### Source code / logs

#### Eager execution disabled

```python
>>> data.make_one_shot_iterator().get_next()
Traceback (most recent call last):
  File ""[...]/tensorflow/python/framework/ops.py"", line 1576, in _create_c_op
    c_op = c_api.TF_FinishOperation(op_desc)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Shape must be rank 1 but is rank 0 for 'CSVDataset' (op: 'CSVDataset') with input shapes: [], [], [], [], [], [], [0], [].

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""[...]/tensorflow/python/data/ops/dataset_ops.py"", line 170, in make_one_shot_iterator
    six.reraise(ValueError, err)
  File ""[...]/six.py"", line 692, in reraise
    raise value.with_traceback(tb)
ValueError: Shape must be rank 1 but is rank 0 for 'CSVDataset' (op: 'CSVDataset') with input shapes: [], [], [], [], [], [], [0], [].
```

#### Eager execution enabled

```
>>> data.make_one_shot_iterator().get_next()
(<tf.Tensor: id=53, shape=(), dtype=int64, numpy=42>,)
```"
22029,tf-1.10.1 freeze_graph 'list index out of range',"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10.1
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0 / 7.1
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
I use the following command to freeze a `ckpt` file to `pb` file:

```
    freeze_graph(
        input_saver=None,
        input_graph=pbTxt,
        input_binary=False,
        clear_devices=True,
        output_graph=pbPath,
        restore_op_name=None,
        initializer_nodes="""",
        input_meta_graph=None,
        filename_tensor_name=None,
        input_saved_model_dir=None,
        output_node_names=outputOp,
        variable_names_blacklist='',
        input_checkpoint=iCheckpoint,
    )
```

In `tf-1.8.0` everything seems well, however in tf-1.10.1, I meet the following error:

```
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/tools/freeze_graph.py"", line 254, in freeze_graph
    checkpoint_version=checkpoint_version)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/tools/freeze_graph.py"", line 128, in freeze_graph_with_def_protos
    var_list=var_list, write_version=checkpoint_version)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1281, in __init__
    self.build()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1293, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1330, in _build
    build_save=build_save, build_restore=build_restore)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 756, in _build_internal
    saveables = self._ValidateAndSliceInputs(names_to_saveables)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 663, in _ValidateAndSliceInputs
    for converted_saveable_object in self.SaveableObjectsForOp(op, name):
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 637, in SaveableObjectsForOp
    variable, """", name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 121, in __init__
    self.handle_op = var.op.inputs[0]
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2001, in __getitem__
    return self._inputs[i]
IndexError: list index out of range
```

I wonder why the `tf-1.10` is not tested before release ???

"
22028,Graph Transorform fold_constants does not work with NoOp as output node,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
OS X 10.13.6
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
N/A
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
v1.10.0-rc1-19-g656e7a2b34 1.10.0
- **Python version**:
Python 3.6.5
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
N/A

### Describe the problem
I run the [Graph Transform](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms/) tool and apply the transformation `fold_constants(ignore_errors=true)`. One of the output nodes I specify is a variable initializer (i.e. a `NoOp`). This causes the `fold_constants` transformation to fail and my graph is not optimized.

### Source code / logs
```python
from tensorflow.tools import graph_transforms

with tf.Graph().as_default():
    # Setup a graph
    
    tf.variables_initializer(variables_to_initialize, name='initializer')
    
    graph_def = tf.get_default_graph().as_graph_def()

output_nodes = ['graph_output', 'initializer']
graph_def = graph_transforms.TransformGraph(
        graph_def, input_nodes, output_nodes, [
           'fold_constants(ignore_errors=true)'
        ])
```
Output

    2018-09-03 14:28:06.751967: I tensorflow/tools/graph_transforms/transform_graph.cc:318] Applying fold_constants
    2018-09-03 14:28:06.756892: E tensorflow/tools/graph_transforms/transform_graph.cc:333] fold_constants: Ignoring error Tried to fetch data for 'initializer', which produces no output.  To run to a node but not fetch any data, pass 'initializer' as an argument to the 'target_node_names' argument of the Session::Run API.
"
22027,tfcompile: minimum_alignment_for_allocation and kAlign = 64,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04.1
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: -
- **TensorFlow installed from (source or binary)**: Yes
- **TensorFlow version (use command below)**: v1.10.1-0-g4dcfddc5d1
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: 0.16.1
- **GCC/Compiler version (if compiling from source)**: 7.3.0
- **CUDA/cuDNN version**: -
- **GPU model and memory**: -
- **Exact command to reproduce**: -


### Describe the problem

tfcompile uses `minimum_alignment_for_allocation` to check if it can call Eigen on a tensor. `minimum_alignment_for_allocation` uses the minimum alignment of `malloc` but the buffers that tfcompile uses are allocated by `MallocContiguousBuffers` which currently always returns buffers with a minimum alignment of `kAlign` = 64.


### Source code / logs

https://github.com/tensorflow/tensorflow/blob/4dcfddc5d12018a5a0fdca652b9221ed95e9eb23/tensorflow/compiler/xla/service/cpu/target_machine_features.cc#L34-L43

https://github.com/tensorflow/tensorflow/blob/4dcfddc5d12018a5a0fdca652b9221ed95e9eb23/tensorflow/compiler/aot/runtime.h#L36-L37

https://github.com/tensorflow/tensorflow/blob/4dcfddc5d12018a5a0fdca652b9221ed95e9eb23/tensorflow/compiler/aot/runtime.h#L29"
22025,[Bug] max_pool_with_argmax has different behaviour on CPU and GPU,"The function ``max_pool_with_argmax`` has different behaviour concerning the returned indices depending on the GPU or CPU backend.

- On GPU, returned indices do not take into account batch dimension. 
It returns indices such that `[b, y, x, c] -> (y * width + x) * channels + c `
- On CPU, returned indices take into account batch dimension.
It returns indices such that `[b, y, x, c] -> ((b * height + y) * width + x) * channels + c `

Isn't it a problem if one wants to run models with the same graph including this op both on CPU and GPU ?

Here is an example to show the problem:
``` python
import numpy as np
import tensorflow as tf 

i = tf.placeholder(tf.float32, [None, 4, 4, 1])
p, ind = tf.nn.max_pool_with_argmax(i, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')

with tf.Session() as sess:
    a = np.random.randint(10, size=(2, 4, 4, 1))
    print a[:,:,:,0]
    print sess.run(p, {i: a})[:,:,:,0]
    print sess.run(ind, {i: a})[:,:,:,0]
```
Which results in something like this on CPU:
```
[[[0 3 0 0]
  [4 0 4 8]
  [7 8 4 5]
  [2 3 4 5]]

 [[2 5 2 7]
  [4 3 4 6]
  [8 3 2 0]
  [7 5 9 3]]]
[[[4. 8.]
  [8. 5.]]

 [[5. 7.]
  [8. 9.]]]
[[[ 4  7]
  [ 9 11]]

 [[17 19]
  [24 30]]]
```
And something like this on GPU:
```
[[[8 3 5 5]
  [0 0 0 3]
  [6 9 4 1]
  [2 0 8 8]]

 [[4 6 6 5]
  [5 7 9 3]
  [7 6 1 3]
  [1 6 1 0]]]
[[[8. 5.]
  [9. 8.]]

 [[7. 9.]
  [7. 3.]]]
[[[ 0  2]
  [ 9 14]]
 [[ 5  6]
  [ 8 11]]]
```
This was obtained with 1.9 CPU/GPU tensorflow versions.
I think it would be more convenient to have the same behaviour on CPU and GPU, isn't it ?"
22023,Tensorflow build fails with vectorization ,"### System information

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Ubuntu 16.04, 18.04 
- **TensorFlow installed from (source or binary)**:  source 
- **TensorFlow version (use command below)**:  1.8.0 
- **Python version**:  2.7.15rc1
- **Bazel version (if compiling from source)**:  0.12.0
- **GCC/Compiler version (if compiling from source)**: gcc-7.3.0, gcc-6.3.0
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**:  ` bazel build -c opt  --incompatible_load_argument_is_label=false --incompatible_disallow_uncalled_set_constructor=false  --config=opt  --copt=""-mzvector"" //tensorflow/tools/pip_package:build_pip_package`

**Description:** 
While building TensorFlow with vectorization on s390x platform, faced following issues :
**With gcc 6.3.0 , Ubuntu 16.04, z13**  :

```
INFO: From Compiling external/snappy/snappy-sinksource.cc [for host]:
cc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++
Slow read: a 7184608-byte read from /root/.cache/bazel/_bazel_root/9b63c66f147d91a5644a0e81d48c020e/external/org_sqlite/sqlite3.c took 26065ms.
ERROR: /root/.cache/bazel/_bazel_root/9b63c66f147d91a5644a0e81d48c020e/external/org_sqlite/BUILD:35:1: C++ compilation of rule '@org_sqlite//:org_sqlite' failed (Exit 1)
external/org_sqlite/sqlite3.c: In function 'vdbeRecordCompareInt':
external/org_sqlite/sqlite3.c:76283:1: internal compiler error: in maybe_record_trace_start, at dwarf2cfi.c:2284
}

0x1385bc7 maybe_record_trace_start
        ../../gcc-6.3.0/gcc/dwarf2cfi.c:2284
0x1388285 scan_trace
        ../../gcc-6.3.0/gcc/dwarf2cfi.c:2462
0x13888dd create_cfi_notes
        ../../gcc-6.3.0/gcc/dwarf2cfi.c:2616
0x13888dd execute_dwarf2_frame
        ../../gcc-6.3.0/gcc/dwarf2cfi.c:2974
0x13888dd execute
        ../../gcc-6.3.0/gcc/dwarf2cfi.c:3454
Please submit a full bug report,
with preprocessed source if appropriate.
Please include the complete backtrace with any bug report.
See <http://gcc.gnu.org/bugs.html> for instructions.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1795.018s, Critical Path: 24.14s
FAILED: Build did NOT complete successfully
```
**With gcc 6.3.0/7.3.0 , Ubuntu 16.04, z14**  :

```
ERROR: /tensorflow/tensorflow/core/kernels/BUILD:436:1: C++ compilation of rule '//tensorflow/core/kernels:split_lib' failed (Exit 1)
In file included from external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:427:0,
                from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14,
                from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                from ./tensorflow/core/kernels/split_lib.h:20,
                from tensorflow/core/kernels/split_lib_cpu.cc:18:
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/ZVector/PacketMath.h: In function 'Eigen::internal::Packet4f Eigen::internal::vec_splat_packet4f(const Packet4f&)':
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/ZVector/PacketMath.h:657:11: error: request for member 'v4f' in 'splat', which is of non-class type 'Eigen::internal::Packet4f {aka __vector(4) float}'
    splat.v4f[0] = vec_splat(from.v4f[0], 0);
          
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/ZVector/PacketMath.h:657:35: error: request for member 'v4f' in 'from', which is of non-class type 'const Packet4f {aka const __vector(4) float}'
    splat.v4f[0] = vec_splat(from.v4f[0], 0);
                                  
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/ZVector/PacketMath.h:657:44: internal compiler error: tree check: expected class 'type', have 'exceptional' (error_mark) in s390_fn_types_compatible, at config/s390/s390-c.c:727
    splat.v4f[0] = vec_splat(from.v4f[0], 0);
                                           
0x80cc535f tree_class_check_failed(tree_node const*, tree_code_class, char const*, int, char const*)
       ../../gcc-6.3.0/gcc/tree.c:9704
```

**With gcc 7.3.0 , Ubuntu 18.04, z14**  :
```

INFO: From Compiling tensorflow/contrib/lite/util.cc:
tensorflow/contrib/lite/util.cc: In function 'TfLiteIntArray* tflite::ConvertArrayToTfLiteIntArray(int, const int*)':
tensorflow/contrib/lite/util.cc:25:24: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (size_t i = 0; i < rank; i++) {
                      
ERROR: /scratch/ecos0030/nayana/tensorflow/tensorflow/contrib/lite/kernels/BUILD:43:1: C++ compilation of rule '//tensorflow/contrib/lite/kernels:eig         en_support' failed (Exit 1)
In file included from external/eigen_archive/Eigen/Core:248:0,
                 from ./third_party/eigen3/Eigen/Core:1,
                 from tensorflow/contrib/lite/kernels/eigen_support.cc:17:
external/eigen_archive/Eigen/src/Core/arch/ZVector/PacketMath.h: In function 'Packet Eigen::internal::pround(const Packet&) [with Packet = __vector(2         ) double]':
external/eigen_archive/Eigen/src/Core/arch/ZVector/PacketMath.h:471:86: error: '__builtin_s390_vfi' was not declared in this scope
template<> EIGEN_STRONG_INLINE Packet2d pround<Packet2d>(const Packet2d& a) { return vec_round(a); }
                                                                                      
external/eigen_archive/Eigen/src/Core/arch/ZVector/PacketMath.h:471:86: note: suggested alternative: '__builtin_s390_vfidb'
external/eigen_archive/Eigen/src/Core/arch/ZVector/PacketMath.h: In function 'Packet Eigen::internal::pceil(const Packet&) [with Packet = __vector(2)          double]':
external/eigen_archive/Eigen/src/Core/arch/ZVector/PacketMath.h:472:86: error: '__builtin_s390_vfi' was not declared in this scope
template<> EIGEN_STRONG_INLINE Packet2d pceil<Packet2d>(const  Packet2d& a) { return vec_ceil(a); }
                                                                                      
external/eigen_archive/Eigen/src/Core/arch/ZVector/PacketMath.h:472:86: note: suggested alternative: '__builtin_s390_vfidb'
external/eigen_archive/Eigen/src/Core/arch/ZVector/PacketMath.h: In function 'Packet Eigen::internal::pfloor(const Packet&) [with Packet = __vector(2         ) double]':
external/eigen_archive/Eigen/src/Core/arch/ZVector/PacketMath.h:473:86: error: '__builtin_s390_vfi' was not declared in this scope
template<> EIGEN_STRONG_INLINE Packet2d pfloor<Packet2d>(const Packet2d& a) { return vec_floor(a); }
                                                                                      
external/eigen_archive/Eigen/src/Core/arch/ZVector/PacketMath.h:473:86: note: suggested alternative: '__builtin_s390_vfidb'
In file included from external/eigen_archive/Eigen/Core:427:0,
                 from ./third_party/eigen3/Eigen/Core:1,
                 from tensorflow/contrib/lite/kernels/eigen_support.cc:17:
external/eigen_archive/Eigen/src/Core/arch/ZVector/PacketMath.h: In function 'Packet Eigen::internal::pmadd(const Packet&, const Packet&, const Packe         t&) [with Packet = __vector(4) float]':
external/eigen_archive/Eigen/src/Core/arch/ZVector/PacketMath.h:1073:141: error: cannot convert 'const Packet4f {aka const __vector(4) float}' to '__         vector(2) double' for argument '1' to '__vector(2) double __builtin_s390_vfmadb(__vector(2) double, __vector(2) double, __vector(2) double)'
template<> EIGEN_STRONG_INLINE Packet4f pmadd<Packet4f>  (const Packet4f& a, const Packet4f& b, const Packet4f& c) { return vec_madd(a, b, c); }
                                                                                                                                             
external/eigen_archive/Eigen/src/Core/arch/ZVector/PacketMath.h: In function 'Packet Eigen::internal::pxor(const Packet&, const Packet&) [with Packet          = __vector(4) float]':
external/eigen_archive/Eigen/src/Core/arch/ZVector/PacketMath.h:1078:118: error: invalid parameter combination for intrinsic '__builtin_s390_vec_xor'
template<> EIGEN_STRONG_INLINE Packet4f pxor<Packet4f>   (const Packet4f& a, const Packet4f& b) { return vec_xor(a, b); }
                                                                                                                      
In file included from external/eigen_archive/Eigen/Core:248:0,
                 from ./third_party/eigen3/Eigen/Core:1,
                 from tensorflow/contrib/lite/kernels/eigen_support.cc:17:
external/eigen_archive/Eigen/src/Core/arch/ZVector/PacketMath.h: In function 'Packet Eigen::internal::pround(const Packet&) [with Packet = __vector(4         ) float]':
external/eigen_archive/Eigen/src/Core/arch/ZVector/PacketMath.h:1080:87: error: '__builtin_s390_vfi' was not declared in this scope
template<> EIGEN_STRONG_INLINE Packet4f pround<Packet4f> (const Packet4f& a) { return vec_round(a); }
                                                                                       
external/eigen_archive/Eigen/src/Core/arch/ZVector/PacketMath.h:1080:87: note: suggested alternative: '__builtin_s390_vfidb'
external/eigen_archive/Eigen/src/Core/arch/ZVector/PacketMath.h: In function 'Packet Eigen::internal::pceil(const Packet&) [with Packet = __vector(4)          float]':
external/eigen_archive/Eigen/src/Core/arch/ZVector/PacketMath.h:1081:87: error: '__builtin_s390_vfi' was not declared in this scope
template<> EIGEN_STRONG_INLINE Packet4f pceil<Packet4f>  (const Packet4f& a) { return vec_ceil(a); }
                                                                                       
external/eigen_archive/Eigen/src/Core/arch/ZVector/PacketMath.h:1081:87: note: suggested alternative: '__builtin_s390_vfidb'
external/eigen_archive/Eigen/src/Core/arch/ZVector/PacketMath.h: In function 'Packet Eigen::internal::pfloor(const Packet&) [with Packet = __vector(4         ) float]':
external/eigen_archive/Eigen/src/Core/arch/ZVector/PacketMath.h:1082:87: error: '__builtin_s390_vfi' was not declared in this scope
template<> EIGEN_STRONG_INLINE Packet4f pfloor<Packet4f> (const Packet4f& a) { return vec_floor(a); }
                                                                                       
external/eigen_archive/Eigen/src/Core/arch/ZVector/PacketMath.h:1082:87: note: suggested alternative: '__builtin_s390_vfidb'
external/eigen_archive/Eigen/src/Core/arch/ZVector/MathFunctions.h: In function 'Packet Eigen::internal::pexp(const Packet&) [with Packet = __vector(         2) double]':
external/eigen_archive/Eigen/src/Core/arch/ZVector/MathFunctions.h:101:8: error: '__builtin_s390_vfi' was not declared in this scope
   fx = vec_floor(fx);
        
external/eigen_archive/Eigen/src/Core/arch/ZVector/MathFunctions.h:101:8: note: suggested alternative: '__builtin_s390_vfidb'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```

"
22022,tflite_convert ,"huangwei@ubuntu:~$ tflite_convert --output_file=foo.tflite  --graph_def_file=opt_mnist_graph.pb --input_arrays=the_input_2 --output_arrays=out_2/truediv
2018-09-03 01:34:59.887931: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Traceback (most recent call last):
  File ""/home/huangwei/.local/lib/python3.5/site-packages/tensorflow/python/framework/importer.py"", line 418, in import_graph_def
    graph._c_graph, serialized, options)  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef expected inputs '' do not match 1 inputs specified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; NodeDef: batch_normalization_55/cond/batchnorm/add/y = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [] values: 1.1e-05>](batch_normalization_55/cond/Switch)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/huangwei/.local/bin/tflite_convert"", line 11, in <module>
    sys.exit(main())
  File ""/home/huangwei/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 370, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/home/huangwei/.local/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/home/huangwei/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 366, in run_main
    _convert_model(tflite_flags)
  File ""/home/huangwei/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 94, in _convert_model
    converter = _get_toco_converter(flags)
  File ""/home/huangwei/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 81, in _get_toco_converter
    return converter_fn(**converter_kwargs)
  File ""/home/huangwei/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/lite.py"", line 220, in from_frozen_graph
    _import_graph_def(graph_def, name="""")
  File ""/home/huangwei/.local/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py"", line 454, in new_func
    return func(*args, **kwargs)
  File ""/home/huangwei/.local/lib/python3.5/site-packages/tensorflow/python/framework/importer.py"", line 422, in import_graph_def
    raise ValueError(str(e))
ValueError: NodeDef expected inputs '' do not match 1 inputs specified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; NodeDef: batch_normalization_55/cond/batchnorm/add/y = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [] values: 1.1e-05>](batch_normalization_55/cond/Switch)

"
22020,What is the difference b.t. tf.layers.dense and tf.layers.Dense,"Hey guys,

Just as the title. I tried to search the Stack Overflow but I can't figure it out. I mean, how to choose the layer b.t. dense and Dense when I construct a neural network?   


Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22019,[Bug] ImportError: cannot import name 'checkpoint_management' tf1.10,"In tf1.10, when I run freeze_graph, it comes error with msg above.

I cloned source file from github and tried it different way and still have same issues.

Thanks for your help

 File ""freeze_graph.py"", line 20, in <module>
    from tensorflow.python.training import checkpoint_management
ImportError: cannot import name 'checkpoint_management'

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MAC & Ubuntu
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: tf1.10
- **Python version**: 3.6
- **Exact command to reproduce**:
python ~/tensorflow/tensorflow/python/tools/freeze_graph.py

"
22018,TensorflowLite compile error on Android,"I can't compile apk file when trying to use Firebase MLKIT using Tensorflowlite. I see this error when compiling:

The last line says: ```<br>...while parsing org/tensorflow/lite/DataType$1.class<br>1 error; aborting```

```
dx tool failed:<br>UNEXPECTED TOP-LEVEL EXCEPTION:<br>com.android.dx.cf.iface.ParseException: bad class file magic (cafebabe) or version (0034.0000)<br>	
at com.android.dx.cf.direct.DirectClassFile.parse0(DirectClassFile.java:472)<br>	
at com.android.dx.cf.direct.DirectClassFile.parse(DirectClassFile.java:406)<br>	
at com.android.dx.cf.direct.DirectClassFile.parseToInterfacesIfNecessary(DirectClassFile.java:388)<br>	
at com.android.dx.cf.direct.DirectClassFile.getMagic(DirectClassFile.java:251)<br>	
at com.android.dx.command.dexer.Main.processClass(Main.java:709)<br>	
at com.android.dx.command.dexer.Main.processFileBytes(Main.java:678)<br>	
at com.android.dx.command.dexer.Main.access$300(Main.java:83)<br>	
at com.android.dx.command.dexer.Main$1.processFileBytes(Main.java:607)<br>	
at com.android.dx.cf.direct.ClassPathOpener.processArchive(ClassPathOpener.java:284)<br>	
at com.android.dx.cf.direct.ClassPathOpener.processOne(ClassPathOpener.java:166)<br>	
at com.android.dx.cf.direct.ClassPathOpener.process(ClassPathOpener.java:144)<br>	
at com.android.dx.command.dexer.Main.processOne(Main.java:637)<br>	
at com.android.dx.command.dexer.Main.processAllFiles(Main.java:506)<br>	
at com.android.dx.command.dexer.Main.runMultiDex(Main.java:335)<br>	
at com.android.dx.command.dexer.Main.run(Main.java:245)<br>	
at com.android.dx.command.dexer.Main.main(Main.java:215)<br>	
at com.android.dx.command.Main.main(Main.java:106)<br>
...while parsing org/tensorflow/lite/DataType$1.class<br>1 error; aborting
```

We're on a Mac, JDK version is 1.8.0_152 and compiling with IntelliJ IDE. Any thought?"
22016,tf.image.resize_images cannot be applied to NCHW format directly?,"Referencing TensorFlow's Performance Guide, It seems NCHW data format is better than NHMC data format in many situations.
Some existing models has already been implemented in NCHW data format, especially using keras, but tf.image.resize_images cannot be applied to NCHW format directly, I think.
Are there any way excepting that we implement by ourselves?"
22014,tf.assign - inputs not compatible with expected types - misleading error message,"-----------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:

TF checkpoint I have built
```
/tmp/tensorflow# git log   
commit 09792df012c22622324f085f46edde33006c7355
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Sun Aug 26 02:07:11 2018 -0700

    compat: Update forward compatibility horizon to 2018-08-26
    
    PiperOrigin-RevId: 210266798
```

```
== cat /etc/issue ===============================================
Linux 3bed2f328777 4.13.0-38-generic #43~16.04.1-Ubuntu SMP Wed Mar 14 17:48:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.5 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
Yes

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux 3bed2f328777 4.13.0-38-generic #43~16.04.1-Ubuntu SMP Wed Mar 14 17:48:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.14.5)
protobuf (3.6.1)
tensorflow (1.10.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.10.0
tf.GIT_VERSION = b'unknown'
tf.COMPILER_VERSION = b'unknown'
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64/stubs:/usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/lib/amd64/server/:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server:/opt/boost/lib:/opt/conda/lib/:/usr/local/cuda/lib64/:/opt/conda/lib/R/lib/:/usr/local/nvidia/lib64/:/usr/local/nvidia/lib:/lib/x86_64-linux-gnu:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu:/opt/opencv/lib
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Wed Aug 29 19:57:14 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 396.26                 Driver Version: 396.26                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A |
|  0%   41C    P0    76W / 250W |    708MiB / 11177MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart.so.9.2.148
/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart_static.a

```
**Bazel** version
```
$ bazel version
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
Build label: 0.16.0
Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Tue Jul 31 17:01:24 2018 (1533056484)
Build timestamp: 1533056484
Build timestamp as int: 1533056484
```

**CUDNN** version:
```
$ nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2018 NVIDIA Corporation
Built on Tue_Jun_12_23:07:04_CDT_2018
Cuda compilation tools, release 9.2, V9.2.148
```

**GPU**: GEFORCE GTX 1080Ti, 11GB, GIGABYTE AORUS

- **Exact command to reproduce**:

```
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import tensorflow as tf

def scope_1():
    print(""DS1 SCOPE ============="")
    with tf.variable_scope(""scope_1"", reuse=tf.AUTO_REUSE):
        x = tf.get_variable(""x"", initializer=lambda: tf.zeros(shape=[], dtype=tf.float32), dtype=tf.float32
                               , trainable=False, use_resource=True)             
        print(""graph: {}"".format(x.graph))
        print(""scope: {}"".format(tf.get_variable_scope().name))
        print("" name: {}"".format(x.name))
        print(""  var: {}"".format(str(x)))
        current_scope = tf.get_variable_scope()       
        assign_one = tf.assign(x, 1.0, name=""x_is_one"")
    
    def scope_2(inputs, label):        
        print(""initial scope: {}"".format(tf.get_variable_scope().name))
        print(""DS1 SCOPE ============="")
        #with tf.variable_scope(""scope_1"", reuse=tf.AUTO_REUSE):
        with tf.variable_scope(current_scope, reuse=tf.AUTO_REUSE):
            y = tf.get_variable(""x"", initializer=lambda: tf.zeros(shape=[], dtype=tf.float32), dtype=tf.float32
                                   , trainable=False)#, use_resource=True)         
            print(""graph: {}"".format(y.graph))
            print(""scope: {}"".format(tf.get_variable_scope().name))
            print("" name: {}"".format(y.name))
            print(""  var: {}"".format(str(y)))
            print(""============="")
            print(y)
            print(inputs)
            #assign_two = tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32),1.0), name=""inputs_plus_1"")
            assign_two = tf.identity(tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32), 1.0, name=""first_assign"")))
            with tf.control_dependencies([assign_two]):
                #with tf.control_dependencies([tf.scatter_nd_update(y, [[0]], [0.22])]):
                return y.read_value(), label
            #return x,label
    
    # test that original x is mutable
    with tf.control_dependencies([assign_one]):
        dataset = (tf.data.Dataset.from_tensor_slices(([1,2,3,4,5], [-1,-2,-3,-4,-5]))
                    .map(scope_2)
                    .batch(1)
                    .repeat(1)        
                    )
    return dataset
    
                
with tf.variable_scope(""scope_0""):
        dataset_fn = scope_1()

with tf.variable_scope(""iterator""):
    # Define iterator from_string_handle. In general it is useful to have
    # this kind of iterator if one wants to switch between train and validation
    # within the training loop.        
    iterator_t = dataset_fn.make_initializable_iterator()
    iterator_handle = tf.placeholder(tf.string, shape=[], name=""iterator_handle"")
    iterator = tf.data.Iterator.from_string_handle(iterator_handle, 
                                                iterator_t.output_types,
                                                iterator_t.output_shapes)
    
    def get_next_item():
        next_elem = iterator.get_next(name=""next_element"")
        x, y = tf.cast(next_elem[0], tf.float32), next_elem[1]# tf.cast(next_elem[1], tf.int32)
        return x, y    
with tf.Session(config=tf.ConfigProto(device_count={'GPU': 0})) as sess:

    sess.run([tf.global_variables_initializer(),tf.local_variables_initializer()])
    handle_t = sess.run(iterator_t.string_handle())
    # Run data iterator initialisation
    sess.run(iterator_t.initializer)
    print(sess.graph.get_operations()) 
    while True:
        try:
            print(sess.run(get_next_item(), feed_dict={iterator_handle:handle_t}))
        except tf.errors.OutOfRangeError:
                        print(""End of training dataset."")
                        break        
    print()
    print(""global vars: {}"".format(tf.global_variables()))
    print(""local vars: {}"".format(tf.local_variables()))
    print(tf.get_default_graph().get_name_scope())
```
### Describe the problem
When using `tf.get_variable` inside function called in `dataset.map()` if it is without `use_resource=True`, which the original variable has, the assign statement gives an error
```
TypeError: In op 'scope_1/Assign', input types ([tf.float32, tf.float32]) are not compatible with expected types ([tf.float32_ref, tf.float32])
```
although the variables used in the assign statement have expected types
```
<tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32>
<tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32_ref>
```
When passing `use_resource=True` to `tf.get_variable` within `def scope_2` they become 
```
<tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32>
<tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>
```
and the whole code works as expected. 

I think the error message in the first case is misleading, because variables have expected types and the whole thing fails. Maybe the input types and expected types in the error msg should be swapped?

Also, as far as I understand `trainable` and `use_resource` flags should be inferred from the original variable when using `tf.get_variable`, so different behavior dependent on the use_resource flag makes me even more confused.

### Source code / logs
RUN 1 - without `use_resource=True`
```
DS1 SCOPE =============
graph: <tensorflow.python.framework.ops.Graph object at 0x7ffb011d4390>
scope: scope_0/scope_1
 name: scope_0/scope_1/x:0
  var: <tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32>
initial scope: 
DS1 SCOPE =============
graph: <tensorflow.python.framework.ops.Graph object at 0x7ffb011d4390>
scope: scope_0/scope_1
 name: scope_0/scope_1/x_1:0
  var: <tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32_ref>
=============
<tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32_ref>
Tensor(""arg0:0"", shape=(), dtype=int32)
Traceback (most recent call last):
  File ""bug.py"", line 51, in <module>
    dataset_fn = scope_1()
  File ""bug.py"", line 43, in scope_1
    .map(scope_2)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1005, in map
    return MapDataset(self, map_func)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 2216, in __init__
    map_func, ""Dataset.map()"", input_dataset)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1473, in __init__
    self._function.add_to_graph(ops.get_default_graph())
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py"", line 479, in add_to_graph
    self._create_definition_if_needed()
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py"", line 335, in _create_definition_if_needed
    self._create_definition_if_needed_impl()
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py"", line 344, in _create_definition_if_needed_impl
    self._capture_by_value, self._caller_device)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py"", line 865, in func_graph_from_py_func
    outputs = func(*func_graph.inputs)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1411, in tf_data_structured_function_wrapper
    ret = func(*nested_args)
  File ""bug.py"", line 34, in scope_2
    assign_two = tf.identity(tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32), 1.0, name=""first_assign"")))
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py"", line 221, in assign
    validate_shape=validate_shape)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/gen_state_ops.py"", line 61, in assign
    use_locking=use_locking, name=name)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py"", line 738, in create_op
    **kwargs)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3254, in create_op
    op_def=op_def)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1728, in __init__
    input_types))
TypeError: In op 'scope_1/Assign', input types ([tf.float32, tf.float32]) are not compatible with expected types ([tf.float32_ref, tf.float32])
```

RUN 2 - with `use_resource=True`
```
DS1 SCOPE =============
graph: <tensorflow.python.framework.ops.Graph object at 0x7f61af7de390>
scope: scope_0/scope_1
 name: scope_0/scope_1/x:0
  var: <tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32>
initial scope: 
DS1 SCOPE =============
graph: <tensorflow.python.framework.ops.Graph object at 0x7f61af7de390>
scope: scope_0/scope_1
 name: scope_0/scope_1/x_1:0
  var: <tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>
=============
<tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>
Tensor(""arg0:0"", shape=(), dtype=int32)
2018-09-02 21:07:18.255844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-09-02 21:07:18.257387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.721
pciBusID: 0000:01:00.0
totalMemory: 10.92GiB freeMemory: 10.23GiB
2018-09-02 21:07:18.258168: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0
2018-09-02 21:07:18.258274: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-09-02 21:07:18.258631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 
2018-09-02 21:07:18.258993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N 
2018-09-02 21:07:18.272483: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
[<tf.Operation 'scope_0/scope_1/x/Initializer/zeros' type=Const>, <tf.Operation 'scope_0/scope_1/x' type=VarHandleOp>, <tf.Operation 'scope_0/scope_1/x/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>, <tf.Operation 'scope_0/scope_1/x/Assign' type=AssignVariableOp>, <tf.Operation 'scope_0/scope_1/x/Read/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'scope_0/scope_1/Const' type=Const>, <tf.Operation 'scope_0/scope_1/x_is_one' type=AssignVariableOp>, <tf.Operation 'scope_0/scope_1/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'scope_0/tensors/component_0' type=Const>, <tf.Operation 'scope_0/tensors/component_1' type=Const>, <tf.Operation 'scope_0/scope_1/x_1/Initializer/zeros' type=Const>, <tf.Operation 'scope_0/scope_1/x_1' type=VarHandleOp>, <tf.Operation 'scope_0/scope_1/x_1/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>, <tf.Operation 'scope_0/scope_1/x_1/Assign' type=AssignVariableOp>, <tf.Operation 'scope_0/scope_1/x_1/Read/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'scope_0/batch_size' type=Const>, <tf.Operation 'scope_0/drop_remainder' type=Const>, <tf.Operation 'scope_0/count' type=Const>, <tf.Operation 'iterator/IteratorV2' type=IteratorV2>, <tf.Operation 'iterator/TensorSliceDataset' type=TensorSliceDataset>, <tf.Operation 'iterator/MapDataset' type=MapDataset>, <tf.Operation 'iterator/BatchDatasetV2' type=BatchDatasetV2>, <tf.Operation 'iterator/RepeatDataset' type=RepeatDataset>, <tf.Operation 'iterator/MakeIterator' type=MakeIterator>, <tf.Operation 'iterator/IteratorToStringHandle' type=IteratorToStringHandle>, <tf.Operation 'iterator/iterator_handle' type=Placeholder>, <tf.Operation 'iterator/IteratorFromStringHandleV2' type=IteratorFromStringHandleV2>, <tf.Operation 'iterator/IteratorToStringHandle_1' type=IteratorToStringHandle>, <tf.Operation 'init' type=NoOp>, <tf.Operation 'init_1' type=NoOp>]
(array([2.], dtype=float32), array([-1], dtype=int32))
(array([3.], dtype=float32), array([-2], dtype=int32))
(array([4.], dtype=float32), array([-3], dtype=int32))
(array([5.], dtype=float32), array([-4], dtype=int32))
(array([6.], dtype=float32), array([-5], dtype=int32))
End of training dataset.

global vars: [<tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32>, <tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>]
local vars: []
```"
22013,tf.scatter_nd_update - Segmentation fault (core dumped),"-----------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:

TF checkpoint I have built
```
/tmp/tensorflow# git log   
commit 09792df012c22622324f085f46edde33006c7355
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Sun Aug 26 02:07:11 2018 -0700

    compat: Update forward compatibility horizon to 2018-08-26
    
    PiperOrigin-RevId: 210266798
```

```
== cat /etc/issue ===============================================
Linux 3bed2f328777 4.13.0-38-generic #43~16.04.1-Ubuntu SMP Wed Mar 14 17:48:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.5 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
Yes

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux 3bed2f328777 4.13.0-38-generic #43~16.04.1-Ubuntu SMP Wed Mar 14 17:48:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.14.5)
protobuf (3.6.1)
tensorflow (1.10.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.10.0
tf.GIT_VERSION = b'unknown'
tf.COMPILER_VERSION = b'unknown'
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64/stubs:/usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/lib/amd64/server/:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server:/opt/boost/lib:/opt/conda/lib/:/usr/local/cuda/lib64/:/opt/conda/lib/R/lib/:/usr/local/nvidia/lib64/:/usr/local/nvidia/lib:/lib/x86_64-linux-gnu:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu:/opt/opencv/lib
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Wed Aug 29 19:57:14 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 396.26                 Driver Version: 396.26                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A |
|  0%   41C    P0    76W / 250W |    708MiB / 11177MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart.so.9.2.148
/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart_static.a

```
**Bazel** version
```
$ bazel version
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
Build label: 0.16.0
Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Tue Jul 31 17:01:24 2018 (1533056484)
Build timestamp: 1533056484
Build timestamp as int: 1533056484
```

**CUDNN** version:
```
$ nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2018 NVIDIA Corporation
Built on Tue_Jun_12_23:07:04_CDT_2018
Cuda compilation tools, release 9.2, V9.2.148
```

**GPU**: GEFORCE GTX 1080Ti, 11GB, GIGABYTE AORUS

- **Exact command to reproduce**:
```
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import tensorflow as tf

def scope_1():
    print(""DS1 SCOPE ============="")
    with tf.variable_scope(""scope_1"", reuse=tf.AUTO_REUSE):
        x = tf.get_variable(""x"", initializer=lambda: tf.zeros(shape=[], dtype=tf.float32), dtype=tf.float32
                               , trainable=False, use_resource=True)             
        print(""graph: {}"".format(x.graph))
        print(""scope: {}"".format(tf.get_variable_scope().name))
        print("" name: {}"".format(x.name))
        print(""  var: {}"".format(str(x)))
        current_scope = tf.get_variable_scope()       
        assign_one = tf.assign(x, 1.0, name=""x_is_one"")
    
    def scope_2(inputs, label):        
        print(""initial scope: {}"".format(tf.get_variable_scope().name))
        print(""DS1 SCOPE ============="")
        #with tf.variable_scope(""scope_1"", reuse=tf.AUTO_REUSE):
        with tf.variable_scope(current_scope, reuse=tf.AUTO_REUSE):
            y = tf.get_variable(""x"", initializer=lambda: tf.zeros(shape=[], dtype=tf.float32), dtype=tf.float32
                                   , trainable=False, use_resource=True)         
            print(""graph: {}"".format(y.graph))
            print(""scope: {}"".format(tf.get_variable_scope().name))
            print("" name: {}"".format(y.name))
            print(""  var: {}"".format(str(y)))
            print(""============="")
            print(y)
            print(inputs)
            #assign_two = tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32),1.0), name=""inputs_plus_1"")
            assign_two = tf.identity(tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32), 1.0)))
            with tf.control_dependencies([assign_two]):
                with tf.control_dependencies([tf.scatter_nd_update(y, [[0]], [0.22])]):
                    return y.read_value(), label
            #return x,label
    
    # test that original x is mutable
    with tf.control_dependencies([assign_one]):
        dataset = (tf.data.Dataset.from_tensor_slices(([1,2,3,4,5], [-1,-2,-3,-4,-5]))
                    .map(scope_2)
                    .batch(1)
                    .repeat(1)        
                    )
    return dataset
    
                
with tf.variable_scope(""scope_0""):
        dataset_fn = scope_1()

with tf.variable_scope(""iterator""):
    # Define iterator from_string_handle. In general it is useful to have
    # this kind of iterator if one wants to switch between train and validation
    # within the training loop.        
    iterator_t = dataset_fn.make_initializable_iterator()
    iterator_handle = tf.placeholder(tf.string, shape=[], name=""iterator_handle"")
    iterator = tf.data.Iterator.from_string_handle(iterator_handle, 
                                                iterator_t.output_types,
                                                iterator_t.output_shapes)
    
    def get_next_item():
        next_elem = iterator.get_next(name=""next_element"")
        x, y = tf.cast(next_elem[0], tf.float32), next_elem[1]# tf.cast(next_elem[1], tf.int32)
        return x, y    
with tf.Session() as sess:

    sess.run([tf.global_variables_initializer(),tf.local_variables_initializer()])
    handle_t = sess.run(iterator_t.string_handle())
    # Run data iterator initialisation
    sess.run(iterator_t.initializer)
    print(sess.graph.get_operations()) 
    while True:
        try:
            print(sess.run(get_next_item(), feed_dict={iterator_handle:handle_t}))
        except tf.errors.OutOfRangeError:
                        print(""End of training dataset."")
                        break        
    print()
    print(""global vars: {}"".format(tf.global_variables()))
    print(""local vars: {}"".format(tf.local_variables()))
    print(tf.get_default_graph().get_name_scope())

```

### Describe the problem
I am trying to create a function that would modify a Tensor within a pipeline of Dataset API.
The scoping may seem weird, but that is minimal example that shows the problem that I created from my project. After adding `tf.scatter_nd_update(y, [[0]], [0.22])` I started to get segmentation fault.
A minimal example with `tf.add` instead of `tf.scatter_nd_update` worked, see https://github.com/tensorflow/tensorflow/issues/22009

I tried disabling GPU with `config=tf.ConfigProto(device_count={'GPU': 0})` and `CUDA_VISIBLE_DEVICES=""""` but the result was the same.

I will recompile TF overnight (from the current master) with `--copt=-g` and try to provide a stacktrace 

```
TF_BUILD_INFO = {container_type: ""gpu"", command: ""bazel build --config=opt --config=cuda --copt=-march=native --copt=-mfpmath=both --copt=-mtune=native --copt=-g --verbose_failures --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 --jobs=8 --config=mkl --action_env=LD_LIBRARY_PATH=/usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64/stubs:/usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/lib/amd64/server/:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server:/opt/boost/lib:/opt/conda/lib/:/usr/local/cuda/lib64/:/opt/conda/lib/R/lib/:/usr/local/nvidia/lib64/:/usr/local/nvidia/lib:/lib/x86_64-linux-gnu:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu:/opt/opencv/lib tensorflow/tools/pip_package:build_pip_package"", source_HEAD: ""201be3d514d7239aa19496dba4dd0c85303b03f1"", source_remote_origin: ""https://github.com/tensorflow/tensorflow"", OS: ""Linux"", kernel: ""4.13.0-38-generic"", architecture: ""x86_64"", processor: ""Intel(R) Core(TM) i7-4770K CPU @ 3.50GHz"", processor_count: ""8"", memory_total: ""32877820 kB"", swap_total: ""69444596 kB"", Bazel_version: ""Build label: 0.16.0"", Java_version: ""1.8.0_181"", Python_version: ""3.6.2"", gpp_version: ""g++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609"", swig_version: """", NVIDIA_driver_version: ""396.26"", CUDA_device_count: ""1"", CUDA_device_names: ""GeForce GTX 1080 Ti   (*PrimaryCard),"", CUDA_toolkit_version: ""V9.2.148""}
```

### Source code / logs

```
DS1 SCOPE =============
graph: <tensorflow.python.framework.ops.Graph object at 0x7f0940f582b0>
scope: scope_0/scope_1
 name: scope_0/scope_1/x:0
  var: <tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32>
initial scope: 
DS1 SCOPE =============
graph: <tensorflow.python.framework.ops.Graph object at 0x7f0940f582b0>
scope: scope_0/scope_1
 name: scope_0/scope_1/x_1:0
  var: <tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>
=============
<tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>
Tensor(""arg0:0"", shape=(), dtype=int32)
2018-09-02 20:52:39.456276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-09-02 20:52:39.456710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.721
pciBusID: 0000:01:00.0
totalMemory: 10.92GiB freeMemory: 10.23GiB
2018-09-02 20:52:39.456728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0
2018-09-02 20:52:39.665323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-09-02 20:52:39.665362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 
2018-09-02 20:52:39.665369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N 
2018-09-02 20:52:39.665731: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9887 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-09-02 20:52:39.756397: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
Segmentation fault (core dumped)
```

log of run on CPU
```
CUDA_VISIBLE_DEVICES="""" python3 bug.py 
DS1 SCOPE =============
graph: <tensorflow.python.framework.ops.Graph object at 0x7ffaa61de390>
scope: scope_0/scope_1
 name: scope_0/scope_1/x:0
  var: <tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32>
initial scope: 
DS1 SCOPE =============
graph: <tensorflow.python.framework.ops.Graph object at 0x7ffaa61de390>
scope: scope_0/scope_1
 name: scope_0/scope_1/x_1:0
  var: <tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>
=============
<tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>
Tensor(""arg0:0"", shape=(), dtype=int32)
2018-09-02 20:54:41.271567: E tensorflow/stream_executor/cuda/cuda_driver.cc:300] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2018-09-02 20:54:41.271604: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] retrieving CUDA diagnostic information for host: 3bed2f328777
2018-09-02 20:54:41.271614: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:170] hostname: 3bed2f328777
2018-09-02 20:54:41.271655: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:194] libcuda reported version is: 396.26.0
2018-09-02 20:54:41.271686: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:198] kernel reported version is: 396.26.0
2018-09-02 20:54:41.271694: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:305] kernel version seems to match DSO: 396.26.0
2018-09-02 20:54:41.271962: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
Segmentation fault (core dumped)
```"
22012,[Bug] Discrepancy in tf.keras and keras in setting model.trainable = False and then compiling,"### System information
- Have I written custom code: Yes
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from: binary
- TensorFlow version: v1.10.1-0-g4dcfddc5d1 1.10.1
- Bazel version: N/A
- CUDA/cuDNN version: CUDA9.1, cuDNN7.0
- GPU model and memory: TITAN V
- Exact command to reproduce: python3 compare.py
- Mobile device: N/A

### Describe the problem
This is a toy case for training GAN problem.
When I run the code shown below in keras2.2.2, I get
```
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
```
only at the beginning of the training once.
However, I run this code in tensorflow1.10.1, the warning raises at every iteration.
Although it seems that the model is appropriately learned (I can make sure the weight is freezed by the result of `.summary()`), raising too many warning is not torelable.

Here is the complete log.
[tf.keras ver](https://github.com/tensorflow/tensorflow/files/2342979/tf_keras.log)
[keras ver](https://github.com/tensorflow/tensorflow/files/2342980/keras.log)

I see the same problem is posted in Stackoverflow
https://stackoverflow.com/questions/50468940/tensorflow-1-8-tf-keras-gives-different-result-in-dcgan-from-keras

```
import numpy as np

# use keras
from keras.layers import Dense, Input
from keras.models import Model

# use tf.keras
# from tensorflow.keras.layers import Dense, Input
# from tensorflow.keras.models import Model

# define input
noise = Input(shape=(10,))
x = Input(shape=(100,))

# define generator and discriminator
gen = Dense(100)
dis = Dense(1)

y = dis(x)
dis_model = Model(x, y)
dis_model.compile(optimizer='rmsprop', loss='mse')
dis_model.summary()

z = dis_model(gen(noise))
dis_model.trainable = False
combined_model = Model(noise, z)
combined_model.compile(optimizer='rmsprop', loss='mse')
combined_model.summary()

for i in range(3):
    dis_model.train_on_batch(x=np.random.rand(10, 100),
                             y=np.random.rand(10, 1))
    combined_model.train_on_batch(x=np.random.rand(10, 10),
                             y=np.random.rand(10, 1))

```
"
22011,bug in string_input_producer,"https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/python/training/input.py

line169:
element_shape = input_tensor.shape[1:].merge_with(element_shape)
 input_tensor.shape[1:] must be compatable with element_shape

line252:
element_shape = []

so, no matter what kind of shape you defined in input_tensor.shape[1:], it will raise an error!

import tensorflow as tf

test_list_1 = ['andy', 'jerry', 'lala', 'coco', 'mimi']
test_list_2 = ['andy', 'jerry', 'lala', 'coco', 'mimi']
test_tensor_1 = tf.convert_to_tensor(test_list_1)
test_tensor_2 = tf.convert_to_tensor(test_list_2)

res = tf.train.string_input_producer([test_tensor_1, test_tensor_2])

sess = tf.Session()

tf.train.start_queue_runners(sess = sess)

print(sess.run(res))

#ValueError: Shapes (5,) and () are not compatible"
22009,tf.get_variable returns Tensor instead of Variable,"-----------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:

TF checkpoint I have built
```
/tmp/tensorflow# git log   
commit 09792df012c22622324f085f46edde33006c7355
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Sun Aug 26 02:07:11 2018 -0700

    compat: Update forward compatibility horizon to 2018-08-26
    
    PiperOrigin-RevId: 210266798
```

```
== cat /etc/issue ===============================================
Linux 3bed2f328777 4.13.0-38-generic #43~16.04.1-Ubuntu SMP Wed Mar 14 17:48:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.5 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
Yes

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux 3bed2f328777 4.13.0-38-generic #43~16.04.1-Ubuntu SMP Wed Mar 14 17:48:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.14.5)
protobuf (3.6.1)
tensorflow (1.10.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.10.0
tf.GIT_VERSION = b'unknown'
tf.COMPILER_VERSION = b'unknown'
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64/stubs:/usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/lib/amd64/server/:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server:/opt/boost/lib:/opt/conda/lib/:/usr/local/cuda/lib64/:/opt/conda/lib/R/lib/:/usr/local/nvidia/lib64/:/usr/local/nvidia/lib:/lib/x86_64-linux-gnu:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu:/opt/opencv/lib
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Wed Aug 29 19:57:14 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 396.26                 Driver Version: 396.26                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A |
|  0%   41C    P0    76W / 250W |    708MiB / 11177MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart.so.9.2.148
/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart_static.a

```
**Bazel** version
```
$ bazel version
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
Build label: 0.16.0
Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Tue Jul 31 17:01:24 2018 (1533056484)
Build timestamp: 1533056484
Build timestamp as int: 1533056484
```

**CUDNN** version:
```
$ nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2018 NVIDIA Corporation
Built on Tue_Jun_12_23:07:04_CDT_2018
Cuda compilation tools, release 9.2, V9.2.148
```

**GPU**: GEFORCE GTX 1080Ti, 11GB, GIGABYTE AORUS

- **Exact command to reproduce**:
```
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import tensorflow as tf

def scope_1():
    print(""DS1 SCOPE ============="")
    with tf.variable_scope(""scope_1"", reuse=tf.AUTO_REUSE):
        x = tf.get_variable(""x"", initializer=0.0, dtype=tf.float32
                               , trainable=False, use_resource=True)             
        print(""graph: {}"".format(x.graph))
        print(""scope: {}"".format(tf.get_variable_scope().name))
        print("" name: {}"".format(x.name))
        print(""  var: {}"".format(str(x)))
        current_scope = tf.get_variable_scope()       
        assign_one = tf.assign(x, 1, name=""x_is_one"")
    
    def scope_2(inputs, label):        
        print(""initial scope: {}"".format(tf.get_variable_scope().name))
        print(""DS1 SCOPE ============="")
        with tf.variable_scope(""scope_1"", reuse=tf.AUTO_REUSE):
        #with tf.variable_scope(current_scope, reuse=tf.AUTO_REUSE):
            y = tf.get_variable(""x"", initializer=0.0, dtype=tf.float32
                                   , trainable=False, use_resource=True)         
            print(""graph: {}"".format(y.graph))
            print(""scope: {}"".format(tf.get_variable_scope().name))
            print("" name: {}"".format(y.name))
            print(""  var: {}"".format(str(y)))
            print(""============="")
            
            assign_two = tf.assign(y, inputs+1, name=""inputs_plus_1"")
            with tf.control_dependencies([assign_two]):
                return y, label
            #return x,label
    
    # test that original x is mutable
    with tf.control_dependencies([assign_one]):
        dataset = (tf.data.Dataset.from_tensor_slices(([1,2,3,4,5], [0,0,0,0,0]))
                    .map(scope_2)
                    .batch(2)
                    .repeat(10)        
                    )
    
                
with tf.variable_scope(""scope_0""):
        dataset_fn = scope_1

with tf.Session() as sess:
    dataset_fn()
    sess.run([tf.global_variables_initializer(),tf.local_variables_initializer()])
    print(sess.graph.get_operations())    
    print()
    print(""global vars: {}"".format(tf.global_variables()))
    print(""local vars: {}"".format(tf.local_variables()))
    print(tf.get_default_graph().get_name_scope())

```

### Describe the problem
I am trying to create a function that would modify a Tensor within a pipeline of Dataset API.
The scoping may seem weird, but that is minimal example that shows the problem that I created from my project.but somehow it is using `ReadVariableOp` that only 

Looks like when I fetch a variable using `tf.get_variable` (within def scope_2) it returns immutable Tensor, whereas the original Variable is mutable. In `scope_1` I am able to run `x.assign(0)` but then within `scope_2` (that is invoked in dataset.map) it throws an `AttributeError: 'Tensor' object has no attribute 'assign'`

According to the docs https://www.tensorflow.org/api_docs/python/tf/get_variable `tf.get_variable` should return a Variable not a Tensor.
```
Returns:
The created or existing Variable (or PartitionedVariable, if a partitioner was used).
```

### Source code / logs

```
2018-09-01 23:26:58.452631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-09-01 23:26:58.453216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.721
pciBusID: 0000:01:00.0
totalMemory: 10.92GiB freeMemory: 9.98GiB
2018-09-01 23:26:58.453237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0
2018-09-01 23:26:58.653750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-09-01 23:26:58.653789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 
2018-09-01 23:26:58.653796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N 
2018-09-01 23:26:58.654003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9644 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-09-01 23:26:58.758539: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
DS1 SCOPE =============
graph: <tensorflow.python.framework.ops.Graph object at 0x7f1ebf351400>
scope: scope_1
 name: scope_1/x:0
  var: <tf.Variable 'scope_1/x:0' shape=() dtype=float32>
initial scope: 
DS1 SCOPE =============
graph: <tensorflow.python.framework.ops.Graph object at 0x7f1ebf351400>
scope: scope_1
 name: ReadVariableOp:0
  var: Tensor(""ReadVariableOp:0"", shape=(), dtype=float32)
=============
Traceback (most recent call last):
  File ""bug.py"", line 50, in <module>
    dataset_fn()
  File ""bug.py"", line 40, in scope_1
    .map(scope_2)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1005, in map
    return MapDataset(self, map_func)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 2216, in __init__
    map_func, ""Dataset.map()"", input_dataset)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1473, in __init__
    self._function.add_to_graph(ops.get_default_graph())
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py"", line 479, in add_to_graph
    self._create_definition_if_needed()
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py"", line 335, in _create_definition_if_needed
    self._create_definition_if_needed_impl()
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py"", line 344, in _create_definition_if_needed_impl
    self._capture_by_value, self._caller_device)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py"", line 865, in func_graph_from_py_func
    outputs = func(*func_graph.inputs)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1411, in tf_data_structured_function_wrapper
    ret = func(*nested_args)
  File ""bug.py"", line 32, in scope_2
    assign_two = tf.assign(y, inputs+1, name=""inputs_plus_1"")
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py"", line 222, in assign
    return ref.assign(value, name=name)
AttributeError: 'Tensor' object has no attribute 'assign'

```"
22008,"After install 1.10.1, it doesn't work now","1. Use 

pip3 install https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.10.1-py3-none-any.whl 

to install

2. just run a hello-world:

```
File ""tf_beginner.py"", line 2, in <module>
    import tensorflow as tf
  File ""/Users/ /Desktop/dev/AI/TF_ENV/lib/python3.7/site-packages/tensorflow/__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/Users/ /Desktop/dev/AI/TF_ENV/lib/python3.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/Users/ /Desktop/dev/AI/TF_ENV/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/Users/ /Desktop/dev/AI/TF_ENV/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 114
    def TFE_ContextOptionsSetAsync(arg1, async):
                                             ^
SyntaxError: invalid syntax

``` "
22007,Newly included absl headers are missing from the include path,"[Newly included](https://github.com/tensorflow/tensorflow/commit/3cb3a450ed845c4602080f43d7bb6cfade298a22) `absl` headers are present in the `tf-nightly` Python package but do not seem to be part of the include path, and now custom op plugin compilation fails.

This has been broken between `tf_nightly-1.11.0.dev20180830` and `tf_nightly-1.11.0.dev20180831`.

```
    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -DHAVE_CUDA=1 -DHAVE_NCCL=1 -DHOROVOD_GPU_ALLREDUCE='N' -I/usr/local/cuda/include -I/usr/include/python2.7 -c horovod/tensorflow/mpi_ops.cc -o build/temp.linux-x86_64-2.7/horovod/tensorflow/mpi_ops.o -std=c++11 -fPIC -O2 -I/usr/local/include -pthread -Wl,-rpath -Wl,/usr/local/lib -Wl,--enable-new-dtags -L/usr/local/lib -lmpi -I/usr/local/lib/python2.7/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0
    cc1plus: warning: command line option '-Wstrict-prototypes' is valid for C/ObjC but not for C++
    In file included from /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/lib/core/status.h:24:0,
                     from /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/op_def_builder.h:25,
                     from /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/op.h:23,
                     from horovod/tensorflow/mpi_ops.cc:22:
    /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/lib/core/stringpiece.h:34:38: fatal error: absl/strings/string_view.h: No such file or directory
    compilation terminated.
    error: command 'x86_64-linux-gnu-gcc' failed with exit status 1
```

cc @yunxing "
22005,Running inference with TensorRT optimized protobuf using C++ API,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.10.1
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: 0.15.1
- **GCC/Compiler version (if compiling from source)**: 5
- **CUDA/cuDNN version**: 9.0 / 7.0
- **GPU model and memory**:

### Describe the problem
I successfully build from source TF 1.10.1 with TensorRT 3.04 support. Using the Python `tensorflow.contrib.tensorrt` API, I can create a protobuf that contains some TRTEngineOp operations, giving me a substantial inference speed-up.
However, now I need to run this optimized protobuf via a C++ API. I build the required .so libraries for the C++ inference API (such as libtensorflow.so, libtensorflow_cc.so and libtensorflow_framework.so). I can successfully load and run inference on the native TF protobuf, but not on the TRT optimized protobuf. 
It gives me an error saying that `TRTEngineOp not available.` Looking through Stackoverflow, I found some suggestions to use `TF_LoadLibrary` in the C API to load the `python/ops/_trt_engine_op.so` available in `tensorflow/contrib/tensorrt`. When I do this, the `TRTEngineOp` not available error disappears, but now it complains that it has no registered kernels. I noticed that the `python/ops/_trt_engine_op.so` Bazel target does not include any kernel definition, which explains why the TF_LoadLibrary doesn't find any kernels I guess. I am wondering how the Python API loads this. Can anyone help? Is it possible to run TensorRT optimized protobufs in C++ ?

### Steps to reproduce
1. Create an optimized protobuf in python, following for example this blog from [Google](https://developers.googleblog.com/2018/03/tensorrt-integration-with-tensorflow.html)

```
# Reserve memory for TensorRT inference engine
gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = number_between_0_and_1)
...  
trt_graph = trt.create_inference_graph(
                 input_graph_def = frozen_graph_def, 
                 outputs = output_node_name,
                 max_batch_size=batch_size,
                 max_workspace_size_bytes=workspace_size,
                 precision_mode=precision)  # Get optimized graph
```
then save as protobuf file.

2. Load optimized protobuf in C++, where we have access to the libtensorflow.so, libtensorflow_cc.so and libtensorflow_framework.so precompiled.
```
#include <tensorflow/core/graph/graph.h>
#include <tensorflow/core/graph/default_device.h>
#include <cuda_runtime.h>
#include tensorflow/c/c_api.h

auto status = TF_NewStatus();
TF_LoadLibrary(getPathToTrtEngineOpSo(), status);

tensorflow::SessionOptions sessionOptions;
sessionOptions.config.set_allow_soft_placement(getAllowSoftPlacement());
sessionOptions.config.set_log_device_placement(getLogDevicePlacement());
auto gpu_options = sessionOptions.config.mutable_gpu_options();
gpu_options->set_allow_growth(getAllowMemoryGrowth());
gpu_options->set_per_process_gpu_memory_fraction(getMemoryFraction());
_session.reset(tensorflow::NewSession(sessionOptions));

tensorflow::GraphDef graphDef;
checkStatusTF(tensorflow::ReadBinaryProto(tensorflow::Env::Default(),
                                                  getModel(),
                                                  &graphDef),
                      _logger);
tensorflow::graph::SetDefaultDevice(getDevice(), &graphDef);
checkStatusTF(_session->Create(graphDef), _logger);
```
followed by the appropriate run call.

The libtensorflow_cc.so, libtensorflow_framework.so and libtensorflow.so are unmodified targets from Tensorflow repostory r1.10.

"
22002,Feature Request: [tf.data] Access to the size of prefetch buffer,"**(TL;DR)** Add a new API that can tell the size of the prefetch buffer, similar to [FIFOQueue.size()](https://www.tensorflow.org/api_docs/python/tf/FIFOQueue#size).

### System information

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ALL
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: 
- **TensorFlow version (use command below)**: 1.10.1 (as of current master, it seems unimplemented yet)
- **Python version**: ALL
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem

`tf.data.Dataset.prefetch()` allows to have an efficient input pipeline by making input processing operations runnable in parallel to downstream GPU operations. It has a similar semantic to [FIFO Queues](https://www.tensorflow.org/api_docs/python/tf/FIFOQueue).

Currently, there is no way to access the number of elements in the queue or prefetch buffers. We may want to add some methods or operations for this in the [PrefetchDataset](https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/python/data/ops/dataset_ops.py#L2348) class. Traditional TF queue objects have [`size()`](https://www.tensorflow.org/api_docs/python/tf/FIFOQueue#size) method that returns a *scalar tensor* for the number of elements in the queue. It is very helpful to debug whether there is any performance issue on the input pipeline or preprocessing.

For example:

```python
  dataset = tf.data.Dataset.from_generator(...)
  dataset = dataset.shuffle(buffer_size=10000).repeat()

  dataset = dataset.prefetch(1024)             # returns an instance of PrefetchDataset
  prefetch_size_op = dataset.size()            # [!] Proposal. Needs to be named properly
  prefetch_capacity_op = dataset._buffer_size         # this is already available, but needs to be exposed public, e.g. dataset.capacity()

  dataset = dataset.batch(32)
  # ...
```

Thanks!"
22001,dataset from generator generate mess data with tf.device('cpu:0'),"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
22000,Map twice on a shuffled dataset got different result,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: source (self built)
- **TensorFlow version (use command below)**: v1.9.0-rc2-1175-g12f51c2 1.9.0
- **Python version**: 3.x
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I do a shuffle on a dataset and then map it twice before zip the result to a pair. I expect the pair has the same value, e.g. (0,0) or (30,30) but the running result shows differently, e.g. (78, 31). Is it a normal behavior of shuffle and map? Why?

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
dataset = tf.data.Dataset.range(100)
dataset = dataset.shuffle(buffer_size=100)
feature = dataset.map(lambda x: x)
label = dataset.map(lambda x: x)
dataset = tf.data.Dataset.zip((feature, label))
next_item = dataset.make_one_shot_iterator().get_next()
with tf.Session() as sess:
    print(sess.run(next_item))
```"
21999,tensorflow master build fail  (with VERBS),"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 and Centos 7.5
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: source/ master branch
- **TensorFlow version (use command below)**: master
- **Python version**:2.7 and 3.5
- **Bazel version (if compiling from source)**:0.15
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:9.0 and 9.2
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
Built (with VERBS) successfully several days ago, but today after new pull from master with same setting as before, build failed.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
ERROR: /mnt/nfs/tensorflow/tensorflow/contrib/verbs/BUILD:90:1: C++ compilation of rule '//tensorflow/contrib/verbs:rdma_rendezvous_mgr' failed (Exit 1)
In file included from ./tensorflow/core/framework/common_shape_fns.h:22:0,
                 from ./tensorflow/core/framework/resource_mgr.h:24,
                 from ./tensorflow/core/common_runtime/device.h:43,
                 from ./tensorflow/core/common_runtime/device_mgr.h:24,
                 from ./tensorflow/core/distributed_runtime/worker_session.h:21,
                 from ./tensorflow/core/distributed_runtime/base_rendezvous_mgr.h:24,
                 from ./tensorflow/contrib/verbs/rdma_rendezvous_mgr.h:22,
                 from tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc:18:
./tensorflow/core/util/tensor_format.h: In function 'tensorflow::TensorShape tensorflow::ShapeFromFormat(tensorflow::TensorFormat, tensorflow::int64, tensorflow
::gtl::ArraySlice<long long int>, tensorflow::int64)':
./tensorflow/core/util/tensor_format.h:501:65: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (format == FORMAT_NHWC_VECT_W && dim == spatial.size() - 1) {
                                                                 ^
tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc: In member function 'virtual void tensorflow::RdmaRemoteRendezvous::RecvFromRemoteAsync(const tensorflow::Rendez
vous::ParsedKey&, const tensorflow::Rendezvous::Args&, tensorflow::Rendezvous::DoneCallback)':
tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc:66:41: error: 'using StringPiece = class absl::string_view' has no member named 'ToString'
   string key(std::move(parsed.FullKey().ToString()));
                                         ^
```"
21998,TensorRT INT8 assertion failed when performing calibration,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
('v1.9.0-rc2-3472-g61038f8', '1.10.0')
github master commit: 61038f89e98fc430c56a9aa056692413c82eaafa

- **Python version**:
- **Bazel version (if compiling from source)**:
0.16.1
- **GCC/Compiler version (if compiling from source)**:
5.4.0
- **CUDA/cuDNN version**:
CUDA 9.0 cuDNN 7.1.4
- **GPU model and memory**:
Titan V
- **Exact command to reproduce**:
I build TensorFlow from source with TensorRT 4.

Use official TensorFlow-TensorRT example:
https://github.com/tensorflow/models/tree/master/research/tensorrt
and do INT8 test with command:

> python tensorrt.py --frozen_graph=resnetv2_imagenet_frozen_graph.pb --image_file=image.jpg --int8 --output_dir=output

Note that you should download ImageNet pretrained model http://download.tensorflow.org/models/official/resnetv2_imagenet_frozen_graph.pb

### Describe the problem
Program core dumped when doing calibration step

> INFO:tensorflow:Starting Warmup cycle
> 2018-08-31 14:53:15.227087: I tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:577] Starting calibration thread on device 0, Calibration Resource @ 0x7f76f0001860                             
> 2018-08-31 14:53:31.166463: I tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:577] Starting calibration thread on device 0, Calibration Resource @ 0x7f76f1709b00                             
> python: helpers.cpp:56: nvinfer1::DimsCHW nvinfer1::getCHW(const nvinfer1::Dims&): Assertion `d.nbDims >= 3' failed.                                                                               
> [1]    4880 abort (core dumped)  python tensorrt.py --frozen_graph=resnetv2_imagenet_frozen_graph.pb  --int8

This problem doesn't exist with tf 1.7 and 1.8.
"
21995,[solved] Android Static Library in Visual Studio with Tensorflow C++ API,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Any Android device
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: 0.7
- **NDK version**: 14b
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:
```sh
bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a
```

### Describe the problem

Hi,
currently, I have been trying to use Tensorflow C++ API to create an Android static library using Visual Studio. There are some Stack Overflow Questions related to this ([1](https://stackoverflow.com/questions/45066790/tensorflow-c-example-on-android), [2](https://stackoverflow.com/questions/48647592/compile-tensorflow-c-api-for-arm64-v8a), [3](https://stackoverflow.com/questions/50964234/using-tensorflow-c-in-android-with-cmake), [4](https://stackoverflow.com/questions/48181092/tensorflow-c-input-output-tensor-reshape)), but all with no answers. I was unable to find issues about it in Tensorflow's repo.

I built Tensorflow for Android using Bazel in a Linux Ubuntu 18.04.1 LTS machine following [this instructions](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/android). Thus, I have a **libtensorflow_inference.so** and a java binding (**libandroid_tensorflow_inference_java.jar**). For my Android static library project in Visual Studio, I'm using the _libtensorflow_inference.so_ as ""_Additional Dependency_"" and the following ""_Additional Include Directories_"":

```
C:\tensorflow
C:\tensorflow\build
C:\tensorflow\build\external\eigen_archive
C:\tensorflow\third_party\eigen3
C:\tensorflow\build\protobuf\src\protobuf\src
C:\tensorflow\build\nsync\src\nsync\public
C:\tensorflow\build\snappy\src\snappy
```

However, I'm getting the following errors:

```
undefined reference to 'tensorflow::SessionOptions::SessionOptions()'
undefined reference to 'tensorflow::ConfigProto::~ConfigProto()'
undefined reference to 'tensorflow::ConfigProto::~ConfigProto()'
undefined reference to 'tensorflow::GraphDef::GraphDef(tensorflow::GraphDef const&)'
undefined reference to 'tensorflow::ConfigProto::ConfigProto(tensorflow::ConfigProto const&)'
undefined reference to 'tensorflow::ConfigProto::~ConfigProto()'
undefined reference to 'tensorflow::GraphDef::~GraphDef()'
undefined reference to 'tensorflow::GraphDef::~GraphDef()'
undefined reference to 'tensorflow::ConfigProto::~ConfigProto()'
undefined reference to 'tensorflow::GraphDef::~GraphDef()'
undefined reference to 'tensorflow::GraphDef::~GraphDef()'
undefined reference to 'tensorflow::GraphDef::GraphDef()'
undefined reference to 'tensorflow::Env::Default()'
undefined reference to 'tensorflow::ReadBinaryProto(tensorflow::Env*, std::string const&, google::protobuf::MessageLite*)'
undefined reference to 'tensorflow::TfCheckOpHelperOutOfLine(tensorflow::Status const&, char const*)'
undefined reference to 'tensorflow::internal::LogMessageFatal::LogMessageFatal(char const*, int)'
undefined reference to 'tensorflow::internal::LogMessageFatal::~LogMessageFatal()'
undefined reference to 'tensorflow::internal::LogMessageFatal::~LogMessageFatal()'
undefined reference to 'tensorflow::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**)'
undefined reference to 'tensorflow::TfCheckOpHelperOutOfLine(tensorflow::Status const&, char const*)'
undefined reference to 'tensorflow::internal::LogMessageFatal::LogMessageFatal(char const*, int)'
undefined reference to 'tensorflow::internal::LogMessageFatal::~LogMessageFatal()'
undefined reference to 'tensorflow::TfCheckOpHelperOutOfLine(tensorflow::Status const&, char const*)'
undefined reference to 'tensorflow::internal::LogMessageFatal::LogMessageFatal(char const*, int)'
undefined reference to 'tensorflow::internal::LogMessageFatal::~LogMessageFatal()'
```

Therefore, my questions are:
- It is possible to use the __libtensorflow_inference.so__ for a C++ Android static library?
- Am I missing addiitional dependencies?
"
21994,Tensorflow install should setup up GPU support automatically,"My only real experience with TensorFlow is using it with Keras and sometimes writing some probabilistic objective functions in TensorFlow. Anytime I require GPU support I switch to PyTorch because when I install it, it is automatically set up to run on GPUs. Is there any plan to implement this for TensorFlow?"
21991,Strange behavior of loss function in WALSMatrixFactorization TF 1.8,"Our current implementation of recommendation system use TF 1.8 and WALS algorithm. The model was trained using self.fit(input_fn=input_fn) and ML Engine with run time version 1.8. Data set was formed following [example](https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/deepdive/10_recommend/wals.ipynb) using tensorflow.train.Example(...) Extraction from training logs shown below. 

[Extraction from training logs](https://drive.google.com/open?id=1HS4dPXC31Qv42YAWb_PT5y_C_uFD03GM) 

The fit was performed with some default parameters. The loss value did decreased on second evaluation. However loss did not changed after that. The final Root weighted squared error (rwse) in this training became 0.126.

Hyperparameter tuning was performed later and the best parameter set was used in the following training. The result of that training is shown below.

[Post hyperparameter tuning log extraction](https://drive.google.com/open?id=1zrt1jsfVdFAtuEegzquV-8sU8AJtlmKY)

Tree things to note here. First, the loss value at the beginning is lower than at later evaluation steps. Low value in the beginning most likely due to choice of parameters from the results of hyperparameter tuning. Increase of the loss value later on looks strange. Second, its unchanged loss value after second evaluation. This pattern remains the same while self.fit(input_fn=input_fn) is used for model training. Third, the final rwse in this training became 0.487 while during hyperparameter tuning with the same parameter set rwse=0.015. 

The question is if anyone observed something similar? Could the reason of unchanged loss value be due to fixed learning rate? If yes, is it possible to change it while using WALSMatrixFactorization class and self.fit(input_fn=input_fn)??

------------------------

### System information

- GCP VM instance  created-with-datalab-version	20180503

- TensorFlow version 1.8.0

"
21988,Feature request for tf.test.TestCase.assertNotAllClose: do not log the differences,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOs
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.10
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
When running this test, it logs the failure of `assertAllClose` which is not necessary.

### Source code / logs

    import tensorflow as tf
    test_case = tf.test.TestCase()
    test_case.assertNotAllClose([1, 2, 3], [2, 3, 4])


The logs are (I would expect no log):

    not close where =  (array([0, 1, 2]),)
    not close lhs =  [1 2 3]
    not close rhs =  [2 3 4]
    not close dif =  [1 1 1]
    not close tol =  [3.e-06 4.e-06 5.e-06]
    dtype = int64, shape = (3,)
"
21987,problem of compiler libtensorflow_cc.so with debug info,"I want to compiler libtensorflow_cc.so with debug info by bazel .
try with lines :bazel build -c dbg --strip=never //tensorflow:libtensorflow_cc.so.
but it's not work .when I gdb .so, (no debug info ).
version info: tf-r1.4 ,bazel-10.0.
What's worng with my lines ?  thanks a lot ."
21986,ValueError: None is only supported in the 1st dimension.,"Using TensorFlow backend.
2018-08-30 20:26:09.136414: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
(?, 32, 280, 1)
(?, 35, 5990)
(?, 32, 280, 1)
(?, 35, 5990)
WARNING:tensorflow:From /home/huangwei/.local/lib/python3.5/site-packages/tensorflow/python/ops/init_ops.py:86: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.
Instructions for updating:
`normal` is a deprecated alias for `truncated_normal`
WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.
Traceback (most recent call last):
  File ""1.py"", line 15, in <module>
    tflite_model = converter.convert()
  File ""/home/huangwei/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/lite.py"", line 394, in convert
    ""invalid shape '{1}'."".format(_tensor_name(tensor), shape))
ValueError: None is only supported in the 1st dimension. Tensor 'the_input' has invalid shape '[None, 32, None, 1]'."
21985,While loop no gradients provided for any variable,"After i used while_loop, ValueError: no gradients provided for any variable , it seems like RefvariableProcessor wraps tf.variable.

![default](https://user-images.githubusercontent.com/12975526/44889979-d9ca6b80-ad0a-11e8-967b-0f31a2467c68.PNG)

# while_loop
def dynamic_pointing_decoder(self, U, mask):
        def _HMN(ut, h, us, ue):
            h_us_ue = tf.concat([h, us, ue], axis=1) #batch,5*d
            WD = tf.get_variable(name=""WD"", shape=(5 * self._config.hidden_dim, self._config.hidden_dim), dtype=tf.float32,initializer=xavier_initializer())
            r = tf.nn.tanh(tf.matmul(h_us_ue, WD)) #batch, d
            ut_r = tf.concat([ut, r], axis=1) #batch,3d
            W1 = tf.get_variable(name=""W1"", shape=(3 * self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype='float32', initializer=xavier_initializer())
            b1 = tf.get_variable(name=""b1_Bias"", shape=(self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())
            mt1 = tf.einsum('bt,top->bop', ut_r, W1) + b1
            mt1 = tf.reduce_max(mt1, axis=2) #batch * d
            W2 = tf.get_variable(name=""W2"", shape=(self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())
            b2 = tf.get_variable(name=""b2_Bias"", shape=(self._config.hidden_dim,self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())
            mt2 = tf.einsum('bi,ijp->bjp', mt1, W2) + b2
            mt2 = tf.reduce_max(mt2, axis=2) #batch * d
            mt12 = tf.concat([mt1, mt2], axis=1) # batch * 2d
            W3 = tf.get_variable(name=""W3"", shape=(2 * self._config.hidden_dim, 1, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())
            b3 = tf.get_variable(name=""b3_Bias"", shape=(1, self._config.pool_size), dtype='float32', initializer=tf.zeros_initializer())
            hmn = tf.einsum('bi,ijp->bjp', mt12, W3) + b3
            hmn = tf.reduce_max(hmn, axis=2) #batch 1
            hmn = tf.reshape(hmn, [-1]) #batch
            return hmn

        def body(time_step, p1s, p2s, alphas, betas, us, ue, state):
            us_ue = tf.concat([us, ue], axis=1)  # batch 4d
            h, state = cell(inputs=us_ue, state=state)  # batch * d

            with tf.variable_scope('alpha_HMN', reuse=tf.AUTO_REUSE):
                alpha = tf.map_fn(lambda ut: _HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)
                alpha = tf.transpose(alpha, [1, 0]) * tf.cast(mask, dtype=tf.float32)  # batch article_len
            i_start = tf.argmax(alpha, axis=1)  # batch
            i_start = tf.cast(i_start, tf.int32)
            s_idx = tf.stack([idx, i_start], axis=1)
            us = tf.gather_nd(U, s_idx)  # batch 2d

            with tf.variable_scope('betas_HMN', reuse=tf.AUTO_REUSE):
                beta = tf.map_fn(lambda ut: _HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)
                beta = tf.transpose(beta, [1, 0]) * tf.cast(mask, dtype=tf.float32)  # batch article_len
            i_end = tf.argmax(beta, axis=1)  # batch
            i_end = tf.cast(i_end, tf.int32)
            e_idx = tf.stack([idx, i_end], axis=1)
            ue = tf.gather_nd(U, e_idx)  # batch 2d

            p1s.write(time_step, i_start)
            p2s.write(time_step, i_end)
            alphas.write(time_step, alpha)
            betas.write(time_step, beta)
            return (time_step+1, p1s, p2s, alphas, betas, us, ue, state)

        def condition(time_step, p1s, p2s,alphas, betas, us, ue, state):
            return tf.less(time_step, 4)


        with tf.variable_scope('dynamic_pointing_decoder'):
            cell = CudnnCompatibleLSTMCell(self._config.hidden_dim)
            i_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)
            i_end = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)
            idx = tf.range(0, tf.shape(U)[0], 1)
            s_idx = tf.stack([idx, i_start], axis=1)
            e_idx = tf.stack([idx, i_end], axis=1)
            us = tf.gather_nd(U, s_idx) #batch 2d
            ue = tf.gather_nd(U, e_idx) #batch 2d
            p1s = tf.TensorArray(tf.int32, size=4)
            p2s = tf.TensorArray(tf.int32, size=4)
            alphas = tf.TensorArray(tf.float32, size=4)
            betas = tf.TensorArray(tf.float32, size=4)
            state = tf.nn.rnn_cell.LSTMStateTuple(tf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32),
                                              tf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32))  # initial hidden state of RNN
            U_transpose = tf.transpose(U, [1, 0, 2]) #a batch 2d
            #time_step, p1s, p2s, us, ue, state
            time_step = 0
            time_step, p1s, p2s, alphas, betas, us, ue, state = tf.while_loop(cond=condition, body=body,
                                                                              loop_vars=(time_step, p1s, p2s, alphas, betas, us, ue, state),
                                                                              maximum_iterations=4)
            self.p1s = tf.transpose(p1s.stack()) #batch*4
            self.p2s = tf.transpose(p2s.stack())
            print(
                ""p1s  shape : {0}"".format(np.shape(self.p1s))
            )
            self.p1 = tf.unstack(self.p1s,axis=0)[-1]
            print(""p1 shape : {0}"".format(np.shape(self.p1)))
            self.p2 = tf.unstack(self.p2s, axis=0)[-1]
            alphas = tf.unstack(alphas.stack(), axis=0)
            betas = tf.unstack(betas.stack(), axis=0)
            print('alphas 0 shape :{0}'.format(np.shape(alphas[0])))
            return alphas, betas

#no while loop
def dynamic_pointing_decoder(self, U, mask):
        def _HMN(ut, h, us, ue):
            h_us_ue = tf.concat([h, us, ue], axis=1) #batch,5*d
            WD = tf.get_variable(name=""WD"", shape=(5 * self._config.hidden_dim, self._config.hidden_dim), dtype=tf.float32,initializer=xavier_initializer())
            r = tf.nn.tanh(tf.matmul(h_us_ue, WD)) #batch, d
            ut_r = tf.concat([ut, r], axis=1) #batch,3d
            W1 = tf.get_variable(name=""W1"", shape=(3 * self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype='float32', initializer=xavier_initializer())
            b1 = tf.get_variable(name=""b1_Bias"", shape=(self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())
            mt1 = tf.einsum('bt,top->bop', ut_r, W1) + b1
            mt1 = tf.reduce_max(mt1, axis=2) #batch * d
            W2 = tf.get_variable(name=""W2"", shape=(self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())
            b2 = tf.get_variable(name=""b2_Bias"", shape=(self._config.hidden_dim,self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())
            mt2 = tf.einsum('bi,ijp->bjp', mt1, W2) + b2
            mt2 = tf.reduce_max(mt2, axis=2) #batch * d
            mt12 = tf.concat([mt1, mt2], axis=1) # batch * 2d
            W3 = tf.get_variable(name=""W3"", shape=(2 * self._config.hidden_dim, 1, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())
            b3 = tf.get_variable(name=""b3_Bias"", shape=(1, self._config.pool_size), dtype='float32', initializer=tf.zeros_initializer())
            hmn = tf.einsum('bi,ijp->bjp', mt12, W3) + b3
            hmn = tf.reduce_max(hmn, axis=2) #batch 1
            hmn = tf.reshape(hmn, [-1]) #batch
            return hmn
        with tf.variable_scope('dynamic_pointing_decoder'):
            #single_cell = lambda: CudnnCompatibleLSTMCell(self._config.hidden_dim)
            #cell = tf.nn.rnn_cell.MultiRNNCell([single_cell() for _ in range(1)])
            #cell = tf.nn.rnn_cell.LSTMCell(self._config.hidden_dim)
            cell = CudnnCompatibleLSTMCell(self._config.hidden_dim)
            i_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)
            i_end = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)
            #pre_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)
            #pre_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)
            idx = tf.range(0, tf.shape(U)[0], 1)
            s_idx = tf.stack([idx, i_start], axis=1)
            e_idx = tf.stack([idx, i_end], axis=1)
            us = tf.gather_nd(U, s_idx) #batch 2d
            ue = tf.gather_nd(U, e_idx) #batch 2d
            alphas, betas = [], []
            state = tf.nn.rnn_cell.LSTMStateTuple(tf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32),
                                              tf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32))  # initial hidden state of RNN
            U_transpose = tf.transpose(U, [1, 0, 2]) #a batch 2d
            for time_step in range(4):
                if time_step >= 1:
                    tf.get_variable_scope().reuse_variables()
                us_ue = tf.concat([us,ue], axis=1) #batch 4d
                h, state = cell(inputs=us_ue, state=state) #batch * d

                with tf.variable_scope('alpha_HMN'):
                    if time_step >= 1:
                        tf.get_variable_scope().reuse_variables()
                    alpha = tf.map_fn(lambda ut:_HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)
                    alpha = tf.transpose(alpha, [1,0]) * tf.cast(mask,dtype=tf.float32) # batch article_len
                i_start = tf.argmax(alpha, axis=1) #batch
                i_start = tf.cast(i_start, tf.int32)
                s_idx = tf.stack([idx, i_start], axis=1)
                us = tf.gather_nd(U, s_idx) #batch 2d

                with tf.variable_scope('betas_HMN'):
                    if time_step >= 1:
                        tf.get_variable_scope().reuse_variables()
                    beta = tf.map_fn(lambda ut:_HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)
                    beta = tf.transpose(beta, [1,0]) * tf.cast(mask,dtype=tf.float32) # batch article_len
                i_end = tf.argmax(alpha, axis=1) #batch
                i_end = tf.cast(i_end, tf.int32)
                e_idx = tf.stack([idx, i_end], axis=1)
                ue = tf.gather_nd(U, e_idx) #batch 2d

                alphas.append(alpha)
                betas.append(beta)
                #if tf.equal(pre_start, i_end) and tf.equal(pre_end, i_end):
                #    break
                #else:
                #    pre_start = i_start
                #    pre_end = i_end
            return alpha, beta

anyone can help me? Thank you very much
"
21984,Build from source failed on MSBuild absl/strings/str_format.h': No such file or directory,"
### System informationOS 
Platform and Distribution: Windows 2016 DataCenter
TensorFlow installed from: #21690
TensorFlow version: 1.10.0
Bazel version: N/A
CUDA/cuDNN version: Cuda v 9.2 / cuDNN 10-x64-v7.2.1.38
GPU model and memory: Nvidia K80 11GB
Exact command to reproduce: cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release ^
-DSWIG_EXECUTABLE=C:/Users/neoxz/Downloads/swigwin-3.0.10/swigwin-3.0.10/swig.exe ^
-DPYTHON_EXECUTABLE=C:/Anaconda/python.exe ^
-DPYTHON_LIBRARIES=C:/Anaconda/libs/python35.lib ^
-Dtensorflow_ENABLE_GPU=ON ^
-DCUDNN_HOME=""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.2"" ^
-Dtensorflow_ENABLE_MKL_SUPPORT=ON ^
-DMKL_HOME=""C:/Program Files (x86)/IntelSWTools/compilers_and_libraries"" ^
-Dtensorflow_CUDA_VERSION=9.2 ^
-Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX2 ^
-Dtensorflow_ENABLE_GRPC_SUPPORT=OFF ^
-Dtensorflow_BUILD_PYTHON_TESTS=OFF ^
-Dtensorflow_BUILD_CC_EXAMPLE=OFF
Mobile device:N/A
[cmakeresult.txt](https://github.com/tensorflow/tensorflow/files/2338523/cmakeresult.txt)
[cmakecommand.txt](https://github.com/tensorflow/tensorflow/files/2338524/cmakecommand.txt)
[old-tf_env.txt](https://github.com/tensorflow/tensorflow/files/2338525/old-tf_env.txt)
[tf_env.txt](https://github.com/tensorflow/tensorflow/files/2338526/tf_env.txt)
[msbuild.txt](https://github.com/tensorflow/tensorflow/files/2338527/msbuild.txt)






You can collect some of this information using our environment capture script:
 Files Attached.

b'v1.10.0-rc1-19-g656e7a2b34' 1.10.0

### Describe the problem
Tensorflow build from source failed on MSBuild.

### Source code / logs
Files attached.

cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release ^
-DSWIG_EXECUTABLE=C:/Users/neoxz/Downloads/swigwin-3.0.10/swigwin-3.0.10/swig.exe ^
-DPYTHON_EXECUTABLE=C:/Anaconda/python.exe ^
-DPYTHON_LIBRARIES=C:/Anaconda/libs/python35.lib ^
-Dtensorflow_ENABLE_GPU=ON ^
-DCUDNN_HOME=""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.2"" ^
-Dtensorflow_ENABLE_MKL_SUPPORT=ON ^
-DMKL_HOME=""C:/Program Files (x86)/IntelSWTools/compilers_and_libraries"" ^
-Dtensorflow_CUDA_VERSION=9.2 ^
-Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX2 ^
-Dtensorflow_ENABLE_GRPC_SUPPORT=OFF ^
-Dtensorflow_BUILD_PYTHON_TESTS=OFF ^
-Dtensorflow_BUILD_CC_EXAMPLE=OFF"
21983,Keras models converted to Estimators do not write summaries.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian Jessie
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: ('v1.9.0-0-g25c197e023', '1.9.0') 
- **Python version**: 2.7.9
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: See provided gist

### Describe the problem
I've been setting up Estimators by using Keras layers to define tensors and then feeding them into my own `model_fn`. This was a pain and prevented me from easily doing stuff like adding regularization, but was necessary until recently due to various bugs within `model_to_estimator`. Luckily those bugs have been fixed, and I can now use the much more canonical and proper `model_to_estimator`. Unfortunately there are still some holes...

Anyways, when I did this I would, in defining the model, set various summaries to be collected simply by calling `tf.summary.scalar` etc. as documented. However, when I take the same tensors and put them through a Keras model and then through `model_to_estimator`, the summaries I've defined do not get 
written.

I've tried a few things here, but largely haven't met with any success. One of the workarounds [I've seen suggested](https://groups.google.com/forum/#!topic/keras-users/rEJ1xYqD3AM) is to try and shove the result of `tf.summary.merge_all` into a `metric`, but this does not work since Keras wants those to be numeric and summaries are strings. This does not work if I put this in as `target_tensors` either, as commented out in the gist.

Note: I recognize that this is filed against TF 1.9, but I don't see anything in the 1.10 changelog to indicate that this was noted or fixed. I am in a weird situation where it is best for me to use 1.9, unfortunately.

### Source code / logs
The [gist](https://gist.github.com/zmjjmz/75ba919d5f2755738252b4d0b0032faa) here reproduces this issue by taking a common function to get the tensors for a model, and creating estimators in both scenarios. 

Below is the first event with a summary from each case. It shows that in the 'plain' case, the summary tag `class_norm` shows up whereas in the `keras` case it does not. This is the crux of the issue I'm encountering.

Plain case:
`{'value': [{'simple_value': 1.0, 'tag': u'enqueue_input/queue/enqueue_input/random_shuffle_queuefraction_over_250_of_750_full'}, {'simple_value': 4.119956016540527, 'tag': u'class_norm'}, {'simple_value': 0.6801050901412964, 'tag': u'loss'}]}`
Keras case:
`{'value': [{'simple_value': 1.0, 'tag': u'enqueue_input/queue/enqueue_input/random_shuffle_queuefraction_over_250_of_750_full'}, {'simple_value': 0.8805878758430481, 'tag': u'loss_1'}]}`

"
21982,`tf.gfile.Rename` failed to operate on S3,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04, Mac OS High Sierra 10.13.6
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: Binary for Ubuntu, source for Mac OS
- **TensorFlow version (use command below)**: 1.10.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.15.2-homebrew (Mac OS)
- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 9.1.0 (clang-902.0.39.2) (Mac OS)
- **CUDA/cuDNN version**: cuda 9.0, cudnn 7.1 (Ubuntu)
- **GPU model and memory**: TITAN V
- **Exact command to reproduce**:

1. Upload a file to s3, say ""s3://test/file.txt""
2. Rename the file through `tf.gfile.Rename` to ""s3://test/file_renamed.txt"" using the following code
```python
import tensorflow as tf
src = ""s3://test/file.txt""
dst = ""s3://test/file_renamed.txt""
tf.gfile.Rename(src, dst)
```

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I believe it is a bug in TensorFlow.

A problem is that this function doesn't seem to work for s3 storage, another problem is that the content of `InternalError` is empty (with only a single colon, see below).

This problem is first noticed when I was trying to start a distributed training session using Estimator. This problem comes out when the underlying `CheckpointSaverHook` tries to save the graph serialization. In the call `graph_io.write_graph`, the `file_io.atomic_write_string_to_file` function is called, which involves `file_io.wrtie_string_to_file` followed by a `file_io.rename` call. The first call succeeded, but the second one failed. I came up with the above reproduction code after going through the error stack, until I hit the empty `InternalError` that I got no information to work with.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

Note that `src` and `dst` are s3 paths, the following error is extracted from an interactive python session.
```
>>> tf.gfile.Rename(src, dst)
2018-08-30 10:38:20.947227: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing config loader against fileName /root//.aws/config and using profilePrefix = 1
2018-08-30 10:38:20.947301: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing config loader against fileName /root//.aws/credentials and using profilePrefix = 0
2018-08-30 10:38:20.947333: I tensorflow/core/platform/s3/aws_logging.cc:54] Setting provider to read credentials from /root//.aws/credentials for credentials file and /root//.aws/config for the config file , for use with profile default
2018-08-30 10:38:20.947379: I tensorflow/core/platform/s3/aws_logging.cc:54] Creating HttpClient with max connections2 and scheme http
2018-08-30 10:38:20.947415: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing CurlHandleContainer with size 2
2018-08-30 10:38:20.947443: I tensorflow/core/platform/s3/aws_logging.cc:54] Creating Instance with default EC2MetadataClient and refresh rate 900000
2018-08-30 10:38:20.947483: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key
2018-08-30 10:38:20.947582: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing CurlHandleContainer with size 25
2018-08-30 10:38:20.947677: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key
2018-08-30 10:38:20.948021: I tensorflow/core/platform/s3/aws_logging.cc:54] Pool grown by 2
2018-08-30 10:38:20.948051: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2018-08-30 10:38:21.031694: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404
2018-08-30 10:38:21.031791: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.
2018-08-30 10:38:21.031929: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key
2018-08-30 10:38:21.032102: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2018-08-30 10:38:21.051376: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key
2018-08-30 10:38:21.051520: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2018-08-30 10:38:21.073577: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key
2018-08-30 10:38:21.073852: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2018-08-30 10:38:21.152881: W tensorflow/core/platform/s3/aws_logging.cc:57] Unable to generate a proper httpResponse from the response stream.   Response code: 404
2018-08-30 10:38:21.153006: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/miniconda3/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py"", line 415, in rename
    compat.as_bytes(oldname), compat.as_bytes(newname), overwrite, status)
  File ""/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 519, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.InternalError: : 
>>> 
```"
21980,Variable in a while loop - Segmentation fault (core dumped) ,"I use TF 1.10.0, build from master last week.

I am trying to make a function to erase patches in images and wanted to apply it with map to a Dataset. I use scatter_nd_update, so I had to define a variable that would be updated outside of the loop and assign tensor to it within the loop, but then I get segmentation fault.


Log and the code

```
Using TensorFlow backend.
2018-08-30 21:36:51.556778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-08-30 21:36:51.557216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.721
pciBusID: 0000:01:00.0
totalMemory: 10.92GiB freeMemory: 10.03GiB
2018-08-30 21:36:51.557236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0
2018-08-30 21:36:51.755752: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-30 21:36:51.755792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 
2018-08-30 21:36:51.755799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N 
2018-08-30 21:36:51.755996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9692 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-08-30 21:36:51.840570: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
Using device /gpu:0, and data format channels_last.
Starting
Tensor(""learning_rate_decay:0"", shape=(), dtype=float32)
WARNING:tensorflow:case: An unordered dictionary of predicate/fn pairs was provided, but exclusive=False. The order of conditional tests is deterministic but not guaranteed.
(300, 28, 28, 1)
2018-08-30 21:36:52.518523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0
2018-08-30 21:36:52.518567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-30 21:36:52.518576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 
2018-08-30 21:36:52.518584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N 
2018-08-30 21:36:52.518686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9692 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-08-30 21:36:52.518876: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
Segmentation fault (core dumped)
```

```
def random_erase(img_z, img, label, width=28, height=28):
        #with g.as_default():
        a = tf.random_uniform(minval=0,   maxval=width-1, shape=[], dtype=tf.int32)
        b = tf.random_uniform(minval=a+1, maxval=width,   shape=[], dtype=tf.int32)

        c = tf.random_uniform(minval=0,   maxval=height-1, shape=[], dtype=tf.int32)
        d = tf.random_uniform(minval=c+1, maxval=height,   shape=[], dtype=tf.int32)

        h = tf.range(start=a, limit=b, dtype=tf.int32)
        v = tf.range(start=c, limit=d, dtype=tf.int32)

        h, v = h[ None, :, None ], v[ :, None, None ]
        cartesian_product = tf.concat( [ h + tf.zeros_like( v ),
                                        tf.zeros_like( h ) + v ], axis = 2, name=""caretsian_product"")
        
        random_patch = tf.random_uniform(minval=0, maxval=255, shape=[d-c,b-a], dtype=tf.float32, name=""random_patch"")
        return tf.scatter_nd_update(tf.assign(img_z, img), cartesian_product, random_patch, name=""image_with_patch""), label

```

and I use it in

```
    img_z = tf.Variable(tf.zeros(shape=[28*28], dtype=tf.float32),
                            trainable=False, use_resource=True)
    ds = mnist_train('.')
    ds = ds.map(lambda img, label:random_erase(img_z, img, label))
```"
21979,[Feature Request] Keeping Keras Callback functionality in Estimators created from `keras.model_to_estimator`,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: n/a
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a
- **TensorFlow installed from (source or binary)**: n/a
- **TensorFlow version (use command below)**: 1.9
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: n/a

### Describe the problem

Currently if I want to define a model in Keras and train it in a distributed fashion, the easiest/most-apparent method is to convert it into an Estimator using [model_to_estimator](https://www.tensorflow.org/api_docs/python/tf/keras/estimator/model_to_estimator). Unfortunately this has a few issues, and one of them is that any [Keras Callbacks](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback) go unpreserved.

Theoretically these callbacks could be converted into some form of [`SessionRunHook`](https://www.tensorflow.org/api_docs/python/tf/train/SessionRunHook) although the SessionRunHook doesn't know things that the Callback does (e.g. epoch information). However, this does not happen at all -- the `EstimatorSpec` that's returned out of the `model_fn` from `_create_keras_model_fn` [does not specify any sort of `training_hooks`](https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/python/estimator/keras.py#L400) regardless of whether or not the Keras model has Callbacks defined.

I'm sure this is a non-trivial problem to solve, however it is frustrating that `model_to_estimator` causes a loss in functionality like this.

Are there any plans to remedy this going forward?

Thanks!"
21978,Problem with TF runtime when running Ape-X code,"I ran the following command. 
`(env) hkaushal-mac01:ape-x hkaushal$ python apex.py --env video_pinball --num-timesteps 1000000000 --logdir=/tmp/agent
`

I am trying to run through the [ape-x code ](https://github.com/uber-research/ape-x), and the above errors are generated.

Python 2.7.15 :: Anaconda, Inc. ( Python version) 
 The other issues opened regarding this didn't help my case, and there are no stack-overflow posts regarding this as well since Ape-X was released only a day ago.
I'm trying to operate under gym_tensorflow of the Ape-X code for some context. 
Thanks! 




```
raceback (most recent call last):
  File ""/Users/hkaushal/Documents/env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/Users/hkaushal/Documents/env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/Users/hkaushal/Documents/env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: dlopen(/Users/hkaushal/Documents/env/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 10): Library not loaded: @rpath/libcublas.8.0.dylib
  Referenced from: /Users/hkaushal/Documents/env/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
  Reason: image not found

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""apex.py"", line 20, in <module>
    import tensorflow as tf
  File ""/Users/hkaushal/Documents/env/lib/python3.6/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/Users/hkaushal/Documents/env/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 51, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/Users/hkaushal/Documents/env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/Users/hkaushal/Documents/env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/Users/hkaushal/Documents/env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/Users/hkaushal/Documents/env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: dlopen(/Users/hkaushal/Documents/env/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 10): Library not loaded: @rpath/libcublas.8.0.dylib
  Referenced from: /Users/hkaushal/Documents/env/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
  Reason: image not found


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

```

`hkaushal-mac01:tools hkaushal$ bash ./tf_env_collect.sh 
`
 also gives me a similar error. ( this the command to provide the system information)


```
g++ -std=c++11 -shared -fPIC -I -I/external/nsync/public -L -D_GLIBCXX_USE_CXX11_ABI=0 -O2 -DGOOGLE_CUDA=1 -I/Users/hkaushal/Documents/ape-x/gym_tensorflow/atari-py/atari_py/ale_interface/src -I/Users/hkaushal/Documents/ape-x/gym_tensorflow/atari-py/atari_py/ale_interface/src/controllers -I/Users/hkaushal/Documents/ape-x/gym_tensorflow/atari-py/atari_py/ale_interface/src/os_dependent -I/Users/hkaushal/Documents/ape-x/gym_tensorflow/atari-py/atari_py/ale_interface/src/environment -I/Users/hkaushal/Documents/ape-x/gym_tensorflow/atari-py/atari_py/ale_interface/src/external -L/Users/hkaushal/Documents/ape-x/gym_tensorflow/atari-py/atari_py/ale_interface/build -framework Cocoa .//*.cpp .//atari/*.cpp -ltensorflow_framework -lale -o gym_tensorflow.so
.//tf_env.cpp:19:10: fatal error: 'tensorflow/core/framework/op_kernel.h' file
      not found
#include ""tensorflow/core/framework/op_kernel.h""
         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
1 error generated.
.//atari/tf_atari.cpp:18:10: fatal error: 'ale_interface.hpp' file not found
#include <ale_interface.hpp>
         ^~~~~~~~~~~~~~~~~~~
1 error generated.
make: *** [gym_tensorflow.so] Error 1

```



"
21977,(ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed,"
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:
1.10
- **Python version**:
3.6.2

Tensorflow CPU


### Describe the problem
whenever trying to import tensorflow i get this error.(ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.)


```
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\Cesar\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\Cesar\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 978, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 961, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 950, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 648, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 560, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 205, in _call_with_frames_removed
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Cesar\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Cesar\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Cesar\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\Cesar\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Cesar\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Cesar\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Cesar\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Cesar\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\Cesar\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 978, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 961, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 950, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 648, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 560, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 205, in _call_with_frames_removed
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Cesar\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Cesar\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Cesar\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\Cesar\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems
```
I know this problem has to do with the windows installation, some problem with the visual c++ compiler 2015 update 3.
The thing is i have them already and it doesnt work. I have tried using an Anaconda enviorment and it didnt work neither whe i used pip3 install --upgrade tensorflow there. But, when i install with the conda installer (conda install tensorflow) it worked perfectly. 

So i assume the problem has to do with some path or something like that in my native enviorment. But i dont have a clue..."
21976,Build can't find standard libraries,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 17.10
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: master
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**: bazel release 0.16.0
- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.5.0-1ubuntu2) 5.4.1 20171010
- **CUDA/cuDNN version**:9.0/7.0
- **GPU model and memory**: Nvidia GTX 970
- **Exact command to reproduce**:  bazel build --verbose_failures -c opt //tensorflow/tools/pip_package:build_pip_package


### Describe the problem
I tried building TF from source, however my build always fails. During the build, some standard library is not found. So far I have seen cassert, fstream, cstdint and stddef.h. When I compile a simple test program which just loads these and returns 0 with gcc everything goes well. I have tried adding paths to CROSSTOOL, but that didn't help (though I am not sure if the place in the file was correct).


### Bazel output
```
ERROR: /home/kocur/.cache/bazel/_bazel_kocur/fe42bf71aeb545557cf2194b2de024c0/external/protobuf_archive/BUILD:260:1: C++ compilation of rule '@protobuf_archive//:js_embed' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command
  (cd /home/kocur/.cache/bazel/_bazel_kocur/fe42bf71aeb545557cf2194b2de024c0/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=:/usr/local/cuda-9.0/lib64:/usr/local/cuda-9.0/extras/CUPTI/lib64:/usr/local/cuda-9.0/targets/x86_64-linux/lib/:/usr/local/cuda-9.0/lib64:/usr/local/cuda-9.0/extras/CUPTI/lib64:/usr/local/cuda-9.0/targets/x86_64-linux/lib/ \
    PATH=/home/kocur/.local/share/virtualenvs/code-B3GmqseA/bin:/home/kocur/bin:/home/kocur/.local/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/local/cuda/bin:/usr/local/Matlab/R2012a/bin:/usr/lib/jvm/java-10-oracle/bin:/usr/lib/jvm/java-10-oracle/db/bin \
    PWD=/proc/self/cwd \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/external/protobuf_archive/_objs/js_embed/embed.d '-frandom-seed=bazel-out/host/bin/external/protobuf_archive/_objs/js_embed/embed.o' -iquote external/protobuf_archive -iquote bazel-out/host/genfiles/external/protobuf_archive -iquote bazel-out/host/bin/external/protobuf_archive -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -iquote bazel-out/host/bin/external/bazel_tools '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIE -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 -g0 -c external/protobuf_archive/src/google/protobuf/compiler/js/embed.cc -o bazel-out/host/bin/external/protobuf_archive/_objs/js_embed/embed.o)
external/protobuf_archive/src/google/protobuf/compiler/js/embed.cc:31:19: fatal error: cassert: No such file or directory
compilation terminated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 4.996s, Critical Path: 0.19s
INFO: 10 processes: 10 local.
FAILED: Build did NOT complete successfully
```

### Output of echo | gcc -E -xc++ - -v

```
Using built-in specs.
COLLECT_GCC=gcc
Target: x86_64-linux-gnu
Configured with: ../src/configure -v --with-pkgversion='Ubuntu 5.5.0-1ubuntu2' --with-bugurl=file:///usr/share/doc/gcc-5/README.Bugs --enable-languages=c,ada,c++,java,go,d,fortran,objc,obj-c++ --prefix=/usr --program-suffix=-5 --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-libmpx --enable-plugin --enable-default-pie --with-system-zlib --disable-browser-plugin --enable-java-awt=gtk --enable-gtk-cairo --with-java-home=/usr/lib/jvm/java-1.5.0-gcj-5-amd64/jre --enable-java-home --with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-5-amd64 --with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-5-amd64 --with-arch-directory=amd64 --with-ecj-jar=/usr/share/java/eclipse-ecj.jar --enable-objc-gc --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu
Thread model: posix
gcc version 5.4.1 20171010 (Ubuntu 5.5.0-1ubuntu2)
COLLECT_GCC_OPTIONS='-E' '-v' '-mtune=generic' '-march=x86-64'
 /usr/lib/gcc/x86_64-linux-gnu/5/cc1plus -E -quiet -v -imultiarch x86_64-linux-gnu -D_GNU_SOURCE - -mtune=generic -march=x86-64 -fstack-protector-strong -Wformat -Wformat-security
ignoring duplicate directory ""/usr/include/x86_64-linux-gnu/c++/5""
ignoring nonexistent directory ""/usr/local/include/x86_64-linux-gnu""
ignoring nonexistent directory ""/usr/lib/gcc/x86_64-linux-gnu/5/../../../../x86_64-linux-gnu/include""
#include ""..."" search starts here:
#include <...> search starts here:
 /usr/include/c++/5
 /usr/include/x86_64-linux-gnu/c++/5
 /usr/include/c++/5/backward
 /usr/lib/gcc/x86_64-linux-gnu/5/include
 /usr/local/include
 /usr/lib/gcc/x86_64-linux-gnu/5/include-fixed
 /usr/include/x86_64-linux-gnu
 /usr/include
End of search list.
# 1 ""<stdin>""
# 1 ""<built-in>""
# 1 ""<command-line>""
# 1 ""/usr/include/stdc-predef.h"" 1 3 4
# 1 ""<command-line>"" 2
# 1 ""<stdin>""
COMPILER_PATH=/usr/lib/gcc/x86_64-linux-gnu/5/:/usr/lib/gcc/x86_64-linux-gnu/5/:/usr/lib/gcc/x86_64-linux-gnu/:/usr/lib/gcc/x86_64-linux-gnu/5/:/usr/lib/gcc/x86_64-linux-gnu/
LIBRARY_PATH=/usr/lib/gcc/x86_64-linux-gnu/5/:/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/:/usr/lib/gcc/x86_64-linux-gnu/5/../../../../lib/:/lib/x86_64-linux-gnu/:/lib/../lib/:/usr/lib/x86_64-linux-gnu/:/usr/lib/../lib/:/usr/lib/gcc/x86_64-linux-gnu/5/../../../:/lib/:/usr/lib/
COLLECT_GCC_OPTIONS='-E' '-v' '-mtune=generic' '-march=x86-64'
```
"
21975,Recalculate a convolution value with the eager execution,"### System information
- **Windows 10**:
- **TensorFlow installed from conda**:
- **TensorFlow version 1.9**:
- **Python version 3.6**:

### Describe the problem
I'm trying to use the eager execution.

I create a training set, a weight and a convolution layer.

I declare the convolution and change the weights.

How can I get the convolution calculated again without having to declare the layer again?

I expected it to be something like that:

	import tensorflow as tf

	tf.enable_eager_execution()    
	tfe = tf.contrib.eager


	TrainingDataExample = tf.constant(0.5, shape=[8, 5, 6, 1], name=""Inputs"") 
	WeightExample = tfe.Variable(tf.truncated_normal([1, 3, 1, 4], seed=1), name=""Weights"")
	ConvExample = tf.nn.conv2d(TrainingDataExample, WeightExample, strides=[1, 1, 1, 1], padding=""VALID"", name=""Conv"")

	NewWeightExample = tf.constant(2.0, shape=[1, 3, 1, 4], name=""NewWeights"")
	WeightExample = tf.assign(WeightExample, NewWeightExample )
	result = ConvExample 

	print (result)
But it does not work, the value of the convolution is not updated.

I know I can do it like this:
	
	NewWeightExample = tf.constant(2.0, shape=[1, 3, 1, 4], name=""NewWeights"")
	WeightExample = tf.assign(WeightExample, NewWeightExample )
	ConvExample = tf.nn.conv2d(TrainingDataExample, WeightExample,
	                           strides=[1, 1, 1, 1], padding=""VALID"",
	                           name=""Conv"")

But I understand that the main class contains a __call__ method just like the model class.

Maybe this is wrong, but I would like to know if I can do this without having to recompute the convolution again.
"
21974,Seeding tf.random_uniform with tf.Tensors,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.13.6
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.10.0
- **Python version**: 3.6.5
- **Exact command to reproduce**: (feature request, see code below)

### Describe the problem

For my application I need to sample a random tensor on one device and later transfer it to a different device. To save on networking I would like to instead only transfer a seed (that gets expanded on both devices before further processing). I need `tf.random_uniform` in particular.

Problem is that currently it seems the seed for `tf.random_uniform` must be a Python `int`, yet for this application the seed cannot be part of the script/graph but must be sampled by the first device.

```
TypeError: Expected int for argument 'seed2' not <tf.Tensor 'random_uniform_1/mod:0' shape=() dtype=int64>.
```

At the same time it's not clear whether this represents an inherent limitation of TensorFlow since `Dataset.shuffle` does seem to support `tf.Tensors` as seeds (inspired by #17284).

### Source code / logs

The following code is what I'd ideally like to do but produces the error message above:
```python
import tensorflow as tf

seed = tf.random_uniform(shape=[], dtype=tf.int64, maxval=1000000)
x = tf.random_uniform(shape=(1,), dtype=tf.int32, maxval=100, seed=seed)

sess = tf.Session()
print(sess.run(x))
```

This code works but seems sub-ideal:
```python
import tensorflow as tf
import numpy as np

seed = tf.random_uniform(shape=[], dtype=tf.int64, maxval=1000000)

dataset = tf.data.Dataset.from_tensor_slices(np.arange(100))
dataset = dataset.shuffle(buffer_size=100, seed=seed)
iterator = dataset.make_initializable_iterator()
x = iterator.get_next()

sess = tf.Session()
sess.run(iterator.initializer)
print(sess.run(x))
```"
21973,C++ compilation of rule '//tensorflow/python:py_func_lib' failed,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source (r1.10)
- **TensorFlow version (use command below)**: r1.10
- **Python version**: 3.7.0
- **Bazel version (if compiling from source)**: 0.16.0
- **GCC/Compiler version (if compiling from source)**: 8.2.0
- **CUDA/cuDNN version**: 9.2/7.2
- **GPU model and memory**: GTX 1060 6GB
- **Exact command to reproduce**: `bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`

### Describe the problem
Unable to build TensorFlow from source.

### Source code / logs

Output of `bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures`

```
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
Loading: 
Loading: 0 packages loaded
DEBUG: /home/mukundan/.cache/bazel/_bazel_mukundan/f5df5e8cbf5aa4e0efe27dc48f25989d/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5: 
Auto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\Windows\Temp' as default
WARNING: /home/mukundan/.cache/bazel/_bazel_mukundan/f5df5e8cbf5aa4e0efe27dc48f25989d/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/mukundan/.cache/bazel/_bazel_mukundan/f5df5e8cbf5aa4e0efe27dc48f25989d/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: /home/mukundan/.cache/bazel/_bazel_mukundan/f5df5e8cbf5aa4e0efe27dc48f25989d/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/mukundan/.cache/bazel/_bazel_mukundan/f5df5e8cbf5aa4e0efe27dc48f25989d/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: /home/mukundan/.cache/bazel/_bazel_mukundan/f5df5e8cbf5aa4e0efe27dc48f25989d/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/mukundan/.cache/bazel/_bazel_mukundan/f5df5e8cbf5aa4e0efe27dc48f25989d/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: /home/mukundan/machine_learning/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: /home/mukundan/machine_learning/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
WARNING: /home/mukundan/machine_learning/tensorflow/tensorflow/contrib/timeseries/python/timeseries/BUILD:357:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries:ar_model: target '//tensorflow/contrib/timeseries/python/timeseries:ar_model' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/mukundan/machine_learning/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:73:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/mukundan/machine_learning/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:230:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/mukundan/machine_learning/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/mukundan/machine_learning/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/mukundan/machine_learning/tensorflow/tensorflow/contrib/kfac/python/ops/BUILD:80:1: in py_library rule //tensorflow/contrib/kfac/python/ops:loss_functions: target '//tensorflow/contrib/kfac/python/ops:loss_functions' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/mukundan/machine_learning/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded).
INFO: Found 1 target...
[13 / 20] [-----] BazelWorkspaceStatusAction stable-status.txt
[98 / 112] Compiling tensorflow/core/kernels/slice_op_gpu.cu.cc; 1s local ... (4 actions running)
ERROR: /home/mukundan/machine_learning/tensorflow/tensorflow/python/BUILD:330:1: C++ compilation of rule '//tensorflow/python:py_func_lib' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/mukundan/.cache/bazel/_bazel_mukundan/f5df5e8cbf5aa4e0efe27dc48f25989d/execroot/org_tensorflow && \
  exec env - \
    PATH=/home/mukundan/.pyenv/plugins/pyenv-virtualenv/shims:/home/mukundan/.pyenv/shims:/home/mukundan/.pyenv/bin:/home/mukundan/bin:/home/mukundan/.pyenv/plugins/pyenv-virtualenv/shims:/home/mukundan/.pyenv/shims:/home/mukundan/.pyenv/bin:/home/mukundan/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/opt/cuda/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl \
    PWD=/proc/self/cwd \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/tensorflow/python/_objs/py_func_lib/py_func.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/python/_objs/py_func_lib/py_func.pic.o' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DTENSORFLOW_USE_JEMALLOC -DTENSORFLOW_USE_ABSL -DTF_USE_SNAPPY -DCURL_STATICLIB -DPLATFORM_LINUX -DENABLE_CURL_CLIENT -DENABLE_NO_ENCRYPTION -DSQLITE_OMIT_DEPRECATED '-DGRPC_ARES=0' '-DPB_FIELD_16BIT=1' -iquote . -iquote bazel-out/host/genfiles -iquote bazel-out/host/bin -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -iquote bazel-out/host/bin/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/host/genfiles/external/eigen_archive -iquote bazel-out/host/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/host/genfiles/external/local_config_sycl -iquote bazel-out/host/bin/external/local_config_sycl -iquote external/com_google_absl -iquote bazel-out/host/genfiles/external/com_google_absl -iquote bazel-out/host/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/host/genfiles/external/nsync -iquote bazel-out/host/bin/external/nsync -iquote external/jemalloc -iquote bazel-out/host/genfiles/external/jemalloc -iquote bazel-out/host/bin/external/jemalloc -iquote external/gif_archive -iquote bazel-out/host/genfiles/external/gif_archive -iquote bazel-out/host/bin/external/gif_archive -iquote external/jpeg -iquote bazel-out/host/genfiles/external/jpeg -iquote bazel-out/host/bin/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/host/genfiles/external/protobuf_archive -iquote bazel-out/host/bin/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/genfiles/external/com_googlesource_code_re2 -iquote bazel-out/host/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/genfiles/external/farmhash_archive -iquote bazel-out/host/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/genfiles/external/fft2d -iquote bazel-out/host/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/genfiles/external/highwayhash -iquote bazel-out/host/bin/external/highwayhash -iquote external/zlib_archive -iquote bazel-out/host/genfiles/external/zlib_archive -iquote bazel-out/host/bin/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/host/genfiles/external/local_config_cuda -iquote bazel-out/host/bin/external/local_config_cuda -iquote external/local_config_python -iquote bazel-out/host/genfiles/external/local_config_python -iquote bazel-out/host/bin/external/local_config_python -iquote external/double_conversion -iquote bazel-out/host/genfiles/external/double_conversion -iquote bazel-out/host/bin/external/double_conversion -iquote external/curl -iquote bazel-out/host/genfiles/external/curl -iquote bazel-out/host/bin/external/curl -iquote external/boringssl -iquote bazel-out/host/genfiles/external/boringssl -iquote bazel-out/host/bin/external/boringssl -iquote external/jsoncpp_git -iquote bazel-out/host/genfiles/external/jsoncpp_git -iquote bazel-out/host/bin/external/jsoncpp_git -iquote external/aws -iquote bazel-out/host/genfiles/external/aws -iquote bazel-out/host/bin/external/aws -iquote external/cub_archive -iquote bazel-out/host/genfiles/external/cub_archive -iquote bazel-out/host/bin/external/cub_archive -iquote external/org_sqlite -iquote bazel-out/host/genfiles/external/org_sqlite -iquote bazel-out/host/bin/external/org_sqlite -iquote external/snappy -iquote bazel-out/host/genfiles/external/snappy -iquote bazel-out/host/bin/external/snappy -iquote external/png_archive -iquote bazel-out/host/genfiles/external/png_archive -iquote bazel-out/host/bin/external/png_archive -iquote external/lmdb -iquote bazel-out/host/genfiles/external/lmdb -iquote bazel-out/host/bin/external/lmdb -iquote external/gemmlowp -iquote bazel-out/host/genfiles/external/gemmlowp -iquote bazel-out/host/bin/external/gemmlowp -iquote external/grpc -iquote bazel-out/host/genfiles/external/grpc -iquote bazel-out/host/bin/external/grpc -isystem external/eigen_archive -isystem bazel-out/host/genfiles/external/eigen_archive -isystem bazel-out/host/bin/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/host/genfiles/external/nsync/public -isystem bazel-out/host/bin/external/nsync/public -isystem external/jemalloc/include -isystem bazel-out/host/genfiles/external/jemalloc/include -isystem bazel-out/host/bin/external/jemalloc/include -isystem external/gif_archive/lib -isystem bazel-out/host/genfiles/external/gif_archive/lib -isystem bazel-out/host/bin/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/host/genfiles/external/protobuf_archive/src -isystem bazel-out/host/bin/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/host/genfiles/external/farmhash_archive/src -isystem bazel-out/host/bin/external/farmhash_archive/src -isystem external/zlib_archive -isystem bazel-out/host/genfiles/external/zlib_archive -isystem bazel-out/host/bin/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda -isystem bazel-out/host/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include/crt -isystem external/local_config_python/numpy_include -isystem bazel-out/host/genfiles/external/local_config_python/numpy_include -isystem bazel-out/host/bin/external/local_config_python/numpy_include -isystem external/local_config_python/python_include -isystem bazel-out/host/genfiles/external/local_config_python/python_include -isystem bazel-out/host/bin/external/local_config_python/python_include -isystem external/double_conversion -isystem bazel-out/host/genfiles/external/double_conversion -isystem bazel-out/host/bin/external/double_conversion -isystem external/curl/include -isystem bazel-out/host/genfiles/external/curl/include -isystem bazel-out/host/bin/external/curl/include -isystem external/boringssl/src/include -isystem bazel-out/host/genfiles/external/boringssl/src/include -isystem bazel-out/host/bin/external/boringssl/src/include -isystem external/jsoncpp_git/include -isystem bazel-out/host/genfiles/external/jsoncpp_git/include -isystem bazel-out/host/bin/external/jsoncpp_git/include -isystem external/aws/aws-cpp-sdk-core/include -isystem bazel-out/host/genfiles/external/aws/aws-cpp-sdk-core/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-core/include -isystem external/aws/aws-cpp-sdk-kinesis/include -isystem bazel-out/host/genfiles/external/aws/aws-cpp-sdk-kinesis/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-kinesis/include -isystem external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/host/genfiles/external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-s3/include -isystem external/png_archive -isystem bazel-out/host/genfiles/external/png_archive -isystem bazel-out/host/bin/external/png_archive -isystem external/local_config_cuda/cuda/cuda/extras/CUPTI/include -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda/cuda/extras/CUPTI/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/extras/CUPTI/include -isystem external/grpc/include -isystem bazel-out/host/genfiles/external/grpc/include -isystem bazel-out/host/bin/external/grpc/include -isystem external/grpc/third_party/address_sorting/include -isystem bazel-out/host/genfiles/external/grpc/third_party/address_sorting/include -isystem bazel-out/host/bin/external/grpc/third_party/address_sorting/include '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 '-march=native' -g0 -c tensorflow/python/lib/core/py_func.cc -o bazel-out/host/bin/tensorflow/python/_objs/py_func_lib/py_func.pic.o)
In file included from ./tensorflow/core/framework/common_shape_fns.h:22:0,
                 from ./tensorflow/core/framework/resource_mgr.h:24,
                 from ./tensorflow/core/common_runtime/device.h:43,
                 from ./tensorflow/core/common_runtime/graph_runner.h:23,
                 from ./tensorflow/core/common_runtime/shape_refiner.h:20,
                 from ./tensorflow/c/c_api_internal.h:30,
                 from ./tensorflow/c/eager/c_api_internal.h:30,
                 from tensorflow/python/lib/core/py_func.cc:24:
./tensorflow/core/util/tensor_format.h: In function 'tensorflow::TensorShape tensorflow::ShapeFromFormat(tensorflow::TensorFormat, tensorflow::int64, tensorflow::gtl::ArraySlice<long long int>, tensorflow::int64)':
./tensorflow/core/util/tensor_format.h:500:45: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (format == FORMAT_NHWC_VECT_W && dim == spatial.size() - 1) {
                                         ~~~~^~~~~~~~~~~~~~~~~~~~~
tensorflow/python/lib/core/py_func.cc: In function 'tensorflow::Status tensorflow::ConvertNdarrayToTensor(PyObject*, tensorflow::Tensor*)':
tensorflow/python/lib/core/py_func.cc:355:39: error: invalid conversion from 'const char*' to 'char*' [-fpermissive]
           el = PyUnicode_AsUTF8AndSize(input_data[i], &el_size);
                ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~
In file included from ./tensorflow/core/platform/default/logging.h:24:0,
                 from ./tensorflow/core/platform/logging.h:25,
                 from ./tensorflow/core/lib/core/refcount.h:20,
                 from ./tensorflow/core/platform/tensor_coding.h:21,
                 from ./tensorflow/core/framework/resource_handle.h:19,
                 from ./tensorflow/core/framework/allocator.h:24,
                 from ./tensorflow/core/framework/tensor.h:20,
                 from ./tensorflow/python/lib/core/py_func.h:22,
                 from tensorflow/python/lib/core/py_func.cc:16:
./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetTensorDim(tensorflow::gtl::ArraySlice<T>, tensorflow::TensorFormat, char) [with T = long long int]':
./tensorflow/core/util/tensor_format.h:452:47:   required from here
./tensorflow/core/util/tensor_format.h:420:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   CHECK(index >= 0 && index < dimension_attributes.size())
./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
./tensorflow/core/util/tensor_format.h:420:3: note: in expansion of macro 'CHECK'
   CHECK(index >= 0 && index < dimension_attributes.size())
   ^
./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetFilterDim(tensorflow::gtl::ArraySlice<T>, tensorflow::FilterTensorFormat, char) [with T = long long int]':
./tensorflow/core/util/tensor_format.h:461:54:   required from here
./tensorflow/core/util/tensor_format.h:435:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   CHECK(index >= 0 && index < dimension_attribute.size())
./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
./tensorflow/core/util/tensor_format.h:435:3: note: in expansion of macro 'CHECK'
   CHECK(index >= 0 && index < dimension_attribute.size())
   ^
./tensorflow/core/platform/default/logging.h: In instantiation of 'std::__cxx11::string* tensorflow::internal::Check_EQImpl(const T1&, const T2&, const char*) [with T1 = long unsigned int; T2 = long long int; std::__cxx11::string = std::__cxx11::basic_string<char>]':
tensorflow/python/lib/core/py_func.cc:323:5:   required from here
./tensorflow/core/platform/default/logging.h:230:25: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
                         ==)  // Compilation error with CHECK_EQ(NULL, x)?
./tensorflow/core/platform/macros.h:88:49: note: in definition of macro 'TF_PREDICT_TRUE'
 #define TF_PREDICT_TRUE(x) (__builtin_expect(!!(x), 1))
                                                 ^
./tensorflow/core/platform/default/logging.h:229:1: note: in expansion of macro 'TF_DEFINE_CHECK_OP_IMPL'
 TF_DEFINE_CHECK_OP_IMPL(Check_EQ,
 ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 6.180s, Critical Path: 5.87s
INFO: 0 processes.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
```
"
21972,Is it possible to use the autograph-compiled code on older Tensorflow versions?,"### Describe the problem
I have tried autograph on the nightly channel. It works and is very useful to me. I could not hope to write such code myself. However, it is not a very good idea to stay on the nightly channel for the whole time. Since autograph can generate the code, it seems natural to me to use such code and migrate back to a more stable channel. 

However, the generated code doesn't seem to be readily usable. It contains unrecognized tokens like `__ag`. I cannot get past that. Is it even possible? Can you disclose some of the plans on that?"
21971,"InvalidArgumentError: Input to reshape is a tensor with 1 values, but the requested shape has 0","### System information
- Custom code:
- OS Platform and Distribution: Windows 7 x64
- TensorFlow installed from: Anaconda
- Bazel version: N/A
- Tensorflow version 1.8.0:
- python 3.6.0 :
- Mobile device: N/A
- Exact command to reproduce: N/A
- GPU model and memory: NVIDIA QUADRO K1100M
- CUDA/cuDNN: 9.0.176/7.0

### The Problem

I strongly believe there's a problem with the gradient computation. 


``````python
import tensorflow as tf
import numpy as np

l2_u = 0.
l2_i = 0.
init_std = 0.1
np_users = np.int64(100)
np_items = np.int64(100)
n_factors = np.int64(10)
lr = 0.1

train_x = np.stack([np.random.randint(np_users, size=1000), np.random.randint(np_items, size=1000)], axis=1)
train_y = np.random.randint(2, size=1000)

def define_graph():
    tf.reset_default_graph()
    n_users = tf.placeholder(dtype=tf.int32, shape=[], name='n_users')
    n_items = tf.placeholder(dtype=tf.int32, shape=[], name='n_items')
    x = tf.placeholder(shape=[None, 2], dtype=tf.int32, name=""x"")
    y = tf.placeholder(shape=[None], dtype=tf.float32, name=""y"")
    w = tf.placeholder(tf.float32, shape=[None], name='sample_weights')
    reg_u = tf.constant(l2_u, dtype=tf.float32, name='lambda_u')
    reg_i = tf.constant(l2_i, dtype=tf.float32, name='lambda_i')

    u_shape = tf.stack([n_users, n_factors])
    u_rnd = tf.random_normal(shape=u_shape, dtype=tf.float32, mean=0, stddev=init_std)
    U = tf.verify_tensor_all_finite(
                tf.Variable(u_rnd,
                            trainable=True,
                            name=""users"",
                            validate_shape=False), msg='NaN or Inf in parameters')

    i_shape = tf.stack([n_items, n_factors])
    i_rnd = tf.random_normal(shape=i_shape,
                                dtype=tf.float32,
                                mean=0,
                                stddev=init_std)
    I = tf.verify_tensor_all_finite(
                tf.Variable(i_rnd,
                            trainable=True,
                            name=""items"",
                            validate_shape=False), msg='NaN or Inf in parameters')

    ub_shape = tf.expand_dims(n_users, axis=0)
    ub_rnd = tf.random_normal(dtype=tf.float32,
                                 shape=ub_shape,
                                 mean=0,
                                 stddev=init_std)
    u_bias = tf.verify_tensor_all_finite(
                    tf.Variable(ub_rnd,
                                trainable=True,
                                name='u_bias',
                                validate_shape=False), msg='NaN or Inf in parameters')

    ib_shape = tf.expand_dims(n_items, axis=0)
    ib_rnd = tf.random_normal(dtype=tf.float32,
                                 shape=ib_shape,
                                 mean=0,
                                 stddev=init_std)
    i_bias = tf.verify_tensor_all_finite(
                    tf.Variable(ib_rnd,
                                trainable=True,
                                name='i_bias',
                                validate_shape=False), msg='NaN or Inf in parameters')
    g_bias = tf.verify_tensor_all_finite(
                    tf.Variable(0., 
                                dtype=tf.float32,
                                trainable=True, 
                                name='g_bias'), msg='NaN or Inf in parameters')
    users = tf.nn.embedding_lookup(U, x[:, 0])
    items = tf.nn.embedding_lookup(I, x[:, 1])
    
    ub = tf.expand_dims(tf.nn.embedding_lookup(u_bias, x[:, 0]), 1)
    ib = tf.expand_dims(tf.nn.embedding_lookup(i_bias, x[:, 1]), 1)
    y_hat = tf.reduce_sum(users * items, axis=1, keepdims=True) + ub + ib + g_bias
    
    loss = tf.square(tf.expand_dims(y, 1) - tf.sigmoid(y_hat))
    reduced_loss = tf.reduce_mean(loss, keepdims=True)
    reg_u = l2_u * tf.nn.l2_loss(U)
    reg_i = l2_i * tf.nn.l2_loss(I)
    reg = reg_i + reg_u
    target = reg + reduced_loss
    checked_target = tf.verify_tensor_all_finite(target, 
                                                 msg='NaN or Inf in target value',
                                                 name='target')
    trainer = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(checked_target)
    return trainer, n_users, n_items, x, y

with tf.device('/job:localhost/replica:0/task:0/device:CPU:0'):
    op, n_users, n_items, x, y = define_graph()

init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init, feed_dict={n_users: np_users, n_items: np_items})

sess.run(op, feed_dict={x: train_x, y: train_y})
``````
Error logs:

``````bash
InvalidArgumentError: Input to reshape is a tensor with 1 values, but the requested shape has 0
	 [[Node: gradients/Mean_grad/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](gradients/add_4_grad/Reshape_1, gradients/Mean_grad/DynamicStitch/_47, ^gradients/add_4_grad/tuple/group_deps)]]

Caused by op 'gradients/Mean_grad/Reshape', defined at:
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\site-packages\ipykernel\__main__.py"", line 3, in <module>
    app.launch_new_instance()
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\site-packages\traitlets\config\application.py"", line 658, in launch_instance
    app.start()
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\site-packages\ipykernel\kernelapp.py"", line 486, in start
    self.io_loop.start()
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\site-packages\tornado\platform\asyncio.py"", line 127, in start
    self.asyncio_loop.run_forever()
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\asyncio\base_events.py"", line 422, in run_forever
    self._run_once()
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\asyncio\base_events.py"", line 1434, in _run_once
    handle._run()
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\asyncio\events.py"", line 145, in _run
    self._callback(*self._args)
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\site-packages\tornado\platform\asyncio.py"", line 117, in _handle_events
    handler_func(fileobj, events)
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\site-packages\tornado\stack_context.py"", line 276, in null_wrapper
    return fn(*args, **kwargs)
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\site-packages\zmq\eventloop\zmqstream.py"", line 450, in _handle_events
    self._handle_recv()
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\site-packages\zmq\eventloop\zmqstream.py"", line 480, in _handle_recv
    self._run_callback(callback, msg)
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\site-packages\zmq\eventloop\zmqstream.py"", line 432, in _run_callback
    callback(*args, **kwargs)
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\site-packages\tornado\stack_context.py"", line 276, in null_wrapper
    return fn(*args, **kwargs)
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\site-packages\ipykernel\kernelbase.py"", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\site-packages\ipykernel\kernelbase.py"", line 233, in dispatch_shell
    handler(stream, idents, msg)
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\site-packages\ipykernel\kernelbase.py"", line 399, in execute_request
    user_expressions, allow_stdin)
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\site-packages\ipykernel\ipkernel.py"", line 208, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\site-packages\ipykernel\zmqshell.py"", line 537, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\site-packages\IPython\core\interactiveshell.py"", line 2662, in run_cell
    raw_cell, store_history, silent, shell_futures)
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\site-packages\IPython\core\interactiveshell.py"", line 2785, in _run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\site-packages\IPython\core\interactiveshell.py"", line 2903, in run_ast_nodes
    if self.run_code(code, result):
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\site-packages\IPython\core\interactiveshell.py"", line 2963, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-210-ae7fb3e220df>"", line 2, in <module>
    op, n_users, n_items, x, y = define_graph()
  File ""<ipython-input-209-8e2c9c7479df>"", line 72, in define_graph
    trainer = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(checked_target)
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\training\optimizer.py"", line 414, in minimize
    grad_loss=grad_loss)
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\training\optimizer.py"", line 526, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\ops\gradients_impl.py"", line 494, in gradients
    gate_gradients, aggregation_method, stop_gradients)
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\ops\gradients_impl.py"", line 636, in _GradientsHelper
    lambda: grad_fn(op, *out_grads))
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\ops\gradients_impl.py"", line 385, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\ops\gradients_impl.py"", line 636, in <lambda>
    lambda: grad_fn(op, *out_grads))
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\ops\math_grad.py"", line 119, in _MeanGrad
    sum_grad = _SumGrad(op, grad)[0]
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\ops\math_grad.py"", line 83, in _SumGrad
    grad = array_ops.reshape(grad, output_shape_kept_dims)
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\ops\gen_array_ops.py"", line 6112, in reshape
    ""Reshape"", tensor=tensor, shape=shape, name=name)
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\framework\ops.py"", line 3392, in create_op
    op_def=op_def)
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\framework\ops.py"", line 1718, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

...which was originally created as op 'Mean', defined at:
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
[elided 23 identical lines from previous traceback]
  File ""<ipython-input-210-ae7fb3e220df>"", line 2, in <module>
    op, n_users, n_items, x, y = define_graph()
  File ""<ipython-input-209-8e2c9c7479df>"", line 64, in define_graph
    reduced_loss = tf.reduce_mean(loss, keepdims=True)
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\util\deprecation.py"", line 432, in new_func
    return func(*args, **kwargs)
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\ops\math_ops.py"", line 1562, in reduce_mean
    name=name))
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\ops\gen_math_ops.py"", line 4495, in mean
    name=name)
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\framework\ops.py"", line 3392, in create_op
    op_def=op_def)
  File ""C:\Users\Daniel\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\framework\ops.py"", line 1718, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): Input to reshape is a tensor with 1 values, but the requested shape has 0
	 [[Node: gradients/Mean_grad/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](gradients/add_4_grad/Reshape_1, gradients/Mean_grad/DynamicStitch/_47, ^gradients/add_4_grad/tuple/group_deps)]]
``````"
21969,Python wheel doesn't work on some Intel Xeon CPU,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux simi 4.4.0-131-generic #157-Ubuntu SMP Thu Jul 12 15:51:36 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
n/a
- **TensorFlow installed from (source or binary)**:
binary (pip install)
- **TensorFlow version (use command below)**:
I cannot run the suggested command (more bellow) but the version installed by pip is tensorflow-1.10.1.
- **Python version**:
3.5.2
- **Bazel version (if compiling from source)**:
n/a
- **GCC/Compiler version (if compiling from source)**:
n/a
- **CUDA/cuDNN version**:
n/a
- **GPU model and memory**:
n/a
- **Exact command to reproduce**:
```
python -c 'import tensorflow'
```

### Describe the problem

I cannot even import the tensorflow module:
```
$ ./tfvenv/bin/python -c 'import tensorflow'
Illegal instruction   
```
Digging through the system log, I found this line that seems to give more details on the problem:
```
aug 30 11:07:27 simi kernel: traps: python[6186] trap invalid opcode ip:7f989e396250 sp:7fff8959df80 error:0 in libtensorflow_framework.so[7f989df07000+c88000]
```

After some googling, my understanding is that the tf build uses instruction that are not available on my CPU.  Since it is a regular x86_64 architecture, I assume it is a bug but maybe not? Are there particular CPU requirements for tensorflow that I didn't manage to find?

### Source code / logs
I'm pasting the output of `lshw -class cpu` from the machine where the issue occurs:
```
  *-cpu:0
       description: CPU
       product: Intel(R) Xeon(R) CPU           E5430  @ 2.66GHz
       vendor: Intel Corp.
       physical id: 400
       bus info: cpu@0
       version: Intel(R) Xeon(R) CPU           E5430  @ 2.66GHz
       slot: CPU1
       size: 2667MHz
       capacity: 3600MHz
       width: 64 bits
       clock: 1333MHz
       capabilities: x86-64 fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx constant_tsc arch_perfmon pebs bts rep_good nopl aperfmperf eagerfpu pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm dca sse4_1 xsave lahf_lm kaiser tpr_shadow vnmi flexpriority dtherm
       configuration: cores=4 enabledcores=4 threads=4
  *-cpu:1
       description: CPU
       product: Intel(R) Xeon(R) CPU           E5430  @ 2.66GHz
       vendor: Intel Corp.
       physical id: 401
       bus info: cpu@1
       version: Intel(R) Xeon(R) CPU           E5430  @ 2.66GHz
       slot: CPU2
       size: 2667MHz
       capacity: 3600MHz
       width: 64 bits
       clock: 1333MHz
       capabilities: x86-64 fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx constant_tsc arch_perfmon pebs bts rep_good nopl aperfmperf eagerfpu pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm dca sse4_1 xsave lahf_lm kaiser tpr_shadow vnmi flexpriority dtherm
       configuration: cores=4 enabledcores=4 threads=4
```"
21968,AssertionError: tensorflow mulitiple GPU train with MirroredStrategy,"I want to train with multiple gpu with tensorflow-1.10. I use the estimator and MirroredStrategy. The model is resnet50 which is official model of tensorflow . The following is my code and error log. Am I use MirroredStrategy in the wrong way?
```
import tensorflow as tf
import numpy as np
from tensorflow.contrib.slim.nets import resnet_v2
import tensorflow.contrib.slim as slim

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(path='mnist.npz')
x_train = np.expand_dims(x_train, 3).astype(np.float32)
y_train = y_train.astype(np.int32)
x_test = np.expand_dims(x_test, 3).astype(np.float32)
y_test = y_test.astype(np.int32)

tf.logging.set_verbosity(tf.logging.INFO)

cls_num = 10


def model_fn(features, labels, mode):
    is_training = (mode == tf.estimator.ModeKeys.TRAIN)
    with slim.arg_scope(resnet_v2.resnet_arg_scope()):
        logits, endpoints = resnet_v2.resnet_v2_50(features, 
                num_classes=cls_num,
                is_training=is_training)

    logits = tf.squeeze(logits, [1, 2])
    preds = tf.argmax(logits, 1)
    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)
    optimizer = tf.train.AdagradOptimizer(learning_rate=0.01)
    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
    with tf.control_dependencies(update_ops):
        train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())

    accuracy = tf.metrics.accuracy(labels=labels, predictions=preds)
    metrics = {'accuracy': accuracy}

    if mode == tf.estimator.ModeKeys.EVAL:
        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)

    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op) 


def train_input_fn():
    dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
    dataset = dataset.repeat(1).batch(128)
    return dataset

def eval_input_fn():
    dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))
    dataset = dataset.repeat(1).batch(128)
    return dataset


distribution = tf.contrib.distribute.MirroredStrategy()
config = tf.estimator.RunConfig(train_distribute=distribution)
estimator = tf.estimator.Estimator(model_fn=model_fn, model_dir='logs', config=config)
train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn)
eval_specs = tf.estimator.EvalSpec(input_fn=eval_input_fn)
for _ in xrange(100):
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs)
```

This is the error log:
```
2018-08-30 16:58:47.446417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 1 
2018-08-30 16:58:47.446428: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N Y 
2018-08-30 16:58:47.446437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 1:   Y N 
2018-08-30 16:58:47.446877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/device:GPU:0 with 21544 MB memory) -> physical GPU (device: 0, name: Tesla P40, pci bus id: 0000:83:00.0, compute capability: 6.1)
2018-08-30 16:58:47.447098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/device:GPU:1 with 21544 MB memory) -> physical GPU (device: 1, name: Tesla P40, pci bus id: 0000:84:00.0, compute capability: 6.1)
INFO:tensorflow:Device is available but not used by distribute strategy: /device:CPU:0
INFO:tensorflow:Configured nccl all-reduce.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Error reported to Coordinator: 
Traceback (most recent call last):
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception
    yield
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 519, in run
    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1133, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""test_mnist.py"", line 22, in model_fn
    is_training=is_training)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/contrib/slim/python/slim/nets/resnet_v2.py"", line 287, in resnet_v2_50
    scope=scope)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/contrib/slim/python/slim/nets/resnet_v2.py"", line 216, in resnet_v2
    net = resnet_utils.stack_blocks_dense(net, blocks, output_stride)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 183, in func_with_args
    return func(*args, **current_args)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/contrib/slim/python/slim/nets/resnet_utils.py"", line 215, in stack_blocks_dense
    net = block.unit_fn(net, rate=1, **unit)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 183, in func_with_args
    return func(*args, **current_args)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/contrib/slim/python/slim/nets/resnet_v2.py"", line 101, in bottleneck
    inputs, activation_fn=nn_ops.relu, scope='preact')
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 183, in func_with_args
    return func(*args, **current_args)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 650, in batch_norm
    outputs = layer.apply(inputs, training=is_training)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 805, in apply
    return self.__call__(inputs, *args, **kwargs)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/python/layers/base.py"", line 362, in __call__
    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 736, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/python/layers/normalization.py"", line 158, in call
    return super(BatchNormalization, self).call(inputs, training=training)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/python/keras/layers/normalization.py"", line 514, in call
    outputs = self._fused_batch_norm(inputs, training=training)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/python/keras/layers/normalization.py"", line 420, in _fused_batch_norm
    momentum)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/python/keras/layers/normalization.py"", line 369, in _assign_moving_average
    with ops.colocate_with(variable):
  File ""/home/soft/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3939, in _colocate_with_for_gradient
    with self.colocate_with(op, ignore_existing):
  File ""/home/soft/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3992, in colocate_with
    op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1255, in internal_convert_to_tensor_or_indexed_slices
    value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1094, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/values.py"", line 414, in _tensor_conversion_mirrored
    assert not as_ref
AssertionError
Traceback (most recent call last):
  File ""test_mnist.py"", line 58, in <module>
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 451, in train_and_evaluate
    return executor.run()
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 590, in run
    return self.run_local()
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 691, in run_local
    saving_listeners=saving_listeners)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 376, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1143, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1255, in _train_model_distributed
    self.config)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/python/training/distribute.py"", line 777, in call_for_each_tower
    return self._call_for_each_tower(fn, *args, **kwargs)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 308, in _call_for_each_tower
    coord.join(threads)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py"", line 389, in join
    six.reraise(*self._exc_info_to_raise)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception
    yield
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 519, in run
    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1133, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""test_mnist.py"", line 22, in model_fn
    is_training=is_training)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/contrib/slim/python/slim/nets/resnet_v2.py"", line 287, in resnet_v2_50
    scope=scope)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/contrib/slim/python/slim/nets/resnet_v2.py"", line 216, in resnet_v2
    net = resnet_utils.stack_blocks_dense(net, blocks, output_stride)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 183, in func_with_args
    return func(*args, **current_args)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/contrib/slim/python/slim/nets/resnet_utils.py"", line 215, in stack_blocks_dense
    net = block.unit_fn(net, rate=1, **unit)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 183, in func_with_args
    return func(*args, **current_args)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/contrib/slim/python/slim/nets/resnet_v2.py"", line 101, in bottleneck
    inputs, activation_fn=nn_ops.relu, scope='preact')
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 183, in func_with_args
    return func(*args, **current_args)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 650, in batch_norm
    outputs = layer.apply(inputs, training=is_training)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 805, in apply
    return self.__call__(inputs, *args, **kwargs)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/python/layers/base.py"", line 362, in __call__
    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 736, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/python/layers/normalization.py"", line 158, in call
    return super(BatchNormalization, self).call(inputs, training=training)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/python/keras/layers/normalization.py"", line 514, in call
    outputs = self._fused_batch_norm(inputs, training=training)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/python/keras/layers/normalization.py"", line 420, in _fused_batch_norm
    momentum)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/python/keras/layers/normalization.py"", line 369, in _assign_moving_average
    with ops.colocate_with(variable):
  File ""/home/soft/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3939, in _colocate_with_for_gradient
    with self.colocate_with(op, ignore_existing):
  File ""/home/soft/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3992, in colocate_with
    op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1255, in internal_convert_to_tensor_or_indexed_slices
    value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1094, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/home/soft/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/values.py"", line 414, in _tensor_conversion_mirrored
    assert not as_ref
AssertionError
```"
21967,ImportError: DLL load failed: The specified procedure could not be found.,"Hello, I have tried many times to install tensorflow 1.5 on windows 10, but I encounter the same error each time : 
`python
Python 3.6.0 (v3.6.0:41df79263a11, Dec 23 2016, 08:06:12) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\jules\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\jules\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 52, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""C:\Users\jules\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\core\framework\graph_pb2.py"", line 6, in <module>
    from google.protobuf import descriptor as _descriptor
  File ""C:\Users\jules\AppData\Local\Programs\Python\Python36\lib\site-packages\google\protobuf\descriptor.py"", line 47, in <module>
    from google.protobuf.pyext import _message
ImportError: DLL load failed: The specified procedure could not be found.`

any help would be greatly appreciated"
21965,"cannot find  ""flatbuffers/flatbuffers.h""  head  file","in tensorflow/contrib/lite/schema/schema_generated.h file ::
```
#ifndef FLATBUFFERS_GENERATED_SCHEMA_TFLITE_H_
#define FLATBUFFERS_GENERATED_SCHEMA_TFLITE_H_

#include ""flatbuffers/flatbuffers.h""
#include <string>
namespace tflite {}
```

I want to ask where is the ""  flatbuffers/flatbuffers.h ""  file .
I cannot find the  head   file in  tensorflow directory . 
 Thanks"
21964,tf.sparse_concat does not infer result shape from inputs with fully defined shape,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
NA
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.10
- **Python version**:
3.6.5
- **Bazel version (if compiling from source)**:
NA
- **GCC/Compiler version (if compiling from source)**:
NA
- **CUDA/cuDNN version**:
7.0
- **GPU model and memory**:
Yes
- **Exact command to reproduce**:
```python
In [9]: import tensorflow as tf

In [10]: x = tf.SparseTensor(indices=[[0,0],[1,1]], values=[1,2], dense_shape=[2,2])

In [11]: y = tf.SparseTensor(indices=[[0,0],[1,1]], values=[1,2], dense_shape=[2,2])

In [12]: x.get_shape()
Out[12]: TensorShape([Dimension(2), Dimension(2)])

In [13]: y.get_shape()
Out[13]: TensorShape([Dimension(2), Dimension(2)])

In [14]: z = tf.sparse_concat(-1, [x,y])

In [15]: z.get_shape()
Out[15]: TensorShape([Dimension(None), Dimension(None)])
```

The shape of z should be infered if shape of x and y are fully defined."
21963,Building Windows Tensorflow C++ Debug version failed,"**System information:**

- Windows 10 Home
- TensorFlow version: r1.4
- Visual studio version: vs2015 Community x64
- CUDA version: 8.0
- CUDNN version: 6.0
- GPU: NVIDIA GeForce 1080ti
- Cmake version: 3.9.0
- Swig version: 3.0.10

**Problem Description:**

- I followed some online guide to build tensorflow, firstly I used ALL_BUILD, but had compiler is out of heap space problem;
- I set compiler to x64 and build tf_core_kernels -> tensroflow_static->tensorflow project only, it said cannot open file ""Debug\tf_core_gpu_kernels.lib"";
- I built tf_core_gpu_kernels only, the error message is: Severity	Code Description Project File Line Suppression State Error calling a __host__ function(""std::_Debug_message"") from a __device__ function(""std::_Debug_lt<const int &, const int &> "") is not allowed	tf_core_gpu_kernels	D:\Programs\Microsoft Visual Studio 14.0\VC\INCLUDE\xutility	911

May I know how should I deal with this problem? Or anybody has successfully built tensorflow c++ windows debug version? If yes, may I know your configurations?

Thank you very much!
"
21962,how to do speech test?,"Hi Tech Support:

There is speech_test.cc in tensorflow/tensorflow/contrib/lite/models. After I modified BUILD file(attached here), I can have bazel-bin/tensorflow/contrib/lite/models/speech_test to run and also I have downloaded the needed model file  Models:Speech hotword model (Svdf rank=1), Speech hotword model (Svdf rank=2)
Speaker-id model, TTS model, ASR AM modelaccording to  
 the instrucitons from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/models/testdata/g3doc/README.md
and also I have created  speech_hotword_model_in.csv and another needed .csv. 

When I run ""bazel-bin/tensorflow/contrib/lite/models/speech_test"", I have errors like:

[ RUN      ] LongTests/SpeechTest.HotwordOkGoogleRank1Test/0
[       OK ] LongTests/SpeechTest.HotwordOkGoogleRank1Test/0 (0 ms)
[ RUN      ] LongTests/SpeechTest.HotwordOkGoogleRank2Test/0
[       OK ] LongTests/SpeechTest.HotwordOkGoogleRank2Test/0 (1 ms)
[ RUN      ] LongTests/SpeechTest.SpeakerIdOkGoogleTest/0
tensorflow/contrib/lite/kernels/lstm.cc:262 node->outputs->size != 3 (4 != 3)
Failed to allocate tensors
tensorflow/contrib/lite/models/speech_test.cc:138: Failure
Value of: testing::ParseAndRunTests(&os, &test_driver, GetMaxInvocations())
  Actual: false
Expected: true
Failed to allocate tensors
[  FAILED  ] LongTests/SpeechTest.SpeakerIdOkGoogleTest/0, where GetParam() = -1 (0 ms)
[ RUN      ] LongTests/SpeechTest.AsrAmTest/0
tensorflow/contrib/lite/kernels/lstm.cc:262 node->outputs->size != 3 (4 != 3)
Failed to allocate tensors
tensorflow/contrib/lite/models/speech_test.cc:151: Failure
Value of: testing::ParseAndRunTests(&os, &test_driver, GetMaxInvocations())
  Actual: false
Expected: true
Failed to allocate tensors
[  FAILED  ] LongTests/SpeechTest.AsrAmTest/0, where GetParam() = -1 (1 ms)
[ RUN      ] LongTests/SpeechTest.AsrLmTest/0
tensorflow/contrib/lite/models/speech_test.cc:162: Failure
Value of: Init(""speech_asr_lm_model.test_spec"", &test_driver, &in_file)
  Actual: false
Expected: true

I don't know where I can get correct speech_hotword_model_in.csv so I just created one including speech_asr_am_model_in.csv.

I am not sure if it is a bug or I don't have the correct model_in.csv file. I cannot find the solutions from website so I have to ask the help from here.

Or could you tell me the correct way to do speech test under ubuntu 16.04 please? Thank you very much in advance.

csv files needed are 
speech_speakerid_model_in.csv, speech_asr_am_model_in.csv
speech_endpointer_model_in.csv, speech_tts_model_in.csv
, speech_hotword_model_in.csv


Thanks and best regards
Frank


Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: modified some code and please see attached speech_test_changes.zip
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:No, try to run in desktop ubuntu machine.
- **TensorFlow installed from (source or binary)**:build from source
- **TensorFlow version (use command below)**:1.8
- **Python version**:3.5
- **Bazel version (if compiling from source)**:0.13.0
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:9.1 see atthced tf.env file
- **GPU model and memory**:GTX 1080Ti
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh
[speech_test_changes.zip](https://github.com/tensorflow/tensorflow/files/2334177/speech_test_changes.zip)



You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem


Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
[tf_env.txt](https://github.com/tensorflow/tensorflow/files/2334116/tf_env.txt)
"
21961,ppc64le: //tensorflow/python/kernel_tests:depthwise_conv_op_test core dumps,"Please assign this issue to me

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ppc64le Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: master branch from 8/29 (065f9b833ffbb3b2f03d63febb186275674ba133)
- **Python version**: 3,6
- **Bazel version (if compiling from source)**: 0.15.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 9.0, 7
- **GPU model and memory**: 4 V100 GPUs with 16 GB of memory each
- **Exact command to reproduce**:
bazel test --config=cuda --test_tag_filters=-no_oss,-oss_serial,-no_gpu,-benchmark-test --test_timeout 300,450,1200,3600 --local_test_jobs=4 --test_output=errors --build_tests_only //tensorflow/tools/ci_build/gpu_build:parallel_gpu_execute //tensorflow/python/kernel_tests:depthwise_conv_op_test

### Describe the problem
Running unit test of gpu code with python3, depthwise_conv_op_test core dumps 

### Source code / logs
```
==================== Test output for //tensorflow/python/kernel_tests:depthwise_conv_op_test:
Running test /home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/depthwise_conv_op_test  on GPU 0
2018-08-29 21:27:33.969227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties:
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0004:04:00.0
totalMemory: 15.75GiB freeMemory: 15.34GiB
2018-08-29 21:27:33.969304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0
2018-08-29 21:31:13.982520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-29 21:31:13.982546: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0
2018-08-29 21:31:13.982556: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N
2018-08-29 21:31:13.983032: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4838 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0004:04:00.0, compute capability: 7.0)
.2018-08-29 21:31:14.042699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0
2018-08-29 21:31:14.042728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-29 21:31:14.042738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0
2018-08-29 21:31:14.042746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N
2018-08-29 21:31:14.043084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4838 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0004:04:00.0, compute capability: 7.0)
2018-08-29 21:31:14.087484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0
2018-08-29 21:31:14.087516: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-29 21:31:14.087524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0
2018-08-29 21:31:14.087531: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N
2018-08-29 21:31:14.087867: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4838 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0004:04:00.0, compute capability: 7.0)
2018-08-29 21:31:21.854127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0
2018-08-29 21:31:21.854182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-29 21:31:21.854189: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0
2018-08-29 21:31:21.854197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N
2018-08-29 21:31:21.854553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4838 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0004:04:00.0, compute capability: 7.0)
2018-08-29 21:31:21.898468: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0
2018-08-29 21:31:21.898491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-29 21:31:21.898498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0
2018-08-29 21:31:21.898511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N
2018-08-29 21:31:21.898847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4838 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0004:04:00.0, compute capability: 7.0)
2018-08-29 21:31:21.955406: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0
2018-08-29 21:31:21.955430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-29 21:31:21.955437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0
2018-08-29 21:31:21.955450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N
2018-08-29 21:31:21.955770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4838 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0004:04:00.0, compute capability: 7.0)
2018-08-29 21:31:21.999863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0
2018-08-29 21:31:21.999884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-29 21:31:21.999892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0
2018-08-29 21:31:21.999900: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N
2018-08-29 21:31:22.000215: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4838 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0004:04:00.0, compute capability: 7.0)
2018-08-29 21:31:22.057828: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0
2018-08-29 21:31:22.057853: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-29 21:31:22.057861: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0
2018-08-29 21:31:22.057869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N
2018-08-29 21:31:22.058188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4838 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0004:04:00.0, compute capability: 7.0)
2018-08-29 21:31:22.101895: F ./tensorflow/core/util/cuda_launch_config.h:186] Check failed: err == cudaSuccess (8 vs. 0)
*** Received signal 6 ***
*** BEGIN MANGLED STACK TRACE ***
/home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so(+0x76ec9c)[0x7fff8aa4ec9c]
[0x7fff9e0d04d8]
/lib/powerpc64le-linux-gnu/libc.so.6(gsignal+0x40)[0x7fff9deeedb0]
/lib/powerpc64le-linux-gnu/libc.so.6(abort+0x2b4)[0x7fff9def1314]
/home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow8internal15LogMessageFatalD1Ev+0x3c)[0x7fff90a6b1dc]
/home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow29LaunchDepthwiseConv2dGPUSmallIN5Eigen4halfELNS_24DepthwiseConv2dDirectionE0ELin1ELin1ELi8ELb1ES2_EENS_6StatusEPNS_15OpKernelContextERKNS_13DepthwiseArgsEPKT_SC_PSA_NS_12TensorFormatE+0x458)[0x7fff8eb22478]
/home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow29LaunchDepthwiseConv2dGPUSmallIN5Eigen4halfELNS_24DepthwiseConv2dDirectionE0ELin1ELin1EEENS_6StatusEPNS_15OpKernelContextERKNS_13DepthwiseArgsEPKT_SC_PSA_NS_12TensorFormatE+0x1e4)[0x7fff8eb2bf14]
/home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow21LaunchDepthwiseConvOpIN5Eigen9GpuDeviceENS1_4halfEEclEPNS_15OpKernelContextERKNS_13DepthwiseArgsEPKS3_SB_PS3_NS_12TensorFormatE+0x1e8)[0x7fff8eb30df8]
/home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow23DepthwiseConv2dNativeOpIN5Eigen9GpuDeviceENS1_4halfEE7ComputeEPNS_15OpKernelContextE+0x6f8)[0x7fff8eae47d8]
/home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so(_ZN10tensorflow13BaseGPUDevice13ComputeHelperEPNS_8OpKernelEPNS_15OpKernelContextE+0x3ec)[0x7fff8a9546cc]
/home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so(_ZN10tensorflow13BaseGPUDevice7ComputeEPNS_8OpKernelEPNS_15OpKernelContextE+0xd8)[0x7fff8a954d98]
/home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so(+0x6ca9f8)[0x7fff8a9aa9f8]
/home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so(_ZN5Eigen26NonBlockingThreadPoolTemplIN10tensorflow6thread16EigenEnvironmentEE10WorkerLoopEi+0x350)[0x7fff8aa1a810]
/home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so(_ZNSt17_Function_handlerIFvvEZN5Eigen26NonBlockingThreadPoolTemplIN10tensorflow6thread16EigenEnvironmentEEC4EibS5_EUlvE_E9_M_invokeERKSt9_Any_data+0x28)[0x7fff8aa1b168]
/home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so(_ZNSt17_Function_handlerIFvvEZN10tensorflow6thread16EigenEnvironment12CreateThreadESt8functionIS0_EEUlvE_E9_M_invokeERKSt9_Any_data+0x74)[0x7fff8aa174d4]
/home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so(_ZNSt6thread5_ImplISt12_Bind_simpleIFSt8functionIFvvEEvEEE6_M_runEv+0x40)[0x7fff8aa50170]
/usr/lib/powerpc64le-linux-gnu/libstdc++.so.6(+0xe49a4)[0x7fff975549a4]
/lib/powerpc64le-linux-gnu/libpthread.so.0(+0x8070)[0x7fff9e088070]
/lib/powerpc64le-linux-gnu/libc.so.6(clone+0x98)[0x7fff9dfd3a70]
*** END MANGLED STACK TRACE ***

*** Begin stack trace ***
        tensorflow::CurrentStackTrace[abi:cxx11]()

        __kernel_sigtramp_rt64
        gsignal
        abort
        tensorflow::internal::LogMessageFatal::~LogMessageFatal()
        tensorflow::Status tensorflow::LaunchDepthwiseConv2dGPUSmall<Eigen::half, (tensorflow::DepthwiseConv2dDirection)0, -1, -1, 8, true, Eigen::half>(tensorflow::OpKernelContext*, tensorflow::DepthwiseArgs const&, Eigen::half const*, Eigen::half const*, Eigen::half*, tensorflow::TensorFormat)
        tensorflow::Status tensorflow::LaunchDepthwiseConv2dGPUSmall<Eigen::half, (tensorflow::DepthwiseConv2dDirection)0, -1, -1>(tensorflow::OpKernelContext*, tensorflow::DepthwiseArgs const&, Eigen::half const*, Eigen::half const*, Eigen::half*, tensorflow::TensorFormat)
        tensorflow::LaunchDepthwiseConvOp<Eigen::GpuDevice, Eigen::half>::operator()(tensorflow::OpKernelContext*, tensorflow::DepthwiseArgs const&, Eigen::half const*, Eigen::half const*, Eigen::half*, tensorflow::TensorFormat)
        tensorflow::DepthwiseConv2dNativeOp<Eigen::GpuDevice, Eigen::half>::Compute(tensorflow::OpKernelContext*)
        tensorflow::BaseGPUDevice::ComputeHelper(tensorflow::OpKernel*, tensorflow::OpKernelContext*)
        tensorflow::BaseGPUDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*)

        Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int)
        std::_Function_handler<void (), Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::NonBlockingThreadPoolTempl(int, bool, tensorflow::thread::EigenEnvironment)::{lambda()#1}>::_M_invoke(std::_Any_data const&)
        std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)
        std::thread::_Impl<std::_Bind_simple<std::function<void ()> ()> >::_M_run()


        clone
*** End stack trace ***
/home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/tools/ci_build/gpu_build/parallel_gpu_execute: line 64: 57550 Aborted                 (core dumped) ""$TEST_BINARY"" $@
================================================================================
```

gdb where of the core dump:

```
[Thread debugging using libthread_db enabled]
Using host libthread_db library ""/lib/powerpc64le-linux-gnu/libthread_db.so.1"".
Core was generated by `/usr/bin/python3 /home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/'.
Program terminated with signal SIGABRT, Aborted.
#0  0x00007fff9deeedb0 in __GI_raise (sig=<optimized out>) at ../sysdeps/unix/sysv/linux/raise.c:54
54      ../sysdeps/unix/sysv/linux/raise.c: No such file or directory.
[Current thread is 1 (Thread 0x7ffd517ff1a0 (LWP 58088))]
(gdb) where
#0  0x00007fff9deeedb0 in __GI_raise (sig=<optimized out>) at ../sysdeps/unix/sysv/linux/raise.c:54
#1  0x00007fff9def1270 in __GI_abort () at abort.c:89
#2  0x00007fff8aa4ed18 in tensorflow::testing::StacktraceHandler(int, siginfo_t*, void*) ()
   from /home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so
#3  <signal handler called>
#4  0x00007fff9deeedb0 in __GI_raise (sig=<optimized out>) at ../sysdeps/unix/sysv/linux/raise.c:54
#5  0x00007fff9def1270 in __GI_abort () at abort.c:89
#6  0x00007fff90a6b1dc in tensorflow::internal::LogMessageFatal::~LogMessageFatal() ()
   from /home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so
#7  0x00007fff8eb22478 in tensorflow::Status tensorflow::LaunchDepthwiseConv2dGPUSmall<Eigen::half, (tensorflow::DepthwiseConv2dDirection)0, -1, -1, 8, true, Eigen::half>(tensorflow::OpKernelContext*, tensorflow::DepthwiseArgs const&, Eigen::half const*, Eigen::half const*, Eigen::half*, tensorflow::TensorFormat) ()
   from /home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so
#8  0x00007fff8eb2bf14 in tensorflow::Status tensorflow::LaunchDepthwiseConv2dGPUSmall<Eigen::half, (tensorflow::DepthwiseConv2dDirection)0, -1, -1>(tensorflow::OpKernelContext*, tensorflow::DepthwiseArgs const&, Eigen::half const*, Eigen::half const*, Eigen::half*, tensorflow::TensorFormat) ()
   from /home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so
#9  0x00007fff8eb30df8 in tensorflow::LaunchDepthwiseConvOp<Eigen::GpuDevice, Eigen::half>::operator()(tensorflow::OpKernelContext*, tensorflow::DepthwiseArgs const&, Eigen::half const*, Eigen::half const*, Eigen::half*, tensorflow::TensorFormat) ()
   from /home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so
#10 0x00007fff8eae47d8 in tensorflow::DepthwiseConv2dNativeOp<Eigen::GpuDevice, Eigen::half>::Compute(tensorflow::OpKernelContext*) ()
   from /home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so
#11 0x00007fff8a9546cc in tensorflow::BaseGPUDevice::ComputeHelper(tensorflow::OpKernel*, tensorflow::OpKernelContext*) ()
   from /home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so
#12 0x00007fff8a954d98 in tensorflow::BaseGPUDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) ()
   from /home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so
#13 0x00007fff8a9aa9f8 in tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) ()
   from /home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so
#14 0x00007fff8aa1a810 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so
#15 0x00007fff8aa1b168 in std::_Function_handler<void (), Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::NonBlockingThreadPoolTempl(int, bool, tensorflow::thread::EigenEnvironment)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()
   from /home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so
#16 0x00007fff8aa174d4 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()
   from /home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so
#17 0x00007fff8aa50170 in std::thread::_Impl<std::_Bind_simple<std::function<void ()> ()> >::_M_run() ()
   from /home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so
#18 0x00007fff975549a4 in ?? () from /usr/lib/powerpc64le-linux-gnu/libstdc++.so.6
#19 0x00007fff9e088070 in start_thread (arg=0x7ffd517ff1a0) at pthread_create.c:335
#20 0x00007fff9dfd3a70 in clone () at ../sysdeps/unix/sysv/linux/powerpc/powerpc64/clone.S:96
(gdb)

```
"
21960,ppc64le: //tensorflow/python/kernel_tests:matrix_exponential_op_test fails,"Please assign this issue to me

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ppc64le Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:  Master branch from 8/29 (065f9b833ffbb3b2f03d63febb186275674ba133)
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.15.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 9.0 / 7
- **GPU model and memory**: 4 V100 GPUs with 16 GB of memory each
- **Exact command to reproduce**: 
bazel test --config=cuda --test_tag_filters=-no_oss,-oss_serial,-no_gpu,-benchmark-test --test_timeout 300,450,1200,3600 --local_test_jobs=4 --test_output=errors --build_tests_only //tensorflow/tools/ci_build/gpu_build:parallel_gpu_execute //tensorflow/python/kernel_tests:matrix_exponential_op_test

### Describe the problem
testL1Norms_complex128_3_2500 in matrix_exponential_op_test fails with a slight difference:
not close lhs =  [14.19404]
not close rhs =  [14.191119]


### Source code / logs
```
======================================================================
FAIL: testL1Norms_complex128_3_2500 (__main__.ExponentialOpTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python                      /kernel_tests/matrix_exponential_op_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/matrix_exponential_op_test.py"", line 234, in Test
    self._verifyExponentialReal(scale * matrix)
  File ""/home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python                      /kernel_tests/matrix_exponential_op_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/matrix_exponential_op_test.py"", line 68, in _verifyExponentialReal
    self._verifyExponential(x, np_type)
  File ""/home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python                      /kernel_tests/matrix_exponential_op_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/matrix_exponential_op_test.py"", line 64, in _verifyExponential
    self.assertAllClose(np_ans, out, rtol=1e-4, atol=1e-3)
  File ""/home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python                      /kernel_tests/matrix_exponential_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1443, in assertAllClose
    self._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)
  File ""/home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python                      /kernel_tests/matrix_exponential_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1413, in _assertAllCloseRecursive
    (path_str, path_str, msg)))
  File ""/home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python                      /kernel_tests/matrix_exponential_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1348, in _assertArrayLikeAllClose
    a, b, rtol=rtol, atol=atol, err_msg=msg, equal_nan=True)
  File ""/usr/local/lib/python3.5/dist-packages/numpy/testing/nose_tools/utils.py"", line 1396, in assert_allclose
    verbose=verbose, header=header, equal_nan=equal_nan)
  File ""/usr/local/lib/python3.5/dist-packages/numpy/testing/nose_tools/utils.py"", line 779, in assert_array_compare
    raise AssertionError(msg)
AssertionError:
Not equal to tolerance rtol=0.0001, atol=0.001
Mismatched value: a is different from b.
(mismatch 5.555555555555557%)
 x: array([[[  4.719045,   3.237183,  -1.798661],
        [  5.548645,   3.793037,  -2.115632],
        [ -5.977193,  -4.089234,   2.273146]],...
 y: array([[[  4.719063,   3.237082,  -1.798567],
        [  5.548592,   3.793213,  -2.115799],
        [ -5.977084,  -4.089204,   2.273036]],...

----------------------------------------------------------------------
Ran 5 tests in 5.153s

FAILED (failures=1)
<class 'numpy.complex128'> (2, 3, 3) 25.0 [[[-0.25091976+0.j  0.90142861+0.j  0.46398788+0.j]
  [ 0.19731697+0.j -0.68796272+0.j -0.68801096+0.j]
  [-0.88383278+0.j  0.73235229+0.j  0.20223002+0.j]]

 [[ 0.41614516+0.j -0.95883101+0.j  0.9398197 +0.j]
  [ 0.66488528+0.j -0.57532178+0.j -0.63635007+0.j]
  [-0.63319098+0.j -0.39151551+0.j  0.04951286+0.j]]]
not close where =  (array([1]), array([2]), array([2]))
not close lhs =  [14.19404]
not close rhs =  [14.191119]
not close dif =  [0.0029211]
not close tol =  [0.00241911]
dtype = float32, shape = (2, 3, 3)
<class 'numpy.complex64'> (2, 3, 3) 5.0 [[[-0.25091976+0.j  0.90142864+0.j  0.4639879 +0.j]
  [ 0.19731697+0.j -0.6879627 +0.j -0.68801093+0.j]
  [-0.88383275+0.j  0.7323523 +0.j  0.20223002+0.j]]

 [[ 0.41614515+0.j -0.958831  +0.j  0.9398197 +0.j]
  [ 0.6648853 +0.j -0.5753218 +0.j -0.6363501 +0.j]
  [-0.633191  +0.j -0.39151552+0.j  0.04951286+0.j]]]
<class 'numpy.float64'> (2, 3, 3) 6.0 [[[-0.25091976  0.90142861  0.46398788]
  [ 0.19731697 -0.68796272 -0.68801096]
  [-0.88383278  0.73235229  0.20223002]]

 [[ 0.41614516 -0.95883101  0.9398197 ]
  [ 0.66488528 -0.57532178 -0.63635007]
  [-0.63319098 -0.39151551  0.04951286]]]
================================================================================

FAILED: //tensorflow/python/kernel_tests:matrix_exponential_op_test (Summary)
```"
21959,tf.contrib.training.batch_sequences_with_states treats batching as duplicating,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Google Colab
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: N/A
- **TensorFlow version (use command below)**: 1.10.0
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
When `train.batch_sequences_with_states` extracts batches from a sequence of samples, what it actually does is duplicating the same segment for `batch_size` times.

### Source code / logs
The code is revised based on [batch_sequences_with_states_test.py](https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/contrib/training/python/training/batch_sequences_with_states_test.py)

```python
from pprint import pprint

import numpy as np
import tensorflow as tf
from tensorflow.contrib.training.python.training import sequence_queueing_state_saver as sqss

batch_size = 3
num_unroll = 2
lstm_size = 4
value_length = 8
input_key = tf.as_string(tf.cast(10000 * tf.random_uniform(()), tf.int32))
input_sequences = {'input': np.random.rand(value_length, 3)}
input_context = {'context_key': [1]}
initial_states = {""lstm_state"": np.random.rand(10, lstm_size)}

with tf.Session() as sess:
    batch = sqss.batch_sequences_with_states(
        input_key=input_key,
        input_sequences=input_sequences,
        input_context=input_context,
        input_length=value_length,
        initial_states=initial_states,
        num_unroll=num_unroll,
        batch_size=batch_size)
    state = batch.state('lstm_state')
    update_state = batch.save_state('lstm_state', state + 1)
    coord = tf.train.Coordinator()
    tf.train.start_queue_runners(sess=sess, coord=coord)
    input_batch_val = sess.run([
        batch.key, batch.next_key, batch.sequences['input'],
        batch.context['context_key'], state, batch.length, update_state][2])
    pprint(input_batch_val)
```
Output:
```
array([[[0.33537843, 0.76504494, 0.368679  ],
        [0.47943187, 0.58871135, 0.06263617]],

       [[0.33537843, 0.76504494, 0.368679  ],
        [0.47943187, 0.58871135, 0.06263617]],

       [[0.33537843, 0.76504494, 0.368679  ],
        [0.47943187, 0.58871135, 0.06263617]]])
```
What further justifies my suspect is the expected values of sequences in unit tests [batch_sequences_with_states_test.py](https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/contrib/training/python/training/batch_sequences_with_states_test.py), which is duplicating the first segment of `""seq1""`.
```python
def _testBasicPadding(self, pad, key=None, make_keys_unique=False):
    num_unroll = 2  # Divisor of value_length - so no padding necessary.
    expected_seq1_batch1 = np.tile(
        self.sequences[""seq1""][np.newaxis, 0:num_unroll, :],
(self.batch_size, 1, 1))
```"
21957,tf.scatter_nd_update - 'Tensor' object has no attribute '_lazy_read',"-----------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:

```
== cat /etc/issue ===============================================
Linux 3bed2f328777 4.13.0-38-generic #43~16.04.1-Ubuntu SMP Wed Mar 14 17:48:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.5 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
Yes

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux 3bed2f328777 4.13.0-38-generic #43~16.04.1-Ubuntu SMP Wed Mar 14 17:48:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.14.5)
protobuf (3.6.1)
tensorflow (1.10.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.10.0
tf.GIT_VERSION = b'unknown'
tf.COMPILER_VERSION = b'unknown'
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64/stubs:/usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/lib/amd64/server/:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server:/opt/boost/lib:/opt/conda/lib/:/usr/local/cuda/lib64/:/opt/conda/lib/R/lib/:/usr/local/nvidia/lib64/:/usr/local/nvidia/lib:/lib/x86_64-linux-gnu:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu:/opt/opencv/lib
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Wed Aug 29 19:57:14 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 396.26                 Driver Version: 396.26                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A |
|  0%   41C    P0    76W / 250W |    708MiB / 11177MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart.so.9.2.148
/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart_static.a

```
1080Ti
- **Exact command to reproduce**:
```
import tensorflow as tf

a = tf.random_uniform(minval=0, maxval=9, shape=[], dtype=tf.int32)
img = tf.zeros([10], dtype=tf.int32)
img_z = tf.scatter_nd_update(img, a, tf.zeros_like(a))
                                           
```

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I am not able to update a variable using tf.scatter_nd_update with indices/updates coming from Tensors `tf.random_uniform`. In another part of my code I use indices and updates coming from `tf.nn.top_k` and that works fine. 

### Source code / logs
```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-9-ed4c565a2efd> in <module>()
      3 a = tf.random_uniform(minval=0, maxval=10, shape=[], dtype=tf.int32)
      4 img = tf.zeros([9], dtype=tf.int32)
----> 5 img_z = tf.scatter_nd_update(img, a, tf.zeros_like(a))
      6 
      7 with tf.Session() as sess:

/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py in scatter_nd_update(ref, indices, updates, use_locking, name)
    357     return gen_state_ops.scatter_nd_update(
    358         ref, indices, updates, use_locking, name)
--> 359   return ref._lazy_read(gen_state_ops.resource_scatter_nd_update(  # pylint: disable=protected-access
    360       ref.handle, indices, ops.convert_to_tensor(updates, ref.dtype),
    361       name=name))

AttributeError: 'Tensor' object has no attribute '_lazy_read'
```
"
21955,tf.stack on windows gives wrong result,"### System information
- Custom code:
- Windows 7 x64:
- Anaconda distributed
- Tensorflow version 1.8.0:
- python 3.6.0 :
- CUDA  V9.0.176 /cuDNN 7.0:
- NVIDIA QUADRO K1100M :

### The problem
``````python
import tensorflow as tf
n_users = tf.placeholder(dtype=tf.int64, shape=[], name='n_users')
n_items = tf.placeholder(dtype=tf.int64, shape=[], name='n_items')
shape = tf.stack([n_users, n_items])

sess = tf.Session()
sess.run(shape, feed_dict={n_users: 100, n_items: 100})
# array([-4631544634021885188,  4529727110648879041], dtype=int64)
``````
It seems that it overflows. 

I also tried by using np.int64 instead of python native integers and it gives: 
array([ 10, 4357074203494797000], dtype=int64). 
It works if I define int32 placeholders. "
21953,Feature request: partial_run_reset(),"Experimental partial_run feature leaks memory. The problem is demonstrated below (with explanatory comments). A simple solution might look like `session.partial_run_reset()` at the end of the while loop. 

Thanks if this can be added!

```
import tensorflow as tf

# Define graph
some_placeholder_value = tf.placeholder(tf.float32, shape=[])
long_list_of_ones = tf.ones(10000)
some_operation = long_list_of_ones + 1
a_second_operation = some_operation + some_placeholder_value

# Initialize
initialize = tf.global_variables_initializer()
session = tf.Session()
session.run(initialize)

# Using partial run in a loop causes a memory leak if not all fetches are fetched. 
# For increased dynamism please allow resetting the partial run graph so that one 
# does not have to know for sure which fetches will be used ahead of time.

while True:
    # Watch as application memory steadily increases... 
    handle = session.partial_run_setup([some_operation, a_second_operation], 
                                       [some_placeholder_value])
    run = session.partial_run(handle, some_operation)
```

Memory usage grows really fast! Since a great advantage of this feature is that it affords greater dynamism and modularity, the option to reset at the end of each iteration would help since it would allow one to use this feature without knowing the full list of fetches a priori.

Alternatively, allowing the same graph components to be re-fetched would also do the trick (by putting handle outside the while loop) and would afford even more dynamism and modularity... but is probably harder to implement."
21952,Failed to load the native TensorFlow runtime.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 x64 1803 
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: TensorFlow 1.10.0
- **Python version**: Python 3.6.3
- **Bazel version (if compiling from source)**: 0.16.1
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: ?
- **GPU model and memory**: I'm not using tensoflow GPU version i'm using CPU Ver
- **Exact command to reproduce**: python decensor.py

You can collect some of this information using our environment capture script:

== cat /etc/issue ===============================================
MINGW64_NT-10.0 DESKTOP-PC1D4GE 2.10.0(0.325/5/3) 2018-06-13 23:34 x86_64 Msys

== are we in docker =============================================
No

== compiler =====================================================
bash: c++: command not found

== uname -a =====================================================
MINGW64_NT-10.0 DESKTOP-PC1D4GE 2.10.0(0.325/5/3) 2018-06-13 23:34 x86_64 Msys

== check pips ===================================================
numpy                 1.14.5             
protobuf              3.6.1              
tensorflow            1.10.0             

== check for virtualenv =========================================
False

== tensorflow import ============================================
Traceback (most recent call last):
  File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
bash: nvidia-smi: command not found

== cuda libs  ===================================================




### Describe the problem
C:\Users\minat\Desktop\Respo\DeepMindBreak>python decensor.py
Traceback (most recent call last):
  File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 658, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 571, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""decensor.py"", line 2, in <module>
    import tensorflow as tf
  File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 658, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 571, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

### Source code
import tensorflow as tf
hello = tf.constant('Hello, TensorFlow!')
sess = tf.Session()
print(sess.run(hello))

"
21949,'''\]['p,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21948,',"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21947,"Where can i find tensorflow 0,12 version?","It seems that i can't find tensorflow versio 0,12.. Can anyone help me to find it?"
21946,[feature request] specify subgraphs to route to TensorRT in tf.contrib.tensorrt,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.10.1
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: 0.15.1
- **GCC/Compiler version (if compiling from source)**: 5
- **CUDA/cuDNN version**: 9.0, 7.0
- **GPU model and memory**: 
- **Exact command to reproduce**:

### Describe the problem
Based on the examples provided in tf.contrib.tensorrt as well as on the internet, it seems that the following:
```
import tensorflow.contrib.tensorrt as trt
with gfile.FastGFile(protobuf_file, 'rb') as f:
    graph_def = tf.GraphDef()
    graph_def.ParseFromString(f.read())
    trt_graph = trt.create_inference_graph(graph_def, trt_nodes,
                                           max_batch_size=batch_size,
                                           max_workspace_size_bytes=memory_allocation,
                                           minimum_segment_size=minimum_segment_size,
                                           precision_mode=""FP32"")  # Get optimized graph
```
is enough to make the parts that are recognized by TensorRT into trt_ops. The only control you have of what to send to TensorRT within the graph is ""minimum_segment_size"" which excludes sections of the graph below this number of compatible ops.
However, it would be helpful to be able to define sections (either explicit ops, or input/output pairs in the subgraph).
For example, at the moment, I am struggling to convert into a hybrid TF/TRT graph a Fast-RCNN architecture. TensorRT will convert for example all convolution operations, however the encoder part will have convolutions with a batch size equal to the number of images provided, while the proposal classification part of the network will have convolutions with batch size equal to the number of proposals. I have everything as a single Tensorflow graph with dynamic batch for the 2nd stage, however TensorRT expects a max batch size. If I set the max batch size to the max number of proposals, I run out of memory since it will also assign this max batch to the encoder part.
At the moment I've set the minimum_segment_size high enough for TensorRT to ignore the proposal part of the network and convert only the encoder. However, it would be helpful to be convert both sections, and leave the unsupported operations (like `tf.image.crop_and_resize`, or `tf.image.non_max_suppression` in Tensorflow).
Any advice? Thanks
"
21943,Can't include no_op in input_map when i import graph_def,"### System information
- **TensorFlow version (use command below)**: current master

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

When i pass no_op  into input_map, it makes an error.
i tracebacked some codes and 
the function _PopulateTFImportGraphDefOptions() in tensorflow/tensorflow/python/framework/importer.py can't support no_op as a control dependency.

under the first loop, it registers the input map operation into `options` when the `input_src` starts with '^'.
However, `no_op` doesn't have `_as_tf_output()` and the line
`dst_op = input_dst._as_tf_output().oper` occurs an exception.
Temporarily, i add the lines
`      src_name, src_idx = _ParseTensorName(input_src[1:])`
`      src_name = compat.as_str(src_name)`
`      dst_output = c_api_util.tf_output(input_dst._c_op, -1)`
`      c_api.TF_ImportGraphDefOptionsAddInputMapping(options, src_name, -1,`
`                                                    dst_output)`
instead of the lines
`      src_name = compat.as_bytes(input_src[1:])`
`      dst_op = input_dst._as_tf_output().oper  # pylint: disable=protected-access`
`      c_api.TF_ImportGraphDefOptionsRemapControlDependency(`
`          options, src_name, dst_op)`
However, I hope this problem to be fixed in the next version.

Thank you

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21942,why we have kernel argument here?  How can I get my custom op kernel depends on a tf_kernel_library,https://github.com/tensorflow/tensorflow/blob/3d35a07179d4d38d0cabac4415c550f1cbce00c0/tensorflow/tensorflow.bzl#L1497
21941, CUDNN_STATUS_INTERNAL_ERROR   CUDNN_STATUS_BAD_PARAM  TypeError: 'NoneType' object is not callable,"2018-08-29 07:55:25.813715: E tensorflow/stream_executor/cuda/cuda_dnn.cc:403] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2018-08-29 07:55:25.813773: E tensorflow/stream_executor/cuda/cuda_dnn.cc:370] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM
2018-08-29 07:55:25.813787: F tensorflow/core/kernels/conv_ops.cc:712] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms) 
Exception ignored in: <bound method BaseSession.__del__ of <tensorflow.python.client.session.Session object at 0x7fa2b1fed588>>
Traceback (most recent call last):
  File ""/root/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 712, in __del__
TypeError: 'NoneType' object is not callable"
21939,Tensorflow 1.10.1 incompatible with latest numpy version 1.15.1 ,"Installed TF with following command `pip3 install tensorflow`. The installation is successful but gave me this error.
`tensorflow 1.10.1 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.15.1 which is incompatible.`

Do I need to downgrade my numpy version?"
21938,MKL build failing at tensorflow/core/kernels/mkl_input_conversion_op.cc,"I'm trying to build TF with the MKL option, and it looks like `tensorflow/core/kernels/mkl_input_conversion_op.cc` is failing to build, with the following error:
(building off of the master branch)
```

[0m[91mERROR: /opt/tensorflow/tensorflow/core/kernels/BUILD:6217:1: C++ compilation of rule '//tensorflow/core/kernels:mkl_input_conversion_op' failed (Exit 1)
[0m[91mIn file included from ./tensorflow/core/kernels/mkl_tfconv_op.h:39,
                 from tensorflow/core/kernels/mkl_input_conversion_op.cc:32:
./tensorflow/core/util/mkl_util.h: In member function 'void tensorflow::FactoryKeyCreator::Append(tensorflow::StringPiece)':
./tensorflow/core/util/mkl_util.h:2057:19: error: 'class tensorflow::StringPiece' has no member named 'ToString'
     key_.append(s.ToString());
                   ^~~~~~~~
[0m[91mTarget //tensorflow/tools/pip_package:build_pip_package failed to build
```"
21935,import tensorflow.contrib fails on arm32,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No. running ""tensorflow/examples/learn/resnet.py""
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: custom embedded linux
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Yes, rpi class device
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.10.1 / 4dcfddc5d12018a5a0fdca652b9221ed95e9eb23
- **Python version**: 2.7.15
- **Bazel version (if compiling from source)**:  0.16.0
- **GCC/Compiler version (if compiling from source)**: gcc 6.3
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None
- **Exact command to reproduce**: ""import tensorflow.contrib""

### Describe the problem
I am cross-compiling tensorflow to work on a 32-bit armhf system. My build system is x86-64. sample apps seem to work ok. but I am having difficulty in the python environment. ""import tensorflow"" works ok. But ""import tensorflow.contrib"" fails with this error:

```
>>> import tensorflow.contrib
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/mnt/nfs/baseline/obj.32/target/usr/local/lib/python2.7/site-packages/tensorflow/contrib/__init__.py"", line 69, in <module>
    from tensorflow.contrib import periodic_resample
  File ""/mnt/nfs/baseline/obj.32/target/usr/local/lib/python2.7/site-packages/tensorflow/contrib/periodic_resample/__init__.py"", line 22, in <module>
    from tensorflow.contrib.periodic_resample.python.ops.periodic_resample_op import periodic_resample
  File ""/mnt/nfs/baseline/obj.32/target/usr/local/lib/python2.7/site-packages/tensorflow/contrib/periodic_resample/python/ops/periodic_resample_op.py"", line 32, in <module>
    resource_loader.get_path_to_datafile('_periodic_resample_op.so'))
  File ""/mnt/nfs/baseline/obj.32/target/usr/local/lib/python2.7/site-packages/tensorflow/contrib/util/loader.py"", line 56, in load_op_library
    ret = load_library.load_op_library(path)
  File ""/mnt/nfs/baseline/obj.32/target/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/load_library.py"", line 73, in load_op_library
    exec(wrappers, module.__dict__)
  File ""<string>"", line 293, in <module>
  File ""<string>"", line 229, in _InitOpDefLibrary
  File ""/mnt/nfs/baseline/obj.32/target/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_registry.py"", line 35, in register_op_list
    assert _registered_ops[op_def.name] == op_def
AssertionError
```

I have tried combinations of bazel 0.12/0.16, tensorflow 1.8/1.10 and python 2.7.10/2.7.15. I get the same result every time. Issue seems to be related to only PeriodicResample & PeriodicResampleOpGrad. Other modules do not assert.

"
21934,[feature request] tensor type `boundaries` in `bucketize` op,"ping @yongtang and refer to https://github.com/tensorflow/tensorflow/pull/14774#issuecomment-416726581 

Currently, `boundaries` has to be `list` type. It is not convenience for the dynamic `boundaries` .
It would be great if`boundaries` could be `tensor` type. "
21932,x86/ppc64le //tensorflow/contrib/lite/delegates/eager:kernel_test and delegate_test core dump,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ppc64le Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: master branch from 8/27 (Last commit 514f65a)
- **Python version**: both 2.7 and 3.6
- **Bazel version (if compiling from source)**: 0.15.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 9.0, 7
- **GPU model and memory**: 4 V100 GPUs with 16 GB of memory each
- **Exact command to reproduce**:
bazel test --config=cuda --test_tag_filters=-no_oss,-oss_serial,-no_gpu,-benchmark-test --test_timeout 300,450,1200,3600 --local_test_jobs=4 --test_output=errors --build_tests_only //tensorflow/tools/ci_build/gpu_build:parallel_gpu_execute //tensorflow/contrib/lite/delegates/eager:kernel_test //tensorflow/contrib/lite/delegates/eager:delegate_test 

### Describe the problem
on x86 and ppc64le when running the Tensorflow Unit Test against contrib, both contrib/lite/delegates/eager:kernel_test and delegate_test core dump. Maybe 'contrib/lite' since it was meant for mobile should be excluded from unit test for x86 and ppc64le?


```
==================== Test output for //tensorflow/contrib/lite/delegates/eager:kernel_test:
Running test /home/wdirons/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/lite/delegates/eager/kernel_test.runfiles/org_tensorflow/tensorflow/contrib/lite/delegates/eager/kernel_test  on GPU 0
[==========] Running 7 tests from 1 test case.
[----------] Global test environment set-up.
[----------] 7 tests from KernelTest
[ RUN      ] KernelTest.FullGraph
2018-08-28 22:14:43.578155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties:
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0004:04:00.0
totalMemory: 15.75GiB freeMemory: 15.34GiB
2018-08-28 22:14:43.578223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0
2018-08-28 22:18:23.446411: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-28 22:18:23.446445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0
2018-08-28 22:18:23.446454: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N
2018-08-28 22:18:23.447047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14846 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0004:04:00.0, compute capability: 7.0)
/home/wdirons/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/lite/delegates/eager/kernel_test.runfiles/org_tensorflow/tensorflow/tools/ci_build/gpu_build/parallel_gpu_execute: line 64: 61525 Segmentation fault      (core dumped) ""$TEST_BINARY"" $@
================================================================================

```

### Source code / logs
gdb where ran on the core dump for kernel_test on ppc64le

```
(gdb) where
#0  __memcpy_power7 () at ../sysdeps/powerpc/powerpc64/power7/memcpy.S:277
#1  0x00000001118dd344 in void tflite::eager::(anonymous namespace)::KernelTest::ConfigureDelegate<tflite::eager::(anonymous namespace)::KernelTest_FullGraph_Test::TestBody()::{lambda(                       TfLiteContext*, _TfLiteDelegate*)#1}>(tflite::eager::(anonymous namespace)::KernelTest_FullGraph_Test::TestBody()::{lambda(TfLiteContext*, _TfLiteDelegate*)#1})::{lambda(TfLiteContext*                       , _TfLiteDelegate*, int, void*, unsigned long)#1}::_FUN ()
#2  0x00007fffa4afd9d0 in tflite::Interpreter::Invoke() ()
   from /home/wdirons/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/lite                       /delegates/eager/../../../../../_solib_local/libtensorflow_Scontrib_Slite_Slibframework.so
#3  0x00007fffa6773b14 in tflite::eager::testing::EagerModelTest::Invoke() ()
   from /home/wdirons/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/lite                       /delegates/eager/../../../../../_solib_local/libtensorflow_Scontrib_Slite_Sdelegates_Seager_Slibtest_Uutil.so
#4  0x00000001118e8444 in tflite::eager::(anonymous namespace)::KernelTest_FullGraph_Test::TestBody() ()
#5  0x00007fffa47f272c in void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) ()
   from /home/wdirons/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/lite                       /delegates/eager/../../../../../_solib_local/libexternal_Scom_Ugoogle_Ugoogletest_Slibgtest.so
#6  0x00007fffa47f2ab4 in testing::Test::Run() ()
   from /home/wdirons/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/lite                       /delegates/eager/../../../../../_solib_local/libexternal_Scom_Ugoogle_Ugoogletest_Slibgtest.so
#7  0x00007fffa47f2f98 in testing::TestInfo::Run() ()
   from /home/wdirons/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/lite                       /delegates/eager/../../../../../_solib_local/libexternal_Scom_Ugoogle_Ugoogletest_Slibgtest.so
#8  0x00007fffa47f33a4 in testing::TestCase::Run() ()
   from /home/wdirons/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/lite                       /delegates/eager/../../../../../_solib_local/libexternal_Scom_Ugoogle_Ugoogletest_Slibgtest.so
#9  0x00007fffa47f387c in testing::internal::UnitTestImpl::RunAllTests() ()
   from /home/wdirons/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/lite                       /delegates/eager/../../../../../_solib_local/libexternal_Scom_Ugoogle_Ugoogletest_Slibgtest.so
#10 0x00007fffa47f3c5c in bool testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::                       UnitTestImpl::*)(), char const*) ()
   from /home/wdirons/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/lite                       /delegates/eager/../../../../../_solib_local/libexternal_Scom_Ugoogle_Ugoogletest_Slibgtest.so
#11 0x00007fffa47f3f60 in testing::UnitTest::Run() ()
   from /home/wdirons/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/lite                       /delegates/eager/../../../../../_solib_local/libexternal_Scom_Ugoogle_Ugoogletest_Slibgtest.so
#12 0x00000001117950fc in main ()
(gdb)
```

gdb where ran on the core dump for delegate_test on ppc64le

```
(gdb) where
#0  __memcpy_power7 () at ../sysdeps/powerpc/powerpc64/power7/memcpy.S:277
#1  0x00007fffaabc2b24 in tflite::eager::delegate::CopyFromBufferHandle(TfLiteContext*, _TfLiteDelegate*, int, void*, unsigned long) ()
   from /home/wdirons/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/lite/delegates/eager/../../../../../_solib_local/libtensorflow_Scontrib_Slite_Sdelegates_Seager_Slibdelegate.so
#2  0x00007fff9716d9d0 in tflite::Interpreter::Invoke() ()
   from /home/wdirons/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/lite/delegates/eager/../../../../../_solib_local/libtensorflow_Scontrib_Slite_Slibframework.so
#3  0x00007fff98ec3b14 in tflite::eager::testing::EagerModelTest::Invoke() ()
   from /home/wdirons/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/lite/delegates/eager/../../../../../_solib_local/libtensorflow_Scontrib_Slite_Sdelegates_Seager_Slibtest_Uutil.so
#4  0x0000000105bf182c in tflite::eager::(anonymous namespace)::DelegateTest_FullGraph_Test::TestBody() ()
#5  0x00007fff96f4272c in void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) ()
   from /home/wdirons/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/lite/delegates/eager/../../../../../_solib_local/libexternal_Scom_Ugoogle_Ugoogletest_Slibgtest.so
#6  0x00007fff96f42ab4 in testing::Test::Run() ()
   from /home/wdirons/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/lite/delegates/eager/../../../../../_solib_local/libexternal_Scom_Ugoogle_Ugoogletest_Slibgtest.so
#7  0x00007fff96f42f98 in testing::TestInfo::Run() ()
   from /home/wdirons/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/lite/delegates/eager/../../../../../_solib_local/libexternal_Scom_Ugoogle_Ugoogletest_Slibgtest.so
#8  0x00007fff96f433a4 in testing::TestCase::Run() ()
   from /home/wdirons/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/lite/delegates/eager/../../../../../_solib_local/libexternal_Scom_Ugoogle_Ugoogletest_Slibgtest.so
#9  0x00007fff96f4387c in testing::internal::UnitTestImpl::RunAllTests() ()
   from /home/wdirons/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/lite/delegates/eager/../../../../../_solib_local/libexternal_Scom_Ugoogle_Ugoogletest_Slibgtest.so
#10 0x00007fff96f43c5c in bool testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) ()
   from /home/wdirons/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/lite/delegates/eager/../../../../../_solib_local/libexternal_Scom_Ugoogle_Ugoogletest_Slibgtest.so
#11 0x00007fff96f43f60 in testing::UnitTest::Run() ()
   from /home/wdirons/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/lite/delegates/eager/../../../../../_solib_local/libexternal_Scom_Ugoogle_Ugoogletest_Slibgtest.so
#12 0x0000000105aa3f7c in main ()
```

On my x86 container I was unable to determine the location of the core that was generated."
21930,"TF logging should support ""exception()""","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: np
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: all
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:1.8
- **Python version**:2.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:N/A
- **GPU model and memory**:N/A
- **Exact command to reproduce**:N/A

### Describe the problem
Feature request:
Tensorflow logging library should support ""exception()"" function  - and forward it to the underlying logger.
"
21929,Poor performance of the model when enabling layer normalization with tf.contrib.rnn.LayerNormBasicLSTMCell,"-------------------------------------------------------------------------------------------------------


### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 

VERSION=""16.04.4 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: 
No
- **TensorFlow installed from (source or binary)**:
 source
- **TensorFlow version (use command below)**:  
tf.VERSION = 1.9.0

- **Python version**: python 2.7

- **Bazel version (if compiling from source)**:  
0.11.1

- **GCC/Compiler version (if compiling from source)**:  
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609

- **CUDA/cuDNN version**: 
 9.1
- **GPU model and memory**:  
Tesla K80
- **Exact command to reproduce**:
( No command)

-------------------------------------------------------------------------------------------------------
Before writing my issue here,  I found a similar question on  [stackoverflow]( https://stackoverflow.com/questions/45150101/why-is-layernormbasiclstmcell-much-slower-and-less-accurate-than-lstmcell) where the author described the same problem I have but there were no valid answer and most of the comments where wrongly referring to technique related to batch_normalization which are non-applicable since we are talking about layer_normalization. 
( [Difference between layer_normalzation and batch_normalization](https://theneuralperspective.com/2016/10/27/gradient-topics/) )
I was trying to do some layer normalization with my model, and I have chosen the 
`tf.contrib.rnn.LayerNormBasicLSTMCell` where `layer_norm` is set to True.
I started training the model and as unexpected the training/validation losses are decreasing slowly.
The paper of layer normalization ( https://arxiv.org/pdf/1607.06450.pdf ) showed that this technique makes the training time much faster but the Tensorflow implementation showed the inverse case.
Are there please any hints on how to solve this problem? Any further explications on why the implementation makes the training slower? "
21927,AttributeError: module 'tensorflow.python.keras' has no attribute 'Model',"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: not custom code / object_detection
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary 
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**:  3.5
- **CUDA/cuDNN version**: 8.0
- **GPU model and memory**: 
Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                56
On-line CPU(s) list:   0-55
Thread(s) per core:    2
Core(s) per socket:    14
Socket(s):             2
NUMA node(s):          2
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 79
Model name:            Intel(R) Xeon(R) CPU E5-2690 v4 @ 2.60GHz
Stepping:              1
CPU MHz:               2599.695
CPU max MHz:           3500.0000
CPU min MHz:           1200.0000
BogoMIPS:              5189.84
Virtualization:        VT-x
L1d cache:             32K
L1i cache:             32K
L2 cache:              256K
L3 cache:              35840K
NUMA node0 CPU(s):     0-13,28-41
NUMA node1 CPU(s):     14-27,42-55
Flags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb invpcid_single intel_pt ibrs ibpb stibp kaiser tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm arat pln pts

- **Exact command to reproduce**:
python3 object_detection/legacy/train.py --logtostderr --train_dir=object_detection/training/ --pipeline_config_path=object_detection/training/ssd_mobilenet_v1_pets.config

When i wanted to start training object detector i get this error even if tensorflow has valid installation. With the same configuration on tf 1.10. its working well. Is this a problem of version? was the ""Model attribute"" released after tf 1.4 version?

ERROR LOG:
Traceback (most recent call last):
  File ""object_detection/legacy/train.py"", line 51, in <module>
    from object_detection.builders import model_builder
  File ""/home/klapajar/cone_detection/tensorflow/models/research/object_detection/builders/model_builder.py"", line 20, in <module>
    from object_detection.builders import box_predictor_builder
  File ""/home/klapajar/cone_detection/tensorflow/models/research/object_detection/builders/box_predictor_builder.py"", line 18, in <module>
    from object_detection.predictors import convolutional_box_predictor
  File ""/home/klapajar/cone_detection/tensorflow/models/research/object_detection/predictors/convolutional_box_predictor.py"", line 18, in <module>
    from object_detection.core import box_predictor
  File ""/home/klapajar/cone_detection/tensorflow/models/research/object_detection/core/box_predictor.py"", line 137, in <module>
    class KerasBoxPredictor(tf.keras.Model):
AttributeError: module 'tensorflow.python.keras' has no attribute 'Model'"
21926,got a bus error on raspberry pi,"I trained my network on my mac, and predict something on my raspberry pi with keras, but I got a bus error.

OS: Raspbian GNU/Linux 9
tensorflow: 1.9.0
keras: 2.2.2

log info:

Using TensorFlow backend.
/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.4 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5
  return f(*args, **kwds)
/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: builtins.type size changed, may indicate binary incompatibility. Expected 432, got 412
  return f(*args, **kwds)
2018-08-28 14:31:25.554008: W tensorflow/core/framework/allocator.cc:108] Allocation of 63539200 exceeds 10% of system memory.
2018-08-28 14:31:26.177700: W tensorflow/core/framework/allocator.cc:108] Allocation of 63539200 exceeds 10% of system memory.
2018-08-28 14:31:26.345901: W tensorflow/core/framework/allocator.cc:108] Allocation of 63539200 exceeds 10% of system memory.
2018-08-28 14:31:36.619646: W tensorflow/core/framework/allocator.cc:108] Allocation of 63539200 exceeds 10% of system memory.
2018-08-28 14:31:36.850119: W tensorflow/core/framework/allocator.cc:108] Allocation of 63539200 exceeds 10% of system memory.
Bus error"
21925,tf.scatter_update gradients silently failing in eager mode,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.10
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: 0.16.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 8.0/6.0
- **GPU model and memory**: GTX 980 4GB
- **Exact command to reproduce**:

### Describe the problem
When I use `tf.scatter_update` in `eager` mode, I get `None` gradients for variables that are involved in the update (not for the updated variable).   
The same code in graph mode produces an error.   
The behaviour I expect is either for the gradients to be properly computed (not sure why tensorflow can't do this), or to give the same kind of error as in graph mode.

### Source code / logs
```
from __future__ import print_function
import tensorflow as tf
tf.enable_eager_execution()

v = tf.Variable(13.0)
alpha = tf.Variable([1, 2, 3], dtype=tf.float32)
idx = 1
with tf.GradientTape() as tape:
    alpha = tf.scatter_update(alpha, idx, v)
    loss = tf.reduce_sum(tf.square(alpha))
grads = tape.gradient(loss, [v, alpha])
print(grads)
```
The output of `print(grads)` is `[None, <tf.Tensor: id=31, shape=(3,), dtype=float32, numpy=array([ 2., 26.,  6.], dtype=float32)>]`, so the gradient is not computed for `v`.

In graph mode
```
from __future__ import print_function
import tensorflow as tf

v = tf.Variable(13.0)
alpha = tf.Variable([1, 2, 3], dtype=tf.float32)
idx = 1
alpha = tf.scatter_update(alpha, idx, v)
loss = tf.reduce_sum(tf.square(alpha))
grads = tf.gradients(loss, [v, alpha])
print(grads)

```
The error I get: `LookupError: No gradient defined for operation 'ScatterUpdate' (op type: ScatterUpdate)`"
21924,toco runs into flatbuffer 2GB limit when converting 181MB frozen graph_def,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: KDE Neon
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: master branch, rev 57919740bf151cb6395aa60e30404ee9caa066d6
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**: 0.16.1
- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
- **CUDA/cuDNN version**: CPU-only build
- **GPU model and memory**: N/A
- **Mobile device**: N/A
- **Exact command to reproduce**: bazel run --compilation_mode=dbg tensorflow/contrib/lite/toco:toco -- --input_file=/path/to/output_graph.pb --inference_type=FLOAT --input_arrays=input_node --input_shapes=1,16,494 --output_array=logits --output_file=/tmp/graph.tflite

### Describe the problem

When trying to convert my GraphDef file to a TFLite file using TOCO I run into the 2GB limit for flatbuffers. The frozen graph protobuf is 181MB. I've uploaded the frozen graph protobuf as well as an unfrozen pbtxt here: https://github.com/reuben/silly_hacks/releases/tag/github_as_file_hosting_service

```
$ bazel run --compilation_mode=dbg tensorflow/contrib/lite/toco:toco -- --input_file=/home/reuben/output_graph.pb --inference_type=FLOAT --input_arrays=input_node --input_shapes=1,16,494 --output_array=logits --output_file=/home/reuben/graph.tflite
INFO: Analysed target //tensorflow/contrib/lite/toco:toco (0 packages loaded).
INFO: Found 1 target...
Target //tensorflow/contrib/lite/toco:toco up-to-date:
  bazel-bin/tensorflow/contrib/lite/toco/toco
INFO: Elapsed time: 0.213s, Critical Path: 0.00s
INFO: 0 processes.
INFO: Build completed successfully, 1 total action
INFO: Running command line: bazel-bin/tensorflow/contrib/lite/toco/toco '--input_file=/home/reuben/output_graph.pb' '--inference_type=FLOAT' '--input_arrays=input_node' '--input_shapes=1,16,494' '--output_INFO: Build completed successfully, 1 total action
2018-08-28 10:42:03.161524: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 262 operators, 398 arrays (0 quantized)
2018-08-28 10:42:03.170762: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 262 operators, 398 arrays (0 quantized)
2018-08-28 10:42:11.384607: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 225 operators, 373 arrays (0 quantized)
2018-08-28 10:42:11.454370: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 223 operators, 371 arrays (0 quantized)
2018-08-28 10:42:11.472062: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 221 operators, 367 arrays (0 quantized)
2018-08-28 10:42:11.487687: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 221 operators, 367 arrays (0 quantized)
2018-08-28 10:42:11.493166: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 278528 bytes, theoretical optimal value: 262144 bytes.
2018-08-28 10:42:11.496319: I tensorflow/contrib/lite/toco/toco_tooling.cc:394] Estimated count of arithmetic ops: 1.52158 billion (note that a multiply-add is counted as 2 ops).
toco: external/flatbuffers/include/flatbuffers/flatbuffers.h:590: size_t flatbuffers::vector_downward::ensure_space(size_t): Assertion `size() < FLATBUFFERS_MAX_BUFFER_SIZE' failed.
Aborted
```

### Source code / logs

Traceback:

```
(gdb) bt
#0  0x00007ffff46a9428 in __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:54
#1  0x00007ffff46ab02a in __GI_abort () at abort.c:89
#2  0x00007ffff46a1bd7 in __assert_fail_base (fmt=<optimized out>, assertion=assertion@entry=0x73cdf0 ""size() < FLATBUFFERS_MAX_BUFFER_SIZE"", file=file@entry=0x73cd28 ""external/flatbuffers/include/flatbuffers/flatbuffers.h"",
    line=line@entry=590, function=function@entry=0x73d640 <flatbuffers::vector_downward::ensure_space(unsigned long)::__PRETTY_FUNCTION__> ""size_t flatbuffers::vector_downward::ensure_space(size_t)"") at assert.c:92
#3  0x00007ffff46a1c82 in __GI___assert_fail (assertion=0x73cdf0 ""size() < FLATBUFFERS_MAX_BUFFER_SIZE"", file=0x73cd28 ""external/flatbuffers/include/flatbuffers/flatbuffers.h"", line=590,
    function=0x73d640 <flatbuffers::vector_downward::ensure_space(unsigned long)::__PRETTY_FUNCTION__> ""size_t flatbuffers::vector_downward::ensure_space(size_t)"") at assert.c:101
#4  0x00000000005baac8 in flatbuffers::vector_downward::ensure_space (this=0x7fffffffc8e0, len=0) at external/flatbuffers/include/flatbuffers/flatbuffers.h:590
#5  0x00000000005baaf1 in flatbuffers::vector_downward::make_space (this=0x7fffffffc8e0, len=0) at external/flatbuffers/include/flatbuffers/flatbuffers.h:595
#6  0x00000000005bacb9 in flatbuffers::vector_downward::fill (this=0x7fffffffc8e0, zero_pad_bytes=0) at external/flatbuffers/include/flatbuffers/flatbuffers.h:647
#7  0x00000000005bb12e in flatbuffers::FlatBufferBuilder::Align (this=0x7fffffffc8e0, elem_size=4) at external/flatbuffers/include/flatbuffers/flatbuffers.h:840
#8  0x00000000005bd1ad in flatbuffers::FlatBufferBuilder::PushElement<unsigned int> (this=0x7fffffffc8e0, element=134217728) at external/flatbuffers/include/flatbuffers/flatbuffers.h:861
#9  0x00000000005bb831 in flatbuffers::FlatBufferBuilder::EndVector (this=0x7fffffffc8e0, len=134217728) at external/flatbuffers/include/flatbuffers/flatbuffers.h:1148
#10 0x00000000005f6fce in flatbuffers::FlatBufferBuilder::CreateVector<unsigned char> (this=0x7fffffffc8e0,
    v=0x7fff645ea010 ""\256<\355\212#\274\v\261w<_\""\016\275(\371\254\275\314I\274\274\206\032\300<\232\037B=~8\236\274\213\366\036<\020\254\251<\314\363\360;2\262\360\274\345\252}="", len=134217728)
    at external/flatbuffers/include/flatbuffers/flatbuffers.h:1194
#11 0x00000000006af292 in toco::tflite::(anonymous namespace)::CopyBuffer<(toco::ArrayDataType)2> (array=..., builder=0x7fffffffc8e0) at tensorflow/contrib/lite/toco/tflite/types.cc:56
#12 0x00000000006aec36 in toco::tflite::DataBuffer::Serialize (array=..., builder=0x7fffffffc8e0) at tensorflow/contrib/lite/toco/tflite/types.cc:141
#13 0x00000000005b9a88 in toco::tflite::ExportBuffers (model=..., buffers_to_write=std::vector of length 368, capacity 512 = {...}, builder=0x7fffffffc8e0) at tensorflow/contrib/lite/toco/tflite/export.cc:307
#14 0x00000000005ba1b2 in toco::tflite::Export (model=..., allow_custom_ops=false, output_file_contents=0x7fffffffd070, ops_by_type=std::map with 80 elements = {...}) at tensorflow/contrib/lite/toco/tflite/export.cc:387
#15 0x00000000005b9b5c in toco::tflite::Export (model=..., allow_custom_ops=false, output_file_contents=0x7fffffffd070) at tensorflow/contrib/lite/toco/tflite/export.cc:317
#16 0x000000000052eae2 in toco::Export (toco_flags=..., model=..., allow_custom_ops=false, output_file_contents=0x7fffffffd070) at tensorflow/contrib/lite/toco/toco_tooling.cc:407
#17 0x00000000005135fa in toco::(anonymous namespace)::ToolMain (parsed_toco_flags=..., parsed_model_flags=...) at tensorflow/contrib/lite/toco/toco.cc:90
#18 0x00000000005138fe in main (argc=1, argv=0x7fffffffd9e8) at tensorflow/contrib/lite/toco/toco.cc:127
```"
21923, trous 1D convolution slow in certain scenarios ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10.1
- **Python version**: 3.5.2
- **CUDA/cuDNN version**: 9.0.176
- **GPU model and memory**: CPU
- **Exact command to reproduce**: See below
- **Bazel version**: N/A
- **Mobile device**: N/A

### Describe the problem
Running

``
tf.nn.atrous_conv2d(images, filters, rate=dilation_rate, padding=""SAME"")
``

and

``
tf.nn.convolution(images, filters, dilation_rate=[dilation_rate, dilation_rate], padding=""SAME"")
``

takes much more time to run than:

``
tf.nn.convolution(images, filters, dilation_rate=[1, dilation_rate], padding=""SAME"")
``

for `1D` convolutions.

I don't think this is supposed to behave like this, provided that the input data is one dimensional along the width or the height (i.e. ``shape = [batch, 1, width, channels] or [batch, height, 1, channels]``) since in this case, no dilation is needed on the corresponding axis.

A person using `tf.nn.atrous_conv2d` for `1D` "" trous"" convolutions might never notice the issue here, and his network will be really slow to train.

### Source code / logs
A small snippet to compare the performance of the different methods

```python
import tensorflow as tf
import numpy as np
import time

# Scenario
batch_size = 100
channels = 3
o_channels = 10
image_size = 160
filter_size = 3

# Dummy images and filters
images = tf.random_normal(shape=[batch_size, 1, image_size, channels],  dtype=tf.float32)
filters = tf.random_normal(shape=[1, filter_size, channels, o_channels], dtype=tf.float32)

# Ops definition
normal_conv = tf.nn.convolution(images, filters, padding=""SAME"")
atrous_convs = []
atrous_convs_wrapper = []
for i in [5, 10, 20, 50]:
    atrous_convs.append({""dilation"": i, ""op"": tf.nn.convolution(images, filters, dilation_rate=[1, i], padding=""SAME"")})
    atrous_convs_wrapper.append({""dilation"": i, ""op"": tf.nn.convolution(images, filters, dilation_rate=[i, i], padding=""SAME"")})
    # Or
    # atrous_convs_wrapper.append({""dilation"": i, ""op"": tf.nn.atrous_conv2d(images, filters, rate=i, padding=""SAME"")})
# Test
with tf.Session() as sess:
    repeats = 50
    
    for conv in atrous_convs:
        op1 = conv[""op""]
        op2 = [elt for elt in atrous_convs_wrapper if elt[""dilation""] == conv[""dilation""]][0][""op""]
        op1_res, op2_res = sess.run([op1, op2])
        np.testing.assert_equal(op1_res, op2_res)
    
    # Benchmark normal method
    start = time.time()
    for _ in range(repeats):
        _ = sess.run(normal_conv)
    end = time.time()
    normal_conv_time = int((end - start) / repeats * 1000)

    # Benchmark atrous conv method
    for conv in atrous_convs:
        start = time.time()
        for _ in range(repeats):
            _ = sess.run(conv[""op""])
        end = time.time()
        conv.update({""time"": int((end - start) / repeats * 1000)})
    
    for conv in atrous_convs_wrapper:
        start = time.time()
        for _ in range(repeats):
            _ = sess.run(conv[""op""])
        end = time.time()
        conv.update({""time"": int((end - start) / repeats * 1000)})

    print(""Normal conv: {}ms"".format(normal_conv_time))
    for conv in atrous_convs:
        print(""Atrous conv w/ dilation {}: {}ms"".format(conv[""dilation""], conv[""time""]))
    for conv in atrous_convs_wrapper:
        print(""Atrous conv(wrapper) w/ dilation {}: {}ms"".format(conv[""dilation""], conv[""time""]))

```

returns

```
Normal conv: 1ms
Atrous conv w/ dilation 5: 1ms
Atrous conv w/ dilation 10: 2ms
Atrous conv w/ dilation 20: 1ms
Atrous conv w/ dilation 50: 2ms
Atrous conv(wrapper) w/ dilation 5: 4ms
Atrous conv(wrapper) w/ dilation 10: 9ms
Atrous conv(wrapper) w/ dilation 20: 15ms
Atrous conv(wrapper) w/ dilation 50: 47ms
```"
21922,TF_GraphImportGraphDef fails with no input mapping on existing nodes,"### System information

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: linux, ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10
- **Python version**:  3.5.2
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A


### Describe the problem

Using `TF_GraphImportGraphDef` to incrementally build a graph (using NodeDefs instead of the extensive `TF_OperationDescription* ` API) returns a `TF_Status` error code (3) with message  **Unknown input node** when a node currently being imported has an input node who is  not in the current import call, but already in the graph.


### Source code / logs

I would assume that it would be okay to import a node whose inputs are already in the graph.

If this assumption is correct the problem arises from line 529 In [graph_constructor.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/graph/graph_constructor.cc):

```
for (int i = 0; i < node_def.input_size(); ++i) {
      StringPiece input_name = node_def.input(i);
      TensorId id(ParseTensorName(input_name));
      if (opts_.input_map.count(id) == 0) {
        // If an input is not mapped, then the input should appear in the graph
        // being imported.
        auto iter = gdef_nodes_.find(id.first);
        if (iter == gdef_nodes_.end()) {
          return errors::InvalidArgument(""Node '"", node_def.name(),
                                         ""': Unknown input node '"",
                                         node_def.input(i), ""'"");
        }
        outputs_[iter->second.gdef_index].push_back(n);
      } else {
        // This input is mapped to an existing edge. Therefore this input is
        // as good as being already processed.
        --pending_count;
        DCHECK_GE(pending_count, 0);
      }
    }
```

Where if no mapping was supplied using `TF_ImportGraphDefOptionsAddInputMapping`, the validation check assumes the input nodes exist in the current import call, along side the node being imported itself, without bothering to check if the graph might already contain these nodes (which is the assumption when a mapping IS supplied).

This means that when importing a graph, unnecessary input mappings have to be supplied.

"
21921, Tensorflow lite model gives very different accuracy value compared to python model,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.10
- **Python version**:3.6
- **Bazel version (if compiling from source)**: 0.16.0
- **CUDA/cuDNN version**:N/A
- **GPU model and memory**:N/A
- **Exact command to reproduce**: See Source Code

### Describe the problem
I am using tensorflow 1.10 Python 3.6

My code is based in the premade iris classification model provided by TensorFlow. This means, I am using a Tensorflow DNN premade classifier, with the difference following difference:

10 features instead 4.
5 classes instead 3.
The test and training files can be downloaded from the following link: https://www.dropbox.com/sh/nmu8i2i8xe6hvfq/AADQEOIHH8e-kUHQf8zmmDMDa?dl=0

I have made a code to export this classifier to a tflite format, however the accuracy in the python model is higher than 75% but when exported the accuracy decrease approximately to 45% this means approximately 30% Accuracy is lost (This is too much). I have tried the code with different set of data and in all of them the accuracy after exporting decrease a lot! This made me think that something is going wrong with the TocoConverter function or that maybe I am exporting to tflite incorrectly, missing a parameter or something like that.

I share the code in which I calculate also the accuracy of the .tflite file.

I hope some of you can identify the error, or give a possible solution

### Source code / logs

```
import argparse
import tensorflow as tf

import pandas as pd
import csv

from tensorflow.python.tools import freeze_graph
from tensorflow.python.tools import optimize_for_inference_lib
import numpy as np


parser = argparse.ArgumentParser()
parser.add_argument('--batch_size', default=100, type=int, help='batch size')
parser.add_argument('--train_steps', default=1000, type=int,
                    help='number of training steps')

features_global = None
feature_spec = None

MODEL_NAME = 'myModel'

def load_data(train_path, test_path):
    """"""Returns the iris dataset as (train_x, train_y), (test_x, test_y).""""""

    with open(train_path, newline='') as f:
        reader = csv.reader(f)
        column_names = next(reader)

    y_name = column_names[-1]

    train = pd.read_csv(train_path, names=column_names, header=0)
    train_x, train_y = train, train.pop(y_name)

    test = pd.read_csv(test_path, names=column_names, header=0)
    test_x, test_y = test, test.pop(y_name)

    return (train_x, train_y), (test_x, test_y)


def train_input_fn(features, labels, batch_size):
    """"""An input function for training""""""
    # Convert the inputs to a Dataset.
    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))

    # Shuffle, repeat, and batch the examples.
    dataset = dataset.shuffle(1000).repeat().batch(batch_size)

    # Return the dataset.
    return dataset


def eval_input_fn(features, labels, batch_size):
    """"""An input function for evaluation or prediction""""""
    features=dict(features)
    if labels is None:
        # No labels, use only features.
        inputs = features
    else:
        inputs = (features, labels)

    # Convert the inputs to a Dataset.
    dataset = tf.data.Dataset.from_tensor_slices(inputs)

    # Batch the examples
    assert batch_size is not None, ""batch_size must not be None""
    dataset = dataset.batch(batch_size)

    # Return the dataset.
    return dataset


def main(argv):
    args = parser.parse_args(argv[1:])

    train_path = ""trainData.csv""
    test_path = ""testData.csv""

    # Fetch the data
    (train_x, train_y), (test_x, test_y) = load_data(train_path, test_path)

    # Load labels
    num_labels = 5

    # Feature columns describe how to use the input.
    my_feature_columns = []
    for key in train_x.keys():
        my_feature_columns.append(tf.feature_column.numeric_column(key=key))

    # Build 2 hidden layer DNN
    classifier = tf.estimator.DNNClassifier(
        feature_columns=my_feature_columns,
        # Two hidden layers of 10 nodes each.
        hidden_units=[100, 500],
        # The model must choose between 'num_labels' classes.
        optimizer=tf.train.AdagradOptimizer(learning_rate=0.003),
        n_classes=num_labels,
        model_dir=""myModel"")

    # Train the Model
    classifier.train(
        input_fn=lambda:train_input_fn(train_x, train_y,
                                                args.batch_size),
        steps=args.train_steps)

    # Evaluate the model.
    eval_result = classifier.evaluate(
        input_fn=lambda:eval_input_fn(test_x, test_y,
                                                args.batch_size))

    print('\nTest set accuracy: {accuracy:0.3f}\n'.format(**eval_result))

    # Export model
    feature_spec = tf.feature_column.make_parse_example_spec(my_feature_columns)
    serve_input_fun = tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)
    saved_model_path = classifier.export_savedmodel(
            export_dir_base=""out"",
            serving_input_receiver_fn=serve_input_fun,
            as_text=True,
            checkpoint_path=classifier.latest_checkpoint(),
        )
    tf.reset_default_graph()
    var = tf.Variable(0)
    with tf.Session() as sess:
        # First let's load meta graph and restore weights
        sess.run(tf.global_variables_initializer())
        latest_checkpoint_path = classifier.latest_checkpoint()
        saver = tf.train.import_meta_graph(latest_checkpoint_path + '.meta')
        saver.restore(sess, latest_checkpoint_path)

        input_arrays = [""dnn/input_from_feature_columns/input_layer/concat""]
        output_arrays = [""dnn/logits/BiasAdd""]

        frozen_graph_def = tf.graph_util.convert_variables_to_constants(
            sess, sess.graph_def,
            output_node_names=[""dnn/logits/BiasAdd""])

        frozen_graph = ""out/frozen_graph.pb""

        with tf.gfile.FastGFile(frozen_graph, ""wb"") as f:
                f.write(frozen_graph_def.SerializeToString())

        # save original graphdef to text file
        with open(""estimator_graph.pbtxt"", ""w"") as fp:
            fp.write(str(sess.graph_def))
        # save frozen graph def to text file
        with open(""estimator_frozen_graph.pbtxt"", ""w"") as fp:
            fp.write(str(frozen_graph_def))

        input_node_names = input_arrays
        output_node_name = output_arrays
        output_graph_def = optimize_for_inference_lib.optimize_for_inference(
                frozen_graph_def, input_node_names, output_node_name,
                tf.float32.as_datatype_enum)

        final_model_path = 'out/opt_' + MODEL_NAME + '.pb'
        with tf.gfile.FastGFile(final_model_path, ""wb"") as f:
            f.write(output_graph_def.SerializeToString())

        tflite_file = ""out/iris.tflite""

        converter = tf.contrib.lite.TocoConverter.from_frozen_graph(final_model_path, input_arrays, output_arrays, input_shapes={""dnn/input_from_feature_columns/input_layer/concat"": [1, 10]})
        tflite_model = converter.convert()
        open(tflite_file, ""wb"").write(tflite_model)

        interpreter = tf.contrib.lite.Interpreter(model_path=tflite_file)
        interpreter.allocate_tensors()

        # Get input and output tensors.
        input_details = interpreter.get_input_details()
        output_details = interpreter.get_output_details()

        # Test model on random input data.
        input_shape = input_details[0]['shape']
        # change the following line to feed into your own data.
        input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
        resultlist = list()
        df = pd.read_csv(test_path)
        expected = df.iloc[:, -1].values.tolist()
        with open(test_path, newline='') as f:
            reader = csv.reader(f)
            column_names = next(reader)
            for x in range(1, len(expected)):
                linea = next(reader)
                linea = linea[:len(linea) - 1]
                input_data2 = np.array(linea, dtype=np.float32)
                interpreter.set_tensor(input_details[0]['index'], [input_data2])
                interpreter.invoke()
                output_data = interpreter.get_tensor(output_details[0]['index'])
                #print(output_data)
                max = 0;
                longitud = len(output_data[0])

                for k in range(0, longitud):
                    if (output_data[0][k] > output_data[0][max]):
                        max = k
                resultlist.append(max)
            print(resultlist)

        coincidences = 0
        for pred_dict, expec in zip(resultlist, expected):
            if pred_dict == expec:
                coincidences = coincidences + 1

        print(""tflite Accuracy: "" + str(coincidences / len(expected)))


if __name__ == '__main__':
    tf.logging.set_verbosity(tf.logging.INFO)
    tf.app.run(main)
```"
21920,BeamSearchDecoder bug,"it seems that the beam search not end with 'eos' symbol. So, there are same sentences generated by beam search. who can slove this problem?
code:
`                    inference_decoder = BeamSearchDecoder(
                        cell=self.decoder_cell,
                        embedding=embed_and_input_proj,
                        start_tokens=start_tokens,
                        end_token=end_token,
                        initial_state=self.decoder_initial_state,
                        beam_width=self.beam_width,
                        output_layer=self.decoder_output_projection,
                    )`

beam_size=5,result:
sentence1  eos unk unk unk unk unk
sentence2  eos  eos  eos eos eos eos eos eos eos eos eos 
sentence3  eos eos eos eos eos eos eos eos eos eos unk
sentence4  eos eos eos eos eos eos eos unk unk unk unk 
sentence5  eos unk unk unk unk unk unk unk unk unk unk

above. sentence2,sentence3,sentence4,sentence5 is alike.
"
21919,Error: Can not allocate memory for the given inputs: external/tensorflow/tensorflow/contrib/lite/kernels/sub.cc:48 NumDimensions(input1) != NumDimensions(input2) (4 != 0),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
I only replace the model we retained refer to tensorflow-for-poets.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 14.04.3 LTS
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
LG G4
- **TensorFlow installed from (source or binary):binary
- **TensorFlow version (use command below):1.7.1
- **Python version:2.7.6
- **Bazel version (if compiling from source): NA
- **GCC/Compiler version (if compiling from source): NA
- **CUDA/cuDNN version: NA
- **GPU model and memory: NA
- **Exact command to reproduce:


### Describe the problem
Hi,
I retrained a model refer to tensorflow-for-poets, the retrain command is:

> python -m scripts.retrain \
  --bottleneck_dir=tf_files/bottlenecks \
  --model_dir=tf_files/models/ \
  --summaries_dir=tf_files/training_summaries/mobilenet_v2_140_224 \
  --output_graph=tf_files/retrained_graph.pb \
  --output_labels=tf_files/retrained_labels.txt \
  --tfhub_module=https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/feature_vector/2 \
  --how_many_training_steps=8000 \
  --learning_rate=0.005 \
  --flip_left_right True \
  --random_crop=10 \
  --random_scale=10 \
  --random_brightness=10 \
  --image_dir=tf_files/tf_Images

retrained_graph.pb works fine in our Android APP.
Then I converted "".pb"" to "".lite"" with this command:

> toco \
  --input_file=tf_files/retrained_graph.pb \
  --output_file=tf_files/retrained_graph.lite \
  --input_format=TENSORFLOW_GRAPHDEF \
  --output_format=TFLITE \
  --input_shape=1,224,224,3 \
  --input_array=Placeholder \
  --output_array=final_result \
  --inference_type=FLOAT \
  --input_data_type=FLOAT

But retrained_graph.lite **can not** work in our Android APP(refer to tensorflow-for-poets-2\android\tflite), the error is:

> 08-23 18:15:33.202  2457  2475 E AndroidRuntime: java.lang.NullPointerException: Can not allocate memory for the given inputs: external/tensorflow/tensorflow/contrib/lite/kernels/sub.cc:48 NumDimensions(input1) != NumDimensions(input2) (4 != 0)
08-23 18:15:33.202  2457  2475 E AndroidRuntime: 	at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)
08-23 18:15:33.202  2457  2475 E AndroidRuntime: 	at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:95)
08-23 18:15:33.202  2457  2475 E AndroidRuntime: 	at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:113)
08-23 18:15:33.202  2457  2475 E AndroidRuntime: 	at org.tensorflow.lite.Interpreter.run(Interpreter.java:94)

**Please help to analyze that why this error occurs? Thanks!**"
21918,Tensorflow MKL build on macOS segfaults on kernel_tests:map_dataset_op_test,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS, 10.10.5
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.10.0
- **Python version**: Doesn't really matter. 2.7/3.5/3.6
- **Bazel version (if compiling from source)**: 0.15.2
- **GCC/Compiler version (if compiling from source)**: Clang 4.0.1
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: Build with bazel --config=mkl

### Describe the problem
Build tensorflow on macOS with mkl enabled. 4/9xx tests fail. One of them is a sefgault.

### Source code / logs
```
FAILED (failures=1)
exec ${PAGER:-/usr/bin/less} ""$0"" || exit 1
Executing tests from //bazel_pip/tensorflow/python/data/kernel_tests:map_dataset_op_test
-----------------------------------------------------------------------------
2018-08-24 05:45:39.432026: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX
2018-08-24 05:45:39.433249: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.
...2018-08-24 05:45:40.202328: W tensorflow/core/framework/op_kernel.cc:1275] OP_REQUIRES failed at resource_variable_ops.cc:391 : Not found: Resource localhost/counter/N10tensorflow3VarE does not exist.
.........*** Received signal 11 ***
*** BEGIN MANGLED STACK TRACE ***
0   libtensorflow_framework.so          0x0000000114e4fa04 _ZN10tensorflow7testing19SafePrintStackTraceEv + 68
1   libtensorflow_framework.so          0x0000000114e4f970 _ZN10tensorflow7testingL17StacktraceHandlerEiP9__siginfoPv + 144
2   libsystem_platform.dylib            0x00007fff896c2f1a _sigtramp + 26
3   ???                                 0x0000000000000000 0x0 + 0
4   libtensorflow_framework.so          0x0000000115151b86 _ZN5nsync18nsync_cv_broadcastEPNS_11nsync_cv_s_E + 70
Segmentation fault: 11
```
"
21917,Bazel Error When Installing tf-gpu or tf-cpu 1.3 from source. ,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Not Custom Code
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.4 LTS
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: tf-cpu/tf-gpu 1.3
- **Python version**: Python 3.5.2
- **Bazel version (if compiling from source)**: 0.16.1
- **GCC/Compiler version (if compiling from source)**: gcc-5
- **CUDA/cuDNN version**: 9.0.176/ 7.1.3
- **GPU model and memory**: GeForce GTX 1080 ti - 11GB
- **Exact command to reproduce**: 
bazel build --config=opt --config=cuda --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow/tools/pip_package:build_pip_package 

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I have CUDA 9.0.176 and CuDNN 7.1.3 installed. To use tensorflow with gpu I have to install tf from Source as tf 1.3 does not support mentioned CUDA versions( I need tf 1.3 and not other higher versions). I am able to configure the installation but I get this error when trying to build the pip package from bazel. 

bazel build --config=opt --config=cuda --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow/tools/pip_package:build_pip_package 
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
ERROR: /home/vijay/.cache/bazel/_bazel_vijay/3b3dec1758244c0f8d4b66ae324b997d/external/local_config_sycl/sycl/BUILD:4:1: First argument of 'load' must be a label and start with either '//', ':', or '@'.
ERROR: /home/vijay/.cache/bazel/_bazel_vijay/3b3dec1758244c0f8d4b66ae324b997d/external/local_config_sycl/sycl/BUILD:6:1: First argument of 'load' must be a label and start with either '//', ':', or '@'.
ERROR: /home/vijay/.cache/bazel/_bazel_vijay/3b3dec1758244c0f8d4b66ae324b997d/external/local_config_sycl/sycl/BUILD:4:1: file 'platform' was not correctly loaded. Make sure the 'load' statement appears in the global scope in your file
ERROR: /home/vijay/.cache/bazel/_bazel_vijay/3b3dec1758244c0f8d4b66ae324b997d/external/local_config_sycl/sycl/BUILD:6:1: file 'platform' was not correctly loaded. Make sure the 'load' statement appears in the global scope in your file
ERROR: /home/vijay/.cache/bazel/_bazel_vijay/3b3dec1758244c0f8d4b66ae324b997d/external/local_config_sycl/sycl/BUILD:30:9: Traceback (most recent call last):
	File ""/home/vijay/.cache/bazel/_bazel_vijay/3b3dec1758244c0f8d4b66ae324b997d/external/local_config_sycl/sycl/BUILD"", line 27
		cc_library(name = ""syclrt"", srcs = [sycl_libr..."")], <3 more arguments>)
	File ""/home/vijay/.cache/bazel/_bazel_vijay/3b3dec1758244c0f8d4b66ae324b997d/external/local_config_sycl/sycl/BUILD"", line 30, in cc_library
		sycl_library_path
name 'sycl_library_path' is not defined
ERROR: /home/vijay/.cache/bazel/_bazel_vijay/3b3dec1758244c0f8d4b66ae324b997d/external/protobuf/BUILD:460:1: Traceback (most recent call last):
	File ""/home/vijay/.cache/bazel/_bazel_vijay/3b3dec1758244c0f8d4b66ae324b997d/external/protobuf/BUILD"", line 460
		cc_proto_library(name = ""cc_test_protos"", srcs = (L...), <4 more arguments>)
	File ""/home/vijay/.cache/bazel/_bazel_vijay/3b3dec1758244c0f8d4b66ae324b997d/external/protobuf/protobuf.bzl"", line 248, in cc_proto_library
		cc_libs += [default_runtime]
trying to mutate a frozen object
ERROR: /home/vijay/.cache/bazel/_bazel_vijay/3b3dec1758244c0f8d4b66ae324b997d/external/protobuf/BUILD:761:1: Traceback (most recent call last):
	File ""/home/vijay/.cache/bazel/_bazel_vijay/3b3dec1758244c0f8d4b66ae324b997d/external/protobuf/BUILD"", line 761
		py_proto_library(name = ""python_specific_test_pro..."", <6 more arguments>)
	File ""/home/vijay/.cache/bazel/_bazel_vijay/3b3dec1758244c0f8d4b66ae324b997d/external/protobuf/protobuf.bzl"", line 374, in py_proto_library
		py_libs += [default_runtime]
trying to mutate a frozen object
ERROR: /home/vijay/.cache/bazel/_bazel_vijay/3b3dec1758244c0f8d4b66ae324b997d/external/local_config_sycl/sycl/BUILD:39:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '@local_config_sycl//sycl:sycl'
ERROR: /home/vijay/speech_tts/multitacotron/tensorflow/third_party/eigen3/BUILD:20:1: Target '@local_config_sycl//sycl:sycl' contains an error and its package is in error and referenced by '//third_party/eigen3:eigen3'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis failed
INFO: Elapsed time: 13.227s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (81 packages loaded)
    currently loading: tensorflow/cc
 
**This my tf source configuration results:** 

$ ./configure
You have bazel 0.16.1 installed.
Please specify the location of python. [Default is /home/vijay/speech_tts/multitacotron/multitaco/bin/python]: 
Found possible Python library paths:
  /home/vijay/speech_tts/multitacotron/multitaco/lib/python3.5/site-packages
Please input the desired Python library path to use.  Default is [/home/vijay/speech_tts/multitacotron/multitaco/lib/python3.5/site-packages]

Using python library path: /home/vijay/speech_tts/multitacotron/multitaco/lib/python3.5/site-packages
Do you wish to build TensorFlow with MKL support? [y/N] n
No MKL support will be enabled for TensorFlow
Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: 
Do you wish to use jemalloc as the malloc implementation? [Y/n] y
jemalloc enabled
Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] n
No Google Cloud Platform support will be enabled for TensorFlow
Do you wish to build TensorFlow with Hadoop File System support? [y/N] n
No Hadoop File System support will be enabled for TensorFlow
Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] n
No XLA JIT support will be enabled for TensorFlow
Do you wish to build TensorFlow with VERBS support? [y/N] n
No VERBS support will be enabled for TensorFlow
Do you wish to build TensorFlow with OpenCL support? [y/N] n
No OpenCL support will be enabled for TensorFlow
Do you wish to build TensorFlow with CUDA support? [y/N] y
CUDA support will be enabled for TensorFlow
Do you want to use clang as CUDA compiler? [y/N] n
nvcc will be used as CUDA compiler
Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 9.0.176
Please specify the location where CUDA 9.0.176 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 
Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: /usr/bin/gcc-5
Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 6.0]: 7.1.3
Please specify the location where cuDNN 7.1.3 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /usr/lib/x86_64-linux-gnu
Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size.
[Default is: ""6.1,6.1,6.1,6.1""]: 
Do you wish to build TensorFlow with MPI support? [y/N] n
MPI support will not be enabled for TensorFlow
Configuration finished



### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

Things that I have tried: 

1) Different versions of Bazel: 0.8.0, 0.9.0, 0.11.1, 0.16.1
2) Cleared the cache before configuration: rm -rf ~/.cache/*
3) Tried Configuring tf-CPU instead of tf-gpu to check if it works but it doesn't. Gives me the same error. 
4) this : https://stackoverflow.com/questions/47688252/tensorflow-trying-to-mutate-a-frozen-object-bazel

I am not really sure if it is a tensorflow issue but nothing seems to be working.

 Any help is appreciated. Thanks! "
21915,Batch normalization implemented incorrectly in canned DNN estimators,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: all
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10
- **Python version**: 3.6.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
Batch normalization is implemented incorrectly in canned DNN estimators.  In _dnn_logit_fn_builder, batch normalization is applied after the dense layer which includes the activation function.  But batch normalization should be applied to the inputs to a layer, prior to the activation function.  See the original paper by Ioffe, Szegedy: "" ... we focus on transforms that consist of an affine transformation followed by anelement-wise nonlinearity:
z = g(Wu + b)
where W and b are learned parameters of the model, and g() is the nonlinearity such as sigmoid or ReLU. ...  We add the BN transform immediately before the nonlinearity, by normalizing x = Wu+ b.""

Also see the many published reference implementations of batch normalization.

The current implementation is not only incorrect but produces very poor results.  

### Source code / logs
"
21914,Read tensor from open cv numpy array with preprocessing for inference,"`label_image.py` implementation for inference of an image works great for images on disk. But i have a case wherein I am reading a streaming video from a webcam and I would like to run inference on each image frame to detect the object in the camera feed.

Currently [label_image.py][1] only accepts an image on disk and using `read_tensor_from_image_file` converts it into a Tensor. How do i get a Tensor with the necessary pre-processing as being done in `read_tensor_from_image_file` from the Open CV image frame that i have in memory?

  [1]: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/label_image.py#L38"
21913,TensorFlow License is missing C-API Tarballs / Zips,"### System information
N/A

### Describe the problem
LICENSE contained in `https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-${TF_TYPE}-${OS}-x86_64-1.10.1.tar.gz`  linked [here](https://www.tensorflow.org/install/install_c) is missing TensorFlow [license](https://github.com/tensorflow/tensorflow/blob/master/LICENSE). 

### Source code / logs
Download https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-1.10.1.tar.gz. 
Examine LICENSE (located at include\tensorflow\c\LICENSE).
This file contains all the licenses for 3rd party components but is missing the LICENSE for TensorFlow itself.
The problem appears to be with the packaging code [here](https://github.com/tensorflow/tensorflow/blob/7df18e0e6f8629dd097a7b71845d9e13d926944a/tensorflow/tools/lib_package/BUILD).

We'd like to understand what the LICENSE is for these `.tar.gz` files, does it permit redistribution?  Also it would be good to have the files updated to contain whatever LICENSE is appropriate.

Thanks! "
21911,"No way to run it, try py 3.6.5 py 3.6.6 py 3.7.0  32 and 64 from pip and from sourse all stuff in PATH No module named '_pywrap_tensorflow' anyway help me plz WIN 10 how it could be the most powerful library and so many people couldnt afford it","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21910,"No way to run it, try py 3.6.5 py 3.6.6 py 3.7.0 all stuff in PATH ","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21909,ppc64le: //tensorflow/contrib/distributions/python/kernel_tests/util:correlation_matrix_volumes_test times out,"Please assign this issue to me

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ppc64le Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: master branch from 8/27 (Last commit 514f65a0cab6fb98bba6d69904ba930ff1c46247)
- **Python version**: both 2.7 and 3.6
- **Bazel version (if compiling from source)**: 0.15.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 9.0, 7
- **GPU model and memory**: 4 V100 GPUs with 16 GB of memory each
- **Exact command to reproduce**:
bazel test --config=cuda --test_tag_filters=-no_oss,-oss_serial,-no_gpu,-benchmark-test --test_timeout 300,450,1200,3600 --local_test_jobs=4 --test_output=errors --build_tests_only //tensorflow/tools/ci_build/gpu_build:parallel_gpu_execute //tensorflow/contrib/distributions/python/kernel_tests/util:correlation_matrix_volumes_test

### Describe the problem
correlation_matrix_volumes_test times out after 450 seconds on ppc64le. On x86 the test finishes around 420 seconds. If I increase the timeouts the test finishes about 920 seconds.

I believe tensorflow/control/distributions is being moved to https://github.com/tensorflow/probability/, as of now this test doesn't exist in tensorflow/probability so this issue might be mute if the test is being removed.

### Source code / logs
```
==================== Test output for //tensorflow/contrib/distributions/python/kernel_tests/util:correlation_matrix_volumes_test:
Running test /home/wdirons/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/python/kernel_tests/util/correlation_matrix_volumes_test.runfiles/org_tensorflow/tensorflow/contrib/distributions/python/kernel_tests/util/correlation_matrix_volumes_test  on GPU 0
2018-08-27 20:33:54.300180: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties:
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0004:04:00.0
totalMemory: 15.75GiB freeMemory: 15.34GiB
2018-08-27 20:33:54.300242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0
2018-08-27 20:33:54.581448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-27 20:33:54.581497: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0
2018-08-27 20:33:54.581505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N
2018-08-27 20:33:54.581977: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4838 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0004:04:00.0, compute capability: 7.0)
2018-08-27 20:33:54.799090: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x3fc7c590
.2018-08-27 20:38:37.918692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0
2018-08-27 20:38:37.918753: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-27 20:38:37.918764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0
2018-08-27 20:38:37.918808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N
2018-08-27 20:38:37.919272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4838 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0004:04:00.0, compute capability: 7.0)
.2018-08-27 20:39:40.471076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0
2018-08-27 20:39:40.471149: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-27 20:39:40.471160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0
2018-08-27 20:39:40.471212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N
2018-08-27 20:39:40.471724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4838 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0004:04:00.0, compute capability: 7.0)
.2018-08-27 20:41:13.685115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0
2018-08-27 20:41:13.685149: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-27 20:41:13.685160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0
2018-08-27 20:41:13.685202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N
2018-08-27 20:41:13.685628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4838 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0004:04:00.0, compute capability: 7.0)
Terminated
================================================================================
Target //tensorflow/contrib/distributions/python/kernel_tests/util:correlation_matrix_volumes_test up-to-date:
  bazel-bin/tensorflow/contrib/distributions/python/kernel_tests/util/correlation_matrix_volumes_test
INFO: Elapsed time: 921.644s, Critical Path: 754.35s
INFO: 3088 processes: 3088 local.
INFO: Build completed, 1 test FAILED, 3090 total actions
//tensorflow/contrib/distributions/python/kernel_tests/util:correlation_matrix_volumes_test TIMEOUT in 465.0s
  /home/wdirons/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/testlogs/tensorflow/contrib/distributions/python/kernel_tests/util/correlation_matrix_volumes_test/test.log

```"
21908,import tensorflow --> Segmentation fault,"OS: CentOS 6.2
Tensorflow installed from source.
Tensorflow version: 1.10
Python version: 2.7
Bazel version: 0.16.0
GCC version: 6.3.0
CUDA 8.0
cuDNN: 6.0
GPU: GeForce GTX 770

Hi,
I have installed tensorflow with the following commands:

./configure
bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures
bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
pip2.7 install /tmp/tensorflow_pkg/tensorflow-1.4.0-cp27-cp27m-linux_x86_64.whl 

And when i tried to import it:
import tensorflow
Segmentation fault

Thanks"
21907,Update Docker description to state CPU requirements,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a
- **TensorFlow installed from (source or binary)**: binary (Docker tensorflow/tensorflow)
- **TensorFlow version (use command below)**: 1.10.1 (from core/public/version.h)
- **Python version**:2.7.12
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: 
\# docker run -it tensorflow/tensorflow bash
root@xxxxxxx:/notebooks# python2.7 -c ""import tensorflow as tf""
Illegal instruction

### Describe the problem
This is a request to update documentation to include minimal system requirements, especially the description for Docker images that include prebuilt binaries.

For example, the main Docker image on DockerHub (tensorflow/tensorflow, 19M downloads) [currently does not operate on CPU's without AVX support](https://github.com/tensorflow/tensorflow/issues/17411).  This includes CPUs sold as recently as 4 years ago.

This requirement should be stated in the Docker image description displayed in DockerHub.  Perhaps also include a link to an alternative docker image that users with non-AVX CPUs may utilize to build from source.

### Source code / logs
n/a
"
21904,Simple memory placement optimizations not performed when targeting TPUs,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 17.10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: r1.9
- **Python version**: N/A
- **Bazel version (if compiling from source)**: 0.11.1
- **GCC/Compiler version (if compiling from source)**: 7.2.0
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem

I'm playing with compiling Julia code to TF graphs for TPU offload. At the moment, I have the two functions:
```
f(X, Y) = Y * (X + c_1*X) * (X + c_2*X) * (X + c_3*X) * (X + c_4*X) * (X + c_5*X) * (X + c_6*X) * (X + c_7*X) * (X + c_8*X)
g(X, Y) = Y * (X + c_1*X) * (X + c_2*X) * (X + c_3*X) * (X + c_4*X) * (X + c_5*X) * (X + c_6*X) * (X + c_7*X) * (X + c_8*X) * (X + c_9 * X)
```
(Note the `*` is matrix-multiply between two matrices). Now, when I run (the TF graph resulting from the compilation of) these two functions against a cloud TPU (Tensorflow version 1.9), with `X`,`Y` both 10,000 x 10,000 Float32 matrices, `c_i` Float32 constants, `f` runs fine, but `g` delivers the following out of memory error:

```
Tensorflow error: Status: Failed to allocate request for 385.74MiB (404480000B) on device ordinal 0

Total hbm usage >= 7.88G:
    reserved                 528.00M
    persistent allocations     7.37G (7.4% fragmentation)
    program                    33.0K

Persistent allocations include some or all of:
    arguments                771.48M (98.9% utilization)
    output                   385.75M (98.9% utilization) [may share some memory with arguments]
```

This seems to indicate to me that it's trying to reserve space for all the intermediate matrix results in high bandwidth memory. I would have expected (and @learyg mentioned that it should) that XLA would have optimized the memory placement in order to avoid this problem. 

Looking at the profile output for the `f` function, it seems to be doing all the broadcasts in order, followed by grouping all the matmuls (which would indeed blow the memory budget):
![screen shot 2018-08-27 at 1 02 37 pm](https://user-images.githubusercontent.com/1291671/44673466-a087b800-a9f9-11e8-9121-86311b5fe40f.png)

I also note that gap between the broadcasts and the matmuls, during which time it seems to be running XLA again. To me that indicates that XLA is not getting the full TF graph on the server side and thus cannot optimize the memory placement properly. I'm hoping that there's just some missing setting in the TF graph def that would allow it to do its job.

### Source code / logs
I was able to reproduce this in raw Tensorflow.jl (which should have a more or less 1:1 API correspondence to python, though of course there may be a bug in the bindings also - cc @malmaud @oxinabox), using code like this:
```
using TensorFlow
using TensorFlow: Device

sess = Session(Graph(); target=""grpc://localhost:8470"")

with_device(TensorFlow.NamedDevice(""/job:tpu_worker/replica:0/task:0/device:TPU:0"")) do
    global X,Y,Z
    X = placeholder(Float32, shape=[10000, 10000])
    Z = Y = placeholder(Float32, shape=[10000, 10000])
    for i = 1:8 # Fails for 1:9
        Z *= X + rand(Float32)*X
    end
end

x, z = rand(Float32, 10000, 10000), rand(Float32, 10000, 10000)
run(sess, Z, Dict(X=>x, Y=>z))
```
A serialized GraphDef of the graph for `f` is at https://gist.github.com/Keno/9321e0f1f278b04fbf3b878e551b3ecb

Is there something missing from this GraphDef that would enable XLA to perform the appropriate memory placement optimization on the device?"
21901,Add gradient for tf.broadcast_to,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: binary (pip tensorflow-gpu)
- **TensorFlow version (use command below)**: v1.10.0-0-g656e7a2b34 1.10.0
- **Python version**: 3.6.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: CUDA 9.0 / CUDNN 7.1.3
- **GPU model and memory**:
- **Exact command to reproduce**:
```
python -c ""import tensorflow as tf; a = tf.zeros([1]); b = tf.broadcast_to(a, [2]); tf.gradients(b, a)""
```
Result:
LookupError: gradient registry has no entry for: BroadcastTo

### Describe the problem
Problem: tf.broadcast_to (BroadcastTo) does not have a gradient defined (on CPU or GPU)
Feature request: Add the gradient for the op.
Relevant links:
https://github.com/tensorflow/tensorflow/pull/19753
https://github.com/tensorflow/tensorflow/pull/15243

### Source code / logs
```
Traceback (most recent call last):
  File ""/h/username/.local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 733, in _GradientsHelper
    grad_fn = ops.get_gradient_function(op)
  File ""/h/username/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2335, in get_gradient_function
    return _gradient_registry.lookup(op_type)
  File ""/h/username/.local/lib/python3.6/site-packages/tensorflow/python/framework/registry.py"", line 93, in lookup
    ""%s registry has no entry for: %s"" % (self._name, name))
LookupError: gradient registry has no entry for: BroadcastTo

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/h/username/.local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 596, in gradients
    gate_gradients, aggregation_method, stop_gradients)
  File ""/h/username/.local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 737, in _GradientsHelper
    (op.name, op.type))
LookupError: No gradient defined for operation 'BroadcastTo' (op type: BroadcastTo)
```"
21900,How is it possible to change the model in the application TF Detect?,"Hello everyone I would like to change the .pb model (and the label file) used in the TF Detect app provided in the android example. How can I do this?

Thanks a lot!"
21899,"How to install versions of tensorflow from 0,12 to 0,9 nowadays, i can't find anywhere ",
21898,AWS lib is verbose when using S3,"I'm using latest version of Tensorflow with AWS S3 as a backend for models and I have those annoying logs:

[tensorflow-server-pr-14-585f869b5f-2wln8] 2018-08-27 13:51:41.183378: I external/org_tensorflow/tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 
[tensorflow-server-pr-14-585f869b5f-2wln8] 2018-08-27 13:51:41.183471: I external/org_tensorflow/tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 
[tensorflow-server-pr-14-585f869b5f-2wln8] 2018-08-27 13:51:41.194855: I external/org_tensorflow/tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 
[tensorflow-server-pr-14-585f869b5f-2wln8] 2018-08-27 13:51:41.194963: I external/org_tensorflow/tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 
[tensorflow-server-pr-14-585f869b5f-2wln8] 2018-08-27 13:51:41.206484: I external/org_tensorflow/tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 
[tensorflow-server-pr-14-585f869b5f-2wln8] 2018-08-27 13:51:41.206714: I external/org_tensorflow/tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 
[tensorflow-server-pr-14-585f869b5f-2wln8] 2018-08-27 13:51:41.212817: E external/org_tensorflow/tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404 
[tensorflow-server-pr-14-585f869b5f-2wln8] 2018-08-27 13:51:41.212852: W external/org_tensorflow/tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer. 

The server is working as expected though. I set the log level to ERROR only but still the 404 error is happening more than every second.

I saw that there is an issue open in `tensorflow/serving` but this seems to be core/3rd party issue.

https://github.com/tensorflow/serving/issues/615


**Have I written custom code:** No
**OS Platform and Distribution:** Kubernetes 1.8 running on CENTOS
**TensorFlow installed from:** Docker image tensorflow/serving:1.10.1
**TensorFlow version:** 1.10.1
**Bazel version**: ??
**CUDA/cuDNN version**: N/A
**GPU model and memory:** N/A
**Exact command to reproduce:** 
```bash
if [ -f /etc/secrets/AWS_SECRET_ACCESS_KEY.secret ]; then
    export AWS_SECRET_ACCESS_KEY=$(echo -e $(cut -d "":"" -f 2 <<< $(cat /etc/secrets/AWS_SECRET_ACCESS_KEY.secret)))
fi

tensorflow_model_server --port=8500 --tensorflow_session_parallelism=1 --enable_model_warmup=true --model_config_file=${MODEL_CONFIG}
```
Example of model_config_file:
```json
model_config_list: {
  config: {
    name: ""model1"",
    base_path: ""s3://tensorflow-models/model1"",
    model_platform: ""tensorflow""
  },
  config: {
    name: ""model2"",
    base_path: ""s3://tensorflow-models/model2"",
    model_platform: ""tensorflow""
  }
}

```
**Mobile device:** None

Thank you
"
21897,"DistributionStrategy multi-node multi-GPU Estimator training: ""cannot assign a device for operation""","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: `Yes`
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: `Linux Ubuntu 16.04`
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: `NA`
- **TensorFlow installed from (source or binary)**: `source`
- **TensorFlow version (use command below)**:  `1.10` (`master` @ 187ed273038ccde054fa27bf19582b0cd8e5cb57)
- **Python version**: `2.7`
- **Bazel version (if compiling from source)**: `0.16.1`
- **GCC/Compiler version (if compiling from source)**: `4.8.5`
- **CUDA/cuDNN version**: `9.1.85`/`7.1.4`
- **GPU model and memory**: `4x NVIDIA P100`,  `16GB` per node
- **Exact command to reproduce**: See below

### Describe the problem
I'm trying to use a DistributionStrategy (e.g. MirroredStrategy) to train an Estimator in a multi-node, multi-GPU cluster. However, when I start two `tf.train.Server` tasks (each on a different node) and try to have the [simple Estimator example from the  `tf.contrib.distribute` source](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distribute/python/examples/simple_estimator_example.py) run across these two workers ([here's my modified example](https://github.com/jppgks/tensorflow-distribution-strategy/blob/master/multi-node/workers/simple_estimator_example.py)), I get the error below saying a device can't be assigned.

I'm suspecting that maybe my cluster setup isn't entirely correct. In that case, I would like to see how the documentation can be updated to include better instructions to run (multi-node) DistributionStrategy. I'm happy to help out with that.

_pinging @mrry @guptapriya @yuefengz @anj-s_

### Source code / logs
**Source.** 
Available on GitHub [here](https://github.com/jppgks/tensorflow-distribution-strategy/tree/master/multi-node/workers).

**Command.**
Our cluster uses PBS/Torque as job manager, hence the `qsub`, but the contents of the file are standard bash.
```bash
qsub launch_tf_workers.sh
```

**Job output.** 
```pytb
CLUSTER_SPEC: {""worker"": [""r22g35:2222"", ""r22g36:2222""]}
2018-08-27 10:18:18.155846: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-08-27 10:18:22.169890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0000:61:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-08-27 10:18:22.788786: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 1 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0000:62:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-08-27 10:18:23.424017: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 2 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0000:89:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-08-27 10:18:24.071758: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 3 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0000:8a:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-08-27 10:18:24.071884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0, 1, 2, 3
2018-08-27 10:18:25.178802: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-27 10:18:25.178891: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 1 2 3 
2018-08-27 10:18:25.178912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N Y Y Y 
2018-08-27 10:18:25.178929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 1:   Y N Y Y 
2018-08-27 10:18:25.178944: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 2:   Y Y N Y 
2018-08-27 10:18:25.178960: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 3:   Y Y Y N 
2018-08-27 10:18:25.179902: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0000:61:00.0, compute capability: 6.0)
2018-08-27 10:18:25.315544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:1 with 15123 MB memory) -> physical GPU (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0000:62:00.0, compute capability: 6.0)
2018-08-27 10:18:25.451748: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:2 with 15123 MB memory) -> physical GPU (device: 2, name: Tesla P100-SXM2-16GB, pci bus id: 0000:89:00.0, compute capability: 6.0)
2018-08-27 10:18:25.587799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:3 with 15123 MB memory) -> physical GPU (device: 3, name: Tesla P100-SXM2-16GB, pci bus id: 0000:8a:00.0, compute capability: 6.0)
2018-08-27 10:18:25.725327: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:222] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2222, 1 -> r22g36:2222}
2018-08-27 10:18:25.725775: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:381] Started server with target: grpc://localhost:2222
2018-08-27 10:18:30.874254: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-08-27 10:18:34.716434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0000:61:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-08-27 10:18:35.336941: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 1 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0000:62:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-08-27 10:18:35.969664: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 2 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0000:89:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-08-27 10:18:36.617192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 3 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0000:8a:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-08-27 10:18:36.617325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0, 1, 2, 3
2018-08-27 10:18:37.725846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-27 10:18:37.725937: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 1 2 3 
2018-08-27 10:18:37.725958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N Y Y Y 
2018-08-27 10:18:37.725974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 1:   Y N Y Y 
2018-08-27 10:18:37.725990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 2:   Y Y N Y 
2018-08-27 10:18:37.726006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 3:   Y Y Y N 
2018-08-27 10:18:37.726951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:worker/replica:0/task:1/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0000:61:00.0, compute capability: 6.0)
2018-08-27 10:18:37.863759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:worker/replica:0/task:1/device:GPU:1 with 15123 MB memory) -> physical GPU (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0000:62:00.0, compute capability: 6.0)
2018-08-27 10:18:37.999537: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:worker/replica:0/task:1/device:GPU:2 with 15123 MB memory) -> physical GPU (device: 2, name: Tesla P100-SXM2-16GB, pci bus id: 0000:89:00.0, compute capability: 6.0)
2018-08-27 10:18:38.135424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:worker/replica:0/task:1/device:GPU:3 with 15123 MB memory) -> physical GPU (device: 3, name: Tesla P100-SXM2-16GB, pci bus id: 0000:8a:00.0, compute capability: 6.0)
2018-08-27 10:18:38.272553: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:222] Initialize GrpcChannelCache for job worker -> {0 -> r22g35:2222, 1 -> localhost:2222}
2018-08-27 10:18:38.273006: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:381] Started server with target: grpc://localhost:2222
WARNING:tensorflow:Could not find a standard reader in the input pipeline(one of TextLineDataset, TFRecordDataset, FixedLengthRecordDataset).Falling back to sharding the dataset anyway. Please verifycorrectness of auto-sharding for your input.
WARNING:tensorflow:Could not find a standard reader in the input pipeline(one of TextLineDataset, TFRecordDataset, FixedLengthRecordDataset).Falling back to sharding the dataset anyway. Please verifycorrectness of auto-sharding for your input.
WARNING:tensorflow:Could not find a standard reader in the input pipeline(one of TextLineDataset, TFRecordDataset, FixedLengthRecordDataset).Falling back to sharding the dataset anyway. Please verifycorrectness of auto-sharding for your input.
WARNING:tensorflow:Could not find a standard reader in the input pipeline(one of TextLineDataset, TFRecordDataset, FixedLengthRecordDataset).Falling back to sharding the dataset anyway. Please verifycorrectness of auto-sharding for your input.
2018-08-27 10:18:46.714305: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-08-27 10:18:50.653650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0000:61:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-08-27 10:18:51.271311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 1 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0000:62:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-08-27 10:18:51.901031: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 2 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0000:89:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-08-27 10:18:52.566886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 3 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0000:8a:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-08-27 10:18:52.567011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0, 1, 2, 3
2018-08-27 10:18:53.672876: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-27 10:18:53.672965: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 1 2 3 
2018-08-27 10:18:53.672987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N Y Y Y 
2018-08-27 10:18:53.673003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 1:   Y N Y Y 
2018-08-27 10:18:53.673019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 2:   Y Y N Y 
2018-08-27 10:18:53.673035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 3:   Y Y Y N 
2018-08-27 10:18:53.673974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0000:61:00.0, compute capability: 6.0)
2018-08-27 10:18:53.809434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 15123 MB memory) -> physical GPU (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0000:62:00.0, compute capability: 6.0)
2018-08-27 10:18:53.946947: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 15123 MB memory) -> physical GPU (device: 2, name: Tesla P100-SXM2-16GB, pci bus id: 0000:89:00.0, compute capability: 6.0)
2018-08-27 10:18:54.084679: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 15123 MB memory) -> physical GPU (device: 3, name: Tesla P100-SXM2-16GB, pci bus id: 0000:8a:00.0, compute capability: 6.0)
2018-08-27 10:18:57.049981: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0, 1, 2, 3
2018-08-27 10:18:57.050183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-27 10:18:57.050217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 1 2 3 
2018-08-27 10:18:57.050244: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N Y Y Y 
2018-08-27 10:18:57.050270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 1:   Y N Y Y 
2018-08-27 10:18:57.050295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 2:   Y Y N Y 
2018-08-27 10:18:57.050319: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 3:   Y Y Y N 
2018-08-27 10:18:57.050912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0000:61:00.0, compute capability: 6.0)
2018-08-27 10:18:57.051179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 15123 MB memory) -> physical GPU (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0000:62:00.0, compute capability: 6.0)
2018-08-27 10:18:57.051476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 15123 MB memory) -> physical GPU (device: 2, name: Tesla P100-SXM2-16GB, pci bus id: 0000:89:00.0, compute capability: 6.0)
2018-08-27 10:18:57.051755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 15123 MB memory) -> physical GPU (device: 3, name: Tesla P100-SXM2-16GB, pci bus id: 0000:8a:00.0, compute capability: 6.0)
Device mapping:
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0000:61:00.0, compute capability: 6.0
/job:localhost/replica:0/task:0/device:GPU:1 -> device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0000:62:00.0, compute capability: 6.0
/job:localhost/replica:0/task:0/device:GPU:2 -> device: 2, name: Tesla P100-SXM2-16GB, pci bus id: 0000:89:00.0, compute capability: 6.0
/job:localhost/replica:0/task:0/device:GPU:3 -> device: 3, name: Tesla P100-SXM2-16GB, pci bus id: 0000:8a:00.0, compute capability: 6.0
2018-08-27 10:18:57.052199: I tensorflow/core/common_runtime/direct_session.cc:291] Device mapping:
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0000:61:00.0, compute capability: 6.0
/job:localhost/replica:0/task:0/device:GPU:1 -> device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0000:62:00.0, compute capability: 6.0
/job:localhost/replica:0/task:0/device:GPU:2 -> device: 2, name: Tesla P100-SXM2-16GB, pci bus id: 0000:89:00.0, compute capability: 6.0
/job:localhost/replica:0/task:0/device:GPU:3 -> device: 3, name: Tesla P100-SXM2-16GB, pci bus id: 0000:8a:00.0, compute capability: 6.0

2018-08-27 10:18:57.066840: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp' because the input edge from 'global_step/replica_3' is a reference connection and already has a device field set to /job:worker/replica:0/task:0/device:GPU:3
2018-08-27 10:18:57.066913: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp_1' because the input edge from 'global_step/replica_2' is a reference connection and already has a device field set to /job:worker/replica:0/task:0/device:GPU:2
2018-08-27 10:18:57.066945: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp_2' because the input edge from 'global_step/replica_1' is a reference connection and already has a device field set to /job:worker/replica:0/task:0/device:GPU:1
2018-08-27 10:18:57.066975: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp_4' because the input edge from 'global_step/replica_4' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:0
2018-08-27 10:18:57.067004: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp_5' because the input edge from 'global_step/replica_5' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:1
2018-08-27 10:18:57.067033: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp_6' because the input edge from 'global_step/replica_6' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:2
2018-08-27 10:18:57.067066: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp_7' because the input edge from 'global_step/replica_7' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:3
2018-08-27 10:18:57.067096: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp_8' because the input edge from 'dense/kernel/replica_3' is a reference connection and already has a device field set to /job:worker/replica:0/task:0/device:GPU:3
2018-08-27 10:18:57.067125: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp_9' because the input edge from 'dense/kernel/replica_2' is a reference connection and already has a device field set to /job:worker/replica:0/task:0/device:GPU:2
2018-08-27 10:18:57.067153: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp_10' because the input edge from 'dense/kernel/replica_1' is a reference connection and already has a device field set to /job:worker/replica:0/task:0/device:GPU:1
2018-08-27 10:18:57.067190: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp_12' because the input edge from 'dense/kernel/replica_4' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:0
2018-08-27 10:18:57.067219: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp_13' because the input edge from 'dense/kernel/replica_5' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:1
2018-08-27 10:18:57.067248: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp_14' because the input edge from 'dense/kernel/replica_6' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:2
2018-08-27 10:18:57.067276: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp_15' because the input edge from 'dense/kernel/replica_7' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:3
2018-08-27 10:18:57.067305: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp_16' because the input edge from 'dense/bias/replica_3' is a reference connection and already has a device field set to /job:worker/replica:0/task:0/device:GPU:3
2018-08-27 10:18:57.067334: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp_17' because the input edge from 'dense/bias/replica_2' is a reference connection and already has a device field set to /job:worker/replica:0/task:0/device:GPU:2
2018-08-27 10:18:57.067362: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp_18' because the input edge from 'dense/bias/replica_1' is a reference connection and already has a device field set to /job:worker/replica:0/task:0/device:GPU:1
2018-08-27 10:18:57.067397: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp_20' because the input edge from 'dense/bias/replica_4' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:0
2018-08-27 10:18:57.067426: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp_21' because the input edge from 'dense/bias/replica_5' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:1
2018-08-27 10:18:57.067454: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp_22' because the input edge from 'dense/bias/replica_6' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:2
2018-08-27 10:18:57.067482: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables/VarIsInitializedOp_23' because the input edge from 'dense/bias/replica_7' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:3
2018-08-27 10:18:57.067512: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp' because the input edge from 'global_step/replica_3' is a reference connection and already has a device field set to /job:worker/replica:0/task:0/device:GPU:3
2018-08-27 10:18:57.067541: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp_1' because the input edge from 'global_step/replica_2' is a reference connection and already has a device field set to /job:worker/replica:0/task:0/device:GPU:2
2018-08-27 10:18:57.067569: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp_2' because the input edge from 'global_step/replica_1' is a reference connection and already has a device field set to /job:worker/replica:0/task:0/device:GPU:1
2018-08-27 10:18:57.067598: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp_4' because the input edge from 'global_step/replica_4' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:0
2018-08-27 10:18:57.067626: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp_5' because the input edge from 'global_step/replica_5' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:1
2018-08-27 10:18:57.067654: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp_6' because the input edge from 'global_step/replica_6' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:2
2018-08-27 10:18:57.067683: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp_7' because the input edge from 'global_step/replica_7' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:3
2018-08-27 10:18:57.067714: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp_8' because the input edge from 'dense/kernel/replica_3' is a reference connection and already has a device field set to /job:worker/replica:0/task:0/device:GPU:3
2018-08-27 10:18:57.067743: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp_9' because the input edge from 'dense/kernel/replica_2' is a reference connection and already has a device field set to /job:worker/replica:0/task:0/device:GPU:2
2018-08-27 10:18:57.067771: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp_10' because the input edge from 'dense/kernel/replica_1' is a reference connection and already has a device field set to /job:worker/replica:0/task:0/device:GPU:1
2018-08-27 10:18:57.067800: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp_12' because the input edge from 'dense/kernel/replica_4' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:0
2018-08-27 10:18:57.067828: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp_13' because the input edge from 'dense/kernel/replica_5' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:1
2018-08-27 10:18:57.067857: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp_14' because the input edge from 'dense/kernel/replica_6' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:2
2018-08-27 10:18:57.067885: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp_15' because the input edge from 'dense/kernel/replica_7' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:3
2018-08-27 10:18:57.067914: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp_16' because the input edge from 'dense/bias/replica_3' is a reference connection and already has a device field set to /job:worker/replica:0/task:0/device:GPU:3
2018-08-27 10:18:57.067942: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp_17' because the input edge from 'dense/bias/replica_2' is a reference connection and already has a device field set to /job:worker/replica:0/task:0/device:GPU:2
2018-08-27 10:18:57.067970: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp_18' because the input edge from 'dense/bias/replica_1' is a reference connection and already has a device field set to /job:worker/replica:0/task:0/device:GPU:1
2018-08-27 10:18:57.067999: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp_20' because the input edge from 'dense/bias/replica_4' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:0
2018-08-27 10:18:57.068030: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp_21' because the input edge from 'dense/bias/replica_5' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:1
2018-08-27 10:18:57.068059: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp_22' because the input edge from 'dense/bias/replica_6' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:2
2018-08-27 10:18:57.068087: I tensorflow/core/common_runtime/placer.cc:747] Ignoring device specification /job:worker/replica:0/task:0/device:GPU:0 for node 'report_uninitialized_variables_1/VarIsInitializedOp_23' because the input edge from 'dense/bias/replica_7' is a reference connection and already has a device field set to /job:worker/replica:0/task:1/device:GPU:3
Traceback (most recent call last):
  File ""simple_estimator_example.py"", line 128, in <module>
    tf.app.run()
  File ""/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""simple_estimator_example.py"", line 113, in main
    estimator.train(input_fn=input_fn, steps=10)
  File ""/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 354, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1177, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File ""/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1320, in _train_model_distributed
    saving_listeners)
  File ""/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1400, in _train_with_estimator_spec
    log_step_count_steps=self._config.log_step_count_steps) as mon_sess:
  File ""/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 504, in MonitoredTrainingSession
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 920, in __init__
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__
    self._sess = _RecoverableSession(self._coordinated_creator)
  File ""/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1106, in __init__
    _WrappedSession.__init__(self, self._create_session())
  File ""/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1111, in _create_session
    return self._sess_creator.create_session()
  File ""/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session
    self.tf_sess = self._session_creator.create_session()
  File ""/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 566, in create_session
    init_fn=self._scaffold.init_fn)
  File ""/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 287, in prepare_session
    sess.run(init_op, feed_dict=init_feed_dict)
  File ""/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 887, in run
    run_metadata_ptr)
  File ""/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1110, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1286, in _do_run
    run_metadata)
  File ""/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1308, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'hooks/steps_per_run/IsInitialized/VarIsInitializedOp': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:worker/replica:0/task:0/device:GPU:0'
Colocation Debug Info:
Colocation group had the following types and devices: 
ReadVariableOp: GPU CPU 
AssignVariableOp: CPU 
VarIsInitializedOp: GPU CPU 
VarHandleOp: CPU 
Const: GPU CPU 

Colocation members and user-requested devices:
  hooks/steps_per_run/Initializer/ones (Const) 
  hooks/steps_per_run (VarHandleOp) 
  hooks/steps_per_run/IsInitialized/VarIsInitializedOp (VarIsInitializedOp) 
  hooks/steps_per_run/Assign (AssignVariableOp) 
  hooks/steps_per_run/Read/ReadVariableOp (ReadVariableOp) 
  report_uninitialized_variables/VarIsInitializedOp_24 (VarIsInitializedOp) /job:worker/replica:0/task:0/device:GPU:0

	 [[{{node hooks/steps_per_run/IsInitialized/VarIsInitializedOp}} = VarIsInitializedOp[](hooks/steps_per_run)]]

Caused by op u'hooks/steps_per_run/IsInitialized/VarIsInitializedOp', defined at:
  File ""simple_estimator_example.py"", line 128, in <module>
    tf.app.run()
  File ""/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""simple_estimator_example.py"", line 113, in main
    estimator.train(input_fn=input_fn, steps=10)
  File ""/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 354, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1177, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File ""/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1240, in _train_model_distributed
    steps_per_run_variable = training.get_or_create_steps_per_run_variable()
  File ""/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/training/basic_session_run_hooks.py"", line 309, in get_or_create_steps_per_run_variable
    use_resource=True)
  File ""/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 1484, in get_variable
    aggregation=aggregation)
  File ""/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 1234, in get_variable
    aggregation=aggregation)
  File ""/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 538, in get_variable
    aggregation=aggregation)
  File ""/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 492, in _true_getter
    aggregation=aggregation)
  File ""/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 920, in _get_single_variable
    aggregation=aggregation)
  File ""/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/ops/variables.py"", line 131, in __call__
    return cls._variable_call(*args, **kwargs)
  File ""/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/ops/variables.py"", line 127, in _variable_call
    aggregation=aggregation)
  File ""/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/ops/variables.py"", line 106, in <lambda>
    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)
  File ""/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 2418, in default_variable_creator
    import_scope=import_scope)
  File ""/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/ops/variables.py"", line 133, in __call__
    return super(VariableMetaclass, cls).__call__(*args, **kwargs)
  File ""/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 297, in __init__
    constraint=constraint)
  File ""/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 455, in _init_from_args
    gen_resource_variable_ops.var_is_initialized_op(self._handle))
  File ""/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py"", line 1263, in var_is_initialized_op
    ""VarIsInitializedOp"", resource=resource, name=name)
  File ""/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3263, in create_op
    op_def=op_def)
  File ""/data/leuven/319/vsc31962/miniconda3/envs/py27-tf-source/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1751, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'hooks/steps_per_run/IsInitialized/VarIsInitializedOp': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:worker/replica:0/task:0/device:GPU:0'
Colocation Debug Info:
Colocation group had the following types and devices: 
ReadVariableOp: GPU CPU 
AssignVariableOp: CPU 
VarIsInitializedOp: GPU CPU 
VarHandleOp: CPU 
Const: GPU CPU 

Colocation members and user-requested devices:
  hooks/steps_per_run/Initializer/ones (Const) 
  hooks/steps_per_run (VarHandleOp) 
  hooks/steps_per_run/IsInitialized/VarIsInitializedOp (VarIsInitializedOp) 
  hooks/steps_per_run/Assign (AssignVariableOp) 
  hooks/steps_per_run/Read/ReadVariableOp (ReadVariableOp) 
  report_uninitialized_variables/VarIsInitializedOp_24 (VarIsInitializedOp) /job:worker/replica:0/task:0/device:GPU:0

	 [[{{node hooks/steps_per_run/IsInitialized/VarIsInitializedOp}} = VarIsInitializedOp[](hooks/steps_per_run)]]
```
"
21896,EvalSpec's param throttle_secs in tf.estimator.train_and_evaluate,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04)
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**: 1.10.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

I use [tf.Estimator](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator) for training my models.
Here is part of my source code: 
```
train_spec = tf.estimator.TrainSpec(input_fn=lambda: input_fn(.....))

eval_spec = tf.estimator.EvalSpec(input_fn=lambda: input_fn(...), throttle_secs=NUM_OF_SECONDS)

tf.estimator.train_and_evaluate(estimator=estimator, train_spec=train_spec, eval_spec=eval_spec)
```

We can see this part of logs after start of training in Tensorflow 1.9.0:
```
INFO:tensorflow:Running training and evaluation locally (non-distributed).
INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after MANY secs (eval_spec.throttle_secs) or training is finished
```

in Tensorflow 1.10.0
```
INFO:tensorflow:Running training and evaluation locally (non-distributed).
INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.
```
My question is how does  param throtlle_secs work now? Because, there is reason to start evaluation process after many steps (for example 1M steps). In previous versions set many seconds to throttle_secs helps me. But in 1.10.0 set many seconds to save_checkpoints_secs is risky.

and how does new release [Improved local run behavior in tf.estimator.train_and_evaluate which does not reload checkpoints for evaluation.](https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md) influence on these changes in 1.10.0 version"
21894,Tensorflow with Keras fit and tensors in dataset input result in list index out of range,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS 10.13.6
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.10.1
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
I encountered a strange error when using tf.keras in python with a dataset that consists out of tensors.

What I do is create a dataset from a csv file and modify it as it contains a time series. In the end, I end up with a dataset that has tuples of input and output tensor.

These are fed into a tf.keras model and here is where it becomes interesting. Fit calls a function in training.py of the keras enginge which is called _standardize_user_data. This should return the input, targets, and so on for fit_loop, which in turn checks something on it and crashes with _if issparse is not None and issparse(ins[i]) and not K.is_sparse(feed[i]): IndexError: list index out of range_ in line 187 of training_arrays.py. This is due to the fact _standardize_user_data returns empty lists, with the reasoning that if tensors are the input, then everything should be set up already, which it apparently isn't.

### Source code / logs
A small script to reproduce the problem:
```
#!/usr/bin/env python3
import argparse
import glob
import logging
import numpy as np
import tensorflow as tf
from tensorflow import keras


def read_dataset(filename, columns, field_defaults, input_size, output_size, stride, input_features):
    def decode_csv(row):
        fields = tf.decode_csv(row, record_defaults=field_defaults, field_delim=',')
        all_columns = dict(zip(columns, fields))
        return all_columns

    def split_window(window):
        inputs = tf.reshape(tf.concat(window['value'][0:input_size], axis=1), [input_size, input_features])
        outputs = tf.reshape(tf.concat(window['value'][input_size:input_size + output_size], axis=1),
                             [output_size, input_features])

        return inputs, outputs

    dataset = tf.data.TextLineDataset(filenames=filename)
    dataset = dataset.map(decode_csv)
    dataset = dataset.apply(tf.contrib.data.sliding_window_batch(window_size=input_size + output_size, stride=stride))
    dataset = dataset.map(split_window)
    dataset = dataset.repeat()

    return dataset


if __name__ == ""__main__"":
    COLUMNS = ['value']
    FIELD_DEFAULTS = [[0.0]]
    INPUT_FEATURES = 1

    epochs = 1
    steps = 1
    INPUT_SIZE = 4
    OUTPUT_SIZE = 2
    STRIDE = 1
    input_train = ""./data/""

    input_train_list = glob.glob(input_train + ""*"")

    model = keras.Sequential()
    model.add(tf.keras.layers.Dense(OUTPUT_SIZE, activation=None))
    set = read_dataset(input_train_list, COLUMNS, FIELD_DEFAULTS, INPUT_SIZE, OUTPUT_SIZE, STRIDE, INPUT_FEATURES)

    model.compile(optimizer=tf.train.AdamOptimizer(0.01), loss='mse', metrics=['mse'])
    model.fit(set, epochs=epochs, steps_per_epoch=steps)
```
And the error itself:
```
2018-08-27 14:29:33.238328: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Traceback (most recent call last):
  File ""/.../test.py"", line 102, in <module>
    model.fit(set, epochs=epochs, steps_per_epoch=steps)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1363, in fit
    validation_steps=validation_steps)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py"", line 187, in fit_loop
    if issparse is not None and issparse(ins[i]) and not K.is_sparse(feed[i]):
IndexError: list index out of range
```
The data itself is something like this:
```
0.047910000000000785
3.0999999999892225e-05
0.0160979999999995
2.9000000000500847e-05
0.01716599999999957
2.800000000036107e-05
2.9999999999752447e-05
0.019235000000000113
```

I am not sure if this behaviour is intended or not. In the Keras example, a dataset is used as well and I assume it should work with datasets consisting out of tensors. Is it possible that the return in this case should not be empty, but should only have an empty sample weight? "
21893,Eager execution error: tf.enable_eager_execution must be called at program startup.,"### System information

- **Have I written custom code**: yes
- **Bazel version**: N/A
- **OS Platform and Distribution:** Windows 10
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A
- **TensorFlow installed from conda**
- **TensorFlow version**: 1.9
- **Python version**: 3.6
- **Mobile device**: N/A

### Describe the problem

I have some errors using the eager execution. 

I have tried to perform an eager execution of a simple code. I've tried it on both Jupyter Notebook and Spyder IDE. 

The code is as follows:

                import tensorflow as tf
                tf.enable_eager_execution ()
                import tensorflow.contrib.eager as tfe

                def square (x):
                     return tf.multiply (x, x)

                grad = tfe.gradients_function (square)
                print (grad (3.))


When I execute the code I get the following error:


	File ""C:\...\lib\site-packages\tensorflow\python\framework\ops.py"", line 5496, in enable_eager_execution ""tf.enable_eager_execution must be called at program startup."")

	ValueError: tf.enable_eager_execution must be called at program startup.


In Jupyter, In the first execution there are no errors but in the following ones. One way to fix it is by restarting the kernel.

In Spyder the error appears in the first execution I made. 
To fix it I found two ways: 

One rebooting the Spyder kernel and the second with a try / except:

		try:
		 tf.enable_eager_execution ()
		except Exception:
		 pass


I leave the link to the question you make in Stackoverflow
[(https://stackoverflow.com/questions/51967975/tf-enable-eager-execution-must-be-called-at-program-startup-only-in-spyder-ide)]


I hope you find it helpful

@alextp Thank you very much for your help."
21892,LookupError: No gradient defined for operation 'High-order/SegmentProd' (op type: SegmentProd),is there any problem in my code or the op SegmentProd do not have gradient implement
21891,Search in Embedding Projector using Japanese or Hindi text causes Cannot read property 'toString' of undefined,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
Visiting http://projector.tensorflow.org/?config=https://ecraft2learn.github.io/ai/word-embeddings/hi/projector.json using Chrome 68

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
After visiting http://projector.tensorflow.org/?config=https://ecraft2learn.github.io/ai/word-embeddings/hi/projector.json using Chrome 68 and then entering any Hindi text in the Search field the console shows

Uncaught TypeError: Cannot read property 'toString' of undefined
    at b (?config=https://ecraft2learn.github.io/ai/word-embeddings/hi/projector.json:formatted:60928)
    at ?config=https://ecraft2learn.github.io/ai/word-embeddings/hi/projector.json:formatted:61345
    at Array.forEach (<anonymous>)
    at a.query (?config=https://ecraft2learn.github.io/ai/word-embeddings/hi/projector.json:formatted:61344)
    at ?config=https://ecraft2learn.github.io/ai/word-embeddings/hi/projector.json:formatted:66646
    at ?config=https://ecraft2learn.github.io/ai/word-embeddings/hi/projector.json:formatted:66171
    at Array.forEach (<anonymous>)
    at HTMLElement.b.notifyInputChanged (?config=https://ecraft2learn.github.io/ai/word-embeddings/hi/projector.json:formatted:66170)
    at HTMLElement.b.onTextChanged (?config=https://ecraft2learn.github.io/ai/word-embeddings/hi/projector.json:formatted:66185)
    at HTMLElement.<anonymous> (?config=https://ecraft2learn.github.io/ai/word-embeddings/hi/projector.json:formatted:66151)
b @ ?config=https://ecraft2learn.github.io/ai/word-embeddings/hi/projector.json:formatted:60928
(anonymous) @ ?config=https://ecraft2learn.github.io/ai/word-embeddings/hi/projector.json:formatted:61345
a.query @ ?config=https://ecraft2learn.github.io/ai/word-embeddings/hi/projector.json:formatted:61344
(anonymous) @ ?config=https://ecraft2learn.github.io/ai/word-embeddings/hi/projector.json:formatted:66646
(anonymous) @ ?config=https://ecraft2learn.github.io/ai/word-embeddings/hi/projector.json:formatted:66171
b.notifyInputChanged @ ?config=https://ecraft2learn.github.io/ai/word-embeddings/hi/projector.json:formatted:66170
b.onTextChanged @ ?config=https://ecraft2learn.github.io/ai/word-embeddings/hi/projector.json:formatted:66185
(anonymous) @ ?config=https://ecraft2learn.github.io/ai/word-embeddings/hi/projector.json:formatted:66151

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21889,Unable to serialize graph with Embedding layers with graph_util.convert_variables_to_constants,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: -
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.10.0-rc1-19-g656e7a2b34
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 
- **GPU model and memory**: 
- **Exact command to reproduce**: (run script inlined in this issue)

### Describe the problem
Attempting to serialize a model containing a keras Embeddings layer with `tf.graph_util.convert_variables_to_constants` yields a graph that can't be restored. Restoring the graph makes TF complain that some nodes are missing; forcing those into the graphdef will cause TF to complain about mismatching types.

### Source code / logs
The following script tries to serialize two very simple models  one without an embeddings layer and the other with such a layer. Restoring the model will the embeddings layer will fail with `Node 'embedding/embedding_lookup' expects to be colocated with unknown node 'embedding/embedding_lookup/Read/ReadVariableOp'`.

    import tensorflow as tf
    import numpy as np

    def serialize_graph(model):
        g = tf.graph_util.convert_variables_to_constants(
            tf.keras.backend.get_session(),
            tf.keras.backend.get_session().graph.as_graph_def(),
            #[n.name for n in tf.keras.backend.get_session().graph.as_graph_def().node],
            [t.op.name for t in model.outputs]
        )
        return g

    def build_save_restore(model):
        model.compile('sgd', loss='mse')
        model.fit(np.array([[1]]),np.array([[1]]), verbose=0)

        gdef = serialize_graph(model)
        newg = tf.Graph()
        with newg.as_default():
            tf.import_graph_def(gdef)
        print(""*""*25)
        print(""restored successfully"")
        print(""*""*25)

    model = tf.keras.models.Sequential([
        tf.keras.layers.Dense(1)
    ])
    build_save_restore(model)

    model = tf.keras.models.Sequential([
        tf.keras.layers.Embedding(1, 1, input_length=1),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(1),
    ])
    build_save_restore(model)
"
21888,TensorFlow for AGI,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21886,Tensorflow 1.10.0 MKL build (win-64) with bazel throws linker error,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.10.0
- **Python version**: 3.6 (doesn't really matter)
- **Bazel version (if compiling from source)**: 0.15.2
- **GCC/Compiler version (if compiling from source)**: VS2015 Update 3
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: <sigh>

### Describe the problem
Tried compiling tensorflow from source, cpu version, with mkl.

### Source code / logs
Build fails with the following output:
```
ERROR: C:/ci/tensorflow-base_1535347248498/work/tensorflow/python/BUILD:3627:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed (Exit 1000): link.exe failed: error executing command
  cd C:/users/nwani/_bazel_nwani/ozaxfmyw/execroot/org_tensorflow
  SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\INCLUDE;C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\include\um;C:\Program Files (x86)\Windows Kits\8.1\include\\shared;C:\Program Files (x86)\Windows Kits\8.1\include\\um;C:\Program Files (x86)\Windows Kits\8.1\include\\winrt;
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\LIB\amd64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\LIB\amd64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.10240.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\lib\um\x64;C:\Program Files (x86)\Windows Kits\8.1\lib\winv6.3\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\amd64;C:\WINDOWS\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\Tools;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Team Tools\Performance Tools;C:\Program Files (x86)\Windows Kits\8.1\bin\x64;C:\Program Files (x86)\Windows Kits\8.1\bin\x86;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.6.1 Tools\x64\;;C:\WINDOWS\system32
    SET PWD=/proc/self/cwd
    SET TEMP=C:\Users\nwani\AppData\Local\Temp
    SET TMP=C:\Users\nwani\AppData\Local\Temp
    SET USE_LINKER=1
  C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/link.exe /nologo /DLL /SUBSYSTEM:CONSOLE -DEFAULTLIB:advapi32.lib /MACHINE:X64 @bazel-out/host/bin/tensorflow/python/_pywrap_tensorflow_internal.so-2.params /DEF:bazel-out/host/genfiles/tensorflow/python/pywrap_tensorflow_filtered_def_file.def /ignore:4070
LINK : warning LNK4044: unrecognized option '/lpthread'; ignored
LINK : warning LNK4044: unrecognized option '/lm'; ignored
LINK : warning LNK4044: unrecognized option '/ldl'; ignored
LINK : warning LNK4044: unrecognized option '/lpthread'; ignored
LINK : warning LNK4044: unrecognized option '/lpthread'; ignored
LINK : warning LNK4044: unrecognized option '/lm'; ignored
LINK : warning LNK4044: unrecognized option '/ldl'; ignored
LINK : warning LNK4044: unrecognized option '/lm'; ignored
LINK : warning LNK4044: unrecognized option '/lm'; ignored

bazel-out/host/bin/external/nsync/nsync_cpp.lib : fatal error LNK1000: Internal error during CImplib::EmitThunk

  Version 14.00.24215.1

  ExceptionCode            = C0000005
  ExceptionFlags           = 00000000
  ExceptionAddress         = 00007FF7EF196896 (00007FF7EF180000) ""C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\bin\amd64\link.exe""
  NumberParameters         = 00000002
  ExceptionInformation[ 0] = 0000000000000000
  ExceptionInformation[ 1] = 0000000000000008

CONTEXT:
  Rax    = 0000000000000000  R8     = 00007FF7EF27FBE0
  Rbx    = 0000000000000000  R9     = 00007FF7EF27E9F0
  Rcx    = 0000000000000000  R10    = 0000000000000000
  Rdx    = 00007FF7EF27FBD8  R11    = 0000000000000000
  Rsp    = 000000A6F5B2DE28  R12    = 00007FF7EF24D950
  Rbp    = 0000023D587FA330  E13    = 0000000000000000
  Rsi    = 0000000000008000  R14    = 0000000000000000
  Rdi    = 0000023D59249700  R15    = 0000000000000000
  Rip    = 00007FF7EF196896  EFlags = 0000000000010246
  SegCs  = 0000000000000033  SegDs  = 000000000000002B
  SegSs  = 000000000000002B  SegEs  = 000000000000002B
  SegFs  = 0000000000000053  SegGs  = 000000000000002B
  Dr0    = 0000000000000000  Dr3    = 0000000000000000
  Dr1    = 0000000000000000  Dr6    = 0000000000000000
  Dr2    = 0000000000000000  Dr7    = 0000000000000000
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 2226.356s, Critical Path: 254.23s
INFO: 4537 processes: 4537 local.
FAILED: Build did NOT complete successfully
+ exit 1
```
"
21884,Adding regularizer causes crash when using eager w/ make_template,"### System information
- Ubuntu 16.04
- Tensorflow installed from binary: v1.9.0.138
- Python 2.7.12
- CUDA: Version 9.0.252
- GPU: GeForce GTX 1080 Ti

Code to reproduce:
```python
import tensorflow as tf
import tensorflow.contrib.eager as tfe
import tensorflow.contrib.layers as layers

tf.enable_eager_execution()

def make_network(input_images, include_regularizer):
    weights_initializer = tf.variance_scaling_initializer(scale=1.)
    l2_regularizer = layers.l2_regularizer(scale=0.000004)

    convolution_args = dict(
        weights_initializer=weights_initializer,
        biases_initializer=tf.zeros_initializer(),
        activation_fn=tf.nn.leaky_relu,
        kernel_size=(3, 3), stride=1,
        padding='SAME',
        weights_regularizer=l2_regularizer if include_regularizer else None,
    )
    return layers.conv2d(inputs=input_images,
                         num_outputs=3,
                         scope='my_layer',
                         **convolution_args)

if __name__ == '__main__':
    images = tf.ones([2, 240, 320, 3], dtype=tf.float32)
    the_template = tfe.make_template(
        'custom_network', make_network,
        create_graph_function_=True,
    )
    # execute the template
    foo = the_template(images, include_regularizer=True)
```

The above code fails with `UnboundLocalError` when `include_regularizer=True`. Setting `include_regularizer=False` runs normally. The crash occurs in `base.py`:

```
/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py in add_weight(self, name, shape, dtype, initializer, regularizer, trainable, constraint, use_resource, synchronization, aggregation, partitioner)
    278         if regularizer:
    279           if context.executing_eagerly() or _should_add_regularizer(
--> 280               variable, existing_variables):
    281             self._handle_weight_regularization(name, variable, regularizer)
    282 

UnboundLocalError: local variable 'existing_variables' referenced before assignment
```

Upon closer inspection, it appears `context.executing_eagerly()` is `False` here, resulting in the `_should_add_regularizer` statement being evaluated. Based on a cursory inspection, it seems like `existing_variables` should be set on line 246 of `base.py`, but this does not occur because within the scope of `ops`, `executing_eagerly()` evaluates to `True`. 

See https://github.com/tensorflow/tensorflow/blob/09792df012c22622324f085f46edde33006c7355/tensorflow/python/layers/base.py#L236-L250

I'm unclear what the expected behavior of this code is - am I doing something wrong or is this a bug?
"
21882,How can I solve the problem ?,"
jiangnanqiao@jiangnanqiao-Lenovo-B40-80:~$ which pip
/home/jiangnanqiao/.local/bin/pip
jiangnanqiao@jiangnanqiao-Lenovo-B40-80:~$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.3.0-cp27-none-linux_x86_64.whl
jiangnanqiao@jiangnanqiao-Lenovo-B40-80:~$ sudo pip install $TF_BINARY_URL[sudo] password for jiangnanqiao: 
The directory '/home/jiangnanqiao/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.
The directory '/home/jiangnanqiao/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.
tensorflow-1.3.0-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.

My os is 64 bits,windows&ubuntu16.04.I know little about this and I want to solve the problem.
"
21881,Tensorflow nearly 2 times slower than Pytorch in training!,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: -
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.7.0 and 1.10.1
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:-
- **GCC/Compiler version (if compiling from source)**:-
- **CUDA/cuDNN version**:9.0/7.0 and 7.1
- **GPU model and memory**: GTX1080/8Gig
- **Exact command to reproduce**:

### Describe the problem
I was trying to train the same network on both `Tensorflow` and `Pytorch` frameworks and noticed `Tensorflow` is nearly **2x slower** than `Pytorch`!  
While each epoch in `Pytorch` takes roughly about **50** seconds, in `Tensorflow` it takes **90** seconds!     
I initially tried Stackoverflow, but after seeing a similar issue (https://github.com/tensorflow/tensorflow/issues/7187)from around a year ago, which reported the same performance issues and ultimately got solved, I changed my mind and thought it should be a bug.  
I uploaded the code snippets regarding Tensorflow (which are taken from [official TF model repository-resnet ][1] and changed to be used with the new architecture(nearly everything is intact except minor changes needed to use the new architecture (simpleNet, which is a very simple convolutional architecture)).  
It should be noted that, the points in the previous similar issue that solved the performance discrepancy are not valid in this one. since the code base is changed and has been updated according to the latest improvements in TF. 
Things such as not using feed_dict, or offloading the input on to the CPU instead of GPU and setting `TF_ENABLE_WINOGRAD_NONFUSED `flags are already addressed in the new example which is used for this case, however the performance issue still persists. 

### Source code / logs 

Here are the code snippets :     

> [simple_model.py][2]   
> [cifar10_main.py][3]      
> [simplenet_run_loop.py][4]

Here are the logs for Pytorch and tensorflow respectively :

**Pytorch (v0.4)** :

    ==>>[2018-08-19 00:00:26] [Epoch=000/450] [Need: 00:00:00] [learning_rate=0.100000] [Best : Accuracy=0.00, Error=100.00]
      Epoch: [000][000/500]   Time 1.345 (1.345)   Data 0.089 (0.089)   Loss 4.8846 (4.8846)   Prec@1 0.000 (0.000)   Prec@5 4.000 (4.000)   [2018-08-19 00:00:28]
      Epoch: [000][200/500]   Time 0.089 (0.096)   Data 0.000 (0.001)   Loss 4.0047 (4.3586)   Prec@1 6.000 (3.771)   Prec@5 28.000 (14.930)   [2018-08-19 00:00:46]
      Epoch: [000][400/500]   Time 0.089 (0.093)   Data 0.000 (0.000)   Loss 3.9328 (4.1781)   Prec@1 9.000 (5.519)   Prec@5 26.000 (20.142)   [2018-08-19 00:01:04]
      **Train** Prec@1 6.352 Prec@5 22.334 Error@1 93.648
      **Test** Prec@1 8.520 Prec@5 31.600 Error@1 91.480
    
    ==>>[2018-08-19 00:01:17] [Epoch=001/450] [Need: 06:07:54] [learning_rate=0.100000] [Best : Accuracy=8.52, Error=91.48]
      Epoch: [001][000/500]   Time 0.128 (0.128)   Data 0.086 (0.086)   Loss 3.7810 (3.7810)   Prec@1 9.000 (9.000)   Prec@5 34.000 (34.000)   [2018-08-19 00:01:17]
      Epoch: [001][200/500]   Time 0.090 (0.090)   Data 0.000 (0.001)   Loss 3.5385 (3.7109)   Prec@1 18.000 (11.517)   Prec@5 39.000 (34.861)   [2018-08-19 00:01:35]
      Epoch: [001][400/500]   Time 0.088 (0.090)   Data 0.000 (0.000)   Loss 3.6088 (3.6151)   Prec@1 11.000 (13.274)   Prec@5 34.000 (38.102)   [2018-08-19 00:01:54]
      **Train** Prec@1 14.048 Prec@5 39.416 Error@1 85.952
      **Test** Prec@1 19.110 Prec@5 45.950 Error@1 80.890
    
    ==>>[2018-08-19 00:02:07] [Epoch=002/450] [Need: 06:10:38] [learning_rate=0.100000] [Best : Accuracy=19.11, Error=80.89]
      Epoch: [002][000/500]   Time 0.133 (0.133)   Data 0.086 (0.086)   Loss 3.4438 (3.4438)   Prec@1 17.000 (17.000)   Prec@5 45.000 (45.000)   [2018-08-19 00:02:07]
      Epoch: [002][200/500]   Time 0.089 (0.091)   Data 0.000 (0.001)   Loss 3.1025 (3.2688)   Prec@1 26.000 (19.721)   Prec@5 56.000 (48.085)   [2018-08-19 00:02:26]
      Epoch: [002][400/500]   Time 0.092 (0.091)   Data 0.000 (0.000)   Loss 2.9271 (3.1983)   Prec@1 24.000 (20.998)   Prec@5 57.000 (50.125)   [2018-08-19 00:02:44]
      **Train** Prec@1 21.658 Prec@5 50.980 Error@1 78.342
      **Test** Prec@1 26.430 Prec@5 56.030 Error@1 73.570
    
    ==>>[2018-08-19 00:02:57] [Epoch=003/450] [Need: 06:10:40] [learning_rate=0.100000] [Best : Accuracy=26.43, Error=73.57]
      Epoch: [003][000/500]   Time 0.136 (0.136)   Data 0.087 (0.087)   Loss 2.8432 (2.8432)   Prec@1 23.000 (23.000)   Prec@5 55.000 (55.000)   [2018-08-19 00:02:57]
      Epoch: [003][200/500]   Time 0.092 (0.091)   Data 0.000 (0.001)   Loss 2.8233 (2.8715)   Prec@1 33.000 (26.567)   Prec@5 62.000 (58.433)   [2018-08-19 00:03:16]
      Epoch: [003][400/500]   Time 0.092 (0.091)   Data 0.000 (0.000)   Loss 2.6975 (2.8040)   Prec@1 29.000 (28.047)   Prec@5 58.000 (60.125)   [2018-08-19 00:03:34]
      **Train** Prec@1 28.784 Prec@5 60.932 Error@1 71.216
      **Test** Prec@1 30.960 Prec@5 61.810 Error@1 69.040



**Tensorflow( v1.7.0)** :

    totalMemory: 7.93GiB freeMemory: 7.38GiB
    2018-08-26 09:06:38.766280: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
    2018-08-26 09:06:38.929447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
    2018-08-26 09:06:38.929480: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 
    2018-08-26 09:06:38.929484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N 
    2018-08-26 09:06:38.929650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/device:GPU:0 with 7131 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
    INFO:tensorflow:Benchmark run: {'model_name': 'simpnet', 'machine_config': {'cpu_info': {'num_cores': 8, 'cpu_info': 'Intel(R) Core(TM) i7-4790K CPU @ 4.00GHz', 'mhz_per_cpu': 4000.0}, 'gpu_info': {'count': 1, 'model': 'GeForce GTX 1080'}, 'memory_total': 20986626048, 'memory_available': 14860541952}, 'run_date': '2018-08-26T04:36:38.596702Z', 'tensorflow_version': {'version': '1.7.0', 'git_hash': 'v1.7.0-3-g024aecf414'}, 'tensorflow_environment_variables': [{'name': 'TF_ENABLE_WINOGRAD_NONFUSED', 'value': '1'}]}
    INFO:tensorflow:Starting a training cycle: 0/250
    INFO:tensorflow:Calling model_fn.
    data_format:  channels_first
    INFO:tensorflow:Done calling model_fn.
    INFO:tensorflow:Create CheckpointSaverHook.
    INFO:tensorflow:Graph was finalized.
    2018-08-26 09:06:40.685552: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
    2018-08-26 09:06:40.685610: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
    2018-08-26 09:06:40.685617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 
    2018-08-26 09:06:40.685630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N 
    2018-08-26 09:06:40.685744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7131 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
    INFO:tensorflow:Running local_init_op.
    INFO:tensorflow:Done running local_init_op.
    INFO:tensorflow:Saving checkpoints for 1 into /tmp/cifar10_model/model.ckpt.
    INFO:tensorflow:learning_rate = 0.1, cross_entropy = 7.7181897, train_accuracy = 0.09
    INFO:tensorflow:loss = 20.531982, step = 0
    INFO:tensorflow:global_step/sec: 5.5106
    INFO:tensorflow:learning_rate = 0.1, cross_entropy = 2.26069, train_accuracy = 0.105 (18.147 sec)
    INFO:tensorflow:loss = 13.960868, step = 100 (18.147 sec)
    INFO:tensorflow:global_step/sec: 5.42846
    INFO:tensorflow:learning_rate = 0.1, cross_entropy = 2.291137, train_accuracy = 0.10666667 (18.421 sec)
    INFO:tensorflow:loss = 12.874262, step = 200 (18.421 sec)
    INFO:tensorflow:global_step/sec: 5.62853
    INFO:tensorflow:learning_rate = 0.1, cross_entropy = 2.1555672, train_accuracy = 0.1175 (17.767 sec)
    INFO:tensorflow:loss = 11.73109, step = 300 (17.767 sec)
    INFO:tensorflow:global_step/sec: 5.45977
    INFO:tensorflow:learning_rate = 0.1, cross_entropy = 2.0706115, train_accuracy = 0.122 (18.316 sec)
    INFO:tensorflow:loss = 10.739094, step = 400 (18.316 sec)
    INFO:tensorflow:Saving checkpoints for 500 into /tmp/cifar10_model/model.ckpt.
    INFO:tensorflow:Loss for final step: 9.72249.
    INFO:tensorflow:Starting to evaluate.
    INFO:tensorflow:Calling model_fn.
    data_format:  channels_first
    INFO:tensorflow:Done calling model_fn.
    INFO:tensorflow:Starting evaluation at 2018-08-26-04:38:16
    INFO:tensorflow:Graph was finalized.
    2018-08-26 09:08:16.838937: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
    2018-08-26 09:08:16.838968: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
    2018-08-26 09:08:16.838986: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 
    2018-08-26 09:08:16.838989: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N 
    2018-08-26 09:08:16.839099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7131 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
    INFO:tensorflow:Restoring parameters from /tmp/cifar10_model/model.ckpt-500
    INFO:tensorflow:Running local_init_op.
    INFO:tensorflow:Done running local_init_op.
    INFO:tensorflow:Finished evaluation at 2018-08-26-04:38:22
    INFO:tensorflow:Saving dict for global step 500: accuracy = 0.2181, global_step = 500, loss = 9.815871
    INFO:tensorflow:Benchmark metric: Name accuracy, value 0, unit None, global_step 500, extras []
    INFO:tensorflow:Benchmark metric: Name loss, value 9, unit None, global_step 500, extras []
    INFO:tensorflow:Starting a training cycle: 1/250
    INFO:tensorflow:Calling model_fn.
    data_format:  channels_first
    INFO:tensorflow:Done calling model_fn.
    INFO:tensorflow:Create CheckpointSaverHook.
    INFO:tensorflow:Graph was finalized.
    2018-08-26 09:08:23.994577: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
    2018-08-26 09:08:23.994610: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
    2018-08-26 09:08:23.994625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 
    2018-08-26 09:08:23.994630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N 
    2018-08-26 09:08:23.994726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7131 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
    INFO:tensorflow:Restoring parameters from /tmp/cifar10_model/model.ckpt-500
    INFO:tensorflow:Running local_init_op.
    INFO:tensorflow:Done running local_init_op.
    INFO:tensorflow:Saving checkpoints for 501 into /tmp/cifar10_model/model.ckpt.
    INFO:tensorflow:learning_rate = 0.1, cross_entropy = 1.831348, train_accuracy = 0.23
    INFO:tensorflow:loss = 9.679985, step = 500
    INFO:tensorflow:global_step/sec: 5.3539
    INFO:tensorflow:learning_rate = 0.1, cross_entropy = 1.8214886, train_accuracy = 0.25 (18.678 sec)
    INFO:tensorflow:loss = 8.9299, step = 600 (18.678 sec)
    INFO:tensorflow:global_step/sec: 5.49204
    INFO:tensorflow:learning_rate = 0.1, cross_entropy = 1.7747709, train_accuracy = 0.26666668 (18.208 sec)
    INFO:tensorflow:loss = 8.215595, step = 700 (18.208 sec)
    INFO:tensorflow:global_step/sec: 5.48954
    INFO:tensorflow:learning_rate = 0.1, cross_entropy = 1.7076583, train_accuracy = 0.275 (18.216 sec)
    INFO:tensorflow:loss = 7.5460052, step = 800 (18.216 sec)
    INFO:tensorflow:global_step/sec: 5.65214
    INFO:tensorflow:learning_rate = 0.1, cross_entropy = 1.6106825, train_accuracy = 0.286 (17.692 sec)
    INFO:tensorflow:loss = 6.904671, step = 900 (17.692 sec)
    INFO:tensorflow:Saving checkpoints for 1000 into /tmp/cifar10_model/model.ckpt.
    INFO:tensorflow:Loss for final step: 6.5854077.
    INFO:tensorflow:Starting to evaluate.
    INFO:tensorflow:Calling model_fn.
    data_format:  channels_first
    INFO:tensorflow:Done calling model_fn.
    INFO:tensorflow:Starting evaluation at 2018-08-26-04:39:57
    INFO:tensorflow:Graph was finalized.
    2018-08-26 09:09:57.431671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
    2018-08-26 09:09:57.431702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
    2018-08-26 09:09:57.431718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 
    2018-08-26 09:09:57.431722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N 
    2018-08-26 09:09:57.431814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7131 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
    INFO:tensorflow:Restoring parameters from /tmp/cifar10_model/model.ckpt-1000
    INFO:tensorflow:Running local_init_op.
    INFO:tensorflow:Done running local_init_op.
    INFO:tensorflow:Finished evaluation at 2018-08-26-04:40:02
    INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.264, global_step = 1000, loss = 6.684446
    INFO:tensorflow:Benchmark metric: Name accuracy, value 0, unit None, global_step 1000, extras []
    INFO:tensorflow:Benchmark metric: Name loss, value 6, unit None, global_step 1000, extras []
    INFO:tensorflow:Starting a training cycle: 2/250
    INFO:tensorflow:Calling model_fn.
    data_format:  channels_first
    INFO:tensorflow:Done calling model_fn.
    INFO:tensorflow:Create CheckpointSaverHook.
    INFO:tensorflow:Graph was finalized.
    2018-08-26 09:10:04.247883: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
    2018-08-26 09:10:04.247914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
    2018-08-26 09:10:04.247930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 
    2018-08-26 09:10:04.247934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N 
    2018-08-26 09:10:04.248028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7131 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
    INFO:tensorflow:Restoring parameters from /tmp/cifar10_model/model.ckpt-1000
    INFO:tensorflow:Running local_init_op.
    INFO:tensorflow:Done running local_init_op.
    INFO:tensorflow:Saving checkpoints for 1001 into /tmp/cifar10_model/model.ckpt.
    INFO:tensorflow:learning_rate = 0.1, cross_entropy = 1.7071365, train_accuracy = 0.32
    INFO:tensorflow:loss = 6.5100193, step = 1000
    INFO:tensorflow:global_step/sec: 5.54579
    INFO:tensorflow:learning_rate = 0.1, cross_entropy = 1.5869155, train_accuracy = 0.335 (18.032 sec)
    INFO:tensorflow:loss = 5.9459677, step = 1100 (18.032 sec)
    INFO:tensorflow:global_step/sec: 5.69552
    INFO:tensorflow:learning_rate = 0.1, cross_entropy = 1.754588, train_accuracy = 0.31 (17.558 sec)
    INFO:tensorflow:loss = 5.7136836, step = 1200 (17.558 sec)
    INFO:tensorflow:global_step/sec: 5.69235
    INFO:tensorflow:learning_rate = 0.1, cross_entropy = 1.6495434, train_accuracy = 0.3225 (17.568 sec)
    INFO:tensorflow:loss = 5.2463627, step = 1300 (17.568 sec)
    INFO:tensorflow:global_step/sec: 5.60754
    INFO:tensorflow:learning_rate = 0.1, cross_entropy = 1.6227012, train_accuracy = 0.326 (17.833 sec)
    INFO:tensorflow:loss = 4.8923674, step = 1400 (17.833 sec)
    INFO:tensorflow:Saving checkpoints for 1500 into /tmp/cifar10_model/model.ckpt.
    INFO:tensorflow:Loss for final step: 4.7713437.
    INFO:tensorflow:Starting to evaluate.
    INFO:tensorflow:Calling model_fn.
    data_format:  channels_first
    INFO:tensorflow:Done calling model_fn.
    INFO:tensorflow:Starting evaluation at 2018-08-26-04:41:36
    INFO:tensorflow:Graph was finalized.
    2018-08-26 09:11:36.521245: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
    2018-08-26 09:11:36.521277: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
    2018-08-26 09:11:36.521285: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 
    2018-08-26 09:11:36.521290: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N 
    2018-08-26 09:11:36.521385: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7131 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
    INFO:tensorflow:Restoring parameters from /tmp/cifar10_model/model.ckpt-1500
    INFO:tensorflow:Running local_init_op.
    INFO:tensorflow:Done running local_init_op.
    INFO:tensorflow:Finished evaluation at 2018-08-26-04:41:41
    INFO:tensorflow:Saving dict for global step 1500: accuracy = 0.3168, global_step = 1500, loss = 4.7169304
    INFO:tensorflow:Benchmark metric: Name accuracy, value 0, unit None, global_step 1500, extras []
    INFO:tensorflow:Benchmark metric: Name loss, value 4, unit None, global_step 1500, extras []
    INFO:tensorflow:Starting a training cycle: 3/250
    INFO:tensorflow:Calling model_fn.
    data_format:  channels_first
    INFO:tensorflow:Done calling model_fn.
    INFO:tensorflow:Create CheckpointSaverHook.
    INFO:tensorflow:Graph was finalized.

**Extra info** :  
Os: Ubuntu 16.04  
GPU: GTX1080  
CPU: Intel 4790K  
RAM : 20Gig  
GPU Utilization : 99%    

  [1]: https://github.com/tensorflow/models/tree/master/official/resnet
  [2]: https://paste.ee/p/pN07i#s=2
  [3]: https://paste.ee/p/pN07i#section0
  [4]: https://paste.ee/p/pN07i#s=1"
21880,no such package '@png_archive//,"When I compile tensorflow 1.10 from source code on windows10, it reported the error ""no such package '@png_archive//""
I have installed the bazel, msys2, vs2017 and set the environment of PAHT,bazel_sh,bazel_vc."
21879,Failed to test mpi_allreduce_test.py,"Have I written custom code: None
OS Platform and Distribution: Linux Centos 7.0
Open MPI version: 3.1.1
TensorFlow installed from: r1.8
Bazel version: 0.10.1
Python version: 2.7
GCC version:  4.9.2
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: mpirun -np 4 python mpi_allreduce_test.py under directory tensorflow/tensorflow/contrib/mpi_collectives

mpirun -np 1 python mpi_allgather_test.py ;the test pass.
mpirun -np 3 python mpi_allgather_test.py ;the test pass.
mpirun -np 1 python mpi_allreduce_test.py ;the test pass.
but when I run mpirun -np 3 python mpi_allreduce_test.py, the test failed

```
1: iter 0
rank 2: My output is 330772870.0
rank 2: Our output is 170543409.0
rank 0: My output is 11908270.0
rank 0: Our output is 170543409.0
2: iter 0
0: iter 0
[e92e09614.em21:08680] Read -1, expected 2362084, errno = 38
[e92e09614.em21:08681] Read -1, expected 2362080, errno = 38
[e92e09614.em21:08679] Read -1, expected 2362080, errno = 38
[e92e09614.em21:08680] Read -1, expected 2362080, errno = 38
[e92e09614.em21:08681] Read -1, expected 2362084, errno = 38
[e92e09614.em21:08679] Read -1, expected 2362080, errno = 38
[e92e09614.em21:08680] Read -1, expected 2362080, errno = 38
```"
21878,"assert d in name_to_node, ""%s is not in graph"" % d","When I convert the keras model to pb file, it reports an error that :

assert d in name_to_node, ""%s is not in graph"" % d

Does anyone know any solution of this issue? Thank you.

"
21875,Incompatible shapes between op input and calculated input gradient for nn.conv3d_transpose,"This is code:

```
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
from utils import init_weight
import tensorflow as tf

slim = tf.contrib.slim


@tf.contrib.framework.add_arg_scope
def conv3d_transpose(inputs,
                     num_outputs,
                     kernel_size,
                     weights,
                     biases,
                     stride,
                     padding,
                     activation_fn=tf.nn.relu,
                     weights_initializer=tf.contrib.layers.xavier_initializer(),
                     biases_initializer=tf.zeros_initializer(),
                     reuse=None,
                     trainable=True,
                     scope=None):

  with tf.variable_scope(
      scope, 'Conv3d_transpose', [inputs], reuse=reuse):
    dtype = inputs.dtype.base_dtype
    kernel_d, kernel_h, kernel_w = kernel_size[0:3]
    num_filters_in = inputs.get_shape()[4]
    weights_shape = [kernel_d, kernel_h, kernel_w, num_outputs, num_filters_in]
    
    weights = tf.get_variable('weights',
                              shape=weights_shape,
                              dtype=dtype,
                              initializer=tf.constant_initializer(weights),
                              trainable=trainable)
    
    tf.contrib.framework.add_model_variable(weights)
    
    input_shape = inputs.get_shape().as_list()
    batch_size = input_shape[0]
    depth = input_shape[1]
    height = input_shape[2]
    width = input_shape[3]

    def get_deconv_dim(dim_size, stride_size,pading,kernel):
      if isinstance(dim_size, tf.Tensor):
        sub = tf.subtract(dim_size,1)
        dim_size = tf.multiply(dim_size,sub)
        dim_size = tf.subtract(dim_size, 2*pading)
        dim_size = tf.add(dim_size,kernel)
        
      elif dim_size is not None:
        dim_size  = (dim_size -1)*stride_size - 2*pading + kernel
      return dim_size
   
    pad = 1
    if padding == 'VALID':
      pad = 0
    out_depth = get_deconv_dim(depth, stride,pad,weights.get_shape().as_list()[0])
    out_height = get_deconv_dim(height, stride,pad,weights.get_shape().as_list()[1])
    out_width = get_deconv_dim(width, stride,pad,weights.get_shape().as_list()[2])

    out_shape = [batch_size, out_depth, out_height, out_width, num_outputs]
    outputs = tf.nn.conv3d_transpose(inputs, weights, out_shape,
                                     [1, stride, stride, stride, 1],
                                     padding)

    biases = tf.get_variable('biases',
                               shape=[num_outputs,],
                               dtype=dtype,
                               initializer=tf.constant_initializer(biases),
                               trainable=trainable)
    tf.contrib.framework.add_model_variable(biases)
    outputs = tf.nn.bias_add(outputs, biases)
    
    if activation_fn:
      outputs = activation_fn(outputs)
    return outputs


def model(identities, params, is_training):
  """"""Model transforming embedding to voxels.""""""
  del is_training  # Unused
  f_dim = params.f_dim
  with slim.arg_scope(
      [slim.fully_connected, conv3d_transpose]):
    w0 = init_weight(""DecoderWeights/Layer1_13824_512_w.txt"",[13824,512],""w"")
    print(identities.get_shape().as_list())
    h0 = slim.fully_connected(
        identities, 3 * 3 * 3 * f_dim * 8, weights_initializer = tf.constant_initializer(w0), activation_fn=tf.nn.relu)
    b0 = init_weight(""DecoderWeights/Layer1_13824_b.txt"",[13824],""b"")
    b0 = tf.get_variable('db0',shape=[13824], initializer=tf.constant_initializer(b0),trainable=True)
    h0 = tf.nn.bias_add(h0,b0)

    h1 = tf.reshape(h0, [-1, 3, 3, 3, f_dim * 8])
    print(h1.get_shape().as_list())
    w1 = init_weight(""DecoderWeights/Layer2_512_256_4_4_4_w.txt"",[512,256,4,4,4],""w"")
    b1 = init_weight(""DecoderWeights/Layer2_256_b.txt"",[256],""b"")
    h1 = conv3d_transpose(
        h1, f_dim * 4, [4, 4, 4],w1, b1,stride=1, padding= 'VALID',activation_fn=tf.nn.relu)
    print(h1.get_shape().as_list())
    w2 = init_weight(""DecoderWeights/Layer3_256_96_5_5_5_w.txt"",[256,96,5,5,5],""w"")
    b2 = init_weight(""DecoderWeights/Layer3_96_b.txt"",[96],""b"")
    h2 = conv3d_transpose(
        h1, int(f_dim * 3 / 2), [5, 5, 5],w2 , b2, stride=2,padding ='VALID' ,activation_fn=tf.nn.relu)
    print(h2.get_shape().as_list())
    w3 = init_weight(""DecoderWeights/Layer4_96_1_6_6_6_w.txt"",[96,1,6,6,6],""w"")
    b3 = init_weight(""DecoderWeights/Layer4_1_b.txt"",[1],""b"")
    h3 = conv3d_transpose(
        h2, 1, [6, 6, 6], w3,b3, stride=2, padding ='SAME' ,activation_fn=tf.nn.sigmoid)
    print(h3.get_shape().as_list())
  return h3
```

This is error:
Traceback (most recent call last):
  File ""train.py"", line 81, in <module>
    epsilon=1e-08).minimize(loss, global_step=global_step)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py"", line 343, in minimize
    grad_loss=grad_loss)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py"", line 414, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 609, in gradients
    % (op.name, i, t_in.shape, in_grad.shape))
ValueError: Incompatible shapes between op input and calculated input gradient.  Forward operation: decoder/Conv3d_transpose_2/conv3d_transpose.  Input index: 2. Original input shape: (6, 15, 15, 15, 96).  Calculated input gradient shape: (6, 16, 16, 16, 96)

Tf version : 1.4.1

I have 4 layers, 1 fc and 3 conv3d_transpose.

Layer shape as follows:

[6, 512]
[6, 3, 3, 3, 512]
[6, 6, 6, 6, 256]
[6, 15, 15, 15, 96]
[6, 32, 32, 32, 1]

I calculated conv3d_tranpose output shape as follows:
```
 def get_deconv_dim(dim_size, stride_size,pading,kernel):
      if dim_size is not None:
        dim_size  = (dim_size -1)*stride_size - 2*pading + kernel
      return dim_size
   
   pad = 1
   if padding == 'VALID':
     pad = 0
   out_depth = get_deconv_dim(depth, stride,pad,weights.get_shape().as_list()[0])
   out_height = get_deconv_dim(height, stride,pad,weights.get_shape().as_list()[1])
   out_width = get_deconv_dim(width, stride,pad,weights.get_shape().as_list()[2])
   out_shape = [batch_size, out_depth, out_height, out_width, num_outputs]
   outputs = tf.nn.conv3d_transpose(inputs, weights, out_shape,
                                     [1, stride, stride, stride, 1],
                                     padding)
```
In the last conv3d_transpose layer, while its output is calculated as (6,15,15,15,96), input gradient is calculated as (6,16,16,16,96). Therefore, this leads to error.

When i call last conv3d_transpose layer with padding=""SAME"", it didn't gives error.

edit:

Environment:
Os: Ubuntu 16.04.3 LTS
bazel: N/a
Tensorflow : 1.4.1 via pip install
cuda: 9.0.176
GPU: tesla k40, memory: 12gb
command: I uploaded only erroneous python script, just run ""python blabla.py"".  If you want, i can upload all python scripts."
21874,Windows docker documentation,"### System information
- Win 10
- Latest version of docker
- docker run -p 8888:8888 --name tensorflow-udacity -it gcr.io/tensorflow/udacity-assignments:1.0.0

### Describe the problem
At https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/udacity. It does not say what command to use for windows.

"
21873,Feature Request: Keras method to extract the graph,"Several parts of the Tensorflow documentation suggest to the reader to use the higher level APIs (Keras / Estimators) when there is no explicit need if the low level API. 

Of the high level APIs, there are some upwards mobility between the levels. For example, a custom Keras model can be converted into an Estimator. 

However, getting access to the lower level APIs from the higher level apis is not so simple.

It would be nice - at least for visualization purposes - to allow users to see the automatically generated graph produced by the higher level APIs. 

Taking a Keras model, converting it to an estimator, defining the `serving_input_receiver_fn`, and then exporting the estimator via `export_savedmodel` to see the graph is a bit convoluted for ""high level"" apis.

A subclassed Keras model, once compiled, could have a method `as_graph` to convert to the computational graph.

"
21872,tf.enable_eager_execution() bug?,"
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:window10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.10
- **Python version**:3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
i used the` tf.enable_eager_execution()` and create InceptionResNetV2 mode will be error and only InceptionResNetV2 have. I need use  `tf.enable_eager_execution()` can't without. 

how to fix it?

### Source code / logs
`import tensorflow as tf`
`tf.enable_eager_execution()`
`image_model =tf.keras.applications.InceptionResNetV2(include_top=False, weights='imagenet')
`
--error---
TypeError                                 Traceback (most recent call last)
<ipython-input-2-b40bb13774b9> in <module>()
----> 1 image_model =tf.keras.applications.InceptionResNetV2(include_top=False, weights='imagenet')

D:\anaconda\lib\site-packages\tensorflow\python\keras\applications\inception_resnet_v2.py in InceptionResNetV2(include_top, weights, input_tensor, input_shape, pooling, classes)
    304   for block_idx in range(1, 11):
    305     x = inception_resnet_block(
--> 306         x, scale=0.17, block_type='block35', block_idx=block_idx)
    307 
    308   # Mixed 6a (Reduction-A block): 17 x 17 x 1088

D:\anaconda\lib\site-packages\tensorflow\python\keras\applications\inception_resnet_v2.py in inception_resnet_block(x, scale, block_type, block_idx, activation)
    187       output_shape=K.int_shape(x)[1:],
    188       arguments={'scale': scale},
--> 189       name=block_name)([x, up])
    190   if activation is not None:
    191     x = Activation(activation, name=block_name + '_ac')(x)

D:\anaconda\lib\site-packages\tensorflow\python\keras\engine\base_layer.py in __call__(self, inputs, *args, **kwargs)
    745           input_shapes = nest.map_structure(lambda x: x.shape, inputs)
    746 
--> 747         output_shapes = self.compute_output_shape(input_shapes)
    748         output_shapes = nest.flatten(output_shapes)
    749         outputs = [

D:\anaconda\lib\site-packages\tensorflow\python\keras\layers\core.py in compute_output_shape(self, input_shape)
    678 
    679   def compute_output_shape(self, input_shape):
--> 680     input_shape = tuple(tensor_shape.TensorShape(input_shape).as_list())
    681 
    682     if self._output_shape is None:

D:\anaconda\lib\site-packages\tensorflow\python\framework\tensor_shape.py in __init__(self, dims)
    539       else:
    540         # Got a list of dimensions
--> 541         self._dims = [as_dimension(d) for d in dims_iter]
    542     self._ndims = None
    543 

D:\anaconda\lib\site-packages\tensorflow\python\framework\tensor_shape.py in <listcomp>(.0)
    539       else:
    540         # Got a list of dimensions
--> 541         self._dims = [as_dimension(d) for d in dims_iter]
    542     self._ndims = None
    543 

D:\anaconda\lib\site-packages\tensorflow\python\framework\tensor_shape.py in as_dimension(value)
    480     return value
    481   else:
--> 482     return Dimension(value)
    483 
    484 

D:\anaconda\lib\site-packages\tensorflow\python\framework\tensor_shape.py in __init__(self, value)
     35       raise TypeError(""Cannot convert %s to Dimension"" % value)
     36     else:
---> 37       self._value = int(value)
     38       if (not isinstance(value, compat.bytes_or_text_types) and
     39           self._value != value):

TypeError: int() argument must be a string, a bytes-like object or a number, not 'TensorShape'
"
21871,tf.float16 not in allowed list?,"when i call dynamic_rnn,  i counter a TypeError: Value passed to parameter 'x' has DataType float16 not in list of allowed values: float32. the dynamic_rnn only support the type of float32?"
21870,Missing quantized implementation of TopK_V2,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Fedora 27
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: `pip install tensorflow-gpu`
- **TensorFlow version (use command below)**: v1.9.0-0-g25c197e023
- **Python version**: 3.6.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0 / 7.1.4
- **GPU model and memory**: 940MX, 2GB VRAM
- **Exact command to reproduce**: see code below

### Describe the problem
I am trying to quantize my network.
There is a special logic in one layer which takes the k highest activations and sets all other to zero.
It is implemented using the following code:
```python
top = tf.nn.top_k(intermediate, k, sorted=True)
min_indices = top.indices[..., -1]
batch_indices = tf.range(intermediate.shape[0], dtype=tf.int32)
total_indices = tf.stack([batch_indices, min_indices], axis=1)
mins = tf.reshape(tf.gather_nd(intermediate, total_indices), [-1, 1])
selection = intermediate >= mins
zeros = tf.zeros(intermediate.shape, dtype=intermediate.dtype)
intermediate = tf.where(selection, x=intermediate, y=zeros)
```
Where `k` is a `int32` placeholder and `intermediate` a Tensor containing the activations of a fully connected layer. It is shaped `[batch_size, nodes]`.

I then call `tf.contrib.quantize.create_training_graph()`, train using Adam and save the weights as a checkpoint. After a reset of the graph I create the eval graph and rewrite it using `tf.contrib.quantize.create_eval_graph()`. That one is saved to a graph definition file. After that I run the following commands:

```
freeze_graph --input_graph definition.pb --input_checkpoint lastcheckpoint --output_graph frozen_graph.pb --output_node=out
```

```
tflite_convert --output_file model.tflite --graph_def_file frozen_graph.pb --inference_type QUANTIZED_UINT8 --input_arrays input/in,density --output_arrays out --mean_values 1,1 --std_dev_values 1,1
```

The second command fails with the following log:

```
2018-08-25 12:39:14.510582: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-08-25 12:39:14.605698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-08-25 12:39:14.606404: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: GeForce 940MX major: 5 minor: 0 memoryClockRate(GHz): 0.8605
pciBusID: 0000:01:00.0
totalMemory: 1.96GiB freeMemory: 1.71GiB
2018-08-25 12:39:14.606429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2018-08-25 12:39:15.293534: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-25 12:39:15.293568: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2018-08-25 12:39:15.293576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2018-08-25 12:39:15.293738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1465 MB memory) -> physical GPU (device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0, compute capability: 5.0)
Traceback (most recent call last):
  File ""/home/t/development/projects/nncompression/JugendForscht2019/venv/bin/tflite_convert"", line 11, in <module>
    sys.exit(main())
  File ""/home/t/development/projects/nncompression/JugendForscht2019/venv/lib/python3.6/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 320, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/home/t/development/projects/nncompression/JugendForscht2019/venv/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/home/t/development/projects/nncompression/JugendForscht2019/venv/lib/python3.6/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 316, in run_main
    _convert_model(tflite_flags)
  File ""/home/t/development/projects/nncompression/JugendForscht2019/venv/lib/python3.6/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 121, in _convert_model
    output_data = converter.convert()
  File ""/home/t/development/projects/nncompression/JugendForscht2019/venv/lib/python3.6/site-packages/tensorflow/contrib/lite/python/lite.py"", line 309, in convert
    allow_custom_ops=self.allow_custom_ops)
  File ""/home/t/development/projects/nncompression/JugendForscht2019/venv/lib/python3.6/site-packages/tensorflow/contrib/lite/python/convert.py"", line 225, in toco_convert
    input_data.SerializeToString())
  File ""/home/t/development/projects/nncompression/JugendForscht2019/venv/lib/python3.6/site-packages/tensorflow/contrib/lite/python/convert.py"", line 107, in toco_convert_protos
    (stdout, stderr))
RuntimeError: TOCO failed see console for info.
b'2018-08-25 12:39:18.308856: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: GatherNd
2018-08-25 12:39:18.308926: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Round
2018-08-25 12:39:18.309390: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 57 operators, 96 arrays (0 quantized)
2018-08-25 12:39:18.309831: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 57 operators, 96 arrays (0 quantized)
2018-08-25 12:39:18.312078: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 28 operators, 55 arrays (1 quantized)
2018-08-25 12:39:18.312642: W tensorflow/contrib/lite/toco/tooling_util.cc:1639] Dropping MinMax information in array model/conv1/weights_quant/FakeQuantWithMinMaxVars. Expect inaccuracy in quantized inference.
2018-08-25 12:39:18.312788: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 26 operators, 51 arrays (1 quantized)
2018-08-25 12:39:18.312907: W tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:1304] Skipping StridedSlice op with output ""model/sparsity/strided_slice_1"". ellipsis_mask is not supported (mask=1)
2018-08-25 12:39:18.313073: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 25 operators, 49 arrays (1 quantized)
2018-08-25 12:39:18.313173: W tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:1304] Skipping StridedSlice op with output ""model/sparsity/strided_slice_1"". ellipsis_mask is not supported (mask=1)
2018-08-25 12:39:18.313322: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 25 operators, 49 arrays (1 quantized)
2018-08-25 12:39:18.313431: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 22 operators, 46 arrays (1 quantized)
2018-08-25 12:39:18.313573: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before quantization graph transformations: 22 operators, 46 arrays (1 quantized)
2018-08-25 12:39:18.313591: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:87] Constant array model/conv1/weights_quant/FakeQuantWithMinMaxVars lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-08-25 12:39:18.315866: F tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:456] Unimplemented: this graph contains an operator of type TopK_V2 for which the quantized form is not yet implemented. Sorry, and patches welcome (that\'s a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).\n'
None
```
The main problem seems to be the missing implementation of TopK_V2. Although it is suggested as a ""fun patch to write"", I only have little knowledge of C/C++ and don't know the tensorflow internals. So I'm opening this issue here to show the need for a quantized implementation of TopK_V2. Maybe somebody can help with that.

I'm unsure if the other warnings are coming from this issue, too.
If you know why they show up, please tell me."
21867,tf.seq2seq has not attribute prepare attention even in version 1.0,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **I have written code in tensorflow for a chatbot.**:
- **OS Platform is windows 10**
- **TensorFlow installed from source **:
- **TensorFlow version (both version 1.0 and 1.1.0)**:
- **Python version 3.6**
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
tf.seq2seq has not attribute prepare attention even in version 1.0


"
21866,tensorflow 1.10.1 requires numpy 1.14.5?,"I have successfully built tensorflow  1.10.1 from source but when  I installed the whl with pip3 it downloaded and installed numpy-1.14.5 and removed numpy-1.15.1. 

After tensorflow was installed I manually uninstalled numpy-1.14.5 and reinstalled numpy-1.15.1, pip gave a warning that tensorflow-1.10.1 requires numpy <= 1.14.5 but went ahead anyway. Afterwards I ran some tests on tensorflow and it works just fine with numpy 1.15.1 so this requirement appears to be unnecessary. **Edited:** there is no complaint about incompatibility with numpy-1.15.1 for tensorflow-1.9

Is there a file I can edit to get rid of this requirement? I am ok with compiling again.

OS Ubuntu 16.04.5 LTS 64 bits , python3.5 (cuda-9.2)
"
21864,Performance of TFLite custom AAR is lower than AAR release,"When I build TFLite custom library for v1.9.0 using tip of v1.9 branch Android performance is lower by ~10-20% compared to a standard AAR release.
Build command used is bazel build --cxxopt='--std=c++11' -c opt --fat_apk_cpu=armeabi-v7a tensorflow/contrib/lite/java:tensorflow-lite

Is there anything I am missing in the build procedure?"
21863,tf.crop_and_resize very slow,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/a
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 1.10
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: 9/7.1
- **GPU model and memory**:1080ti/11gb dual
- **Exact command to reproduce**:

Crop and Resize takes too long to run, please see the attachment
![1](https://user-images.githubusercontent.com/4759327/44611642-eb36e380-a7f1-11e8-8fd6-7e681e124529.png)
![2](https://user-images.githubusercontent.com/4759327/44611643-ebcf7a00-a7f1-11e8-8d16-79f2ca1846c5.png)


You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21862,op_resolver documentation update,"Hello,

I am having problems converting my frozen graphs to tflite models. I could do it without problems with v1.8.0-rc0, but the latest version adds an extra input to the FindOp function of the MutableOpResolver (see lines 34 and 31 of op_resolver.h). Here is what they look like

  virtual const TfLiteRegistration* FindOp(tflite::BuiltinOperator op,
                                           int version) const = 0;
  // Finds the op registration of a custom operator by op name.
  virtual const TfLiteRegistration* FindOp(const char* op,
                                           int version) const = 0;

What should be the value of 'version'? How can I figure that out? The documentation does not mention that variable or how to do any sort of mapping.

Thanks,
Juan"
21856,Loading frozen graph changes behavior of `tf.placeholder()`,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: no
- **TensorFlow installed from (source or binary)**: `pip install tensorflow`
- **TensorFlow version (use command below)**: v1.10.0-0-g656e7a2b34 1.10.0
- **Python version**: `Python 3.6.6 :: Anaconda, Inc.`
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: Run the program below

### Describe the problem

Usually, `tf.placeholder(tf.float32, shape=[]).get_shape()` is an empty shape, `()`. However, if you import a specific frozen graph first, the result of `get_shape()` will be `unknown`.

Here is an example of a script that reproduces the issue and prints `unknown`:

```python
import tensorflow as tf

IN_OUT = ['images_ph:0', 'inception_v3/logits/flatten/Reshape:0']

with open('inception-v3.pb', 'rb') as in_file:
    graph_def = tf.GraphDef()
    graph_def.ParseFromString(in_file.read())
    tf.import_graph_def(graph_def, name='', return_elements=IN_OUT)

print('shape result', tf.placeholder(tf.float32, shape=[]).get_shape())
```

Where `inception-v3.pb` can be downloaded at: https://storage.googleapis.com/agi-data/models/inception-v3.pb"
21855,Tensorflow Lite issue on Raspberry Pi,"Hi Everyone. I am working on running some scripts on Tensorflow on Raspberry Pi 3. When I try to import a tflite model into the tflite interpreter, it throws an error. Please help :) :) 
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Raspbian Stretch
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:Rasp Pi 3
- **TensorFlow installed from (source or binary)**:Binary
- **TensorFlow version (use command below)**:1.9
- **Python version**:2.7
- **Bazel version (if compiling from source)**:NA
- **GCC/Compiler version (if compiling from source)**:NA
- **CUDA/cuDNN version**:NA
- **GPU model and memory**:NA
- **Exact command to reproduce**:-


### Source code / logs

###THE CODE###
 interpreter = tf.contrib.lite.Interpreter(model_path='mobilenet_v1_0.25_128_quant.tflite')

###ERROR###

/home/pi/.local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py:32: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 56, got 52
  from tensorflow.python.framework import fast_tensor_util
Traceback (most recent call last):
  File ""mobilenet_int_tflite.py"", line 24, in <module>
    interpreter = tf.contrib.lite.Interpreter(model_path='mobilenet_v1_0.25_128_quant.tflite')
  File ""/home/pi/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/interpreter.py"", line 50, in __init__
    _interpreter_wrapper.InterpreterWrapper_CreateWrapperCPPFromFile(
  File ""/home/pi/.local/lib/python2.7/site-packages/tensorflow/python/util/lazy_loader.py"", line 53, in __getattr__
    module = self._load()
  File ""/home/pi/.local/lib/python2.7/site-packages/tensorflow/python/util/lazy_loader.py"", line 42, in _load
    module = importlib.import_module(self.__name__)
  File ""/usr/lib/python2.7/importlib/__init__.py"", line 37, in import_module
    __import__(name)
  File ""/home/pi/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py"", line 28, in <module>
    _tensorflow_wrap_interpreter_wrapper = swig_import_helper()
  File ""/home/pi/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_tensorflow_wrap_interpreter_wrapper', fp, pathname, description)
ImportError: /home/pi/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so: undefined symbol: _ZN6tflite12tensor_utils39NeonMatrixBatchVectorMultiplyAccumulateEPKfiiS2_iPfi
 

"
21852,Tensorflow 1.10 source compilation fails on aarch64,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.10
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: 0.16.1
- **GCC/Compiler version (if compiling from source)**: 
- **CUDA/cuDNN version**: 9.0, 7.0
- **GPU model and memory**: DrivePX2
- **Exact command to reproduce**:
`bazel build -s --local_resources 3000,.8,1.0 --config=opt --config=cuda //tensorflow:libtensorflow.so //tensorflow:libtensorflow_cc.so //tensorflow/tools/pip_package:build_pip_package`

### Describe the problem

If I try to build tensorflow, I get the following error pretty much immediately

```
ERROR: No default_toolchain found for cpu 'aarch64'. Valid cpus are: [
 k8, 
 piii,
 arm,
 darwin,
 ppc,
 x64_windows,
]
```
I also tried recompiling Bazel 0.16.1 to include in the tools/cpp/CROSSTOOL a configuration using aarch64.
"
21851,How to install tensorflow in python3.7?,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21850,Error while using create_inference_graph - Can't find a device placement for the op!,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.10
- **Python version**: 3.5
- **Bazel version (if compiling from source)**:  0.16.1
- **GCC/Compiler version (if compiling from source)**:  5.4.0
- **CUDA/cuDNN version**: 9.0/7.0.5
- **GPU model and memory**: 4 x GTX 1080Ti 394.44 11GB VRAM each
- **TensorRT version**:3.0.4

### Describe the problem
error while running trt.create_inference_graph on a frozen graph pb

### Source code
```
import os
import tensorflow as tf
import tensorflow.contrib.tensorrt as trt
from tensorflow.python.framework import graph_io

_input = 'input_1'
_output = 'output_node0'
outputs = [_output]


def get_frozen_graph():
  with tf.gfile.FastGFile('model.h5.pb', ""rb"") as f:
    graph_def = tf.GraphDef()
    graph_def.ParseFromString(f.read())
  return graph_def


frozen_graph_def = get_frozen_graph()
trt_graph_def = trt.create_inference_graph(frozen_graph_def, 
					outputs,
					max_batch_size=256, 
					max_workspace_size_bytes=1 << 30, 
					precision_mode='FP16')
tf.reset_default_graph()
g = tf.Graph()
with tf.Session(graph=g) as sess:
	with g.as_default():
		tf.import_graph_def(
  		graph_def=trt_graph_def,
  		name='')
	graph_io.write_graph(g, '.', 'trt_frozen.pb', as_text=False)
```
### logs
2018-08-24 15:34:59.873109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-08-24 15:34:59.964048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-08-24 15:35:00.046908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-08-24 15:35:00.149867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-08-24 15:35:00.150535: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 4
2018-08-24 15:35:00.150615: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session
2018-08-24 15:35:00.151247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:01:00.0
totalMemory: 10.91GiB freeMemory: 10.38GiB
2018-08-24 15:35:00.151374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 1 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:02:00.0
totalMemory: 10.92GiB freeMemory: 10.76GiB
2018-08-24 15:35:00.151494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 2 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:03:00.0
totalMemory: 10.92GiB freeMemory: 10.76GiB
2018-08-24 15:35:00.151608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 3 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:05:00.0
totalMemory: 10.92GiB freeMemory: 10.76GiB
2018-08-24 15:35:00.156119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0, 1, 2, 3
2018-08-24 15:35:01.123215: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-24 15:35:01.123254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 1 2 3 
2018-08-24 15:35:01.123259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N Y Y Y 
2018-08-24 15:35:01.123263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 1:   Y N Y Y 
2018-08-24 15:35:01.123266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 2:   Y Y N Y 
2018-08-24 15:35:01.123269: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 3:   Y Y Y N 
2018-08-24 15:35:01.123691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10016 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-08-24 15:35:01.202813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10386 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)
2018-08-24 15:35:01.286295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 10386 MB memory) -> physical GPU (device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1)
2018-08-24 15:35:01.369518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 10386 MB memory) -> physical GPU (device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:05:00.0, compute capability: 6.1)
2018-08-24 15:35:01.827178: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2951] Segment @scope '', converted to graph
2018-08-24 15:35:01.827279: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:418] Can't find a device placement for the op!
2018-08-24 15:35:01.901005: W tensorflow/contrib/tensorrt/log/trt_logger.cc:34] DefaultLogger Half2 support requested on hardware without native FP16 support, performance will be negatively affected.
2018-08-24 15:35:02.432465: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger resources.cpp (199) - Cuda Error in gieCudaMalloc: 2
2018-08-24 15:35:02.433733: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger resources.cpp (199) - Cuda Error in gieCudaMalloc: 2
2018-08-24 15:35:02.434291: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine my_trt_op_0 creation for segment 0, composed of 1089 nodes failed: Internal: Failed to build TensorRT engine. Skipping...
2018-08-24 15:35:02.751864: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2951] Segment @scope '', converted to graph
2018-08-24 15:35:02.751964: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:418] Can't find a device placement for the op!
2018-08-24 15:35:02.856268: W tensorflow/contrib/tensorrt/log/trt_logger.cc:34] DefaultLogger Half2 support requested on hardware without native FP16 support, performance will be negatively affected.
2018-08-24 15:35:02.904384: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger resources.cpp (199) - Cuda Error in gieCudaMalloc: 2
2018-08-24 15:35:02.905595: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger resources.cpp (199) - Cuda Error in gieCudaMalloc: 2
2018-08-24 15:35:02.906038: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine my_trt_op_0 creation for segment 0, composed of 1089 nodes failed: Internal: Failed to build TensorRT engine. Skipping...
2018-08-24 15:35:03.064587: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-08-24 15:35:03.127441: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-08-24 15:35:03.160255: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:403] Optimization results for grappler item: tf_graph
2018-08-24 15:35:03.160298: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:405]   constant folding: Graph size after: 1098 nodes (-606), 1155 edges (-606), time = 96.013ms.
2018-08-24 15:35:03.160304: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:405]   layout: Graph size after: 1161 nodes (63), 1157 edges (2), time = 32.452ms.
2018-08-24 15:35:03.160322: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:405]   TensorRTOptimizer: Graph size after: 1161 nodes (0), 1157 edges (0), time = 808.131ms.
2018-08-24 15:35:03.160329: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:405]   constant folding: Graph size after: 1100 nodes (-61), 1157 edges (0), time = 80.726ms.
2018-08-24 15:35:03.160332: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:405]   TensorRTOptimizer: Graph size after: 1100 nodes (0), 1157 edges (0), time = 390.558ms.
2018-08-24 15:35:03.160335: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:403] Optimization results for grappler item: my_trt_op_0_native_segment
2018-08-24 15:35:03.160349: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:405]   constant folding: Graph size after: 1090 nodes (0), 1147 edges (0), time = 65.266ms.
2018-08-24 15:35:03.160434: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:405]   layout: Invalid argument: The graph is already optimized by layout optimizer.
2018-08-24 15:35:03.160438: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:405]   TensorRTOptimizer: Graph size after: 1090 nodes (0), 1147 edges (0), time = 9.5ms.
2018-08-24 15:35:03.160455: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:405]   constant folding: Graph size after: 1090 nodes (0), 1147 edges (0), time = 53.271ms.
2018-08-24 15:35:03.160458: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:405]   TensorRTOptimizer: Graph size after: 1090 nodes (0), 1147 edges (0), time = 9.467ms.
2018-08-24 15:35:03.336425: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0, 1, 2, 3
2018-08-24 15:35:03.336534: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-24 15:35:03.336542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 1 2 3 
2018-08-24 15:35:03.336545: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N Y Y Y 
2018-08-24 15:35:03.336548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 1:   Y N Y Y 
2018-08-24 15:35:03.336551: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 2:   Y Y N Y 
2018-08-24 15:35:03.336553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 3:   Y Y Y N 
2018-08-24 15:35:03.336822: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10016 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-08-24 15:35:03.336960: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10386 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)
2018-08-24 15:35:03.337073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 10386 MB memory) -> physical GPU (device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1)
2018-08-24 15:35:03.337142: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 10386 MB memory) -> physical GPU (device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:05:00.0, compute capability: 6.1)"
21845,android yolo sample output format is different from the official documentation,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  MacOS 10.13.5 (Non-Relevant)
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:  Non-Relevant
- **TensorFlow installed from (source or binary)**: Unsure
- **TensorFlow version (use command below)**: org.tensorflow:tensorflow-android:1.9.0 Non-Relevant
- **Python version**:  N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:N/A

### Describe the problem

In Android Yolo Sample Code

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowYoloDetector.java#L171

```java
        new float[gridWidth * gridHeight * (NUM_CLASSES + 5) * NUM_BOXES_PER_BLOCK];
        ............
        for (int y = 0; y < gridHeight; ++y) {
           for (int x = 0; x < gridWidth; ++x) {
               for (int b = 0; b < NUM_BOXES_PER_BLOCK; ++b) {
                   ..........
                    for (int c = 0; c < NUM_CLASSES; ++c) {
                        classes[c] = output[offset + 5 + c];
```
along with the following output decoding process, is not compatible with the original paper
these code assumes the output formats was 

 **grid * grid * (class + 5) * box** 

but in the original paper and other resource , this should be   

**grid * grid * (class + box * 5)**

as per in the paper [https://arxiv.org/pdf/1506.02640.pdf]( S * S * (B * 5 + C) )
this is totally different in format and size since every offset of the grid messed up
the paper indicates that the network only predicts one set of class probabilities per cell, regardless of the number of boxes B

### Source code / logs
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowYoloDetector.java#L171

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowYoloDetector.java#L187-L213

https://arxiv.org/pdf/1506.02640.pdf
"
21844,How to reshape and slice a variable without converting it to an untrainable tensor?,"[Feature Request]

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: pip-install
- **TensorFlow version (use command below)**: v1.10.0-0-g656e7a2b34 1.10.0
- **Python version**: Python 2.7.12
- **Bazel version (if compiling from source)**: 
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: Cuda compilation tools, release 7.5, V7.5.17
- **GPU model and memory**: TITAN Xp 
- **Exact command to reproduce**:  Described below 


### Describe the problem

I am using custom trainable variables in my layers, I pass the variables as a parameter to the layer (which are custom too).

I want to set (or pass) only a slice of my trainable variables array to the layer. 

For example: 

    custom_train_vars = tf.get_variable(name, [10,30,60,60], initializer=initializer, trainable=trainable)
    layer_weight = custom_train_vars[2]
    x = my_custom_conv_layer(x, layer_weight, args..)


The problem is after slicing (or reshaping with `tf.reshape`) the variable i.e. `layer_weight = custom_train_vars[2]`, it gets converted to the type `Tensor(""strided_slice:0"",args..)`

And I'm unable to use this as a training variable inside my custom layer, because my optimizer throws an error while finding the gradient of this Tensor. 

For example:

    Optimizer.compute_gradients(train_loss, layer_weight)

 Is there a workaround/feature where the slicing won't change the type of variable? 

"
21843,How to use multi-gpu with estimator ??? Hard to find the answer with simple usage ???,pip install tensorflow-gpu==1.8.0
21842,Zombie python kernel when interrupted with Ctrl+C,"Hi everyone.
I am using tensorflow 1.5 and facing the following problem.
The system UI may stuck occasionally when a python kernel running tensorflow model is interrupted with Ctrl+C. 
But I can still use ssh to log into the system and found the python kernel becomes a zombie process, which cannot be killed by terminating itself or its parent process.
The problem do not happen every time I interrupt the python program, thus I have no idea about how to  trac the cause. But since only the UI gets stuck, and *nvidia-smi* command give no response, I guess the problem may occur on releasing GPU memory or something related.   
 
I've googled it, but did not get any helpful information.
Does anyone meet the same problem or have some idea on how to debug this problem?

------------------------

### System information
- Ubuntu 16.04 LTS
- Tensorflow-gpu r1.5 with CUDA 9.0 and cudnn 7.1, installed via pip method.
- Anaconda Python 3.6
- Nvidia Geforce GTX Titan X
- Intel Core i7-4790
- 32GB Memory

"
21840,Does tensorflow provide an operation like caffe average_loss operation?,"due to the limiting of gpu, I want to update my weight after every two step training. Specifically, the network will firstly calculate the fisrt batch inputs and save the loss. And then the network calculate the next batch inputs and average these two losses and will update the weights once. It likes average_loss op in caffe, for [example](https://github.com/shelhamer/fcn.berkeleyvision.org/blob/master/voc-fcn8s/solver.prototxt) . and how to calculate the batchnorm update-ops."
21839,SSD_mobilenet_v1/0.75_quantized_coco trained model is not detecting anything after porting on Android [Detect app],"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
     No, I have only changed few lines in the config file.
- **OS Platform and Distribution: 14.04 LTS
- **Mobile device if the issue happens on mobile device**: Motoroloa, Android version 7.1.1
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.10.0
- **Python version**: 3.6
- **GCC/Compiler version (if compiling from source)**: 7.2.0
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: 4GB, 64-Bit
- **Bazel Version**: 0.16.0

### Describe the problem
Could you please help me regarding the results of the trained model.

I trained two different models from tensorflow zoo directory named ssd_mobilenet_v1_0.75_depth_quantized_coco and ssd_mobilenet_v1_quantized_coco each with 10000 training steps. I had changed the no of classes to 5 (for my data set) and used a batch size of 10 for both the models. However, the total no of of images are limited to 205.

Both the model trained well with all files generated. I **tested** the trained model using **webcam** and it worked perfectly fine.

As a next step, I used this blog by @achowdhery  [Blog Link](https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193) here and converted the .pb file into .tflite format. The generated detect.tflite is of size 3.2 Mb (This is what authors have also suggested in the blog that the file should be less than 4 Mb). However, after build over android device, **there is no detection at all**.

I tried to hold it for at least 2 minutes for each classes (by showing images of each class), but there is no result in detection. It did not even overfit and show any false detection.

What I have done wrong? Please share your thoughts.

I am trying with both the models but no luck at all. I wonder the same model **(frozen graph) is working perfectly fine for webcam based detection.**

Thank you,"
21838,Keras Layer add_variable not taking Dimension objects,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Heavily based on example custom layer on tensorflow's keras page.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Google Colab (Linux Ubuntu 17.10)
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: Unsure 
- **TensorFlow version (use command below)**: 1.10.0
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: Unsure. nvcc --version did not work.
- **GPU model and memory**: K80 GPU (Memory unsure)
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
My [question](https://stackoverflow.com/questions/51939618/what-could-cause-a-type-error-during-the-build-process-of-keras-layer) was answered on StackOverflow and a user recommended me to create a bug report here. I was creating a keras layer based on the NALU design, but I found that while on my local version of tensorflow (1.8.0) the code would run as expected, the same code would not run on Google Colab's version of tensorflow(1.10.0). The issue ended up being that tensorflow's Dimension type was not accepted in the Layer.add_variable method. It requires the dimensions to be converted to ints, which does not happen implicitly. I think that it would make sense for the method to accept Dimension objects as they are used to specify the shape of Tensors. As well, it provides backwards compatibility to previous versions of tensorflow, which do support pass Dimension objects to Layers.add_variable.


### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

Source:
```
import tensorflow as tf

class NALU(tf.keras.layers.Layer):
    def __init__(self, num_outputs, **kwargs):
        self.num_outputs = num_outputs
        super(NALU, self).__init__(**kwargs)
        
    def build(self, input_shape):
        shape = tf.TensorShape((input_shape[1], self.num_outputs)).as_list()
        get = tf.keras.initializers.get
        self.W_ = self.add_variable(""W_"", shape=shape, initializer=get('glorot_uniform'))
        self.M_ = self.add_variable(""M_"", shape=shape, initializer=get('glorot_uniform'))
        self.GAM = self.add_variable(""GAM"", shape=shape, initializer=get('glorot_uniform')) # Gate add & multiply
        self.GM = self.add_variable(""GM"", shape=shape, initializer=get('glorot_uniform')) # Gate multiply
        
        super(NALU, self).build(input_shape)
    
    def call(self, x):
        gam = tf.sigmoid(tf.matmul(x, self.GAM)) # The gate
        gm = tf.sigmoid(tf.matmul(x, self.GM)) # The gate
        
        W = tf.tanh(self.W_) * tf.sigmoid(self.M_)
        add = tf.matmul(x, W)
        
        # represents positive multiplication
        m = tf.exp(
            tf.matmul(tf.log(tf.abs(x) + 1e-14), W)
        )
        
        mul = (-1 * m) * (1.0 - gm) + gm * m 
        
        y = gam * add + (1.0 - gam) * mul
        return y
    
    def compute_output_shape(self, input_shape):
        # --------------IMPORTANT LINE---------------------
        shape = tf.TensorShape(input_shape)
        #  shape = tf.TensorShape(input_shape).as_list() is required to make the function work
        # -------------------------------------------------
        shape[-1] = self.num_outputs
        return tf.TensorShape(shape)
    
    def get_config(self):
        base_config = super(NALU, self).get_config()
        base_config['num_outputs'] = self.num_outputs
        
    @classmethod
    def from_config(cls, config):
        return cls(**config)

def nalu_model():
    inp = tf.keras.layers.Input(shape=(2,))
    out = NALU(1)(inp)
    
    model = tf.keras.models.Model(inputs=inp, outputs=out)
    return model
```
Test Case:
```
import numpy as np

def create_train():
#     x_train = np.random.randint(-100, 100, size=(10000000, 2), dtype=np.int32)
    x_train = np.random.uniform(-100, 100, size=(10000000, 2))

    first_neg = np.copy(x_train)
    first_neg[:, 0] *= -1

    second_neg = np.copy(x_train)
    second_neg[:, 1] *= -1

    all_neg = x_train * -1

    x_train = np.append(x_train, first_neg)
    x_train = np.append(x_train, second_neg)
    x_train = np.append(x_train, all_neg)
    x_train = x_train.reshape((40000000, 2))

    com = x_train[:,::-1]
    x_train = np.append(x_train, com).reshape((80000000, 2))
    y_train = x_train[:, 0] * x_train[:, 1]
    
    return x_train, y_train

x_train, y_train = create_train()
x_test = np.random.randint(-1000, 1000, size=(10000, 2))
y_test = x_test[:, 0] * x_test[:, 1]

model = nalu_model()

model.compile(optimizer='RMSProp',
              loss='MSE',
              metrics=['accuracy', 'MAE'])

cb = [tf.keras.callbacks.TensorBoard(log_dir='./add_logs', histogram_freq=1, batch_size=32, write_graph=True, write_grads=True),
      tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.00000001, patience=5, verbose=2, mode='auto')]

model.fit(x_train, y_train, epochs=500, batch_size=256, shuffle=True, callbacks=cb, validation_split=0.4)
model.evaluate(x_test, y_test)
```


Error:
```
TypeError                                 Traceback (most recent call last)
<ipython-input-7-55fb94a8f3b1> in <module>()
     82 y_test = x_test[:, 0] * x_test[:, 1]
     83 
---> 84 model = nalu_model()
     85 
     86 model.compile(optimizer='RMSProp',

<ipython-input-7-55fb94a8f3b1> in nalu_model()
     48 def nalu_model():
     49     inp = tf.keras.layers.Input(shape=(2,))
---> 50     out = NALU(1)(inp)
     51 
     52     model = tf.keras.models.Model(inputs=inp, outputs=out)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    726         if all(hasattr(x, 'shape') for x in input_list):
    727           input_shapes = nest.map_structure(lambda x: x.shape, inputs)
--> 728         self.build(input_shapes)
    729         self.built = True
    730 

<ipython-input-7-55fb94a8f3b1> in build(self, input_shape)
      9         shape = tf.TensorShape((input_shape[1], self.num_outputs))
     10         get = tf.keras.initializers.get
---> 11         self.W_ = self.add_variable(""W_"", shape=shape, initializer=get('glorot_uniform'))
     12         self.M_ = self.add_variable(""M_"", shape=shape, initializer=get('glorot_uniform'))
     13         self.GAM = self.add_variable(""GAM"", shape=shape, initializer=get('glorot_uniform')) # Gate add & multiply

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in add_variable(self, *args, **kwargs)
    459   def add_variable(self, *args, **kwargs):
    460     """"""Alias for `add_weight`.""""""
--> 461     return self.add_weight(*args, **kwargs)
    462 
    463   def add_weight(self,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in add_weight(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, getter)
    563         use_resource=use_resource,
    564         synchronization=synchronization,
--> 565         aggregation=aggregation)
    566 
    567     if regularizer is not None:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/checkpointable/base.py in _add_variable_with_custom_getter(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)
    533     new_variable = getter(
    534         name=name, shape=shape, dtype=dtype, initializer=initializer,
--> 535         **kwargs_for_getter)
    536 
    537     # If we set an initializer and the variable processed it, tracking will not

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in make_variable(name, shape, dtype, initializer, partition_info, trainable, caching_device, validate_shape, constraint, use_resource, synchronization, aggregation, partitioner)
   1916       use_resource=use_resource,
   1917       synchronization=synchronization,
-> 1918       aggregation=aggregation)
   1919   return v

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in variable(initial_value, trainable, collections, validate_shape, caching_device, name, dtype, constraint, use_resource, synchronization, aggregation)
   2441       use_resource=use_resource,
   2442       synchronization=synchronization,
-> 2443       aggregation=aggregation)
   2444 
   2445 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in <lambda>(**kwargs)
   2423              synchronization=VariableSynchronization.AUTO,
   2424              aggregation=VariableAggregation.NONE):
-> 2425   previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)
   2426   for getter in ops.get_default_graph()._variable_creator_stack:  # pylint: disable=protected-access
   2427     previous_getter = _make_getter(getter, previous_getter)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in default_variable_creator(next_creator, **kwargs)
   2393         collections=collections, validate_shape=validate_shape,
   2394         caching_device=caching_device, name=name, dtype=dtype,
-> 2395         constraint=constraint)
   2396   elif not use_resource and context.executing_eagerly():
   2397     raise RuntimeError(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py in __init__(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint)
    310           name=name,
    311           dtype=dtype,
--> 312           constraint=constraint)
    313 
    314   # pylint: disable=unused-argument

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py in _init_from_args(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, constraint)
    415               with ops.name_scope(""Initializer""), ops.device(None):
    416                 initial_value = ops.convert_to_tensor(
--> 417                     initial_value(), name=""initial_value"", dtype=dtype)
    418               self._handle = _eager_safe_variable_handle(
    419                   shape=initial_value.get_shape(),

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in <lambda>()
   1901         initializer = initializer(dtype=dtype)
   1902       init_val = lambda: initializer(  # pylint: disable=g-long-lambda
-> 1903           shape, dtype=dtype, partition_info=partition_info)
   1904       variable_dtype = dtype.base_dtype
   1905   if use_resource is None:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py in __call__(self, shape, dtype, partition_info)
    474       scale /= max(1., fan_out)
    475     else:
--> 476       scale /= max(1., (fan_in + fan_out) / 2.)
    477     if self.distribution == ""normal"" or self.distribution == ""truncated_normal"":
    478       # constant taken from scipy.stats.truncnorm.std(a=-2, b=2, loc=0., scale=1.)

TypeError: unsupported operand type(s) for /: 'Dimension' and 'float'
```"
21837,Incorrect gradients when different Python variables are assigned the same tf.constant() value.,"When different Python variables are assigned the same tf.constant() value, the computed gradients are incorrect.
```
import tensorflow as tf

tf.enable_eager_execution()
tfe = tf.contrib.eager

# When `x` and `w` are assigned the same constant value, the gradient is
# incorrect.
x = tf.constant(3.0)
w = tf.constant(3.0)
with tf.GradientTape() as g:
  g.watch(x)
  y = x * w
print(g.gradient(y, x))  # Prints 6.0 when the gradient should be 3.0.

# I assume this happens because `x` and `w` reference the same object.
print(x is w)  # Prints True.

# When `x` and `w` are assigned different constant values, the gradient is
# correct.
x = tf.constant(3.1)
w = tf.constant(3.0)
with tf.GradientTape() as g:
  g.watch(x)
  y = x * w
print(g.gradient(y, x))  # Prints 3.0, the correct gradient.


# This also happens with `tfe.gradients_function()`.
def multiply_by_3(x):
  return tf.multiply(x, tf.constant(3.0))

grad = tfe.gradients_function(multiply_by_3)

print(grad(3.0))  # Prints 6.0 when the gradient should be 3.0.
print(grad(3.1))  # Prints 3.0, the correct gradient.
```
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Only the code above.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Mac OS X 10.13.6
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
N/A
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version**:
1.10.0
- **Python version**:
2.7.15
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
Running the code above.
"
21836,tf.test.is_gpu_available(True) allocates all GPU(s) VRAM,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**:  1.10.0-devel-gpu-py3
- **TensorFlow version (use command below)**: 1.10.0-devel-gpu-py3
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: CUDA: V9.0.176 cuDNN: V7.1.4
- **GPU model and memory**: 2x 1080ti 11GB
- **Exact command to reproduce**:

```
import tensorflow as tf

data_format = 'channels_first' if tf.test.is_gpu_available(True) \
    else 'channels_last'
# At this point, all memory is allocated across all GPUs
config = tf.ConfigProto()
config.gpu_options.allow_growth = True
sess = tf.Session(config=config)
# All VRAM still allocated
```

### Describe the problem
Using tf.test.is_gpu_available(True) to check if a system GPU is available allocates all the VRAM available."
21835,New feature request: LOBPCG in addition to Lanczos?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
all
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
all
- **TensorFlow installed from (source or binary)**:
N/A
- **TensorFlow version (use command below)**:
N/A
- **Python version**:
N/A
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
N/A

### Describe the problem
TensorFlow now has a native implementation of Lanczos https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/solvers/python/ops/lanczos.py

It could be useful to add a native TensorFlow implementation of LOBPCG, see https://en.wikipedia.org/wiki/LOBPCG which is an alternative to Lanczos  and has some advantages, e.g., warm-starts. SciPy has the Python native implementation https://docs.scipy.org/doc/scipy-1.1.0/reference/generated/scipy.sparse.linalg.lobpcg.html used in Scikit for manifold spectral embedding http://scikit-learn.org/stable/modules/generated/sklearn.manifold.spectral_embedding.html and for spectral clustering http://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html

Reference implementations of LOBPCG are described in 

1. Knyazev, Andrew V. (2001). ""Toward the Optimal Preconditioned Eigensolver: Locally Optimal Block Preconditioned Conjugate Gradient Method"". SIAM Journal on Scientific Computing. 23 (2): 517541. doi:10.1137/S1064827500366124
2. Knyazev, A. V.; Argentati, M. E.; Lashuk, I.; Ovtchinnikov, E. E. (2007). ""Block Locally Optimal Preconditioned Eigenvalue Xolvers (BLOPEX) in Hypre and PETSc"". SIAM Journal on Scientific Computing. 29 (5): 2224. arXiv:0705.2626Freely accessible. doi:10.1137/060661624

LOBPCG can be easily adopted to compute partial SVD and PCA for a data matrix X without ever computing its covariance matrix X'*X, i.e. in matrix-free fashion, see comments at https://www.mathworks.com/matlabcentral/fileexchange/48-lobpcg-m

NVIDIA has implemented LOBPCG in its nvGRAPH library introduced in CUDA 8.

A first simple step could be to write a single-vector (non-block) version LOPCG, which would be very similar to already existing https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/solvers/python/ops/linear_equations.py
but instead of solving Ax=rhs would produce a single eigenvector of selfadjoint matrix `A`  matrix-free where the action of the matrix A is represented by  `operator`, corresponding to the largest (or smallest) eigenvalue, possibly constrained to be orthogonal to a set of given vectors Y. The latter would allow computing several main eigenvectors one-by-one, putting all previously computed eigenvectors into Y.

I can help with the implementation if numerical issues arise.

### Source code / logs
N/A
"
21834,Compiling tensorflow lite on gcc 4.7,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 
Raspbian 

- **TensorFlow installed from (source or binary)**:
Installed from source

- **TensorFlow version (use command below)**:
Release 1.9.0

- **Bazel version (if compiling from source)**:
N/A using make to compile Tensorflow Lite

- **GCC/Compiler version (if compiling from source)**:
GCC version 4.7.3
- **CUDA/cuDNN version**:
N/a
- **GPU model and memory**:
N/a
- **Mobile device**:
N/a

- **Exact command to reproduce**:
1. Run ./tensorflow/contrib/lite/download_dependencies.sh command from root directory 
2. Copy resulting downloads folder, ios_makefile.inc, rpi_makefile.inc and the Makefile found in ./tensorflow/contrib/lite folder into the root directory
3. From the root directory run make 
4. Should result in ""gen"" folder with tensorflowlite.a library

### Describe the problem
I am trying to get tensorflow lite up and running on some older IOT devices that have gcc version 4.7 installed (not possible to upgrade from this version either). Have not been able to do so yet so I am wondering if anyone has had an success doing so or if anyone one knows what steps I would have to take to do so. 

### Source code / logs
When attempting to compile on gcc 4.7 the make output is:

g++ -mfpu=neon -pthread -fPIC --std=c++11 -O3 -DNDEBUG -I. -I/home/Documents/Test/ststicLibs/include/tensorflow-master/../../../ -I/home/Documents/Test/ststicLibs/include/tensorflow-master/downloads/ -I/home/Documents/Test/ststicLibs/include/tensorflow-master/downloads/eigen -I/home/Documents/Test/ststicLibs/include/tensorflow-master/downloads/gemmlowp -I/home/Documents/Test/ststicLibs/include/tensorflow-master/downloads/neon_2_sse -I/home/Documents/Test/ststicLibs/include/tensorflow-master/downloads/farmhash/src -I/home/Documents/Test/ststicLibs/include/tensorflow-master/downloads/flatbuffers/include -I/home/Documents/Test/ststicLibs/include/tensorflow-master/gen/obj/ -I/usr/local/include -c tensorflow/contrib/lite/allocation.cc -o /home/Documents/Test/ststicLibs/include/tensorflow-master/gen/obj/tensorflow/contrib/lite/allocation.o
In file included from /usr/include/c++/4.7/memory:86:0,
                 from ./tensorflow/contrib/lite/simple_memory_arena.h:19,
                 from ./tensorflow/contrib/lite/allocation.h:25,
                 from tensorflow/contrib/lite/allocation.cc:29:
/usr/include/c++/4.7/bits/unique_ptr.h: In instantiation of std::unique_ptr<_Tp [], _Dp>& std::unique_ptr<_Tp [], _Dp>::operator=(std::unique_ptr<_Up, _Ep>&&) [with _Up = char []; _Ep = std::default_delete<char []>; _Tp = const char; _Dp = std::default_delete<const char []>; std::unique_ptr<_Tp [], _Dp> = std::unique_ptr<const char []>]:
tensorflow/contrib/lite/allocation.cc:102:36:   required from here
/usr/include/c++/4.7/bits/unique_ptr.h:340:4: error: use of deleted function void std::unique_ptr<_Tp [], _Dp>::reset(_Up) [with _Up = char*; _Tp = const char; _Dp = std::default_delete<const char []>]
/usr/include/c++/4.7/bits/unique_ptr.h:404:7: error: declared here
/usr/include/c++/4.7/bits/unique_ptr.h:341:4: error: no match for operator= in std::unique_ptr<_Tp [], _Dp>::get_deleter<const char, std::default_delete<const char []> >() = std::forward<std::default_delete<char []> >((* &(& __u)->std::unique_ptr<_Tp [], _Dp>::get_deleter<char, std::default_delete<char []> >()))
/usr/include/c++/4.7/bits/unique_ptr.h:341:4: note: candidates are:
/usr/include/c++/4.7/bits/unique_ptr.h:71:12: note: std::default_delete<const char []>& std::default_delete<const char []>::operator=(const std::default_delete<const char []>&)
/usr/include/c++/4.7/bits/unique_ptr.h:71:12: note:   no known conversion for argument 1 from std::default_delete<char []> to const std::default_delete<const char []>&
/usr/include/c++/4.7/bits/unique_ptr.h:71:12: note: std::default_delete<const char []>& std::default_delete<const char []>::operator=(std::default_delete<const char []>&&)
/usr/include/c++/4.7/bits/unique_ptr.h:71:12: note:   no known conversion for argument 1 from std::default_delete<char []> to std::default_delete<const char []>&&
Makefile:203: recipe for target '/home/Documents/Test/ststicLibs/include/tensorflow-master/gen/obj/tensorflow/contrib/lite/allocation.o' failed
make: *** [/home/Documents/Test/ststicLibs/include/tensorflow-master/gen/obj/tensorflow/contrib/lite/allocation.o] Error 1

"
21833,ppc64le: //tensorflow/contrib/tensor_forest:scatter_add_ndim_op_test,"Please assign this issue to me

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: PPC64LE Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: master branch git clone from 8/23/18 (Last commit 9289302ad3d7941ddb9ce2d0dff56b333cbcf208)
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.15.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 9.0, 7
- **GPU model and memory**: 4 V100 GPUs with 16 GB of memory each
- **Exact command to reproduce**:
azel test --config=cuda --test_tag_filters=-no_oss,-oss_serial,-no_gpu,-benchmark-test --test_timeout 300,450,1200,3600 --local_test_jobs=4 --test_output=errors --build_tests_                              //tensorflow/tools/ci_build/gpu_build:parallel_gpu_execute //tensorflow/contrib/tensor_forest:scatter_add_ndim_op_test


### Describe the problem

5 of 6 testcases fail with error similar to:
```
======================================================================
FAIL: testBadInput (__main__.ScatterAddNdimTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorf                              on/kernel_tests/scatter_add_ndim_op_test.py"", line 75, in testBadInput
    self.assertAllEqual(init_val, input_data.eval())
  File ""/opt/anaconda2/lib/python2.7/contextlib.py"", line 35, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorf                              .py"", line 1615, in assertRaisesWithPredicateMatch
    str(e)))
AssertionError: Exception of type <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>: AttrValue must not have reference type value of float_ref
         for attr 'tensor_type'
        ; NodeDef: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_dev                              =""edge_2_Variable"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas); Op<name=_Recv; signature= -> tensor:tensor_type; attr=t                              ame:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>
         [[Node: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_devic                              edge_2_Variable"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas)]]

----------------------------------------------------------------------
```


### Source code / logs
```
[root@690470e3d41a workspace]# bazel test --config=cuda --test_tag_filters=-no_oss,-oss_serial,-no_gpu,-benchmark-test --test_timeout 300,450,1200,3600 --local_test_jobs=4 --test_output=errors --build_tests_only --config=opt --run_under=//tensorflow/tools/ci_build/gpu_build:parallel_gpu_execute //tensorflow/contrib/tensor_forest:scatter_add_ndim_op_test
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
WARNING: /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/grpc/bazel/grpc_build_system.bzl:172:12
INFO: Analysed target //tensorflow/contrib/tensor_forest:scatter_add_ndim_op_test (129 packages loaded).
INFO: Found 1 test target...
FAIL: //tensorflow/contrib/tensor_forest:scatter_add_ndim_op_test (see /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/testlogs/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test/test.log)
INFO: From Testing //tensorflow/contrib/tensor_forest:scatter_add_ndim_op_test:
==================== Test output for //tensorflow/contrib/tensor_forest:scatter_add_ndim_op_test:
Running test /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test  on GPU 0
2018-08-21 21:51:07.422276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties:
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0004:04:00.0
totalMemory: 15.75GiB freeMemory: 15.32GiB
2018-08-21 21:51:07.422327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2018-08-21 21:51:07.669332: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-21 21:51:07.669384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0
2018-08-21 21:51:07.669393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N
2018-08-21 21:51:07.669859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4838 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0004:04:00.0, compute capability: 7.0)
2018-08-21 21:51:07.725500: E tensorflow/core/framework/op_segment.cc:53] Create kernel failed: Invalid argument: AttrValue must not have reference type value of float_ref
         for attr 'tensor_type'
        ; NodeDef: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_2_Variable"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>
2018-08-21 21:51:07.725565: E tensorflow/core/common_runtime/executor.cc:697] Executor failed to create kernel. Invalid argument: AttrValue must not have reference type value of float_ref
         for attr 'tensor_type'
        ; NodeDef: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_2_Variable"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>
         [[Node: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_2_Variable"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas)]]
E2018-08-21 21:51:07.730994: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2018-08-21 21:51:07.731026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-21 21:51:07.731034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0
2018-08-21 21:51:07.731041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N
2018-08-21 21:51:07.731432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4838 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0004:04:00.0, compute capability: 7.0)
2018-08-21 21:51:07.741437: E tensorflow/core/framework/op_segment.cc:53] Create kernel failed: Invalid argument: AttrValue must not have reference type value of float_ref
         for attr 'tensor_type'
        ; NodeDef: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_2_Variable"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>
2018-08-21 21:51:07.741477: E tensorflow/core/common_runtime/executor.cc:697] Executor failed to create kernel. Invalid argument: AttrValue must not have reference type value of float_ref
         for attr 'tensor_type'
        ; NodeDef: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_2_Variable"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>
         [[Node: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_2_Variable"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas)]]
E2018-08-21 21:51:07.745582: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2018-08-21 21:51:07.745601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-21 21:51:07.745608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0
2018-08-21 21:51:07.745615: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N
2018-08-21 21:51:07.745978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4838 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0004:04:00.0, compute capability: 7.0)
2018-08-21 21:51:07.755530: E tensorflow/core/framework/op_segment.cc:53] Create kernel failed: Invalid argument: AttrValue must not have reference type value of float_ref
         for attr 'tensor_type'
        ; NodeDef: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_2_Variable"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>
2018-08-21 21:51:07.755565: E tensorflow/core/common_runtime/executor.cc:697] Executor failed to create kernel. Invalid argument: AttrValue must not have reference type value of float_ref
         for attr 'tensor_type'
        ; NodeDef: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_2_Variable"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>
         [[Node: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_2_Variable"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas)]]
F2018-08-21 21:51:07.760553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2018-08-21 21:51:07.760575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-21 21:51:07.760583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0
2018-08-21 21:51:07.760590: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N
2018-08-21 21:51:07.760945: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4838 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0004:04:00.0, compute capability: 7.0)
2018-08-21 21:51:07.770746: E tensorflow/core/framework/op_segment.cc:53] Create kernel failed: Invalid argument: AttrValue must not have reference type value of float_ref
         for attr 'tensor_type'
        ; NodeDef: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_2_Variable"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>
2018-08-21 21:51:07.770780: E tensorflow/core/common_runtime/executor.cc:697] Executor failed to create kernel. Invalid argument: AttrValue must not have reference type value of float_ref
         for attr 'tensor_type'
        ; NodeDef: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_2_Variable"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>
         [[Node: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_2_Variable"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas)]]
E2018-08-21 21:51:07.774856: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2018-08-21 21:51:07.774874: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-21 21:51:07.774882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0
2018-08-21 21:51:07.774889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N
2018-08-21 21:51:07.775259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4838 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0004:04:00.0, compute capability: 7.0)
2018-08-21 21:51:07.787517: E tensorflow/core/framework/op_segment.cc:53] Create kernel failed: Invalid argument: AttrValue must not have reference type value of float_ref
         for attr 'tensor_type'
        ; NodeDef: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_2_Variable"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>
2018-08-21 21:51:07.787565: E tensorflow/core/common_runtime/executor.cc:697] Executor failed to create kernel. Invalid argument: AttrValue must not have reference type value of float_ref
         for attr 'tensor_type'
        ; NodeDef: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_2_Variable"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>
         [[Node: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_2_Variable"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas)]]
E.
======================================================================
ERROR: test1dim (__main__.ScatterAddNdimTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/contrib/tensor_forest/python/kernel_tests/scatter_add_ndim_op_test.py"", line 37, in test1dim
    tensor_forest_ops.scatter_add_ndim(input_data, indices, updates).run()
  File ""/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 2241, in run
    _run_using_default_session(self, feed_dict, self.graph, session)
  File ""/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 4986, in _run_using_default_session
    session.run(operation, feed_dict)
  File ""/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 877, in run
    run_metadata_ptr)
  File ""/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1272, in _do_run
    run_metadata)
  File ""/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1291, in _do_call
    raise type(e)(node_def, op, message)
InvalidArgumentError: AttrValue must not have reference type value of float_ref
         for attr 'tensor_type'
        ; NodeDef: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_2_Variable"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>
         [[Node: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_2_Variable"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas)]]

======================================================================
ERROR: test3dim (__main__.ScatterAddNdimTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/contrib/tensor_forest/python/kernel_tests/scatter_add_ndim_op_test.py"", line 50, in test3dim
    tensor_forest_ops.scatter_add_ndim(input_data, indices, updates).run()
  File ""/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 2241, in run
    _run_using_default_session(self, feed_dict, self.graph, session)
  File ""/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 4986, in _run_using_default_session
    session.run(operation, feed_dict)
  File ""/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 877, in run
    run_metadata_ptr)
  File ""/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1272, in _do_run
    run_metadata)
  File ""/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1291, in _do_call
    raise type(e)(node_def, op, message)
InvalidArgumentError: AttrValue must not have reference type value of float_ref
         for attr 'tensor_type'
        ; NodeDef: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_2_Variable"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>
         [[Node: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_2_Variable"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas)]]

======================================================================
ERROR: testIncompleteIndices (__main__.ScatterAddNdimTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/contrib/tensor_forest/python/kernel_tests/scatter_add_ndim_op_test.py"", line 85, in testIncompleteIndices
    tensor_forest_ops.scatter_add_ndim(input_data, indices, updates).run()
  File ""/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 2241, in run
    _run_using_default_session(self, feed_dict, self.graph, session)
  File ""/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 4986, in _run_using_default_session
    session.run(operation, feed_dict)
  File ""/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 877, in run
    run_metadata_ptr)
  File ""/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1272, in _do_run
    run_metadata)
  File ""/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1291, in _do_call
    raise type(e)(node_def, op, message)
InvalidArgumentError: AttrValue must not have reference type value of float_ref
         for attr 'tensor_type'
        ; NodeDef: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_2_Variable"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>
         [[Node: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_2_Variable"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas)]]

======================================================================
ERROR: testNoUpdates (__main__.ScatterAddNdimTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/contrib/tensor_forest/python/kernel_tests/scatter_add_ndim_op_test.py"", line 62, in testNoUpdates
    tensor_forest_ops.scatter_add_ndim(input_data, indices, updates).run()
  File ""/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorf                              line 2241, in run
    _run_using_default_session(self, feed_dict, self.graph, session)
  File ""/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorf                              line 4986, in _run_using_default_session
    session.run(operation, feed_dict)
  File ""/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorf                               line 877, in run
    run_metadata_ptr)
  File ""/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorf                               line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorf                               line 1272, in _do_run
    run_metadata)
  File ""/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorf                               line 1291, in _do_call
    raise type(e)(node_def, op, message)
InvalidArgumentError: AttrValue must not have reference type value of float_ref
         for attr 'tensor_type'
        ; NodeDef: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_dev                              =""edge_2_Variable"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas); Op<name=_Recv; signature= -> tensor:tensor_type; attr=t                              ame:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>
         [[Node: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_devic                              edge_2_Variable"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas)]]

======================================================================
FAIL: testBadInput (__main__.ScatterAddNdimTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorf                              on/kernel_tests/scatter_add_ndim_op_test.py"", line 75, in testBadInput
    self.assertAllEqual(init_val, input_data.eval())
  File ""/opt/anaconda2/lib/python2.7/contextlib.py"", line 35, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorf                              .py"", line 1615, in assertRaisesWithPredicateMatch
    str(e)))
AssertionError: Exception of type <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>: AttrValue must not have reference type value of float_ref
         for attr 'tensor_type'
        ; NodeDef: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_dev                              =""edge_2_Variable"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas); Op<name=_Recv; signature= -> tensor:tensor_type; attr=t                              ame:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>
         [[Node: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_devic                              edge_2_Variable"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas)]]

----------------------------------------------------------------------
Ran 6 tests in 1.097s

FAILED (failures=1, errors=4)
================================================================================
Target //tensorflow/contrib/tensor_forest:scatter_add_ndim_op_test up-to-date:
  bazel-bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test
INFO: Elapsed time: 64.835s, Critical Path: 45.36s
INFO: 1 process: 1 local.
INFO: Build completed, 1 test FAILED, 5 total actions
//tensorflow/contrib/tensor_forest:scatter_add_ndim_op_test              FAILED in 2.8s
  /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/testlogs/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test/test.log
```
"
21832,Status: CUDA driver version is insufficient for CUDA runtime version,"- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 
Kernel: 2.6.32-573.12.1.el6.x86_64 
Host: RHEL 6.7
 Container: Ubuntu 16.04.5 LTS 

- **TensorFlow installed from (source or binary)**: 
Singularity 

- **TensorFlow version (use command below)**:
Tensorflow:1.10.0-devel-gpu-py3

- **Python version**:
Python 3.5.2

- **GCC/Compiler version (if compiling from source)**:
GCC 5.4.0

- **CUDA/cuDNN version**:
9

- **GPU model and memory**:
Singularity tensorflow:1.10.0-devel-gpu-py3:~> nvidia-smi
Thu Aug 23 00:24:41 2018
+------------------------------------------------------+
| NVIDIA-SMI 352.39 Driver Version: 352.39 |
|-------------------------------+----------------------+----------------------+
| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |
| Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |
|===============================+======================+======================|
| 0 Tesla K80 Off | 0000:84:00.0 Off | 0 |
| N/A 39C P0 58W / 149W | 22MiB / 11519MiB | 0% E. Process |
+-------------------------------+----------------------+----------------------+

- **Exact command to reproduce**:
$ # install nvidia driver v352.39
$ sudo singularity build --sandbox /path/to/sandbox docker://tensorflow/tensorflow/1.10.0-devel-gpu-py3
$ singularity shell -nv /path/to/sandbox
Singularity tensorflow:1.10.0-devel-gpu-py3:~> nvidia-smi
Thu Aug 23 00:24:41 2018
+------------------------------------------------------+
| NVIDIA-SMI 352.39 Driver Version: 352.39 |
|-------------------------------+----------------------+----------------------+
| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |
| Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |
|===============================+======================+======================|
| 0 Tesla K80 Off | 0000:84:00.0 Off | 0 |
| N/A 39C P0 58W / 149W | 22MiB / 11519MiB | 0% E. Process |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes: GPU Memory |
| GPU PID Type Process name Usage |
|=============================================================================|
| No running processes found |
+-----------------------------------------------------------------------------+
Singularity tensorflow:1.10.0-devel-gpu-py3:~> python3
Python 3.5.2 (default, Nov 23 2017, 16:37:01)
[GCC 5.4.0 20160609] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.

            from tensorflow.python.client import device_lib
            print(device_lib.list_local_devices())
            2018-08-23 00:26:35.424225: I
            tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports
            instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
            2018-08-23 00:26:38.208490: I
            tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with
            properties:
            name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
            pciBusID: 0000:84:00.0
            totalMemory: 11.25GiB freeMemory: 11.16GiB
            2018-08-23 00:26:38.208576: I
            tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu
            devices: 0
            Traceback (most recent call last):
            File """", line 1, in
            File
            ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/device_lib.py"",
            line 41, in list_local_devices
            for s in pywrap_tensorflow.list_devices(session_config=session_config)
            File
            ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"",
            line 1679, in list_devices
            return ListDevices(status)
            File
            ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py"",
            line 519, in exit
            c_api.TF_GetCode(self.status.status))
            tensorflow.python.framework.errors_impl.InternalError: cudaGetDevice() failed.
            Status: CUDA driver version is insufficient for CUDA runtime version


### Describe the problem
I built a tensorflow container with singularity. I think there might be a mismatch between the some of the card drivers and cuda libraries between the host and container. I have the container built as a sandbox so I'm able to make modifications quiet easily, I was curious if there's a way I can install appropriate cuda driver and runtimes to the container, and have the container run off those instead of pulling libraries from the host which are incompatible with the container? Is this the right way to do it? Or should I be updating the cuda drivers / libraries on the host to match the container? 
"
21831,Running Tensorflow example graph with TensorRT 4 backend fails,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: r1.10
- **Python version**: python 3.5
- **Bazel version (if compiling from source)**: 1.6
- **GCC/Compiler version (if compiling from source)**: 5
- **CUDA/cuDNN version**: 7.0
- **GPU model and memory**: 4GB Quadro M4000M
- **Exact command to reproduce**:

### Describe the problem
I am trying to follow the examples provided in tensorflow/contrib/tensorrt/test and the one provided by the Google blog post [here](https://developers.googleblog.com/2018/03/tensorrt-integration-with-tensorflow.html) which provides code on running a more complex ResNet architecture [here](https://developer.download.nvidia.com/devblogs/tftrt_sample.tar.xz).

The tests test_tftrt.py and tf_trt_integration_test.py are marked as passed. However, the code provided in the other link fails. In both situations I get information like this, which claims it found no eligible GPUs (?) I am definitely using the GPU with Tensorflow though, I can see this in nvidia-smi and via a simple log placement test (separately).

```
2018-08-23 17:20:22.067239: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 0
2018-08-23 17:20:22.078438: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:756] MULTIPLE tensorrt candidate conversion: 2
2018-08-23 17:20:22.078599: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph
2018-08-23 17:20:22.079079: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph
2018-08-23 17:20:22.079471: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:724] Can't determine the device, constructing an allocator at device 0
2018-08-23 17:20:22.186648: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:724] Can't determine the device, constructing an allocator at device 0
```

The output message of running the ResNet execution is the following. It complains that it's out of memory, even though I try to reduce the batch size to 1, reduce the image size to a tiny size.

Am I missing something? The 4GB memory is not huge, but it's the same as a DrivePX2 discrete GPU...

```
francesco@franny:~/Downloads/tftrt$ ./run_all.sh 
/home/francesco/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Namespace(FP16=True, FP32=True, INT8=True, batch_size=1, dump_diff=False, native=True, num_loops=10, topN=5, update_graphdef=False, with_timeline=False, workspace_size=2048)
Starting at 2018-08-23 17:26:13.644094
2018-08-23 17:26:13.719781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-08-23 17:26:13.720536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: 
name: Quadro M2000M major: 5 minor: 0 memoryClockRate(GHz): 1.137
pciBusID: 0000:01:00.0
totalMemory: 3.95GiB freeMemory: 3.69GiB
2018-08-23 17:26:13.720568: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2018-08-23 17:26:14.058416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-23 17:26:14.058443: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 
2018-08-23 17:26:14.058451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N 
2018-08-23 17:26:14.058669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2020 MB memory) -> physical GPU (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0, compute capability: 5.0)
INFO:tensorflow:Starting execution
2018-08-23 17:26:14.508197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2018-08-23 17:26:14.508248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-23 17:26:14.508254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 
2018-08-23 17:26:14.508258: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N 
2018-08-23 17:26:14.508394: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2020 MB memory) -> physical GPU (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0, compute capability: 5.0)
INFO:tensorflow:Starting Warmup cycle
2018-08-23 17:26:17.058524: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.25GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-08-23 17:26:17.072068: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.26GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
INFO:tensorflow:Warmup done. Starting real timing
iter  0   0.025112714767456055
iter  1   0.025174999237060548
iter  2   0.02513185977935791
iter  3   0.025080199241638183
iter  4   0.02514298439025879
iter  5   0.025306134223937987
iter  6   0.02516295909881592
iter  7   0.02516087532043457
iter  8   0.025412368774414062
iter  9   0.025518035888671874
Comparison= True
INFO:tensorflow:Timing loop done!
images/s : 39.7 +/- 0.2, s/batch: 0.02522 +/- 0.00014
RES, Native, 1, 39.65, 0.21, 0.02522, 0.00014
INFO:tensorflow:Running against TensorRT version 4.0.1
2018-08-23 17:26:30.386025: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 0
2018-08-23 17:26:30.831639: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope 'resnet_v1_50/', converted to graph
2018-08-23 17:26:30.929338: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:724] Can't determine the device, constructing an allocator at device 0
2018-08-23 17:26:41.310315: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.00GiB.  Current allocation summary follows.
2018-08-23 17:26:41.310388: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (256): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-08-23 17:26:41.310413: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (512): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-08-23 17:26:41.310434: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1024): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-08-23 17:26:41.310453: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2048): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-08-23 17:26:41.310473: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4096): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-08-23 17:26:41.310492: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8192): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-08-23 17:26:41.310515: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16384): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-08-23 17:26:41.310535: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (32768): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-08-23 17:26:41.310555: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (65536): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-08-23 17:26:41.310574: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (131072): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-08-23 17:26:41.310594: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (262144): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-08-23 17:26:41.310622: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (524288): 	Total Chunks: 1, Chunks in use: 1. 620.0KiB allocated for chunks. 620.0KiB in use in bin. 620.0KiB client-requested in use in bin.
2018-08-23 17:26:41.310643: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1048576): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-08-23 17:26:41.310666: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2097152): 	Total Chunks: 1, Chunks in use: 1. 3.06MiB allocated for chunks. 3.06MiB in use in bin. 3.06MiB client-requested in use in bin.
2018-08-23 17:26:41.310698: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4194304): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-08-23 17:26:41.310730: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8388608): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-08-23 17:26:41.310762: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16777216): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-08-23 17:26:41.310794: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (33554432): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-08-23 17:26:41.310827: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (67108864): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-08-23 17:26:41.310860: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (134217728): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-08-23 17:26:41.310887: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (268435456): 	Total Chunks: 1, Chunks in use: 0. 1.97GiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-08-23 17:26:41.310910: I tensorflow/core/common_runtime/bfc_allocator.cc:646] Bin for 2.00GiB was 256.00MiB, Chunk State: 
2018-08-23 17:26:41.310937: I tensorflow/core/common_runtime/bfc_allocator.cc:652]   Size: 1.97GiB | Requested Size: 3.9KiB | in_use: 0, prev:   Size: 3.06MiB | Requested Size: 3.06MiB | in_use: 1
2018-08-23 17:26:41.310959: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb03140000 of size 634880
2018-08-23 17:26:41.310975: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb031db000 of size 3211264
2018-08-23 17:26:41.310991: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb034eb000 of size 2114605056
2018-08-23 17:26:41.311006: I tensorflow/core/common_runtime/bfc_allocator.cc:671]      Summary of in-use Chunks by size: 
2018-08-23 17:26:41.311026: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 634880 totalling 620.0KiB
2018-08-23 17:26:41.311044: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 3211264 totalling 3.06MiB
2018-08-23 17:26:41.311060: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Sum Total of in-use chunks: 3.67MiB
2018-08-23 17:26:41.311083: I tensorflow/core/common_runtime/bfc_allocator.cc:680] Stats: 
Limit:                  2118451200
InUse:                     3846144
MaxInUse:               1317554688
NumAllocs:                  119350
MaxAllocSize:           1212416000

2018-08-23 17:26:41.311112: W tensorflow/core/common_runtime/bfc_allocator.cc:279] *___________________________________________________________________________________________________
2018-08-23 17:26:41.311161: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger ./resources.h (119) - OutOfMemory Error in GpuMemory
2018-08-23 17:26:41.311420: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger GPU memory allocation failed during tactic selection for layer: resnet_v1_50/conv1/Conv2D + (Unnamed Layer* 2) [Activation]
2018-08-23 17:26:41.319106: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger ./resources.h (119) - OutOfMemory Error in GpuMemory
2018-08-23 17:26:41.319785: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:857] Engine creation for segment 0, composed of 452 nodes failed: Internal: Failed to build TensorRT engine. Skipping...
INFO:tensorflow:Starting execution
2018-08-23 17:26:45.089652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2018-08-23 17:26:45.089705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-23 17:26:45.089714: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 
2018-08-23 17:26:45.089721: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N 
2018-08-23 17:26:45.089863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2020 MB memory) -> physical GPU (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0, compute capability: 5.0)
INFO:tensorflow:Starting Warmup cycle
... etc
```

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21830,Canned TensorForest Tracking,"Implementation steps for https://github.com/tensorflow/community/blob/master/rfcs/20180626-tensor-forest.md


- [x] classification only and inference only  the tf/core/kernerl part https://github.com/tensorflow/tensorflow/pull/21803
- [ ] classification only and inference only the tf/estimator python part https://github.com/tensorflow/estimator/pull/9
- [ ] classification only and with training https://github.com/tensorflow/tensorflow/pull/23889
- [ ] end2end classification estimator with tests and docs.
- [ ] regression support 
"
21829,//tensorflow/contrib/fused_conv:fused_conv2d_bias_activation_op_test  running into `implementation not found` error,"------------------------

### System information
- **Have I written custom code**: No
- **OS Platform and Distribution**:
`tensorflow/tensorflow:1.9.0-devel-gpu-py3` docker container running on `Linux Ubuntu 16.04` host
- **Mobile device**:  Not Applicable
- **TensorFlow installed from (source or binary)**: cloned from repo
- **TensorFlow version (use command below)**: `b'v1.9.0-0-g25c197e' 1.9.0`
- **Python version**: `Python 3.5.2`
- **Bazel version (if compiling from source)**: `0.15.0`
- **GCC/Compiler version (if compiling from source)**: `5.4.0`
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: Not Applicable
- **Exact command to reproduce**: 
```
bazel test --config=cuda  //tensorflow/contrib/fused_conv:fused_conv2d_bias_activation_op_test 
```

### Describe the problem

I am unable to run the `fused_conv` example from the contrib area. The error I run into is shown in the `Source code / logs` section below. The problem seems to be that 
- the code in kernel implementation for the `fused_conv2d_bias_activation_op` calls routines that are present in the `_pywrap_tensorflow_internal.so` library
- the shared library created for the custom op `_fused_conv2d_bias_activation_op.so` does **not** have a dependency on the `_pywrap_tensorflow_internal.so` library

I think one way to get past this error would be add the missing library dependency, but I do not know how to go about doing that. 


### Source code / logs

error from bazel command output

```
exec ${PAGER:-/usr/bin/less} ""$0"" || exit 1
Executing tests from //tensorflow/contrib/fused_conv:fused_conv2d_bias_activation_op_test
-----------------------------------------------------------------------------
Traceback (most recent call last):
 File ""/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/fused_conv/fused_conv2d_bias_activation_op_test.runfiles/org_tensorflow/tensorflow/contrib/fused_conv/python/ops/fused_conv2d_bia\
s_activation_op_test.py"", line 23, in <module>
   from tensorflow.contrib.fused_conv.python.ops import fused_conv2d_bias_activation_op
 File ""/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/fused_conv/fused_conv2d_bias_activation_op_test.runfiles/org_tensorflow/tensorflow/contrib/fused_conv/__init__.py"", line 22, in <\
module>
   from tensorflow.contrib.fused_conv.python.ops.fused_conv2d_bias_activation_op import *
 File ""/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/fused_conv/fused_conv2d_bias_activation_op_test.runfiles/org_tensorflow/tensorflow/contrib/fused_conv/python/ops/fused_conv2d_bia\
s_activation_op.py"", line 26, in <module>
   resource_loader.get_path_to_datafile(""_fused_conv2d_bias_activation_op.so""))
 File ""/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/fused_conv/fused_conv2d_bias_activation_op_test.runfiles/org_tensorflow/tensorflow/contrib/util/loader.py"", line 56, in load_op_l\
ibrary
   ret = load_library.load_op_library(path)
 File ""/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/fused_conv/fused_conv2d_bias_activation_op_test.runfiles/org_tensorflow/tensorflow/python/framework/load_library.py"", line 56, in\
load_op_library
   lib_handle = py_tf.TF_LoadLibrary(library_filename)
tensorflow.python.framework.errors_impl.NotFoundError: /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/fused_conv/fused_conv2d_bias_activation_op_test.runfiles/org_tensorflow/tensorflow\
/contrib/fused_conv/python/ops/_fused_conv2d_bias_activation_op.so: undefined symbol: _ZN10tensorflow7functor10NHWCToNCHWIN5Eigen9GpuDeviceEfLi4EEclERKS3_NS2_9TensorMapINS2_6TensorIKfLi4ELi1ElEELi16ENS2_11MakePointerEEENS7_INS8_IfLi4ELi1ElEELi16ESB_EE
```

note the lack of dependency on `_pywrap_tensorflow_internal.so`

```
root@fptitan1:~/tensorflow# ldd ./bazel-tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/fused_conv/python/ops/_fused_conv2d_bias_activation_op.so
	linux-vdso.so.1 =>  (0x00007ffebf7a9000)
	libtensorflow_framework.so => /root/tensorflow/./bazel-tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/fused_conv/python/ops/../../../../../_solib_local/_U_S_Stensorflow_Scontrib_Sfused_Uconv_Cpython_Sops_S_Ufused_Uconv2d_Ubias_Uactivation_Uop.so___Utensorflow/libtensorflow_framework.so (0x00007f93a5e5e000)
	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f93a5b55000)
	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f93a5951000)
	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f93a5734000)
	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f93a552c000)
	libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f93a51aa000)
	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f93a4f94000)
	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f93a4bca000)
	libcublas.so.9.0 => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcublas.so.9.0 (0x00007f93a12c5000)
	libcuda.so.1 => /usr/local/nvidia/lib64/libcuda.so.1 (0x00007f93a0725000)
	libcudnn.so.7 => /usr/lib/x86_64-linux-gnu/libcudnn.so.7 (0x00007f938c734000)
	libcufft.so.9.0 => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcufft.so.9.0 (0x00007f9384693000)
	libcurand.so.9.0 => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcurand.so.9.0 (0x00007f938072f000)
	libcudart.so.9.0 => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0 (0x00007f93804c2000)
	/lib64/ld-linux-x86-64.so.2 (0x00007f93a70d5000)
	libnvidia-fatbinaryloader.so.390.30 => /usr/local/nvidia/lib64/libnvidia-fatbinaryloader.so.390.30 (0x00007f9380276000)
```
"
21828,I make my own Network to train machine and data is used Mnist But it's give Low accuracy and somtimes my softmax code not working as well.. so please tell me what's wrong with my code.... ,"**somtimes softmax give null value after updating weights so it's throw divison by zero error please check my code and tell me what's wrong with it.
Used: python 3.6.5**


import numpy as np
import gzip
import math
IMAGE_SIZE = 28


def sigmoid(x):
    # return (2 / (1 + np.exp(-2 * (x)))) - 1
    return 1.0 / (1.0 + np.exp(-x))


def derivation_sigmoid(x):
    # return 1 - (x ** 2)
    return x * (1.0 - x)


def softmax(x):
    # print np.sum(np.exp(x))
    return (np.exp(x)) / (np.sum(np.exp(x)))


def extract_data(filename, num_images):
  with gzip.open(filename) as bytestream:
    bytestream.read(16)
    buf = bytestream.read(IMAGE_SIZE * IMAGE_SIZE * num_images)
    data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)
    data = data.reshape(num_images, IMAGE_SIZE, IMAGE_SIZE, 1)
    return data



def extract_labels(filename, num_images):
  with gzip.open(filename) as bytestream:
    bytestream.read(8)
    buf = bytestream.read(1 * num_images)
    labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)
  return labels


def derivation_cross(x, y):
    return -1 * ((x * (1 / y)) + (1 - x) * (1 / (1 - y)))

def derivation_softmax(x):
    tem = sum(np.exp(x)) ** 2
    # print tem
    tem1 = sum(np.exp(x))
    a = []
    for i_ in range(len(x)):
        t1 = (np.exp(x[i_]) * (tem1 - np.exp(x[i_]))) / tem
        a.append(t1)
    return a

train_data_filename = ""/home/rootpranav/Downloads/train-images-idx3-ubyte.gz""
train_data = extract_data(train_data_filename, 60000)

train_data_label = ""/home/rootpranav/Downloads/train-labels-idx1-ubyte.gz""
tarin_data_label_1 = extract_labels(train_data_label, 6000)

input_array = []
counter = 0


input_neurons = train_data.shape[1] * train_data.shape[1]   # Number of Feature
out_neurons = 10
hidden_layer_nurons = 15
lr = 0.1

bias_hidden = np.ones((15, 1))
bias_output = np.ones((10, 1))

wih = np.ones((15,784))
who = np.ones((10, 15))

def neural_network(wih, who, train_data, tarin_data_label_1):
    for e_ in range(0, 1000):
        for da_ in range(900, 980):
            
            c_error = []
            m1 = np.matrix((train_data[da_]))
            m2 = np.reshape(m1, (784, 1))
            m2 = ((m2 * 1) / 255)
            
            hidden_sigmoid = sigmoid((np.dot(wih, m2)))
            output = (np.dot(who, hidden_sigmoid))
            output_softmax = softmax((np.dot(who, hidden_sigmoid)))

            index = tarin_data_label_1[da_]
            for i in range(0, 10):
                if i != index:
                    c_error.append([0])
                else:
                    c_error.append([1])
            
            a1 = np.array(c_error)
            a2 = np.array(output_softmax)

            # This is for Hidden -> Output change weight
            cross_entropy_derivation = derivation_cross(a1, a2)

            # print cross_entropy_derivation
            softmax_derivation = derivation_softmax(output)
            sigmoid_derivation = derivation_softmax(hidden_sigmoid)
            sigmoid_derivation = np.reshape(sigmoid_derivation, (15, 1))

            m = np.reshape(softmax_derivation, (10, 1))
            d_output = cross_entropy_derivation * m

            who -= (d_output.dot(hidden_sigmoid.T))

            # this is for Hidden -> Input

            d_hidden = who.T.dot(d_output)
            d1 =  d_hidden * sigmoid_derivation
            wih -= d1.dot(m2.T)
            # print who[0]
            test = np.argmax(output_softmax)
            print test, index



neural_network(wih, who, train_data, tarin_data_label_1)"
21825,TensorFlow lite android example simply does not sync or build.,
21820,Dataset.padded_batch doc improvement request,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: n/a
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a
- **TensorFlow installed from (source or binary)**: n/a
- **TensorFlow version (use command below)**: 1.10
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: n/a

### Describe the problem
Documentation problem. In the method documentation for 'tf.data.Dataset.padded_batch', the argument of `padded_shapes` is described to be 

> A nested structure of `tf.TensorShape` or `tf.int64` vector tensor-like objects...

Python contains a myriad of different nesting structures. I would like the doc to at least refer to some complete definition of what this nested structure can be. Can it contain lists, tuples, dicts, sets? Maybe even more complex objects? Can it be an arbitrary non-cyclic graph? Please make clear any limitations on this structure.

### Source code / logs
[https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/python/data/ops/dataset_ops.py](https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/python/data/ops/dataset_ops.py)
"
21819,[XLA] WhileLoopConstantSinking hangs compilation when used together with DCE,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.13.6 (17G65)
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.10.0
- **Python version**: 3.6.0
- **Bazel version (if compiling from source)**: 0.15.2-homebrew
- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 9.1.0 (clang-902.0.39.2)
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:

```
bazel-out/host/bin/tensorflow/compiler/aot/tfcompile --graph=tfgraph.pb --config=aot_config.pbtxt --entry_point=____xla_test_rnn --cpp_class=xla_test::TestNetwork --target_triple=x86_64-none-darwin --out_header=bazel-out/darwin-opt/genfiles/xla_test_rnn.h --out_metadata_object=bazel-out/darwin-opt/genfiles/xla_test_rnn_tfcompile_metadata.o --out_function_object=bazel-out/darwin-opt/genfiles/xla_test_rnn_tfcompile_function.o --target_features=+sse4.1 --target_features=+sse4.2 --target_features=+avx
```

[tfgraph.pb.zip](https://github.com/tensorflow/tensorflow/files/2311756/tfgraph.pb.zip)

aot_config.pbtxt:

```
feed {
  id { node_name: ""0"" }
  shape {
    dim { size: 1 }
    dim { size: 128 }
    dim { size: 194 }
  }
}

fetch {
  id { node_name: ""transpose_4713"" }
}
```

### Describe the problem

When tfcompile optimizes the attached graph with logging enabled, it seems to get stuck in an infinite loop in this fashion:

```
......

tensorflow/compiler/xla/service/hlo_dce.cc:58] Removing dead root %constant.17644..sunk.5 = f32[256]{0} constant({...}), metadata={op_type=""Add"" op_name=""4GC""} and it's unused operands
tensorflow/compiler/xla/service/hlo_dce.cc:58] Removing dead root %constant.17657..sunk.5 = f32[384,256]{1,0} constant({...}), metadata={op_type=""ConcatV2"" op_name=""6Kn""} and it's unused operands
tensorflow/compiler/xla/service/hlo_dce.cc:58] Removing dead root %constant.17668..sunk.5 = f32[256]{0} constant({...}), metadata={op_type=""Add"" op_name=""5XK""} and it's unused operands
tensorflow/compiler/xla/service/hlo_pass_pipeline.cc:112]   HLO pass reshape-mover
tensorflow/compiler/xla/service/hlo_pass_pipeline.cc:112]   HLO pass constant_folding
tensorflow/compiler/xla/service/hlo_pass_pipeline.cc:112]   HLO pass simplify-conditional
tensorflow/compiler/xla/service/hlo_pass_pipeline.cc:63] Running HLO pass pipeline simplification
tensorflow/compiler/xla/service/hlo_pass_pipeline.cc:112]   HLO pass batchnorm_expander
tensorflow/compiler/xla/service/hlo_pass_pipeline.cc:112]   HLO pass algsimp
tensorflow/compiler/xla/service/hlo_pass_pipeline.cc:112]   HLO pass dce
tensorflow/compiler/xla/service/hlo_pass_pipeline.cc:112]   HLO pass while-loop-invariant-code-motion
tensorflow/compiler/xla/service/hlo_pass_pipeline.cc:112]   HLO pass tuple-simplifier
(vvv This is actually WhileLoopConstantSinking not WhileLoopInvariantCodeMotion, and is causing the infinite loop)
tensorflow/compiler/xla/service/hlo_pass_pipeline.cc:112]   HLO pass while-loop-invariant-code-motion
tensorflow/compiler/xla/service/hlo_pass_pipeline.cc:112]   HLO pass simplify-while-loops
tensorflow/compiler/xla/service/hlo_pass_pipeline.cc:112]   HLO pass dce
tensorflow/compiler/xla/service/hlo_dce.cc:58] Removing dead root %constant.9423..sunk.6 = f32[284,256]{1,0} constant({...}), metadata={op_type=""ConcatV2"" op_name=""2IL""} and it's unused operands
tensorflow/compiler/xla/service/hlo_dce.cc:58] Removing dead root %constant.9433..sunk.6 = f32[256]{0} constant({...}), metadata={op_type=""Add"" op_name=""6hz""} and it's unused operands
tensorflow/compiler/xla/service/hlo_dce.cc:58] Removing dead root %constant.9444..sunk.6 = f32[256,256]{1,0} constant({...}), metadata={op_type=""ConcatV2"" op_name=""3hb""} and it's unused operands
tensorflow/compiler/xla/service/hlo_dce.cc:58] Removing dead root %constant.9454..sunk.6 = f32[256]{0} constant({...}), metadata={op_type=""Add"" op_name=""19n""} and it's unused operands
tensorflow/compiler/xla/service/hlo_dce.cc:58] Removing dead root %constant.9465..sunk.6 = f32[384,256]{1,0} constant({...}), metadata={op_type=""ConcatV2"" op_name=""6Ye""} and it's unused operands
tensorflow/compiler/xla/service/hlo_dce.cc:58] Removing dead root %constant.9475..sunk.6 = f32[256]{0} constant({...}), metadata={op_type=""Add"" op_name=""3z9""} and it's unused operands

......
```

I deleted all the ""Invariant checker"" lines in the log to highlight the passes. The graph mainly consists of lots of GRUs converted from an ONNX trace. To minimize any suspicions of tfcompile taking a long time simply due to a large model I left it running over a week without finishing. This was not the behavior in 1.8.0.

I was able to narrow it down to a newly added optimization pass called `WhileLoopConstantSinking`. That particular pass seems to make changes that always triggers dead code elimination, causing the entire pass to never reach fix point. In particular, if I modify `RunHloPasses` to run `AlgebraicSimplifier` once, followed by a `HloPassFix` pipeline wrapping only a `HloDCE` then a `WhileLoopConstantSinking`, it'll get stuck in the pipeline. Once `WhileLoopConstantSinking` is disabled by commenting this line https://github.com/tensorflow/tensorflow/blob/e924d67bff8c4fb58c8316d00b662f8d1e80eb95/tensorflow/compiler/xla/service/cpu/cpu_compiler.cc#L285, `tfcompile` finishes under ten minutes.
"
21818,Unable to import tensorrt on macOS,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS, 10.13.6
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10.0
- **Python version**: 2.7.15
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:  `python -c 'from tensorflow.contrib import tensorrt as trt'`


### Describe the problem
Unable to import tensorrt

### Source code / logs
```
(/private/tmp/scratch.nwani/1534993632/dev)   1534993632 python
Python 2.7.15 |Anaconda, Inc.| (default, May  1 2018, 18:37:05)
[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from tensorflow.contrib import tensorrt as trt
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/private/tmp/scratch.nwani/1534993632/dev/lib/python2.7/site-packages/tensorflow/contrib/tensorrt/__init__.py"", line 25, in <module>
    from tensorflow.contrib.tensorrt.python import *
  File ""/private/tmp/scratch.nwani/1534993632/dev/lib/python2.7/site-packages/tensorflow/contrib/tensorrt/python/__init__.py"", line 23, in <module>
    from tensorflow.contrib.tensorrt.python.trt_convert import calib_graph_to_infer_graph
  File ""/private/tmp/scratch.nwani/1534993632/dev/lib/python2.7/site-packages/tensorflow/contrib/tensorrt/python/trt_convert.py"", line 23, in <module>
    from tensorflow.contrib.tensorrt.wrap_conversion import calib_convert
  File ""/private/tmp/scratch.nwani/1534993632/dev/lib/python2.7/site-packages/tensorflow/contrib/tensorrt/wrap_conversion.py"", line 28, in <module>
    _wrap_conversion = swig_import_helper()
  File ""/private/tmp/scratch.nwani/1534993632/dev/lib/python2.7/site-packages/tensorflow/contrib/tensorrt/wrap_conversion.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_wrap_conversion', fp, pathname, description)
ImportError: dynamic module does not define init function (init_wrap_conversion)
>>>
```
"
21817,tflite conv bug,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:N/A
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.10.0
- **Python version**:3.6.0
- **Bazel version (if compiling from source)**:0.16.1
- **GCC/Compiler version (if compiling from source)**:7.3.0
- **CUDA/cuDNN version**:N/A
- **GPU model and memory**:N/A
- **Exact command to reproduce**:N/A

There is a bug in the multithreaded implementation of conv2d.  In certain cases when batch_size > 1, only the first image are considered while others ignored. For the reason see file contrib/lite/kernels/internal/optimized/multithreaded_conv.h
```C++
 } else if (filter_height == input_height && filter_width == input_width &&
               pad_width == 0 && pad_height == 0) {
      // If the input data and filter have the same height/width,
      // the 2D convolution is reduced to matrix multiplication.
      const int k =  // Length of reduction dimension.
          filter_width * filter_height * input_depth;
      Eigen::array<Eigen::IndexPair<Eigen::DenseIndex>, 1> dim_pair;
      dim_pair[0] = Eigen::IndexPair<Eigen::DenseIndex>(1, 0);
      EigenMatrix output(output_data, 1, filter_count);
      ConstEigenMatrix input(input_data, 1, k);
      ConstEigenMatrix filter(filter_data, k, filter_count);
      MatMulConvFunctor<Eigen::ThreadPoolDevice, T>()(device, output, input,
                                                      filter, dim_pair);
    } else {
```
In the above code, the input_batches are ignored.
I have also verified a quick fix:
```C++
    } else if (filter_height == input_height && filter_width == input_width &&
               pad_width == 0 && pad_height == 0) {
      // If the input data and filter have the same height/width,
      // the 2D convolution is reduced to matrix multiplication.
      const int k =  // Length of reduction dimension.
          filter_width * filter_height * input_depth;
	  Eigen::array<Eigen::IndexPair<Eigen::DenseIndex>, 1> dim_pair;
      dim_pair[0] = Eigen::IndexPair<Eigen::DenseIndex>(1, 0);
      EigenMatrix output(output_data, input_batches, filter_count);
      ConstEigenMatrix input(input_data, input_batches, k);
      ConstEigenMatrix filter(filter_data, k, filter_count);
      MatMulConvFunctor<Eigen::ThreadPoolDevice, T>()(device, output, input,
		  filter, dim_pair);
    } else {
```
Just change 1 to input_batches.
My code works well based on my experiments.
It is a simple and quick fix, please merge it into the master if possible.
"
21815,Please provide cmake-based build files,"Currently tensorflow fails to build due to this problem: https://github.com/bazelbuild/bazel/issues/5763
Bazel project doesn't appear to be willing to fix the problem.

Could you please provide cmake-based build? cmake is known to work much better than bazel, and many projects are using it without problems.
"
