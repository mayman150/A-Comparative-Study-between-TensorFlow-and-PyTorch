Issue Number,Issue Title,Issue Body
1705,Also taking contributions from the Gerrit?,"In my opinion doing code review on GitHub is horrible except for very small fixes / tweaks. In addition, all those merge commits from ""pull requests"" really clutter up the git log.

I wonder if TensorFlow would consider also taking patches from Gerrit. Blaze (Bazel) does this: all external pull requests are not merged directly, but committed through the Gerrit repo. As a result, their commit history looks much cleaner: https://github.com/bazelbuild/bazel/commits/master.
"
1704,"""Setting up TensorFlow for Development"" no longer works with recent TF versions","### Environment info

Operating System: OS X 10.11, Ubuntu 15.10
Source build, commit hash: e39d8feebb9666a331345cd8d960f5ade4652bba
### Steps to reproduce

I have TF as a submodule and build it local to the project. I use more or less the exact same commands in the [development setup](https://www.tensorflow.org/versions/r0.7/get_started/os_setup.html#setting-up-tensorflow-for-development). The only minor change is that I don't run `python setup.py develop` (under OS X, this breaks my Anaconda python installation completely via screwing with the site-packages). So the steps are (in Makefile syntax):

``` bash
git submodule update --init --recursive
cd $(TF_DIR) && ./configure
cd $(TF_DIR) && bazel build -c opt //tensorflow/tools/pip_package:build_pip_package
cd $(TF_INSTALL_DIR) && ln -s $(addprefix ../../,$(wildcard $(TF_DIR)/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/*)) .
$(TF_INSTALL_DIR) && ln -s $(addprefix ../../,$(wildcard $(TF_DIR)/tensorflow/tools/pip_package/*)) .
```

The prefix commands are for making globs work in Make. I have confirmed they expand to the same symlink commands.
### Issue

Importing tensorflow no longer works using development guidelines. Specifically, assuming tensorflow is cloned into $TF_DIR, symlinked into `$TF_INSTALL_DIR`, and built by bazel in $BAZEL_TMP, this no longer works: (output sanitized manually)

```
$ PYTHONPATH=$TF_INSTALL_DIR python
Python 2.7.10 (default, Oct 14 2015, 16:09:02) 
[GCC 5.2.1 20151010] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""$TF_INSTALL_DIR/tensorflow/__init__.py"", line 23, in <module>
    from tensorflow.python import *
  File ""$TF_INSTALL_DIR/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""$TF_INSTALL_DIR/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>
    from google.protobuf import descriptor as _descriptor
  File ""$BAZEL_TMP/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/tools/pip_package/build_pip_package.runfiles/google/protobuf/descriptor.py"", line 46, in <module>
    from google.protobuf.pyext import _message
  File ""$BAZEL_TMP/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/tools/pip_package/build_pip_package.runfiles/google/protobuf/pyext/__init__.py"", line 2, in <module>
    __import__('pkg_resources').declare_namespace(__name__)
  File ""/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py"", line 2226, in declare_namespace
    _handle_ns(packageName, path_item)
  File ""/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py"", line 2195, in _handle_ns
    path.sort(key=sort_key)
  File ""/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py"", line 2193, in sort_key
    return sys_path.index(_normalize_cached(os.sep.join(parts)))
ValueError: '$BAZEL_TMP/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/tools/pip_package/build_pip_package.runfiles' is not in list
```

Linux output shown, but OS X also fails with a similar exception.
### What have you tried?

I believe the issue comes down to the same one as #1535. I am creating this issue separately so that if it turns out to be a different problem, it isn't muddled. If it's the same, we can close it.

The exception is a result of namespace package resolution in google.protobuf trying to find `$BAZEL_TMP/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/tools/pip_package/build_pip_package.runfiles/google/protobuf/pyext`. It is trying to find it in a list of system paths which includes `$TF_INSTALL_DIR`. Recall that due to the development setup rules, the files in `$TF_INSTALL_DIR` are symlinks to files in `$BAZEL_TMP`. Specifically, `$TF_INSTALL_DIR/google/protobuf` is the same directory as `$BAZEL_TMP/google/protobuf`. (I have verified this with `pwd -P`.

What seems to be going on is that Python is attempting to unify the protobuf namespace package, but its filename (`$BAZEL_TMP/google/protobuf/pyext`) does not match the `sys.path` entry (`$TF_INSTALL_DIR`).

**Note:** This only appears to be a problem when trying to use the development install rules, and it only appears to be a problem when there's another tensorflow or protobuf installed in the system somewhere else. At least on OS X; the other version of tensorflow on my Linux machine is in use by other users, so I can't uninstall it safely. I have not attempted to create a clean virtual environment just for my development directory (for one, because virtual environments are a pain in the neck; and two, that's part of the whole point of having a local build of tensorflow).

Workaround: Replacing the symlinks with a recursive copy (from `$BAZEL_TMP` to `$TF_INSTALL_DIR`) is apparently sufficient to convince Python that it's unified the namespace packages correctly, and it chooses the `$TF_INSTALL_DIR` specified in `$PYTHONPATH`.

So I don't know what the appropriate solution here is, but the problem seems to be the result of a convergence of semi-hacky solutions, any one of which alone might not be an issue, but work together to cause problems:
- There's no clean way to extract the necessary files from a built tensorflow without going through pip install. Perhaps there's a method using pip I'm not aware of (maybe something with `--target`?).
- Python namespace packages have poor path resolution semantics. It looks like this ship has sailed.
- The symlink approach is kind of coarse and tricky to script around.
"
1702,Print operation flattens the tensors,"### Environment info

Operating System: Linux
Using: pip, 0.7.1 for Python 3
### Steps to reproduce

Run:

```
a=tf.constant([[1], [2]])
b=tf.Print(a, [a])
```
### Output

```
I tensorflow/core/kernels/logging_ops.cc:79] [1 2]
```

while the tensor evalutes to:

```
array([[1],
       [2]], dtype=int32)
```
### Expected output

```
I tensorflow/core/kernels/logging_ops.cc:79] [[1], [2]]
```
"
1701,Building from source with CUDA PATH problem,"### Environment info

Operating System: Fedora 23

If installed from sources, provide the commit hash:
8e035ea
### Steps to reproduce
1.  ../bazel/output/bazel --output_base=/projects/.cache build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures
### What have you tried?
1.  ./configure on root directory
2.  cuda_config.sh script 
3.  building without --config=cuda is successful.
### Logs or other output that would be helpful

(If logs are large, please upload as attachment).
Output from build
http://pastebin.com/17BejSXD

I noticed that PATH in the log doesn't have the directory to libcudart.so.7.5, which is here:
/usr/local/cuda/lib64/

echo $LD_LIBRARY_PATH:
:/usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/lib/nvidia:/usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/lib/nvidia:/usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/lib/nvidia

Pointers will be welcome.
"
1699,Android Bazel Build - Java Failure,"Ran into another build issue when running `bazel build //tensorflow/examples/android:tensorflow_demo --verbose_failures`

```
WARNING: /Users/username/tensorflow/tensorflow/core/BUILD:632:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:avgpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/username/tensorflow/tensorflow/core/BUILD:632:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:bounds_check.h' directly. You should either move the file to this package or depend on an appropriate rule there.
...
Lots more of the same warnings
...
WARNING: /Users/username/tensorflow/tensorflow/core/BUILD:665:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib: please do not import '//tensorflow/core/kernels:where_op.cc' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /Users/username/tensorflow/tensorflow/core/BUILD:665:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib: please do not import '//tensorflow/core/kernels:xent_op.cc' directly. You should either move the file to this package or depend on an appropriate rule there.
INFO: Found 1 target...
ERROR: /private/var/tmp/_bazel_username/d0aa3a34ac01d28ffabd89cde457364e/external/bazel_tools/src/tools/android/java/com/google/devtools/build/android/BUILD:8:1: Java compilation in rule '@bazel_tools//src/tools/android/java/com/google/devtools/build/android:AndroidResourceProcessingAction' failed: java failed: error executing command 
  (cd /private/var/tmp/_bazel_username/d0aa3a34ac01d28ffabd89cde457364e/tensorflow && \
  exec env - \
  external/local_jdk/bin/java -Xbootclasspath/p:external/bazel_tools/third_party/java/jdk/langtools/javac.jar -client -jar external/bazel_tools/tools/jdk/JavaBuilder_deploy.jar @bazel-out/host/bin/external/bazel_tools/src/tools/android/java/com/google/devtools/build/android/AndroidResourceProcessingAction.jar-2.params): com.google.devtools.build.lib.shell.AbnormalTerminationException: Process terminated by signal 11: java failed: error executing command 
  (cd /private/var/tmp/_bazel_username/d0aa3a34ac01d28ffabd89cde457364e/tensorflow && \
  exec env - \
  external/local_jdk/bin/java -Xbootclasspath/p:external/bazel_tools/third_party/java/jdk/langtools/javac.jar -client -jar external/bazel_tools/tools/jdk/JavaBuilder_deploy.jar @bazel-out/host/bin/external/bazel_tools/src/tools/android/java/com/google/devtools/build/android/AndroidResourceProcessingAction.jar-2.params): com.google.devtools.build.lib.shell.AbnormalTerminationException: Process terminated by signal 11.
Target //tensorflow/examples/android:tensorflow_demo failed to build
INFO: Elapsed time: 0.743s, Critical Path: 0.53s
```

I am on OSX 10.11, had to download android ndk-r10e (since r11e didn't seem to work). Java version is 1.8.0_73 and Bazel version is 0.2.0 with TensorFlow 0.7.1. My WORKSPACE is configured as follows in the beginning. I changed the build_tools_version for the androidsdk to 23.0.2 (from 23.0.1) since that's what I have locally on my machine. Any pointers?

```
android_sdk_repository(
    name = ""androidsdk"",
    api_level = 23,
    build_tools_version = ""23.0.2"",
    path = ""/Users/mchong5/Library/Android/sdk"",
)

android_ndk_repository(
    name=""androidndk"",
    path=""/Users/mchong5/android-ndk-r10e/"",
    api_level=21)
```
"
1695,Android Bazel Build Error,"I'm at the step where I enter `bazel build //tensorflow/examples/android:tensorflow_demo` and I get the following output:

```
ERROR: /Users/mchong5/tensorflow/WORKSPACE:5:13: invalid character: '?'.
ERROR: /Users/mchong5/tensorflow/WORKSPACE:5:14: invalid character: '?'.
ERROR: /Users/mchong5/tensorflow/WORKSPACE:5:50: invalid character: '?'.
ERROR: /Users/mchong5/tensorflow/WORKSPACE:5:51: invalid character: '?'.
ERROR: /Users/mchong5/tensorflow/WORKSPACE:10:11: invalid character: '?'.
ERROR: /Users/mchong5/tensorflow/WORKSPACE:10:12: invalid character: '?'.
ERROR: /Users/mchong5/tensorflow/WORKSPACE:10:46: invalid character: '?'.
ERROR: /Users/mchong5/tensorflow/WORKSPACE:10:47: invalid character: '?'.
ERROR: /Users/mchong5/tensorflow/WORKSPACE:5:49: syntax error at '?': expected ,
ERROR: /Users/mchong5/tensorflow/WORKSPACE:6:1: non-keyword arg after keyword arg.
ERROR: no such package 'external': Error encountered while dealing with the WORKSPACE file: Failed to parse WORKSPACE file.
INFO: Elapsed time: 0.557s
```

My WORKSPACE is modified as follows... I'm not sure why it is telling me there are invalid characters

```
android_sdk_repository(
    name = ""androidsdk"",
    api_level = 23,
    build_tools_version = ""23.0.2"",
    path = “/Users/mchong5/Library/Android/sdk”,
)

android_ndk_repository(
    name=""androidndk"",
    path=“/Users/mchong5/android-ndk-r11b/”,
    api_level=21)
```

For reference, I am on OSX 10.11 using Bazel 0.2.0 built from source with tensorflow 0.7.1
"
1692,TF fails to compile on Ubuntu 14.04: crosstool_wrapper_driver_is_not_gcc failed,"Hello, 
I'm trying to build TF from source and I'm getting the error included below. 
I've seen similar issues but none of the fixes works for me.

What doesn't work:
- official ""installation and setup"" steps (duh...) with bazel 0.2.0
- updating bazel from 0.2.0 to master
- compiling with `--spawn_strategy=standalone`
- including `-fPIC` flag in `gpus/crosstools/CROSSTOOL`
- deleting `-fPIE` flag from the same file

The command I use is:
`bazel build -c opt --config=cuda --spawn_strategy=standalone --verbose_failures //tensorflow/cc:tutorials_example_trainer`

the error is:

```
ERROR: /home/marcin/Moje/Software/tensorflow/tensorflow/cc/BUILD:61:1: Linking of rule '//tensorflow/cc:tutorials_example_trainer' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/marcin/.cache/bazel/_bazel_marcin/259f71b95269c287fa42ef117628b45f/tensorflow && \
  exec env - \
  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc  ...
...
/usr/bin/ld: /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu/libz.a(crc32.o): relocation R_X86_64_32 against `.rodata' can not be used when making a shared object; recompile with -fPIC
/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu/libz.a: error adding symbols: Bad value
collect2: error: ld returned 1 exit status
```

the full error message is here:
[https://gist.github.com/elanmart/f462692553e1e53fc540](https://gist.github.com/elanmart/f462692553e1e53fc540)
"
1690,'Graph' object has no attribute 'SerializeToString' when run 'sudo python cifar10_train.py',"### Environment info

Operating System:
Mac os 10.10.5

I install the Tensorflow with `sudo pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.7.1-cp27-none-any.whl`

and run
`sudo python cifar10_train.py`

I got an error.
### ERROR in shell

```
$ sudo python cifar10_train.py 
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Traceback (most recent call last):
  File ""cifar10_train.py"", line 134, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/platform/default/_app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""cifar10_train.py"", line 130, in main
    train()
  File ""cifar10_train.py"", line 96, in train
    summary_writer = tf.train.SummaryWriter(FLAGS.train_dir, sess.graph)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/summary_io.py"", line 104, in __init__
    self.add_graph(graph_def)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/summary_io.py"", line 168, in add_graph
    graph_bytes = graph_def.SerializeToString()
AttributeError: 'Graph' object has no attribute 'SerializeToString'
```

Please help.
"
1689,The time cost of Tensorflow is confused,"I want to improve the speed of the convolution layer. I measure the cost of two different filters.
the shape of these two filters are [1,5,3,64] and[5,5,3,64]. The computation of the latter one is five times   as much as the former one, however, the time cost are almost the same  of these two convolution layer. 
conv = tf.nn.conv2d(images, kernel1, [1, 1, 1, 1], padding='SAME') 
conv = tf.nn.conv2d(images, kernel2, [1, 1, 1, 1], padding='SAME')
what is the reason that the cost can not be improved?
"
1688,A small bug in tutorials/mnist/mnist_with_summaries.py,"Please fix the bug in **Line 85** of file _tensorflow/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py_
the code 
`writer = tf.train.SummaryWriter(FLAGS.summaries_dir, sess.graph)` 
should be changed to 
`writer = tf.train.SummaryWriter(FLAGS.summaries_dir, sess.graph.as_graph_def())`,
otherwise there will be an error `'Graph' object has no attribute 'SerializeToString'`
"
1686,Distributed cluster manager support: Slurm,"Per the comment on [this introduction](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/distributed/index.md), i.e. 

> N.B. Manually specifying these cluster specifications can be tedious, especially for large clusters. We are working on tools for launching tasks programmatically, e.g. using a cluster manager like Kubernetes. If there are particular cluster managers for which you'd like to see support, please raise a GitHub issue.

Is there is any possibility of supporting [Slurm](http://slurm.schedmd.com/)? Forgive my ignorance but I've really only played around with TensorFlow and I've only used Slurm for fairly simple MPI projects, but I recently got access to a cluster with some GPU nodes and I'd like to incorporate TF in my research project. It would be great if I was able to use all the resources I could to speed things along.

If it helps, specific info about the setup can be found [here](https://wiki.auckland.ac.nz/display/CER/Centre+for+eResearch+User+Documentation+Start).
"
1675,ImportError libcudart.so.7.5 in Pycharm console not in Terminal python console,"sorry for my ignorant... simple linux problem...
"
1674,install  mac os by pip3,"### Environment info

Operating System:
Mac OS 10.11.1
i try:
sudo pip3 install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.7.1-cp35-none-any.whl
### Logs or other output that would be helpful

(If logs are large, please upload as attachment).
bogon:~ liu$ sudo pip3 install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.7.1-cp35-none-any.whl
The directory '/Users/liu/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.
The directory '/Users/liu/Library/Caches/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.
tensorflow-0.7.1-cp35-none-any.whl is not a supported wheel on this platform.
You are using pip version 7.1.2, however version 8.1.1 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
bogon:~ liu$ 
"
1672,Shape information is not preserved through sampling,"When I create a sample from a normal based on the shape of another tensor, that shape information is not retained.

Using Tensorflow 0.7.1:

```
>>> import tensorflow as tf
>>> x = tf.placeholder(tf.float32, (None, 12))
>>> y = tf.random_normal(tf.shape(x))
>>> y.get_shape()
TensorShape([Dimension(None), Dimension(None)])
```

I would have expected `TensorShape([Dimension(None), Dimension(12)])` instead.
"
1671,Tensorboard does not list any event,"Tensorboard does not list any event stored in disk.
### Environment info

Operating System: Docker on Windows 10

Tensorflow version 0.7.1
### Steps to reproduce

```
graph = tf.Graph()
with graph.as_default(), tf.device('/cpu:0'):
  a = tf.constant(5.0)
  b = tf.constant(6.0)
  c = a * b

  # Enter data into summary.
  c_summary = tf.scalar_summary(""c"", c)
  merged = tf.merge_all_summaries()

with tf.Session(graph=graph) as session:
  writer = tf.train.SummaryWriter(""log/test_logs"", session.graph_def)

  result = session.run([merged])
  tf.initialize_all_variables().run()
  writer.add_summary(result[0], 0)
```

I then ran `tensorboard --logdir={absolute path to log/test_logs}` but no event was listed there. Is there anything I should have written differently in the code maybe?

Note that `log/test_logs` does contain files like `events.out.tfevents.1459102927.0a8840dee548`.
### What have you tried?

Running `https://github.com/tensorflow/tensorflow/blob/r0.7/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py` did not correct this (despite the events correctly outputted to disk).
"
1670,What is the best way to max pool top k elements instead of 1?,"I would like to know if there is a convenient way to pool top k elements instead of just one.

The tf.nn.top_k seems not for this kind of job, because for each window, I want the top k elements.
"
1669,Function for flexible access to the content of a saver / checkpoint file,"As far as I am aware, the only ways to interact with a saver file through API are (from https://www.tensorflow.org/versions/r0.7/how_tos/variables/index.html , I also looked at the API documentation online):
- to restore a full model into exactly the same computational graph
- to restore variables 'one at a time' through an op.
  Am I right or did I miss something?

Would it be possible to release additional API functions giving more flexible access to a saver file? Like making it possible to:
- list all tensors saved and their size, name etc contained in a saver file
- access a tensor saved in a saver file by just reading its content and outputting it as a series of numpy arrays (for easy post processing), instead of needing to add an op, run it etc.
"
1668,Getting Attention Activations to Visualize Attention in Seq2Seq,"All attention papers feature some visualization of the attention weights on some input. Has anyone been able to run a sample through the Seq2Seq Attention Decoder model in `translate.py` and get the attention activations to do such a visualization?
"
1666,"There is no log10 function in tensorflow, only the loge exist.",
1663,Nightly debian CPU build fails during docker build ,"For example, see:
http://ci.tensorflow.org/view/Nightly/job/nightly-debian-cpu/28/console

The error happens during the step: RUN /install/install_deb_packages.sh
"
1662,Tag mismatch! ,"envy@ub1404:~/os_pri/github/tensorflow$ bazel build -c opt //tensorflow/tools/pip_package:build_pip_package
ERROR: /home/envy/os_pri/github/tensorflow/tensorflow/core/distributed_runtime/rpc/BUILD:207:1: no such package '@grpc//': Tag mismatch! and referenced by '//tensorflow/core/distributed_runtime/rpc:grpc_server_lib'.
ERROR: Loading failed; build aborted.
INFO: Elapsed time: 221.272s
envy@ub1404:~/os_pri/github/tensorflow$ 
"
1661,conv4d and higher dimension generalization,"Are there some plans to include a 4D (and possibly higher dimension generalization) convolution op? Would be nice for doing 3D + time processing for example (for the 4D version).

Also, should something be done to offer the user the possibility to choose the shape of the kernel? For example allowing both the natural hyper cube kernel nodes location or a hyper thetrahedron location? Could have consequences on the computing power requested:

http://arxiv.org/pdf/1505.02890v2.pdf
"
1660,Possible memory leak with TensorArray,"I'm not sure if the usage is incorrect (see below), but I am seeing a memory leak in cpu ram when using the `TensorArray`. Seems to be consistent regardless of device placement. I noticed this because I've been using the `dynamic_rnn`.  Happy to provide further info if needed.
### Environment info

Operating System:
Ubuntu 14.04

If installed from sources, provide the commit hash:

`35c3f16f81b6a1d39a004c97160c22b73d3798e5`
### Steps to reproduce

Run this in a script  (I had to add `from tensorflow.python.ops import tensor_array_ops` to `tensorflow/python/__init__.py`)

```
import tensorflow as tf

def ta_mem_leak():

    ta = tf.python.tensor_array_ops.TensorArray(
                        dtype=tf.float32, size=10000)
    ta = ta.write(0, [0.])
    final_outputs = ta.read(0)
    return [final_outputs]

if __name__ == ""__main__"":
    with tf.Graph().as_default(), tf.Session() as session:
        ops = ta_mem_leak()
        for i in xrange(200000):
            res = session.run(ops)                          
```
"
1658,Tensorflow with GPU-support crashes when opening many CPU-only sessions in parallel,"The following code works fine in a CPU-only Tensorflow, but crashes on a GPU-enabled Tensorflow installation (0.7.1, with a Titan X, NVidia driver 352.79) when run many times in parallel:

``` python
# test_tensorflow_session.py
import tensorflow as tf
import os
os.environ['CUDA_VISIBLE_DEVICES'] = ''
sess = tf.Session()
```

Bash command to run it in parallel:

``` bash
for i in {1..48}; do ((python ./test_tensorflow_session.py 2> output$i.err 1> output$i.out ) &) ; done
```

If you then look into the `output*.err` files, you will see that most of the processes crashed. The output will look like this:

```
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally
E tensorflow/stream_executor/cuda/cuda_driver.cc:481] failed call to cuInit: CUDA_ERROR_NO_DEVICE
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:114] retrieving CUDA diagnostic information for host: gpu1
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:121] hostname: gpu1
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:146] libcuda reported version is: 352.79
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:257] driver version file contents: """"""NVRM version: NVIDIA UNIX x86_64 Kernel Module  352.79  Wed Jan 13 16:17:53 PST 2016
GCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-4) (GCC) 
""""""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:150] kernel reported version is: 352.79
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:226] kernel version seems to match DSO: 352.79
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 
terminate called after throwing an instance of 'std::system_error'
  what():  Resource temporarily unavailable
```

For the sake of completeness, here is the use case: applying a Tensorflow model is just a small part in processing large amounts of data; processing is parallelised simply by using multiple processes.

I also tried wrapping the `tf.Session()` call in `with tf.device('/cpu:0'):`, but that didn't change anything. I assume that it is trying to get exclusive access to the GPU, and if it can't do that, it crashes. This is a bit annoying, since by setting `os.environ['CUDA_VISIBLE_DEVICES'] = ''`, I am actively trying to disable the GPU.

Ideas?
"
1656,any way to build train and save pd file on Android?,"It seems no API of C++ to train and save graph,is there any way to run python API of Tensroflow on Android?
"
1655,How to search similiar images?,"This is not an issue, but i don't know where to ask this. I just get started with tensor flow and it's interesting to me. I know tensorflow will help classifying images, but in my case, i need to search similiar images. I read that google using tensorflow for search image in photos app. So how to make tensorflow return similiar image?
Thank you very much
"
1653,Anyone implemented a Deconvolutional layer combined the Keras and Tensorflow?,"I am a little confused by the output shape in conv_transpose( ) function. How to calculate it?

``` python
class Convolution2D_Transpose(Layer):
    input_ndim = 4

    def __init__(self, nb_filter, nb_row, nb_col,
                 init='glorot_uniform', activation='linear', weights=None,
                 border_mode='valid', subsample=(1, 1),deconv_shape = (),
                 W_regularizer=None, b_regularizer=None, activity_regularizer=None,
                 W_constraint=None, b_constraint=None, **kwargs):

        if border_mode not in {'valid', 'same'}:
            raise Exception('Invalid border mode for Convolution2D:', border_mode)
        self.nb_filter = nb_filter
        self.nb_row = nb_row
        self.nb_col = nb_col
        self.deconv_shape = deconv_shape
        self.init = initializations.get(init)
        self.activation = activations.get(activation)
        assert border_mode in {'valid', 'same'}, 'border_mode must be in {valid, same}'
        self.border_mode = border_mode
        self.subsample = tuple(subsample)

        self.W_regularizer = regularizers.get(W_regularizer)
        self.b_regularizer = regularizers.get(b_regularizer)
        self.activity_regularizer = regularizers.get(activity_regularizer)

        self.W_constraint = constraints.get(W_constraint)
        self.b_constraint = constraints.get(b_constraint)
        self.constraints = [self.W_constraint, self.b_constraint]

        self.initial_weights = weights
        self.input = K.placeholder(ndim=4)
        super(Convolution2D_Transpose, self).__init__(**kwargs)

    def build(self):
        stack_size = self.input_shape[1]
        #self.W_shape = (self.nb_filter, stack_size, self.nb_row, self.nb_col)
        self.W_shape = ( self.nb_col, self.nb_row, stack_size, self.nb_filter)
        print self.W_shape
        self.W = self.init(self.W_shape)
        self.b = K.zeros((self.nb_filter,))
        self.trainable_weights = [self.W, self.b]
        self.regularizers = []

        if self.W_regularizer:
            self.W_regularizer.set_param(self.W)
            self.regularizers.append(self.W_regularizer)

        if self.b_regularizer:
            self.b_regularizer.set_param(self.b)
            self.regularizers.append(self.b_regularizer)

        if self.activity_regularizer:
            self.activity_regularizer.set_layer(self)
            self.regularizers.append(self.activity_regularizer)

        if self.initial_weights is not None:
            self.set_weights(self.initial_weights)
            del self.initial_weights

    @property
    def output_shape(self):
        input_shape = self.input_shape
        rows = input_shape[2]
        cols = input_shape[3]
        self.deconv_shape = (self.nb_filter, input_shape[0], rows+2, cols+2)
        return self.deconv_shape

    def get_output(self, train=False):
        X = self.get_input(train)
        X = K.permute_dimensions(X,(0,2,3,1))
        #batch_size = tf.shape(X)[0]
        #deconv_shape = tf.pack([batch_size, 40, 40, 32])
        print 'deconv_shape: {0}'.format(self.deconv_shape)
        print 'value: {0}'.format(X)
        print 'W shape: {0}'.format(K.eval(self.W).shape)
        conv_out = tf.nn.conv2d_transpose(X, self.W, strides=self.subsample,
                            padding='VALID',
                            output_shape=self.deconv_shape)

        output = conv_out + K.reshape(self.b, (1, 1, 1,self.nb_filter))
        return output

    def get_config(self):
        config = {'name': self.__class__.__name__,
                  'nb_filter': self.nb_filter,
                  'nb_row': self.nb_row,
                  'nb_col': self.nb_col,
                  'init': self.init.__name__,
                  'activation': self.activation.__name__,
                  'border_mode': self.border_mode,
                  'subsample': self.subsample,
                  'W_regularizer': self.W_regularizer.get_config() if self.W_regularizer else None,
                  'b_regularizer': self.b_regularizer.get_config() if self.b_regularizer else None,
                  'activity_regularizer': self.activity_regularizer.get_config() if self.activity_regularizer else None,
                  'W_constraint': self.W_constraint.get_config() if self.W_constraint else None,
                  'b_constraint': self.b_constraint.get_config() if self.b_constraint else None}
        base_config = super(Convolution2D_Transpose, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))
```
"
1652,RNN Cell: different device placement for variables and ops,"I want to use RNN cell on multiple GPUs. For maximum performance, I want to place RNN cell variables on CPU and operations (matmul, tanh, etc) on GPU, just like in cifar10 multi gpu example. However, the current implementation of RNN cell seems to only allow same device placement for both variables and ops. Could you enable different device placement?
"
1651,Support for inter-example-dependent labels?,"This is likely the wrong place for this, but I've run into a dead end on other websites.

I have a somewhat unusual learning task: the ""labels"" for my examples (i.e., images) are actually relations between specific images. For instance, suppose each batch consists of 2*n images, where the pair (i, j) has label 1 while the pair (j, i) has label -1. Suppose most (i.e., the vast majority) pairs possible from the set of images lack any label, and as such random sampling will only rarely yield even a single relation to train on. Therefore, in this example, each batch consists of a set 2N images, N pairs of which are known to have some relation.

TensorFlow is so flexible it's easy to design a net that can compute the appropriate loss function. However, creating the input is problematic,requiring many queue objects (why do readers only accept queues in TensorFlow instead of outputs from previous nodes?!). Assuming that I have some set of files which are serialized examples, each a list of the form {file1: <...>, file2: <...>, label: <...>}, is there a simple way to serve them to the net as a tensor of shape 2N \* W \* H \* C (with image i and image i + 1 belonging to a single relation, where i is (0, 2, 4, 6, ...) i.e., [relation_n_image_1, relation_n_image_2, relation_m_image_1, ...])? My current creation, individual components of which work but works as a whole only in theory, employs 5 queue objects of various kinds and does not shuffle perfectly.

Will more flexible readers be implemented in the future, or is there something I'm missing?
"
1645,Error on SummaryWriter creation,"I'm loading a graph from a file and I'd like to see what the topology is. As there doesn't seem to be a way to do this in python (the best I can do is guess the tensor names and use `graph.get_tensor_by_name`), I'm trying to produce a Summary which I can then view with TensorBoard. Unfortunately, instantiating the SummaryWriter object gives me an error:

```
AttributeError                            Traceback (most recent call last)
/home/vlad/Repos/vision/Hand/simple_imagenet.py in <module>()
      9 sess = tf.Session()
     10 
---> 11 writer = tf.train.SummaryWriter('./', sess.graph_def)
     12 
     13 #pool = sess.graph.get_tensor_by_name('pool_3:0')

/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/summary_io.py in __init__(self, logdir, graph_def, max_queue, flush_secs)
     95     if not gfile.IsDirectory(self._logdir):
     96       gfile.MakeDirs(self._logdir)
---> 97     self._event_queue = six.moves.queue.Queue(max_queue)
     98     self._ev_writer = pywrap_tensorflow.EventsWriter(
     99         compat.as_bytes(os.path.join(self._logdir, ""events"")))

AttributeError: 'module' object has no attribute 'Queue'
```

I'm using the pip3 install of tensorflow 7.1 on Ubuntu 15.10.
"
1644,Support AWS cfncluster definition in distributed_runtime,
1642,Poor performance of Android camera demo on arm64,"I've been running the Android camera demo on both a Nexus 5 (armeabi-v7a) and Nexus 5X (arm64-v8a).  The older Nexus 5 is able to perform a single recognition in about ~350ms.  The Nexus 5X however performs at around ~750ms.  I wouldn't expect a newer device to perform worse (especially not that much worse) than the older one.

I've tried running the resulting apk from 
`bazel build //tensorflow/examples/android:tensorflow_demo` 
as well as
`bazel build //tensorflow/examples/android:tensorflow_demo --fat_apk_cpu=arm64-v8a`

There's no change in performance for either option on the Nexus 5X.

I know that the `-mfpu=neon` flag seems to be a problem when building for arm64-v8a so I've had to remove it for those builds.  I suspect the performance issue might be related to the performance benefits of neon.  Is there a different flag that needs to be specified to enable neon support for the arm64-v8a architecture?  Or am I completely off base on the performance expectations on Nexus 5X?

This is related to #1019 
"
1636,dynamic_rnn got  an error,"I want to test dynamic_rnn based on ptb_word_lm.py.

when I use rnn,it's ok!

```
inputs = [tf.squeeze(input_, [1])
          for input_ in tf.split(1, num_steps, inputs)]
outputs, state = rnn.rnn(cell, inputs, initial_state=self._initial_state)
```

But,when I use dynamic_rnn,like this:

```
outputs,state=rnn.dynamic_rnn(cell,inputs,initial_state=self._initial_state)
```

It's got an error:

```
Traceback (most recent call last):
  File ""ptb_word_lm.py"", line 391, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/default/_app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""ptb_word_lm.py"", line 368, in main
    m = PTBModel(is_training=True, config=config)
  File ""ptb_word_lm.py"", line 207, in __init__
    self._train_op=tf.train.GradientDescentOptimizer(0.01).minimize(loss)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py"", line 190, in minimize
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py"", line 241, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients.py"", line 481, in gradients
    in_grads = _AsList(grad_fn(op, *out_grads))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/tensor_array_grad.py"", line 137, in _TensorArrayPackGrad
    grad_source = _GetGradSource(grad)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/tensor_array_grad.py"", line 62, in _GetGradSource
    % op_or_tensor.name)
ValueError: Expected op/tensor name to start with gradients, got: model/gradients/model/RNN/transpose_grad/transpose:0
```
"
1634,KeyboardInterrupt make aws g2.2xlarge sys  no response 'cuda_gpu_executor.cc:624] Deallocating stream',"### Environment info

Operating System:
aws 
https://console.aws.amazon.com/ec2/v2/home?region=us-east-1#LaunchInstanceWizard:ami=ami-e191b38b

KeyboardInterrupt
E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:624] Deallocating stream with pending work
^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C

If installed from sources, provide the commit hash:
### Steps to reproduce

1.
2.
3.

http://ramhiser.com/2016/01/05/installing-tensorflow-on-an-aws-ec2-instance-with-gpu-support/  
he installed
### What have you tried?

1.
### Logs or other output that would be helpful

(If logs are large, please upload as attachment).
"
1632,how to set subtensor on Tensor objects,"As a naive example, in numpy: 

```
x = numpy.zeros((2, 3, 4))
y = numpy.ones((2, 3, 4))
x[1:] = y[:-1]
array([[[ 0.,  0.,  0.,  0.],
        [ 0.,  0.,  0.,  0.],
        [ 0.,  0.,  0.,  0.]],

       [[ 1.,  1.,  1.,  1.],
        [ 1.,  1.,  1.,  1.],
        [ 1.,  1.,  1.,  1.]]])
```

How to achieve this in tensorflow? I've looked into tf.scatter_update(), but it only works on Variable objects, not on Tensors. 
"
1628,Saver restoring multiple sessions throws errors,"### Environment info

Operating System: Linux
### Steps to reproduce
1. Save two sessions independently from each other
2. Run a Saver.restore on the first session
3. Run a Saver.restore on the second session
### Logs or other output that would be helpful

W tensorflow/core/common_runtime/executor.cc:1161] 0x3437b20 Compute status: Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [6453,128] rhs shape= [1853,128]
     [[Node: save/Assign_21 = Assign[T=DT_FLOAT, use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](embedding/W/Adam, save/restore_slice_21)]]
W tensorflow/core/common_runtime/executor.cc:1161] 0x3437b20 Compute status: Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [6453,128] rhs shape= [1853,128]
     [[Node: save/Assign_22 = Assign[T=DT_FLOAT, use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](embedding/W/Adam_1, save/restore_slice_22)]]
W tensorflow/core/common_runtime/executor.cc:1161] 0x3437b20 Compute status: Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [6453,128] rhs shape= [1853,128]
     [[Node: save/Assign_20 = Assign[T=DT_FLOAT, use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](embedding/W, save/restore_slice_20)]]
"
1619,a bug in tensorflow/core/graph/gradient.cc,"in the above file, i think that in line 257 there is a error
for (const Edge\* e : y->in_edges()) {
    if (e->IsControlEdge()) continue;
    BackpropAlongEdge(y_grad_node_outputs_[i], { e->src(), e->src_output() });
}
i think y->in_edges() should be y->out_edges(). please check this out as soon as possible, thank you
"
1618,Implement reduceByKey or groupByKey?,"Can we implement a Implement reduceByKey or groupByKey operation?
"
1610,1_notminst.ipynb not readable by Jupyter - SHA: b885fe,"The latest version seems unreadable. Earlier version works.
"
1607,Android Ndk 11,"Why Android Ndk 11 and other versions are not being supported by bazel android build?
It only supports ""android-ndk-r10e"".
"
1606,Confusion Matrix,"Is there any operation which generates Confusion Matrix in Tensorflow?
Can't find it on http://stackoverflow.com/questions/tagged/tensorflow
"
1604,Feature request: Implementing spatially-sparse conv networks in TensorFlow,"I am inspired by Dr. Ben Graham's recent work regarding spatially-sparse convolutional neural networks:

http://arxiv.org/abs/1409.6070 (particularly section 2.3)
http://www2.warwick.ac.uk/fac/sci/statistics/staff/academic-research/graham/sparse3d.pdf

He has graciously open-sourced his neural network library but I'd like to utilize these same ideas in my models, which are implemented in Tensorflow.

Unfortunately, I'm finding his implementation and citations a little hard to follow, but his description of a ""feature matrix"" and a ""pointer matrix"" sounds a little like something that can be implemented using sparse variable updates in Tensorflow (https://www.tensorflow.org/versions/r0.7/api_docs/python/state_ops.html#sparse-variable-updates), but I'm afraid that it might not be so simple and would require building custom kernels to support a new-ish type of convolution. But I don't know enough to say for certain which direction to take.

Any thoughts on how we can bring spatially-sparse convolutions to Tensorflow? Anyone interested in collaborating on implementing this?
"
1598,Android Demo build Error,"Get an error when building the Android demo on Ubuntu 14.04.

My Workspace setting for Android SDK and NDK are given below:

android_sdk_repository(
name = ""androidsdk"",
api_level = 23,
build_tools_version = ""23.0.1"",
path = ""/home/kuntal/Android/Sdk""
)

android_ndk_repository (
name = ""androidndk"",
path = ""/home/kuntal/knowledge/IDE/android/android-ndk-r10e/"",
api_level = 21
)

Also i have the proper version in the SDK/NDk repo. NDK version (r10e).

Now im trying to build with bazel using the following command:

sudo bazel build tensorflow_demo -c opt --copt=-mfpu=neon

And i even tried with sudo bazel build tensorflow_demo

_but i'm getting the following error:_

INFO: Found 1 target...
*_ERROR: *_/home/kuntal/knowledge/codebase/PRACTICE/BIG-DATA/TensorFlow/tensorflow/tensorflow/examples/android/BUILD:65:1: Processing resources failed: resources_processor failed: error executing command bazel-out/host/bin/external/bazel_tools/tools/android/resources_processor --buildToolsVersion 23.0.1 --aapt bazel-out/host/bin/external/androidsdk/aapt_binary --annotationJar ... (remaining 13 argument(s) skipped).
src/main/tools/namespace-sandbox.c:558: mount(opt->mount_sources[i], full_sandbox_path, NULL, MS_REC | MS_BIND | MS_RDONLY, NULL): Permission denied
Target //tensorflow/examples/android:tensorflow_demo failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1.950s, Critical Path: 0.14s

Please suggest how to fix this issue??
"
1597,Is it possible to make IPython able to plot with matplotlib as default?,"I am doing the assignment 1 of Deep learning course on Udacity. There is a problem to ask us to plot for checking the dataset.
I can not plot any figure with matplotlib initially (nothing happened). Then, looked for the solution on the internet. I found it should be with a line that will make IPython able to attach the figure:
`%matplotlib notebook` or `%matplotlib inline`

if we don't want to key the line above each time we can change the config of IPython. 
Is it possible to add the line as default in this repository? I guess for novice they would be stuck a while on this point.
"
1595,using tensor as bool in cifar10 example,"Hello,

I have just recompiled TF to commit f952246 and got an error on cifar10 example. Line `if grad:` should be updated to `if grad is not None:` due to recent changes. Just minor stuff.
"
1594,Install from sources: Error: can't copy 'tensorflow/python/pywrap_tensorflow.py': doesn't exist or not a regular file,"I recently tried to install Tensorflow on Ubuntu 14.10. I had cuda 7.0 and cudnn 3.0, so I had to install from sources following the instructions. I got ""error: can't copy 'tensorflow/python/pywrap_tensorflow.py': doesn't exist or not a regular file"" when building the pip package (with GPU support):

`$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
`
So far, I have set the configuration for cuda and cudnn correctly. What should I do? Thanks. 
"
1593,Android Camera Demo : Failed to build on Mac OS due to missing file ../darwin-x86/bin/arm-linux-androideabi-gcc,"GitHub issues are for bugs / installation problems / feature requests.  
For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
### Environment info

Operating System:
OS X EI Capitan version 10.11

If installed from sources, provide the commit hash:
[b3d862b](https://github.com/tensorflow/tensorflow/commit/b3d862b376656c0f58b729e1848ea6e0c5bebf2a)
### Steps to reproduce
1. Follow guidelines here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/README.md (Download Android NDK for MAC)
2. build android demo using `$bazel build //tensorflow/examples/android:tensorflow_demo --verbose_failures`
3. Error : `process-wrapper: execvp(""external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86/bin/arm-linux-androideabi-gcc"", ...): No such file or directory
   Target //tensorflow/examples/android:tensorflow_demo failed to build`
### What have you tried?
1. Tried another way to build using `$bazel build //tensorflow/examples/android:tensorflow_demo --verbose_failures --config=android_arm`
2. Also tried Android NDK version `r10e`
### Logs or other output that would be helpful

(If logs are large, please upload as attachment).
`ERROR: /Users/hassanabid/Documents/hassan/open_source/tensorflow/tensorflow_source/tensorflow/examples/android/BUILD:43:1: Linking of rule '//tensorflow/examples/android:libpthread.so' failed: arm-linux-androideabi-gcc failed: error executing command 
  (cd /private/var/tmp/_bazel_hassanabid/96ab15ac510a01a39eca351cb09446d8/tensorflow_source && \
  exec env - \
  external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86/bin/arm-linux-androideabi-gcc -o bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-py3-fastbuild/bin/tensorflow/examples/android/libpthread.so -Wl,-whole-archive -Wl,-no-whole-archive external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/libgnustl_static.a external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/libsupc++.a -shared -static-libgcc -no-canonical-prefixes '-march=armv7-a' -Wl,--fix-cortex-a8 -Wl,-S '--sysroot=external/androidndk/ndk/platforms/android-21/arch-arm'): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: arm-linux-androideabi-gcc failed: error executing command 
  (cd /private/var/tmp/_bazel_hassanabid/96ab15ac510a01a39eca351cb09446d8/tensorflow_source && \
  exec env - \
  external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86/bin/arm-linux-androideabi-gcc -o bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-py3-fastbuild/bin/tensorflow/examples/android/libpthread.so -Wl,-whole-archive -Wl,-no-whole-archive external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/libgnustl_static.a external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/libsupc++.a -shared -static-libgcc -no-canonical-prefixes '-march=armv7-a' -Wl,--fix-cortex-a8 -Wl,-S '--sysroot=external/androidndk/ndk/platforms/android-21/arch-arm'): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
process-wrapper: execvp(""external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86/bin/arm-linux-androideabi-gcc"", ...): No such file or directory
Target //tensorflow/examples/android:tensorflow_demo failed to build
INFO: Elapsed time: 0.438s, Critical Path: 0.04s`
"
1592,Binary ops,"Is there already a plan to add binary ops like bitcount for [XNOR-NET](http://arxiv.org/abs/1603.05279)?
"
1588,Slicing sparse tensor,"There does not appear to be a way to extract a (sparse or dense) slice from a sparse tensor in the same manner as there is from a dense tensor. Will this be a feature in the future, or is it already present and I'm just missing something? Perhaps it should be added to the planned improvements for indexing.
"
1587,TensorBoard doesn't handle ~ in paths properly,"Right now, TensorBoard does not properly evaluate a path beginning with `~` (at least on mac). Example:

```
(tensorflow) ~/space❯ pwd
/Users/danmane/space
(tensorflow) ~/space❯ tensorboard --logdir=~/foo/zoid --debug --host=localhost
INFO:tensorflow:TensorBoard is in debug mode.
INFO:tensorflow:Starting TensorBoard in directory /Users/danmane/space
INFO:tensorflow:TensorBoard path_to_run is: {'/Users/danmane/space/~/foo/zoid': None}
```
"
1586,TensorBoard prints warning: TAG not not found,"On start, TensorBoard prints the following message:

```
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/Users/danmane/tensorflow/lib/python2.7/site-packages/tensorflow/tensorboard/TAG' on path /Users/danmane/tensorflow/lib/python2.7/site-packages/tensorflow/tensorboard/TAG
WARNING:tensorflow:Unable to read TensorBoard tag
```

This issue is harmless but distracting. 
"
1582,Crash on retraining flowers,"I am following the tutorial [How to Retrain Inception's Final Layer for New Categories](https://www.tensorflow.org/versions/master/how_tos/image_retraining/index.html) but am getting this error:   

```
...
3600 bottleneck files created.
Traceback (most recent call last):
  File ""/Users/rick/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/tensorflow/examples/image_retraining/retrain.py"", line 829, in <module>
    tf.app.run()
  File ""/Users/rick/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/tensorflow/python/platform/default/_app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""/Users/rick/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/tensorflow/examples/image_retraining/retrain.py"", line 785, in main
    ground_truth_tensor: train_ground_truth})
  File ""/Users/rick/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/tensorflow/python/client/session.py"", line 332, in run
    result = self._run(None, fetches, feed_dict, options_ptr, run_outputs_ptr)
  File ""/Users/rick/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/tensorflow/python/client/session.py"", line 530, in _run
    % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))
ValueError: Cannot feed value of shape (100, 2048) for Tensor u'pool_3/_reshape:0', which has shape '(1, 2048)'
```
### Environment info

Operating System: OSX  10.11.1 El Capitan

I've installed from source from this commit `ad1d98011ebc`.  I ran `./configure` with the default answers and followed [these build instructions](https://www.tensorflow.org/versions/master/get_started/os_setup.html#installation-for-mac-os-x)
### Steps to reproduce

```
cd ~
curl -O http://download.tensorflow.org/example_images/flower_photos.tgz
tar xzf flower_photos.tgz
bazel build -c opt --copt=-mavx tensorflow/examples/image_retraining:retrain
# Crashes with error above
bazel-bin/tensorflow/examples/image_retraining/retrain --image_dir ~/flower_photos
```
### What have you tried?
1.  I've tried deleting `/tmp/bottleneck` and re-running, but I got the same error.
"
1581,Odd error message for empty session run with queues,"I have encountered a counter intuative error when working with queues and accidental `sess.run([])`.
This is a stretch, but possibly related to #1277 where batch norm updates are being run but there is no batch norm in the graph. 
### Environment info

Operating System: Ubuntu 14.04, tf version (63409bd23facad471973b110df998782c0e19c06) with cuda.
### Steps to reproduce

``` python
import tensorflow as tf

BREAK_ME = False

def get_iter():
    img_list = [(tf.zeros((12,)), ), (tf.zeros((12, )), )]
    xx = tf.train.shuffle_batch_join(
        img_list, batch_size=5,
        capacity=1000 + 3 * 5,
        min_after_dequeue=1000)
    return xx

if BREAK_ME:
    w = get_iter()

sess = tf.Session()
coord = tf.train.Coordinator()
threads = tf.train.start_queue_runners(sess=sess, coord=coord)

sess.run([])
```

As expected, when `BREAK_ME` is false I get:
`RuntimeError: The Session graph is empty.  Add operations to the graph before calling run().`

When `BREAK_ME` is true, enabling the creation of a queue I get the following:

```
Traceback (most recent call last):
  File ""test.py"", line 20, in <module>
    sess.run([])
  File ""/home/indico/Apps/tensorflow/_python_build/tensorflow/python/client/session.py"", line 332, in run
    result = self._run(None, fetches, feed_dict, options_ptr, run_outputs_ptr)
  File ""/home/indico/Apps/tensorflow/_python_build/tensorflow/python/client/session.py"", line 537, in _run
    feed_dict_string, options, run_outputs)
  File ""/home/indico/Apps/tensorflow/_python_build/tensorflow/python/client/session.py"", line 599, in _do_run
    target_list, options, run_outputs)
  File ""/home/indico/Apps/tensorflow/_python_build/tensorflow/python/client/session.py"", line 621, in _do_call
    e.code)
tensorflow.python.framework.errors.OutOfRangeError: RandomShuffleQueue '_0_shuffle_batch_join/random_shuffle_queue' is closed and has insufficient elements (requested 5, current size 0)
     [[Node: shuffle_batch_join = QueueDequeueMany[component_types=[DT_FLOAT], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](shuffle_batch_join/random_shuffle_queue, shuffle_batch_join/n/_8)]]
Caused by op u'shuffle_batch_join', defined at:
  File ""test.py"", line 14, in <module>
    w = get_iter()
  File ""test.py"", line 10, in get_iter
    min_after_dequeue=1000)
  File ""/home/indico/Apps/tensorflow/_python_build/tensorflow/python/training/input.py"", line 770, in shuffle_batch_join
    dequeued = queue.dequeue_many(batch_size, name=name)
  File ""/home/indico/Apps/tensorflow/_python_build/tensorflow/python/ops/data_flow_ops.py"", line 301, in dequeue_many
    self._queue_ref, n, self._dtypes, name=name)
  File ""/home/indico/Apps/tensorflow/_python_build/tensorflow/python/ops/gen_data_flow_ops.py"", line 359, in _queue_dequeue_many
    timeout_ms=timeout_ms, name=name)
  File ""/home/indico/Apps/tensorflow/_python_build/tensorflow/python/ops/op_def_library.py"", line 655, in apply_op
    op_def=op_def)
  File ""/home/indico/Apps/tensorflow/_python_build/tensorflow/python/framework/ops.py"", line 2104, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/indico/Apps/tensorflow/_python_build/tensorflow/python/framework/ops.py"", line 1129, in __init__
    self._traceback = _extract_stack()
```
"
1578,Release GPU memory after computation,"Is it possible to release all resources after computation?

For example,

``` python
import time
import tensorflow as tf

for i in range(0,10000000):
  t0 = time.clock()

  with tf.Graph().as_default():
    sess = tf.Session()

    a = tf.placeholder(tf.int16, name='a')
    y = tf.identity(a, name='y')

    sess.run(y, feed_dict={a: 3})
    sess.close()

  time.sleep(20)

print time.clock() - t0
```

When the program is sleeping, I type `nvidia-smi`, and the memory is always occupied.

``` bash
| NVIDIA-SMI 352.79     Driver Version: 352.79         |                       
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX TIT...  Off  | 0000:03:00.0      On |                  N/A |
| 29%   48C    P2    81W / 250W |   5843MiB /  6143MiB |      1%      Default |
```

The behavior I have observed is that only after the program exit, the memory is released. It makes using multiprocessing hard. Suppose one process is waited on a lock for another progress to finish, and all two processes need to join the main process. Then when process one release the lock, process two cannot get GPU memory, so it would fail.

Is there any way to release memory, so when the above program(not the two process example) is sleeping, it will release memory? 

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
### Environment info

Operating System:

If installed from binary pip package, provide:
1. Which pip package you installed.

``` bash
sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl
```
1. The output from python -c ""import tensorflow; print(tensorflow.**version**)"".

``` bash
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally
0.7.1
```
"
1577,TensorBoard: histogram charts sometimes duplicated,"I unclicked run1 and run2, but when I click on some variables, the run1 and run2 still show up. If I click on run1 and run2 again, five plots will be shown, with 2 of them repeated! By unclicking and clicking a run1, my board just messed up with more and more plots. This is a Google product? :-O

Screenshots:

![image](https://cloud.githubusercontent.com/assets/5016962/13942946/3a633a78-efb5-11e5-9dec-795b306778b3.png)

![image](https://cloud.githubusercontent.com/assets/5016962/13942974/8c51f4dc-efb5-11e5-9b14-3813f4110586.png)
"
1575,da,"GitHub issues are for bugs / installation problems / feature requests.  
For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
### Environment info

Operating System:

If installed from binary pip package, provide:
1. Which pip package you installed.
2. The output from python -c ""import tensorflow; print(tensorflow.**version**)"".

If installed from sources, provide the commit hash:
### Steps to reproduce

1.
2.
3.
### What have you tried?

1.
### Logs or other output that would be helpful

(If logs are large, please upload as attachment).
"
1574,"build offline documentation, how to?","I've noticed the API doc in https://www.tensorflow.org/versions/master/api_docs/index.html is not complete. Is there a way to build a nice HTML based API _locally_ that contains a complete documentation?

Basically the question is: how is this https://www.tensorflow.org/versions/master/api_docs/index.html being automatically generated? What's the magical command?
"
1570,AdaGradOptimizer on GPU (Making word2vec tutorial runnable on GPU),"When I try running the [word2vec tutorial](https://www.tensorflow.org/versions/r0.7/tutorials/word2vec/index.html) on GPU, I get the following error: 

```
Executor failed to create kernel. Invalid argument: AttrValue must not have reference type value of float_ref for attr 'tensor_type
```

It seems that the problem is with `AdaGradOptimizer`: when I replace it with `GradientDescentOptimizer`, I can run the code (see [here](https://gist.github.com/alexeygrigorev/ed884c433c495075f000))

Also see #86 
"
1569,[master] User op example doesn't work,"A recent change to remove the static linking in user ops seems to have broken them.
From the example:
`g++ -std=c++11 -shared zero_out.cc -o zero_out.so -I $TF_INC`
you need -fPIC to compile it:
`/usr/bin/ld: /tmp/ccvknfY4.o: relocation R_X86_64_32 against '.text' can not be used when making a shared object; recompile with -fPIC`

It used to be:

```
g++ -std=c++11 -shared zero_out.cc -o zero_out.so \
-I $TF_INC -l tensorflow_framework -L $TF_LIB \
-fPIC -Wl,-rpath $TF_LIB
```

which no longer works since tensorflow_framework was removed.

However, it builds just by adding -fPIC, and you can try to load it:
`tf.load_op_library('zero_out.so')`
resulting in:
`undefined symbol: _ZN10tensorflow8internal21CheckOpMessageBuilder9NewStringB5cxx11Ev`

(and on a minor note, the doc link to load_op_library is broken, it's missing a '.html')
"
1568,ResourceExhaustedError in translate example (with GPU),"GitHub issues are for bugs / installation problems / feature requests.  
For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
### Environment info

Operating System: Ubuntu 15.10

If installed from binary pip package, provide:

Which pip package you installed.

> sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl

The output from python -c ""import tensorflow; print(tensorflow.**version**)"".

> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally
> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally
> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally
> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally
> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally
> 0.7.1

If installed from sources, provide the commit hash:
### Steps to reproduce

Just run translate.py with 10M pairs training data.
### What have you tried?

smaller batch size 32
### Logs or other output that would be helpful

(If logs are large, please upload as attachment).

I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:475] Sum Total of in-use chunks: 3.63GiB
W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:211] Ran out of memory trying to allocate 32.00MiB.  See logs for memory state
W tensorflow/core/kernels/matmul_op.cc:158] Resource exhausted: OOM when allocating tensor with shape[2048,4096]
W tensorflow/core/common_runtime/executor.cc:1102] 0xe723bd30 Compute status: Resource exhausted: OOM when allocating tensor with shape[2048,4096]
     [[Node: gradients_2/model_with_buckets/embedding_attention_seq2seq_2/RNN/MultiRNNCell/Cell0/BasicLSTMCell/Linear/MatMul_grad/MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=true, transpose_b=false, _device=""/job:localhost/replica:0/task:0/gpu:0""](model_with_buckets/embedding_attention_seq2seq_2/RNN/MultiRNNCell/Cell0/BasicLSTMCell/Linear/concat, gradients_2/model_with_buckets/embedding_attention_seq2seq_2/RNN/MultiRNNCell/Cell0/BasicLSTMCell/add_grad/Reshape)]]
Traceback (most recent call last):
  File ""translate.py"", line 289, in <module>
    tf.app.run()
  File ""/home/yy/virtualenv/env1/local/lib/python2.7/site-packages/tensorflow/python/platform/default/_app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""translate.py"", line 286, in main
    train()
  File ""translate.py"", line 187, in train
    target_weights, bucket_id, False)
  File ""/home/yy/virtualenv/env1/local/lib/python2.7/site-packages/tensorflow/models/rnn/translate/seq2seq_model.py"", line 224, in step
    outputs = session.run(output_feed, input_feed)
  File ""/home/yy/virtualenv/env1/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 315, in run
    return self._run(None, fetches, feed_dict)
  File ""/home/yy/virtualenv/env1/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 511, in _run
    feed_dict_string)
  File ""/home/yy/virtualenv/env1/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 564, in _do_run
    target_list)
  File ""/home/yy/virtualenv/env1/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 586, in _do_call
    e.code)
tensorflow.python.framework.errors.ResourceExhaustedError: OOM when allocating tensor with shape[2048,4096]
     [[Node: gradients_2/model_with_buckets/embedding_attention_seq2seq_2/embedding_attention_decoder/attention_decoder/MultiRNNCell_3/Cell0/BasicLSTMCell/Linear/MatMul_grad/MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=true, transpose_b=false, _device=""/job:localhost/replica:0/task:0/gpu:0""](model_with_buckets/embedding_attention_seq2seq_2/embedding_attention_decoder/attention_decoder/MultiRNNCell_3/Cell0/BasicLSTMCell/Linear/concat, gradients_2/model_with_buckets/embedding_attention_seq2seq_2/embedding_attention_decoder/attention_decoder/MultiRNNCell_3/Cell0/BasicLSTMCell/add_grad/Reshape)]]
     [[Node: clip_by_global_norm_2/clip_by_global_norm_2/_9/_3023 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_179468_clip_by_global_norm_2/clip_by_global_norm_2/_9"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
Caused by op u'gradients_2/model_with_buckets/embedding_attention_seq2seq_2/embedding_attention_decoder/attention_decoder/MultiRNNCell_3/Cell0/BasicLSTMCell/Linear/MatMul_grad/MatMul_1', defined at:
  File ""translate.py"", line 289, in <module>
    tf.app.run()
  File ""/home/yy/virtualenv/env1/local/lib/python2.7/site-packages/tensorflow/python/platform/default/_app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""translate.py"", line 286, in main
    train()
  File ""translate.py"", line 155, in train
    model = create_model(sess, False)
  File ""translate.py"", line 132, in create_model
    forward_only=forward_only)
  File ""/home/yy/virtualenv/env1/local/lib/python2.7/site-packages/tensorflow/models/rnn/translate/seq2seq_model.py"", line 161, in __init__
    gradients = tf.gradients(self.losses[b], params)
  File ""/home/yy/virtualenv/env1/local/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py"", line 483, in gradients
    in_grads = _AsList(grad_fn(wrapped_op, *out_grads))
  File ""/home/yy/virtualenv/env1/local/lib/python2.7/site-packages/tensorflow/python/ops/math_grad.py"", line 431, in _MatMulGrad
    math_ops.matmul(op.inputs[0], grad, transpose_a=True))
  File ""/home/yy/virtualenv/env1/local/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py"", line 951, in matmul
    name=name)
  File ""/home/yy/virtualenv/env1/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 686, in _mat_mul
    transpose_b=transpose_b, name=name)
  File ""/home/yy/virtualenv/env1/local/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py"", line 655, in apply_op
    op_def=op_def)
  File ""/home/yy/virtualenv/env1/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2040, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/yy/virtualenv/env1/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1087, in __init__
    self._traceback = _extract_stack()

...which was originally created as op u'model_with_buckets/embedding_attention_seq2seq_2/embedding_attention_decoder/attention_decoder/MultiRNNCell_3/Cell0/BasicLSTMCell/Linear/MatMul', defined at:
  File ""translate.py"", line 289, in <module>
    tf.app.run()
[elided 3 identical lines from previous traceback]
  File ""translate.py"", line 132, in create_model
    forward_only=forward_only)
  File ""/home/yy/virtualenv/env1/local/lib/python2.7/site-packages/tensorflow/models/rnn/translate/seq2seq_model.py"", line 152, in **init**
    softmax_loss_function=softmax_loss_function)
  File ""/home/yy/virtualenv/env1/local/lib/python2.7/site-packages/tensorflow/python/ops/seq2seq.py"", line 926, in model_with_buckets
    decoder_inputs[:bucket[1]])
  File ""/home/yy/virtualenv/env1/local/lib/python2.7/site-packages/tensorflow/models/rnn/translate/seq2seq_model.py"", line 151, in <lambda>
    lambda x, y: seq2seq_f(x, y, False),
  File ""/home/yy/virtualenv/env1/local/lib/python2.7/site-packages/tensorflow/models/rnn/translate/seq2seq_model.py"", line 115, in seq2seq_f
    feed_previous=do_decode)
  File ""/home/yy/virtualenv/env1/local/lib/python2.7/site-packages/tensorflow/python/ops/seq2seq.py"", line 691, in embedding_attention_seq2seq
    initial_state_attention=initial_state_attention)
  File ""/home/yy/virtualenv/env1/local/lib/python2.7/site-packages/tensorflow/python/ops/seq2seq.py"", line 621, in embedding_attention_decoder
    initial_state_attention=initial_state_attention)
  File ""/home/yy/virtualenv/env1/local/lib/python2.7/site-packages/tensorflow/python/ops/seq2seq.py"", line 530, in attention_decoder
    cell_output, state = cell(x, state)
  File ""/home/yy/virtualenv/env1/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell.py"", line 663, in **call**
    cur_inp, new_state = cell(cur_inp, cur_state)
"
1565,Some problem about softmax,"Hi I have problem about the softmax

I use the Softmax_with_loss to train the VOC2007 image data set.
and image have been normal between -1,1.
but it have two experiment result.

network have 5 conv weight and 3 fully-connect
only train 2 image and batch size 1
learning rate = 0.005

In case 1:
initialize weight and biases arg stddev = 0.01
The loss continued to decline on very small value

In case 2
initialize weight and biases arg 0.1
The loss decline to 0.6
but it doesn't continued to low
and it can't effective prediction the training data

Is it right for training?
The image number and batch size are affect the initialize weight and biases?
"
1564,Does  custom Op run slower than inner Op?,"l want to incorporate an operation that isn't covered by the existing library, so I create a custom Op.
Howerver it run slowly. Does  custom Op run slower than inner Op? I have test on it by adding an new Op to do the same thing as matmul Op, the result is that the new op run slower than the exist matmul Op.
"
1563,"[Solved, please close it] cuda 7.5 and tensorflow 0.7.1 error: cuda_gpu_executor.cc could not open file to read NUMA node","Hi, when test tensorflow after installation, I get the titled error.

> cuda_gpu_executor.cc  could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node

In fact my cuda toolkit runs properly, and I can run all samples in it.
When run `import tensorflow as tf` and `hello = tf.constant('Hello, TensorFlow!')`, nothing is wrong and cuda libs can be found and loaded by tensorflow.

 BTW, my GPU is _GTX 980TI_ and my os is _Slackware current 64_. Latest driver is installed.

Thanks.
"
1561,Floating point exception (v0.7.1),"I can import tensorflow, build graphs, but Tensorflow always fails during 
`with tf.session(graph=graph) as session:
`
`
    tf.initialize_all_variables.run()
`

What's most strange is that if there are only simple tf.Variables, it works fine.
e.g.
`b = tf.Varaible(tf.zeros((5,5)))`
This works.

But for variables like this
`W1 = tf.Variable(`
`
        tf.truncated_normal(`
`
            shape=[szPatch1, szPatch1, nChannel, nFilter1],`
`
            stddev=1.0/(szPatch1*szPatch1)))`
Tensorflow always crashed with ""Floating point exception (core dumped)

I never experienced such a problem when I was using version 0.6.0.
Anyone has an idea how to deal with it?
### Code

`import tensorflow as tf`
`graph = tf.Graph()`
`with graph.as_default():`
`
    W = tf.Variable(tf.truncated_normal(shape=[10, 3]))`
`
with tf.Session(graph=graph) as session:`
`
    tf.initialize_all_variables().run()`
### Command and Error message:

~$ python ~/.local/lib/python2.7/site-packages/tensorflow/models/image/cifar10/cifar10_train.py

I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:
name: Graphics Device
major: 5 minor: 2 memoryClockRate (GHz) 1.0885
pciBusID 0000:03:00.0
Total memory: 12.00GiB
Free memory: 11.87GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:717] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Graphics Device, pci bus id: 0000:03:00.0)
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 32.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 64.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 128.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 256.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 512.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 32.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 64.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 128.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 256.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 512.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.00GiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.00GiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.00GiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.00GiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.00GiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:73] Allocating 11.27GiB bytes.
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:83] GPU 0 memory begins at 0x1306d40000 extends to 0x15d8484a67
Floating point exception (core dumped)
### OS:

~$ uname -vr
3.19.0-51-generic #58~14.04.1-Ubuntu SMP Fri Feb 26 22:02:58 UTC 2016
### GPU:

03:00.0 VGA compatible controller [0300]: NVIDIA Corporation GM200 [GeForce GTX TITAN X] [10de:17c2](rev a1) (prog-if 00 [VGA controller])
### Installation:

~$ pip uninstall tensorflow
~$ pip uninstall protobuf
~$ pip install --user --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl    
### Environment Variables:

~$ vim .bashrc
    export PATH=$PATH:/usr/local/cuda-7.5/bin
    export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-7.5/lib64
    export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cudnn_v4
"
1560,Add name scopes (op_scopes) to tf.image ops so they look nice in TensorBoard,"For instance, this:

```
image = tf.image.decode_png(tf.read_file(""image.png""))
whitened_image = tf.image.per_image_whitening(image)
```

produces this:
![screenshot from 2016-03-19 19 33 06](https://cloud.githubusercontent.com/assets/9740006/13902377/f7b47c6a-ee0a-11e5-8ee4-06161b7ceaf1.png)
instead of something like this:
![screenshot from 2016-03-19 19 37 56](https://cloud.githubusercontent.com/assets/9740006/13902386/61696c38-ee0b-11e5-9230-9410c11a3fb1.png)

I know we can create the name scopes (that's how I generated the image) but it should probably be default behaviour.
"
1558,Cyclic dependency error on build,"On my laptop, I get a cyclic dependency error
### Environment info

Operating System: Ubuntu 15.10

If installed from sources, provide the commit hash:
fd464caaa40cfa16c81712939e39bd14d88c6fd4 (most recent HEAD as of this submission)

google protobuf commit
fb714b3606bd663b823f6960a73d052f97283b74

bazel version 
Build label: 0.1.5
Build target: bazel-out/local_linux-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Tue Feb 9 19:15:13 2016 (1455045313)
Build timestamp: 1455045313
Build timestamp as int: 1455045313
### Steps to reproduce
1.  bazel test tensorflow/...  or bazel build -c opt //tensorflow/cc:tutorials_example_trainer
### What have you tried?
1.  trying to go through the chain of protobuf dependencies, but pointers would be appreciated.
2.  tried HEAD on protobuf
### Logs or other output that would be helpful

(If logs are large, please upload as attachment).
../../bin/bazel build //tensorflow/cc:tutorials_example_trainer
____Loading...
ERROR: /projects/tensorflow/google/protobuf/BUILD:272:1: in cc_binary rule //google/protobuf:protoc: cycle in dependency graph:
    //tensorflow/cc:tutorials_example_trainer
    //tensorflow/core:tensorflow
    //tensorflow/core:tensorflow_opensource
    //tensorflow/core:core
    //tensorflow/core:core_cpu
    //tensorflow/core:core_cpu_internal
    //tensorflow/core:framework_internal
    //tensorflow/core/kernels:bounds_check
    //tensorflow/core:lib
    //tensorflow/core:lib_internal
    //tensorflow/core:protos_all_cc
    //tensorflow/core:framework/function.pb.h
    //tensorflow/core:protos_all_cc_genproto
- //google/protobuf:protoc
  //google/protobuf:protoc_lib
  //google/protobuf:protobuf
  //google/protobuf:src/google/protobuf/map_lite_unittest.pb.h
  //google/protobuf:cc_test_protos_genproto
- //google/protobuf:protoc.
  ERROR: Loading of target '//tensorflow/cc:tutorials_example_trainer' failed; build aborted.
  ERROR: Loading failed; build aborted.
"
1553,SSL certification error,"I install the SSL certification but still got this error

please notify what i have to do

/usr/bin/pip run on Sat Mar 19 15:15:43 2016
Downloading/unpacking https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl
Cleaning up...
  Removing temporary dir /tmp/pip_build_trutech...
**Exception:**
Traceback (most recent call last):
  File ""/usr/lib/python2.7/dist-packages/pip/basecommand.py"", line 122, in main
    status = self.run(options, args)
  File ""/usr/lib/python2.7/dist-packages/pip/commands/install.py"", line 278, in run
    requirement_set.prepare_files(finder, force_root_egg_info=self.bundle, bundle=self.bundle)
  File ""/usr/lib/python2.7/dist-packages/pip/req.py"", line 1198, in prepare_files
    do_download,
  File ""/usr/lib/python2.7/dist-packages/pip/req.py"", line 1376, in unpack_url
    self.session,
  File ""/usr/lib/python2.7/dist-packages/pip/download.py"", line 546, in unpack_http_url
    resp = session.get(target_url, stream=True)
  File ""/usr/share/python-wheels/requests-2.2.1-py2.py3-none-any.whl/requests/sessions.py"", line 467, in get
    return self.request('GET', url, **kwargs)
  File ""/usr/lib/python2.7/dist-packages/pip/download.py"", line 237, in request
    return super(PipSession, self).request(method, url, _args, *_kwargs)
  File ""/usr/share/python-wheels/requests-2.2.1-py2.py3-none-any.whl/requests/sessions.py"", line 455, in request
    resp = self.send(prep, *_send_kwargs)
  File ""/usr/share/python-wheels/requests-2.2.1-py2.py3-none-any.whl/requests/sessions.py"", line 558, in send
    r = adapter.send(request, *_kwargs)
  File ""/usr/share/python-wheels/requests-2.2.1-py2.py3-none-any.whl/requests/adapters.py"", line 385, in send
    raise SSLError(e)
**SSLError: [Errno 1] _ssl.c:510: error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify failed**
"
1551,tensorflow-0.7.1-cp35-none-linux_x86_64.whl,"sudo pip3 install --upgrade http://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.1-cp35-none-linux_x86_64.whl
tensorflow-0.7.1-cp35-none-linux_x86_64.whl is not a supported wheel on this platform.
Storing debug log for failure in /home/xxx/.pip/pip.log
"
1548,How to add run name to TensorBoard?,"On my TensorBoard, the run name always shows as ""."" The Board put all of my runs under the same log path into one plot. How do I separate them by setting a run name? I did not find it out from your documentary.

![image](https://cloud.githubusercontent.com/assets/5016962/13895427/48971808-ed30-11e5-9f94-8be43e00252c.png)

How did you achieve this in your documentary?
![image](https://cloud.githubusercontent.com/assets/5016962/13895456/9d7438ce-ed30-11e5-86bf-e94173e20d95.png)

I tried your https://github.com/tensorflow/tensorflow/blob/r0.7/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py

The run name is also just a "".""....
"
1545,why I use conv2d with padding='VALID' is more slow than SAME?,"I use cnn for text classification.Document max len:200;Embedding size:120
When I use conv2d with padding='VALID'.  127.6 examples/sec
When I use conv2d with padding='SAME'.  333.3 examples/sec

def cnn_model(X,prefix,is_train=False):
    with tf.variable_scope(prefix):
        with tf.device('/gpu:0'):
            embeddings = tf.Variable(
                tf.random_uniform([WORDS_NUM, EMBEDDING_SIZE], -1.0, 1.0))
            embed = tf.nn.embedding_lookup(embeddings, X)
        word_vectors = tf.expand_dims(embed, 3)
        with tf.variable_scope('CNN_Layer1'):
            conv1=skflow.ops.conv2d(word_vectors,64,(2,EMBEDDING_SIZE),(1,1,1,1),padding='VALID')
            conv1=tf.nn.relu(conv1)
            pool1=tf.nn.max_pool(conv1,ksize=[1,2,1,1],strides=[1,2,1,1],padding='SAME')
        net=tf.reshape(pool1,[BATCH_SIZE,-1])
        if is_train:
            net=tf.nn.dropout(net, 0.7)
        net = slim.ops.fc(net, NUM_LABELS, activation=None, scope='fc8')
        return net
"
1542,Target //tensorflow/examples/android:tensorflow_demo failed to build,"ERROR: /home/saurabh/Downloads/tensorflow/tensorflow/examples/android/BUILD:43:1: Linking of rule '//tensorflow/examples/android:libpthread.so' failed: arm-linux-androideabi-gcc failed: error executing command 
  (cd /home/saurabh/.cache/bazel/_bazel_saurabh/b5e3c50abcfa34dcecd4eb2855a378fe/tensorflow && \
  exec env - \
  external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86/bin/arm-linux-androideabi-gcc -o bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/bin/tensorflow/examples/android/libpthread.so -Wl,-whole-archive -Wl,-no-whole-archive -shared -no-canonical-prefixes '-march=armv7-a' -Wl,--fix-cortex-a8 -Wl,-S '--sysroot=external/androidndk/ndk/platforms/android-21/arch-arm'): arm-linux-androideabi-gcc failed: error executing command 
  (cd /home/saurabh/.cache/bazel/_bazel_saurabh/b5e3c50abcfa34dcecd4eb2855a378fe/tensorflow && \
  exec env - \
  external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86/bin/arm-linux-androideabi-gcc -o bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/bin/tensorflow/examples/android/libpthread.so -Wl,-whole-archive -Wl,-no-whole-archive -shared -no-canonical-prefixes '-march=armv7-a' -Wl,--fix-cortex-a8 -Wl,-S '--sysroot=external/androidndk/ndk/platforms/android-21/arch-arm').
src/main/tools/namespace-sandbox.c:672: execvp(argv[0], argv): No such file or directory
Target //tensorflow/examples/android:tensorflow_demo failed to build
INFO: Elapsed time: 25.023s, Critical Path: 3.25s

I am not able to understand the issue!! ANy help appreciated
"
1541,Loading tensorflow before scipy.misc seems to cause imread to fail,"If I load `tensorflow` before loading `scipy.mics`, `scipy.misc.imread` seems to return `PIL.JpegImagePlugin.JpegImageFile`.

E.g. if I write

```
import tensorflow as tf
import scipy.misc

im = scipy.misc.imread('1.jpg')
```

`im` will be something like `array(<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=178x218 at 0x7F85ED8AA050>, dtype=object)`.

However, if I write

```
import scipy.misc
import tensorflow as tf

im = scipy.misc.imread('1.jpg')
```

`im` will be a normal `array` with `dtype=uint8`.

Why loading `tensorflow` first causes `scipy.misc.imread` to fail? I suppose this loading order should not matter.

I was building from source with commit `945a4b4cd12df0fdf3f0cc24b128bb4ce66ebf88`. I don't think I had this problem with older versions.
"
1539,[clang+CUDA] No ZeroesLike[DT_BOOL] kernel,"I'm working on getting tensorflow to build its GPU code with clang.

I have a hacked up crosstool, a hacked up clang (for std::complex support), and some minor changes to eigen (which are awaiting review).  The branch is at jlebar/tensorflow@cuda-clang, but checking that out isn't sufficient, because you need to customize some paths to make crosstool happy.  Anyway you don't have my WIP compiler changes, so you won't get very far.  :)

Having said all that, I'm trying to get the tensorflow tests to pass, while I wait for tra@ to hopefully figure out the crosstool business.  I'm looking at this one

```
$ bazel test -c opt --config=cuda_clang //tensorflow/core:ops_array_grad_test 
```

, which fails with

```
E tensorflow/core/common_runtime/executor.cc:332] Executor failed to create kernel. Not found: No registered 'ZerosLike' OpKernel for GPU devices compatible with node n4 = ZerosLike[T=DT_BOOL](n1)
     [[Node: n4 = ZerosLike[T=DT_BOOL](n1)]]
F tensorflow/core/ops/array_grad_test.cc:365] Check failed: ::tensorflow::Status::OK() == (sess->Run({{""x:0"", x}, {""dims:0"", dims}, {""dy:0"", dy}}, {""dx:0"", ""dx:1""}, {}, &out)) (OK vs. Not found: No registered 'ZerosLike' OpKernel for GPU devices compatible with node n4 = ZerosLike[T=DT_BOOL](n1)
     [[Node: n4 = ZerosLike[T=DT_BOOL](n1)]]
     [[Node: dx = SymbolicGradient[Tin=[DT_FLOAT, DT_BOOL, DT_FLOAT], Tout=[DT_FLOAT, DT_BOOL], f=Reverse[T=DT_FLOAT], _device=""/job:localhost/replica:0/task:0/gpu:0""](_recv_x_0/_2, _recv_dims_0/_4, _recv_dy_0/_6)]]
     [[Node: dx/_8 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_14_dx"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]])
external/bazel_tools/tools/test/test-setup.sh: line 52: 106730 Aborted                 (core dumped) ""$@""
```

Looking through the code, it seems that there's no zeroing kernel for bools, so I'm not sure how this is supposed to work?  The obvious change to add a kernel, jlebar/tensorflow@286c1647cca9ebcdbce4497995c794d4b0c55633, doesn't work -- we seem to invoke the new kernel, but the whole program just silently dies.

I'm pretty confused by what's going on here, what with the To32Bit functor being applied to an array of doubles (?) and so on.  Any pointers would be very much appreciated.
"
1538,Why does one graph affects the cost of the other graph althongh the two graphs do not have any relationship?,"I have got a strange problem, the first code is as below:

``` python
with tf.device('/gpu:0'):
    local3_value = np.load(""local3.npy"")
    weights4 = np.load(""weights4.npy"")
    biases4 = np.load(""biases4.npy"")
    weights3 = np.load(""weights4.npy"")
    biases3 = np.load(""biases4.npy"")
    with tf.variable_scope('local3') as scope:
        local3 = tf.matmul(local3_value, weights3)
    with tf.variable_scope('local4') as scope:
        local4 = tf.nn.relu(tf.matmul(local3_value, weights4)+biases4, name = scope.name)

    with tf.Session(config=tf.ConfigProto(log_device_placement=True))as sess:
        sess.run(local3)
        start_time  = time.time()
        predictions = sess.run(local4)
        duration = time.time() - start_time
    print(duration)
```

The duration is 0.244572162628

The second code is as below:

``` python
with tf.device('/gpu:0'):
    local3_value = np.load(""local3.npy"")
    weights4 = np.load(""weights4.npy"")
    biases4 = np.load(""biases4.npy"")
    weights3 = np.load(""weights4.npy"")
    biases3 = np.load(""biases4.npy"")
    with tf.variable_scope('local3') as scope:
        local3 = tf.nn.softplus(weights3)
    with tf.variable_scope('local4') as scope:
        local4 = tf.nn.relu(tf.matmul(local3_value, weights4)+biases4, name = scope.name)


    with tf.Session(config=tf.ConfigProto(log_device_placement=True))as sess:
        sess.run(local3)
        start_time  = time.time()
        predictions = sess.run(local4)
        duration = time.time() - start_time
        print(duration)
```

The duration is 0.0679910182953

The only difference between these two codes is the local3 layer. Why the graph of local3 affects the cost of graph of loca4? These two graphs do not have any relationship.
"
1535,google.protobuf imports are resolving to /usr/local instead of TF's local copy,"### Environment info

Operating System: Current Goobuntu

TF commit 945a4b4cd12df0fdf3f0cc24b128bb4ce66ebf88
google/protobuf@fb714b3606bd663b823f6960a73d052f97283b74
### Steps to reproduce

```
$ bazel run -c opt //tensorflow/python:default_platform__resource_loader_test
```

Results in

```
Traceback (most recent call last):
  File ""/usr/local/google/home/jlebar/.cache/bazel/_bazel_jlebar/b1c0b5b92791600151e68af1e7d712c0/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/default_platform__resource_loader_test.runfiles/tensorflow/python/platform/default/_resource_loader_test.py"", line 20, in <module>
from tensorflow.python.platform import googletest
  File ""/usr/local/google/home/jlebar/.cache/bazel/_bazel_jlebar/b1c0b5b92791600151e68af1e7d712c0/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/default_platform__resource_loader_test.runfiles/tensorflow/__init__.py"", line 23, in <module>
from tensorflow.python import *
  File ""/usr/local/google/home/jlebar/.cache/bazel/_bazel_jlebar/b1c0b5b92791600151e68af1e7d712c0/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/default_platform__resource_loader_test.runfiles/tensorflow/python/__init__.py"", line 49, in <module>
from tensorflow.core.framework.graph_pb2 import *
  File ""/usr/local/google/home/jlebar/.cache/bazel/_bazel_jlebar/b1c0b5b92791600151e68af1e7d712c0/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/default_platform__resource_loader_test.runfiles/tensorflow/core/framework/graph_pb2.py"", line 9, in <module>
from google.protobuf import symbol_database as _symbol_database
  File ""/usr/local/google/home/jlebar/.cache/bazel/_bazel_jlebar/b1c0b5b92791600151e68af1e7d712c0/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/default_platform__resource_loader_test.runfiles/google/protobuf/symbol_database.py"", line 186, in <module>
_DEFAULT = SymbolDatabase(pool=descriptor_pool.Default())
AttributeError: 'module' object has no attribute 'Default'
```

With @Yhg1s help, I've traced this down some, but now I'm stuck.

The issue seems to be that the descriptor_pool we import is not TF's local copy, but is instead the copy from /usr/lib/python2.7/dist-packages.

TF's local copy of symbol_database.py imports google.protobuf.descriptor_pool, and when it does so, it has the correct sys.path.  Yet somehow we end up at the wrong descriptor_pool.

Thomas's idea was that maybe we're importing google from the wrong path before this point, and that's what's causing us to find the wrong path to descriptor_pool.  Indeed, judging from a -vv log [1], this seems to be the case.

Here we import TF's local copy of google, and then _reimport_ it from /usr/lib.  Python doesn't normally do this, so Thomas's idea was that someone may be messing with sys.modules or performing other dark rituals.

At this point I'm kind of lost.

I suspect things will work fine if I use a virtualenv, and Thomas gave me enough of an idea of how to do this.  But the build should work without that, right?  (It's not even documented how to build within a virtualenv.)

[1] http://pastebin.com/raw/M3PTCTfY

(Edit: Wrong twouters.)
"
1534,Error running example on gpu,"Running `bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu` with latest development code results in:

```
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so.7.0 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so.4.0.7 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so.7.0 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so.7.0 locally
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:
name: GeForce GTX 780
major: 3 minor: 5 memoryClockRate (GHz) 1.006
pciBusID 0000:03:00.0
Total memory: 3.00GiB
Free memory: 2.93GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 780, pci bus id: 0000:03:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 780, pci bus id: 0000:03:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 780, pci bus id: 0000:03:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 780, pci bus id: 0000:03:00.0)
F tensorflow/stream_executor/cuda/cuda_driver.cc:383] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(context) (0 vs. 216)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 780, pci bus id: 0000:03:00.0)
Aborted (core dumped)
```

What could be wrong with my gpu setting?
"
1533,zlib.error: Error -3 while decompressing: invalid block type ,"I used pip to install Tensorflow . I ran the commands provided in the link https://www.tensorflow.org/versions/r0.7/get_started/os_setup.html#common-problems

The commands i Ran were :

sudo apt-get install python-pip python-dev
sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl

Tensorflow got installed properly and a folder was created in the usr/local/lib/python2.7/dist-packages folder.

I tried to create a model using the mnist  dataset and ran the following command in /usr/local/lib/python2.7/dist-packages/tensorflow/models/image/mnist folder

python convolution.py 

The following message was displayed in the console :

Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.
Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.
Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.
Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.
Extracting data/train-images-idx3-ubyte.gz
Traceback (most recent call last):
  File ""convolutional.py"", line 316, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/default/_app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""convolutional.py"", line 128, in main
    train_data = extract_data(train_data_filename, 60000)
  File ""convolutional.py"", line 75, in extract_data
    buf = bytestream.read(IMAGE_SIZE \* IMAGE_SIZE \* num_images)
  File ""/usr/lib/python2.7/gzip.py"", line 261, in read
    self._read(readsize)
  File ""/usr/lib/python2.7/gzip.py"", line 312, in _read
    uncompress = self.decompress.decompress(buf)
zlib.error: Error -3 while decompressing: invalid block type

I removed the mnist data files present in the data folder and then again executed the python command  : python convolution.py , Now surprisingly it is giving a bit different error as :
zlib.error: Error -3 while decompressing: invalid stored block lengths . 

I am using python2.7 running inside Ubuntu 14.04 Operating system . 
"
1532,why the tensorflow run slower on gpu than on cpu,"The tensorflow should run faster on gpu than on cpu , however, I got the opposite result. The time cost when running on gpu is longer than on cpu.The code is as below:

``` python
    with tf.device('/cpu:0'):
        local3_value = np.load(""local3.npy"")
    with tf.variable_scope('local4') as scope:
        weights4 = tf.get_variable('weights',[384,192])
        biases4 = tf.get_variable('biases',[192])
        local4 = tf.nn.relu(tf.matmul(local3_value, weights4)+biases4, name = scope.name)

    saver = tf.train.Saver()

    with tf.Session(config=tf.ConfigProto(log_device_placement=True))as sess:
        saver.restore(sess, ""/tmp/cifar10_train/model.ckpt-27000"")
        start_time  = time.time()
        predictions = sess.run(local4)
        duration = time.time() - start_time
        print(duration)
```

The variables such as weights4 and biases4 are restored from a trained model.
"
1530,Internal Compiler Error with retrain tutorial,"Bryan Hynds reported this problem using one of our tutorials, in these YouTube comments:
https://www.youtube.com/watch?v=h7xuEiZjqqo

---

I'm running into some problems with compiling.  I'm getting a compile error that's failing to build the target:

```
root@d121482d25c8:/tensorflow# bazel build -c opt --copt=-mavx tensorflow/examples/image_retraining:retrain
INFO: Reading 'startup' options from /root/.bazelrc: --batch
WARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.
INFO: Found 1 target...
INFO: From Compiling tensorflow/core/kernels/conv_ops.cc:
In file included from external/eigen_archive/eigen-eigen-0b9ab889fac2/unsupported/Eigen/CXX11/Core:35:0,
                 from external/eigen_archive/eigen-eigen-0b9ab889fac2/unsupported/Eigen/CXX11/Tensor:14,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/core/framework/types.h:23,
                 from ./tensorflow/core/framework/type_traits.h:22,
                 from ./tensorflow/core/framework/allocator.h:25,
                 from ./tensorflow/core/framework/op_kernel.h:22,
                 from ./tensorflow/core/framework/numeric_op.h:19,
                 from tensorflow/core/kernels/conv_ops.cc:22:
external/eigen_archive/eigen-eigen-0b9ab889fac2/unsupported/Eigen/CXX11/src/Core/util/EmulateArray.h: In static member function 'static void Eigen::internal::TensorExecutor<Expression, Eigen::ThreadPoolDevice, Vectorizable>::run(const Expression&, const Eigen::ThreadPoolDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long int>, 16>, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long int>, 1ul>, const Eigen::TensorMap<Eigen::Tensor<const float, 2, 1, long int>, 16>, const Eigen::TensorMap<Eigen::Tensor<const float, 2, 1, long int>, 16> > >; bool Vectorizable = true]':
external/eigen_archive/eigen-eigen-0b9ab889fac2/unsupported/Eigen/CXX11/src/Core/util/EmulateArray.h:24:67: warning: array subscript is above array bounds [-Warray-bounds]
   EIGEN_STRONG_INLINE T& operator[] (size_t index) { return values[index]; }
                                                                   ^
INFO: From Compiling tensorflow/core/kernels/argmax_op.cc:
gcc: internal compiler error: Killed (program cc1plus)
Please submit a full bug report,
with preprocessed source if appropriate.
See <file:///usr/share/doc/gcc-4.8/README.Bugs> for instructions.
ERROR: /tensorflow/tensorflow/core/BUILD:358:1: C++ compilation of rule '//tensorflow/core:kernel_lib' failed: gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections ... (remaining 79 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 4.
Target //tensorflow/examples/image_retraining:retrain failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 424.939s, Critical Path: 412.00s﻿
```

I've wiped the container and rebuilt it several times, and I can reproduce it every time I reach this point.  I'm compiling with --verbose_failures now to get more details.  I'll post the log once I have it.﻿
"
1528,Unique Error with Image Retraining Tutorial,
1524,Can tensorflow support slicing tensor every N element,"Slicing an array every N elements is a basic function of numpy or python list. However, I found that tensorflow doesn't support it. For example,

a = tf.constant(np.random.rand(3,100))
b = a[:,::4]
Traceback (most recent call last):
  File ""/Applications/Eclipse.app/Contents/Eclipse/plugins/org.python.pydev_4.3.0.201508182223/pysrc/pydevd_exec.py"", line 3, in Exec
    exec exp in global_vars, local_vars
  File ""<console>"", line 1, in <module>
  File ""/Users/yin.zheng/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py"", line 131, in _SliceHelper
    ""Steps other than 1 are not currently supported"")
NotImplementedError: Steps other than 1 are not currently supported

I wonder that how can we use slicing on tensor to select every N elements (rows or lines)?

Best wishes 
Yin
"
1522,closing file object in read_data() function for word2vec tutorials,"The `read_data()` function that appears in the following files:
- tensorflow/tensorflow/examples/tutorials/word2vec/word2vec_basic.py
- tensorflow/tensorflow/examples/udacity/5_word2vec.ipynb

Look like this: 

```
def read_data(filename):
  f = zipfile.ZipFile(filename)
  for name in f.namelist():
    return f.read(name).split()
  f.close()
```

```
def read_data(filename):
  f = zipfile.ZipFile(filename)
  for name in f.namelist():
    return tf.compat.as_str(f.read(name)).split()
  f.close()
```

There are two problems that these two versions of the function share: 
1. The for loop is redundant since the `return` statement exits the loop (and the function) with the first file in the loop. Also, there is only one file within the zip file anyway. 
2. Since the return statement exits the function early, it never executes the `f.close()` line. Therefore it doesn't get to close the file object properly. 

I propose something like this instead: 

```
def read_data(filename):
  """"""Extract the first file enclosed in a zip file as a list of words""""""
  with zipfile.ZipFile(filename) as f:
    data = f.read(f.namelist()[0]).split()
  return data
```

```
def read_data(filename):
  """"""Extract the first file enclosed in a zip file as a list of words""""""
  with zipfile.ZipFile(filename) as f:
    data = tf.compat.as_str(f.read(f.namelist()[0])).split()
  return data
```

I will submit a pull request with the proposed changes. 
"
1519,Number of broadcasting dimensions is limited,"### Environment info

Operating System: Ubuntu 14.04.4

If installed from sources, provide the commit hash: f92e338b178f75b0e7fc79aedc00680a49a982ac
### Steps to reproduce

Running this script:

``` python
import tensorflow as tf

shape_1 = (1, 20, 30, 1, 50)
shape_2 = (30, 1, 30, 20, 50)

A = tf.Variable(tf.random_normal(shape_1))
B = tf.Variable(tf.random_normal(shape_2))

with tf.Session() as S:
    S.run(tf.initialize_all_variables())
    C = S.run(A+B)
```

produces

``` python
tensorflow.python.framework.errors.UnimplementedError: Broadcast between [1,20,30,1,50] and [30,1,30,20,50] is not supported yet.
```
### What did you try?

Reducing the tensor ranks results in success.

``` python
shape_1 = (1, 20, 30)
shape_2 = (30, 1, 30)
```
### Logs or other output that would be helpful

``` python
Traceback (most recent call last):
  File ""tf_broadcast_issue.py"", line 11, in <module>
    C = S.run(A+B)
  File ""/home/sperkins/work/ska/code/tensorflow/_python_build/tensorflow/python/client/session.py"", line 332, in run
    result = self._run(None, fetches, feed_dict, options_ptr, run_outputs_ptr)
  File ""/home/sperkins/work/ska/code/tensorflow/_python_build/tensorflow/python/client/session.py"", line 536, in _run
    feed_dict_string, options, run_outputs)
  File ""/home/sperkins/work/ska/code/tensorflow/_python_build/tensorflow/python/client/session.py"", line 598, in _do_run
    target_list, options, run_outputs)
  File ""/home/sperkins/work/ska/code/tensorflow/_python_build/tensorflow/python/client/session.py"", line 620, in _do_call
    e.code)
tensorflow.python.framework.errors.UnimplementedError: Broadcast between [1,20,30,1,50] and [30,1,30,20,50] is not supported yet.
     [[Node: add = Add[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](Variable/read, Variable_1/read)]]
Caused by op u'add', defined at:
  File ""tf_broadcast_issue.py"", line 11, in <module>
    C = S.run(A+B)
  File ""/home/sperkins/work/ska/code/tensorflow/_python_build/tensorflow/python/ops/variables.py"", line 544, in <lambda>
    setattr(Variable, operator, lambda a, b: Variable._RunOp(operator, a, b))
  File ""/home/sperkins/work/ska/code/tensorflow/_python_build/tensorflow/python/ops/variables.py"", line 559, in _RunOp
    return getattr(ops.Tensor, operator)(a._AsTensor(), b)
  File ""/home/sperkins/work/ska/code/tensorflow/_python_build/tensorflow/python/ops/math_ops.py"", line 502, in binary_op_wrapper
    return func(x, y, name=name)
  File ""/home/sperkins/work/ska/code/tensorflow/_python_build/tensorflow/python/ops/gen_math_ops.py"", line 44, in add
    return _op_def_lib.apply_op(""Add"", x=x, y=y, name=name)
  File ""/home/sperkins/work/ska/code/tensorflow/_python_build/tensorflow/python/ops/op_def_library.py"", line 655, in apply_op
    op_def=op_def)
  File ""/home/sperkins/work/ska/code/tensorflow/_python_build/tensorflow/python/framework/ops.py"", line 2102, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/sperkins/work/ska/code/tensorflow/_python_build/tensorflow/python/framework/ops.py"", line 1129, in __init__
    self._traceback = _extract_stack()
```
"
1518,Anyway to save the session as an object but not a file?,"Hi,

I know that the tensorflow can save all variables into a file as a checkpoint
Is there anyway to save the tensor flow session into an object? like
a=Queue()
a.put( sess.status({%variables%}) )  #or maybe a.put( tf.train.status({%variables%}) )

So when I want to restore a session status. I just do a.get()

Is that possible?

Thank you.
"
1517,Extend existing ops to complex128,"This is a follow-up to #1420.
Now that `complex128` is supported in tensorflow, we should extend existing ops like `complex`, `real`, `imag`, … to support `complex128` as well.
I've already done some work on this, but I'm currently stuck on expressing dependencies between input and output types.
For example, the spec for the `Real` op is currently

``` c++
REGISTER_OP(""Real"")
    .Input(""in: complex64"")
    .Output(""out: float"");
```

I've extended this to

``` c++
REGISTER_OP(""Real"")
    .Input(""in: T"")
    .Output(""out: Tout"")
    .Attr(""T: {complex64, complex128} = DT_COMPLEX64"")
    .Attr(""Tout: {float, double} = DT_FLOAT"");
```

and specified corresponding `Tout` constraints when registering the kernels.

The problem is that the auto-generated Python op will default to `float32` for the output.
(It also turns `Tout` into an optional argument).
I'd like to express ""If `T==complex128`, then `Tout==float64`"" somehow.

The only way of doing it that I could come up with is by writing a Python wrapper that essentially does

``` python
def real(in):
    if in.dtype == tf.complex64:
        Tout = tf.float32
    elif in.dtype == tf.complex128:
        Tout = tf.float64
    return generated_real(in, Tout=Tout)
```

Would that be an acceptable solution, or is there a better one?
"
1516,can't seem to find the urls to your provided binary packages,"I was expecting to download a portable package (.whl or .egg) and use it as such:

```
import sys; sys.path.append( '/extensions/TensorFlow...win32.egg' )
import tensorflow # if this line doesn't work, the egg/whl is not portable.
# ...
```

when you say ""provided binary packages"" that just means I expect .pyd files within the egg or whl.

for an example of a portable egg, just look at pillow ;)
"
1515,rnn.bidirectional_rnn  cause a problem,"code:

``` python
import tensorflow as tf
from tensorflow.models.rnn import rnn,rnn_cell

seq_length = 5
rnn_size = 6
input_data = [[1,2,3,4,5],[6,1,8,9,10],[11,12,1,14,15]]
with tf.device(""/cpu:0""):
 embedding = tf.get_variable(""embedding"", [20, rnn_size])
 result = tf.nn.embedding_lookup(embedding, input_data)
 inputs = tf.split(1,seq_length,result)
 inputs = [tf.squeeze(input_, [1]) for input_ in inputs]

cell_bw = rnn_cell.BasicLSTMCell(rnn_size)
cell_fw = rnn_cell.BasicLSTMCell(rnn_size)
bi = rnn.bidirectional_rnn(cell_fw,cell_bw,inputs,cell_fw.zero_state(3, tf.float32),cell_bw.zero_state(3, tf.float32),sequence_length=5)

with tf.Session() as session:
    tf.initialize_all_variables().run()
    print(session.run(bi))
```

error:
""Shapes %s and %s must have the same rank"" % (self, other))
ValueError: Shapes TensorShape([]) and TensorShape([Dimension(None)]) must have the same rank

it seem that the inputs have a problem, Can you tell me why ?
"
1513,seems issues with softmax_cross_entropy_with_logits,"matrix1 = tf.constant([[ -2.88181912e+11,-4.40300175e+11], [ -2.29688918e+11, -3.53027424e+11] ,[ -2.14650274e+11, -3.81851140e+11]])
matrix2 = tf.constant([[0, 1.0], [0, 1], [0, 1]])
r = tf.nn.softmax_cross_entropy_with_logits(matrix1, matrix2)

with tf.Session() as sess:
    result = sess.run(r)
    print result

when i run this, i get:

[  1.52118264e+11   1.23338506e+11   1.67200866e+11]

is this wrong ?
"
1511,No gradients  provided for any variable ?,"Hi,

When using tensorflow, I found_ '**ValueError: No gradients provided for any variable**' _

I used AdamOptimizer and GradientDescentOptimizer, and I could see this same error.

I didn't used tf.argmax but tf.nn.softmax, tf.tile, tf.nn.relu, tf.matmul, tf.tanh, tf.tile, tf.mul ...

What is the reason for this error??
and How can I solve this one??

Thanks !
"
1506,ImportError: No module named tensorboard,"``` bash
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ImportError: No module named tensorboard
```

I noticed that this issue is related to #1199 

It occurs after upgraded to v0.7.1 from v0.6. Seems `pip` did not overwrite `/usr/local/bin/tensorboard` when installing `tensorflow`. And the following `import` statement in the old script cause the error:

``` python
from tensorflow.tensorboard.tensorboard import main
```
"
1503,"[Feature Request] Graph embedding (DeepWalk, LINE, ...)","Hello,

Recently algorithms such as DeepWalk or LINE, by applying word embedding techniques to graph, have opened a way to apply deep learning to graph analysis. I wonder if there is any future plan for Tensorflow to support graph / network analysis in general and graph embedding in specific? 
"
1502,BatchNorm kernel?,"Hello,
Adding batch norm to a model results in a significant slowdown (using tf.nn.batch_normalize).

I was wondering why a dedicated kernel was removed in favour of a symbolic approach?
(seemingly unrelated commit in question: https://github.com/tensorflow/tensorflow/commit/03869e6e9cfac85d3e0f87ded01f4dd8de10ed67)

Was this a design decisions to not want these ""one off"" style optimizations/kernels?

Having a faster kernel, (like cudnn #714, or something like what torch does) would be great if its not against tensorflows design. I will look into it if it is not.

Thank you.
"
1501,ImportError: libcudart.so.7.5: cannot open shared object file,"For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
### Environment info

Operating System: Fedora 23

If installed from sources, provide the commit hash:
b44a68232087284eccd55c6031e7fc3ebe5663ab
### Steps to reproduce
1.  ./configure with GPU support in /usr/local/cuda 
2.  build tests with --config=cuda option
   3.
### What have you tried?
1.  building tests without --config=cuda has no problem
2.  made sure libcudart.so.7.5 exists in /usr/local/cuda/lib64 (as a soft link to libcudart.so.7.5.18)
3.  ./configure with /usr/local/cuda/lib64 (says file not found)
### Logs or other output that would be helpful

(If logs are large, please upload as attachment).
http://pastebin.com/xKrvzawQ
"
1500,beam search in translation model,"Do we have plan to support beam search in the decoding process of translation model, since the raw paper shows better result with beam search?
"
1499,'Math Processing Error' in web docs,"See; tensorflow.org/versions/r0.7/tutorials/mnist/beginners/index.html  --> Training section. ""Math Processing Error"" appears in multiple places in page. Used OSX / Safari, Firefox, and Chrome.  Screen shot attached.
![screen shot 2016-03-14 at 3 15 14 pm](https://cloud.githubusercontent.com/assets/351870/13761735/a3f2a3b6-e9f7-11e5-9373-5b7eb37fd534.png)
"
1498,bazel build nvcc: nvcc fatal : Unsupported gpu architecture 'compute_21',"After I executed the command
`bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer`

I get an error like 
`ERROR: /home/rajarshee/tensorflow/tensorflow/core/kernels/BUILD:517:1: error while parsing .d file: /home/rajarshee/.cache/bazel/_bazel_rajarshee/0d043bf46cad9f31127eb8d06453610d/tensorflow/bazel-out/local_linux-py3-opt/bin/tensorflow/core/kernels/_objs/adjust_contrast_op_gpu/tensorflow/core/kernels/adjust_contrast_op_gpu.cu.d (No such file or directory).
nvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.
nvcc fatal   : Unsupported gpu architecture 'compute_21'
Target //tensorflow/cc:tutorials_example_trainer failed to build`

I provided 2.1 gpu compute capability in ./configure earlier.
"
1497,"Hey, I wanted to know where can i find the implementation of the tensorflow with respect to devices like GPU. Basically I want to understand from source code level what changes are done inside the tensor flow to make it comptabile to GPU when launched. How  does the CUDA compile/run come into play when a session is launched ? Appreciate the help/pointers.","GitHub issues are for bugs / installation problems / feature requests.  
For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
### Environment info

Operating System:

If installed from binary pip package, provide:
1. Which pip package you installed.
2. The output from python -c ""import tensorflow; print(tensorflow.**version**)"".

If installed from sources, provide the commit hash:
### Steps to reproduce

1.
2.
3.
### What have you tried?

1.
### Logs or other output that would be helpful

(If logs are large, please upload as attachment).
"
1494,Does the tensorflow support structured numpy array as inputs?," data = np.array([(1,2.), (2,3.)],dtype=[('foo', 'i4'),('bar', 'f4')])

I want to feed this into
x = tf.placeholder(""float"", [None,2])

Will the tensorflow automatically convert they datatypes?
"
1486,DocBug,"This tutorial *_will how you *_how to run the example script on your own images, and will explain some of the options you have to help control the training process.

found at
https://www.tensorflow.org/versions/master/how_tos/image_retraining/index.html
"
1482,getting the dynamic shape of a Tensor as an integer,"How can I get xshape as a int variable? Is this feature available?

Of course if I evaluate, I will get the value of xshape, but the input is dynamic and I don't want to use the get_sum function in for loop, because it would break the graph.

Problem: TypeError: range() integer end argument expected, got Tensor

```
import tensorflow as tf
import numpy as np


x = tf.placeholder(tf.float32, shape=[None])
xshape = tf.placeholder(tf.float32, shape=[])


def get_sum(x, xshape):
  sum = 0
  for i in range(xshape):
    sum += x[i]

init = tf.initialize_all_variables()
sum = get_sum(x, xshape)

with tf.Session() as sess:
  sess.run(init)
  for i in range(100):
    length = np.random.randint(0,10)
    a = np.random.randint(0, 10, length)
  print sess.run(sum,feed_dict={x:a, xshape:length})
```
"
1481,3D convnets,"Hi All, do you have any plans to release code for tensor flow implementing 3D convnets?
"
1480,Renaming of input arguments for clarity,"Sorry in advance for being a bit pedantic... I bring this up because of lack of clarity when it comes to `gather`.

Right now there are many names for a generic input:
- `to_float` and others use `x` instead of `tensor`
- `shape`, `size`, `rank`, and others use `input` instead of `tensor`
- `reshape` and others use `tensor`
- `gather` uses `params` instead of `tensor`
- `dynamic_partition` uses `data` instead of `tensor`
- `pack` and `unpack` use `values` and `value` instead of `tensor_list` and `tensor`

`params` suggests that `gather` is special purpose, perhaps for efficiency or some other reason. Is it? If so, should we update the documentation? If not, can we change the name of `params`?

In addition, I think `value`(s) and `input` should be changed: a `Tensor` isn't a value and `input` is a Python built-in.
"
1477,"CUDA_ERROR_MISALIGNED_ADDRESS running CIFAR10 on CUDA7.5 with cuDNN v3, built from source","### Environment info

Operating System: Ubuntu 14.04.4 LTS, NVIDIA GeForce 840M

Installed from sources,  commit hash:
30b52579f6d66071ac7cdc7179e2c4aae3c9cb88
### Steps to reproduce
1. run python  cifar10_train.py
2. program errorr out after a while
   3.
### What have you tried?
1. restart machine and rerun several times. The same issue.
### Logs or other output that would be helpful

(If logs are large, please upload as attachment).

2016-03-12 14:02:02.607728: step 4010, loss = 1.22 (421.6 examples/sec; 0.304 sec/batch)
2016-03-12 14:02:05.731932: step 4020, loss = 1.18 (425.2 examples/sec; 0.301 sec/batch)
2016-03-12 14:02:08.874431: step 4030, loss = 1.21 (440.1 examples/sec; 0.291 sec/batch)
2016-03-12 14:02:12.053320: step 4040, loss = 1.30 (367.8 examples/sec; 0.348 sec/batch)
2016-03-12 14:02:15.304938: step 4050, loss = 1.17 (391.1 examples/sec; 0.327 sec/batch)
2016-03-12 14:02:18.456706: step 4060, loss = 1.23 (416.5 examples/sec; 0.307 sec/batch)
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 8333348 get requests, put_count=8333340 evicted_count=1000 eviction_rate=0.00012 and unsatisfied allocation rate=0.00013296
2016-03-12 14:02:21.675055: step 4070, loss = 1.19 (424.8 examples/sec; 0.301 sec/batch)
2016-03-12 14:02:24.810989: step 4080, loss = 1.26 (412.4 examples/sec; 0.310 sec/batch)
2016-03-12 14:02:28.031250: step 4090, loss = 1.21 (424.3 examples/sec; 0.302 sec/batch)
2016-03-12 14:02:31.328527: step 4100, loss = 1.13 (396.7 examples/sec; 0.323 sec/batch)
2016-03-12 14:02:35.028202: step 4110, loss = 1.23 (414.8 examples/sec; 0.309 sec/batch)
2016-03-12 14:02:38.254790: step 4120, loss = 1.35 (364.6 examples/sec; 0.351 sec/batch)
2016-03-12 14:02:41.484692: step 4130, loss = 1.47 (415.9 examples/sec; 0.308 sec/batch)
2016-03-12 14:02:44.718649: step 4140, loss = 1.26 (406.7 examples/sec; 0.315 sec/batch)
2016-03-12 14:02:47.988837: step 4150, loss = 1.07 (408.5 examples/sec; 0.313 sec/batch)
2016-03-12 14:02:51.287844: step 4160, loss = 1.17 (404.1 examples/sec; 0.317 sec/batch)
2016-03-12 14:02:54.654917: step 4170, loss = 1.33 (404.0 examples/sec; 0.317 sec/batch)
2016-03-12 14:02:57.782336: step 4180, loss = 1.18 (404.1 examples/sec; 0.317 sec/batch)
2016-03-12 14:03:01.022658: step 4190, loss = 1.15 (409.9 examples/sec; 0.312 sec/batch)
2016-03-12 14:03:04.232610: step 4200, loss = 1.24 (365.7 examples/sec; 0.350 sec/batch)
2016-03-12 14:03:07.854789: step 4210, loss = 1.11 (400.8 examples/sec; 0.319 sec/batch)
2016-03-12 14:03:11.121573: step 4220, loss = 1.31 (371.6 examples/sec; 0.344 sec/batch)
2016-03-12 14:03:14.307746: step 4230, loss = 1.46 (382.4 examples/sec; 0.335 sec/batch)
2016-03-12 14:03:17.566388: step 4240, loss = 1.14 (425.0 examples/sec; 0.301 sec/batch)
2016-03-12 14:03:20.750270: step 4250, loss = 1.21 (412.2 examples/sec; 0.311 sec/batch)
2016-03-12 14:03:24.010454: step 4260, loss = 1.16 (401.9 examples/sec; 0.319 sec/batch)
2016-03-12 14:03:27.315912: step 4270, loss = 1.16 (351.9 examples/sec; 0.364 sec/batch)
2016-03-12 14:03:30.528823: step 4280, loss = 1.24 (397.8 examples/sec; 0.322 sec/batch)
2016-03-12 14:03:33.777292: step 4290, loss = 1.45 (393.2 examples/sec; 0.326 sec/batch)
2016-03-12 14:03:37.022320: step 4300, loss = 1.19 (396.9 examples/sec; 0.323 sec/batch)
2016-03-12 14:03:40.659591: step 4310, loss = 1.36 (415.7 examples/sec; 0.308 sec/batch)
2016-03-12 14:03:43.828922: step 4320, loss = 1.18 (396.4 examples/sec; 0.323 sec/batch)
2016-03-12 14:03:47.227371: step 4330, loss = 0.96 (377.0 examples/sec; 0.340 sec/batch)
2016-03-12 14:03:50.400040: step 4340, loss = 1.13 (415.3 examples/sec; 0.308 sec/batch)
2016-03-12 14:03:53.671297: step 4350, loss = 1.12 (405.1 examples/sec; 0.316 sec/batch)
2016-03-12 14:03:56.815646: step 4360, loss = 1.09 (414.5 examples/sec; 0.309 sec/batch)
2016-03-12 14:04:00.007053: step 4370, loss = 1.22 (382.2 examples/sec; 0.335 sec/batch)
2016-03-12 14:04:03.242795: step 4380, loss = 1.19 (414.5 examples/sec; 0.309 sec/batch)
2016-03-12 14:04:06.424938: step 4390, loss = 1.13 (392.6 examples/sec; 0.326 sec/batch)
2016-03-12 14:04:09.578756: step 4400, loss = 1.02 (402.1 examples/sec; 0.318 sec/batch)
2016-03-12 14:04:13.179099: step 4410, loss = 1.34 (409.3 examples/sec; 0.313 sec/batch)
2016-03-12 14:04:16.348951: step 4420, loss = 1.21 (408.8 examples/sec; 0.313 sec/batch)
2016-03-12 14:04:19.599094: step 4430, loss = 1.10 (420.9 examples/sec; 0.304 sec/batch)
2016-03-12 14:04:22.736091: step 4440, loss = 1.11 (407.3 examples/sec; 0.314 sec/batch)
2016-03-12 14:04:25.979362: step 4450, loss = 1.27 (389.7 examples/sec; 0.328 sec/batch)
2016-03-12 14:04:29.173873: step 4460, loss = 1.18 (387.9 examples/sec; 0.330 sec/batch)
2016-03-12 14:04:32.369984: step 4470, loss = 1.36 (422.4 examples/sec; 0.303 sec/batch)
2016-03-12 14:04:35.565128: step 4480, loss = 1.19 (409.5 examples/sec; 0.313 sec/batch)
2016-03-12 14:04:38.783202: step 4490, loss = 1.14 (391.8 examples/sec; 0.327 sec/batch)
2016-03-12 14:04:41.968468: step 4500, loss = 1.20 (389.9 examples/sec; 0.328 sec/batch)
2016-03-12 14:04:45.529786: step 4510, loss = 1.14 (397.3 examples/sec; 0.322 sec/batch)
2016-03-12 14:04:48.693115: step 4520, loss = 1.12 (393.7 examples/sec; 0.325 sec/batch)
2016-03-12 14:04:51.892441: step 4530, loss = 1.25 (399.6 examples/sec; 0.320 sec/batch)
2016-03-12 14:04:55.102551: step 4540, loss = 1.26 (411.5 examples/sec; 0.311 sec/batch)
2016-03-12 14:04:58.258904: step 4550, loss = 1.27 (395.5 examples/sec; 0.324 sec/batch)
2016-03-12 14:05:01.485362: step 4560, loss = 1.08 (404.6 examples/sec; 0.316 sec/batch)
2016-03-12 14:05:04.711979: step 4570, loss = 1.25 (365.1 examples/sec; 0.351 sec/batch)
2016-03-12 14:05:07.864737: step 4580, loss = 1.31 (390.1 examples/sec; 0.328 sec/batch)
2016-03-12 14:05:11.075490: step 4590, loss = 1.21 (385.9 examples/sec; 0.332 sec/batch)
2016-03-12 14:05:14.205806: step 4600, loss = 0.96 (419.8 examples/sec; 0.305 sec/batch)
2016-03-12 14:05:17.764024: step 4610, loss = 1.16 (419.8 examples/sec; 0.305 sec/batch)
2016-03-12 14:05:20.992224: step 4620, loss = 1.17 (407.9 examples/sec; 0.314 sec/batch)
2016-03-12 14:05:24.182559: step 4630, loss = 1.20 (406.1 examples/sec; 0.315 sec/batch)
2016-03-12 14:05:27.346289: step 4640, loss = 1.30 (421.5 examples/sec; 0.304 sec/batch)
2016-03-12 14:05:30.537761: step 4650, loss = 1.05 (403.8 examples/sec; 0.317 sec/batch)
2016-03-12 14:05:33.755113: step 4660, loss = 1.10 (394.2 examples/sec; 0.325 sec/batch)
2016-03-12 14:05:36.904973: step 4670, loss = 1.20 (402.2 examples/sec; 0.318 sec/batch)
2016-03-12 14:05:40.113832: step 4680, loss = 1.10 (407.4 examples/sec; 0.314 sec/batch)
2016-03-12 14:05:43.286804: step 4690, loss = 1.05 (406.8 examples/sec; 0.315 sec/batch)
2016-03-12 14:05:46.492911: step 4700, loss = 1.19 (427.0 examples/sec; 0.300 sec/batch)
2016-03-12 14:05:50.100880: step 4710, loss = 1.09 (414.9 examples/sec; 0.309 sec/batch)
2016-03-12 14:05:53.295059: step 4720, loss = 1.16 (403.4 examples/sec; 0.317 sec/batch)
2016-03-12 14:05:56.461129: step 4730, loss = 1.03 (418.4 examples/sec; 0.306 sec/batch)
2016-03-12 14:05:59.686046: step 4740, loss = 1.01 (392.7 examples/sec; 0.326 sec/batch)
2016-03-12 14:06:02.874850: step 4750, loss = 1.11 (403.2 examples/sec; 0.317 sec/batch)
2016-03-12 14:06:06.063848: step 4760, loss = 1.15 (421.9 examples/sec; 0.303 sec/batch)
2016-03-12 14:06:09.311339: step 4770, loss = 1.00 (401.5 examples/sec; 0.319 sec/batch)
2016-03-12 14:06:12.400291: step 4780, loss = 1.03 (412.0 examples/sec; 0.311 sec/batch)
2016-03-12 14:06:15.594995: step 4790, loss = 1.08 (372.0 examples/sec; 0.344 sec/batch)
2016-03-12 14:06:18.774854: step 4800, loss = 1.31 (378.7 examples/sec; 0.338 sec/batch)
2016-03-12 14:06:22.455985: step 4810, loss = 1.17 (359.9 examples/sec; 0.356 sec/batch)
2016-03-12 14:06:25.655368: step 4820, loss = 1.04 (420.3 examples/sec; 0.305 sec/batch)
2016-03-12 14:06:28.822332: step 4830, loss = 1.26 (404.5 examples/sec; 0.316 sec/batch)
2016-03-12 14:06:32.007472: step 4840, loss = 1.01 (411.2 examples/sec; 0.311 sec/batch)
2016-03-12 14:06:35.283917: step 4850, loss = 1.41 (434.8 examples/sec; 0.294 sec/batch)
2016-03-12 14:06:38.506687: step 4860, loss = 1.03 (401.3 examples/sec; 0.319 sec/batch)
2016-03-12 14:06:41.663434: step 4870, loss = 1.12 (395.4 examples/sec; 0.324 sec/batch)
2016-03-12 14:06:44.828674: step 4880, loss = 1.21 (405.5 examples/sec; 0.316 sec/batch)
2016-03-12 14:06:48.007878: step 4890, loss = 1.09 (410.4 examples/sec; 0.312 sec/batch)
2016-03-12 14:06:51.222651: step 4900, loss = 1.00 (396.6 examples/sec; 0.323 sec/batch)
2016-03-12 14:06:54.860536: step 4910, loss = 1.20 (383.4 examples/sec; 0.334 sec/batch)
2016-03-12 14:06:58.063615: step 4920, loss = 1.17 (410.9 examples/sec; 0.312 sec/batch)
2016-03-12 14:07:01.184445: step 4930, loss = 1.05 (397.0 examples/sec; 0.322 sec/batch)
2016-03-12 14:07:04.394785: step 4940, loss = 1.06 (371.3 examples/sec; 0.345 sec/batch)
2016-03-12 14:07:07.583337: step 4950, loss = 1.17 (394.5 examples/sec; 0.324 sec/batch)
2016-03-12 14:07:10.724218: step 4960, loss = 1.09 (397.4 examples/sec; 0.322 sec/batch)
2016-03-12 14:07:13.931494: step 4970, loss = 1.04 (389.5 examples/sec; 0.329 sec/batch)
2016-03-12 14:07:17.116672: step 4980, loss = 1.30 (405.9 examples/sec; 0.315 sec/batch)
2016-03-12 14:07:20.301438: step 4990, loss = 1.26 (427.9 examples/sec; 0.299 sec/batch)
2016-03-12 14:07:23.464006: step 5000, loss = 1.18 (380.4 examples/sec; 0.337 sec/batch)
2016-03-12 14:07:27.304553: step 5010, loss = 1.21 (378.2 examples/sec; 0.338 sec/batch)
2016-03-12 14:07:30.435311: step 5020, loss = 0.96 (420.0 examples/sec; 0.305 sec/batch)
2016-03-12 14:07:33.525055: step 5030, loss = 1.50 (426.5 examples/sec; 0.300 sec/batch)
2016-03-12 14:07:36.664561: step 5040, loss = 1.09 (400.9 examples/sec; 0.319 sec/batch)
2016-03-12 14:07:39.873659: step 5050, loss = 1.09 (422.8 examples/sec; 0.303 sec/batch)
2016-03-12 14:07:43.060801: step 5060, loss = 1.00 (409.5 examples/sec; 0.313 sec/batch)
2016-03-12 14:07:46.216866: step 5070, loss = 1.22 (424.8 examples/sec; 0.301 sec/batch)
2016-03-12 14:07:49.424010: step 5080, loss = 1.20 (396.9 examples/sec; 0.323 sec/batch)
2016-03-12 14:07:52.615564: step 5090, loss = 1.03 (393.3 examples/sec; 0.325 sec/batch)
2016-03-12 14:07:55.773694: step 5100, loss = 1.43 (421.5 examples/sec; 0.304 sec/batch)
2016-03-12 14:07:59.381470: step 5110, loss = 1.00 (420.5 examples/sec; 0.304 sec/batch)
2016-03-12 14:08:02.574327: step 5120, loss = 1.08 (412.5 examples/sec; 0.310 sec/batch)
2016-03-12 14:08:05.758566: step 5130, loss = 0.94 (413.8 examples/sec; 0.309 sec/batch)
2016-03-12 14:08:08.940488: step 5140, loss = 1.07 (414.1 examples/sec; 0.309 sec/batch)
2016-03-12 14:08:12.128661: step 5150, loss = 1.15 (374.6 examples/sec; 0.342 sec/batch)
2016-03-12 14:08:15.321906: step 5160, loss = 1.05 (388.2 examples/sec; 0.330 sec/batch)
2016-03-12 14:08:18.465653: step 5170, loss = 1.05 (394.7 examples/sec; 0.324 sec/batch)
2016-03-12 14:08:21.695237: step 5180, loss = 1.17 (405.3 examples/sec; 0.316 sec/batch)
2016-03-12 14:08:24.817479: step 5190, loss = 1.37 (404.4 examples/sec; 0.317 sec/batch)
2016-03-12 14:08:28.045635: step 5200, loss = 1.06 (411.1 examples/sec; 0.311 sec/batch)
2016-03-12 14:08:31.625961: step 5210, loss = 0.98 (393.0 examples/sec; 0.326 sec/batch)
2016-03-12 14:08:34.803401: step 5220, loss = 1.16 (404.8 examples/sec; 0.316 sec/batch)
2016-03-12 14:08:38.064956: step 5230, loss = 1.22 (386.5 examples/sec; 0.331 sec/batch)
2016-03-12 14:08:42.036235: step 5240, loss = 0.93 (309.9 examples/sec; 0.413 sec/batch)
2016-03-12 14:08:45.755935: step 5250, loss = 1.09 (332.2 examples/sec; 0.385 sec/batch)
2016-03-12 14:08:48.961625: step 5260, loss = 1.24 (413.0 examples/sec; 0.310 sec/batch)
2016-03-12 14:08:52.615963: step 5270, loss = 1.15 (318.3 examples/sec; 0.402 sec/batch)
2016-03-12 14:08:56.692628: step 5280, loss = 1.28 (331.9 examples/sec; 0.386 sec/batch)
2016-03-12 14:09:00.013660: step 5290, loss = 1.18 (429.9 examples/sec; 0.298 sec/batch)
2016-03-12 14:09:03.394530: step 5300, loss = 0.97 (307.2 examples/sec; 0.417 sec/batch)
2016-03-12 14:09:07.474397: step 5310, loss = 1.28 (250.3 examples/sec; 0.511 sec/batch)
2016-03-12 14:09:11.289658: step 5320, loss = 1.03 (307.2 examples/sec; 0.417 sec/batch)
2016-03-12 14:09:14.478318: step 5330, loss = 0.94 (399.5 examples/sec; 0.320 sec/batch)
2016-03-12 14:09:17.677547: step 5340, loss = 1.04 (382.3 examples/sec; 0.335 sec/batch)
2016-03-12 14:09:20.832572: step 5350, loss = 1.08 (433.3 examples/sec; 0.295 sec/batch)
2016-03-12 14:09:24.024293: step 5360, loss = 1.07 (398.9 examples/sec; 0.321 sec/batch)
2016-03-12 14:09:27.197269: step 5370, loss = 1.06 (416.3 examples/sec; 0.307 sec/batch)
2016-03-12 14:09:30.623629: step 5380, loss = 1.04 (324.2 examples/sec; 0.395 sec/batch)
2016-03-12 14:09:34.399873: step 5390, loss = 1.22 (241.2 examples/sec; 0.531 sec/batch)
2016-03-12 14:09:38.530928: step 5400, loss = 1.13 (270.8 examples/sec; 0.473 sec/batch)
2016-03-12 14:09:42.934882: step 5410, loss = 0.90 (268.9 examples/sec; 0.476 sec/batch)
2016-03-12 14:09:47.006334: step 5420, loss = 0.95 (347.2 examples/sec; 0.369 sec/batch)
2016-03-12 14:09:50.212403: step 5430, loss = 1.19 (441.3 examples/sec; 0.290 sec/batch)
2016-03-12 14:09:53.401629: step 5440, loss = 1.11 (417.6 examples/sec; 0.307 sec/batch)
2016-03-12 14:09:56.592893: step 5450, loss = 1.09 (413.3 examples/sec; 0.310 sec/batch)
2016-03-12 14:09:59.731023: step 5460, loss = 1.14 (392.4 examples/sec; 0.326 sec/batch)
2016-03-12 14:10:02.961094: step 5470, loss = 1.07 (413.1 examples/sec; 0.310 sec/batch)
2016-03-12 14:10:06.190783: step 5480, loss = 1.17 (380.0 examples/sec; 0.337 sec/batch)
2016-03-12 14:10:09.358733: step 5490, loss = 1.06 (390.4 examples/sec; 0.328 sec/batch)
2016-03-12 14:10:12.525256: step 5500, loss = 1.03 (423.3 examples/sec; 0.302 sec/batch)
2016-03-12 14:10:16.103487: step 5510, loss = 0.98 (418.0 examples/sec; 0.306 sec/batch)
2016-03-12 14:10:19.302679: step 5520, loss = 1.08 (395.4 examples/sec; 0.324 sec/batch)
2016-03-12 14:10:22.487560: step 5530, loss = 1.16 (423.4 examples/sec; 0.302 sec/batch)
2016-03-12 14:10:25.632955: step 5540, loss = 1.20 (416.8 examples/sec; 0.307 sec/batch)
2016-03-12 14:10:28.837518: step 5550, loss = 1.03 (409.2 examples/sec; 0.313 sec/batch)
2016-03-12 14:10:32.006405: step 5560, loss = 1.06 (399.6 examples/sec; 0.320 sec/batch)
2016-03-12 14:10:35.165191: step 5570, loss = 0.95 (442.7 examples/sec; 0.289 sec/batch)
2016-03-12 14:10:38.384486: step 5580, loss = 1.17 (439.7 examples/sec; 0.291 sec/batch)
2016-03-12 14:10:41.575061: step 5590, loss = 1.10 (368.8 examples/sec; 0.347 sec/batch)
2016-03-12 14:10:44.729184: step 5600, loss = 0.89 (423.4 examples/sec; 0.302 sec/batch)
2016-03-12 14:10:48.343008: step 5610, loss = 1.09 (416.0 examples/sec; 0.308 sec/batch)
2016-03-12 14:10:51.464913: step 5620, loss = 1.18 (388.3 examples/sec; 0.330 sec/batch)
2016-03-12 14:10:54.620341: step 5630, loss = 1.05 (421.4 examples/sec; 0.304 sec/batch)
2016-03-12 14:10:57.837872: step 5640, loss = 1.17 (419.3 examples/sec; 0.305 sec/batch)
2016-03-12 14:11:01.090029: step 5650, loss = 1.13 (411.3 examples/sec; 0.311 sec/batch)
2016-03-12 14:11:04.395310: step 5660, loss = 1.05 (399.1 examples/sec; 0.321 sec/batch)
2016-03-12 14:11:07.784719: step 5670, loss = 1.05 (366.3 examples/sec; 0.349 sec/batch)
2016-03-12 14:11:11.604344: step 5680, loss = 0.99 (380.1 examples/sec; 0.337 sec/batch)
2016-03-12 14:11:15.243982: step 5690, loss = 1.10 (332.7 examples/sec; 0.385 sec/batch)
2016-03-12 14:11:19.011393: step 5700, loss = 1.14 (388.5 examples/sec; 0.330 sec/batch)
2016-03-12 14:11:22.897067: step 5710, loss = 1.10 (389.7 examples/sec; 0.328 sec/batch)
2016-03-12 14:11:26.257571: step 5720, loss = 1.10 (392.0 examples/sec; 0.327 sec/batch)
2016-03-12 14:11:29.648071: step 5730, loss = 1.05 (378.9 examples/sec; 0.338 sec/batch)
2016-03-12 14:11:33.027954: step 5740, loss = 1.25 (349.1 examples/sec; 0.367 sec/batch)
2016-03-12 14:11:36.389940: step 5750, loss = 0.88 (388.9 examples/sec; 0.329 sec/batch)
2016-03-12 14:11:39.740017: step 5760, loss = 1.15 (386.6 examples/sec; 0.331 sec/batch)
2016-03-12 14:11:43.111565: step 5770, loss = 0.99 (384.3 examples/sec; 0.333 sec/batch)
2016-03-12 14:11:46.472003: step 5780, loss = 1.01 (387.7 examples/sec; 0.330 sec/batch)
2016-03-12 14:11:49.950465: step 5790, loss = 0.89 (363.1 examples/sec; 0.352 sec/batch)
2016-03-12 14:11:53.508106: step 5800, loss = 1.23 (348.4 examples/sec; 0.367 sec/batch)
2016-03-12 14:11:57.342506: step 5810, loss = 1.13 (407.0 examples/sec; 0.314 sec/batch)
2016-03-12 14:12:01.130117: step 5820, loss = 1.17 (409.0 examples/sec; 0.313 sec/batch)
2016-03-12 14:12:04.352433: step 5830, loss = 1.05 (399.8 examples/sec; 0.320 sec/batch)
2016-03-12 14:12:07.511581: step 5840, loss = 1.05 (435.9 examples/sec; 0.294 sec/batch)
2016-03-12 14:12:10.730863: step 5850, loss = 1.02 (412.3 examples/sec; 0.310 sec/batch)
2016-03-12 14:12:13.959593: step 5860, loss = 1.10 (355.5 examples/sec; 0.360 sec/batch)
2016-03-12 14:12:17.241367: step 5870, loss = 1.10 (441.1 examples/sec; 0.290 sec/batch)
2016-03-12 14:12:20.945846: step 5880, loss = 0.96 (339.6 examples/sec; 0.377 sec/batch)
2016-03-12 14:12:24.060260: step 5890, loss = 1.14 (398.4 examples/sec; 0.321 sec/batch)
2016-03-12 14:12:27.196011: step 5900, loss = 1.02 (425.1 examples/sec; 0.301 sec/batch)
2016-03-12 14:12:30.779291: step 5910, loss = 1.06 (420.0 examples/sec; 0.305 sec/batch)
2016-03-12 14:12:33.974906: step 5920, loss = 1.10 (403.1 examples/sec; 0.318 sec/batch)
2016-03-12 14:12:37.188986: step 5930, loss = 1.03 (381.9 examples/sec; 0.335 sec/batch)
2016-03-12 14:12:40.549341: step 5940, loss = 0.97 (399.6 examples/sec; 0.320 sec/batch)
2016-03-12 14:12:43.776408: step 5950, loss = 1.10 (386.8 examples/sec; 0.331 sec/batch)
2016-03-12 14:12:46.910332: step 5960, loss = 1.15 (383.1 examples/sec; 0.334 sec/batch)
2016-03-12 14:12:50.070772: step 5970, loss = 0.97 (436.3 examples/sec; 0.293 sec/batch)
2016-03-12 14:12:53.283043: step 5980, loss = 0.91 (400.2 examples/sec; 0.320 sec/batch)
2016-03-12 14:12:57.333740: step 5990, loss = 1.07 (389.0 examples/sec; 0.329 sec/batch)
2016-03-12 14:13:00.647640: step 6000, loss = 0.97 (371.8 examples/sec; 0.344 sec/batch)
2016-03-12 14:13:04.837750: step 6010, loss = 1.17 (304.9 examples/sec; 0.420 sec/batch)
2016-03-12 14:13:08.816024: step 6020, loss = 1.07 (336.6 examples/sec; 0.380 sec/batch)
2016-03-12 14:13:12.670182: step 6030, loss = 1.00 (260.2 examples/sec; 0.492 sec/batch)
2016-03-12 14:13:16.635798: step 6040, loss = 0.98 (295.0 examples/sec; 0.434 sec/batch)
2016-03-12 14:13:20.590717: step 6050, loss = 1.05 (436.9 examples/sec; 0.293 sec/batch)
2016-03-12 14:13:24.389427: step 6060, loss = 1.12 (303.6 examples/sec; 0.422 sec/batch)
2016-03-12 14:13:27.919392: step 6070, loss = 1.12 (403.5 examples/sec; 0.317 sec/batch)
2016-03-12 14:13:31.241030: step 6080, loss = 1.19 (374.7 examples/sec; 0.342 sec/batch)
2016-03-12 14:13:36.332882: step 6090, loss = 1.19 (342.0 examples/sec; 0.374 sec/batch)
2016-03-12 14:13:39.823059: step 6100, loss = 0.99 (363.2 examples/sec; 0.352 sec/batch)
2016-03-12 14:13:44.250455: step 6110, loss = 1.10 (352.2 examples/sec; 0.363 sec/batch)
2016-03-12 14:13:47.462684: step 6120, loss = 0.93 (429.3 examples/sec; 0.298 sec/batch)
2016-03-12 14:13:50.718381: step 6130, loss = 1.24 (405.0 examples/sec; 0.316 sec/batch)
2016-03-12 14:13:53.870993: step 6140, loss = 0.91 (382.4 examples/sec; 0.335 sec/batch)
2016-03-12 14:13:56.999939: step 6150, loss = 1.09 (415.8 examples/sec; 0.308 sec/batch)
2016-03-12 14:14:00.147406: step 6160, loss = 1.03 (408.5 examples/sec; 0.313 sec/batch)
2016-03-12 14:14:03.290188: step 6170, loss = 1.04 (417.4 examples/sec; 0.307 sec/batch)
2016-03-12 14:14:06.453738: step 6180, loss = 0.88 (422.1 examples/sec; 0.303 sec/batch)
2016-03-12 14:14:09.616241: step 6190, loss = 0.98 (393.4 examples/sec; 0.325 sec/batch)
2016-03-12 14:14:12.722891: step 6200, loss = 1.06 (413.3 examples/sec; 0.310 sec/batch)
2016-03-12 14:14:16.362099: step 6210, loss = 1.01 (433.5 examples/sec; 0.295 sec/batch)
2016-03-12 14:14:19.472411: step 6220, loss = 1.02 (436.2 examples/sec; 0.293 sec/batch)
2016-03-12 14:14:22.663895: step 6230, loss = 1.07 (416.9 examples/sec; 0.307 sec/batch)
2016-03-12 14:14:26.117081: step 6240, loss = 1.17 (386.0 examples/sec; 0.332 sec/batch)
2016-03-12 14:14:29.231502: step 6250, loss = 0.90 (411.4 examples/sec; 0.311 sec/batch)
2016-03-12 14:14:32.351659: step 6260, loss = 0.97 (409.6 examples/sec; 0.312 sec/batch)
2016-03-12 14:14:35.495727: step 6270, loss = 0.98 (401.1 examples/sec; 0.319 sec/batch)
2016-03-12 14:14:38.677070: step 6280, loss = 1.17 (390.6 examples/sec; 0.328 sec/batch)
2016-03-12 14:14:41.822706: step 6290, loss = 0.97 (436.9 examples/sec; 0.293 sec/batch)
2016-03-12 14:14:44.955144: step 6300, loss = 0.91 (442.9 examples/sec; 0.289 sec/batch)
2016-03-12 14:14:48.526671: step 6310, loss = 0.89 (356.7 examples/sec; 0.359 sec/batch)
2016-03-12 14:14:51.623469: step 6320, loss = 1.15 (395.3 examples/sec; 0.324 sec/batch)
2016-03-12 14:14:54.781165: step 6330, loss = 1.25 (387.0 examples/sec; 0.331 sec/batch)
2016-03-12 14:14:57.893973: step 6340, loss = 1.13 (426.3 examples/sec; 0.300 sec/batch)
2016-03-12 14:15:01.030509: step 6350, loss = 0.92 (411.7 examples/sec; 0.311 sec/batch)
2016-03-12 14:15:04.212174: step 6360, loss = 0.96 (414.8 examples/sec; 0.309 sec/batch)
2016-03-12 14:15:07.610402: step 6370, loss = 1.07 (397.9 examples/sec; 0.322 sec/batch)
E tensorflow/stream_executor/cuda/cuda_driver.cc:1088] could not wait stream on event: CUDA_ERROR_MISALIGNED_ADDRESS
I tensorflow/stream_executor/stream.cc:3187] stream 0x2f7bdd0 did not memcpy device-to-host; source: 0x7012acd00
E tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_MISALIGNED_ADDRESS
E tensorflow/stream_executor/cuda/cuda_driver.cc:1197] failed to enqueue async memcpy from device to host: CUDA_ERROR_MISALIGNED_ADDRESS; host dst: 0x7fb6276cff80; GPU src: 0x7011c2300; size: 1=0x1
I tensorflow/stream_executor/stream.cc:826] stream 0x2f7bdd0 did not wait for stream: 0x2f7a8b0
F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:193] Unexpected Event status: 1
I tensorflow/stream_executor/stream.cc:3187] stream 0x2f7bdd0 did not memcpy device-to-host; source: 0x7011e2d00
Aborted (core dumped)
"
1475,Udacity Example 1_notmnist failing to download,"### Environment info

Operating System: Mac OS X
### Steps to reproduce
1. `virtualenv venv`
2. `source venv/bin/activate`
3. `pip install ipython`
4. `pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.7.1-cp27-none-any.whl`
5. `git clone https://github.com/tensorflow/tensorflow.git`
6. `cd tensorflow/tensorflow/examples/udacity`
7. `ipython notebook`
8. In the browser (Chrome 48.0.2564.116) load `1_notmnist.ipynb`
9. Run cell 1 (imports)
10. Run cell 2

Cell 2 fails with `Exception: Failed to verify notMNIST_large.tar.gz. Can you get to it with a browser?`. Checked url (`http://yaroslavvb.com/upload/notMNIST/notMNIST_large.tar.gz`) with browser, downloads fine.

Checked working directory, found 422 byte `notMNIST_large.tar.gz`. Opened and found this;

```
<!DOCTYPE HTML PUBLIC ""-//IETF//DTD HTML 2.0//EN"">
<html><head>
<title>403 Forbidden</title>
</head><body>
<h1>Forbidden</h1>
<p>You don't have permission to access /upload/notMNIST/notMNIST_large.tar.gz
on this server.</p>
<p>Additionally, a 403 Forbidden
error was encountered while trying to use an ErrorDocument to handle the request.</p>
<hr>
<address>Apache Server at yaroslavvb.com Port 80</address>
</body></html>
```

Small thing, and easy to work around, but would improve our students' experience if they didn't have to debug this.
"
1474,python3.4 doesn't works well?,"GitHub issues are for bugs / installation problems / feature requests.  
For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
### Environment info

Operating System:

ubuntu 14.04

envy@ub1404:~/os_pri/github/tensorflow$ python3
Python 3.4.3 (default, Oct 14 2015, 20:28:29) 
[GCC 4.8.4] on linux

If installed from sources, provide the commit hash:
envy@ub1404:~/os_pri/github/tensorflow$ git log
commit 639d48f1f225e5dba865a5eeef513dbe88d6470e
Merge: aea1bf5 993c77d
Author: Manjunath Kudlur keveman@gmail.com
Date:   Mon Feb 22 21:28:30 2016 -0800

```
Merge pull request #1207 from dongjoon-hyun/avoid_copying_invalid_large_constant_data.
```
### Steps to reproduce

1.
import tensorflow
### Logs or other output that would be helpful

(If logs are large, please upload as attachment).
envy@ub1404:~/os_pri/github/tensorflow-deepq$ PYTHONPATH=~/os_pri/github/tensorflow-deepq:$PYTHONPATH python3 
Python 3.4.3 (default, Oct 14 2015, 20:28:29) 
[GCC 4.8.4] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.

> > > import tensorflow
> > > Traceback (most recent call last):
> > >   File ""<stdin>"", line 1, in <module>
> > >   File ""/home/envy/os_pri/github/tensorflow/_python3_build/tensorflow/__init__.py"", line 23, in <module>
> > >     from tensorflow.python import *
> > >   File ""/home/envy/os_pri/github/tensorflow/_python3_build/tensorflow/python/__init__.py"", line 49, in <module>
> > >     from tensorflow import contrib
> > >   File ""/home/envy/os_pri/github/tensorflow/_python3_build/tensorflow/contrib/__init__.py"", line 23, in <module>
> > >     from tensorflow.contrib import layers
> > >   File ""/home/envy/os_pri/github/tensorflow/_python3_build/tensorflow/contrib/layers/__init__.py"", line 67, in <module>
> > >     from tensorflow.contrib.layers.python.framework.tensor_util import *
> > >   File ""/home/envy/os_pri/github/tensorflow/_python3_build/tensorflow/contrib/layers/python/framework/tensor_util.py"", line 21, in <module>
> > >     from tensorflow.python.framework.ops import Tensor
> > >   File ""/home/envy/os_pri/github/tensorflow/_python3_build/tensorflow/python/framework/ops.py"", line 39, in <module>
> > >     from tensorflow.python.framework import versions
> > >   File ""/home/envy/os_pri/github/tensorflow/_python3_build/tensorflow/python/framework/versions.py"", line 22, in <module>
> > >     from tensorflow.python import pywrap_tensorflow
> > >   File ""/home/envy/os_pri/github/tensorflow/_python3_build/tensorflow/python/pywrap_tensorflow.py"", line 28, in <module>
> > >     _pywrap_tensorflow = swig_import_helper()
> > >   File ""/home/envy/os_pri/github/tensorflow/_python3_build/tensorflow/python/pywrap_tensorflow.py"", line 24, in swig_import_helper
> > >     _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
> > >   File ""/usr/lib/python3.4/imp.py"", line 243, in load_module
> > >     return load_dynamic(name, filename, file)
> > > ImportError: /home/envy/os_pri/github/tensorflow/_python3_build/tensorflow/python/_pywrap_tensorflow.so: undefined symbol: PyClass_Type
"
1473,Tensorflow GPU version support with skflow,"![error](https://cloud.githubusercontent.com/assets/10511526/13720964/02b25a4c-e83e-11e5-9a03-f4250d7c8ab2.png)
![version](https://cloud.githubusercontent.com/assets/10511526/13720966/0807d9b8-e83e-11e5-83c8-26699934e698.png)

I have been trying to execute text_classification.py example of skflow but it is generating an error. Please see the attached document for an error as well as the version of tensorflow and the skflow.

**the error is tensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'train/update_words/words_embeddings/Assign_1': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/GPU:0'**
"
1471,Android Demo Building Issue,"GitHub issues are for bugs / installation problems / feature requests.  
For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
### Environment info

Operating System:
Ubuntu 14.04

If installed from binary pip package, provide:
1. Which pip package you installed.
2. The output from python -c ""import tensorflow; print(tensorflow.**version**)"".

If installed from sources, provide the commit hash:
### Steps to reproduce
1. I have followed step by step the Android Demo installation
### What have you tried?
1. changing the ndk version
### Logs or other output that would be helpful

ERROR: /home/islamoc/tensorflow/tensorflow/examples/android/BUILD:65:1: Processing resources failed: resources_processor failed: error executing command bazel-out/host/bin/external/bazel_tools/tools/android/resources_processor --buildToolsVersion 23.0.1 --aapt bazel-out/host/bin/external/androidsdk/aapt_binary --annotationJar ... (remaining 14 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
Error: bazel-out/host/bin/external/androidsdk/aapt_binary: line 3: /root/.cache/bazel/_bazel_root/a492118e4b53c9510c58595e273faae6/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary.runfiles/external/androidsdk/build-tools/23.0.1/aapt: No such file or directory
Error: bazel-out/host/bin/external/androidsdk/aapt_binary: line 3: /root/.cache/bazel/_bazel_root/a492118e4b53c9510c58595e273faae6/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary.runfiles/external/androidsdk/build-tools/23.0.1/aapt: No such file or directory
Error: bazel-out/host/bin/external/androidsdk/aapt_binary: line 3: /root/.cache/bazel/_bazel_root/a492118e4b53c9510c58595e273faae6/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary.runfiles/external/androidsdk/build-tools/23.0.1/aapt: No such file or directory
Error: bazel-out/host/bin/external/androidsdk/aapt_binary: line 3: /root/.cache/bazel/_bazel_root/a492118e4b53c9510c58595e273faae6/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary.runfiles/external/androidsdk/build-tools/23.0.1/aapt: No such file or directory
Mar 12, 2016 4:13:36 AM com.google.devtools.build.android.AndroidResourceProcessingAction main
SEVERE: Error during merging resources
Error: Failed to run command:
        bazel-out/host/bin/external/androidsdk/aapt_binary s -i /tmp/android_resources_tmp4328338078871950922/tmp-deduplicated/tensorflow/examples/android/res/drawable-hdpi/ic_launcher.png -o /tmp/android_resources_tmp4328338078871950922/merged_resources/drawable-hdpi-v4/ic_launcher.png
Error Code:
        1
Output:
        bazel-out/host/bin/external/androidsdk/aapt_binary: line 3: /root/.cache/bazel/_bazel_root/a492118e4b53c9510c58595e273faae6/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary.runfiles/external/androidsdk/build-tools/23.0.1/aapt: No such file or directory

```
    at com.android.ide.common.res2.MergeWriter.end(MergeWriter.java:54)
    at com.android.ide.common.res2.MergedResourceWriter.end(MergedResourceWriter.java:113)
    at com.android.ide.common.res2.DataMerger.mergeData(DataMerger.java:291)
    at com.android.ide.common.res2.ResourceMerger.mergeData(ResourceMerger.java:48)
    at com.google.devtools.build.android.AndroidResourceProcessor.mergeData(AndroidResourceProcessor.java:390)
    at com.google.devtools.build.android.AndroidResourceProcessingAction.main(AndroidResourceProcessingAction.java:321)
```

Caused by: com.android.ide.common.internal.LoggedErrorException: Failed to run command:
        bazel-out/host/bin/external/androidsdk/aapt_binary s -i /tmp/android_resources_tmp4328338078871950922/tmp-deduplicated/tensorflow/examples/android/res/drawable-hdpi/ic_launcher.png -o /tmp/android_resources_tmp4328338078871950922/merged_resources/drawable-hdpi-v4/ic_launcher.png
Error Code:
        1
Output:
        bazel-out/host/bin/external/androidsdk/aapt_binary: line 3: /root/.cache/bazel/_bazel_root/a492118e4b53c9510c58595e273faae6/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary.runfiles/external/androidsdk/build-tools/23.0.1/aapt: No such file or directory

```
    at com.android.ide.common.internal.CommandLineRunner.runCmdLine(CommandLineRunner.java:123)
    at com.android.ide.common.internal.CommandLineRunner.runCmdLine(CommandLineRunner.java:96)
    at com.android.ide.common.internal.AaptCruncher.crunchPng(AaptCruncher.java:58)
    at com.android.ide.common.res2.MergedResourceWriter$1.call(MergedResourceWriter.java:188)
    at com.android.ide.common.res2.MergedResourceWriter$1.call(MergedResourceWriter.java:139)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```

Target //tensorflow/examples/android:tensorflow_demo failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 369.392s, Critical Path: 319.96s
"
1470,Error when run optimizer.compute_gradients,"I'm trying to do training with multiple threads

I tried to run optimizer.minimize() in each threads but it didn't work because threads overwrited each other's weight updates. So I'm trying to stack computed gradients in each threads and apply mean or sum of them in another thread.
So I made a code like

w1 = tf.Variable(tf.random_normal([shape], stddev=0.01))
b1 = tf.Variable(tf.constant(0.1, shape=[filters]))
rmsprop = tf.train.RMSPropOptimizer(lr,decay,0.0,eps)
grads = rmsprop.compute_gradients(cost,[w1,b1])
apply_grads = rmsprop.apply_gradients(grads)

If i run apply_grads, it works fine. but when i run grads to get gradients, error occurs like below.
`[(<tf.Tensor ~~ shape=~~ dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7f6da5fd7350>)] of [(<tf.Tensor ~~ shape=~~ dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7f6da5fd7350>)]  has invalid type <type ""list"">, must be a string or Tensor. (Can not convert a list into a Tensor or Operation.)`

How can I get gradients for optimizer?

Or is there a efficient way to do training with multiple threads?
"
1468,Android Camera Demo: Android NDK r11 can't be compiled by bazel after modified the WORKSPACE,"GitHub issues are for bugs / installation problems / feature requests.  
For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
### Environment info

Operating System:
OS X EI Capitan version 10.11.3

If installed from sources, provide the commit hash:
13ea3ca91ba5aecab6f21acc14b9cb6a9afa8630
### Steps to reproduce
1. Build TensorFlow without uncommenting the Android SDK and NDK parts successfully.
2. We can 

```
import tensorflow 
```

in python environment after step 1 and following installation steps.
4. Download SDK and NDK from what https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/README.md suggested.
3. Uncomment the Android SDK and NDK repository in WORKSPACE and change the path.
4. Rebuild using 

```
$ bazel build -c opt //tensorflow/tools/pip_package:build_pip_package
```

the console returns the error,

``` shell
ERROR: .../tensorflow/WORKSPACE:10:1: no such package '@androidndk//': Could not read RELEASE.TXT in Android NDK: /private/var/tmp/.../49fff0428.../external/androidndk/ndk/RELEASE.TXT (No such file or directory) and referenced by '//external:android/crosstool'.
```
### What have you tried?
1. Commenting NDK repository and leave the SDK in TensorFlow WORKSPACE, build using 

```
$ bazel build -c opt //tensorflow/tools/pip_package:build_pip_package
```

passed and without error.

2.Finding that Downloaded NDK and NDK in /private/var/tmp/.../49fff0428.../external/androidndk/ndk are the same. And there are no RELEASE.TXT in in directory.
### Logs or other output that would be helpful

(If logs are large, please upload as attachment).

``` shell
ERROR: .../tensorflow/WORKSPACE:10:1: no such package '@androidndk//': Could not read RELEASE.TXT in Android NDK: /private/var/tmp/.../49fff0428.../external/androidndk/ndk/RELEASE.TXT (No such file or directory) and referenced by '//external:android/crosstool'.
```

Is this error due to the codes or my building processes?
"
1467,Allow suppressing log output,"Two recent requests recently to suppress logging on console [1](http://stackoverflow.com/questions/35911252/disable-tensorflow-debugging-information), [2](http://stackoverflow.com/questions/35869137/avoid-tensorflow-print-on-standard-error)

Since TensorFlow uses Google logging library standard flags should work, ie ""export GLOG_logtostderr=0"" should turn off logging to console. However that doesn't work, I suspect these flags are explicitly overriden in code somewhere
"
1464,Feature request: make TensorBoard side bar resizable?,"There's often a bunch of unused space on the right side of the TensorBoard interface, and if we have long directory names on the left (for example listing parameters during validation), they end up getting wrapped to multiple lines. Right now it's possible to adjust the sidebar width with Chrome's developer tools, but it seems natural for it to be resizable in the first place.
"
1463,Can I specify other gpu device not first one  in notebook?,"Can I specify other gpu device not first one  in notebook?thanks!
"
1461,Typo in How-To?,"I'm reading [this](https://www.tensorflow.org/versions/r0.7/how_tos/adding_an_op/index.html#implement-the-kernel-for-the-op) how-to page, and read the line:

`auto output = output_tensor->template flat<int32>();`

Is the `template flat` a typo, or some feature of C++ that I'm not familiar with? 

This line appears a few times in that section of the how-to.
"
1459,partial_run segfault,"Running partial_run twice results in a segfault. By running twice I mean: setting up placeholder, ops and completing one run through the graph and then trying to do another run.
### Environment info

Operating System: Linux. 
Version: 0.7.1
Hash: 028d0b4
### Steps to reproduce

```
import tensorflow as tf
from tensorflow.python import array_ops
from tensorflow.python import math_ops
from tensorflow.python.framework import dtypes
sess = tf.Session()

a = array_ops.placeholder(dtypes.float32, shape=[])
b = array_ops.placeholder(dtypes.float32, shape=[])
r1 = math_ops.add(a, b)

h = sess.partial_run_setup([r1], [a, b])
res = sess.partial_run(h, r1, feed_dict={a: 1, b: 2})

h = sess.partial_run_setup([r1], [a, b])
res = sess.partial_run(h, r1, feed_dict={a: 1, b: 2})

> Segmentation fault (core dumped)
```
### What have you tried?
1. Changing execution target to CPU instead of GPU (same problem)
2. Reusing the same handle for the second run results in a `StatusNotOK` error because the input has already been fed
### Logs or other output that would be helpful

Core dump is 1.7 GB. Can attach if useful
"
1458,CIFAR-10 Model has (almost) pointless ops in distorted_inputs,"While preparing an intro talk I noticed something odd happening in cifar10_input.py. 

In the following piece of code:
https://github.com/tensorflow/tensorflow/blob/a641ff6aca15b8e6e5ecf96a7a9188c0c1246c8e/tensorflow/models/image/cifar10/cifar10_input.py#L176

The sequence of ops is: random brightness, which adds a random constant to each pixel, then random contrast, which multiplies the difference from the mean by a random factor ... and then per_image_whitening, which transforms the image to zero mean and unit variance, effectively canceling the previous random operations out, if you don't cross the threshold the per_image_whitening has for low-variance images. That doesn't make sense to me - I would think you want either the data augmentation ops for brightness/contrast OR the whitening to deal with different lighting conditions, but not both. Am I mistaken there?
"
1454,Python multiple inequalities do not work as expected,"```
import tensorflow as tf

with tf.device(""/cpu:0""):
    lower_bound = tf.constant(
        [[ 0.,   0.,   0. ],
         [ 0.,   0.,   0. ],
         [ 0.5,  0.5,  0.5],
         [ 0.5,  0.5,  0.5],])
    upper_bound = tf.constant(
        [[ 0.5,  0.5,  0.5],
         [ 0.5,  0.5,  0.5],
         [ 1.,   1.,   1. ],
         [ 1.,   1.,   1. ],])
    test = tf.constant(
        [[ 0.30000001,  0.2,  0.82],
         [ 0.30000001,  0.2,  0.82],
         [ 0.30000001,  0.2,  0.82],
         [ 0.30000001,  0.2,  0.82],])
    multiple_inequality = lower_bound <= test < upper_bound
    lower_bound_filter = lower_bound <= test
    upper_bound_filter = test < upper_bound
    expected_filter = tf.logical_and(lower_bound_filter, upper_bound_filter)

with tf.Session() as sess:
    sess.run(tf.initialize_all_variables())
    print('Multiple inequality:', multiple_inequality.eval())
    print('Upper bound inequality:', upper_bound_filter.eval())
    print('Lower bound inequality:', lower_bound_filter.eval())
    print('Expected:', expected_filter.eval())
```

Output:

> ('Multiple inequality:', array([[ True,  True, False],
>        [ True,  True, False],
>        [ True,  True,  True],
>        [ True,  True,  True]], dtype=bool))
> ('Upper bound inequality:', array([[ True,  True, False],
>        [ True,  True, False],
>        [ True,  True,  True],
>        [ True,  True,  True]], dtype=bool))
> ('Lower bound inequality:', array([[ True,  True,  True],
>        [ True,  True,  True],
>        [False, False,  True],
>        [False, False,  True]], dtype=bool))
> ('Expected:', array([[ True,  True, False],
>        [ True,  True, False],
>        [False, False,  True],
>        [False, False,  True]], dtype=bool))

It seems like `multiple_inequality = lower_bound <= test < upper_bound` becomes `multiple_inequality = test < upper_bound`
"
1453,Multidimensional RNN,"Hi all,

This is a feature request for [Multidimensional LSTM](http://people.idsia.ch/~juergen/nips2009.pdf). Is there any plan to support this in tensorflow?

I might be able to help with implementation if there is such a plan.

Cheers,
"
1452,install error,"when i am installing from sources, there is an error like this:

ERROR: tensorflow/tensorflow/core/BUILD:98:1: //tensorflow/core:worker_service_proto_cc: no such attribute 'use_grpc_plugin' in 'cc_library' rule.
ERROR: tensorflow/tensorflow/core/BUILD:121:1: //tensorflow/core:master_service_proto_cc: no such attribute 'use_grpc_plugin' in 'cc_library' rule.
ERROR: tensorflow/tensorflow/cc/BUILD:61:1: Target '//tensorflow/core:tensorflow' contains an error and its package is in error and referenced by '//tensorflow/cc:tutorials_example_trainer'.
ERROR: Loading failed; build aborted.
"
1451,Is there a way to cache intermediate Tensor result?,"I attempt to implement naive RBM/DBN through tensorflow's python API.
The functionality is easy to implement if you don't care about computational performance.

For now, I've tried many alternatives solutions and find it difficult to implement with high performance since there's no way to cache the intermediate tensor result.

While lacking of the ability to cache the intermediate result, there will be too much memory migrate from GPU to RAM or vice versa (say I'm using GPU) if I want to do something through intermediate result.

Here's part of my code to illustrate my meaning.

``` python
    import tensorflow as tf
    alpha = .01
    mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)
    trX, trY, teX, teY = mnist.train.images, mnist.train.labels,\
        mnist.test.images, mnist.test.labels
    W = tf.Variable(tf.random_normal([784, 100]), name=""weights"")
    hb = tf.Variable(tf.zeros([100]), name=""hbias"")
    vb = tf.Variable(tf.zeros([784]), name=""vbias"")
    v0 = tf.placeholder(""float"", [None, 784])

    h0 = tf.nn.sigmoid(tf.matmul(v0, W) + hb)
    h0 = sample_prob(h0)
    v1 = tf.nn.sigmoid(tf.matmul(h0, tf.transpose(W)) + vb)
    v1 = sample_prob(v1)
    h1 = tf.nn.sigmoid(tf.matmul(v1, W) + hb)

    pg = tf.matmul(tf.transpose(v0), h0)
    ng = tf.matmul(tf.transpose(v1), h1)

    dW = (pg - ng) / tf.to_float(tf.shape(v0)[0])
    dhb = tf.reduce_mean(h0 - h1, 0)
    dvb = tf.reduce_mean(v0 - v1, 0)

    update_W_op = W.assign_add(alpha * dW)
    update_hb_op = hb.assign_add(alpha * dhb)
    update_vb_op = vb.assign_add(alpha * dvb)

    with tf.Session() as sess:
        sess.run(tf.initialize_all_variables())
        for _ in range(10):
            for start, end in zip(
                    range(0, 50000, 100), range(100, 50000, 100)):
                X = trX[start:end]
                # sess.run([dW, dhb, dvb, h1], feed_dict={v0: X})
                # sess.run(h1, feed_dict={v0: X})
                # sess.run(dW)
                # sess.run(dhb)
                # sess.run(dvb)
                # _W = sess.run(update_W_op)
                # _hb = sess.run(update_hb_op)
                # _vb = sess.run(update_vb_op)
                [_W, _hb, _vb] = sess.run([update_W_op, update_hb_op, update_vb_op], feed_dict={v0: X})
```

This is the most efficient way I could think out. But, there's no way to optimize the progress by calculating `[_W, _hb, _vb]` with the aid of intermediate result `pg, ng, h0, h1, v0, v1`. It's obvious that here comes so much duplicated  but useless extra computation.
If feed `v0` with `X` once, the follower `Tensor` need the former `Tensor`.
`h0` needs `v0`, **so needs `X`**
`v1` needs  `h0`, **so needs `X`**
`h1` needs `v1`, **so needs `X`**
......
`update_W_op` needs `dW`, **so needs `X`**
`update_hb_op` needs  `dhb`, **so needs `X`**
`update_vb_op` needs `dvb`, **so needs `X`**

Since there's no way to cache the intermediate result, you can only get one `Tensor` result for one `computation`. There's no way to avoid the duplication computation and `X` memory migration cost.

I was wondering if there's a property of `Tensor` say `cached = False`, we could set it to `True` manually. And when intermediate cached result is enough to go on, we need not to feed data in, and just using intermediate cache to compute. If you want to flush the cache, just feed in the new data into the placeholder. But for now, if the graph contains **placeholder** in the path, the data must be feed in specifically.

For example, if the cache is enabled.

``` python
h0.cached = True
h1.cached = True
v0.cached = True
v1.cached = True
sess.run(h1, feed_dict={v0: X})
sess.run([update_W_op, update_hb_op, update_vb_op])  # using cache h0,h1,v0,v1 is enough to compute the three update without X fed in.
```
"
1450, GPU sync failed,"```
`I` tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 970, pci bus id: 0000:01:00.0)
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 256B
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 512B
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 1.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 2.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 4.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 8.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 16.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 32.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 64.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 128.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 256.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 512.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 1.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 2.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 4.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 8.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 16.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 32.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 64.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 128.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 256.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:107] Allocating 3.51GiB bytes.
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:118] GPU 0 memory begins at 0x503dc0000 extends to 0x5e45e3000
E tensorflow/stream_executor/cuda/cuda_driver.cc:1099] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS :: No stack trace available
F tensorflow/core/common_runtime/gpu/gpu_util.cc:370] GPU sync failed
```

hi!
what's wrong with this, how i can solve this.I'm using cuda7.5 cudnn7.0 and all they are ok running on CPU. but when run GPU ,it occur wrong.
And I can local the operation which can't run on GPU

```
with tf.device(""/cpu:0""):
 optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)
```

when i remvoe ""tf.device(""/cpu:0""),it ocure the bug reported above
"
1449,Need better doc about the difference between name scope and variable scope in tensorflow and What's the context manager?(Soved)),"Hi,
Although stack overflow have a question about this, http://stackoverflow.com/questions/34215746/difference-between-variable-scope-and-name-scope-in-tensorflow. But right now the answers are not so clear and the doc also not tell the difference too, but I believe understanding the difference between name scope and variable scope and context manager is beneficial to programming for newbie using tensorflow.
"
1448,bazel build failed ,"Hi,
I'm running on ubuntu 14.04 with cuda 7.0, cudnn4 in system. 
I followed the instruction on  https://www.tensorflow.org/versions/r0.7/get_started/os_setup.html#optional-install-cuda-gpus-on-linux. But when build with bazel (0.2.0), I always get:

$ bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer
INFO: Found 1 target...
ERROR: missing input file '//third_party/gpus/cuda:include/thrust/detail/config/exec_check_disable.h':/home/wenchen/frameworks/tensorflow/third_party/gpus/cuda/include/thrust/detail/config/exec_check_disable.h (Permission denied).
ERROR: /home/wenchen/frameworks/tensorflow/tensorflow/stream_executor/BUILD:5:1: //tensorflow/stream_executor:stream_executor: missing input file '//third_party/gpus/cuda:include/thrust/detail/config/exec_check_disable.h'.
Target //tensorflow/cc:tutorials_example_trainer failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /home/wenchen/frameworks/tensorflow/tensorflow/stream_executor/BUILD:5:1 1 input file(s) do not exist.
INFO: Elapsed time: 0.814s, Critical Path: 0.00s

please help. 

Thanks,
Wenchen
"
1446,Tensorflow is using the wrong cuda toolkit,"I recently installed tensorflow on an Ubuntu 12.04 machine that has both cuda 6.5 and 7.0 installed. However, when I import tensorflow I get the following error:

```
    import tensorflow as tf
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/__init__.py"", line 23, in <module>
    from tensorflow.python import *
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow import contrib
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/__init__.py"", line 23, in <module>
    from tensorflow.contrib import layers
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/layers/__init__.py"", line 68, in <module>
    from tensorflow.contrib.layers.python.layers import *
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/layers/python/layers/__init__.py"", line 22, in <module>
    from tensorflow.contrib.layers.python.layers.initializers import *
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/layers/python/layers/initializers.py"", line 24, in <module>
    from tensorflow.python.ops import random_ops
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/random_ops.py"", line 23, in <module>
    from tensorflow.python.framework import ops
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py"", line 39, in <module>
    from tensorflow.python.framework import versions
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/versions.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
  File ""/usr/lib/python3.4/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
ImportError: libcudart.so.7.5: cannot open shared object file: No such file or directory
```

Am I doing something wrong? Can I use cuda 6.5 or 7.0?

Edit: installation was via pip(3).
"
1444,SummaryWriter writes constants with the graph definition,"It looks like SummaryWriter includes the values of constants when writing the graph definition. I'm currently loading word embeddings using python and using the numpy array to initialize a variable. This results in the ~300M of embeddings to be dumped to disk during summarization.  I understand why a constant would be part of the model graph if the goal was to save and restore the graph, but I don't believe this is the goal of the SummaryWriter. Is it possible to strip these out to save disk space?

Here's a trivial example. If the declaration of `random_b` is removed, the resulting summary is ~4.6K, but with it, it's about 7.6M.

```
import numpy as np
import os
import tensorflow as tf

session = tf.Session()

logdir = ""/tmp/tflogs""

random_a = tf.Variable(tf.random_normal([1000000]))
random_b = tf.Variable(np.random.rand(1000000))
tf.histogram_summary(""random_var"", random_a)
os.makedirs(logdir)
writer = tf.train.SummaryWriter(logdir, session.graph_def)
init = tf.initialize_all_variables()
merged_summary_op = tf.merge_all_summaries()
session.run(init)

summary = session.run(merged_summary_op)
writer.add_summary(summary, 0)
```
"
1443,Hard-coded gcc path,"When compiling with GPU support, the compiler path is set in the file
//third_party/gpus/crosstool/crosstool_wrapper_is_not_gcc on line 47 to 50:

```
CPU_COMPILER = ('/usr/bin/gcc')
etc
```

which can result in using an old version on e.g. clusters where several compilers might be installed under a path such as
`/appl/gcc/5.3.0/bin/gcc`

It would be nice to be able to configure the compiler path.

Edit: This overlaps with the hard coded paths in other parts of Bazel, see for example bazelbuild/bazel#760
"
1442,Same model behaves different when restored in Linux vs OSX,"Using the recent TensorFlow pip install 0.7.0 I have encountered some frustrating behavior. It might be something I'm doing wrong on my end, but I've ruled out everything I can.

Here's what's happening:
I've trained a model on a linux system using a GPU. I then scp the checkpoint file to my mac, and restored the checkpoint file into exactly the same code-base. However, the results were much poorer. Thinking I might have a bug or a bad saved model, I went back to the linux and loaded the saved model there, reran the exact same evaluation procedure using identical data, code, etc. and the results were much better once again. I don't know anything about the purpose of the ckpt.meta file, but copying this into the same directory didn't help.

Does this sound familiar at all? Again, I can't rule out a bug on my end, but everything is identical as far as I can tell, apart from the running environment (Linux+GPU vs Mac). Any ideas?
"
1441,valueerror when install 0.7.1 in centos 7,"[martin@localhost ~]$ sudo pip install ./tensorflow-0.7.1-cp27-none-linux_x86_64.whl
[sudo] password for martin: 
Exception:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/pip/basecommand.py"", line 139, in main
    status = self.run(options, args)
  File ""/usr/lib/python2.7/site-packages/pip/commands/install.py"", line 235, in run
    InstallRequirement.from_line(name, None))
  File ""/usr/lib/python2.7/site-packages/pip/req.py"", line 118, in from_line
    return cls(req, comes_from, url=url)
  File ""/usr/lib/python2.7/site-packages/pip/req.py"", line 43, in **init**
    req = pkg_resources.Requirement.parse(req)
  File ""/usr/lib/python2.7/site-packages/pkg_resources.py"", line 2914, in parse
    reqs = list(parse_requirements(s))
  File ""/usr/lib/python2.7/site-packages/pkg_resources.py"", line 2839, in parse_requirements
    line, p, specs = scan_list(VERSION,LINE_END,line,p,(1,2),""version spec"")
  File ""/usr/lib/python2.7/site-packages/pkg_resources.py"", line 2807, in scan_list
    raise ValueError(""Expected ""+item_name+"" in"",line,""at"",line[p:])
ValueError: ('Expected version spec in', './tensorflow-0.7.1-cp27-none-linux_x86_64.whl', 'at', '/tensorflow-0.7.1-cp27-none-linux_x86_64.whl')

Storing complete log in /root/.pip/pip.log
[martin@localhost ~]$ 
[martin@localhost ~]$ python -V
Python 2.7.5
[martin@localhost ~]$ uname -a
Linux localhost.localdomain 3.10.0-327.10.1.el7.x86_64 #1 SMP Tue Feb 16 17:03:50 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
"
1440,Cuda 3.0 not working,"When installing from pip tensorflow GPU version (Linux), minimum cuda version is 3.5.
Install instructions have been changed to:
""TensorFlow GPU support requires having a GPU card with NVidia Compute Capability >= 3.0""
However, gpu_device.cc is still requiring cuda 3.5:

```
// ""configure"" uses the specific name to substitute the following string.
// If you change it, make sure you modify ""configure"" as well.
std::vector<CudaVersion> supported_cuda_compute_capabilities = {
    CudaVersion(""3.5""), CudaVersion(""5.2"")};

}  // namespace
```
"
1439,why tensorflow run slow in loop,"I just run many times of sess.run in loop like this:

```
input1 = tf.placeholder(tf.float32)
input2 = tf.placeholder(tf.float32)
sess = tf.Session()
for image_num in range(128):
  image_result_table=np.zeros((96,32),dtype=np.float32)
  for sub_num in range(96):
      for centroids_num in range(32):
       vector_a = local3_value[image_num,sub_num*4:(sub_num+1)*4]
       vector_b = centroids[sub_num,centroids_num]
       mat_result = tf.mul(input1, input2)
           reduce_sum = tf.reduce_sum(mat_result)
       result =sess.run(reduce_sum,feed_dict={input1:vector_a, input2:vector_b})
           print(result)
```

at first ,it run fast. howerver, it become slower and slower. It takes almost an hour to get the whole result. 
"
1438,Tensorflow missing symbol in compilation,"Running on ArchLinux and compiling with the 2 suggested changes in [Issue 1346](https://github.com/tensorflow/tensorflow/issues/1346) -  GCC4.9 and -D_FORCE_INLINES.

_Env_
GCC 4.9.3
Bazel version states ""from-head"" - build taken from https://github.com/bazelbuild/bazel/archive/0.2.0.tar.gz
Building with Cuda and cudnn

I'm running into an issue where the linking step for the tutorials_example_trainer, where the CheckOpMessageBuilder::NewString() symbol is missing:

```
ERROR: /mnt/work/work/tensorflow/tensorflow/cc/BUILD:61:1: Linking of rule '//tensorflow/cc:tutorials_example_trainer' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/local_linux-opt/bin/tensorflow/cc/tutorials_example_trainer ... (remaining 627 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
bazel-out/local_linux-opt/bin/tensorflow/core/kernels/libtranspose_functor_gpu.lo(transpose_functor_gpu.cu.o): In function `std::string* tensorflow::internal::MakeCheckOpString<int, int>(int const&, int const&, char const*)':
tmpxft_000060ce_00000000-7_transpose_functor_gpu.cu.compute_35.cudafe1.cpp:(.text._ZN10tensorflow8internal17MakeCheckOpStringIiiEEPSsRKT_RKT0_PKc[_ZN10tensorflow8internal17MakeCheckOpStringIiiEEPSsRKT_RKT0_PKc]+0x40): undefined reference to `tensorflow::internal::CheckOpMessageBuilder::NewString()'
bazel-out/local_linux-opt/bin/tensorflow/core/kernels/libtranspose_functor_gpu.lo(transpose_functor_gpu.cu.o): In function `std::string* tensorflow::internal::MakeCheckOpString<int, unsigned long>(int const&, unsigned long const&, char const*)':
tmpxft_000060ce_00000000-7_transpose_functor_gpu.cu.compute_35.cudafe1.cpp:(.text._ZN10tensorflow8internal17MakeCheckOpStringIimEEPSsRKT_RKT0_PKc[_ZN10tensorflow8internal17MakeCheckOpStringIimEEPSsRKT_RKT0_PKc]+0x41): undefined reference to `tensorflow::internal::CheckOpMessageBuilder::NewString()'
bazel-out/local_linux-opt/bin/tensorflow/core/kernels/libtranspose_functor_gpu.lo(transpose_functor_gpu.cu.o): In function `std::string* tensorflow::internal::MakeCheckOpString<tensorflow::DataType, tensorflow::DataType>(tensorflow::DataType const&, tensorflow::DataType const&, char const*)':
tmpxft_000060ce_00000000-7_transpose_functor_gpu.cu.compute_35.cudafe1.cpp:(.text._ZN10tensorflow8internal17MakeCheckOpStringINS_8DataTypeES2_EEPSsRKT_RKT0_PKc[_ZN10tensorflow8internal17MakeCheckOpStringINS_8DataTypeES2_EEPSsRKT_RKT0_PKc]+0x40): undefined reference to `tensorflow::internal::CheckOpMessageBuilder::NewString()'
bazel-out/local_linux-opt/bin/tensorflow/core/kernels/libtranspose_functor_gpu.lo(transpose_functor_gpu.cu.o): In function `std::string* tensorflow::internal::MakeCheckOpString<long long, int>(long long const&, int const&, char const*)':
tmpxft_000060ce_00000000-7_transpose_functor_gpu.cu.compute_35.cudafe1.cpp:(.text._ZN10tensorflow8internal17MakeCheckOpStringIxiEEPSsRKT_RKT0_PKc[_ZN10tensorflow8internal17MakeCheckOpStringIxiEEPSsRKT_RKT0_PKc]+0x40): undefined reference to `tensorflow::internal::CheckOpMessageBuilder::NewString()'
```

I have followed the tutorial thus far and am unsure why
"
1437,Tensorflow missing symbol in compilation,"Running on ArchLinux and compiling with the 2 suggested changes in [Issue 1346](https://github.com/tensorflow/tensorflow/issues/1346) -  GCC4.9 and -D_FORCE_INLINES.

_Env_
GCC 4.9.3
Bazel version states ""from-head"" - build taken from https://github.com/bazelbuild/bazel/archive/0.2.0.tar.gz
Building with Cuda and cudnn

I'm running into an issue where the linking step for the tutorials_example_trainer, where the CheckOpMessageBuilder::NewString() symbol is missing:

```
ERROR: /mnt/work/work/tensorflow/tensorflow/cc/BUILD:61:1: Linking of rule '//tensorflow/cc:tutorials_example_trainer' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/local_linux-opt/bin/tensorflow/cc/tutorials_example_trainer ... (remaining 627 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
bazel-out/local_linux-opt/bin/tensorflow/core/kernels/libtranspose_functor_gpu.lo(transpose_functor_gpu.cu.o): In function `std::string* tensorflow::internal::MakeCheckOpString<int, int>(int const&, int const&, char const*)':
tmpxft_000060ce_00000000-7_transpose_functor_gpu.cu.compute_35.cudafe1.cpp:(.text._ZN10tensorflow8internal17MakeCheckOpStringIiiEEPSsRKT_RKT0_PKc[_ZN10tensorflow8internal17MakeCheckOpStringIiiEEPSsRKT_RKT0_PKc]+0x40): undefined reference to `tensorflow::internal::CheckOpMessageBuilder::NewString()'
bazel-out/local_linux-opt/bin/tensorflow/core/kernels/libtranspose_functor_gpu.lo(transpose_functor_gpu.cu.o): In function `std::string* tensorflow::internal::MakeCheckOpString<int, unsigned long>(int const&, unsigned long const&, char const*)':
tmpxft_000060ce_00000000-7_transpose_functor_gpu.cu.compute_35.cudafe1.cpp:(.text._ZN10tensorflow8internal17MakeCheckOpStringIimEEPSsRKT_RKT0_PKc[_ZN10tensorflow8internal17MakeCheckOpStringIimEEPSsRKT_RKT0_PKc]+0x41): undefined reference to `tensorflow::internal::CheckOpMessageBuilder::NewString()'
bazel-out/local_linux-opt/bin/tensorflow/core/kernels/libtranspose_functor_gpu.lo(transpose_functor_gpu.cu.o): In function `std::string* tensorflow::internal::MakeCheckOpString<tensorflow::DataType, tensorflow::DataType>(tensorflow::DataType const&, tensorflow::DataType const&, char const*)':
tmpxft_000060ce_00000000-7_transpose_functor_gpu.cu.compute_35.cudafe1.cpp:(.text._ZN10tensorflow8internal17MakeCheckOpStringINS_8DataTypeES2_EEPSsRKT_RKT0_PKc[_ZN10tensorflow8internal17MakeCheckOpStringINS_8DataTypeES2_EEPSsRKT_RKT0_PKc]+0x40): undefined reference to `tensorflow::internal::CheckOpMessageBuilder::NewString()'
bazel-out/local_linux-opt/bin/tensorflow/core/kernels/libtranspose_functor_gpu.lo(transpose_functor_gpu.cu.o): In function `std::string* tensorflow::internal::MakeCheckOpString<long long, int>(long long const&, int const&, char const*)':
tmpxft_000060ce_00000000-7_transpose_functor_gpu.cu.compute_35.cudafe1.cpp:(.text._ZN10tensorflow8internal17MakeCheckOpStringIxiEEPSsRKT_RKT0_PKc[_ZN10tensorflow8internal17MakeCheckOpStringIxiEEPSsRKT_RKT0_PKc]+0x40): undefined reference to `tensorflow::internal::CheckOpMessageBuilder::NewString()'
```

I have followed the tutorial thus far and am unsure why
"
1434,tf.nn.xw_plus_b not in API docs ,"The method tensorflow.nn.xw_plus_b() exists but is not documented in the [online API documentation](https://www.tensorflow.org/versions/master/api_docs/python/nn.html). 
"
1433,reduce mean for gpu is not registered,"The reduce_mean [GPU implementations](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/reduction_ops_gpu.cu.cc#L44-L60) are not registered [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/reduction_ops_mean.cc), so that the reduce_mean is still placed on CPU.

Wondering if it's a negligence or intended. If it's intended, please close this.
"
1432,seq2seq tutorial example not working for python 3.4,"### Environment info

Operating System:
Ubuntu 14.04
1. Which pip package you installed.
   sudo pip3 install --upgrade /tmp/pip/tensorflow-*.whl
2. The output from python -c ""import tensorflow; print(tensorflow.**version**)"".
   0.7.1
   (Built from commit 263d00d271077 with `TF_UNOFFICIAL_SETTING=1 ./configure` )
### Steps to reproduce
1. Apply https://github.com/tensorflow/tensorflow/commit/cdd0e2b7c542a59322c054aa1a52b2753c1cf69e 
2. Run `python3 ../tensorflow/tensorflow/models/rnn/translate/translate.py --data_dir .`
3. Fail with trying to decode training data as ascii (see log at the end)
### What have you tried?

Modified https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/default/_gfile.py#L63 to `self._fp = open(name, mode, encoding=""latin-1"")`

This seems to fix this issue for python 3.4.
### Logs or other output that would be helpful

Creating vocabulary ./vocab40000.fr from data ./giga-fren.release2.fr
Traceback (most recent call last):
  File ""../../../tensorflow/tensorflow/models/rnn/translate/translate.py"", line 276, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/platform/default/_app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""../../../tensorflow/tensorflow/models/rnn/translate/translate.py"", line 273, in main
    train()
  File ""../../../tensorflow/tensorflow/models/rnn/translate/translate.py"", line 137, in train
    FLAGS.data_dir, FLAGS.en_vocab_size, FLAGS.fr_vocab_size)
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/models/rnn/translate/data_utils.py"", line 268, in prepare_wmt_data
    create_vocabulary(fr_vocab_path, train_path + "".fr"", fr_vocabulary_size)
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/models/rnn/translate/data_utils.py"", line 136, in create_vocabulary
    for line in f:
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/platform/default/_gfile.py"", line 176, in __next__
    return next(self._fp)
  File ""/usr/lib/python3.4/encodings/ascii.py"", line 26, in decode
    return codecs.ascii_decode(input, self.errors)[0]
"
1431,"Install Error：SSLError: hostname 'storage.googleapis.com' doesn't match either of 'accounts.google.com', '*.partner.android.com'","When I use command :
""sudo pip install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl"" 
to install Iget this error, please tell me how can I solve?
"
1430,node placement algorithm in the source code?,"I have read the whitepaper and felt very excited about the **node placement** algorithm decribed in **section 3.2**, so I tried to figure out the details from the source code. However, I cannot find the files that implement this algorithm. 
I have read the tensorflow/core/common_runtime/**Simple_placer**.h (& Simple_placer.cc) and it seems it's too ""simple"" to cover the algorithm descibed in the whitepaper. Other files like tensorflow/core/graph/**Graph_partition**.h (& Graph_partition.cc) didn't have that algorithm neither. And the costmodel implemented in tensorflow/core/graph/**Costmodel**.h (& Costmodel.cc), I cannot find their caller in other place.
Someone please help me?
"
1427,Mandlebrot example fails on docker image,"I'm running through the [Mandlebrot Set tutorial](https://www.tensorflow.org/versions/r0.7/tutorials/mandelbrot/index.html) and hit a snag.

I am running tensor flow from Docker -- the `b.gcr.io/tensorflow/tensorflow` image

When I try to initialize all variables, I get this error:

```
W tensorflow/core/kernels/cast_op.cc:125] Unimplemented: Cast complex64 to float is not supported
E tensorflow/core/common_runtime/executor.cc:275] Executor failed to create kernel. Unimplemented: Cast complex64 to float is not supported
```

Full error after copy/pasting tutorial code to python file:

```
W tensor [[Node: zeros_like/Cast = Cast[DstT=DT_FLOAT, SrcT=DT_COMPLEX64, _device=""/job:localhost/replica:0/task:0/cpu:0""](zeros_like/ZerosLike)]]
Traceback (most recent call last):xecutor.cc:275] Executor failed to create kernel.
  File ""mb.py"", line 36, in <module>oat is not supported
    tf.i [[Node: zeros_like/Cast = Cast[DstT=DT_FLOAT, SrcT=DT_COMPLEX64, _device=""/job:localhost/replica:0/task:0/cpu:0""](zeros_like/ZerosLike)]]n/framework/ops.py"",
Caused by op u'zeros_like/Cast', defined at:
  File ""mb.py"", line 34, in <module> feed_dict, self.graph, session)
    ns = tf.Variable(tf.zeros_like(xs, tf.float32))orflow/python/framework/ops.py"",
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py"", line 632, in zeros_liken, feed_dict)
    ret = gen_math_ops.cast(ret, dtype)ackages/tensorflow/python/client/session.py""
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py"", line 193, in castNone, fetches, feed_dict)
    return _op_def_lib.apply_op(""Cast"", x=x, DstT=DstT, name=name)lient/session.py""
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py"", line 655, in apply_op
    op_def=op_def)/lib/python2.7/dist-packages/tensorflow/python/client/session.py""
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2040, in create_op
    original_op=self._default_original_op, op_def=op_def)/python/client/session.py""
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1087, in __init__
    self._traceback = _extract_stack()mplementedError: Cast complex64 to float is n
ot supported
```

Is the docker image out of date?
"
1426,Is CentOS 6.5 supported?,"I have try each method in installation guide to install GPU-tensorflow in my centos 6.5 server?
However, the error below is still exists.
`ImportError: /lib64/libc.so.6: version GLIBC_2.14 not found (required by /usr/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so)`
Is there anyone who've successfully installed tensor flow in centos 6.5?
"
1423,Need force_gpu_if_available for tests,"For GPU tests, we have the `force_gpu` and `use_gpu` flags. `force_gpu=True` is good for manual testing during development, but if an op cannot be placed on a gpu because there isn't a GPU or because TensorFlow was built without GPU support, it will make the test fail. 

For tests, we really need a way to say ""fail if a GPU is available, but it's not used"". Could be as simple as providing the equivalent of `force_gpu=tf.compiled_with_GPU_support() and tf.get_GPU_devices()`. Both functions don't exist, but really should.
"
1422,Tensorflow just not working,"I stopped using tensorflow for a while. Now i want to get back to it, but after installing, when I try to import tensorflow module on python i get this message:

```
Error importing tensorflow.  Unless you are using bazel,
you should not try to import tensorflow from its source directory;
please exit the tensorflow source tree, and relaunch your python interpreter
from there
```

Have tried uninstalling all previous existing versions and exiting any tensorflow folder, i'm running python from root folder, but I still get the error. Also if I download tensorflow from github and run from tensorflow version 6.0 I get same error
Any idea?
Thanks
"
1421,TensorBoard showing nothing! ,"I am trying to run the example in my Linux machine (Ubuntu 14.04): 
**tensorflow/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py**

First, I type ""`python mnist_with_summaries.py`"" in command line. Everything looks good.
Then following the tutorial, I type ""`tensorboard --logdir =/tmp/mnist_logs`"". Following message appeared.

WARNING:tensorflow:Unable to read TensorBoard tag
Starting TensorBoard  on port 6006
(You can navigate to http://0.0.0.0:6006)

Then I visit ""http://0.0.0.0:6006"" on Chrome,  nothing appears except:

"" **No scalar summary tags were found.
Maybe data hasn't loaded yet, or maybe you need to add some tf.scalar_summary ops to your graph, and serialize them using the tf.training.summary_io.SummaryWriter**. ""

In the terminal, following messages showed up:

""127.0.0.1 - - [07/Mar/2016 11:45:06] ""GET / HTTP/1.1"" 200 -
127.0.0.1 - - [07/Mar/2016 11:45:07] ""GET /lib/css/global.css HTTP/1.1"" 200 -
127.0.0.1 - - [07/Mar/2016 11:45:07] ""GET /external/lodash/lodash.min.js HTTP/1.1"" 200 -
127.0.0.1 - - [07/Mar/2016 11:45:07] ""GET /external/plottable/plottable.min.js HTTP/1.1"" 200 -
127.0.0.1 - - [07/Mar/2016 11:45:07] ""GET /external/d3/d3.min.js HTTP/1.1"" 200 -
127.0.0.1 - - [07/Mar/2016 11:45:07] ""GET /external/plottable/plottable.css HTTP/1.1"" 200 -
127.0.0.1 - - [07/Mar/2016 11:45:07] ""GET /external/graphlib/dist/graphlib.core.min.js HTTP/1.1"" 200 -
127.0.0.1 - - [07/Mar/2016 11:45:07] ""GET /external/polymer/polymer.html HTTP/1.1"" 200 -
127.0.0.1 - - [07/Mar/2016 11:45:07] ""GET /external/webcomponentsjs/webcomponents-lite.min.js HTTP/1.1"" 200 -
127.0.0.1 - - [07/Mar/2016 11:45:07] ""GET /external/iron-ajax/iron-ajax.html HTTP/1.1"" 200 -
127.0.0.1 - - [07/Mar/2016 11:45:07] ""GET /external/dagre/dist/dagre.core.min.js HTTP/1.1
... ... ""

I checked **/tmp/mnist_log/**, there is an event file named ""**events.out.tfevents.1457371474.rdii-Alienware-X51**"". Why nothing shows up on my TensorBoard ?  Please help me! 
"
1420,Support for double precision complex numbers,"I'd like to use TensorFlow to write an application that would need `complex128` as a dtype.
But TensorFlow currently only supports `complex64`.
Are there any plans for supporting `complex128` as well in the future?

Seems that there is a TODO for this in  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/types.proto#L36.
"
1419,Dependency required for user Reader ops?,"When creating a Reader op inside core/user_ops that inherits from ReaderBase, Bazel responds with 

```
tensorflow/core/BUILD:325:1: undeclared inclusion(s) in rule '//tensorflow/core:user_ops_op_lib':
this rule is missing dependency declarations for the following files included by 'tensorflow/core/user_ops/user_reader_op.cc':
  'bazel-out/local_darwin-opt/genfiles/tensorflow/core/kernels/reader_base.pb.h'.
```

Is this because the user_ops_op_lib depends only on framework? user_reader_op.cc includes reader_base.h which includes reader_base.pb.h. What dependency should I add to make this work? I'm still figuring out Bazel atm. 

This is with latest source code. 
"
1418,Using 2 computers - Distributed Tensorflow,"Copied from: http://stackoverflow.com/questions/35824742/tensorflow-distributed-assing-devices

I recently installed the version of tensorflow for distributed computers. From the trend: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/distributed_runtime , i tried to implemented multiples gpus at multiples computers, and also looking at ""https://stackoverflow.com/questions/34439045/tensorflow-setup-for-distributed-computing"" found a white paper for some additional specifications. I can run the server and a worker on 2 different computers with 2 and 1 gpu, and using the session gprc, allocate and run the program on remote or local mode. i ran locally tensorflow in the remote computer with:

```
bazel-bin/tensorflow/core/distributed_runtime/rpc/grpc_tensorflow_server \
--cluster_spec='local|localhost:2500' --job_name=local --task_id=0 &
```

and for using on the server

```
bazel-bin/tensorflow/core/distributed_runtime/rpc/grpc_tensorflow_server \
--cluster_spec='worker|192.168.170.193:2500,prs|192.168.170.226:2500' --job_name=worker --task_id=0 \
--job_name=prs --task_id=0 &
```

However, when i try to specify the device for running on 2 computers at the same time the python show me the error:

`Could not satisfy explicit device specification '/job:worker/task:0'`

when i use

```
with tf.device(""/job:prs/task:0/device:gpu:0""):
  x = tf.placeholder(tf.float32, [None, 784], name='x-input')
  W = tf.Variable(tf.zeros([784, 10]), name='weights')
with tf.device(""/job:prs/task:0/device:gpu:1""):
  b = tf.Variable(tf.zeros([10], name='bias'))
# Use a name scope to organize nodes in the graph visualizer
with tf.device(""/job:worker/task:0/device:gpu:0""):
  with tf.name_scope('Wx_b'):
    y = tf.nn.softmax(tf.matmul(x, W) + b)
```

or even changing the name of job. So I am wondering if it is required to Add a New Device (https://stackoverflow.com/questions/35213172/add-a-new-device-in-tensorflow) or probably i am doing something wrong with the initialization of the cluster.

Best Regards
"
1415,pip install version 0.7.1 from source: No module named google.protobuf,"Hello,

I have an issue with Google Protobuf. Even after the install Python does not find the module ""google.protobuf"".
### Environment info

Operating System: Ubuntu 14.04
Python: 2.7.6
Bazel: 0.2.0
Cuda: 7.5
CudNN: 4

If installed from sources, provide the commit hash: https://github.com/tensorflow/tensorflow/commit/1e47e2f9bb7112f1a763e758a7bcdaff36edec5b
### Steps to reproduce
1. bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer
2. bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
3. bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
4. PYTHONUSERBASE=/usr/local/python pip install --user /tmp/tensorflow_pkg/tensorflow-0.7.1-py2-none-any.whl

Output for pip install:

```
Unpacking /tmp/tensorflow_pkg/tensorflow-0.7.1-py2-none-any.whl
Requirement already satisfied (use --upgrade to upgrade): six>=1.10.0 in /usr/local/python/lib/python2.7/site-packages (from tensorflow==0.7.1)
Downloading/unpacking protobuf==3.0.0b2 (from tensorflow==0.7.1)
  Downloading protobuf-3.0.0b2-py2.py3-none-any.whl (326kB): 326kB downloaded
Requirement already satisfied (use --upgrade to upgrade): wheel in /usr/local/python/lib/python2.7/site-packages (from tensorflow==0.7.1)
Requirement already satisfied (use --upgrade to upgrade): numpy>=1.8.2 in /usr/local/python/lib/python2.7/site-packages (from tensorflow==0.7.1)
Requirement already satisfied (use --upgrade to upgrade): setuptools in /usr/local/python/lib/python2.7/site-packages (from protobuf==3.0.0b2->tensorflow==0.7.1)
Installing collected packages: tensorflow, protobuf
Successfully installed tensorflow protobuf
Cleaning up...
```

Error in Python interpreter:

```
Python 2.7.6 (default, Jun 22 2015, 17:58:13) 
[GCC 4.8.2] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import google.protobuf
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ImportError: No module named google.protobuf
>>>
```

In the Python prompt, when I run `help('modules')`, I can see the module `tensorflow` but not the module `google`.

The first thing I tried is:
1. pip uninstall protobuf 
2. PYTHONUSERBASE=/usr/local/python pip install --user --upgrade /tmp/tensorflow_pkg/tensorflow-0.7.1-cp27-none-any.whl

Output:

```
Installing collected packages: tensorflow, protobuf
Successfully installed tensorflow protobuf
Cleaning up...
```

Same result, no Protobuf module found.

The second thing I tried is:
1. pip uninstall protobuf
2. pip uninstall tensorflow
3. PYTHONUSERBASE=/usr/local/python pip install --user /tmp/tensorflow_pkg/tensorflow-0.7.1-py2-none-any.whl

Output:

```
Installing collected packages: tensorflow, protobuf
Successfully installed tensorflow protobuf
Cleaning up...
```

Same result again, not Protobuf module found.

Any idea of why Protobuf is properly installed but not recognized by Python whereas Tensorflow is propertly recognized by Python?

Thanks for any help that you can provide.
"
1413,error when building from source,"bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

Warning: ignoring LD_PRELOAD in environment.
ERROR: /home/wenjian/pkgs/tensorflow/tensorflow/core/distributed_runtime/rpc/BUILD:87:1: no such package '@grpc//': Error cloning repository: https://boringssl.googlesource.com/boringssl: cannot open git-upload-pack caused by https://boringssl.googlesource.com/boringssl: cannot open git-upload-pack caused by https://boringssl.googlesource.com/boringssl: cannot open git-upload-pack and referenced by '//tensorflow/core/distributed_runtime/rpc:grpc_channel'.
ERROR: Loading failed; build aborted.
INFO: Elapsed time: 342.460s
"
1412,resize_image_with_crop_or_pad() on variable image sizes,"I'm setting up what (I think) should be a straightforward image loading pipeline:

``` python
decoded_image = tf.image.decode_jpeg(value)
resized_image = tf.image.resize_image_with_crop_or_pad(decoded_image, width, height)
```

However, I get an error `'image' must be fully defined.` - the image not being fully defined because the size of the JPEG is variable. When using the `tf.image.resize_images()` function, it seems that this constraint is not enforced. Is it necessary to have fixed image dimensions to use the `resize_image_with_crop_or_pad()` function? That would seem to somewhat defeat the point of an auto-crop/resizing function, no?
"
1411,"Mac OS;import error ""ImportError: cannot import name _message""","### Environment info

MAC OS
python:
Python 2.7.11 |Anaconda 2.5.0 (x86_64)| (default, Dec  6 2015, 18:57:58) 
[GCC 4.2.1 (Apple Inc. build 5577)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Anaconda is brought to you by Continuum Analytics.
Please check out: http://continuum.io/thanks and https://anaconda.org

install command:
pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.7.1-cp27-none-any.whl

error:

> > > import tensorflow as tf
> > > Traceback (most recent call last):
> > >   File ""<stdin>"", line 1, in <module>
> > >   File ""//anaconda/lib/python2.7/site-packages/tensorflow/**init**.py"", line 23, in <module>
> > >     from tensorflow.python import *
> > >   File ""//anaconda/lib/python2.7/site-packages/tensorflow/python/**init**.py"", line 41, in <module>
> > >     raise ImportError(msg)
> > > ImportError: Traceback (most recent call last):
> > >   File ""//anaconda/lib/python2.7/site-packages/tensorflow/python/**init**.py"", line 35, in <module>
> > >     from tensorflow.core.framework.graph_pb2 import *
> > >   File ""//anaconda/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>
> > >     from google.protobuf import descriptor as _descriptor
> > >   File ""//anaconda/lib/python2.7/site-packages/google/protobuf/descriptor.py"", line 46, in <module>
> > >     from google.protobuf.pyext import _message
> > > ImportError: cannot import name _message

Error importing tensorflow.  Unless you are using bazel,
you should not try to import tensorflow from its source directory;
please exit the tensorflow source tree, and relaunch your python interpreter
from there.

but I didn't start python at the tensorfliw source directory
$ pwd
/anaconda/bin

How to  solve the problem?
"
1410,"tensorflor cond(pred, fn1, fn2) evaluate fn1 and fn2 together regardless of pred","```
import tensorflow as tf
import numpy as np

isTrain = tf.placeholder(tf.bool)
user_input = tf.placeholder(tf.float32)

with tf.device('/cpu:0'):
    alpha = tf.Variable(tf.zeros([1]))
    beta = tf.Variable(tf.zeros([1]))

    c = tf.Variable(tf.zeros([1]))

    alpha_incre = alpha.assign(alpha + 1)
    beta_incre = beta.assign(beta + 1)

    def train():
        with tf.control_dependencies([ alpha_incre ]):
            return c.assign(user_input + user_input)

    def test():
        with tf.control_dependencies([ beta_incre ]):
            return c.assign(user_input)

    result = tf.cond(isTrain,
        train,
        test
    )

init = tf.initialize_all_variables()
sess = tf.Session()
sess.run(init)

aa = sess.run([result, alpha, beta], feed_dict={user_input:[2], isTrain: True })
print(""Train"", aa)
aa = sess.run([result, alpha, beta], feed_dict={user_input:[2], isTrain: False })
print(""Test"", aa)
```
### Environment info

ubuntu 14.04

If installed from binary pip package, provide:
tensorflow 0.7.1
### Steps to reproduce
1. run the code example, expecting to see  

output 
- iteratio 1: to see alpha = 1, beta = 1 
- iteratio 2: to see alpha = 2, beta = 2 

expected
- iteratio 1: to see alpha = 1, beta = 0 
- iteratio 2: to see alpha = 1, beta = 1 
### What have you tried?
1. Changing the code structure slightly will provide the expected output

```
import tensorflow as tf
import numpy as np

isTrain = tf.placeholder(tf.bool)
user_input = tf.placeholder(tf.float32)

with tf.device('/cpu:0'):
    alpha = tf.Variable(tf.zeros([1]))
    beta = tf.Variable(tf.zeros([1]))

    c = tf.Variable(tf.zeros([1]))

    alpha_incre = alpha.assign(alpha + 1)
    beta_incre = beta.assign(beta + 1)

    def train():
        with tf.control_dependencies([ alpha.assign(alpha + 1) ]):
            return c.assign(user_input + user_input)

    def test():
        with tf.control_dependencies([ beta.assign(beta + 1) ]):
            return c.assign(user_input)

    result = tf.cond(isTrain,
        train,
        test
    )

init = tf.initialize_all_variables()
sess = tf.Session()
sess.run(init)

aa = sess.run([result, alpha, beta], feed_dict={user_input:[2], isTrain: True })
print(""Train"", aa)
aa = sess.run([result, alpha, beta], feed_dict={user_input:[2], isTrain: False })
print(""Test"", aa)
```
"
1409,Image ops do not all accept NumPy arrays as arguments,"From [Stack Overflow](http://stackoverflow.com/q/35824798/3574081):

> When I run the line :
> 
> `flipped_images = tf.image.random_flip_left_right(images)`
> I get the following error :
> 
> `AttributeError: 'numpy.ndarray' object has no attribute 'get_shape'`
> My Tensor ""images"" is an ndarray (shape=[batch, im_size, im_size, channels]) of ""batch"" ndarrays (shape=[im_size, im_size, channels]).

On looking at the code, this happens because our shape checks expect arguments to already have been converted to a `tf.Tensor`. We should call `ops.convert_to_tensor()` on them before performing the check.
"
1408,Generalized matrix multiplication with semiring?,"Would it be possible to support (generalized) matrix multiplication with non-standard semirings? 

In matrix multiplication, we have `A times B = C`, where `C(i,j) = sum_k A(i,k) times B(k,j)`

Using a different semiring practically means redefining the `plus` and `times` operations. 

For example, the log semiring assumes that all numbers in the matrices are log numbers and redefines `plus` as `logplus` (aka [logaddexp](http://docs.scipy.org/doc/numpy/reference/generated/numpy.logaddexp.html#numpy.logaddexp) or [logsumexp](http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.misc.logsumexp.html)) and `times` as `plus`. This is useful for computing the log denominator in log-linear models, e.g., `log Z = log sum_y exp(f1(y)) times exp(f2(y))`.

The [max-plus semiring](https://en.wikipedia.org/wiki/Max-plus_algebra) (aka Viterbi or tropical semiring) redefines `plus` as `max` and `times` as `plus`. This is useful for finding the best path (assuming all matrix entries are log numbers).
"
1407,C++ documentation broken,"None of the links on this page work:

https://www.tensorflow.org/versions/r0.7/api_docs/cc/index.html
"
1406,[feature request] disable harmful broadcast,"Broadcast may lead to hidden bugs. For example, the following code,

```
xent = tf.nn. sigmoid_cross_entropy_with_logits(logits, y)
loss = tf.reduce_mean(xent)
```

When `logits` is of the shape `(200,1)` and the `y` is of the shape `(200,)`, the xent becomes `(200, 200)`, which is wrong. What's worse, the `reduce_mean` will hide this problem by reducing everything into a scalar.

Request,
1. more shape validation in `sigmoid_cross_entropy_with_logits`
2. option to totally disable broadcast
"
1405,Shouldn't tensorboard warn you that not all summaries have loaded yet?,"Hi,

since I started using Tensorboard, I noticed some odd behavior that sometimes I'd open it and it seemed like a lot of the summary values were missing, i.e, even though it's already at 10k steps, the summary only shows 2k steps or so. Today I actually finally noticed that it simply meant that it was just still being loaded. I refreshed the page after a minute or so and all data was there. My PC is not bad (i5-2500k) so this was quite surprising. Any reason why the summaries sometimes take so long to load? Also can we get some sort of warning that not all summaries have been completely loaded yet?
"
1402,"Error importing tensorflow.  Unless you are using bazel, you should not try to import tensorflow from its source directory","These are the sequence of steps i followed to install the distributed version of TF

> > git clone --recurse-submodules https://github.com/tensorflow/tensorflow
> > ./configure (default with GPU)
> > bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer
> > bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu (at this point saw the expected output and GPU being used)
> >  bazel build -c opt --config=cuda --define=use_fast_cpp_protos=true //tensorflow/tools/pip_package:build_pip_package
> > bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
> > sudo pip install /tmp/tensorflow_pkg/tensorflow-0.7.1-py2-none-any.whl 
> > Processing /tmp/tensorflow_pkg/tensorflow-0.7.1-py2-none-any.whl
> > Installing collected packages: tensorflow
> > pip show tensorflow
> > Metadata-Version: 2.0
> > Name: tensorflow
> > Version: 0.7.1
> > Summary: TensorFlow helps the tensors flow
> > Home-page: http://tensorflow.org/
> > Author: Google Inc.
> > Author-email: opensource@google.com
> > License: Apache 2.0
> > Location: /usr/local/lib/python2.7/dist-packages
> > Requires: six, protobuf, wheel, numpy
> > Classifiers:
> >   Development Status :: 4 - Beta
> >   Intended Audience :: Developers
> >   Intended Audience :: Education
> >   Intended Audience :: Science/Research
> >   License :: OSI Approved :: Apache Software License
> >   Programming Language :: Python :: 2.7
> >   Topic :: Scientific/Engineering :: Mathematics
> >   Topic :: Software Development :: Libraries :: Python Modules
> >   Topic :: Software Development :: Libraries
> > Entry-points:
> >   [console_scripts]
> >   tensorboard = tensorflow.tensorboard.tensorboard:main

Now for the error

> > python
> > Python 2.7.9 (default, Apr  2 2015, 15:33:21) 
> > [GCC 4.9.2] on linux2
> > Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
> > 
> > > import tensorflow
> > > Traceback (most recent call last):
> > >   File ""<stdin>"", line 1, in <module>
> > >   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/**init**.py"", line 23, in <module>
> > >     from tensorflow.python import *
> > >   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/**init**.py"", line 41, in <module>
> > >     raise ImportError(msg)
> > > ImportError: Traceback (most recent call last):
> > >   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/**init**.py"", line 35, in <module>
> > >     from tensorflow.core.framework.graph_pb2 import *
> > >   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>
> > >     from google.protobuf import descriptor as _descriptor
> > > ImportError: No module named protobuf

Error importing tensorflow.  Unless you are using bazel,
you should not try to import tensorflow from its source directory;
please exit the tensorflow source tree, and relaunch your python interpreter
from there.

Can someone help me understand what might cause this? Thank you.
"
1401,Error in string_input_producer,"### Environment info

Operating System: Ubuntu 15.04

If installed from sources, provide the commit hash: c1a40c7
### Steps to reproduce

Run the following commands:

```
import tensorflow as tf
sess = tf.InteractiveSession()
filenames = [""1"", ""2"", ""3""]
filename_queue = tf.train.string_input_producer(filenames)
test_value = tf.convert_to_tensor(filename_queue.size())
print(sess.run([test_value]))
```

For me, this last command returns `[0]`. If I'm understanding things right, I'm assuming it should return `[3]`. I've also tried to run the dequeue() op, which appears to hang and become unresponsive. 

It's also possible I'm just completely misunderstanding how this is supposed to work - in which case, please disregard!
"
1399,Error using tf.image.random._ : 'numpy.ndarray' object has no attribute 'get_shape',"**Intro**

I am using a modified version of the Tensorflow tutorial ""Deep MNIST for experts"" with the Python API for a medical images classification project using convolutionnal networks.

I want to artificially increase the size of my training set by applying random modifications on the images of my training set.

**Problem**

When I run the line : 

`flipped_images = tf.image.random_flip_left_right(images)`

I get de following error :

> AttributeError: 'numpy.ndarray' object has no attribute 'get_shape'

My Tensor ""images"" is an ndarray (shape=[batch, im_size, im_size, channels]) of ""batch"" ndarrays (shape=[im_size, im_size, channels]).

Just to check if my input data was packed in the right shape and type, I have tried to apply this simple function in the (not modified) tutorial ""Tensorflow Mechanics 101"" and I get the same error.

Finally, I still get the same error trying to use the following functions :
- tf.image.random_flip_up_down() 
- tf.image.random_brightness()
- tf.image.random_contrast()

**Questions**

As input data is usually carried in Tensorflow as ndarrays, I would like to know :   
1. Is it a bug of Tensorflow Python API or is it my ""fault"" because
   of the type/shape of my input data?
2. How could I get it to work and be able to apply    ""tf.image.random_flip_left_right"" to my training set?
"
1398,Udacity examples should be improved (/tensorflow/examples/udacity),"https://www.udacity.com/course/viewer#!/c-ud730 is a really cool course in the context of Machine Learning and TensorFlow. After completing Coursera Andrew Ng course, they give valuable reasoning why and how something should be done in TensorFlow.

But, there is an issue.

Theory part in videos and implementing it in TensorFlow do not click together. Forum is full of questions in the format ""how should it be done?"", ""am I doing it correctly?"".

Yes, I can spend 2 days on a one assignment, search internet and take parts from manuals that are on the tensorflow.org page, but still I am not convinced that I am doing it correctly. 

Please improve https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/udacity so that best practice, how previous task would have been correctly solved, is included in the next task. 
"
1397,Strange Multi-GPU Performance,"I am encountering a strange Multi-GPU performance issue. To validate I've run the following script [TensorFlow Examples: Multi-GPU basics](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/multigpu_basics.py).

The performance on a single GPU was 19 seconds while dual GPU computation took 28 seconds.

The same effect was reproducible on my system when I used multi-GPU for computing my own networks. You can see a short description and an ASCII illustration of my architecture here: [stackoverflow link](http://stackoverflow.com/questions/35822251/tensorflow-one-network-two-gpus).

The problem seems to get worse with bigger computations, since my own network is quite big and on a **single** GPU it is about **10 times** faster than on two GPUs :/
### Environment info
##### Operating System information:

`uname -or`:

> 3.10.0-327.10.1.el7.x86_64 GNU/Linux

`cat /etc/redhat-release`:

> CentOS Linux release 7.2.1511 (Core)
##### Hardware

Graphics card is a [NVIDIA Dual K80 Tesla](http://www.nvidia.com/object/tesla-k80.html). 
##### Software environment:

`pip2.7 freeze`:

> backports-abc==0.4
> backports.ssl-match-hostname==3.5.0.1
> certifi==2015.11.20.1
> cycler==0.10.0
> Cython==0.23.4
> enum34==1.1.2
> h5py==2.5.0
> libxml2-python==2.9.3
> Mako==1.0.1
> matplotlib==1.5.1
> mercurial==3.6.2
> nose==1.3.7
> numpy==1.10.4
> pkgconfig==1.1.0
> protobuf==3.0.0b2
> pyparsing==2.1.0
> python-dateutil==2.4.2
> pytz==2015.7
> singledispatch==3.4.0.3
> six==1.10.0
> tensorflow==0.7.1
> tornado==4.3
> virtualenv==13.1.2
> wheel==0.29.0

`python -c ""import tensorflow; print(tensorflow.__version__)`:

> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so.7.0 locally
> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so.4.0.7 locally
> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so.7.0 locally
> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally
> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so.7.0 locally
> 0.7.1

Built with **bazel** from sources, commit hash:

> commit eda89e930cfcbd992ecacafd40267d733e2153dc
### What have you tried?
1. different scripts and setups
### Logs or other output that would be helpful

Complete output of `multigpu_basics.py` with `tf.ConfigProto(log_device_placement=log_device_placement)` is attached.
[out.txt](https://github.com/tensorflow/tensorflow/files/160210/out.txt)
"
1396,Throw out exception to python when errors happen in c++ code,"Required information is after the description.

Hi tensorflow team,

I have enjoyed using tensorflow much. Thanks for the commitment to make this happen.

After writing many code based on it, I found it is very inconvenient that when the C++ code meets an error, it just breaks instead of passing the error to the python front end, so it is impossible to deal with the error gracefully.

For example, when training a network with a large learning rate with ReLU without BN, the activations could easily go out of range. As this is unavoidable since learning rate has to be tuned. It would be great to catch the out of range error in python, so things could be logged and shut down properly, instead of just break.

A log could be

```
W tensorflow/core/common_runtime/executor.cc:1102] 0x33ea350 Compute status: Out of range: Nan in summary histogram for: maxout-relu-cifar10/conv2/HistogramSummary_2
```

Hope this suggestion could be taken into consideration.
### Environment info

Operating System:

```
3.13.0-74-generic #118-Ubuntu SMP x86_64 x86_64 x86_64 GNU/Linux
```

If installed from binary pip package, provide:
1. Which pip package you installed.

```
sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl
```
1. The output from python -c ""import tensorflow; print(tensorflow.**version**)"".

```
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally
0.7.1
```
"
1392,Minor Error in Tutorial,"On [this tutorial page](https://www.tensorflow.org/versions/master/tutorials/mnist/download/index.html), it says that the MNIST data has been rescaled to [-.5, .5]:

""The image data is extracted into a 2d tensor of: [image index, pixel index] where each entry is the intensity value of a specific pixel in a specific image, rescaled from [0, 255] to [-0.5, 0.5].""

However, in [the code](https://github.com/tensorflow/tensorflow/blob/r0.7/tensorflow/examples/tutorials/mnist/input_data.py), the data is rescaled to [0,1] as per the comment on line 123 (and confirmed by loading a sample image and inspecting):

`# Convert from [0, 255] -> [0.0, 1.0].`

Just a minor error, but thought it was worth pointing out! I'm actually curious which is the better method of rescaling, but that's a topic for another discussion..
"
1391,install from pip package fails on Ubuntu 15.10,"### Environment info

Operating System: Ubuntu 15.10
1. pip package: tensorflow-0.7.1-cp27-none-linux_x86_64.whl
2. The screen output: 
   Downloading tensorflow-0.7.1-cp27-none-linux_x86_64.whl (13.8MB): 13.8MB downloaded
   Cleaning up...
   Exception:
   Traceback (most recent call last):
   File ""/usr/lib/python2.7/dist-packages/pip/basecommand.py"", line 122, in main
     status = self.run(options, args)
   File ""/usr/lib/python2.7/dist-packages/pip/commands/install.py"", line 304, in run
     requirement_set.prepare_files(finder, force_root_egg_info=self.bundle, bundle=self.bundle)
   File ""/usr/lib/python2.7/dist-packages/pip/req.py"", line 1271, in prepare_files
     req_to_install)
   File ""/usr/lib/python2.7/dist-packages/pip/req.py"", line 109, in **init**
     self.prereleases = any([is_prerelease(x[1]) and x[0] != ""!="" for x in self.req.specs])
   File ""/usr/lib/python2.7/dist-packages/pip/util.py"", line 739, in is_prerelease
     return any([any([y in set([""a"", ""b"", ""c"", ""rc"", ""dev""]) for y in x]) for x in parsed])
   TypeError: 'int' object is not iterable

Storing debug log for failure in /home/osboxes/.pip/pip.log
### Steps to reproduce
1. sudo apt-get install python-pip python-dev
   2.sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl
"
1390,Attempting to use uninitialized value lstm/LSTMCell/W_0,"### Environment info

Operating System: Rocks OS (Centos 6.5)

I installed from sources, and here is my version:
https://github.com/shiyemin/tensorflow/
Nothing changed but to make it compile successfully on our server.
### Steps to reproduce
1. add a LSTM layer between local3 and local4 in cifar10.py
   
   ```
   # lstm
   with tf.variable_scope('lstm') as scope:
     lstm_cell = rnn_cell.LSTMCell(512, input_size=384)
     feed_in = tf.split(0, 16, local3)
     outputs, states = rnn.rnn(lstm_cell, feed_in, scope=scope, dtype=tf.float32)
     lstm1 = tf.concat(0, outputs)
   
   # local4
   with tf.variable_scope('local4') as scope:
     weights = _variable_with_weight_decay('weights', shape=[512, 192],
                                           stddev=0.04, wd=0.004)
     biases = _variable_on_cpu('biases', [192], tf.constant_initializer(0.1))
     local4 = tf.nn.relu(tf.matmul(lstm1, weights) + biases, name=scope.name)
     _activation_summary(local4)
   ```
2. run cifar10_multi_gpu_train.py
3. Error: Attempting to use uninitialized value lstm/LSTMCell/W_0
### What have you tried?
1. Run this network in Test mode, and everything is fine. 
2. Run this network in Train mode, error ""Attempting to use uninitialized value lstm/LSTMCell/W_0"" occurs.
### Logs or other output that would be helpful

I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:107] Allocating 10.60GiB bytes.
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:118] GPU 0 memory begins at 0x23ee00000 extends to 0x4e573199a
W tensorflow/core/common_runtime/executor.cc:1221] 0xb26c6f0 Compute status: Failed precondition: Attempting to use uninitialized value lstm/LSTMCell/W_0
     [[Node: lstm/LSTMCell/W_0/_9 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_292_lstm/LSTMCell/W_0"", _device=""/job:localhost/replica:0/task:0/gpu:0""](lstm/LSTMCell/W_0)]]
     [[Node: Identity_7/_8 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_267_Identity_7"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]
W tensorflow/core/common_runtime/executor.cc:1221] 0xb26c6f0 Compute status: Failed precondition: Attempting to use uninitialized value lstm/LSTMCell/W_0
     [[Node: lstm/LSTMCell/W_0/_9 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_292_lstm/LSTMCell/W_0"", _device=""/job:localhost/replica:0/task:0/gpu:0""](lstm/LSTMCell/W_0)]]
     [[Node: Identity_6/_16 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_295_Identity_6"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]
W tensorflow/core/common_runtime/executor.cc:1221] 0xb255bf0 Compute status: Failed precondition: Attempting to use uninitialized value lstm/LSTMCell/W_0
     [[Node: lstm/LSTMCell/W_0/_9 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_292_lstm/LSTMCell/W_0"", _device=""/job:localhost/replica:0/task:0/gpu:0""](lstm/LSTMCell/W_0)]]
     [[Node: lstm/LSTMCell/B/Assign/_5 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_265_lstm/LSTMCell/B/Assign"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
W tensorflow/core/common_runtime/executor.cc:1221] 0xb255bf0 Compute status: Failed precondition: Attempting to use uninitialized value lstm/LSTMCell/W_0
     [[Node: lstm/LSTMCell/W_0/_9 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_292_lstm/LSTMCell/W_0"", _device=""/job:localhost/replica:0/task:0/gpu:0""](lstm/LSTMCell/W_0)]]
     [[Node: lstm/LSTMCell/W_0/Assign/_13 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_293_lstm/LSTMCell/W_0/Assign"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
W tensorflow/core/common_runtime/executor.cc:1221] 0xb255bf0 Compute status: Failed precondition: Attempting to use uninitialized value lstm/LSTMCell/W_0
     [[Node: lstm/LSTMCell/W_0/_9 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_292_lstm/LSTMCell/W_0"", _device=""/job:localhost/replica:0/task:0/gpu:0""](lstm/LSTMCell/W_0)]]
     [[Node: init/NoOp_1/_19 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_5848_init/NoOp_1"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
W tensorflow/core/common_runtime/executor.cc:1221] 0xb26c6f0 Compute status: Failed precondition: Attempting to use uninitialized value lstm/LSTMCell/W_0
     [[Node: lstm/LSTMCell/W_0/_9 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_292_lstm/LSTMCell/W_0"", _device=""/job:localhost/replica:0/task:0/gpu:0""](lstm/LSTMCell/W_0)]]
     [[Node: lstm/LSTMCell/B/Assign/_4 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_265_lstm/LSTMCell/B/Assign"", _device=""/job:localhost/replica:0/task:0/gpu:0""](lstm/LSTMCell/B/Assign/_3)]]
W tensorflow/core/common_runtime/executor.cc:1221] 0xb26c6f0 Compute status: Failed precondition: Attempting to use uninitialized value lstm/LSTMCell/W_0
     [[Node: lstm/LSTMCell/W_0/_9 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_292_lstm/LSTMCell/W_0"", _device=""/job:localhost/replica:0/task:0/gpu:0""](lstm/LSTMCell/W_0)]]
     [[Node: lstm/LSTMCell/W_0/Assign/_12 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_293_lstm/LSTMCell/W_0/Assign"", _device=""/job:localhost/replica:0/task:0/gpu:0""](lstm/LSTMCell/W_0/Assign/_11)]]
Traceback (most recent call last):
  File ""cifar10_multi_gpu_train.py"", line 282, in <module>
    tf.app.run()
  File ""/home/shiyemin/code/tensorflow/_python_build/tensorflow/python/platform/default/_app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""cifar10_multi_gpu_train.py"", line 278, in main
    train()
  File ""cifar10_multi_gpu_train.py"", line 237, in train
    sess.run(init)
  File ""/home/shiyemin/code/tensorflow/_python_build/tensorflow/python/client/session.py"", line 315, in run
    return self._run(None, fetches, feed_dict)
  File ""/home/shiyemin/code/tensorflow/_python_build/tensorflow/python/client/session.py"", line 511, in _run
    feed_dict_string)
  File ""/home/shiyemin/code/tensorflow/_python_build/tensorflow/python/client/session.py"", line 564, in _do_run
    target_list)
  File ""/home/shiyemin/code/tensorflow/_python_build/tensorflow/python/client/session.py"", line 586, in _do_call
    e.code)
tensorflow.python.framework.errors.FailedPreconditionError: Attempting to use uninitialized value lstm/LSTMCell/W_0
     [[Node: lstm/LSTMCell/W_0/_9 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_292_lstm/LSTMCell/W_0"", _device=""/job:localhost/replica:0/task:0/gpu:0""](lstm/LSTMCell/W_0)]]
     [[Node: lstm/LSTMCell/B/Assign/_5 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_265_lstm/LSTMCell/B/Assign"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
"
1389,"Tensorflow tutorial ""How to Retrain"": error when saving the model","For bugs/issues, please fill in the following.  The more information you
provide, the more likely we can help you.

hello,
I am trying to run the tutorial 'How to Retrain Inception's Final Layer for New Categories'
available here: https://www.tensorflow.org/versions/master/how_tos/image_retraining/index.html
### Environment info

Operating System: Ubuntu 14.04

If installed from binary pip package, provide:
1. Which pip package you installed: I use the following AWS community AMI: 
   Axel TensorFlow machine - ami-a41147ce   in Northern Virginia region.
2. The output from python -c ""import tensorflow; print(tensorflow.**version**)"":

I tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcublas.so.7.0 locally
I tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcudnn.so.6.5 locally
I tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcufft.so.7.0 locally
I tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcuda.so locally
I tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcurand.so.7.0 locally
0.5.0

If installed from sources, provide the commit hash:
### Steps to reproduce

step 1. download and unpack images: curl -O http://download.tensorflow.org/example_images/flower_photos.tgz
tar xzf flower_photos.tgz

step 2. run:    python retrain.py --image_dir ~/flower_photos
### What have you tried?
1. nothing: I do not know what to do
### Logs or other output that would be helpful

(If logs are large, please upload as attachment).

the only difference with the tutorial is that I do not use Bazel, I just do: 

python retrain.py --image_dir ~/flower_photos

Everything runs smoothly, until I get this error:

Final test accuracy = 91.6%
Traceback (most recent call last):
File ""retrain.py"", line 829, in <module>
tf.app.run()
File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/default/_app.py"", line 30, in run
sys.exit(main(sys.argv))
File ""retrain.py"", line 820, in main
output_graph_def = graph_util.convert_variables_to_constants(
AttributeError: 'module' object has no attribute 'convert_variables_to_constants'

The problematic portion of the code is:

  # Write out the trained graph and labels with the weights stored as constants.
  output_graph_def = graph_util.convert_variables_to_constants(
      sess, graph.as_graph_def(), [FLAGS.final_tensor_name])
  with gfile.FastGFile(FLAGS.output_graph, 'wb') as f:
    f.write(output_graph_def.SerializeToString())
  with gfile.FastGFile(FLAGS.output_labels, 'w') as f:
    f.write('\n'.join(image_lists.keys()) + '\n')

What should I do?
"
1388,Potential bug: Gradients currently flow through variables that are fed,"```
x = tf.Variable(1.0)
y = 2*x
z = 3*y
dzdx, = tf.gradients(z, x)

sess.run(tf.initialize_all_variables())
print(sess.run([z, dzdx], feed_dict={y: 10.0}))

# Result: z = 30.0, dzdx = 6.0
```

To me this seems like a bug: the only reasonable behavior here seems to be to treat `y` as constant, so that `dzdx` is 0.0. This would let us feed parts of models when needed, in turn automatically shutting off all gradient updates with respect to the parameters that would have been responsible for generating these fed values.

This would be a nice thing to have in my current use case. If we want to perform iterative optimization with respect to two parameter sets, we would like to feed results from set 1 and optimize set 2; feed results from set 2 and optimize set 1; etc.

Is the current behavior intended and/or useful in some use cases? If not, can we fix this?
"
1387,For poor guys without access to boringssl located in google's server,"When I cloned the latest tensorflow and compile it with bazel, bazel clones grpc.git from github.
However grpc.git need clone submodule boringssl which is located in google's server. Since I have no stable vpn to have access to boringssl, I forked grpc.git to my github and changed WORKSPACE of tensorflow (located in tensorflow's root path) as follows:
 git_repository(
     name = ""grpc"",
-    commit = ""73979f4"",
-    commit = ""403cd6c"",
   init_submodules = True,
-    remote = ""https://github.com/grpc/grpc.git"",
-    remote = ""https://github.com/melody-rain/grpc.git"",
  )

and also I changed the .gitmodule of grpc.git so that boringssl is downloaded from  

[submodule ""third_party/boringssl""]
    path = third_party/boringssl
    url = https://github.com/doubler/boringssl.git

Thanks @doubler for his help. 

For those have access to google's service, just ignore this issue,
"
1385,bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package,"Why executing 
""bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package""

deletes grpc.git and repulls it If I execute ""bazel clean"" ?
"
1383,Attempting to use uninitialized value RNN/GRUCell/Gates/Linear/Bias,"### Environment info

Operating System:
Rocks OS (Centos 6.5)

I installed from sources, and here is my version:
https://github.com/shiyemin/tensorflow/
Nothing changed but to make it compile successfully on our server.
### ERROR

I use https://github.com/ethereon/caffe-tensorflow to convert caffe model to tensorflow and the GoogLeNet is selected to construct our network.

I add  a LSTM layer to this code as follows:

```
@layer
def lstm(self, input, lstm_type, n_steps, initial_state, num_units, name):
    #  with tf.variable_scope(name) as scope:
    input_shape = input.get_shape()
    dim = 1
    for d in input_shape[1:].as_list():
        dim *= d
    input = tf.reshape(input, [input_shape[0].value, -1])

    # select LSTM type, Define a lstm cell with tensorflow
    if lstm_type == 'basic':
        lstm_cell = rnn_cell.BasicLSTMCell(num_units, input_size=dim)
    elif lstm_type == 'lstm':
        lstm_cell = rnn_cell.LSTMCell(num_units, input_size=dim)
    elif lstm_type == 'GRU':
        lstm_cell = rnn_cell.GRUCell(num_units, input_size=dim)
    else:
        raise ValueError(""LSTM type %s error.""%lstm_type)

    # Split data because rnn cell needs a list of inputs for the RNN inner loop
    input = tf.split(0, n_steps, input) # n_steps * (batch_size, n_hidden)

    # Get lstm cell output
    outputs, states = rnn.rnn(lstm_cell, input, initial_state=initial_state) # , scope=scope)
    outputs = tf.concat(0, outputs)
    return outputs #, states
```

When i add this LSTM layer to GoogLeNet, ""Failed precondition: Attempting to use uninitialized value  RNN/GRUCell/Gates/Linear/Bias"" occurs. But when i using the code from https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3%20-%20Neural%20Networks/recurrent_network.py, everything works well.

Anyone knows what happened? I don't know how to debug this error.
"
1382,"Tensorflow Tutorial websites are full of ""Math Processing Error"" messages","Hi,

I just wanted to bring this up, many of the tutorial pages in the current website are showing an error as follows:

[Math Processing Error]

This is showing instead of the formulas and make the learning processed a little bit more complicated.

Example affected page:

https://www.tensorflow.org/versions/r0.7/tutorials/word2vec/index.html#vector-representations-of-words
"
1381,inner product,"how to do inner product with tensorflow?
"
1379,"IF Exists the method about ""dropconnect"", THANK YOU !!","For bugs/issues, please fill in the following.  The more information you
provide, the more likely we can help you.
### Environment info

Operating System:

If installed from binary pip package, provide:
1. Which pip package you installed.
2. The output from python -c ""import tensorflow; print(tensorflow.**version**)"".

If installed from sources, provide the commit hash:
### Steps to reproduce

1.
2.
3.
### What have you tried?

1.
### Logs or other output that would be helpful

(If logs are large, please upload as attachment).
"
1378,load_op_library breaks when loading 2 different .so ,"For bugs/issues, please fill in the following.  The more information you
provide, the more likely we can help you.
### Environment info

Operating System:
Rhel 7

If installed from binary pip package, provide:
1. Which pip package you installed.
   Built from source.  0.7.1 master 3/3/2016
2. The output from python -c ""import tensorflow; print(tensorflow.**version**)"".
   0.7.1
   If installed from sources, provide the commit hash:
   99952d68444cd2b08e88af972bd9eb7034fbc1e7
### Steps to reproduce
1.  Follow Instructions via:
   https://www.tensorflow.org/versions/master/how_tos/adding_an_op/index.html
2.  Create zero_out.cc and build zero_out.so

```
#include ""tensorflow/core/framework/op.h""

REGISTER_OP(""ZeroOut"")
    .Input(""to_zero: int32"")
    .Output(""zeroed: int32"");

#include ""tensorflow/core/framework/op_kernel.h""

using namespace tensorflow;

class ZeroOutOp : public OpKernel {
 public:
  explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) {}

  void Compute(OpKernelContext* context) override {
    // Grab the input tensor
    const Tensor& input_tensor = context->input(0);
    auto input = input_tensor.flat<int32>();

    // Create an output tensor
    Tensor* output_tensor = NULL;
    OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),
                                                     &output_tensor));
    auto output = output_tensor->template flat<int32>();

    // Set all but the first element of the output tensor to 0.
    const int N = input.size();
    for (int i = 1; i < N; i++) {
      output(i) = 0;
    }

    // Preserve the first input value if possible.
    if (N > 0) output(0) = input(0);
  }
};

REGISTER_KERNEL_BUILDER(Name(""ZeroOut"").Device(DEVICE_CPU), ZeroOutOp);
```
1.  Create one_out.cc and build one_out.so

```
#include ""tensorflow/core/framework/op.h""

REGISTER_OP(""OneOut"")
    .Input(""to_one: int32"")
    .Output(""oneed: int32"");

#include ""tensorflow/core/framework/op_kernel.h""

using namespace tensorflow;

class OneOutOp : public OpKernel {
 public:
  explicit OneOutOp(OpKernelConstruction* context) : OpKernel(context) {}

  void Compute(OpKernelContext* context) override {
    // Grab the input tensor
    const Tensor& input_tensor = context->input(0);
    auto input = input_tensor.flat<int32>();

    // Create an output tensor
    Tensor* output_tensor = NULL;
    OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),
                                                     &output_tensor));
    auto output = output_tensor->template flat<int32>();

    // Set all but the first element of the output tensor to 0.
    const int N = input.size();
    for (int i = 1; i < N; i++) {
      output(i) = 1;
    }

    // Preserve the first input value if possible.
    if (N > 0) output(0) = input(0);
  }
};

REGISTER_KERNEL_BUILDER(Name(""OneOut"").Device(DEVICE_CPU), OneOutOp);
```
1.  Attempt to load both so in same script:

```
import tensorflow as tf

zero_out_module = tf.load_op_library('./zero_out.so')
one_out_module = tf.load_op_library('./one_out.so')

with tf.Session('') as session:
    inputs = tf.constant([[1,1,1], [-2, -2, -2]], dtype=tf.int32)
    print(inputs.eval())
```
1. Get Error:

```
F tensorflow/core/framework/op.cc:119] Check failed: ::tensorflow::Status::OK() == (RegisterAlreadyLocked(op_def)) (OK vs. Already exists: Op with name ZeroOu
t)Attempting to register: Op<name=ZeroOut; signature=to_zero:int32 -> zeroed:int32>
Aborted

```
"
1377,Tensorflow Website : Missing Mathjax results in broken equations,"The website tries to load,

`
https://www.tensorflow.org/MathJax/jax/output/HTML-CSS/fonts/STIX/fontdata.js?rev=2.6.1
`

and fails. Someone needs to fix the path to get the right version of mathjax, and have all the LaTeX symbols show up.

Thanks.
"
1376,AttributeError: 'module' object has no attribute 'random_crop',"I'm on using Ubuntu 14.04 and installed tensorflow. When I ran the image processing example as in the website, it failed complaining about random_crop not found. I took the code from here 
https://github.com/tensorflow/tensorflow/blob/r0.7/tensorflow/models/image/cifar10/

Here's the exception stack:

```
Traceback (most recent call last):
  File ""cifar10_train.py"", line 136, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/default/_app.py"", line 11, in run
    sys.exit(main(sys.argv))
  File ""cifar10_train.py"", line 132, in main
    train()
  File ""cifar10_train.py"", line 67, in train
    images, labels = cifar10.distorted_inputs()
  File ""/home/sarah/Documents/SVHN/cifar10.py"", line 150, in distorted_inputs
    batch_size=FLAGS.batch_size)
  File ""/home/sarah/Documents/SVHN/cifar10_input.py"", line 161, in distorted_inputs
    distorted_image = tf.random_crop(reshaped_image, [height, width, 3])
AttributeError: 'module' object has no attribute 'random_crop'

```
"
1375,"Compilation issue: 'Matrix' is not a class, namespace, or enumeration","### Environment info

Operating System: CentOS 6.7
If installed from sources, provide the commit hash: eda89e930cfcbd992ecacafd40267d733e2153dc
### Steps to reproduce
1. Configure for gcc 4.8.2
2. ./configure
3. Edit third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc
   - This is necessary; otherwise `gcc` can't find `as`.
   - In fact, there are notes about this:
     - `# TODO(zhengxq): for some reason, 'gcc' needs this help to find 'as'.`
     - `# Need to investigate and fix.`
   - I had to comment their fix to get gcc to find `as` on my system.
   - Specifically, `# cmd = 'PATH=' + PREFIX_DIR + ' ' + cmd`
4. `bazel build -c opt --config=cuda --verbose_failures --genrule_strategy=standalone --spawn_strategy=standalone //tensorflow/tools/pip_package:build_pip_package`
### What have you tried?
1. I don't know what else to try.
### Logs or other output that would be helpful

Most of the build succeeds, but then I get

```
INFO: Found 1 target...
ERROR: /home-4/rdipiet2@jhu.edu/install/tensorflow/tensorflow/core/kernels/BUILD:560:1: C++ compilation of rule '//tensorflow/core/kernels:matrix_solve_ls_op' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home-4/rdipiet2@jhu.edu/.cache/bazel/_bazel_rdipiet2@jhu.edu/549db212089e33b4d213773753834e47/tensorflow && \
  exec env - \
    PATH=/home-4/rdipiet2@jhu.edu/install/bazel/output:/home-4/rdipiet2@jhu.edu/.usr/bin:/home-4/rdipiet2@jhu.edu/vc/scripts:/home-4/rdipiet2@jhu.edu/.local/bin:/cm/shared/apps/java/JDK_1.8.0_45/bin:/cm/shared/apps/cuda/7.0/bin:/cm/shared/apps/git/2.6.4/bin:/cm/shared/apps/anaconda/2.7.10/bin:/cm/shared/apps/slurm/current/sbin:/cm/shared/apps/slurm/current/bin:/cm/shared/apps/gcc/4.8.2/bin:/cm/shared/apps/Intel/openmpi/1.8.4/bin:/cm/shared/apps/binutils:/cm/shared/apps/binutils/2.25/src/bin:/cm/shared/apps/parallel_studio_xe_2015_update2/composer_xe_2015.2.164/bin/intel64:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/ibutils/bin:/sbin:/usr/sbin:/cm/local/apps/environment-modules/3.2.10/bin:/opt/dell/srvadmin/bin:/home-4/rdipiet2@jhu.edu/bin \
  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -iquote external/jpeg_archive -iquote bazel-out/local_linux-opt/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/local_linux-opt/genfiles/external/png_archive -iquote external/re2 -iquote bazel-out/local_linux-opt/genfiles/external/re2 -iquote external/eigen_archive -iquote bazel-out/local_linux-opt/genfiles/external/eigen_archive -isystem google/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/google/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local_linux-opt/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-017cff30cf74 -isystem bazel-out/local_linux-opt/genfiles/external/eigen_archive/eigen-eigen-017cff30cf74 -isystem third_party/gpus/cuda -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda -isystem third_party/gpus/cuda/include -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda/include -fno-exceptions -DEIGEN_AVOID_STL_ARRAY '-DGOOGLE_CUDA=1' -pthread '-DGOOGLE_CUDA=1' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fno-canonical-system-headers '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/matrix_solve_ls_op/tensorflow/core/kernels/matrix_solve_ls_op.pic.o' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/matrix_solve_ls_op/tensorflow/core/kernels/matrix_solve_ls_op.pic.d -fPIC -c tensorflow/core/kernels/matrix_solve_ls_op.cc -o bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/matrix_solve_ls_op/tensorflow/core/kernels/matrix_solve_ls_op.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home-4/rdipiet2@jhu.edu/.cache/bazel/_bazel_rdipiet2@jhu.edu/549db212089e33b4d213773753834e47/tensorflow && \
  exec env - \
    PATH=/home-4/rdipiet2@jhu.edu/install/bazel/output:/home-4/rdipiet2@jhu.edu/.usr/bin:/home-4/rdipiet2@jhu.edu/vc/scripts:/home-4/rdipiet2@jhu.edu/.local/bin:/cm/shared/apps/java/JDK_1.8.0_45/bin:/cm/shared/apps/cuda/7.0/bin:/cm/shared/apps/git/2.6.4/bin:/cm/shared/apps/anaconda/2.7.10/bin:/cm/shared/apps/slurm/current/sbin:/cm/shared/apps/slurm/current/bin:/cm/shared/apps/gcc/4.8.2/bin:/cm/shared/apps/Intel/openmpi/1.8.4/bin:/cm/shared/apps/binutils:/cm/shared/apps/binutils/2.25/src/bin:/cm/shared/apps/parallel_studio_xe_2015_update2/composer_xe_2015.2.164/bin/intel64:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/ibutils/bin:/sbin:/usr/sbin:/cm/local/apps/environment-modules/3.2.10/bin:/opt/dell/srvadmin/bin:/home-4/rdipiet2@jhu.edu/bin \
  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -iquote external/jpeg_archive -iquote bazel-out/local_linux-opt/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/local_linux-opt/genfiles/external/png_archive -iquote external/re2 -iquote bazel-out/local_linux-opt/genfiles/external/re2 -iquote external/eigen_archive -iquote bazel-out/local_linux-opt/genfiles/external/eigen_archive -isystem google/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/google/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local_linux-opt/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-017cff30cf74 -isystem bazel-out/local_linux-opt/genfiles/external/eigen_archive/eigen-eigen-017cff30cf74 -isystem third_party/gpus/cuda -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda -isystem third_party/gpus/cuda/include -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda/include -fno-exceptions -DEIGEN_AVOID_STL_ARRAY '-DGOOGLE_CUDA=1' -pthread '-DGOOGLE_CUDA=1' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fno-canonical-system-headers '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/matrix_solve_ls_op/tensorflow/core/kernels/matrix_solve_ls_op.pic.o' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/matrix_solve_ls_op/tensorflow/core/kernels/matrix_solve_ls_op.pic.d -fPIC -c tensorflow/core/kernels/matrix_solve_ls_op.cc -o bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/matrix_solve_ls_op/tensorflow/core/kernels/matrix_solve_ls_op.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
tensorflow/core/kernels/matrix_solve_ls_op.cc: In member function 'void tensorflow::MatrixSolveLsOp<Scalar, SupportsBatchOperationT>::ComputeMatrix(tensorflow::OpKernelContext*, const typename tensorflow::BinaryLinearAlgebraOp<Scalar, SupportsBatchOperationT>::ConstMatrixMap&, const typename tensorflow::BinaryLinearAlgebraOp<Scalar, SupportsBatchOperationT>::ConstMatrixMap&, typename tensorflow::BinaryLinearAlgebraOp<Scalar, SupportsBatchOperationT>::MatrixMap*)':
tensorflow/core/kernels/matrix_solve_ls_op.cc:111:41: error: 'Matrix' is not a class, namespace, or enumeration
               (Scalar(l2_regularizer) * Matrix::Ones(cols, 1)).asDiagonal();
                                         ^
tensorflow/core/kernels/matrix_solve_ls_op.cc:131:41: error: 'Matrix' is not a class, namespace, or enumeration
               (Scalar(l2_regularizer) * Matrix::Ones(rows, 1)).asDiagonal();
                                         ^
In file included from ./tensorflow/core/framework/op_kernel.h:22:0,
                 from tensorflow/core/kernels/matrix_solve_ls_op.cc:23:
./tensorflow/core/framework/allocator.h: In member function 'virtual std::size_t tensorflow::Allocator::RequestedSize(void*)':
./tensorflow/core/framework/allocator.h:152:3: warning: control reaches end of non-void function [-Wreturn-type]
   }
   ^
In file included from ./tensorflow/core/framework/op_kernel.h:25:0,
                 from tensorflow/core/kernels/matrix_solve_ls_op.cc:23:
./tensorflow/core/framework/device_base.h: In member function 'virtual tensorflow::Allocator* tensorflow::DeviceBase::GetAllocator(tensorflow::AllocatorAttributes)':
./tensorflow/core/framework/device_base.h:150:3: warning: control reaches end of non-void function [-Wreturn-type]
   }
   ^
./tensorflow/core/framework/device_base.h: In member function 'virtual const tensorflow::DeviceAttributes& tensorflow::DeviceBase::attributes() const':
./tensorflow/core/framework/device_base.h:181:3: warning: control reaches end of non-void function [-Wreturn-type]
   }
   ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 12.708s, Critical Path: 12.06s

```

I'm hoping very much to get this running on a CentOS cluster – any help will be very appreciated. Thanks.
"
1374,Document trick with slash in scope names,"The documentation for `tf.name_scope` is rather ambiguous on what happens if the scope name ends with '/', merely stating what happens if it doesn't end with a '/'. The fact that the behavior is different, specifically that ending with a '/' does not result in automatically uniqueifying the scope name, is very important and useful, because sometimes one wants to reenter a previously created scope. This possibility is only alluded to when describing recapturing `scope` variables, but the functionality is more general than that.
"
1373,import tensorflow results in segfault.,"For bugs/issues, please fill in the following.  The more information you
provide, the more likely we can help you.
### Environment info

Operating System:
ubuntu 15.10 64-bit
Python 3.4.3+

If installed from binary pip package, provide:
1. Which pip package you installed.
https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.1-cp34-none-linux_x86_64.whl
2. The output from python -c ""import tensorflow; print(tensorflow.**version**)"".
python3 -c ""import tensorflow; print(tensorflow.**version**)""
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally
0.7.1
### Steps to reproduce
1. import gi
2. from gi.repository import Gtk
3. import tensorflow
### What have you tried?
1. Identified the necessary steps to reproduce the segfault.
2. Importing in opposite order does NOT produce any segfault.
   1. import tensorflow
   2. import gi
   3. from gi.repository import Gtk
### Logs or other output that would be helpful

Segmentation fault (core dumped)
"
1372,Compilation error: missing dependency declarations ... external/re2/util/rune.cc,"### Environment info

Operating System: CentOS 6.7

If installed from sources, provide the commit hash: eda89e930cfcbd992ecacafd40267d733e2153dc
### Steps to reproduce
1. ./configure (CUDA enabled)
2. bazel build -c opt --config=cuda --verbose_failures --genrule_strategy=standalone --spawn_strategy=standalone //tensorflow/cc:tutorials_example_trainer
### What have you tried?
1. Using --genrule_strategy=standalone --spawn_strategy=standalone as possible solutions from other errors that seem related, but this doesn't work.
### Logs or other output that would be helpful

```
WARNING: Output base '/home-4/rdipiet2@jhu.edu/.cache/bazel/_bazel_rdipiet2@jhu.edu/549db212089e33b4d213773753834e47'
 is on NFS. This may lead to surprising failures and undetermined behavior.
WARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. Se
e http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ig
nore_unsupported_sandboxing.
INFO: Found 1 target...
ERROR: /home-4/rdipiet2@jhu.edu/.cache/bazel/_bazel_rdipiet2@jhu.edu/549db212089e33b4d213773753834e47/external/re2/BU
ILD:9:1: undeclared inclusion(s) in rule '@re2//:re2':
this rule is missing dependency declarations for the following files included by 'external/re2/util/rune.cc':
  '/cm/shared/apps/gcc/4.8.2/lib/gcc/x86_64-unknown-linux-gnu/4.8.2/include/stdarg.h'
  '/cm/shared/apps/gcc/4.8.2/lib/gcc/x86_64-unknown-linux-gnu/4.8.2/include/stddef.h'
  '/cm/shared/apps/gcc/4.8.2/lib/gcc/x86_64-unknown-linux-gnu/4.8.2/include/stdint.h'.
Target //tensorflow/cc:tutorials_example_trainer failed to build
INFO: Elapsed time: 5.291s, Critical Path: 2.47s
```
### Other Info

I was able to compile Bazel with older libc, and now I'm hoping I can compile TensorFlow on this server. Any help will be very much appreciated.
"
1370,MultiRNNCell with different cell sizes,"When I try to create a MultiRNNCell with different number of units in the recurrent cells (e.g. 5-layer RNN with 32-64-32 GRUCells in the hidden layers) I got the following error:

`ValueError: In MultiRNNCell, the input size of each next cell must match the output size of the previous one. Mismatched output size in cell 0.`

Is it currently possible to create a MultiRNNCell with different number of hidden units?
"
1369,Tensorflow issue ,"For bugs/issues, please fill in the following.  The more information you
provide, the more likely we can help you.
### Environment info

Operating System:
OSx
If installed from binary pip package, provide:
pip, Virtualenv
1. Which pip package you installed.
2. The output from python -c ""import tensorflow; print(tensorflow.**version**)"".
0.7.1
If installed from sources, provide the commit hash:
### Steps to reproduce

1.activate the virtualenv
2.ipython nootebook
3.
### What have you tried?
1. (tensorflow) ➜  ~  ipython notebook
### Logs or other output that would be helpful

(If logs are large, please upload as attachment).

[TerminalIPythonApp] WARNING | Subcommand `ipython notebook` is deprecated and will be removed in future versions.
[TerminalIPythonApp] WARNING | You likely want to use `jupyter notebook`... continue in 5 sec. Press Ctrl-C to quit now.
Traceback (most recent call last):
  File ""/Users/maheshwarligade/tensorflow/bin/ipython"", line 9, in <module>
    load_entry_point('ipython==4.1.1', 'console_scripts', 'ipython')()
  File ""/Users/maheshwarligade/tensorflow/lib/python3.5/site-packages/ipython-4.1.1-py3.5.egg/IPython/**init**.py"", line 119, in start_ipython
    return launch_new_instance(argv=argv, *_kwargs)
  File ""/Users/maheshwarligade/tensorflow/lib/python3.5/site-packages/traitlets-4.1.0-py3.5.egg/traitlets/config/application.py"", line 588, in launch_instance
    app.initialize(argv)
  File ""<decorator-gen-111>"", line 2, in initialize
  File ""/Users/maheshwarligade/tensorflow/lib/python3.5/site-packages/traitlets-4.1.0-py3.5.egg/traitlets/config/application.py"", line 74, in catch_config_error
    return method(app, *args, *_kwargs)
  File ""/Users/maheshwarligade/tensorflow/lib/python3.5/site-packages/ipython-4.1.1-py3.5.egg/IPython/terminal/ipapp.py"", line 297, in initialize
    super(TerminalIPythonApp, self).initialize(argv)
  File ""<decorator-gen-7>"", line 2, in initialize
  File ""/Users/maheshwarligade/tensorflow/lib/python3.5/site-packages/traitlets-4.1.0-py3.5.egg/traitlets/config/application.py"", line 74, in catch_config_error
    return method(app, _args, *_kwargs)
  File ""/Users/maheshwarligade/tensorflow/lib/python3.5/site-packages/ipython-4.1.1-py3.5.egg/IPython/core/application.py"", line 401, in initialize
    self.parse_command_line(argv)
  File ""/Users/maheshwarligade/tensorflow/lib/python3.5/site-packages/ipython-4.1.1-py3.5.egg/IPython/terminal/ipapp.py"", line 292, in parse_command_line
    return super(TerminalIPythonApp, self).parse_command_line(argv)
  File ""<decorator-gen-4>"", line 2, in parse_command_line
  File ""/Users/maheshwarligade/tensorflow/lib/python3.5/site-packages/traitlets-4.1.0-py3.5.egg/traitlets/config/application.py"", line 74, in catch_config_error
    return method(app, _args, *_kwargs)
  File ""/Users/maheshwarligade/tensorflow/lib/python3.5/site-packages/traitlets-4.1.0-py3.5.egg/traitlets/config/application.py"", line 485, in parse_command_line
    return self.initialize_subcommand(subc, subargv)
  File ""/Users/maheshwarligade/tensorflow/lib/python3.5/site-packages/ipython-4.1.1-py3.5.egg/IPython/core/application.py"", line 211, in initialize_subcommand
    return super(BaseIPythonApplication, self).initialize_subcommand(subc, argv)
  File ""<decorator-gen-3>"", line 2, in initialize_subcommand
  File ""/Users/maheshwarligade/tensorflow/lib/python3.5/site-packages/traitlets-4.1.0-py3.5.egg/traitlets/config/application.py"", line 74, in catch_config_error
    return method(app, _args, *_kwargs)
  File ""/Users/maheshwarligade/tensorflow/lib/python3.5/site-packages/traitlets-4.1.0-py3.5.egg/traitlets/config/application.py"", line 416, in initialize_subcommand
    subapp = import_item(subapp)
  File ""/Users/maheshwarligade/tensorflow/lib/python3.5/site-packages/ipython_genutils-0.1.0-py3.5.egg/ipython_genutils/importstring.py"", line 31, in import_item
ImportError: No module named 'notebook'
"
1368,svhn: tensorflow/core/common_runtime/executor.cc:1027] 0x2076650 Compute status: Invalid argument: Indices are not valid (out of bounds).  Shape: dim { size: 128 } dim { size: 10 },"When I tried to train model by SVHN data I faced the above exception.
Stackoverflow link is here: http://stackoverflow.com/questions/35756236/tensorflow-tried-svhn-by-editing-label-0-9-still-not-working

Did I did something wrong/ missed any logic or is it a bug?
Please help me. I have already attempted in several ways but could not figure out the reason.

Thanks
"
1367,ImportError: No module named core.framework.graph_pb2,"Hello, I am trying to build tensorflow from source without CPU support. When I try to import it in python I am getting the error ""ImportError: No module named core.framework.graph_pb2"".

I have tried this on Arch Linux AND on Ubuntu which gives me the impression I might be doing something wrong. However I am exactly following the instructions on the TF homepage.

I would appreciate any help!!
#### Environment info

Operating System: Arch Linux
Commit hash: 263d00d2710779d5c4ac66e335b2ba07d8385b6b
### Steps to reproduce
1. ./configure
2. bazel build -c opt //tensorflow/cc:tutorials_example_trainer
3. python2
4. import tensorflow
### What have you tried?
1. I made sure I am not in the build directory
2. I have uninstalled protobuf and reinstalled it
3. I have reinstalled six
4. I tried with different versions of protobuf
5. I tried the same on Ubuntu 15.04 -> getting the same error ()
### Logs or other output that would be helpful

> > > import tensorflow
> > > Traceback (most recent call last):
> > >   File ""<stdin>"", line 1, in <module>
> > >   File ""/home/.../Development/tensorflow/tensorflow/**init**.py"", line 23, in <module>
> > >     from tensorflow.python import *
> > >   File ""/home/.../Development/tensorflow/tensorflow/python/**init**.py"", line 41, in <module>
> > >     raise ImportError(msg)
> > > ImportError: Traceback (most recent call last):
> > >   File ""/home/.../Development/tensorflow/tensorflow/python/**init**.py"", line 35, in <module>
> > >     from tensorflow.core.framework.graph_pb2 import *
> > > ImportError: No module named core.framework.graph_pb2
"
1366,"Implement the Special Functions incbet,igam,igamc for CPU+GPU for float & double.","incbet: [incomplete beta integral](http://www.netlib.org/cephes/doubldoc.html#incbet)
igam: [incomplete gamma integral](http://www.netlib.org/cephes/doubldoc.html#igam)
igamc: [complemented incomplete gamma integral](http://www.netlib.org/cephes/doubldoc.html#igamc)
"
1365,histogram_summary does not respect name_scope,"I am getting a Duplicate tag error when I try to write out histogram summaries for a multi-layer network that I generate procedurally. The problem is that the name given to a histogram_summary (and I assume all summaries) does not respect name_scope. As a consequence, the names are not unique. This seems like a bug.

For example, in:

```
with tf.name_scope(some_unique_name):
  ...
  _ = tf.histogram_summary('weights', kernel_weights)
```

I'd assume that 'weights' would be scoped to some_unique_name but I'm suspecting that it is not. I added code to make the names unique and the Duplicate tag problem goes away.
### Environment info

Operating System: OSX
TensorFlow version: 0.7.0 
"
1363,Tests in //tensorflow/core/distributed_runtime/... sometimes time out,"Some of the tests in this directory (in particular, `master_test` and `rpc/grpc_session_test`) spawn subprocesses containing TensorFlow servers that listen on unused ports. There is a potential TOCTOU bug in this code because the process is:
1. Pick unused ports in the parent.
2. Fork processes and instruct them to bind to the picked ports. (At this point another process can bind the same ports.)
3. Connect to subprocesses from the parent.

Running these tests in exclusive mode is a workaround for the problem.
"
1362,matrix_solve_ls_op Compile Error on gcc 4.8 (using typename error),"I have been trying to compile tensorflow from source without success.
I could compile version 0.6.0 with no trouble, but trying to compile 0.7.1 produces the following error:

<blockquote>
tensorflow/core/kernels/matrix_solve_ls_op.cc: In member function 'void tensorflow::MatrixSolveLsOp<Scalar, SupportsBatchOperationT>::ComputeMatrix(tensorflow::OpKernelContext*, const typename tensorflow::BinaryLinearAlgebraOp<Scalar, SupportsBatchOperationT>::ConstMatrixMap&, const typename tensorflow::BinaryLinearAlgebraOp<Scalar, SupportsBatchOperationT>::ConstMatrixMap&, typename tensorflow::BinaryLinearAlgebraOp<Scalar, SupportsBatchOperationT>::MatrixMap*)':
tensorflow/core/kernels/matrix_solve_ls_op.cc:111:41: error: 'Matrix' is not a class, namespace, or enumeration
               (Scalar(l2_regularizer) * Matrix::Ones(cols, 1)).asDiagonal();
                                         ^
tensorflow/core/kernels/matrix_solve_ls_op.cc:131:41: error: 'Matrix' is not a class, namespace, or enumeration
               (Scalar(l2_regularizer) * Matrix::Ones(rows, 1)).asDiagonal();
                                         ^
In file included from ./tensorflow/core/framework/op_kernel.h:22:0,
                 from tensorflow/core/kernels/matrix_solve_ls_op.cc:23:
./tensorflow/core/framework/allocator.h: In member function 'virtual std::size_t tensorflow::Allocator::RequestedSize(void*)':
./tensorflow/core/framework/allocator.h:128:3: warning: control reaches end of non-void function [-Wreturn-type]
   }
   ^
In file included from ./tensorflow/core/framework/op_kernel.h:25:0,
                 from tensorflow/core/kernels/matrix_solve_ls_op.cc:23:
./tensorflow/core/framework/device_base.h: In member function 'virtual tensorflow::Allocator* tensorflow::DeviceBase::GetAllocator(tensorflow::AllocatorAttributes)':
./tensorflow/core/framework/device_base.h:149:3: warning: control reaches end of non-void function [-Wreturn-type]
   }
   ^
./tensorflow/core/framework/device_base.h: In member function 'virtual const tensorflow::DeviceAttributes& tensorflow::DeviceBase::attributes() const':
./tensorflow/core/framework/device_base.h:179:3: warning: control reaches end of non-void function [-Wreturn-type]
   }
   ^
</blockquote>


I tried to find the definition of Matrix::Ones in all folders, but couldn't find anything. In fact 

> grep ""Ones"" tensorflow/\* -R 

returns 

<blockquote>
core/kernels/matrix_solve_ls_op.cc:              (Scalar(l2_regularizer) * Matrix::Ones(cols, 1)).asDiagonal();
core/kernels/matrix_solve_ls_op.cc:              (Scalar(l2_regularizer) * Matrix::Ones(rows, 1)).asDiagonal();
python/kernel_tests/constant_op_test.py:class OnesTest(tf.test.TestCase):
python/kernel_tests/constant_op_test.py:  def _Ones(self, shape):
python/kernel_tests/constant_op_test.py:    self.assertTrue(np.array_equal(self._Ones([2, 3]), np.array([[1] * 3] * 2)))
python/kernel_tests/constant_op_test.py:class OnesLikeTest(tf.test.TestCase):
python/kernel_tests/constant_op_test.py:  def testOnesLike(self):
python/kernel_tests/constant_op_test.py:  def testOnesLikePartialShape(self):
python/kernel_tests/shape_ops_test.py:  def testSqueezeAllOnes(self):
python/kernel_tests/shape_ops_test.py:  def testSqueezeOnlyOnes(self):
</blockquote>

### Environment info

OS : CentOS 6.6, but I have gcc 4.8.2 installed.
### Steps to reproduce

> ./configure

say yes to GPU, cuda 7.5, cudnn 4

> bazel build --config=cuda --jobs 6 --verbose_failures --linkopt=""-lrt"" --linkopt=""-lm"" --genrule_strategy=standalone --spawn_strategy=standalone -c opt //tensorflow/tools/pip_package:build_pip_package
"
1361,"Installation error ""error: identifier ""__shfl_down"" is undefined""  (Trying with Compute Capability  2.0) ","I'm trying to install tensor flow from source, as I would like to use GPU's 
that have compute capability == 2.0. (And I suspect that problems lies here and might be not have a trivial solution)
### Environment info

Operating System:
uname -or
3.16.0-4-amd64 GNU/Linux
bazel release 0.2.0
# Commit hash

git log --pretty=format:'%h' -n 1
99952d6
git log -1 --format=""%H""
99952d68444cd2b08e88af972bd9eb7034fbc1e7

I use TF_UNOFFICIAL_SETTING=1 ./configure 
and  choose Compute Capability  2.0 

bazel build -c opt  --config=cuda  --spawn_strategy=standalone --verbose_failures  //tensorflow/cc:tutorials_example_trainer

Full output here : https://gist.github.com/cbonnett/97aec5a35209f0501993

**What seems to be the key-error:**
INFO: From Compiling tensorflow/core/kernels/sparse_xent_op_gpu.cu.cc:
external/eigen_archive/eigen-eigen-017cff30cf74/unsupported/Eigen/CXX11/src/Tensor/TensorReductionCuda.h(111): **error: identifier ""__shfl_down"" is undefined**

It seems ""__shfl_down"" was added in Kepler (compute capability == 3.x )
https://devblogs.nvidia.com/parallelforall/faster-parallel-reductions-kepler/
and as my cards are pre-Kepler, this seems to be the culprit. Can anybody confirm this ? or might there be a solution ?  
"
1360,Tensorflow Cifar-10 evaluation error: Enqueue operation was cancelled,"Operating System: Mac Os X 10.9.5

Current tensorflow version 0.7.1

I am trying to get the Cifar-10 tutorial running on my own data set (so using other images, but the same code). Training works fine. However, the evaluation seems to have a problem. Whenever I run the evaluation, I get the following message:

W tensorflow/core/common_runtime/executor.cc:1102] 0x7fc6fe504a20 Compute status                                                                                                                     : Cancelled: Enqueue operation was cancelled
         [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomp                                                                                                                     onents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/ta                                                                                                                     sk:0/cpu:0""](shuffle_batch/random_shuffle_queue, Div, Cast)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fc6fe403eb0 Compute status                                                                                                                     : Cancelled: Enqueue operation was cancelled
         [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomp                                                                                                                     onents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/ta                                                                                                                     sk:0/cpu:0""](shuffle_batch/random_shuffle_queue, Div, Cast)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fc6fe206b10 Compute status                                                                                                                     : Cancelled: Enqueue operation was cancelled
         [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomp                                                                                                                     onents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/ta                                                                                                                     sk:0/cpu:0""](shuffle_batch/random_shuffle_queue, Div, Cast)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fc6fbe16130 Compute status                                                                                                                     : Cancelled: Enqueue operation was cancelled
         [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomp                                                                                                                     onents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/ta                                                                                                                     sk:0/cpu:0""](shuffle_batch/random_shuffle_queue, Div, Cast)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fc6f8e6bd80 Compute status                                                                                                                     : Cancelled: Enqueue operation was cancelled
         [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomp                                                                                                                     onents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/ta                                                                                                                     sk:0/cpu:0""](shuffle_batch/random_shuffle_queue, Div, Cast)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fc6fbcca1e0 Compute status                                                                                                                     : Cancelled: Enqueue operation was cancelled
         [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomp                                                                                                                     onents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/ta                                                                                                                     sk:0/cpu:0""](shuffle_batch/random_shuffle_queue, Div, Cast)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fc6fbbb5e30 Compute status                                                                                                                     : Cancelled: Enqueue operation was cancelled
         [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomp                                                                                                                     onents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/ta                                                                                                                     sk:0/cpu:0""](shuffle_batch/random_shuffle_queue, Div, Cast)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fc6f8daee90 Compute status                                                                                                                     : Cancelled: Enqueue operation was cancelled
         [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomp                                                                                                                     onents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/ta                                                                                                                     sk:0/cpu:0""](shuffle_batch/random_shuffle_queue, Div, Cast)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fc6fe304d50 Compute status                                                                                                                     : Cancelled: Enqueue operation was cancelled
         [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomp                                                                                                                     onents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/ta                                                                                                                     sk:0/cpu:0""](shuffle_batch/random_shuffle_queue, Div, Cast)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fc6fbcb9570 Compute status                                                                                                                     : Cancelled: Enqueue operation was cancelled
         [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomp                                                                                                                     onents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/ta                                                                                                                     sk:0/cpu:0""](shuffle_batch/random_shuffle_queue, Div, Cast)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fc6f8e01660 Compute status                                                                                                                     : Cancelled: Enqueue operation was cancelled
         [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomp                                                                                                                     onents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/ta                                                                                                                     sk:0/cpu:0""](shuffle_batch/random_shuffle_queue, Div, Cast)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fc6fba2a1a0 Compute status                                                                                                                     : Cancelled: Enqueue operation was cancelled
         [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomp                                                                                                                     onents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/ta                                                                                                                     sk:0/cpu:0""](shuffle_batch/random_shuffle_queue, Div, Cast)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fc6f8e357a0 Compute status                                                                                                                     : Cancelled: Enqueue operation was cancelled
         [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomp                                                                                                                     onents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/ta                                                                                                                     sk:0/cpu:0""](shuffle_batch/random_shuffle_queue, Div, Cast)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fc6fe207e90 Compute status                                                                                                                     : Cancelled: Enqueue operation was cancelled
         [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomp                                                                                                                     onents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/ta                                                                                                                     sk:0/cpu:0""](shuffle_batch/random_shuffle_queue, Div, Cast)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fc6fe2089b0 Compute status                                                                                                                     : Cancelled: Enqueue operation was cancelled
         [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomp                                                                                                                     onents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/ta                                                                                                                     sk:0/cpu:0""](shuffle_batch/random_shuffle_queue, Div, Cast)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fc6fa6bfe20 Compute status                                                                                                                     : Cancelled: Enqueue operation was cancelled
         [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomp                                                                                                                     onents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/ta                                                                                                                     sk:0/cpu:0""](shuffle_batch/random_shuffle_queue, Div, Cast)]]
I tensorflow/core/kernels/queue_base.cc:286] Skipping cancelled enqueue attempt
I tensorflow/core/kernels/queue_base.cc:286] Skipping cancelled enqueue attempt
I tensorflow/core/kernels/queue_base.cc:286] Skipping cancelled enqueue attempt
I tensorflow/core/kernels/queue_base.cc:286] Skipping cancelled enqueue attempt
I tensorflow/core/kernels/queue_base.cc:286] Skipping cancelled enqueue attempt
I tensorflow/core/kernels/queue_base.cc:286] Skipping cancelled enqueue attempt
I tensorflow/core/kernels/queue_base.cc:286] Skipping cancelled enqueue attempt
I tensorflow/core/kernels/queue_base.cc:286] Skipping cancelled enqueue attempt
I tensorflow/core/kernels/queue_base.cc:286] Skipping cancelled enqueue attempt
I tensorflow/core/kernels/queue_base.cc:286] Skipping cancelled enqueue attempt
I tensorflow/core/kernels/queue_base.cc:286] Skipping cancelled enqueue attempt
I tensorflow/core/kernels/queue_base.cc:286] Skipping cancelled enqueue attempt
I tensorflow/core/kernels/queue_base.cc:286] Skipping cancelled enqueue attempt
I tensorflow/core/kernels/queue_base.cc:286] Skipping cancelled enqueue attempt
I tensorflow/core/kernels/queue_base.cc:286] Skipping cancelled enqueue attempt
I tensorflow/core/kernels/queue_base.cc:286] Skipping cancelled enqueue attempt
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fc6f8e689f0 Compute status                                                                                                                     : Cancelled: Enqueue operation was cancelled
         [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueMany[Tc                                                                                                                     omponents=[DT_STRING], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/c                                                                                                                     pu:0""](input_producer, input_producer/RandomShuffle)]]
I tensorflow/core/kernels/queue_base.cc:286] Skipping cancelled enqueue attempt

Right before this message, if I let the code run long enough, I get the output for a precision (precision @ 1 = xxx). However, the message still irritates me, I looks like an error message...

I've googled this error, and so far I've only found some posts talking about that one should first initialize placeholders before starting the queuerunners. However, since there are no placeholders used anywhere in the code before, and I am very new to tensorflow/machine learning in general, I have no idea where one would put these placeholders, or what they would be. 
"
1359, run time error:could not find cudnnConvolutionBackwardData_v2 in cudnn DSO,"Operating System:ubuntu 14.04LTS
pip install 0.6 and  sudo pip install --upgrade .. to 0.7
The output from python -c ""import tensorflow; print(tensorflow.**version**)"" is:
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally
0.7.1

below is errors when running cifar10_train.py:
 tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.00GiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.00GiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.00GiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:73] Allocating 10.57GiB bytes.
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:83] GPU 0 memory begins at 0x13047a0000 extends to 0x15a8b24400
F tensorflow/stream_executor/cuda/cuda_dnn.cc:220] could not find cudnnConvolutionBackwardFilter_v2 in cudnn DSO; dlerror: /usr/local/lib/libcudnn.so: undefined symbol: cudnnConvolutionBackwardFilter_v2
"
1356,Compile error ,"MacOS, master branch pull minutes ago.
I intended to try this `bazel build -c opt //tensorflow/tools/pip_package:build_pip_package`
### Bazel version:

Build label: 0.1.5
Build target: bazel-out/local_darwin-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Jan 21 12:53:53 2016 (1453380833)
Build timestamp: 1453380833
Build timestamp as int: 1453380833
I've also tried on 0.1.4, same error as below.
### gcc --version:

Configured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.11.sdk/usr/include/c++/4.2.1
Apple LLVM version 7.0.2 (clang-700.1.81)
Target: x86_64-apple-darwin15.3.0
Thread model: posix
### Logs:

ERROR: /Users/elin/gitHub_e-lin/tensorflow/tensorflow/contrib/linear_optimizer/BUILD:14:1: Linking of rule '//tensorflow/contrib/linear_optimizer:python/ops/_sdca_ops.so' failed: osx_gcc_wrapper.sh failed: error executing command 
  (cd /private/var/tmp/_bazel_elin/c89d25ef263422ce5293ac2a19ea0c36/tensorflow && \
  exec env - \
  external/bazel_tools/tools/cpp/osx_gcc_wrapper.sh -shared -o bazel-out/local_darwin-opt/bin/tensorflow/contrib/linear_optimizer/python/ops/_sdca_ops.so -Wl,-all_load bazel-out/local_darwin-opt/bin/tensorflow/contrib/linear_optimizer/_objs/python/ops/_sdca_ops.so/tensorflow/contrib/linear_optimizer/kernels/sdca_ops.pic.o bazel-out/local_darwin-opt/bin/tensorflow/contrib/linear_optimizer/_objs/python/ops/_sdca_ops.so/tensorflow/contrib/linear_optimizer/ops/sdca_ops.pic.o bazel-out/local_darwin-opt/bin/tensorflow/core/libframework_internal.pic.lo bazel-out/local_darwin-opt/bin/tensorflow/core/liblib_internal.pic.a bazel-out/local_darwin-opt/bin/external/jpeg_archive/libjpeg.pic.a bazel-out/local_darwin-opt/bin/external/png_archive/libpng.pic.a bazel-out/local_darwin-opt/bin/external/re2/libre2.pic.a bazel-out/local_darwin-opt/bin/tensorflow/core/libprotos_all_cc.pic.a bazel-out/local_darwin-opt/bin/google/protobuf/libprotobuf.pic.a bazel-out/local_darwin-opt/bin/google/protobuf/libprotobuf_lite.pic.a -Wl,-noall_load -Wl,-Bsymbolic -lm -ldl -lm -ldl -lz -pthread -lpthread -lstdc++ -undefined dynamic_lookup -no-canonical-prefixes): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: osx_gcc_wrapper.sh failed: error executing command 
  (cd /private/var/tmp/_bazel_elin/c89d25ef263422ce5293ac2a19ea0c36/tensorflow && \
  exec env - \
  external/bazel_tools/tools/cpp/osx_gcc_wrapper.sh -shared -o bazel-out/local_darwin-opt/bin/tensorflow/contrib/linear_optimizer/python/ops/_sdca_ops.so -Wl,-all_load bazel-out/local_darwin-opt/bin/tensorflow/contrib/linear_optimizer/_objs/python/ops/_sdca_ops.so/tensorflow/contrib/linear_optimizer/kernels/sdca_ops.pic.o bazel-out/local_darwin-opt/bin/tensorflow/contrib/linear_optimizer/_objs/python/ops/_sdca_ops.so/tensorflow/contrib/linear_optimizer/ops/sdca_ops.pic.o bazel-out/local_darwin-opt/bin/tensorflow/core/libframework_internal.pic.lo bazel-out/local_darwin-opt/bin/tensorflow/core/liblib_internal.pic.a bazel-out/local_darwin-opt/bin/external/jpeg_archive/libjpeg.pic.a bazel-out/local_darwin-opt/bin/external/png_archive/libpng.pic.a bazel-out/local_darwin-opt/bin/external/re2/libre2.pic.a bazel-out/local_darwin-opt/bin/tensorflow/core/libprotos_all_cc.pic.a bazel-out/local_darwin-opt/bin/google/protobuf/libprotobuf.pic.a bazel-out/local_darwin-opt/bin/google/protobuf/libprotobuf_lite.pic.a -Wl,-noall_load -Wl,-Bsymbolic -lm -ldl -lm -ldl -lz -pthread -lpthread -lstdc++ -undefined dynamic_lookup -no-canonical-prefixes): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
clang: warning: argument unused during compilation: '-pthread'
ld: warning: option -noall_load is obsolete and being ignored
ld: unknown option: -Bsymbolic
clang: error: linker command failed with exit code 1 (use -v to see invocation)
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 3.198s, Critical Path: 2.04s
"
1355,Resource exhausted,"This error happens when training several models in a for loop:

`W tensorflow/core/common_runtime/executor.cc:1076] 0x18ef02bb0 W tensorflow/core/common_runtime/executor.cc:1076] 0x18ef02bb0 Compute status: Resource exhausted: OOM when allocating tensor with shape
     [[Node: sub_2518 = Sub[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""](sub_2518/x, Variable_1955/read)]]
Traceback (most recent call last):
  File ""/media/konet/01D15611945D72C0/Pycharm_ubuntu/deep_neural_network_binary/dnn_127_models_binary.py"", line 398, in <module>
    fited = model.fit(Xtrain, ytrain, nb_epoch=n_epochs, batch_size=batch_size, show_accuracy=True, shuffle=True, validation_split=0.35)
  File ""/usr/local/lib/python2.7/dist-packages/keras/models.py"", line 581, in fit
    shuffle=shuffle, metrics=metrics)
  File ""/usr/local/lib/python2.7/dist-packages/keras/models.py"", line 239, in _fit
    outs = f(ins_batch)
  File ""/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py"", line 336, in __call__
    updated = session.run(self.outputs + self.updates, feed_dict=feed_dict)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 368, in run
    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 444, in _do_run
    e.code)
tensorflow.python.framework.errors.ResourceExhaustedError: OOM when allocating tensor with shapedim { size: 3200 } dim { size: 2000 }
     [[Node: mul_4305 = Mul[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""](Variable_1954/read, Variable_1956/read)]]
     [[Node: add_3103/_9711 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_423_add_3103"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
Caused by op u'mul_4305', defined at:
  File ""/media/konet/01D15611945D72C0/Pycharm_ubuntu/deep_neural_network_binary/dnn_127_models_binary.py"", line 387, in <module>
    model.compile(loss='binary_crossentropy', optimizer='adam', class_mode='binary')
  File ""/usr/local/lib/python2.7/dist-packages/keras/models.py"", line 440, in compile
    train_loss)
  File ""/usr/local/lib/python2.7/dist-packages/keras/optimizers.py"", line 262, in get_updates
    m_t = (self.beta_1 * m) + (1 - self.beta_1) * g
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py"", line 452, in <lambda>
    setattr(Variable, operator, lambda a, b: Variable._RunOp(operator, a, b))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py"", line 467, in _RunOp
    return getattr(ops.Tensor, operator)(a._AsTensor(), b)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py"", line 426, in binary_op_wrapper
    return func(x, y, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py"", line 728, in mul
    return _op_def_lib.apply_op(""Mul"", x=x, y=y, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py"", line 664, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1834, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1043, in __init__
    self._traceback = _extract_stack()
: OOM when allocating tensor with shape
     [[Node: sub_2518 = Sub[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""](sub_2518/x, Variable_1955/read)]]
Traceback (most recent call last):
  File ""/media/konet/01D15611945D72C0/Pycharm_ubuntu/deep_neural_network_binary/dnn_127_models_binary.py"", line 398, in <module>
    fited = model.fit(Xtrain, ytrain, nb_epoch=n_epochs, batch_size=batch_size, show_accuracy=True, shuffle=True, validation_split=0.35)
  File ""/usr/local/lib/python2.7/dist-packages/keras/models.py"", line 581, in fit
    shuffle=shuffle, metrics=metrics)
  File ""/usr/local/lib/python2.7/dist-packages/keras/models.py"", line 239, in _fit
    outs = f(ins_batch)
  File ""/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py"", line 336, in __call__
    updated = session.run(self.outputs + self.updates, feed_dict=feed_dict)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 368, in run
    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 444, in _do_run
    e.code)
tensorflow.python.framework.errors.ResourceExhaustedError: OOM when allocating tensor with shapedim { size: 3200 } dim { size: 2000 }
     [[Node: mul_4305 = Mul[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""](Variable_1954/read, Variable_1956/read)]]
     [[Node: add_3103/_9711 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_423_add_3103"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
Caused by op u'mul_4305', defined at:
  File ""/media/konet/01D15611945D72C0/Pycharm_ubuntu/deep_neural_network_binary/dnn_127_models_binary.py"", line 387, in <module>
    model.compile(loss='binary_crossentropy', optimizer='adam', class_mode='binary')
  File ""/usr/local/lib/python2.7/dist-packages/keras/models.py"", line 440, in compile
    train_loss)
  File ""/usr/local/lib/python2.7/dist-packages/keras/optimizers.py"", line 262, in get_updates
    m_t = (self.beta_1 * m) + (1 - self.beta_1) * g
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py"", line 452, in <lambda>
    setattr(Variable, operator, lambda a, b: Variable._RunOp(operator, a, b))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py"", line 467, in _RunOp
    return getattr(ops.Tensor, operator)(a._AsTensor(), b)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py"", line 426, in binary_op_wrapper
    return func(x, y, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py"", line 728, in mul
    return _op_def_lib.apply_op(""Mul"", x=x, y=y, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py"", line 664, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1834, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1043, in __init__
    self._traceback = _extract_stack()
`

Any advice on how to solve this?

Thank you
"
1354,How to decrease memory usage on GPU card,"When I run example like mnist on GPU card, I see that it consumes almost all device memory of that card. It seems like the tensorflow reserves as much memory as possible for memory pool at the start of process.
However, we need to run the tensor flow on a shared GPU card, on which runs other users' processes. So we can't occupy all device memory of it. Is there any switch to turn this memory pool off, or decrease the memory pool to a user defined size?
Thanks a lot in advance!
"
1353,AttributeError: 'module' object has no attribute 'truncated_normal',"Operating System: **Ubuntu 14.04 LTS**
Environment: **Python 2.7.11 :: Continuum Analytics, Inc.**
IDE: **Spyder and IPython**
TensorFlow Version: **0.7.1**
Pip Package File: **tensorflow-0.7.1-cp27-none-linux_x86_64.whl 64-Bit CPU ONLY**
Pip Package File from: [https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl](https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl)

Trying to follow tutorial from Kaggle to implement Convolutional NN. `import tensorflow` seems to execute without returning an error.

Code to reproduce:

```
In [74]: 
def weight_variable(shape):
     initial = tf.truncated_normal(shape, stddev=0.1) #Outputs random values from truncated normal distribution.
     return tf.Variable(initial)

In [75]: 
W_convl = weight_variable([5,5,1,32])

Traceback (most recent call last):

  File ""<ipython-input-75-65b9a0522cb2>"", line 1, in <module>
    W_convl = weight_variable([5,5,1,32])

  File ""<ipython-input-74-ae39e79c161a>"", line 2, in weight_variable
    initial = tf.truncated_normal(shape, stddev=0.1) #Outputs random values from truncated normal distribution.

AttributeError: 'module' object has no attribute 'truncated_normal'
```

I have tried:

`import tensorflow as tf`

And:

`from tensorflow import truncated_normal`

Not sure what is going on.
"
1352,fix build bug come from 8041c54,"For bugs/issues, please fill in the following.  The more information you
provide, the more likely we can help you.
### Environment info

Operating System: OSX

If installed from sources, provide the commit hash: 8041c54

**tensorflow/contrib/linear_optimizer/BUILD**

```
linkopts = [
     ""-Wl,-Bsymbolic"",
     ""-lm"",
],
```

**should change to:**

```
linkopts = select({
    ""//conditions:default"": [
     ""-Wl,-Bsymbolic"",
     ""-lm"",
    ],
    ""//tensorflow:darwin"": [],
}),
```
"
1350,nan gradients with low memory gpu,"### Environment info

Operating System: 
`3.12.9-201.fc19.x86_64 #1 SMP Wed Jan 29 15:44:35 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux`

GPU card info from `lspci -v | grep gtx -i`

```
NVIDIA Corporation GK110 [GeForce GTX Titan] (rev a1)
```

If installed from binary pip package, provide:
1. pip package installed:
   `https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl`
2. The output from python -c ""import tensorflow; print(tensorflow.**version**)"".

```
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally                                                                             
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally                                                                              
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally                                                                              
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally                                                                             
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally                                                                             
0.7.1                                                                        
```
### Steps to reproduce
1. Increase batch size, so that you see the following warning message
2. check the gradients, and there are `nan` values
3. decrease batch size, and do the same check. Everything is normal if I don't see the following warning message
### What have you tried?
1. reduce batch size
### Logs or other output that would be helpful

```
W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:131] Ran out of memory trying to allocate 1.85GiB. The caller indicates that this is not a failure, but may mean that th
ere could be performance gains if more memory is available.
```
"
1349,Question about back propagation in time-delayed recurrent models,"The full question is available here: http://stackoverflow.com/questions/35733302/back-propagation-in-tensorflow

The question concerns whether I would have to implement a custom backward step in order to create something similar to the [clockwork model](http://jmlr.org/proceedings/papers/v32/koutnik14.pdf). In the event that I would, I was wondering if there is reference code for how I might properly go about it.

Thank you in advance for your support.
"
1348,Error when running User Defined Op Tutorial,"For bugs/issues, please fill in the following.  The more information you
provide, the more likely we can help you.
### Environment info

Operating System:  Tried both on OS X and Rhel7

If installed from binary pip package, provide:
1. Which pip package you installed.

sudo pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.7.1-cp27-none-any.whl
2. The output from python -c ""import tensorflow; print(tensorflow.**version**)"".

0.7.1

If installed from sources, provide the commit hash:
### Steps to reproduce
1. Follow instructions from https://www.tensorflow.org/versions/master/how_tos/adding_an_op/index.html
2. Attempt to build zero out: 

```
g++ -v -std=c++11 -shared zero_out.cc -o zero_out.so \
-I $TF_INC -l tensorflow_framework -L $TF_LIB \
-fPIC -Wl,-rpath $TF_LIB
```
1.  Get Error:

```
 g++ -v -std=c++11 -shared zero_out.cc -o zero_out.so -I $TF_INC -l tensorflow_framework -L $TF_LIB -fPIC -Wl,-rpath /usr/local/lib/python2.7/site-packages/
Apple LLVM version 7.0.2 (clang-700.1.81)
Target: x86_64-apple-darwin14.5.0
Thread model: posix
 ""/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/clang"" -cc1 -triple x86_64-apple-macosx10.10.0 -Wdeprecated-objc-isa-usage -Werror=deprecated-objc-isa-usage -emit-obj -mrelax-all -disable-free -disable-llvm-verifier -main-file-name zero_out.cc -mrelocation-model pic -pic-level 2 -mthread-model posix -mdisable-fp-elim -masm-verbose -munwind-tables -target-cpu core2 -target-linker-version 253.9 -v -dwarf-column-info -resource-dir /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../lib/clang/7.0.2 -I /usr/local/lib/python2.7/site-packages/tensorflow/include -stdlib=libc++ -std=c++11 -fdeprecated-macro -fdebug-compilation-dir /usr/local/lib/python2.7/site-packages/tensorflow/include/tensorflow/core/user_ops -ferror-limit 19 -fmessage-length 158 -stack-protector 1 -mstackrealign -fblocks -fobjc-runtime=macosx-10.10.0 -fencode-extended-block-signature -fcxx-exceptions -fexceptions -fmax-type-align=16 -fdiagnostics-show-option -fcolor-diagnostics -o /var/folders/pg/fbr4jqk15zz3wx4b2h1pgzlr002cz4/T/zero_out-31d965.o -x c++ zero_out.cc
clang -cc1 version 7.0.2 based upon LLVM 3.7.0svn default target x86_64-apple-darwin14.5.0
ignoring nonexistent directory ""/usr/include/c++/v1""
#include ""..."" search starts here:
#include <...> search starts here:
 /usr/local/lib/python2.7/site-packages/tensorflow/include
 /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1
 /usr/local/include
 /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../lib/clang/7.0.2/include
 /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include
 /usr/include
 /System/Library/Frameworks (framework directory)
 /Library/Frameworks (framework directory)
End of search list.
In file included from zero_out.cc:1:
In file included from /usr/local/lib/python2.7/site-packages/tensorflow/include/tensorflow/core/framework/op.h:22:
/usr/local/lib/python2.7/site-packages/tensorflow/include/tensorflow/core/framework/op_def.pb.h:9:10: fatal error: 'google/protobuf/stubs/common.h' file not
      found
#include <google/protobuf/stubs/common.h>
         ^
1 error generated.
```
"
1347,Embedding Lookup - shared weights: Dimensions are not compatible error,"### Environment info

Operating System: Linux

Installed from sources, commit hash: 7b5fef17806e2138ef16e8399ef5b1078d404dbe
Python 2.7.11 |Anaconda 2.3.0 (64-bit)| (default, Dec  6 2015, 18:08:32)
### Steps to reproduce

``` python
import tensorflow as tf

vocab_size = 1337
embedding_size = 100
batch_size = 512
Qmax = 19
Tmax = 17

W = tf.Variable(
    tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0) ,name=""W"")

#W2 = tf.Variable(
#    tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0) ,name=""W2"")

_q = tf.placeholder(""int32"", [batch_size, Qmax])
_t = tf.placeholder(""int32"", [batch_size, Tmax])

A = tf.reduce_mean(tf.nn.embedding_lookup(W, _q), [1])

# DOES NOT WORK
B = tf.reduce_mean(tf.nn.embedding_lookup(W, _t), [1])

# WORKS
#B = tf.reduce_mean(tf.nn.embedding_lookup(W2, _t), [1])

cost = tf.reduce_mean(tf.add(A, B))

optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)
```
### What have you tried?
1. Commented lines seem to mitigate the problem, but then the embedding matrix is not shared, which is desired for my use case.
### Logs or other output that would be helpful

```
jd@jd-cuda:~$ python tensor_fail.py 
I tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcublas.so.7.0 locally
I tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcudnn.so.6.5 locally
I tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcufft.so.7.0 locally
I tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcuda.so locally
I tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcurand.so.7.0 locally
Traceback (most recent call last):
  File ""tensor_fail.py"", line 28, in <module>
    optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)
  File ""/home/jd/anaconda/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py"", line 186, in minimize
    aggregation_method=aggregation_method)
  File ""/home/jd/anaconda/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py"", line 232, in compute_gradients
    aggregation_method=aggregation_method)
  File ""/home/jd/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py"", line 416, in gradients
    aggregation_method)
  File ""/home/jd/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py"", line 638, in _AggregatedGrads
    for x in out_grad]), out_grad[0].dense_shape)
  File ""/home/jd/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py"", line 309, in concat
    name=name)
  File ""/home/jd/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 70, in _concat
    name=name)
  File ""/home/jd/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py"", line 664, in apply_op
    op_def=op_def)
  File ""/home/jd/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1836, in create_op
    set_shapes_for_outputs(ret)
  File ""/home/jd/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1476, in set_shapes_for_outputs
    shapes = shape_func(op)
  File ""/home/jd/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py"", line 364, in _ConcatShape
    concat_dim + 1:].merge_with(value_shape[concat_dim + 1:])
  File ""/home/jd/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py"", line 530, in merge_with
    new_dims.append(dim.merge_with(other[i]))
  File ""/home/jd/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py"", line 119, in merge_with
    self.assert_is_compatible_with(other)
  File ""/home/jd/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py"", line 94, in assert_is_compatible_with
    % (self, other))
ValueError: Dimensions Dimension(19) and Dimension(17) are not compatible
```

I would be grateful for any help.

Best regards,
## 

Jacek Dabrowski
"
1346,CUDA compilation on ArchLinux fails,"## Overview

Build fails with 

```
/usr/include/string.h: In function 'void* __mempcpy_inline(void*, const void*, size_t)':
/usr/include/string.h:652:42: error: 'memcpy' was not declared in this scope
     return (char *) memcpy (__dest, __src, __n) + __n;
```
## Specific information:
- ArchLinux system
- Compute Capability 3.0 necessary
- CuDNN 4, CUDA 7.5
- Bazel 0.2.0
#### Followed the installation from source instructions:
- CuDNN 4, CUDA 7.5 are installed in /opt/cuda
- `git clone -b v0.7.1 --recurse-submodules https://github.com/tensorflow/tensorflow`
- `cd tensorflow`
- `./configure` Details as attached. Building with cuda support and compute capability 3.0
- `~/bin/bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer`
  This results in the following error:

```

/usr/lib/gcc/x86_64-unknown-linux-gnu/5.3.0/include/mwaitxintrin.h(36): error: identifier ""__builtin_ia32_monitorx"" is undefined

/usr/lib/gcc/x86_64-unknown-linux-gnu/5.3.0/include/mwaitxintrin.h(42): error: identifier ""__builtin_ia32_mwaitx"" is undefined

2 errors detected in the compilation of ""/tmp/tmpxft_00006c12_00000000-7_avgpooling_op_gpu.cu.cpp1.ii"".
```

This is probably due to cuda requiring gcc 4.9, while I am using 5.3. I fixed this by editing `crosstool_wrapper_driver_is_not_gcc` in `tensorflow/third_party/gpus/crosstool/clang/bin`, changing line 49 to the explicit
`GCC_HOST_COMPILER_PATH = ('/usr/bin/gcc-4.9')`
This allows compilation to continue; however, it then fails with 

```
/usr/include/string.h: In function 'void* __mempcpy_inline(void*, const void*, size_t)':
/usr/include/string.h:652:42: error: 'memcpy' was not declared in this scope
   return (char *) memcpy (__dest, __src, __n) + __n;
                                          ^
ERROR: /home/RED/src/tensorflow/tensorflow/core/BUILD:334:1: output 'tensorflow/core/_objs/gpu_kernels/tensorflow/core/kernels/aggregate_ops_gpu.cu.o' was not created.
ERROR: /home/RED/src/tensorflow/tensorflow/core/BUILD:334:1: not all outputs were created.
Target //tensorflow/cc:tutorials_example_trainer failed to build
```

Additional information:
- I had been able to compile 0.6.0 (though with CUDA 7.0 and CuDNN 2). This also necessitated switching GCC.
- CPU-Only compilation works.
- Wheel installation works, too, but is obviously unable to use my CC3.0 device.

Thanks!

Attachments:
- [tensorflow_configure.txt](https://github.com/tensorflow/tensorflow/files/153447/tensorflow_configure.txt)
- [tensorflow_output.txt](https://github.com/tensorflow/tensorflow/files/153446/tensorflow_output.txt)
"
1344,op.device no longer showing which device an op is actually placed on,"Using the latest builds, it appears that `op.device` now returns the device that an op is supposed to be placed on, i.e. what the user specifies, as opposed to what it is actually placed on. I believe this is true because when I set `allow_soft_placement` to `False` when starting a `Session`, some ops, for example `global_step`, cannot be placed on the GPU. But if I assign them to the GPU and then allow soft placement, `op.device` returns `/gpu:0` for ops that were previously not placeable on the GPU.

This is a break from previous behavior, which showed the actual device an op is placed on. The new behavior makes it very difficult to debug device placement. Is there are any way to probe the actual device that an op is placed on?

P.S. I'm using a device function for my placement logic. Something like:

```
def _device_function(op):
    if complicated_logic_true:
        return '/cpu:0'
    else:
        return '/gpu:0'
```
"
1343,sigmoid cross entropy doesn't verify shapes,"Hello,

I have found a misleading behaviour in `sigmoid_cross_entropy_with_logits` function. If I pass a tensor of shape `(n, 1)` as logits and a tensor of shape `(n,)` as targets it outputs a tensor of `(n, n)` which is obviously different from tensor `(n,)` if both tensors have the same shape `(n,)`. Although api docs say that logits and targets must have the same shape, this example doesn't fail properly nor broadcast correctly.

OS: Fedora 21
TF v0.7.1 compiled from sources (tagged commit).
"
1342,download link of pdf version of the documentation/get_startred/tutorials etc,"Tried a lot of searching, but I am still not able to find a pdf version of the tensorflow docs. Kindly provide the same.
"
1341,cifar10_eval.py stop problem,"### Environment info

Operating System: Ubuntu 14.04.4 LTS

If installed from binary pip package, provide:
1. Which pip package you installed.
   : tensorflow-0.7.1-cp27-none-linux_x86_64.whl
2. The output from python -c ""import tensorflow; print(tensorflow.**version**)"".
   : 0.7.1
### Steps to reproduce
1. cd git/tensorflow/tensorflow/models/image/cifar10/
2. python cifar10_eval.py

I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 
name: GeForce GTX 980 Ti
major: 5 minor: 2 memoryClockRate (GHz) 1.3545
pciBusID 0000:01:00.0
Total memory: 6.00GiB
Free memory: 5.56GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:717] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980 Ti, pci bus id: 0000:01:00.0)
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 32.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 64.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 128.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 256.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 512.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 32.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 64.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 128.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 256.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 512.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.00GiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.00GiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.00GiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.00GiB

and stop this line.

but sometimes not stop this line when run same as above. and

==========check3.25==========
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:73] Allocating 5.27GiB bytes.
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:83] GPU 0 memory begins at 0x706400000 extends to 0x857ab6000
==========check3.5==========
==========check3.75==========
==========check3.25==========

.
.
.

==========check3.25==========
==========check3.5==========
==========check3.75==========
2016-03-01 15:41:09.935657: precision @ 1 = 0.801
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fb37405e7b0 Compute status: Cancelled: Enqueue operation was cancelled
     [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueMany[Tcomponents=[DT_STRING], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](input_producer, input_producer/RandomShuffle)]]
I tensorflow/core/kernels/queue_base.cc:286] Skipping cancelled enqueue attempt
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fb350035000 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.
     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fb36403ada0 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.
     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fb3223f5240 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.
     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fb35c03ada0 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.
     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fb338035000 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.
     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fb36c00c600 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.
     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fb354035000 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.
     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fb340035000 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.
     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fb34c035000 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.
     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fb3480095e0 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.
     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fb33000c470 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.
     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fb370011050 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.
     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fb328010190 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.
     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fb3600095e0 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.
     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fb358035000 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.
     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fb33c05b3b0 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.
     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]
### What have you tried?
1.  Add some instruction for checking stop point.

cifar10_eval.py

def eval_once(saver, summary_writer, top_k_op, summary_op):

```
# Start the queue runners.
coord = tf.train.Coordinator()
try:
  threads = []
  for qr in tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS):
    threads.extend(qr.create_threads(sess, coord=coord, daemon=True,
                                     start=True))
  num_iter = int(math.ceil(FLAGS.num_examples / FLAGS.batch_size))
  true_count = 0  # Counts the number of correct predictions.
  total_sample_count = num_iter * FLAGS.batch_size
  step = 0
  while step < num_iter and not coord.should_stop():
    print(""==========check3.25=========="")
    predictions = sess.run([top_k_op])
    print(""==========check3.5=========="")
    true_count += np.sum(predictions)
    step += 1
    print(""==========check3.75=========="")

  # Compute precision @ 1.
  precision = true_count / total_sample_count
  print('%s: precision @ 1 = %.3f' % (datetime.now(), precision))

  summary = tf.Summary()
  summary.ParseFromString(sess.run(summary_op))
  summary.value.add(tag='Precision @ 1', simple_value=precision)
  summary_writer.add_summary(summary, global_step)
except Exception as e:  # pylint: disable=broad-except
  coord.request_stop(e)

coord.request_stop()
coord.join(threads, stop_grace_period_secs=10)
```

look like #389 issue but different. and many time stop 

predictions = sess.run([top_k_op])

this line. 
"
1340,Error in installing the tensorflow on 32 bit Ubuntu 14.04 LTS,"Hi Developers:

Please, guide me to fix the issue and help me to install the tensorflow on the 32bit Ubuntu 14.04 LTS.

Error i am getting :
hunter@hunter:~$ sudo pip install --upgrade 
![screenshot from 2016-03-01 13 00 32](https://cloud.githubusercontent.com/assets/9070249/13420607/b4ccc094-dfad-11e5-9e32-fd9880f4056f.png)
https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl

tensorflow-0.7.1-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.
Storing debug log for failure in /home/hunter/.pip/pip.log

Thanks you for your time in advance.
"
1336,pip install/upgrade version 0.7.1 issue -- ImportError: No module named protobuf,"Had a previously working version of tensorflow 0.6

Executed single command which appeared to proceed without incident:

`sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl`

However, now protobuf not found during python runtime though pip seems to have installed protobuf==3.0.0b2 
### Environment info

Operating System: Centos 7

If installed from binary pip package, provide:
1. Which pip package you installed.

`[root@localhost ~]# sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl

Collecting tensorflow==0.7.1 from https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl

  Using cached https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl

/usr/lib/python2.7/site-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:315: 

SNIMissingWarning: An HTTPS request has been made, but the SNI (Subject Name Indication) extension to TLS is not available on this platform. This may cause the server to present an incorrect TLS certificate, which can cause validation failures. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#snimissingwarning.
  SNIMissingWarning

/usr/lib/python2.7/site-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:120: 
InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning.
  InsecurePlatformWarning

Requirement already up-to-date: six>=1.10.0 in /usr/lib/python2.7/site-packages (from tensorflow==0.7.1)

Requirement already up-to-date: protobuf==3.0.0b2 in /usr/lib/python2.7/site-packages (from tensorflow==0.7.1)

Requirement already up-to-date: wheel in /usr/lib/python2.7/site-packages (from tensorflow==0.7.1)

Requirement already up-to-date: numpy>=1.8.2 in /usr/lib64/python2.7/site-packages (from tensorflow==0.7.1)

Requirement already up-to-date: setuptools in /usr/lib/python2.7/site-packages (from protobuf==3.0.0b2->tensorflow==0.7.1)

Installing collected packages: tensorflow

Successfully installed tensorflow-0.7.1
`
1. The output from python -c ""import tensorflow; print(tensorflow.**version**)"".

`Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/usr/lib/python2.7/site-packages/tensorflow/**init**.py"", line 23, in <module>
    from tensorflow.python import *
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/**init**.py"", line 41, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/**init**.py"", line 35, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""/usr/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>
    from google.protobuf import descriptor as _descriptor
ImportError: No module named protobuf

Error importing tensorflow.  Unless you are using bazel,
you should not try to import tensorflow from its source directory;
please exit the tensorflow source tree, and relaunch your python interpreter
from there.
`
"
1334,Compile issue,"Ubuntu 14, master branch pull minutes ago.

ERROR: /home/ggg/000/tensorflow/tensorflow/core/kernels/BUILD:712:1: undeclared inclusion(s) in rule '//tensorflow/core/kernels:depthwise_conv_op':
this rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/depthwise_conv_op.cc':
  '/home/ggg/000/tensorflow/tensorflow/core/common_runtime/device.h'
  '/home/ggg/000/tensorflow/tensorflow/core/graph/graph.h'
  '/home/ggg/000/tensorflow/tensorflow/core/graph/edgeset.h'
  '/home/ggg/000/tensorflow/tensorflow/core/graph/types.h'.
Target //tensorflow/cc:tutorials_example_trainer failed to build
"
1333,Problem building label_image,"Hi all, 
I tried to build the label_image example (I have tensor flow version 0.7 and I installed bazel) but when I build the example with
`../../bin/bazel build tensorflow/examples/label_image/...`
I get:
`INFO: Found 2 targets...
ERROR: /home/informatica/.cache/bazel/_bazel_informatica/91a9bba81c77fb078ff5ab9ac13df451/external/png_archive/BUILD:23:1: Executing genrule @png_archive//:configure failed: bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped).
/home/informatica/.cache/bazel/_bazel_informatica/91a9bba81c77fb078ff5ab9ac13df451/tensorflow/external/png_archive/libpng-1.2.53 /home/informatica/.cache/bazel/_bazel_informatica/91a9bba81c77fb078ff5ab9ac13df451/tensorflow
/tmp/tmp.dPQmFTp3hh /home/informatica/.cache/bazel/_bazel_informatica/91a9bba81c77fb078ff5ab9ac13df451/tensorflow/external/png_archive/libpng-1.2.53 /home/informatica/.cache/bazel/_bazel_informatica/91a9bba81c77fb078ff5ab9ac13df451/tensorflow`

At the end of this, in bazel-bin/tensorflow/examples/label_image I have no executable file, only a manifest and a param file.
What's wrong?
"
1330,Problematic links in official website,"In https://www.tensorflow.org/versions/r0.7/how_tos/tool_developers/index.html, some of the links that referred to this repository contain ""%0A"" in random places which make them invalid. 
"
1329,Our shell wrapper around swig is a bug according to bazel,"Bazel likes to strip things it doesn't like out of `PATH`, as discussed at https://github.com/bazelbuild/bazel/issues/957.  For TensorFlow, this is a problem since we have a shell wrapper around swig that just says `swig ""$@""`.  In my case, `swig` is in `~/homebrew/bin`.

Since Bazel considers this a bug in TensorFlow rather than in Bazel, it may be more useful to have the discussion here than on the Bazel site.
"
1328,default initializer for seq2seq example,"What is the default initialization for the weight in seq2seq example? I can find it no where, it is completely not defined.
"
1326,Add forget_bias option to LSTMCell,"Currently `BasicLSTMCell` accepts a `forget_bias` option but `LSTMCell` does not and always defaults to a forget bias of 1 that cannot be controlled by the user.
"
1325,tf.get_variable() cannot recognize existing variables,"`with tf.variable_scope(""conv"", reuse=True):
    x = tf.get_variable(""w"",[1])`

The above code cannot recognize an existing variable, but clearly the existing variable was created before as print out of the variable .name shows : `conv/w:0`

I get an error when using tf.get_variable:
`ValueError: Under-sharing: Variable conv/w does not exist, disallowed. Did you mean to set reuse=None in VarScope?`

If set to reuse=None...
`with tf.variable_scope(""conv"", reuse=None):
    x = tf.get_variable(""w"",[1])`

Then it creates another variable with .name  `conv/w_1:0`

It's a bug! Now I have 2 variables, with names `'conv/w'`  and `'conv/w_1'`
"
1324,GLIBC error,"I searched for this issue but did not find any useful way. I installed tensorflow with pip and I am using Ubuntu 12.04 and python 2.7.3.
When in python I write import tensorflow, it raises the following error:

ImportError: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.17' not found (required by /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so)

Any useful suggestion?
Thanks
"
1323,BasicLSTMCell cannot calculate output shape correctly when batchsize is 1,"I found that `BasicLSTMCell` cannot infer output shape correctly when the input batchsize is 1, after my graph construction failed within this case.

Tensorflow version: 0.7.1, installed with pip. Operating system: Ubuntu 14.04

Minimum code snip to reproduce:

```
import tensorflow as tf

batchsize = 1
# batchsize = 2
D_in, D_out = 1000, 2000

input_ = tf.zeros([batchsize, D_in], tf.float32)
lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(D_out)
state = lstm_cell.zero_state(batchsize, tf.float32)

output, _ = lstm_cell(input_, state)
print(output.get_shape().as_list())
```

When `batchsize = 2`, the shape of output is `[2, 2000]`, with the first dimension equal to batchsize. However, when `batchsize = 1`, the shape of output is `[None, 2000]` instead of `[1, 2000]`.

I'm not sure why the output shape is not correctly inferred as `[1, 2000]` when `batchsize = 1`. Could someone please have a look?
"
1322,Custom padding for convolutions,"For the moment it looks like TF only supports VALID or SAME convolutions ? Is there a clever work around to do custom padding convolutions like having a padding of length h defined by the user like in this [neon implementation](http://neon.nervanasys.com/docs/latest/generated/neon.layers.layer.Conv.html) ? Is implementing it in the pipeline ?
"
1319,Unable to execute example: models/image/mnist/convolutional.py on MacOS,"For bugs/issues, please fill in the following.  The more information you
provide, the more likely we can help you.
### Environment info

Operating System: Mac OS (El Capitan / Python 2.7)

If installed from binary pip package, provide:
1. Which pip package you installed.
   Virtualenv
2. The output from python -c ""import tensorflow; print(tensorflow.**version**)"".
   0.7.1

If installed from sources, provide the commit hash:
### Steps to reproduce
1. python /Users/foo/tensorflow/lib/python2.7/site-packages/tensorflow/models/image/mnist/convolutional.py
### Logs or other output that would be helpful

Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.
Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.
Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.
Extracting data/train-images-idx3-ubyte.gz
Traceback (most recent call last):
  File ""/Users/yuntatsai/tensorflow/lib/python2.7/site-packages/tensorflow/models/image/mnist/convolutional.py"", line 316, in <module>
    tf.app.run()
  File ""/Users/yuntatsai/tensorflow/lib/python2.7/site-packages/tensorflow/python/platform/default/_app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""/Users/yuntatsai/tensorflow/lib/python2.7/site-packages/tensorflow/models/image/mnist/convolutional.py"", line 128, in main
    train_data = extract_data(train_data_filename, 60000)
  File ""/Users/yuntatsai/tensorflow/lib/python2.7/site-packages/tensorflow/models/image/mnist/convolutional.py"", line 75, in extract_data
    buf = bytestream.read(IMAGE_SIZE \* IMAGE_SIZE \* num_images)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/gzip.py"", line 268, in read
    self._read(readsize)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/gzip.py"", line 315, in _read
    self._read_eof()
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/gzip.py"", line 354, in _read_eof
    hex(self.crc)))
IOError: CRC check failed 0x381c74b0 != 0x805e6c18L
"
1318,Lua API,"Hi!

Is somebody working on a Lua API? 
If not, are you interested on contributing to develop a Lua API?

Thanks!
"
1317,"tf.get_variable() behavior somewhat inconsistent across reuse=True, reuse=None","By default, calling `tf.get_variable()` for a variable that does not exist yet creates that variable in the current scope, if the scope has `reuse=Nne`. However, if the scope has reuse=True, then  a Under-sharing error is raised.

``` python
import tensorflow as tf
with tf.variable_scope(""scope1"",reuse=None):
  x=tf.get_variable(""x"", [1,1]) # this works
with tf.variable_scope(""rnnscope"",reuse=True):
  w=tf.get_variable(""w"",[1,1]) # this fails
ValueError: Under-sharing: Variable rnnscope/w does not exist, disallowed. Did you mean to set reuse=None in VarScope?
```

I think I understand the motivation behind this behavior - variables retrieved in a reuse=True scope should be in ""reuse-mode"", so an error should be raised if it hasn't been created yet.

The way I get around this is a global variable DO_SHARE=None that is permanently set to True for all t > 0. 

An example implementation of building an RNN this way is here: https://github.com/ericjang/draw/blob/master/draw.py#L163

However, this seems a bit inelegant - wouldn't it be easier to just have `get_variable()` create nonexistent variables, regardless of the value of `reuse`? Perhaps there is a better way to build RNNs that I'm not aware of?
### Environment info

Operating System: **Debian**

If installed from binary pip package, provide:
1. Which pip package you installed.
   **GPU-enabled wheel for Linux**
2. The output from python -c ""import tensorflow; print(tensorflow.**version**)"".
   **0.6.0**
### Steps to reproduce
### What have you tried?

1.
### Logs or other output that would be helpful

(If logs are large, please upload as attachment).
"
1311,"dynamic_rnn incompatible with BasicRNNCell, GRUCell, and BasicLSTMCell","### Description

I've been playing around with `rnn.dynamic_rnn` and have thus far only had success with it when using `rnn_cell.BasicLSTMCell` instances. When using any other cell, a `TypeError` is raised. Looking at the traceback, the culprit appears to be the calls to `rnn_cell.linear` at each time step, which fail when `rnn_cell.linear` attempts to get the shape of its input slice. This, in turn, fails because the input slices are produced from a `TensorArray` and have shape `<unknown>`.
### Environment info

Operating System: Ubuntu 14.04 LTS
Installed from source at:  b88971051fbc49fa1e0b91ec1b0b60defa11697e
### Steps to reproduce

The following will attempt to call `dynamic_rnn` with three different cells, catch the `TypeError` that is raised, then print the exception + traceback to stdout.

```
import sys
import traceback
import tensorflow as tf
from tensorflow.python.ops.rnn import dynamic_rnn

inputs = tf.placeholder('float32', [None, None, 32])
mbsz = tf.shape(inputs)[0]
max_seq_len = tf.shape(inputs)[1]
sequence_length = tf.fill(tf.expand_dims(mbsz, 0), max_seq_len)

n_units = 32
cells = [tf.nn.rnn_cell.BasicRNNCell(n_units),
    tf.nn.rnn_cell.GRUCell(n_units),
    tf.nn.rnn_cell.BasicLSTMCell(n_units)]
for cell in cells:
    print('='*40)
    print('Attempting with: %s' % cell.__class__.__name__)
    print('='*40)
    try:
        rnn_out, _ = dynamic_rnn(cell, inputs, sequence_length, dtype='float32')
    except TypeError as e:
        tb = sys.exc_traceback
        traceback.print_exception(TypeError, e, tb)
        print('\n'*3)
```
### Output of above snippet

```
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so.7.0 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so.4.0.4 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so.7.0 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so.7.0 locally
========================================
Attempting with: BasicRNNCell
========================================
Traceback (most recent call last):
  File ""<ipython-input-1-aa87c2141668>"", line 20, in <module>
    rnn_out, _ = dynamic_rnn(cell, inputs, sequence_length, dtype='float32')
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"", line 432, in dynamic_rnn
    parallel_iterations=parallel_iterations)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"", line 501, in _dynamic_rnn_loop
    parallel_iterations=parallel_iterations)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1590, in While
    result = context.BuildLoop(cond, body, loop_vars)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1507, in BuildLoop
    body_result = body(*vars_for_body_with_tensor_arrays)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"", line 491, in _time_step
    zero_output, state, lambda: cell(input_t, state))
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"", line 218, in _rnn_step
    time < max_sequence_length, call_cell, empty_update)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1210, in cond
    res_t = context_t.BuildCondBranch(fn1)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1133, in BuildCondBranch
    r = fn()
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"", line 491, in <lambda>
    zero_output, state, lambda: cell(input_t, state))
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell.py"", line 122, in __call__
    output = tanh(linear([inputs, state], self._num_units, True))
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell.py"", line 692, in linear
    shapes = [a.get_shape().as_list() for a in args]
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py"", line 738, in as_list
    return [dim.value for dim in self._dims]
TypeError: 'NoneType' object is not iterable




========================================
Attempting with: GRUCell
========================================
Traceback (most recent call last):
  File ""<ipython-input-1-aa87c2141668>"", line 20, in <module>
    rnn_out, _ = dynamic_rnn(cell, inputs, sequence_length, dtype='float32')
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"", line 432, in dynamic_rnn
    parallel_iterations=parallel_iterations)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"", line 501, in _dynamic_rnn_loop
    parallel_iterations=parallel_iterations)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1590, in While
    result = context.BuildLoop(cond, body, loop_vars)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1507, in BuildLoop
    body_result = body(*vars_for_body_with_tensor_arrays)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"", line 491, in _time_step
    zero_output, state, lambda: cell(input_t, state))
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"", line 218, in _rnn_step
    time < max_sequence_length, call_cell, empty_update)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1210, in cond
    res_t = context_t.BuildCondBranch(fn1)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1133, in BuildCondBranch
    r = fn()
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"", line 491, in <lambda>
    zero_output, state, lambda: cell(input_t, state))
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell.py"", line 151, in __call__
    2 * self._num_units, True, 1.0))
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell.py"", line 692, in linear
    shapes = [a.get_shape().as_list() for a in args]
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py"", line 738, in as_list
    return [dim.value for dim in self._dims]
TypeError: 'NoneType' object is not iterable




========================================
Attempting with: BasicLSTMCell
========================================
Traceback (most recent call last):
  File ""<ipython-input-1-aa87c2141668>"", line 20, in <module>
    rnn_out, _ = dynamic_rnn(cell, inputs, sequence_length, dtype='float32')
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"", line 432, in dynamic_rnn
    parallel_iterations=parallel_iterations)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"", line 501, in _dynamic_rnn_loop
    parallel_iterations=parallel_iterations)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1590, in While
    result = context.BuildLoop(cond, body, loop_vars)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1507, in BuildLoop
    body_result = body(*vars_for_body_with_tensor_arrays)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"", line 491, in _time_step
    zero_output, state, lambda: cell(input_t, state))
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"", line 218, in _rnn_step
    time < max_sequence_length, call_cell, empty_update)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1210, in cond
    res_t = context_t.BuildCondBranch(fn1)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1133, in BuildCondBranch
    r = fn()
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"", line 491, in <lambda>
    zero_output, state, lambda: cell(input_t, state))
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell.py"", line 203, in __call__
    concat = linear([inputs, h], 4 * self._num_units, True)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell.py"", line 692, in linear
    shapes = [a.get_shape().as_list() for a in args]
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py"", line 738, in as_list
    return [dim.value for dim in self._dims]
TypeError: 'NoneType' object is not iterable
```
"
1310,Could not specify explicit device specification '',"Several users have reported an issue that occurs with the following steps (e.g. Issue #1297):
1. Start TensorFlow in a setting with a GPU.
2. Define some variables with no explicit `tf.device()` set. This most often happens with embedding variables (used as arguments to `tf.gather()` or `tf.embedding_lookup()`).
3. Initialize them. (They will be placed on the GPU, because it is the ""best available device"".)
4. On running the first training step, the following error (or similar) is raised:

```
InvalidArgumentError: Cannot assign a device to node 'Adagrad/update_Variable_2/SparseApplyAdagrad': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/GPU:0'
```

The `SparseApplyAdagrad` op (or in general most `SparseApplyFoo` ops) is only defined on CPU. The variable has already been placed on GPU, so the graph is not placeable. Attempting to run the same program with `use_soft_placement=True` also fails, although with a stranger error.

**TL;DR:** If this affects you, create your variables in a `with tf.device(""/cpu:0""):` block, until this issue is resolved.

The issue arises because (i) TensorFlow places variables on the first device where they run, (ii) it always prefers GPU over CPU when it is availabe, (iii) initialization ops are available on GPU, and (iv) it applies the placement algorith to the _pruned subgraph_ (not the entire client graph).

One workaround would be to apply the placement algorithm to the entire client graph. (This is the approach used in the separate `master_session.cc`/`simple_graph_execution_state.cc` codepath, used in the distributed runtime.) However, this has the effect of leaving the session in a broken state as soon as an unplaceable node is encountered. Switching to this behavior might cause issues for people doing exploratory graph construction in a REPL (IPython etc.) because the only remedy would be to recreate the entire graph on seeing such an error. Therefore, while failing fast in a non-interactive setting would be fine, a different solution for interactive use might be required.
"
1308,ImportError: cannot import name server,"Operating System:
Ubuntu 14.04

Built tensor board from the source (from tensor flow git repo)

`sudo apt-get install nodejs nodejs-legacy npm`

`sudo npm install -g gulp bower`

cd tensorflow/tensorflow/tensorboard/

`npm install`

`bower install`

Everything goes fine. Now I try to run the TensorFlow using command

`>> python tensorflow/tensorflow/tensorboard/tensorboard.py --logdir=bla_logs`

```
Traceback (most recent call last):
  File ""tensorflow/tensorflow/tensorboard/tensorboard.py"", line 33, in <module>
    from tensorflow.tensorboard.backend import server
ImportError: cannot import name server
```

Then I changed the following line in tensorboard.py 

`from tensorflow.tensorboard.backend import server`

to

`from tensorflow.tensorboard.backend import tensorboard_server as server`

Now it works fine. 
"
1307,"Python import error, undefined symbol: secure_getenv","I got python import error, undefined symbol: secure_getenv

Operating System:
Redhat 6.7

Installed from sources using gcc 4.8 (I am using gcc 4.8 installed in a custom path since the default gcc is too old):
commit b88971051fbc49fa1e0b91ec1b0b60defa11697e
Date:   Fri Feb 26 05:08:35 2016 -0800

```
Python 2.7.11 |Anaconda 2.2.0 (64-bit)| (default, Dec  6 2015, 18:08:32)
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Anaconda is brought to you by Continuum Analytics.
Please check out: http://continuum.io/thanks and https://anaconda.org
>>> import tensorflow
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/nfs/m01/anaconda/lib/python2.7/site-packages/tensorflow/__init__.py"", line 23, in <module>
    from tensorflow.python import *
  File ""/nfs/m01/anaconda/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow import contrib
  File ""/nfs/m01/anaconda/lib/python2.7/site-packages/tensorflow/contrib/__init__.py"", line 23, in <module>
    from tensorflow.contrib import layers
  File ""/nfs/m01/anaconda/lib/python2.7/site-packages/tensorflow/contrib/layers/__init__.py"", line 67, in <module>
    from tensorflow.contrib.layers.python.framework.tensor_util import *
  File ""/nfs/m01/anaconda/lib/python2.7/site-packages/tensorflow/contrib/layers/python/framework/tensor_util.py"", line 21, in <module>
    from tensorflow.python.framework.ops import Tensor
  File ""/nfs/m01/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 39, in <module>
    from tensorflow.python.framework import versions
  File ""/nfs/m01/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/versions.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/nfs/m01/anaconda/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 25, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""/nfs/m01/anaconda/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 21, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
ImportError: /nfs/m01/anaconda/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so: undefined symbol: secure_getenv
```

If I do ldd ~/anaconda/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so

```
    linux-vdso.so.1 =>  (0x00007ffd393ef000)
    libcudart.so.7.5 => /usr/local/cuda/7.5.18/lib64/libcudart.so.7.5 (0x00002ad591b36000)
    libdl.so.2 => /lib64/libdl.so.2 (0x00002ad591dc3000)
    libm.so.6 => /lib64/libm.so.6 (0x00002ad591fc7000)
    libz.so.1 => /lib64/libz.so.1 (0x00002ad59224c000)
    libpthread.so.0 => /lib64/libpthread.so.0 (0x00002ad592462000)
    libstdc++.so.6 => /usr/local/gcc/4.8.4/lib64/libstdc++.so.6 (0x00002ad59267f000)
    libgcc_s.so.1 => /usr/local/gcc/4.8.4/lib64/libgcc_s.so.1 (0x00002ad592989000)
    libc.so.6 => /lib64/libc.so.6 (0x00002ad592b9f000)
    /lib64/ld-linux-x86-64.so.2 (0x0000003a39400000)
    librt.so.1 => /lib64/librt.so.1 (0x00002ad592f33000)
```
"
1306,Multiple bugs in dynamic_rnn,"I know this feature remains alpha and unsupported, but I've run into a number of various bugs so I thought it may be worth reporting.

Just a simple RNN with a simple loss function + optimizer doesn't work. Take this code snippet for example:

```
num_stepss = [34, 20, 44, 18]
inputs = tf.constant(npr.rand(50, 4, 20).astype('float32'))
cell = tf.nn.rnn_cell.LSTMCell(num_units=100, 
                               input_size=20, 
                               initializer=tf.constant_initializer(0.05),
                               use_peepholes=True,
                               cell_clip=None)
outputs, _ = tf.nn.dynamic_rnn(cell=cell, inputs=inputs, sequence_length=num_stepss, dtype=tf.float32, time_major=True)
loss = tf.reduce_sum(outputs)
trainer = tf.train.GradientDescentOptimizer(0.01).minimize(loss)
```

This fails with the following error: 

`ValueError: Shapes () and (120, 400) must have the same rank`

A slightly fancier version of the above that encloses it within scope `'my_scope'` using `tf.variable_scope` gives the following error: 

`ValueError: Expected op/tensor name to start with gradients, got: my_scope/gradients/my_scope/Sum_grad/Tile:0`

I've run into other issues as well, but I don't yet know if they're related to the above or they're distinct bugs.
"
1304,Training a model using GPUs across different machines,"Hi, if/how can TensorFlow's distributed runtime be used for training a model using GPU resources across machines in a cluster?
"
1303,Broken links in C++ API docs,"Apparently most of the links in the main part of the [index of the C++ API documentation](https://www.tensorflow.org/versions/r0.7/api_docs/cc/index.html) are broken.

The problem seems to be that the first letter of each file name is in lowercase, so the links look like this:
[https://www.tensorflow.org/versions/r0.7/api_docs/cc/classEnv.html](https://www.tensorflow.org/versions/r0.7/api_docs/cc/classEnv.html)
Instead of this:
[https://www.tensorflow.org/versions/r0.7/api_docs/cc/ClassEnv.html](https://www.tensorflow.org/versions/r0.7/api_docs/cc/ClassEnv.html)
"
1300,Support for half-floats (float16/fp16),"This is a tracking bug for adding support for the half type (aka float16, or fp16) in TensorFlow. Half computation is supported by GPUs only, although newer Intel CPUs (Haswell and newer) have support for converting back and forth between fp16 and fp32 in hardware (F16C). CUDA has some support for half since 7.5, although it's a bit cumbersome (it's not a first-class type, but relies on macros containing asm statements; effectively intrinsics).

fp16 is interesting for two primary reasons: It would allow us to fit twice as large models in available GPU RAM, and it reduces memory bandwidth use, a precious resource on the GPU. The next generation of NVIDIA GPUs (Pascal) will also be able to do computation directly on two half-floats (in a SIMD-like structure) as fast as on a single float, although that would be somewhat more intrusive in code.

It is not 100% clear exactly how much of TensorFlow we need fp16 support for; interested parties are asked to comment. CuBLAS and CuDNN already has some support for half in their latest versions (so it would be natural to provide those interfaces), and Eigen also has beginning support. 
"
1299,Convolution produces wrong result depending on the batch size.,"### Environment info

Operating System: Ubuntu 14.04 LTS
Graphics: GeForce GTX 770/PCIe/SSE2
Cuda compute capability: 3.0
cuda version: 7.0
cudnn version: 6.5

installed from sources, commit hash: 5a30c8f07
### Steps to reproduce
1. Download the following numpy array in .npy format: https://www.dropbox.com/s/wljk6r83d0tee14/fail_tensor.npy?dl=0
2. Below is shown the code to reproduce the problem. The numpy.float32 array stored in fail_tensor.npy has shape (4, 33, 33, 1). Only the first slice (a[0]) of the array holds the values that causes the problem. The values in the other slice can be any value. The original array with batch_size=4 will cause a problem, however if we use batch_size less than 4 (e.g., a[:3]), then there will be no problem. A convolution with a kernel of ones in the original array will produce a tensor with a negative number even though the input tensor has only non-negative values.

``` python
import tensorflow as tf
import numpy as np
from scipy.ndimage import convolve

def test(batch_size, set_others):
    tt = np.load('fail_tensor.npy')
    tt = tt[:batch_size]
    tt[1:,...] = set_others
    t = tf.constant(tt)
    kernel = tf.constant(1.0, shape=[5, 5, 1, 1])
    r = tf.nn.conv2d(t, kernel, [1, 1, 1, 1], padding='SAME')
    sess = tf.Session()
    print tt.shape
    print tt[0].min()
    rr = r.eval(session=sess)
    print rr[0].min()
    scipy_r = convolve(tt[0,:,:,0], np.ones((5,5), np.float32), mode='constant')
    print np.allclose(scipy_r, rr[0,:,:,0])
    sess.close()
```

3: On IPython:

``` python
In [0]: import fail_example

# with batch_size=3 the result is the same as scipy convolve.
In [1]: fail_example.test(batch_size=3, set_others=0)
(3, 33, 33, 1)
0.0
4.47035e-08
True

# with batch_size=4 the result ISN'T the same as scipy convolve.
In [2]: fail_example.test(batch_size=4, set_others=0)
(4, 33, 33, 1)
0.0
-0.410374
False

# Setting set_others to 100 doesn't change the result
In [3]: fail_example.test(batch_size=3, set_others=100)
(3, 33, 33, 1)
0.0
4.47035e-08
True

# Setting set_others to 100 doesn't change the result
In [4]: fail_example.test(batch_size=4, set_others=100)
(4, 33, 33, 1)
0.0
-0.410374
False

# smaller batch_sizes than 4 produces correct result
In [5]: fail_example.test(batch_size=2, set_others=0)
(3, 33, 33, 1)
0.0
4.47035e-08
True

# smaller batch_sizes than 4 produces correct result
In [6]: fail_example.test(batch_size=1, set_others=0)
(3, 33, 33, 1)
0.0
4.47035e-08
True
```
"
1297,Exception thrown running  example/udacity/5_word2vec,"Hi all. Does anyone reproduce this? The description of the problem is below in the suggested format. A brief summary is: running the examples/udacity/4_convolutions.ipynb runs fine on gpu, but the 5_word2vec notebook fails  with an AdaGrad-related exception: 

`InvalidArgumentError: Cannot assign a device to node 'Adagrad/update_Variable_2/SparseApplyAdagrad': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/GPU:0'`

though I never modified the code (except to print some things out). I never modified code to assign any ops to a device, as it seems to be claiming.

This looks at least superficially related to [this issue](https://github.com/tensorflow/tensorflow/issues/505).

Thanks,
Jim
### Environment info

Operating System: Ubuntu 14.04 LTS

**If installed from binary pip package, provide:**
Built from r0.7 source.

**1. Which pip package you installed.**
`sudo pip install --upgrade /tmp/tensorflow_pkg/tensorflow-0.7.1-cp27-none-linux_x86_64.whl`

**2. The output from python -c ""import tensorflow; print(tensorflow.**version**)"".**

```
jd@jd-All-Series:~/dev/thirdparty/scikit-learn/scikit-learn$ python -c ""import tensorflow; print(tensorflow.__version__)""

I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so.7.0 locally

I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally

I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so.7.0 locally

I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally

I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so.7.0 locally

0.7.1
```

**If installed from sources, provide the commit hash:**
`41fcc7edd63c01dfd56382be4755c90f5a7eb565`
### Steps to reproduce
1. Run `jupyter notebook` in the examples/udacity/ directory.
2. Open the 5_word2vec.ipynb notebook in the web browser (from http://localhost:8888).
3. Execute each of the steps through the first (Google-provided) session.run().
4. Observe the exception when you get to the first session.run() after 'Train a skip-gram model.' step, which is provided by notebook (before I have to write any code). The exception happens on the line with session.run() invocation.

`InvalidArgumentError: Cannot assign a device to node 'Adagrad/update_Variable_2/SparseApplyAdagrad': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/GPU:0'
     [[Node: Adagrad/update_Variable_2/SparseApplyAdagrad = SparseApplyAdagrad[T=DT_FLOAT, Tindices=DT_INT64, use_locking=false](Variable_2, Variable_2/Adagrad, Adagrad/learning_rate, gradients/concat_2, gradients/concat_3)]]`

I have not modified the python code assign to do any device assignments in this notebook. 
### What have you tried?
1. I've been working through the Udacity DeepLearning course. After encountering this error, I killed the jupyter kernel for 5_word2vec, and re-run my previously-saved notebook for the fourth exercise, 4_convolutions.ipynb, and it executes without error on the gpu. Which I think must confirm that TensorFlow is installed OK. 

Done for the day, but when I come back, I will probably try building without gpu support, and see if that runs OK so I can at least carry on with the course.
### Logs or other output that would be helpful

Please let me know if there's anything you'd like me to log. Hardware is a Titan-X.
"
1295,miss file after install tensorflow,"After installing tensorflow, I found only one tensorflow folder in the path tensorflow/inlcude. In fact, it shoud consist of many other folders such as third_party, google, Eigen and so on. I don't known the reason why I could not find these folders. The way which I used to install tensorflow is ""sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl""

Thanks
"
1293,Gradient for tf.diag,"tf.diag doesn't support the gradient (as far as I can see here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_grad.py#L161)
Is it difficult to add support for that?
Thank you
Some simple example:

```
tSv = tf.Variable(tf.truncated_normal([r]))
t = tf.diag(tSv)
train = tf.train.GradientDescentOptimizer(0.01).minimize(t)

sess = tf.Session()
init = tf.initialize_all_variables()
sess.run(init)
sess.run(train)
...
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-96-28cf62ca1be1> in <module>()
      1 tSv = tf.Variable(tf.truncated_normal([r]))
      2 t = tf.diag(tSv)
----> 3 train = tf.train.GradientDescentOptimizer(0.0001).minimize(t)
      4 
      5 sess = tf.Session()
ValueError: No inputs provided
```
"
1292,RMSPropOptimizer incompatible with embedding layers.,"### Description

`RMSPropOptimizer` does not work with models utilizing embedding layers, which appears to be a consequence of it not implementing `_apply_sparse`. The other optimizers all implement `_apply_sparse`, so I assume this is an oversight.
### Environment info

Operating System: Ubuntu 14.04 LTS
Installed from source with hash: 03bff43060229357cbe2cc1659e7d129c2799b06
### Steps to reproduce

```
import numpy as np
import tensorflow as tf

sess = tf.Session()

weights = tf.get_variable('weights', [100, 32], 'float32', trainable=True)
words = tf.constant(np.arange(100, dtype='int32'))
logits = tf.nn.embedding_lookup(weights, words)
labels = tf.constant(np.arange(100, dtype='int64') % 32)
loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels)
optimizer = tf.train.RMSPropOptimizer(0.001)
step = optimizer.minimize(loss)
```
### Logs

[log.txt](https://github.com/tensorflow/tensorflow/files/147495/log.txt)
"
1291,Messy graph edges,"### Steps to reproduce
1. python tensorflow/examples/tutorials/mnist/mnist_with_summaries.py
2. python tensorflow/tensorboard/tensorboard.py --logdir=/tmp/mnist_logs
3. Expand 'xent' node: edge from 'y-input' to 'mul' ops has an S-like shape instead of a more appropriate  C-like.
   See the image below:

![screen shot 2016-02-25 at 2 37 18 pm](https://cloud.githubusercontent.com/assets/953399/13337087/731d612c-dbce-11e5-919a-f07ca9da6a2a.png)
"
1287,Unable to visualize Inception v3 graph in TensorBoard with TensorFlow 0.7.1,"### Summary

Attempting to visualize the Inception v3 graph with TensorBoard results in an empty graph (after several minutes of loading).

_Update: an earlier version of this issue indicated that the progress bar hung forever, but apparently, I just didn't wait long enough._
### Environment info

Operating System: OS X 10.11.3, Chrome 48.0.2564.116, Anaconda 1.2.2

If installed from binary pip package, provide:
1. Which pip package you installed: https://storage.googleapis.com/tensorflow/mac/tensorflow-0.7.1-cp35-none-any.whl
2. The output from python -c ""import tensorflow; print(tensorflow.**version**)"".: `0.7.1`
### Steps to reproduce
1. Downloaded and un-tar the [inception v3 model](http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz). The graph protobuffer is in `/tmp/imagenet/classify_image_graph_def.pb`.
2. Run this code to dump the graph:

``` python
    import os
    import os.path
    import tensorflow as tf
    from tensorflow.python.platform import gfile

    INCEPTION_LOG_DIR = '/tmp/inception_v3_log'

    if not os.path.exists(INCEPTION_LOG_DIR):
        os.makedirs(INCEPTION_LOG_DIR)
    with tf.Session() as sess:
        model_filename = '/tmp/imagenet/classify_image_graph_def.pb'
        with gfile.FastGFile(model_filename, 'rb') as f:
            graph_def = tf.GraphDef()
            graph_def.ParseFromString(f.read())
            _ = tf.import_graph_def(graph_def, name='')
        writer = tf.train.SummaryWriter(INCEPTION_LOG_DIR, graph_def)
        writer.close()
```
1. Run tensorboard: `tensorboard --logdir /tmp/inception_v3_log`
2. Navigate to graphs tab at http://0.0.0.0:6006/#graphs

**Expected result:** the graph
**Actual result:** Empty graph screen (after several minutes of loading with no movement of the progress bar)

A 91 MB file (same size as the graph protobuffer) called `events.out.tfevents.1456423256.[hostname]` is correctly saved to the log directory, so it seems that the graph is in there somewhere.
### What have you tried?
1. Installing Python 2 pip version of TensorFlow 0.7.1 in a separate conda environment; same results.
2. Running [mnist_summaries_example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py); graph is shown in TensorBoard properly, so this is a problem with the Inception model
"
1286,Make RNN cells add bias to the bias variable collection,"Currently helper layers like fully_connected add their bias variables to the 'bias' collection, which is a useful convenience. The RNN cell code doesn't do that, and it would be nice if it did for consistency. Needless to say the same is true for the main weights and the 'weights' collection.
"
1285,install issue! --No module named core.framework.graph_pb2,"I build with the source code.
envy@ub1404:~/os_pri/github/tensorflow$ git status
On branch master
Your branch is up-to-date with 'origin/master'.

envy@ub1404:~/os_pri/github$ sudo pip install /tmp/tensorflow_pkg/tensorflow-0.7.1-cp27-none-linux_x86_64.whl
Unpacking /tmp/tensorflow_pkg/tensorflow-0.7.1-cp27-none-linux_x86_64.whl
Requirement already satisfied (use --upgrade to upgrade): six>=1.10.0 in /home/envy/.local/lib/python2.7/site-packages (from tensorflow==0.7.1)
Downloading/unpacking protobuf==3.0.0b2 (from tensorflow==0.7.1)
  Downloading protobuf-3.0.0b2-py2.py3-none-any.whl (326kB): 326kB downloaded
Requirement already satisfied (use --upgrade to upgrade): wheel in /usr/lib/python2.7/dist-packages (from tensorflow==0.7.1)
Requirement already satisfied (use --upgrade to upgrade): numpy>=1.8.2 in /home/envy/.local/lib/python2.7/site-packages (from tensorflow==0.7.1)
Requirement already satisfied (use --upgrade to upgrade): setuptools in /home/envy/.local/lib/python2.7/site-packages (from protobuf==3.0.0b2->tensorflow==0.7.1)
Installing collected packages: tensorflow, protobuf
Successfully installed tensorflow protobuf
Cleaning up...
## envy@ub1404:~/os_pri/github$ pip show protobuf

Name: protobuf
Version: 3.0.0b2
Location: /usr/local/lib/python2.7/dist-packages
Requires: setuptools, six
## envy@ub1404:~/os_pri/github$ pip show tensorflow

Name: tensorflow
Version: 0.7.1
Location: /usr/local/lib/python2.7/dist-packages
Requires: six, protobuf, wheel, numpy
envy@ub1404:~/os_pri/github$ python tensorflow/tensorflow/models/image/mnist/convolutional.py
Traceback (most recent call last):
  File ""tensorflow/tensorflow/models/image/mnist/convolutional.py"", line 34, in <module>
    import tensorflow as tf
  File ""/home/envy/os_pri/github/tensorflow/tensorflow/**init**.py"", line 23, in <module>
    from tensorflow.python import *
  File ""/home/envy/os_pri/github/tensorflow/tensorflow/python/**init**.py"", line 41, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/envy/os_pri/github/tensorflow/tensorflow/python/**init**.py"", line 35, in <module>
    from tensorflow.core.framework.graph_pb2 import *
ImportError: No module named core.framework.graph_pb2

Error importing tensorflow.  Unless you are using bazel,
you should not try to import tensorflow from its source directory;
please exit the tensorflow source tree, and relaunch your python interpreter
from there.
envy@ub1404:~/os_pri/github$
"
1284,Clarify Udacity assignment #2,"- expand on 'hiddent'.
- Point to nn.relu:
  https://www.tensorflow.org/versions/r0.7/api_docs/python/nn.html#relu
"
1283,DEFINE_bool alias not working on Mac Python test-on-install,"For bugs/issues, please fill in the following.  The more information you
provide, the more likely we can help you.
### Environment info

Operating System: Mac

If installed from binary pip package, provide: This error occurs in Mac Python (2 and 3) test-on-install
1. Which pip package you installed.
2. The output from python -c ""import tensorflow; print(tensorflow.**version**)"".

If installed from sources, provide the commit hash:
### Steps to reproduce
1. Go to http://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/9/TF_BUILD_CONTAINER_TYPE=CPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=mac-slave/console 
2. See failures in tests: 
   tensorflow/python/platform/default/flags_test.py
   tensorflow/python/tools/graph_metrics_test.py
3. Notice that both failures complain about ""DEFINE_bool"" not being an attribute. 
4. Interestingly, ""DEFINE_bool"" is found at: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/default/_flags.py#L111

But somehow it is not recognized during Python test-on-install on Mac. 
### What have you tried?

1.
### Logs or other output that would be helpful

(If logs are large, please upload as attachment).
(See above)
"
1282,Get an error when building the Op library,"When I Build the Op library, I got the error :1234: 
In file included from /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/op.h:22:0,
                 from zero_out.cc:1:
/usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/op_def.pb.h:12:2: error: #error This file was generated by a newer version of protoc which is
 #error This file was generated by a newer version of protoc which is
  ^
/usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/op_def.pb.h:13:2: error: #error incompatible with your Protocol Buffer headers. Please update
 #error incompatible with your Protocol Buffer headers.  Please update
  ^
/usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/op_def.pb.h:14:2: error: #error your headers.
 #error your headers.
  ^
/usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/op_def.pb.h:22:35: fatal error: google/protobuf/arena.h: No such file or directory
 #include <google/protobuf/arena.h>

I install tensorflow by pip installation,branch r0.7.
"
1281,ImportError: cannot import name tensorboard_server ,"I've successfully built tensorflow ""version: 0.6.0 "" and tensor board from source on Mac OS X. The problem is I am getting this error when I try to run the tensorboard module. 

command: 
`
./bazel-bin/tensorflow/tensorboard/tensorboard --help
`

the produced error 

```
Ahmeds-MacBook-Pro:tensorflow ahmedabobakr$ ./bazel-bin/tensorflow/tensorboard/tensorboard --help
Traceback (most recent call last):
  File ""/Users/ahmedabobakr/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/tensorflow/tensorboard/backend/tensorboard.py"", line 36, in <module>
    from tensorflow.tensorboard.backend import tensorboard_server
ImportError: cannot import name tensorboard_server

```
"
1280,Implementation of CW-RNN?,"Has anyone considered implementing the CW-RNN architecture discussed here: http://arxiv.org/abs/1402.3511 

The cited results seem extremely promising, and the structure is quite simple. If it's not in the pipeline or already present, I may try implementing it myself.
"
1279,Arch doesn't support it,"After `sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl` or as root, I got:
`tensorflow-0.7.1-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.`

It seems it doesn't work with Python 3. What should I do?
"
1277,Cifar breaking,"I am running cifar multiGPU example with different batch sizes and number of GPUs. The code is breaking this way:

tensorflow.python.framework.errors.OutOfRangeError: RandomShuffleQueue '_1_tower_0/shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 128, current size 0)
     [[Node: tower_0/shuffle_batch = QueueDequeueMany[component_types=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](tower_0/shuffle_batch/random_shuffle_queue, tower_0/shuffle_batch/n/_775)]]
     [[Node: tower_1/shuffle_batch/n/_664 = _HostSend[T=DT_INT32, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:1"", send_device_incarnation=1, tensor_name=""edge_170_tower_1/shuffle_batch/n"", _device=""/job:localhost/replica:0/task:0/gpu:1""](tower_1/shuffle_batch/n)]]
Caused by op u'tower_0/shuffle_batch', defined at:
  File ""lib/python2.7/site-packages/tensorflow/models/image/cifar10/cifar10_multi-gpu_train.py"", line 224, in <module>
    tf.app.run()
  File ""/home/saoni.m/tensorflow/lib/python2.7/site-packages/tensorflow/python/platform/default/_app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""lib/python2.7/site-packages/tensorflow/models/image/cifar10/cifar10_multi-gpu_train.py"", line 222, in main
    train()
  File ""lib/python2.7/site-packages/tensorflow/models/image/cifar10/cifar10_multi-gpu_train.py"", line 150, in train
    loss = tower_loss(scope)
  File ""lib/python2.7/site-packages/tensorflow/models/image/cifar10/cifar10_multi-gpu_train.py"", line 65, in tower_loss
    images, labels = cifar10.distorted_inputs()
  File ""/home/saoni.m/tensorflow/lib/python2.7/site-packages/tensorflow/models/image/cifar10/cifar10.py"", line 119, in distorted_inputs
    batch_size=FLAGS.batch_size)
  File ""/home/saoni.m/tensorflow/lib/python2.7/site-packages/tensorflow/models/image/cifar10/cifar10_input.py"", line 153, in distorted_inputs
    min_queue_examples, batch_size)
  File ""/home/saoni.m/tensorflow/lib/python2.7/site-packages/tensorflow/models/image/cifar10/cifar10_input.py"", line 104, in _generate_image_and_label_batch
    min_after_dequeue=min_queue_examples)
  File ""/home/saoni.m/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/input.py"", line 496, in shuffle_batch
    return queue.dequeue_many(batch_size, name=name)
  File ""/home/saoni.m/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py"", line 287, in dequeue_many
    self._queue_ref, n, self._dtypes, name=name)
  File ""/home/saoni.m/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py"", line 319, in _queue_dequeue_many
    timeout_ms=timeout_ms, name=name)
  File ""/home/saoni.m/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py"", line 664, in apply_op
    op_def=op_def)
  File ""/home/saoni.m/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1834, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/saoni.m/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1043, in __init__
    self._traceback = _extract_stack()

The values that are passed to shuffle_batch():

```
 images, label_batch = tf.train.shuffle_batch(
      [image, label],
      batch_size=batch_size,
      num_threads=num_preprocess_threads,
      capacity=min_queue_examples + 3 * batch_size,
      min_after_dequeue=min_queue_examples)
```

are:
batch size 128, num_threads 16, capacity 20384, min_after_deque 20000
"
1276,Deep Dream Tutorial,"I wanted to make a tracking issue for the Deep Dream tutorial, as mentioned as 'Coming Soon' on this page: https://www.tensorflow.org/versions/r0.7/tutorials/index.html

I'm very interested to see this.
"
1274,'utf-8' codec can't decode byte (image_retraining),"On the Training on Flowers how to;
https://www.tensorflow.org/versions/master/how_tos/image_retraining/index.html 

Unable to get to get the .pb file and the .txt file to create in /tmp due to;
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xbb in position 1: invalid start byte
### Environment info

Operating System:
Mac OSX El Captain 10.11.3 (15D21)
Python 3.5.1
Tensorflow from source, commit [bfd5f0b](https://github.com/tensorflow/tensorflow/commit/bfd5f0bfaa13f397e80063ce59951033928b04b6)
If installed from binary pip package, provide:
1. pip3 install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.7.1-cp35-none-any.whl
2. The output from python3 -c 'import os; import inspect; import tensorflow; print(os.path.dirname(inspect.getfile(tensorflow)))'
   /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow
### Steps to reproduce
1. brand new mac, fresh brew, python3, pip3 etc
2. all tutorials so far are great, then on to;
   https://www.tensorflow.org/versions/master/how_tos/image_retraining/index.html
3. bazel build -c opt --copt=-mavx tensorflow/examples/image_retraining:retrain
   OK
4. bazel-bin/tensorflow/examples/image_retraining/retrain --image_dir ~/flower_photos
   Error, see output below
### Logs or other output that would be helpful

Traceback (most recent call last):
  File ""/Users/paulg/proj/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/tensorflow/examples/image_retraining/retrain.py"", line 828, in <module>
    tf.app.run()
  File ""/Users/paulg/proj/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/tensorflow/python/platform/default/_app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""/Users/paulg/proj/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/tensorflow/examples/image_retraining/retrain.py"", line 715, in main
    graph = create_inception_graph()
  File ""/Users/paulg/proj/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/tensorflow/examples/image_retraining/retrain.py"", line 301, in create_inception_graph
    graph_def.ParseFromString(f.read())
  File ""/Users/paulg/proj/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/tensorflow/python/platform/default/_gfile.py"", line 45, in sync
    return fn(self, _args, *_kwargs)
  File ""/Users/paulg/proj/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/tensorflow/python/platform/default/_gfile.py"", line 199, in read
    return self._fp.read(n)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/codecs.py"", line 321, in decode
    (result, consumed) = self._buffer_decode(data, self.errors, final)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xbb in position 1: invalid start byte
"
1273,docker run failed with the image,"### Environment info

Operating System: Ubuntu 14.04
Docker version: Docker version 1.10.2, build c3959b1
same result with or without proxy(I use tsocks for proxy)
### What have you tried?
1.  Failed when running tensorflow with docker using the following command

```
sudo docker run -it -p 8888:8888 b.gcr.io/tensorflow/tensorflow
```

the output

```
abc@abc:~/source$ sudo docker run -it -p 8888:8888 b.gcr.io/tensorflow/tensorflow
Unable to find image 'b.gcr.io/tensorflow/tensorflow:latest' locally
docker: Error response from daemon: unable to ping registry endpoint https://b.gcr.io/v0/
v2 ping attempt failed with error: Get https://b.gcr.io/v2/: dial tcp 74.125.204.82:443: i/o timeout
 v1 ping attempt failed with error: Get https://b.gcr.io/v1/_ping: dial tcp 74.125.204.82:443: i/o timeout.
See 'docker run --help'.

```
"
1271,Implementing Striving for Simplicity: The All Convolutional Net in Tensorflow: results on the test set lower than expected,"I posted [this issue with my code](http://stackoverflow.com/questions/35339636/erratic-training-for-all-cnn-on-cifar-10) on stackoverflow a while ago but it has not received any answer or comment.
The preprocessing I used is in the dataset class (that I adapted from the Deep MNIST for experts tutorial) and is exactly the same as in the neon/pylearn2 implementation: calculating the mean and Wzca matrix on training set and use it to whiten the data on the training and test set followed by a global contrast normalization step with Goodfellow scale factor of 55.  
The only difference of my implementation according to me (I would like to be mistaken) with the paper is the use of `tf.reduce_mean` instead of `tf.reduce_sum` (which led me to exploding ReLU grad).
So I divide the weight decay by a factor batch_size to keep the change.  
With this I got up to 85.something% on the test set instead of the 91% claimed by the authors.  
Do you see something wrong immediately ? For instance I feel the weight decay part of the cross entropy is ugly but I could not find any better way of doing that. Do you have some ideas ?  
If you think that this is not an appropriate place for my question tell me I will remove it but I do not know where to look.
"
1270,User ops documentation not sufficient,"I followed the documentation for building a user ops (https://www.tensorflow.org/versions/master/how_tos/adding_an_op/index.html), 
trying the zero_out op, there are a couple of places where the doc is not clear and also after hacking my way around it the op failed to work.

I am following the instructions for adding an op with Tensorflow source installation. 
Here are the questions:
1- Where should the bazel build rule be placed? 
2- How to exactly use bazel to build the op?

I appreciate if you can clarify the documentation, it will help a lot.
"
1269,Loading the newer inception model in Android demo example and No OpKernel was registered to support Op error,"### Steps to reproduce
1. Load the model in the asset folder from here: http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz
   rather than here: https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip
   This step loads the model which is used in image_retrain, label_image and classify examples. The link for the model in the android demo is old and has some differences that don't work with the other demos as mentioned in #1253 
2. Change the input_width and input_mean in TensorflowImageListener.java
3. Change the input tensor name and output tensor name as follows in the tensorflow_jni.cc
### What have you tried?
1. I changed the input_width from 224 to 299 and input_mean from 117 to 128.
2. When the input tensor name was **input:0** and output tensor name was **output:0** I got the following node not found error

`02-23 23:56:11.066 15084-15105/org.tensorflow.demo I/native: tensorflow/examples/android/jni/tensorflow_jni.cc:271 Width: 299
02-23 23:56:11.066 15084-15105/org.tensorflow.demo I/native: tensorflow/examples/android/jni/tensorflow_jni.cc:272 Stride: 1196
02-23 23:56:11.066 15084-15105/org.tensorflow.demo I/native: tensorflow/examples/android/jni/tensorflow_jni.cc:176 Tensorflow: Copying Data.
02-23 23:56:11.070 15084-15105/org.tensorflow.demo I/native: tensorflow/examples/android/jni/tensorflow_jni.cc:194 Start computing.
02-23 23:56:11.157 15084-15105/org.tensorflow.demo I/native: tensorflow/examples/android/jni/tensorflow_jni.cc:205 End computing. Ran in 86ms (91ms avg over 7 runs)
02-23 23:56:11.157 15084-15105/org.tensorflow.demo E/native: tensorflow/examples/android/jni/tensorflow_jni.cc:210 Error during inference: Not found: FetchOutputs node output:0: not found`
1. When the input tensor name was **Mul:0** and output tensor name was **softmax:0** I got the following error

`02-24 00:07:46.149 23093-23123/org.tensorflow.demo I/native: tensorflow/examples/android/jni/tensorflow_jni.cc:270 Height: 299
02-24 00:07:46.149 23093-23123/org.tensorflow.demo I/native: tensorflow/examples/android/jni/tensorflow_jni.cc:271 Width: 299
02-24 00:07:46.149 23093-23123/org.tensorflow.demo I/native: tensorflow/examples/android/jni/tensorflow_jni.cc:272 Stride: 1196
02-24 00:07:46.149 23093-23123/org.tensorflow.demo I/native: tensorflow/examples/android/jni/tensorflow_jni.cc:176 Tensorflow: Copying Data.
02-24 00:07:46.152 23093-23123/org.tensorflow.demo I/native: tensorflow/examples/android/jni/tensorflow_jni.cc:194 Start computing.
02-24 00:07:46.222 23093-23123/org.tensorflow.demo I/native: tensorflow/examples/android/jni/tensorflow_jni.cc:205 End computing. Ran in 70ms (77ms avg over 35 runs)
02-24 00:07:46.222 23093-23123/org.tensorflow.demo E/native: tensorflow/examples/android/jni/tensorflow_jni.cc:210 Error during inference: Invalid argument: No OpKernel was registered to support Op 'BatchNormWithGlobalNormalization' with these attrs
                                                                 [[Node: conv/batchnorm = BatchNormWithGlobalNormalization[T=DT_FLOAT, scale_after_normalization=false, variance_epsilon=0.001](conv/Conv2D, conv/batchnorm/moving_mean, conv/batchnorm/moving_variance, conv/batchnorm/beta, conv/batchnorm/gamma)]]
`

That's good news that the android demo code is finding the nodes in the new model after the changes. However, I have no idea what this error means. I would really appreciate if someone can explain. (Although this issue is being taken care of in #1253, I am still doing it myself in an attempt to learn more about tensorflow)
`No OpKernel was registered to support Op 'BatchNormWithGlobalNormalization' with these attrs
                                                                 [[Node: conv/batchnorm = BatchNormWithGlobalNormalization[T=DT_FLOAT, scale_after_normalization=false, variance_epsilon=0.001](conv/Conv2D, conv/batchnorm/moving_mean, conv/batchnorm/moving_variance, conv/batchnorm/beta, conv/batchnorm/gamma)]]`

Looking at the main.cc in the label_image example, I see something about ""normalized"" in the ReadTensorFromImage function. Don't know if that's helpful information. 
"
1268,Running language model example on multiple GPUs?,"Hi all,

First of all thanks for this great library and state of the art examples provided.

I am trying to train a language model on multiple GPUs following your language model example(https://www.tensorflow.org/versions/r0.7/tutorials/recurrent/index.html). This is because (i) Dataset is huge (ii) Vocabulary is large. 

I tried to change `with tf.device(""/cpu:0""):` in the code to something like: 

``` python
for d in ['/gpu:0', '/gpu:1']:
    with tf.device(d):
        embedding = tf.get_variable(""embedding"", [vocab_size, size])
        inputs = tf.nn.embedding_lookup(embedding, self._input_data) 
```

But got an error from `MatMul`. It happened even when I set it to one GPU. To resolve the problem with one GPU I tried to use:

``` python
def device_for_node(n):
  if n.type == ""MatMul"":
      return ""/gpu:1""
  else:
      return ""/cpu:0""
<some code>
with tf.Graph().as_default(), tf.Session() as session:
    with session.graph.device( device_for_node ):
```

which worked but speed dropped significantly, e.g., from 32,000 wps to 1200 wps which apparently means that it is not a good idea probably because TF optimizes both CPU and GPU usage. 
I still can not figure out how to use multiple GPUs without losing the speed for this example. Anyone has had the same problem?

Thanks!
Hamid
"
1267,learning rate reset to 0 in seq2seq example,"In the `translate.py` example, I used four layers without attention, the learning rate suddenly becomes zero...

```
 itr 1/311830 epoch 1/10 lr 0.7000 cost 9.77 ppl 17455.77
 itr 2/311830 epoch 1/10 lr 0.0000 cost 9.86 ppl 19082.18
 itr 3/311830 epoch 1/10 lr 0.0000 cost 9.80 ppl 18084.05
 itr 4/311830 epoch 1/10 lr 0.0000 cost 9.78 ppl 17700.32
```

This is parameter specific, when I set the batch size to be 128, learning rate to be 0.7 or 1.
When I change the batch size to be 64, or set the learning rate to be 0.5, the problem DISAPPEARS.

In debugging, when I try to set it back to 0.7 after it becomes 0., it will automatically go back to 0 in the next time step...

I confirm the learning rate update operation is not called.

I am using the `0.7.1` version with `cuda 7.5`
"
1266,Does tensorflow support Tesla K80?,"Tesla K80 is not listed in the device support list.
"
1264,It seems there is protobuf problem after building tensorflow 0.7 from source code,"For bugs/issues, please fill in the following.  The more information you
provide, the more likely we can help you.
### Environment info

Operating System: ubuntu 1404

Hi all

I am caffe user.
I've executed caffe application since the end of the last year.
Until i tried to install tensorflow, my system is working well.
After install tensorflow 0.7 from source code.
it shows me some error like below.

Even though i installed protobuf 3.0 via pip and source code, i couldn't solve this problem.
I think there is some static variable for requiring specific version of protobuf(in this case 3.0).
Do you have any idea for this problem?

I0224 10:41:24.543294 19528 layer_factory.hpp:74] Creating layer data
[libprotobuf FATAL google/protobuf/stubs/common.cc:61] This program requires version 3.0.0 of the Protocol Buffer runtime library, but the installed version is 2.5.0.  Please update your library.  If you compiled the program yourself, make sure that your headers are from the same version of Protocol Buffers as your link-time library.  (Version verification failed in ""google/protobuf/src/google/protobuf/any.pb.cc"".)
terminate called after throwing an instance of 'google::protobuf::FatalException'
  what():  This program requires version 3.0.0 of the Protocol Buffer runtime library, but the installed version is 2.5.0.  Please update your library.  If you compiled the program yourself, make sure that your headers are from the same version of Protocol Buffers as your link-time library.  (Version verification failed in ""google/protobuf/src/google/protobuf/any.pb.cc"".)

Thank you
"
1263,How to extract final embedding matrix from word2vec.py?,"I am using word2vec.py. I want to extract the final embedding matrix and save it to a txt file. How can I extract the final embedding?
"
1262,"Import error when installing from the latest binaries on Ubuntu14, Python3.4","I wanted to updat tensorflow to the newest release and now cannot import it (I am not in a source tree).
### Environment info

Operating System:
Ubuntu 14

If installed from binary pip package, provide:
1. Which pip package you installed.
   Latest 3.4 cpu Ubuntu
   `sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl`
2. The output from python -c ""import tensorflow; print(tensorflow.**version**)"".

If installed from sources, provide the commit hash:
### What have you tried?

`python3 -c ""import tensorflow; print(tensorflow.__version__)"".`
### Logs or other output that would be helpful

```
dima@dima-TP-T410s:~/data/repos$ python3 -c ""import tensorflow; print(tensorflow.__version__)""
Traceback (most recent call last):
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/__init__.py"", line 35, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>
    from google.protobuf import descriptor as _descriptor
ImportError: cannot import name 'descriptor'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/__init__.py"", line 23, in <module>
    from tensorflow.python import *
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/__init__.py"", line 35, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>
    from google.protobuf import descriptor as _descriptor
ImportError: cannot import name 'descriptor'


Error importing tensorflow.  Unless you are using bazel,
you should not try to import tensorflow from its source directory;
please exit the tensorflow source tree, and relaunch your python interpreter
from there.
```
"
1261,Setting momentum,"Hi,

I am trying to change the value of momentum for SGD. Can you please tell me where it is set?

Thanks!
"
1258,Create a way to control log message output.,"Create a way to control log message output., most options should be passed via ConfigProto or another proto. Maybe add a LogOptions structure to hold options related to logging.  #1229
### Environment info

Operating System: Mac 10.9.5
### Feature request
1. Have a way to control logs that you record.As in gflags/re2 flags.Refer  #1229 
### What have you tried?
1. #1229 
"
1257,Attribute Error from Tensorboard.py following upgrade to v0.7.1 upgrade,"For bugs/issues, please fill in the following.  The more information you
provide, the more likely we can help you.
### Environment info

Operating System:Ubuntu 14.0.4

If installed from binary pip package, provide:
1. Which pip package you installed.
   sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl
2. The output from python -c ""import tensorflow; print(tensorflow.**version**)"".
   0.7.1
   If installed from sources, provide the commit hash:
### Steps to reproduce

1.In the command prompt try to launch a tensorboard graph
2.
3.
### What have you tried?
1. I tried using the command      **tensorboard --logdir=/home/tattoo/Tabor_Stuff**
   Previously for v0.6.0 the graph launched. However after installing v0.7.1 with the command above I get the following attribute error. Am I missing an install step or is this really an error?
### Logs or other output that would be helpful

(If logs are large, please upload as attachment).
Traceback (most recent call last):
  File ""/usr/local/bin/tensorboard"", line 11, in <module>
    sys.exit(main())
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/tensorboard.py"", line 117, in main
    multiplexer = event_multiplexer.AutoloadingMultiplexer(
AttributeError: 'module' object has no attribute 'AutoloadingMultiplexer'
"
1256,install issue!,"envy@ub1404:~/os_pri/github/tensorflow$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
INFO: Found 1 target...
ERROR: /home/envy/os_pri/github/tensorflow/third_party/gpus/cuda/BUILD:126:1: declared output 'third_party/gpus/cuda/include/cudnn.h' is a dangling symbolic link.
ERROR: /home/envy/os_pri/github/tensorflow/third_party/gpus/cuda/BUILD:126:1: not all outputs were created.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 38.334s, Critical Path: 37.13s
envy@ub1404:~/os_pri/github/tensorflow$

but I can see:
envy@ub1404:~/os_pri/github/tensorflow$ ll third_party/gpus/cuda/include/cudnn.h 
lrwxrwxrwx 1 envy envy 35  2月 24 01:13 third_party/gpus/cuda/include/cudnn.h -> /usr/local/cuda-7.0/include/cudnn.h

envy@ub1404:~/os_pri/github/tensorflow$ ll /usr/local/cuda/include/cudnn.h
-rw------- 1 root root 38830  2月 23 13:07 /usr/local/cuda/include/cudnn.h

envy@ub1404:~/os_pri/github/tensorflow$ ll /usr/local/cuda-7.0/include/cudnn.h
-rw------- 1 root root 38830  2月 23 13:07 /usr/local/cuda-7.0/include/cudnn.h
envy@ub1404:~/os_pri/github/tensorflow$ 
"
1254,import tensorflow failed with 0.7.1 pip package,"### Environment info

Operating System: Ubuntu 14.04

If installed from binary pip package, provide:
1. Which pip package you installed.
   I flollowed: sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl
2. The output from python -c ""import tensorflow "".
   Traceback (most recent call last):
   File ""<stdin>"", line 1, in <module>
   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/**init**.py"", line 23, in <module>
     from tensorflow.python import *
   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/**init**.py"", line 41, in <module>
     raise ImportError(msg)
   ImportError: Traceback (most recent call last):
   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/**init**.py"", line 35, in <module>
     from tensorflow.core.framework.graph_pb2 import *
   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>
     from google.protobuf import descriptor as _descriptor
   ImportError: No module named protobuf

Error importing tensorflow.  Unless you are using bazel,
you should not try to import tensorflow from its source directory;
please exit the tensorflow source tree, and relaunch your python interpreter
### What have you tried?
1. I reinstall the old 0.6.0 pip package, everything works
2. I installed 0.7.1 cpu only version, it still doesn't work
3. I upgrade cuda & cudnn to 7.5 &v4, it still doesn't work
### Logs or other output that would be helpful

(If logs are large, please upload as attachment).
"
1253,Update the Android demo to use retrained models,"The Android example currently uses an older Inception model. It requires some changes to load the one used by the label_image, classify_image, and image_retraining examples. These include altering the names of the input and output layers, ensuring the larger model can be loaded, and updating the mean, standard deviation, and image size parameters.

Once this is done, people should be able to transfer retrained models to a device easily. This issue also covers documenting that process.
"
1252,tensorflow-0.7.1-cp27-none-any.whl is not a supported wheel on this platform.,"For bugs/issues, please fill in the following.  The more information you
provide, the more likely we can help you.
### Environment info

Operating System:
yosemite os x
If installed from binary pip package, provide:
1. Which pip package you installed.
2. The output from python -c ""import tensorflow; print(tensorflow.**version**)"".

If installed from sources, provide the commit hash:
### Steps to reproduce

1.
2.
3.
### What have you tried?

1.source ~/tensorflow/bin/activate
2.pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.7.1-cp27-none-any.whl
### Logs or other output that would be helpful

(If logs are large, please upload as attachment).
tensorflow-0.7.1-cp27-none-any.whl is not a supported wheel on this platform.
"
1251,install from source failed!,"envy@ub1404:~/os_pri/github/tensorflow$ sudo rm -rf ~/.cache/bazel   # just try at second time

envy@ub1404:~/os_pri/github/tensorflow$ sudo bazel build -c opt --config=cuda --verbose_failures //tensorflow/cc:tutorials_example_trainer 
Extracting Bazel installation...
.....
INFO: Found 1 target...
INFO: From Executing genrule @png_archive//:configure [for host]:
src/main/tools/namespace-sandbox.c:460: mount(opt->sandbox_root, opt->sandbox_root, NULL, MS_BIND | MS_NOSUID, NULL): Permission denied
ERROR: /home/envy/.cache/bazel/_bazel_root/d161cc2b736082b8df6a4c27a7f7e3a8/external/png_archive/BUILD:23:1: Executing genrule @png_archive//:configure failed: bash failed: error executing command 
  (cd /home/envy/.cache/bazel/_bazel_root/d161cc2b736082b8df6a4c27a7f7e3a8/tensorflow && \
  exec env - \
    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; pushd external/png_archive/libpng-1.2.53; workdir=$(mktemp -d -t tmp.XXXXXXXXXX); cp -a \* $workdir; pushd $workdir; ./configure --enable-shared=no --with-pic=no; popd; popd; cp $workdir/config.h bazel-out/host/genfiles/external/png_archive/libpng-1.2.53; rm -rf $workdir;'): bash failed: error executing command 
  (cd /home/envy/.cache/bazel/_bazel_root/d161cc2b736082b8df6a4c27a7f7e3a8/tensorflow && \
  exec env - \
    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; pushd external/png_archive/libpng-1.2.53; workdir=$(mktemp -d -t tmp.XXXXXXXXXX); cp -a \* $workdir; pushd $workdir; ./configure --enable-shared=no --with-pic=no; popd; popd; cp $workdir/config.h bazel-out/host/genfiles/external/png_archive/libpng-1.2.53; rm -rf $workdir;').
Target //tensorflow/cc:tutorials_example_trainer failed to build
INFO: Elapsed time: 27.500s, Critical Path: 2.21s
envy@ub1404:~/os_pri/github/tensorflow$ 
"
1250,Let sparse_softmax_cross_entropy_with_logits accept int32 labels?,"Right now `sparse_softmax_cross_entropy_with_logits` only accepts `labels` as an int64 `Tensor`. Is this intentional? Why would we want to avoid int32 `labels`?
"
1248,libtensorflow_framework.so: cannot open shared object file: No such file or directory,"centos 7
python -c ""import tensorflow; print(tensorflow.**version**)""
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally
0.7.1

$ python -c 'import tensorflow as tf; print(tf.sysconfig.get_lib())'
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally
/usr/lib/python2.7/site-packages/tensorflow/core

$ ls /usr/lib/python2.7/site-packages/tensorflow/core/
example/                    **init**.py                 kernels/                    libtensorflow_framework.so  util/  
framework/                  **init**.pyc                lib/                        protobuf/                   

ipython
Python 2.7.5 (default, Nov 20 2015, 02:00:19) 
Type ""copyright"", ""credits"" or ""license"" for more information.

IPython 3.2.1 -- An enhanced Interactive Python.
?         -> Introduction and overview of IPython's features.
%quickref -> Quick reference.
help      -> Python's own help system.
object?   -> Details about 'object', use 'object??' for extra details.

In [1]: import tensorflow as tf
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally
## In [2]: tf.load_op_library('./auc.so')

RuntimeError                              Traceback (most recent call last)
<ipython-input-2-32f9df443d84> in <module>()
----> 1 tf.load_op_library('./auc.so')

/usr/lib/python2.7/site-packages/tensorflow/python/framework/load_library.pyc in load_op_library(library_filename)
     55   try:
     56     if py_tf.TF_GetCode(status) != 0:
---> 57       raise RuntimeError(compat.as_text(py_tf.TF_Message(status)))
     58   finally:
     59     py_tf.TF_DeleteStatus(status)

RuntimeError: libtensorflow_framework.so: cannot open shared object file: No such file or directory

In [3]: 

I need to copy libtensorflow_framework.so to the working path to run script.
"
1246,Feature Request: spatial local response normalization.,"Currently tf.nn.local_response_normalization only normalizes along the the depth. I would like to suggest adding a spatial version of LRN as in caffe: http://caffe.berkeleyvision.org/tutorial/layers.html#local-response-normalization-lrn
"
1245,tf.train.match_filenames_once incompatible with checkpointer?,"On Mac 0.7.1 pip binary I'm getting the following error when trying to load a model that had called `tf.train.match_filenames_once` (named 'file' below) to obtain a list of file names:

`InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [] rhs shape= [60]
     [[Node: geomnet/save/Assign_6 = Assign[T=DT_STRING, use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](geomnet/files, geomnet/save/restore_slice_6)]]
Caused by op u'geomnet/save/Assign_6', defined at:
  File ""/Applications/Canopy.app/appdata/canopy-1.4.1.1975.macosx-x86_64/Canopy.app/Contents/lib/python2.7/runpy.py"", line 162, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)`

Is this expected behavior?
"
1244,Can't import tensorflow 0.7.1 It thinks I'm importing from source dir.,"For bugs/issues, please fill in the following.  The more information you
provide, the more likely we can help you.
### Environment info

Operating System: Ubuntu 64bit 
CUDA: 7.5

If installed from binary pip package, provide:
I tried installing using both pip CPU and GPU (making sure I uninstalled the previous first):

```
$ pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp34-none-linux_x86_64.whl

$ pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.1-cp34-none-linux_x86_64.wh
```
### Logs or other output that would be helpful

I get the following error using the new 0.7.1 wheel, although it is fine with the previous one 0.7.0

```
(If logs are large, please upload as attachment).
In [1]: import tensorflow as tf
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-1-41389fad42b5> in <module>()
----> 1 import tensorflow as tf

/home/eders/anaconda/lib/python2.7/site-packages/tensorflow/__init__.py in <module>()
     21 from __future__ import print_function
     22
---> 23 from tensorflow.python import *

/home/eders/anaconda/lib/python2.7/site-packages/tensorflow/python/__init__.py in <module>()
     39 please exit the tensorflow source tree, and relaunch your python interpreter
     40 from there."""""" % traceback.format_exc()
---> 41   raise ImportError(msg)
     42
     43 from tensorflow.core.framework.summary_pb2 import *

ImportError: Traceback (most recent call last):
  File ""/home/eders/anaconda/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 35, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""/home/eders/anaconda/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>
    from google.protobuf import descriptor as _descriptor
ImportError: No module named protobuf


Error importing tensorflow.  Unless you are using bazel,
you should not try to import tensorflow from its source directory;
please exit the tensorflow source tree, and relaunch your python interpreter
from there.
```

Note that I don't even have the source files on my computer. Any idea of what it might be?
Thanks for the help.
"
1243,Tensorflow on ARMv7,"I am running Ubuntu 14.04 on a ARMv7 equipped SoC. I successfully compiled _bazel_ and installed the necessary dependencies.

Some details on:
**gcc**

```
$ g++ --version
g++ (Ubuntu/Linaro 4.8.4-2ubuntu1~14.04.1) 4.8.4
```

**python**

```
$ python --version
Python 2.7.6
```

**pip**

```
$ pip --version
pip 8.0.2 from /usr/local/lib/python2.7/dist-packages (python 2.7) 
```

and **bazel**

```
$ bazel version
Build label: head (@d774022)
```

I just cloned the TensorFlow repository (commit: 225de5e336d4d241ce02652eb961cdb6e0eec847) and I run most of the TensorFlow compilation. But at a certain point I get this error:

```
root@soc:~/tensorflow$ bazel build --verbose_failures -c opt //tensorflow/tools/pip_package:build_pip_package
INFO: Waiting for response from Bazel server (pid 2045)...
WARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.
INFO: Found 1 target...
INFO: From Compiling tensorflow/python/lib/core/py_func.cc:
In file included from third_party/py/numpy/numpy_include/numpy/ndarraytypes.h:1761:0,
                 from third_party/py/numpy/numpy_include/numpy/ndarrayobject.h:17,
                 from third_party/py/numpy/numpy_include/numpy/arrayobject.h:4,
                 from tensorflow/python/lib/core/py_func.cc:19:
third_party/py/numpy/numpy_include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning ""Using deprecated NumPy API, disable it by "" ""#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION"" [-Wcpp]
 #warning ""Using deprecated NumPy API, disable it by "" \
  ^
ERROR: /home/turing/tensorflow/tensorflow/core/BUILD:359:1: C++ compilation of rule '//tensorflow/core:kernel_lib' failed: gcc failed: error executing command 
  (cd /home/turing/.cache/bazel/_bazel_turing/508b67e7022bdc82335b30f64f498c49/tensorflow && \
  exec env - \
    PATH=/opt/bazel/output:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games \
  /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -iquote external/jpeg_archive -iquote bazel-out/local_linux-opt/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/local_linux-opt/genfiles/external/png_archive -iquote external/re2 -iquote bazel-out/local_linux-opt/genfiles/external/re2 -iquote external/eigen_archive -iquote bazel-out/local_linux-opt/genfiles/external/eigen_archive -isystem google/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/google/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local_linux-opt/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-fe78cbc4f8f9 -isystem bazel-out/local_linux-opt/genfiles/external/eigen_archive/eigen-eigen-fe78cbc4f8f9 -isystem third_party/gpus/cuda -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda -isystem third_party/gpus/cuda/include -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda/include -fno-exceptions -DEIGEN_AVOID_STL_ARRAY -pthread -no-canonical-prefixes -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernel_lib/tensorflow/core/kernels/relu_op.pic.o' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernel_lib/tensorflow/core/kernels/relu_op.pic.d -fPIC -c tensorflow/core/kernels/relu_op.cc -o bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernel_lib/tensorflow/core/kernels/relu_op.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 4: gcc failed: error executing command 
  (cd /home/turing/.cache/bazel/_bazel_turing/508b67e7022bdc82335b30f64f498c49/tensorflow && \
  exec env - \
    PATH=/opt/bazel/output:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games \
  /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -iquote external/jpeg_archive -iquote bazel-out/local_linux-opt/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/local_linux-opt/genfiles/external/png_archive -iquote external/re2 -iquote bazel-out/local_linux-opt/genfiles/external/re2 -iquote external/eigen_archive -iquote bazel-out/local_linux-opt/genfiles/external/eigen_archive -isystem google/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/google/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local_linux-opt/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-fe78cbc4f8f9 -isystem bazel-out/local_linux-opt/genfiles/external/eigen_archive/eigen-eigen-fe78cbc4f8f9 -isystem third_party/gpus/cuda -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda -isystem third_party/gpus/cuda/include -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda/include -fno-exceptions -DEIGEN_AVOID_STL_ARRAY -pthread -no-canonical-prefixes -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernel_lib/tensorflow/core/kernels/relu_op.pic.o' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernel_lib/tensorflow/core/kernels/relu_op.pic.d -fPIC -c tensorflow/core/kernels/relu_op.cc -o bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernel_lib/tensorflow/core/kernels/relu_op.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 4.
gcc: internal compiler error: Killed (program cc1plus)
Please submit a full bug report,
with preprocessed source if appropriate.
See <file:///usr/share/doc/gcc-4.8/README.Bugs> for instructions.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 599.857s, Critical Path: 587.04s
```

How can I fix or workaround it?

Thank you.
"
1242,Mac PIP 0.7.1 breaks protobuf installation,"I just upgraded to the latest 0.7.1 on Mac OSX using the PIP package, and I can no longer run TF. When importing in Python 2.7 it reports that protobuf cannot be found, even though during the pip install it supposedly upgraded protobuf to 3.0.0b2.
"
1241,Sparse matrix multiplication?,"Hi There,

Is there currently support for sparse matrix multiplication in tensorflow?

I found this post:
http://stackoverflow.com/questions/34030140/is-sparse-tensor-multiplication-implemented-in-tensorflow

But it seems to indicate that one must first convert the sparse matrix into a dense matrix before running the multiplication. 
"
1236,"Error while running fully_connected_feed.py of the tutorial ""TensorFlow Mechanics""","Hello, 

while running fully_connected_feed.py i had this error. I was trying the TensorFlow Mechanics 101 Tutorial

I attach the whole execution of the py file.

$ python3 fully_connected_feed.py 
Extracting data/train-images-idx3-ubyte.gz
Extracting data/train-labels-idx1-ubyte.gz
Extracting data/t10k-images-idx3-ubyte.gz
Extracting data/t10k-labels-idx1-ubyte.gz
Step 0: loss = 2.32 (0.010 sec)
Step 100: loss = 2.17 (0.005 sec)
Step 200: loss = 1.96 (0.004 sec)
Step 300: loss = 1.73 (0.004 sec)
Step 400: loss = 1.46 (0.004 sec)
Step 500: loss = 1.01 (0.005 sec)
Step 600: loss = 0.85 (0.009 sec)
Step 700: loss = 0.75 (0.004 sec)
Step 800: loss = 0.78 (0.004 sec)
Step 900: loss = 0.62 (0.004 sec)
Traceback (most recent call last):
  File ""fully_connected_feed.py"", line 228, in <module>
    tf.app.run()
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/platform/default/_app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""fully_connected_feed.py"", line 224, in main
    run_training()
  File ""fully_connected_feed.py"", line 199, in run_training
    saver.save(sess, FLAGS.train_dir, global_step=step)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/training/saver.py"", line 970, in save
    self.export_meta_graph(meta_graph_file_name)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/training/saver.py"", line 990, in export_meta_graph
    as_text=as_text)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/training/saver.py"", line 1315, in export_meta_graph
    os.path.basename(filename), as_text=as_text)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/training/training_util.py"", line 70, in write_graph
    gfile.MakeDirs(logdir)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/platform/default/_gfile.py"", line 295, in MakeDirs
    os.makedirs(path, mode)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/os.py"", line 241, in makedirs
    mkdir(name, mode)
FileNotFoundError: [Errno 2] No such file or directory: ''

My Operating System is OSX ElCapitan Upgrade. 
I installed tensorflow from pip package. 
The same error happens for both python2.7 and python3.5

Thanks a lot in advance for your help.

Silvio
"
1235,No module 'sparse_softmax_cross_entropy_with_logits' in cifar10_train.py on master,"For bugs/issues, please fill in the following.  The more information you
provide, the more likely we can help you.
### Environment info

Operating System: Ubuntu 15.10

I have the following issue when running cifar10_train.py on master

```
python cifar10_train.py
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Traceback (most recent call last):
  File ""cifar10_train.py"", line 101, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/default/_app.py"", line 11, in run
    sys.exit(main(sys.argv))
  File ""cifar10_train.py"", line 98, in main
    train()
  File ""cifar10_train.py"", line 41, in train
    loss = cifar10.loss(logits, labels)
  File ""/home/kde/Code/tf/cifar10/cifar10.py"", line 244, in loss
    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(
AttributeError: 'module' object has no attribute 'sparse_softmax_cross_entropy_with_logits'
```
"
1234,Update documentation for softmax-with-cross-entropy loss functions,"`softmax_cross_entropy_with_logits` and `sparse_softmax_cross_entropy_with_logits` both have useful  behavior that conflicts with current documentation.

In `softmax_cross_entropy_with_logits`: ""All that is required is that each row of `labels` is
a valid probability distribution."" In `sparse_softmax_cross_entropy_with_logits`: ""labels: Each entry `labels[i]` must be an index in `[0, num_classes)`.""

Neither of these statements is true, and using all 0s in the case of `softmax_cross_entropy_with_logits` or -1s in the case of `sparse_softmax_cross_entropy_with_logits` is useful when signals include entries for which we don't want to compute loss.

`softmax_cross_entropy_with_logits` example:

```
logits_list = [tf.Variable([[0.0, 0.0, 0.0]]), tf.Variable([[99.0, 0.0, 0.0]])]
labels = tf.Variable([[0.0, 0.0, 0.0]])
loss_list = [tf.nn.softmax_cross_entropy_with_logits(logits, labels)
             for logits in logits_list]

with tf.Session() as sess:
    sess.run(tf.initialize_all_variables())
    print(sess.run(loss_list))

# [array([ 0.], dtype=float32), array([ 0.], dtype=float32)]
```

`sparse_softmax_cross_entropy_with_logits` example:

```
logits_list = [tf.Variable([[0.0, 0.0, 0.0]]), tf.Variable([[99.0, 0.0, 0.0]])]
labels = tf.cast(tf.Variable([-1]), tf.int64)
loss_list = [tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels)
             for logits in logits_list]

with tf.Session() as sess:
    sess.run(tf.initialize_all_variables())
    print(sess.run(loss_list))

# [array([ 0.], dtype=float32), array([ 0.], dtype=float32)]
```

We could filter invalid entries out before computing logits/loss, but in my use cases this would add a lot of boilerplate code and make only a negligible difference performance wise. (RNNs with long sequences, a fairly small number of time steps, multiple predictions per time step, and batch entries with varying `sequence_length`s. Using this trick lets us avoid having to break up logits/targets etc. into valid chunks vs. invalid chunks.

Edit: I ended up applying a boolean mask anyway since I wanted to view the loss over time (not just take gradients and minimize). In any case I like the current behavior so that I can apply masks after I obtain loss values over all targets, both valid and invalid.
"
1233,"python ""import tensorfow"" fail after source build","TF 0.7 built from source on Ubuntu 15.10, PIP package built and installed.  python import fails.

```
Python 2.7.10 (default, Oct 14 2015, 16:09:02) 
[GCC 5.2.1 20151010] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 23, in <module>
    from tensorflow.python import *
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 35, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/graph_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/tensor_shape_pb2.py"", line 22, in <module>
    serialized_pb=_b('\n,tensorflow/core/framework/tensor_shape.proto\x12\ntensorflow\""z\n\x10TensorShapeProto\x12-\n\x03\x64im\x18\x02 \x03(\x0b\x32 .tensorflow.TensorShapeProto.Dim\x12\x14\n\x0cunknown_rank\x18\x03 \x01(\x08\x1a!\n\x03\x44im\x12\x0c\n\x04size\x18\x01 \x01(\x03\x12\x0c\n\x04name\x18\x02 \x01(\tB/\n\x18org.tensorflow.frameworkB\x11TensorShapeProtosP\x01\x62\x06proto3')
TypeError: __init__() got an unexpected keyword argument 'syntax'

```
"
1231,sampled softmax vs regular softmax,"For the seq2seq example, sampled softmax is 2-3 times slower than regular softmax, is it because sampled softmax is on CPU while regular softmax is on GPU? Then why do we need sampled softmax, it saves gpu memory?

But the convergence speed of sampled softmax (in terms of iterations) is much faster then regular softmax? Any mathematical reasons?

Many thanks!
"
1230,Imagenet example error,"### Environment info

Operating System: Windows with latest docker image (only cpu)

after running: python classify_image.py and loading the model I got the folling error:

`Succesfully downloaded inception-2015-12-05.tgz 88931400 bytes.                                                                                                    [libprotobuf ERROR google/protobuf/src/google/protobuf/io/coded_stream.cc:207] A protocol message was rejected because it was too big (more than 67108864 bytes).  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.                             Traceback (most recent call last):                                                                                                                                   File ""classify_image.py"", line 213, in <module>                                                                                                                      tf.app.run()                                                                                                                                                     File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/default/_app.py"", line 30, in run                                                            sys.exit(main(sys.argv))                                                                                                                                         File ""classify_image.py"", line 209, in main                                                                                                                          run_inference_on_image(image)                                                                                                                                    File ""classify_image.py"", line 159, in run_inference_on_image                                                                                                        create_graph()                                                                                                                                                   File ""classify_image.py"", line 141, in create_graph                                                                                                                  graph_def.ParseFromString(f.read())                                                                                                                            google.protobuf.message.DecodeError: Error parsing message`   

what can I do ?
"
1228,sequence2sequence model padding ,"In the seq2seq models, paddings are applied to make all sequences in a bucket have the same lengths. And apart from this, it looks like no special handling is applied to the paddings: 
- the encoder encodes the paddings as well
- the basic decoder w/o attention decodes using the last encoding which encodes the paddings
- the decoder with attention attends to the hidden states of the padding inputs too

It would be really helpful if this could be clarified: is it true that, basically the paddings are just a special id/embedding, and the current seq2seq implementation treats them just like other embeddings? And no special mechanism is needed to ignore these padding, for example when encoding a sequence containing paddings; or to decode a sequence containing paddings using the attention-based decoder? So after padding, nothing special is done to the paddings, we can just pretend a padding is just another embedding  (apart from maybe when doing the weighted x-entropy using target_weights)?

If the above is true, then when testing a trained model, is padding needed at all (since at test time, each sentence is decoded separately and not in a batch)? --- It looks like from the code, at test time an input sentence is still bucketed first and then padded?
"
1227,bazel build //tensorflow/examples/android:tensorflow_demo missing dependency declarations,"For bugs/issues, please fill in the following.  The more information you
provide, the more likely we can help you.
### Environment info

Operating System: Mac OS X (10.10.5)

If installed from binary pip package, provide:
1. Which pip package you installed.
2. The output from python -c ""import tensorflow; print(tensorflow.**version**)"".

If installed from sources, provide the commit hash:  3b8f0df34b80f2b0019e24dd91e099e0e6874bd6
### Steps to reproduce
1. Build TensorFlow from source following [these instructions](https://gist.github.com/ageitgey/819a51afa4613649bd18) for building with CUDA support on Mac
2. Tested TensorFlow installed correctly
3. Attempted to build Android demo following [these instructions](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android)
### What have you tried?
1. Tried adding the missing dependencies into this [BUILD file](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/BUILD) between lines 14 and 15.  Added lines that looked like:
   `""tensorflow/core/platform/default/logging.h"", ""tensorflow/core/platform/macros.h"",`
   - My thought was that maybe adding the asked for dependencies might do the trick.
   - My thought when that didn't work was maybe I need to use the ""deps"" option instead of the ""srcs"" option, but since I have NO familiarity with Bazel that seems problematic. :-/
### Logs or other output that would be helpful

(If logs are large, please upload as attachment).
`bazel build //tensorflow/examples/android:tensorflow_demo --verbose_failures
INFO: Found 1 target...
ERROR: /Users/[USERNAME]/Documents/Dev/tensorflow/tensorflow/examples/android/BUILD:10:1: undeclared inclusion(s) in rule '//tensorflow/examples/android:libtensorflow_demo.so':
this rule is missing dependency declarations for the following files included by 'tensorflow/examples/android/jni/jni_utils.cc':
  '/Users/[USERNAME]/Documents/Dev/tensorflow/tensorflow/core/platform/default/logging.h'
  '/Users/[USERNAME]/Documents/Dev/tensorflow/tensorflow/core/platform/macros.h'.
Target //tensorflow/examples/android:tensorflow_demo failed to build
INFO: Elapsed time: 2.053s, Critical Path: 1.93s
`
"
1225,resize_image_with_crop_or_pad() for a batch of images,"`resize_image_with_crop_or_pad()` should work for multiple images (batch of images) similar to `tf.image.resize_images()`
"
1224,dynamic_rnn gradients unable to be applied,"Operating System: Ubuntu 14.04.3 LTS
Installation: 0.7.0 via pip
Output from `python -c ""import tensorflow; print(tensorflow.__version__)""`: 0.7.0

I realize `dynamic_rnn` isn't official yet, but I'm hoping to see whether I'm using it correctly / whether there's a bug.

If I use `rnn.rnn`, `optimizer.apply_gradients(grad_var_pairs)` works with no issues. If I use rnn.dynamic_rnn, with the only change being how inputs must be handled, I get an error: `ValueError: Shapes () and (276, 1024) must have the same rank`

So I looked at the gradients. All gradients have identical shapes/names except two.

In the `rnn.rnn` case, we have

`<tf.Tensor 'gradients/AddN_596:0' shape=(?, ?) dtype=float32>
<tf.Tensor 'gradients/AddN_595:0' shape=<unknown> dtype=float32>`

Whereas in the `rnn.dynamic_rnn` case, we have

`<tf.Tensor 'gradients/model/model/rnn_fw/RNN/While/cond/LSTMCell/MatMul/Enter_grad/b_acc_3:0' shape=() dtype=float32>
<tf.Tensor 'gradients/model/model/rnn_fw/RNN/While/cond/LSTMCell/BiasAdd/Enter_grad/b_acc_3:0' shape=() dtype=float32>`

Do I have to manipulate these in some way before applying them?
"
1222,Check failed: params_->device->RequiresRecordingAccessedTensors() == record_tensor_accesses_ ,"The code used to run ok on both CPU or  GPU mode, but I don't know what happend , now it will fail whether cpu or gpu version
python -c ""import tensorflow; print(tensorflow.**version**)""

F /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/op_kernel.h:1087] Check failed: params_->device->RequiresRecordingAccessedTensors() == record_tensor_accesses_ 
"
1216,Android demo fails to build due to protobuf visibility error,"When running the instructions to build the Android demo with the latest open-source repo, a protobuf visibility error causes the build to fail.

Steps to reproduce:
- On an Ubuntu system, git clone the repo following the install instructions.
- Run `bazel build //tensorflow/examples/android:tensorflow_demo`.

Results:

```
ERROR: /usr/local/google/home/petewarden/projects/tensorflow/tensorflow/core/BUILD:77:1: Target '//google/protobuf:cc_wkt_protos' is not visible from target '//tensorflow/core:protos_all_cc'. Check the visibility declaration of the former target if you think the dependency is legitimate.
```

Notes:
I think this might be a protobuf Bazel rule problem. I was able to fix it by patching google/protobuf/protobuf.bzl like so:

```
+++ b/protobuf.bzl
@@ -132,6 +132,11 @@ def cc_proto_library(
   if include != None:
     includes = [include]

+  if ""visibility"" in kargs:
+    visibility = kargs[""visibility""]
+  else:
+    visibility = None
+
   if internal_bootstrap_hack:
     # For pre-checked-in generated files, we add the internal_bootstrap_hack
     # which will skip the codegen action.
@@ -141,6 +146,7 @@ def cc_proto_library(
         deps=[s + ""_genproto"" for s in deps],
         includes=includes,
         protoc=protoc,
+        visibility=visibility,
     )
     # An empty cc_library to make rule dependency consistent.
     native.cc_library(
@@ -157,6 +163,7 @@ def cc_proto_library(
       protoc=protoc,
       gen_cc=1,
       outs=outs,
+      visibility=visibility,
   )

   if default_runtime and not default_runtime in cc_libs:
```
"
1214,Unable to build from source with Bazel 1.4,"I'm running linux x86_64, OpenSuse 13.1

I am trying to compile from the repo master branch 875a67f with Bazel 1.4, installed from `bazel-0.1.4-installer-linux-x86_64.sh`

I was getting dependency errors and had to make the modifications described in [this stack overflow post](http://stackoverflow.com/questions/35256110/tensorflow-build-fails-with-missing-dependency-error) to my Bazel setup

This got me past my initial dependency errors (described exactly in detail by StackOverflow OP in above post), but now I've got another error when trying to run this command:

`bazel build --verbose_failures -c opt //tensorflow/tools/pip_package:build_pip_package
`

The relevant output:

```
INFO: From Compiling tensorflow/python/lib/core/py_func.cc:
tensorflow/python/lib/core/py_func.cc:19:31: fatal error: numpy/arrayobject.h: No such file or directory
 #include ""numpy/arrayobject.h""
                               ^
compilation terminated.
ERROR: /home/personal/files/learning/tensorflow/tensorflow/tensorflow/python/BUILD:75:1: C++ compilation of rule '//tensorflow/python:py_func_lib' failed: gcc failed: error executing command
  (cd /root/.cache/bazel/_bazel_root/f202f08f55600b1b30922b1248222df1/tensorflow && \
  exec env - \
    PATH=/usr/lib64/mpi/gcc/openmpi/bin:/sbin:/usr/sbin:/usr/local/sbin:/root/bin:/usr/local/bin:/usr/bin:/bin:/usr/bin/X11:/usr/X11R6/bin:/usr/games:/opt/cross/bin:/usr/local/go/bin \
  /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -iquote external/jpeg_archive -iquote bazel-out/local_linux-opt/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/local_linux-opt/genfiles/external/png_archive -iquote external/re2 -iquote bazel-out/local_linux-opt/genfiles/external/re2 -iquote external/eigen_archive -iquote bazel-out/local_linux-opt/genfiles/external/eigen_archive -isystem google/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/google/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local_linux-opt/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-70505a059011 -isystem bazel-out/local_linux-opt/genfiles/external/eigen_archive/eigen-eigen-70505a059011 -isystem third_party/py/numpy/numpy_include -isystem bazel-out/local_linux-opt/genfiles/third_party/py/numpy/numpy_include -isystem util/python/python_include -isystem bazel-out/local_linux-opt/genfiles/util/python/python_include -no-canonical-prefixes -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/python/_objs/py_func_lib/tensorflow/python/lib/core/py_func.pic.o' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/python/_objs/py_func_lib/tensorflow/python/lib/core/py_func.pic.d -fPIC -c tensorflow/python/lib/core/py_func.cc -o bazel-out/local_linux-opt/bin/tensorflow/python/_objs/py_func_lib/tensorflow/python/lib/core/py_func.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: gcc failed: error executing command
  (cd /root/.cache/bazel/_bazel_root/f202f08f55600b1b30922b1248222df1/tensorflow && \
  exec env - \
```

Can anyone offer assistance with building?
"
1205,Inconsistent memory usage between CPU and GPU,"Running a large network on CPU uses ~2GB memory, whereas running the same network on GPU takes ~6GB of memory (the full size of the card). Is it just allocating all the available GPU memory even though it might not all be utilized? This is using the recent 0.7 pip version.
"
1204,Possible bug in    master/tensorflow/python/ops/rnn.py,"in rnn.py and the function def _rnn_step(...):

line 261: copy_cond = (time >= sequence_length)

sequence_length is a vector of ints and time is an int,  should time be compared with one element of sequence_length, instead of the whole thing? 

(Is this function still work in progress?)
"
1203,Unable to safely terminate tensor flow running on gpu,"I have implement a network using tensorflow on a Nvidia GPU. Even after I call sess.close(), the program won't stop. When I hit ctrl+c for termination, the program crashes the nvidia driver and creates a zombie process named ""python"". I am not able to kill the zombie process, neither can I reboot ubuntu system by sudo reboot.

Is this a known issue? Please help!
"
1201,run within PHP ,"Is it possible to run 

`bazel-bin/tensorflow/examples/label_image/label_image`

inside PHP script and echo the output of main.cc to PHP script?

Is there any way to remotely call and get the results?
"
1199,No module named tensorboard in the last pull and build 0.7,"I just pulled the repo and build it from source and when calling tensorboard in the terminal I get following error:

``` shell
Traceback (most recent call last):
  File ""/Users/tensorflow/bin/tensorboard"", line 7, in <module>
    from tensorflow.tensorboard.backend.tensorboard import main
ImportError: No module named tensorboard
```

Env is Mac OS X El Capitan. 
"
1198,reverse_sequence's inability to accept int32 can break bidirectional_rnn,"In the latest releases `bidirectional_rnn` has been changed to accept int32 tensors for the `sequence_length` argument, but `tf.reverse_sequence` only accepts int64 tensors, and this is currently causing an error when an int32 tensor is passed to `bidirectional_rnn`.
"
1197,Problem with OpenCV - Initializing TensorFlow variables takes very long,"For some reason, the variable initialization of TensorFlow takes a very long time when OpenCV is imported (`import cv2`). Running the operation `tf.init_all_variables()` takes less than 1 second without OpenCV, but as soon as I import `cv2` this takes 22 seconds (!). See attachment for the simple - Hello World - code that I run. [hello_world.txt](https://github.com/tensorflow/tensorflow/files/138119/hello_world.txt) 

This problem occurs in TensorFlow 0.6 and 0.7. During the initialization the CPU load is 100%, GPU is idle but all available memory is initialized by TensorFlow.

```
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally
0.7.0
```

TensorFlow is installed in an virtualenv using `pip` after installing CUDA Toolkit 7.5 and CuDNN v3. My OpenCV installation is from sources (GitHub, master), so this is version 3.1+. Some possibly relevant installation parameters are `WITH_CUDA=ON`, `WITH_CUBLAS=OFF`.

I am using a powerful machine running Linux Mint, 32 CPU cores and an Titan X.  
"
1196,tensorflow using cpu+2 gpus turns out 3 times slower than theano using 1 gpu,"1. Does tensorflow automatically use all cpus and gpus to run the computational task?

2.I found that tensorflow use 1 5960X cpu and 2 Titan X gpus cost 300% time compared to theano use only 1 Titan X gpu, is there anything wrong with my configuration? I use the same model generated by keras, using different backend(theano and tensorflow)

I installed on Ubuntu 15.10, cuda 7.5, cudnn 4, gcc 4.9, installed from source code, built with GPU support.
"
1195,Dynamic Partition Gradient,"For bugs/issues, please fill in the following.  The more information you
provide, the more likely we can help you.
### Environment info

Operating System: Linux (CentOS) cluster, custom build. tensorflow built from source by IT folks managing the cluster. 

If installed from sources, provide the commit hash:
### Steps to reproduce
1. The functions of interest contain many dynamic_partitions
2. I then compute the gradient with respect to the variables for optimization
3. I obtain the following error
   
   train_op = optimizer.minimize(loss_instance, global_step=global_step)
   File ""/share/sw/free/tensorflow/0.6.0/tensorflow/python/training/optimizer.py"", line 186, in minimize
     aggregation_method=aggregation_method)
   File ""/share/sw/free/tensorflow/0.6.0/tensorflow/python/training/optimizer.py"", line 232, in compute_gradients
     aggregation_method=aggregation_method)
   File ""/share/sw/free/tensorflow/0.6.0/tensorflow/python/ops/gradients.py"", line 426, in gradients
     (op.name, op.type))
   LookupError: No gradient defined for operation 'DynamicPartition_1' (op type: DynamicPartition)
### What have you tried?
1. I am currently trying to see if I can remove the dynamic partition from my code, but it would be nice if we could keep the dynamic partition inside; however, I understand why the dynamic partition might not be differentiable if variable indices are used. In my case, I just have a constant placeholder that performs that indexing. Could there be a way to compute gradients in this case? Thank you so much!
"
1193,'utf-8' codec can't decode byte (in tutorial),"I'm getting an error ""'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte"" error when I try to run the MNIST tutorial (specifically, the dataset import).
### Environment info

Operating System: Ubuntu 15.04
Python: 3.4.3
Tensorflow from source, commit: 03bff43
### Steps to reproduce

1) Install fresh python3 venv (pyvenv venv)
2) Activate venv
3) uninstall and re-install protobuf 3.0.0a3 to fix [487](https://github.com/tensorflow/tensorflow/issues/487)
3) pip install ""built whl file""
4) In a python terminal, run:

from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets('MNIST_data', one_hot=True)
### Full Error Output

UnicodeDecodeError                        Traceback (most recent call last)
<ipython-input-1-4fcb59292e9b> in <module>()
      1 import tensorflow
      2 from tensorflow.examples.tutorials.mnist import input_data
----> 3 mnist = input_data.read_data_sets('MNIST_data', one_hot=True)

/home/ubuntu/venv/standard/lib/python3.4/site-packages/tensorflow/examples/tutorials/mnist/input_data.py in read_data_sets(train_dir, fake_data, one_hot, dtype)
    197 
    198   local_file = maybe_download(TRAIN_IMAGES, train_dir)
--> 199   train_images = extract_images(local_file)
    200 
    201   local_file = maybe_download(TRAIN_LABELS, train_dir)

/home/ubuntu/venv/standard/lib/python3.4/site-packages/tensorflow/examples/tutorials/mnist/input_data.py in extract_images(filename)
     56   print('Extracting', filename)
     57   with tf.gfile.Open(filename) as f, gzip.GzipFile(fileobj=f) as bytestream:
---> 58     magic = _read32(bytestream)
     59     if magic != 2051:
     60       raise ValueError(

/home/ubuntu/venv/standard/lib/python3.4/site-packages/tensorflow/examples/tutorials/mnist/input_data.py in _read32(bytestream)
     49 def _read32(bytestream):
     50   dt = numpy.dtype(numpy.uint32).newbyteorder('>')
---> 51   return numpy.frombuffer(bytestream.read(4), dtype=dt)[0]
     52 
     53 

/usr/lib/python3.4/gzip.py in read(self, size)
    363         else:               # just get some more of it
    364             while size > self.extrasize:
--> 365                 if not self._read(readsize):
    366                     if size > self.extrasize:
    367                         size = self.extrasize

/usr/lib/python3.4/gzip.py in _read(self, size)
    431             # jump to the next member, if there is one.
    432             self._init_read()
--> 433             if not self._read_gzip_header():
    434                 return False
    435             self.decompress = zlib.decompressobj(-zlib.MAX_WBITS)

/usr/lib/python3.4/gzip.py in _read_gzip_header(self)
    290 
    291     def _read_gzip_header(self):
--> 292         magic = self.fileobj.read(2)
    293         if magic == b'':
    294             return False

/usr/lib/python3.4/gzip.py in read(self, size)
     88             self._read = None
     89             return self._buffer[read:] + \
---> 90                    self.file.read(size-self._length+read)
     91 
     92     def prepend(self, prepend=b'', readprevious=False):

/home/ubuntu/venv/standard/lib/python3.4/site-packages/tensorflow/python/platform/default/_gfile.py in sync(self, _args, *_kwargs)
     43       if hasattr(self, '_locker'): self._locker.lock()
     44       try:
---> 45         return fn(self, _args, *_kwargs)
     46       finally:
     47         if hasattr(self, '_locker'): self._locker.unlock()

/home/ubuntu/venv/standard/lib/python3.4/site-packages/tensorflow/python/platform/default/_gfile.py in read(self, n)
    197       A string of the bytes read, up to the end of file.
    198     """"""
--> 199     return self._fp.read(n)
    200 
    201   @_synchronized

/usr/lib/python3.4/codecs.py in decode(self, input, final)
    317         # decode input (taking the buffer into account)
    318         data = self.buffer + input
--> 319         (result, consumed) = self._buffer_decode(data, self.errors, final)
    320         # keep undecoded input until the next call
    321         self.buffer = data[consumed:]

UnicodeDecodeError: 'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte
"
1192,gcc: error: unrecognized command line option '-fcolor-diagnostics',"I had counted such problem when compiling tensorflow from source in mac system:
gcc: error: unrecognized command line option '-fcolor-diagnostics'
gcc: error: unrecognized command line option '-Wthread-safety'
gcc: error: unrecognized command line option '-Wself-assign'
ERROR: /Users/clhuang/Downloads/tensorflow/google/protobuf/BUILD:29:1: C++ compilation of rule '//google/protobuf:protobuf_lite' failed: osx_gcc_wrapper.sh failed: error executing command external/bazel_tools/tools/cpp/osx_gcc_wrapper.sh '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g0 -O2 -DNDEBUG ... (remaining 37 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException

And my gcc version is 4.9,when I just run the command /my/path/gcc-4.9 -Wthread-safety example.cc,
it also shows gcc: error: unrecognized command line option '-Wthread-safety'.could anyone give me some advise ,thanks in advance.
"
1191,Build of pip package with current HEAD of bazel fails,"Bazel built from it's current head (cc8fd606f2f1878cbb5ed0ff8b533e514818c9dd for me) fails to build the pip package of tensorflow due to exclamation marks occurring in the path of some files. Error message:

```
ERROR: /home/panmari/tensorflow/tensorflow/tensorboard/BUILD:31:1: //tensorflow/tensorboard:all_files: invalid label 'components/prism/tests/languages/css!+css-extras/entity_feature.test' in element 1233 of attribute 'srcs' in 'filegroup' rule: invalid target name 'components/prism/tests/languages/css!+css-extras/entity_feature.test': target names may not contain '!'.
ERROR: /home/panmari/tensorflow/tensorflow/tools/pip_package/BUILD:23:1: Target '//tensorflow/tensorboard:tensorboard' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:build_pip_package'.
ERROR: Loading failed; build aborted.
INFO: Elapsed time: 1.032s
```

Simply removing the exclamation mark in the offending paths by renaming them does make the build proceed again. But I still have to check if the tensorboard still works.
"
1187,Build fail: hd_warning_disable.h broken link,"Complete noob - please forgive my ignorance...

Ubuntu 15.10
python 2.7
cuda 7.5
cudnn 4.0
gcc  (Ubuntu 5.2.1-22ubuntu2) 5.2.1 20151010

git fetch

```
bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer
INFO: Found 1 target...
ERROR: missing input file '//third_party/gpus/cuda:include/thrust/detail/config/hd_warning_disable.h'.
ERROR: /home/keith/tensorflow/tensorflow/core/BUILD:1048:1: //tensorflow/core:gpu_runtime: missing input file '//third_party/gpus/cuda:include/thrust/detail/config/hd_warning_disable.h'.
Target //tensorflow/cc:tutorials_example_trainer failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /home/keith/tensorflow/tensorflow/core/BUILD:1048:1 1 input file(s) do not exist.
INFO: Elapsed time: 0.284s, Critical Path: 0.02s
root@NL015:/home/keith/tensorflow# 
```

... and the problem is fixed - repeated git fetch and bazel build a few hours later.
"
1185,Logistic Regression(LR) Results looks strange when using bias but good without bias,"I tried the LR for sparse dataset using the tensorflow package(0.7.0)
The following is part of my procedure:

weight_values=generateWeight([trainset.feature_num,1],name='weight')
bias=init_bias([1,1],name='bias')
sp_shape=tf.placeholder(tf.int64)
sp_indices=tf.placeholder(tf.int64)
sp_ids_value=tf.placeholder(tf.int64)
sp_features_value=tf.placeholder(tf.float32)
Y=tf.placeholder('float',name='Y')

sp_ids=tf.SparseTensor(sp_indices,sp_ids_value,sp_shape)
sp_values=tf.SparseTensor(sp_indices,sp_features_value,sp_shape)
**#Z=tf.nn.embedding_lookup_sparse(weight_values,sp_ids,sp_values);
Z_b=tf.nn.embedding_lookup_sparse(weight_values,sp_ids,sp_values)+bias**
predict_op=tf.sigmoid(Z_b,name='result')
# cost=tf.nn.sigmoid_cross_entropy_with_logits(Z,Y)

cost=tf.nn.sigmoid_cross_entropy_with_logits(Z_b,Y)
train_op=tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)

...
**but I find the results is  wired when using the bias,using the bias I find the abs(cost) became larger and larger and even the abs(bias) also became larger and larger.** 
IN the early iterations the results like this :
      1. labels[-1], j-sample 1662, i-bias 0.100000 cost 0.761811, z_b 0.045602
      2 labels[1], j-sample 823, i-bias 0.084886 cost 0.653277, z_b 0.081396
      3 labels[-1], j-sample 20802, i-bias 0.089683 cost 0.826316, z_b 0.088132
      4 labels[-1], j-sample 25965, i-bias 0.074462 cost 0.806052, z_b 0.074804
      5 labels[1], j-sample 10322, i-bias 0.059276 cost 0.664358, z_b 0.058433
      6 labels[-1], j-sample 23946, i-bias 0.064129 cost 0.795182, z_b 0.067642
      ......
but with more iteration the results like this :
 270504 labels[-1], j-sample 446, i-bias -248.318787 cost -250.818787, z_b -250.818787
 270505 labels[1], j-sample 10314, i-bias -248.328781 cost 248.306259, z_b -248.306259
 270506 labels[1], j-sample 3820, i-bias -248.318787 cost 247.367340, z_b -247.367340
 270507 labels[1], j-sample 2922, i-bias -248.308792 cost 248.276184, z_b -248.276184
 270508 labels[-1], j-sample 20797, i-bias -248.298798 cost -255.061432, z_b -255.061432
 270509 labels[-1], j-sample 19755, i-bias -248.308792 cost -251.686646, z _b -251.686646
 270510 labels[1], j-sample 9528, i-bias -248.318787 cost 248.405624, z_ b 248.405624

However if I donot use the bias the results looks good, what maybe the problems and I wonder the reasons very much.

Can anynone help me, Thank you!
"
1182,"Small bug in TensorBoard: Visualizing Learning tutorial [FIXED, pending website pushes]","There is a small bug in the code on the site: https://www.tensorflow.org/versions/r0.7/how_tos/summaries_and_tensorboard/index.html

b = tf.Variable(tf.zeros([10], name='bias'))
should be:

b = tf.Variable(tf.zeros([10]), name='bias')

The consequence is that when you view the graph, the 'bias' variable has no useful label and is simply Variable.
"
1181,tensorflow.gfile.Open() error with MNIST example in python 3.4,"The `tf.gfile.Open(filename)` function throws an error when a bytestream is read. The issue arises in tensorflow/examples/tutorials/mnist using python 3.4
### Environment info

Operating System:
Ubuntu LTS 14.04 64bit
1. pip package installed: 
   `sudo pip3 install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.0-cp34-none-linux_x86_64.whl`
2. The output from python -c ""import tensorflow; print(tensorflow.**version**)"". 
   `python3 -c ""import tensorflow; print(tensorflow.__version__)""
   I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally
   I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally
   I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally
   I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so locally
   I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally
   0.7.0
   `
   ### Steps to reproduce
   Run the mnist example in tensorflow with python3
   `from tensorflow.examples.tutorials.mnist import input_data
   mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)`
### Steps to overcome

Got rid of the tf.gfile.Open function call. Replaced the line:
`with tf.gfile.Open(filename) as f, gzip.GzipFile(fileobj=f) as bytestream:`
with
`with gzip.GzipFile(filename=filename) as bytestream:`
### Logs

`Extracting MNIST_data/train-labels-idx1-ubyte.gz`
`Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.`
`Extracting MNIST_data/train-images-idx3-ubyte.gz`
`Traceback (most recent call last):`
`File ""<stdin>"", line 1, in <module>`
`File ""/usr/local/lib/python3.4/dist-packages/tensorflow/examples/tutorials/mnist/input_data.py"", line 202, in read_data_sets`
`train_images = extract_images(local_file)`
`File ""/usr/local/lib/python3.4/dist-packages/tensorflow/examples/tutorials/mnist/input_data.py"", line 58, in extract_images`
`magic = _read32(bytestream)`
`File ""/usr/local/lib/python3.4/dist-packages/tensorflow/examples/tutorials/mnist/input_data.py"", line 51, in _read32`
`return numpy.frombuffer(bytestream.read(4), dtype=dt)[0]`
`File ""/usr/lib/python3.4/gzip.py"", line 365, in read`
`if not self._read(readsize):`
`File ""/usr/lib/python3.4/gzip.py"", line 433, in _read`
`if not self._read_gzip_header():`
`File ""/usr/lib/python3.4/gzip.py"", line 292, in _read_gzip_header`
`magic = self.fileobj.read(2)`
`File ""/usr/lib/python3.4/gzip.py"", line 90, in read`
`self.file.read(size-self._length+read)`
`File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/platform/default/_gfile.py"", line 45, in sync`
`return fn(self, *args, **kwargs)`
`File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/platform/default/_gfile.py"", line 199, in read`
`return self._fp.read(n)`
`File ""/usr/lib/python3.4/codecs.py"", line 319, in decode`
`(result, consumed) = self._buffer_decode(data, self.errors, final)`
`UnicodeDecodeError: 'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte`
"
1180,Installation Issue 0.7,"For bugs/issues, please fill in the following.  The more information you
provide, the more likely we can help you.
### Environment info

Operating System:
**Ubuntu 14.04**

**Python 2.7.11**

If installed from binary pip package, provide:
1. Which pip package you installed.

```
Name: pip
Version: 8.0.2
```
1. The output from python -c ""import tensorflow; print(tensorflow.**version**)"".

```
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/noname/tensorflow/lib/python2.7/site-packages/tensorflow/__init__.py"", line 23, in <module>
    from tensorflow.python import *
  File ""/home/noname/tensorflow/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 41, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/noname/tensorflow/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 35, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""/home/noname/tensorflow/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>
    from google.protobuf import descriptor as _descriptor
  File ""/home/noname/tensorflow/lib/python2.7/site-packages/google/protobuf/descriptor.py"", line 46, in <module>
    from google.protobuf.pyext import _message
ImportError: /home/noname/tensorflow/lib/python2.7/site-packages/google/protobuf/pyext/_message.so: undefined symbol: PyUnicodeUCS4_FromEncodedObject


Error importing tensorflow.  Unless you are using bazel,
you should not try to import tensorflow from its source directory;
please exit the tensorflow source tree, and relaunch your python interpreter
from there.
```

If installed from sources, provide the commit hash:
### Steps to reproduce
1. `pip install --upgrade virtualenv`
2. `virtualenv --system-site-packages ~/tensorflow`
3. Download Cuda Toolkit v 7.5 from: http://developer.download.nvidia.com/compute/cuda/7.5/Prod/local_installers/cuda-repo-ubuntu1404-7-5-local_7.5-18_amd64.deb
4. Install cuda toolkit 7.5:
   `sudo dpkg -i cuda-repo-ubuntu1404-7-5-local_7.5-18_amd64.deb`
   `sudo apt-get update`
   `sudo apt-get install cuda`
5. Download cuDNN v4: https://developer.nvidia.com/rdp/assets/cudnn-70-linux-x64-v40
6.  Followed by: 
   `tar xvzf cudnn-7.0-linux-x64-v4.0-prod.tgz`
   `sudo cp cuda/include/cudnn.h /usr/local/cuda/include`
   `sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64`
   `sudo chmod a+r /usr/local/cuda/lib64/libcudnn*`
7. Add the following to the end of ~/.bashrc file:
   `export LD_LIBRARY_PATH=""$LD_LIBRARY_PATH:/usr/local/cuda/lib64""`
   `export CUDA_HOME=/usr/local/cuda`
8. Activate the Virtualenv environment created in step 2: `source ~/tensorflow/bin/activate`
9. Finally install tensorflow with GPU support enabled: `pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.0-py2-none-linux_x86_64.whl`
### What have you tried?
1. I have tried to test the tensorflow installation as described here: https://www.tensorflow.org/versions/r0.7/get_started/os_setup.html#test-the-tensorflow-installation
   and it gives me the same import error as above.
2. I deleted all of the virtual folder (incl contents) and removed cuda toolkit and cudNN and installed them, repeating 3 times, but it doesn't seem to go pass the import error. 

Please if anyone could help out, a whole lot of time has gone by with this ...
"
1179,libcuda suffix issue,"For bugs/issues, please fill in the following.  The more information you
provide, the more likely we can help you.
### Environment info

Operating System: Centos 7
Cuda 7.5, cuDNN v4

If installed from sources, provide the commit hash: 99813c26f9fae11b2d6a6f8d12e68493cb06f73c
### Steps to reproduce
1. Followed ""Installing from sources"" instructions
2. `./configure` - run and cuda version explicitly set to 7.5 
3. `bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer` - runs without issue
4. `bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu` - fails (relevant log below) due to an attempt to load libcuda.so.7.5 via `tensorflow/stream_executor/dso_loader.cc`. 
### What have you tried?
1. Attempting to create a symbolic link of `libcuda.so.7.5` pointing at `libcuda.so` did not work.
2. Editing `tensorflow/stream_executor/dso_loader.cc` line 99 from:

``` c
/* static */ port::Status DsoLoader::GetLibcudaDsoHandle(void** dso_handle) {
  return GetDsoHandle(FindDsoPath(""libcuda.so"" + GetCudaVersion(),
                                  ""third_party/gpus/cuda/driver/lib64""),
                      dso_handle);
}
```

to

``` c
/* static */ port::Status DsoLoader::GetLibcudaDsoHandle(void** dso_handle) {
  return GetDsoHandle(FindDsoPath(""libcuda.so"", // + GetCudaVersion(),
                                  ""third_party/gpus/cuda/driver/lib64""),
                      dso_handle);
}
```

fixed the issue.
3. This naming convention issue also appears to be the case on Ubuntu 14.04 - but I have not attempted to rebuilt everything there.
### Logs or other output that would be helpful

```
[tensorflow]$ bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:99] Couldn't open CUDA library libcuda.so.7.5. LD_LIBRARY_PATH:
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:121] hostname: ---
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:146] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program
...
```
"
1178,Use of angle brackets around file names for include statements,"Would you like to [replace any double quotes](https://github.com/tensorflow/tensorflow/blob/31849e0a44a1c9c99c4524a65c9ee90e80c41499/tensorflow/core/common_runtime/device.h#L35) by [angle brackets around file names for include statements](http://stackoverflow.com/questions/21593/what-is-the-difference-between-include-filename-and-include-filename)?
"
1173,Problems with viewing a graph using the mnist tutorial code,"For bugs/issues, please fill in the following.  The more information you
provide, the more likely we can help you.
### Environment info

Operating System:Ubuntu 14.0

If installed from binary pip package, provide:
1. Which pip package you installed. 
2. The output from python -c ""import tensorflow; print(tensorflow.**version**)"".
   0.06
   If installed from sources, provide the commit hash:
### Steps to reproduce
1. This doesn't make generate a graph
   summary_writer = tf.train.SummaryWriter(train_dir,graph_def=sess.graph_def)
2. This generates a graph
   summary_writer= tf.python.training.summary_io.SummaryWriter(train_dir,graph_def=sess.graph_def)
### What have you tried?

1.
### Logs or other output that would be helpful

(If logs are large, please upload as attachment).
"
1171,Undefined reference to symbol 'ceil@@GLIBC_2.2.5' at build time,"I'm working on compiling Tensorflow from source, using non-standard GCC/etc. installations.  Environment info: RHEL 6.7, GCC 5.2.1, Bazel 0.1.5.  I'm installing Tensorflow from HEAD (commit f82ad36).  I'm using a non-CUDA configuration.  I've followed the steps @sethbruder suggests in his comment on [bazel#649](https://github.com/bazelbuild/bazel/issues/649), including copying the contents of `tools` from bazel into `tensorflow/tools/` and into `tensorflow/google/protobuf/tools/`.  This is possibly related to #332, as I'm getting the same error, but at build time as opposed to API usage.

I've tried to set up the relevant paths for my non-standard system resource install using the following settings before invoking bazel:

``` sh
export LDFLAGS=""-Wl,-rpath,/opt/rh/devtoolset-4/root/usr/lib64 -lrt -lm""
export CC=""/opt/rh/devtoolset-4/root/usr/bin/gcc""
export CXX=""/opt/rh/devtoolset-4/root/usr/bin/g++""
export JAVA_HOME=""/u/drspeech/opt/jdks/jdk1.8.0_25""
export LD_LIBRARY_PATH=""/opt/rh/devtoolset-4/root/usr/lib:${LD_LIBRARY_PATH}""
export BAZEL_ARGS=""--verbose_failures""
export EXTRA_BAZEL_ARGS=""${EXTRA_BAZEL_ARGS} --linkopt=-Wl,-rpath,/opt/rh/devtoolset-4/root/usr/lib64""
export EXTRA_BAZEL_ARGS=""${EXTRA_BAZEL_ARGS} --linkopt=-Wl,-rpath,/u/drspeech/opt/jdks/jdk1.8.0_25/lib""
export EXTRA_BAZEL_ARGS=""${EXTRA_BAZEL_ARGS} --linkopt=-lz""
#export EXTRA_BAZEL_ARGS=""${BAZEL_ARGS} --linkopt=-Wl,-rpath,/usr/local/cuda-7.0/lib64""
export PYTHON_MAJOR_VERSION=3
export PYTHON_BINARY=/u/drspeech/opt/python-3.5.1/bin/python3
export MYBAZEL=/u/drspeech/opt/bazel-0.1.5/0.1.5/bazel-0.1.5/output/bazel
```

(there may be some leftover settings; this is adapted from what I used for building bazel in [bazel#925](https://github.com/bazelbuild/bazel/issues/925))

When I invoke bazel with

``` sh
${MYBAZEL} build -c opt //tensorflow/tools/pip_package:build_pip_package
```

I get the following error output:

```
WARNING: Output base '/homes/2/griffisd/.cache/bazel/_bazel_griffisd/294e12ab714f8384c060bacb49311f55' is on NFS. This may lead to surprising failures and undetermined behavior.
WARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.
____Loading...
____Found 1 target...
____Building...
____[1 / 12] Compiling google/protobuf/python/google/protobuf/pyext/descriptor.cc
____[1 / 147] Compiling external/re2/re2/compile.cc
ERROR: /homes/0/drspeech/opt/tensorflow-0.6.0/0.7.0/tensorflow/google/protobuf/BUILD:272:1: Linking of rule '//google/protobuf:protoc' failed: gcc failed: error executing command 
  (cd /homes/2/griffisd/.cache/bazel/_bazel_griffisd/294e12ab714f8384c060bacb49311f55/tensorflow && \
  exec env - \
  /opt/rh/devtoolset-4/root/usr/bin/gcc -o bazel-out/host/bin/google/protobuf/protoc -B/opt/rh/devtoolset-4/root/usr/bin/ -Wl,-z,relro,-z,now -no-canonical-prefixes -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -Wl,-S -Wl,--gc-sections -Wl,@bazel-out/host/bin/google/protobuf/protoc-2.params): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: gcc failed: error executing command 
  (cd /homes/2/griffisd/.cache/bazel/_bazel_griffisd/294e12ab714f8384c060bacb49311f55/tensorflow && \
  exec env - \
  /opt/rh/devtoolset-4/root/usr/bin/gcc -o bazel-out/host/bin/google/protobuf/protoc -B/opt/rh/devtoolset-4/root/usr/bin/ -Wl,-z,relro,-z,now -no-canonical-prefixes -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -Wl,-S -Wl,--gc-sections -Wl,@bazel-out/host/bin/google/protobuf/protoc-2.params): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
/opt/rh/devtoolset-4/root/usr/bin/ld: /opt/rh/devtoolset-4/root/usr/lib/gcc/x86_64-redhat-linux/5.2.1/libstdc++_nonshared.a(hashtable_c++0x.o): undefined reference to symbol 'ceil@@GLIBC_2.2.5'
//lib64/libm.so.6: error adding symbols: DSO missing from command line
collect2: error: ld returned 1 exit status
Target //tensorflow/tools/pip_package:build_pip_package failed to build
____Elapsed time: 0.707s, Critical Path: 0.34s
```

It seems like I may be missing an ""-lm"" flag in the invocation to gcc to the protobuf target; I've tried including it in LDFLAGS as shown above, but it doesn't seem to be appearing in the gcc invocation.
"
1169,"Broken link in ""A Tool Developer's Guide..."" for graph_metrics.py","Here is the link for `graph_metrics.py` in [this tutorial](https://www.tensorflow.org/versions/r0.7/how_tos/tool_developers/index.html#graphdef) but it's broken:

https://github.com/tensorflow/tensorflow/blob/master/tensorfl%0Aow/python/tools/graph_metrics.py

Update:
Just found other links to the files, such as `freeze_graph()` and `tensorflow/core/framework/tensor.proto` also not working.
"
1168,Program using GPU and CPU mix got slower in  Tensorflow 0.7,"I have a Titan X, 
On 0.6, code running well and fast 5 sec for 10 epoch.
I upgrade to 0.7, code running so slow 85 sec for 10 epoch and no matter python2.7 or python3.4.
GPU always on and detected so no CPU is used (unless it has to : ""allow_soft_placement=True"").
With nvidia-smi, i checked that the GPU was 40 % used in 0.6 version. And it is 97% used in 0.7 version.
So, the GPU is more used for a worst training time ?
"
1167,Unsupported Wheel,"### Environment info

Operating System: Ubuntu 15.10

If installed from binary pip package, provide:
1. pip package: pip3
   ### Steps to reproduce
2. I installed pip3 and Python3 along with the build-essentials setup
3. I installed Scipy Pack comprising numpy, and the rest.
4. I tried the pip3 install for CPU using the instructions provided [here](https://www.tensorflow.org/versions/r0.7/get_started/os_setup.html#pip-installation) for 64 bits CPU
### What have you tried?
1. I read the error file but I did not understand it
### Logs or other output that would be helpful

---

/usr/bin/pip3 run on Thu Feb 18 04:37:08 2016
tensorflow-0.7.0-py3-none-linux_x86_64.whl is not a supported wheel on this platform.
Exception information:
Traceback (most recent call last):
  File ""/usr/lib/python3/dist-packages/pip/basecommand.py"", line 122, in main
    status = self.run(options, args)
  File ""/usr/lib/python3/dist-packages/pip/commands/install.py"", line 283, in run
    InstallRequirement.from_line(name, None))
  File ""/usr/lib/python3/dist-packages/pip/req.py"", line 168, in from_line
    raise UnsupportedWheel(""%s is not a supported wheel on this platform."" % wheel.filename)
pip.exceptions.UnsupportedWheel: tensorflow-0.7.0-py3-none-linux_x86_64.whl is not a supported wheel on this platform.
"
1165,Can tensor take advantage of  multiple machines with multiple gpus?,"As title
"
1162,Inconsistent Naming with different input shapes,"Install from binary pip package
1. Which pip package you installed. CPU only for Linux
2. The output from python -c ""import tensorflow; print(tensorflow.**version**)"": 0.7.0
### Steps to reproduce

Run the following code:

```
import tensorflow as tf

a = tf.placeholder(tf.float32, name='haha', shape=[128, 20, 20, 20])
c = tf.nn.moments(a, axes=[0])[0]
print c.name

with tf.Graph().as_default():
    a = tf.placeholder(tf.float32, name='haha', shape=[None, 20, 20, 20])
    c = tf.nn.moments(a, axes=[0])[0]
    print c.name
```

It prints:

```
moments/Squeeze:0
moments/Squeeze_1:0
```

In short, with different input shapes, the op returns a variable with different names.
I'm not sure whether this is expected to happen, or something to be fixed. IMHO, this could cause some problems because one may use a fixed batch size for training, and a `None` batch size for inference. Getting a different name make it harder to manage variable load & restore.
"
1161,Trouble importing tensorflow after installing with virtualenv,"### Environment info

Fedora 19
Python2.7.8
pip 8.0.2
### Steps to reproduce
1. I installed tensorflow from the Download and Setup page using virtualenv. 
2. When I typed ""import tensorflow as tf"" I got the following error:

"">>> import tensorflow
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/emerson/tensorflow/lib/python2.7/site-packages/tensorflow/**init**.py"", line 23, in <module>
    from tensorflow.python import *
  File ""/home/emerson/tensorflow/lib/python2.7/site-packages/tensorflow/python/**init**.py"", line 69, in <module>
    from tensorflow.python.training import training as train
  File ""/home/emerson/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/training.py"", line 149, in <module>
    from tensorflow.python.training.saver import generate_checkpoint_state_proto
  File ""/home/emerson/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 29, in <module>
    from google.protobuf.any_pb2 import Any
ImportError: No module named any_pb2""

I have searched for this error and have not seen a clear solution. I am new with linux and tensorflow. I think I might need to set the PYTHONPATH or get out of the working directory that contains tensorflow. However I am not sure how to do either of those things. 

Thanks
"
1158,Kernel version vs DSO version mismatch when running example after bazel build,"After successful bazel build, running into error when executing example with gpu. Please suggest on how to resolve. The GPU has been used with MathConvNet (another deep learning package) without this error.
## Log:

`bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally
E tensorflow/stream_executor/cuda/cuda_driver.cc:481] failed call to cuInit: CUDA_ERROR_NO_DEVICE
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:114] retrieving CUDA diagnostic information for host: en4113750l
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:121] hostname: en4113750l
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:146] libcuda reported version is: 352.63
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:257] driver version file contents: """"""NVRM version: NVIDIA UNIX x86_64 Kernel Module  352.55  Thu Oct  8 15:18:00 PDT 2015
GCC version:  gcc version 4.8.4 (Ubuntu 4.8.4-2ubuntu1~14.04) 
""""""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:150] kernel reported version is: 352.55
E tensorflow/stream_executor/cuda/cuda_diagnostics.cc:229] kernel version 352.55 does not match DSO version 352.63 -- cannot find working devices in this configuration
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 
F tensorflow/cc/tutorials/example_trainer.cc:116] Check failed: ::tensorflow::Status::OK() == (session->Run({{""x"", x}}, {""y:0"", ""y_normalized:0""}, {}, &outputs)) (OK vs. Invalid argument: Cannot assign a device to node 'Const/_2': Could not satisfy explicit device specification '/gpu:0'
     [[Node: Const/_2 = Const[dtype=DT_INT32, value=Tensor<type: int32 shape: [] values: 0>, _device=""/gpu:0""]()]])
F tensorflow/cc/tutorials/example_trainer.cc:116] Check failed: ::tensorflow::Status::OK() == (session->Run({{""x"", x}}, {""y:0"", ""y_normalized:0""}, {}, &outputs)) (OK vs. Invalid argument: Cannot assign a device to node 'Const/_2': Could not satisfy explicit device specification '/gpu:0'
     [[Node: Const/_2 = Const[dtype=DT_INT32, value=Tensor<type: int32 shape: [] values: 0>, _device=""/gpu:0""]()]])
F tensorflow/cc/tutorials/example_trainer.cc:116] Check failed: ::tensorflow::Status::OK() == (session->Run({{""x"", x}}, {""y:0"", ""y_normalized:0""}, {}, &outputs)) (OK vs. Invalid argument: Cannot assign a device to node 'Const/_2': Could not satisfy explicit device specification '/gpu:0'
     [[Node: Const/_2 = Const[dtype=DT_INT32, value=Tensor<type: int32 shape: [] values: 0>, _device=""/gpu:0""]()]])
Aborted (core dumped)`
## Specifications:

Ubuntu LTS 14.04
Titan X GPU
CUDA 7.5
"
1157,Still seeing 'missing dependency declarations' when building with Bazel 0.1.4/0.1.5,"I'm still getting the error from #469, even with the fix in #788.

Debian, gcc 4.9.2, bazel 0.1.5 (tried 0.1.4 as well with the same results).

Command line:

```
bazel build -c opt --config=cuda --verbose_failures --spawn_strategy=standalone  //tensorflow/cc:tutorials_example_trainer
```

Error output:

```
ERROR: /usr/local/src/tensorflow/tensorflow/core/BUILD:334:1: undeclared inclusion(s) in rule '//tensorflow/core:gpu_kernels':
this rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/cwise_op_gpu_cos.cu.cc':
  '/usr/local/cuda/include/vector_types.h'
  '/usr/local/cuda/include/math_constants.h'
  '/usr/local/cuda/include/cuda_runtime.h'
  '/usr/local/cuda/include/curand_kernel.h'
  '/usr/local/cuda/include/curand.h'
  '/usr/local/cuda/include/curand_discrete.h'
  '/usr/local/cuda/include/curand_precalc.h'
  '/usr/local/cuda/include/curand_mrg32k3a.h'
  '/usr/local/cuda/include/curand_mtgp32_kernel.h'
  '/usr/local/cuda/include/cuda.h'
  '/usr/local/cuda/include/curand_mtgp32.h'
  '/usr/local/cuda/include/curand_philox4x32_x.h'
  '/usr/local/cuda/include/curand_globals.h'
  '/usr/local/cuda/include/curand_uniform.h'
  '/usr/local/cuda/include/curand_normal.h'
  '/usr/local/cuda/include/curand_normal_static.h'
  '/usr/local/cuda/include/curand_lognormal.h'
  '/usr/local/cuda/include/curand_poisson.h'
  '/usr/local/cuda/include/curand_discrete2.h'.
Target //tensorflow/cc:tutorials_example_trainer failed to build
```
"
1150,tensorflow 0.7.0 gpu-enabled version crashes bad on import,"DISTRIB_CODENAME=trusty
DISTRIB_DESCRIPTION=""Ubuntu 14.04 LTS (upgraded from: Ubuntu 12.04.4 LTS)""

Linux .... 3.13.0-76-generic #120-Ubuntu SMP Mon Jan 18 15:59:10 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux

I used the trick of renaming the wheel to:
  tensorflow-0.7.0-cp34-none-linux_x86_64.whl
to get it installed

launched ipython via:

``` python
CUDA_HOME=/usr/local/cuda-7.0 LD_LIBRARY_PATH=/usr/local/cuda-7.0/lib64/ ipython3

In [1]: import tensorflow
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-1-a649b509054f> in <module>()
----> 1 import tensorflow

/usr/local/lib/python3.4/dist-packages/tensorflow/__init__.py in <module>()
     21 from __future__ import print_function
     22 
---> 23 from tensorflow.python import *

/usr/local/lib/python3.4/dist-packages/tensorflow/python/__init__.py in <module>()
     47 
     48 # Import things out of contrib
---> 49 from tensorflow import contrib
     50 
     51 # Framework

/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/__init__.py in <module>()
     21 
     22 # Add projects here, they will show up under tf.contrib.
---> 23 from tensorflow.contrib import layers
     24 from tensorflow.contrib import util

/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/layers/__init__.py in <module>()
     66 # pylint: disable=unused-import,wildcard-import
     67 from tensorflow.contrib.layers.python.framework.tensor_util import *
---> 68 from tensorflow.contrib.layers.python.layers import *

/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/layers/python/layers/__init__.py in <module>()
     20 
     21 # pylint: disable=wildcard-import
---> 22 from tensorflow.contrib.layers.python.layers.initializers import *
     23 from tensorflow.contrib.layers.python.layers.layers import *
     24 from tensorflow.contrib.layers.python.layers.regularizers import *

/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/layers/python/layers/initializers.py in <module>()
     22 
     23 from tensorflow.python.framework import dtypes
---> 24 from tensorflow.python.ops import random_ops
     25 
     26 

/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/random_ops.py in <module>()
     21 
     22 from tensorflow.python.framework import dtypes
---> 23 from tensorflow.python.framework import ops
     24 from tensorflow.python.framework import tensor_shape
     25 from tensorflow.python.framework import tensor_util

/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py in <module>()
     37 from tensorflow.python.framework import registry
     38 from tensorflow.python.framework import tensor_shape
---> 39 from tensorflow.python.framework import versions
     40 from tensorflow.python.util import compat
     41 from tensorflow.python.platform import logging

/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/versions.py in <module>()
     20 from __future__ import print_function
     21 
---> 22 from tensorflow.python import pywrap_tensorflow
     23 
     24 __version__ = pywrap_tensorflow.__version__

/usr/local/lib/python3.4/dist-packages/tensorflow/python/pywrap_tensorflow.py in <module>()
     26                 fp.close()
     27             return _mod
---> 28     _pywrap_tensorflow = swig_import_helper()
     29     del swig_import_helper
     30 else:

/usr/local/lib/python3.4/dist-packages/tensorflow/python/pywrap_tensorflow.py in swig_import_helper()
     22         if fp is not None:
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
     25             finally:
     26                 fp.close()

/usr/lib/python3.4/imp.py in load_module(name, file, filename, details)
    241                 return load_dynamic(name, filename, opened_file)
    242         else:
--> 243             return load_dynamic(name, filename, file)
    244     elif type_ == PKG_DIRECTORY:
    245         return load_package(name, filename)

ImportError: /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so: undefined symbol: PyClass_Type

In [2]: 
Do you really want to exit ([y]/n)? y
libgcc_s.so.1 must be installed for pthread_cancel to work
Aborted (core dumped)
```

---

Note the core dump at the bottom!
"
1148,Issue and Pull Request Templates,"Would help with organization and getting all the relevant info. Requires adding files issue_template.md and pull_request_template.md
"
1147,Python 3 test failure: //tensorflow/tensorboard/backend:server_test,"This seems to on both Linux and Mac, under Python 3:
`bazel test //tensorflow/tensorboard/backend:server_test`

Failure log:

> ---
> 
> .127.0.0.1 - - [17/Feb/2016 18:23:02] code 404, message Not Found
> 127.0.0.1 - - [17/Feb/2016 18:23:02] ""GET /../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../etc/passwd HTTP/1.1"" 404 -
> .127.0.0.1 - - [17/Feb/2016 18:23:03] ""GET /data/histograms?tag=histogram&run=run1 HTTP/1.1"" 200 -
> E127.0.0.1 - - [17/Feb/2016 18:23:03] ""GET /data/images?tag=image&run=run1 HTTP/1.1"" 200 -
> E127.0.0.1 - - [17/Feb/2016 18:23:04] ""GET / HTTP/1.1"" 200 -
> ./usr/local/google/home/cais/.cache/bazel/_bazel_cais/10ddfdc323bb20c9bc23987d4925ae7b/tensorflow-caisq/bazel-out/local_linux-py3-fastbuild/bin/tensorflow/tensorboard/backend/server_test.runfiles/tensorflow/python/platform/default/_resource_loader.py:49: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
>   logging.warning('IOError %s on path %s', e, path)
> WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/usr/local/google/home/cais/.cache/bazel/_bazel_cais/10ddfdc323bb20c9bc23987d4925ae7b/tensorflow-caisq/bazel-out/local_linux-py3-fastbuild/bin/tensorflow/tensorboard/backend/server_test.runfiles/tensorflow/tensorboard/asdf' on path /usr/local/google/home/cais/.cache/bazel/_bazel_cais/10ddfdc323bb20c9bc23987d4925ae7b/tensorflow-caisq/bazel-out/local_linux-py3-fastbuild/bin/tensorflow/tensorboard/backend/server_test.runfiles/tensorflow/tensorboard/asdf
> 127.0.0.1 - - [17/Feb/2016 18:23:04] code 404, message Not Found
> 127.0.0.1 - - [17/Feb/2016 18:23:04] ""GET /asdf HTTP/1.1"" 404 -
> .127.0.0.1 - - [17/Feb/2016 18:23:05] ""GET /data/runs HTTP/1.1"" 200 -
> E127.0.0.1 - - [17/Feb/2016 18:23:05] ""GET /data/scalars?sample_count=10 HTTP/1.1"" 200 -
> E127.0.0.1 - - [17/Feb/2016 18:23:06] ""GET /data/scalars?sample_count=999999 HTTP/1.1"" 200 -
> # E.
> 
> ERROR: testHistograms (**main**.TensorboardServerTest)
> ## Test the format of /data/histograms.
> 
> Traceback (most recent call last):
>   File ""/usr/local/google/home/cais/.cache/bazel/_bazel_cais/10ddfdc323bb20c9bc23987d4925ae7b/tensorflow-caisq/bazel-out/local_linux-py3-fastbuild/bin/tensorflow/tensorboard/backend/server_test.runfiles/tensorflow/tensorboard/backend/server_test.py"", line 105, in testHistograms
>     self._getJson('/data/histograms?tag=histogram&run=run1'),
>   File ""/usr/local/google/home/cais/.cache/bazel/_bazel_cais/10ddfdc323bb20c9bc23987d4925ae7b/tensorflow-caisq/bazel-out/local_linux-py3-fastbuild/bin/tensorflow/tensorboard/backend/server_test.runfiles/tensorflow/tensorboard/backend/server_test.py"", line 71, in _getJson
>     return json.load(response)
>   File ""/usr/lib/python3.4/json/__init__.py"", line 268, in load
>     parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)
>   File ""/usr/lib/python3.4/json/**init**.py"", line 312, in loads
>     s.**class**.**name**))
> TypeError: the JSON object must be str, not 'bytes'
> # 
> 
> ERROR: testImages (**main**.TensorboardServerTest)
> ## Test listing images and retrieving an individual image.
> 
> Traceback (most recent call last):
>   File ""/usr/local/google/home/cais/.cache/bazel/_bazel_cais/10ddfdc323bb20c9bc23987d4925ae7b/tensorflow-caisq/bazel-out/local_linux-py3-fastbuild/bin/tensorflow/tensorboard/backend/server_test.runfiles/tensorflow/tensorboard/backend/server_test.py"", line 127, in testImages
>     image_json = self._getJson('/data/images?tag=image&run=run1')
>   File ""/usr/local/google/home/cais/.cache/bazel/_bazel_cais/10ddfdc323bb20c9bc23987d4925ae7b/tensorflow-caisq/bazel-out/local_linux-py3-fastbuild/bin/tensorflow/tensorboard/backend/server_test.runfiles/tensorflow/tensorboard/backend/server_test.py"", line 71, in _getJson
>     return json.load(response)
>   File ""/usr/lib/python3.4/json/__init__.py"", line 268, in load
>     parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)
>   File ""/usr/lib/python3.4/json/**init**.py"", line 312, in loads
>     s.**class**.**name**))
> TypeError: the JSON object must be str, not 'bytes'
> # 
> 
> ERROR: testRuns (**main**.TensorboardServerTest)
> ## Test the format of the /data/runs endpoint.
> 
> Traceback (most recent call last):
>   File ""/usr/local/google/home/cais/.cache/bazel/_bazel_cais/10ddfdc323bb20c9bc23987d4925ae7b/tensorflow-caisq/bazel-out/local_linux-py3-fastbuild/bin/tensorflow/tensorboard/backend/server_test.runfiles/tensorflow/tensorboard/backend/server_test.py"", line 95, in testRuns
>     self._getJson('/data/runs'),
>   File ""/usr/local/google/home/cais/.cache/bazel/_bazel_cais/10ddfdc323bb20c9bc23987d4925ae7b/tensorflow-caisq/bazel-out/local_linux-py3-fastbuild/bin/tensorflow/tensorboard/backend/server_test.runfiles/tensorflow/tensorboard/backend/server_test.py"", line 71, in _getJson
>     return json.load(response)
>   File ""/usr/lib/python3.4/json/__init__.py"", line 268, in load
>     parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)
>   File ""/usr/lib/python3.4/json/**init**.py"", line 312, in loads
>     s.**class**.**name**))
> TypeError: the JSON object must be str, not 'bytes'
> # 
> 
> ERROR: testSampleScalars (**main**.TensorboardServerTest)
> ## Test the sample_count parameter of /data/scalars.
> 
> Traceback (most recent call last):
>   File ""/usr/local/google/home/cais/.cache/bazel/_bazel_cais/10ddfdc323bb20c9bc23987d4925ae7b/tensorflow-caisq/bazel-out/local_linux-py3-fastbuild/bin/tensorflow/tensorboard/backend/server_test.runfiles/tensorflow/tensorboard/backend/server_test.py"", line 111, in testSampleScalars
>     samples = self._getJson('/data/scalars?sample_count=%d' % i)
>   File ""/usr/local/google/home/cais/.cache/bazel/_bazel_cais/10ddfdc323bb20c9bc23987d4925ae7b/tensorflow-caisq/bazel-out/local_linux-py3-fastbuild/bin/tensorflow/tensorboard/backend/server_test.runfiles/tensorflow/tensorboard/backend/server_test.py"", line 71, in _getJson
>     return json.load(response)
>   File ""/usr/lib/python3.4/json/__init__.py"", line 268, in load
>     parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)
>   File ""/usr/lib/python3.4/json/**init**.py"", line 312, in loads
>     s.**class**.**name**))
> TypeError: the JSON object must be str, not 'bytes'
> # 
> 
> ERROR: testSampleScalarsWithLargeSampleCount (**main**.TensorboardServerTest)
> ## Test using a large sample_count.
> 
> Traceback (most recent call last):
>   File ""/usr/local/google/home/cais/.cache/bazel/_bazel_cais/10ddfdc323bb20c9bc23987d4925ae7b/tensorflow-caisq/bazel-out/local_linux-py3-fastbuild/bin/tensorflow/tensorboard/backend/server_test.runfiles/tensorflow/tensorboard/backend/server_test.py"", line 121, in testSampleScalarsWithLargeSampleCount
>     samples = self._getJson('/data/scalars?sample_count=999999')
>   File ""/usr/local/google/home/cais/.cache/bazel/_bazel_cais/10ddfdc323bb20c9bc23987d4925ae7b/tensorflow-caisq/bazel-out/local_linux-py3-fastbuild/bin/tensorflow/tensorboard/backend/server_test.runfiles/tensorflow/tensorboard/backend/server_test.py"", line 71, in _getJson
>     return json.load(response)
>   File ""/usr/lib/python3.4/json/__init__.py"", line 268, in load
>     parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)
>   File ""/usr/lib/python3.4/json/**init**.py"", line 312, in loads
>     s.**class**.**name**))
> TypeError: the JSON object must be str, not 'bytes'
> ---
> 
> Ran 10 tests in 5.303s
> 
> FAILED (errors=5)
"
1146,Python 3 test failure: //tensorflow/python:default_platform_gfile_test,"This happens on both Linux and Mac under Python 3:
`bazel test //tensorflow/python:default_platform_gfile_test`

Failure log:

> ---
> # ..........E........
> ## ERROR: testOpen (**main**.FunctionTests)
> 
> Traceback (most recent call last):
>   File ""/usr/local/google/home/cais/.cache/bazel/_bazel_cais/10ddfdc323bb20c9bc23987d4925ae7b/tensorflow-caisq/bazel-out/local_linux-py3-fastbuild/bin/tensorflow/python/default_platform_gfile_test.runfiles/tensorflow/python/platform/default/gfile_test.py"", line 225, in testOpen
>     f.write(""foo"")
>   File ""/usr/local/google/home/cais/.cache/bazel/_bazel_cais/10ddfdc323bb20c9bc23987d4925ae7b/tensorflow-caisq/bazel-out/local_linux-py3-fastbuild/bin/tensorflow/python/default_platform_gfile_test.runfiles/tensorflow/python/platform/default/_gfile.py"", line 45, in sync
>     return fn(self, _args, *_kwargs)
>   File ""/usr/local/google/home/cais/.cache/bazel/_bazel_cais/10ddfdc323bb20c9bc23987d4925ae7b/tensorflow-caisq/bazel-out/local_linux-py3-fastbuild/bin/tensorflow/python/default_platform_gfile_test.runfiles/tensorflow/python/platform/default/_gfile.py"", line 98, in write
>     self._fp.write(data)
> TypeError: 'str' does not support the buffer interface
> ---
> 
> Ran 19 tests in 0.043s
> 
> FAILED (errors=1)
"
1145,Specify --no-install-recommends in apt-get install in Dockerfiles,"Explicitly list needed packaged in Dockerfile and then add `--no-install-recommends` so that unneeded packaged are not added. For example right now install OpenJDK tried to install fuse.
- `unzip` needs to be added to the list of packages to install.
"
1143,"interpreter crashes with SIGSEGV (tf 0.7.0-cpu, python 3.5)","I pip-installed the 0.7.0-cpu (linux) release on a python 3.5 conda environment, and now get the following error:

```
Python 3.5.1 |Continuum Analytics, Inc.| (default, Dec  7 2015, 11:16:01) 
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
fish: “python” terminated by signal SIGSEGV (Address boundary error)
```

I also installed the 2.7 version in a separate conda environment, and everything works fine.  Both environments are up-to-date (`conda upgrade --all`).  Any ideas?
"
1142,0.7.0 pip packages are not named properly (error: is not a supported wheel on this platform).  [Will be fixed in next binary release],"My os is ubuntu 14.04
My python version is 2.7.6

Following the instructions for installing with pip I am getting this error:

> $ sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.0-py2-none-linux_x86_64.whl
> tensorflow-0.7.0-py2-none-linux_x86_64.whl is not a supported wheel on this platform.
> Storing debug log for failure in /home/terence/.pip/pip.log

It works fine installing older releases of tensorflow with pip but not this one.
"
1141,support different horizontal axises for different scalar summaries,"Is it possible to have different horizontal axises for different scalar summaries?
For example, I want to have a scalar summary of loss for each global step, and a scalar summary of network outputs for each epoch.
Currently, I have the same horizontal axis showing the global step for both like the following image.
Is it possible to have the global step (0 ... 8.0k) for `loss` and the epoch (0 ... 2.0k) for `output_[0,0]`?

![image](https://cloud.githubusercontent.com/assets/6128440/13111093/405a40b4-d5c7-11e5-9d03-566f19c76b2c.png)

If I set the epoch for `output_[0,0]`, I get this:

![image](https://cloud.githubusercontent.com/assets/6128440/13112966/519aa52c-d5d0-11e5-8ead-552ac22898e2.png)
"
1140,Math ops support for float64,"I am raising this issue following [this stackoverflow discussion](https://stackoverflow.com/questions/35443080/tensorflow-critical-graph-operations-assigned-to-cpu-rather-than-gpu?lq=1). I found that there is not that much support for float64 ops, that is needed for some numerical applications. A number of other issues  #1061 #761 #547 and PR #1089 have been raised around this.

On the basis of how my graph is initialised, things like **MatMul** (also raised in this other [stackoverflow question](https://stackoverflow.com/questions/35428297/tensorflow-issue-with-gpu-on-matmul-gpu-isnt-recognized?lq=1)), **Mean, L2Loss, Mean_grad/Prod, Mul_grad/Sum, AddN, SoftmaxCrossEntropyWithLogits** are not implemented for float64.

Would it be a good idea to extend PR #1089 to more ops?  
"
1136,Support for 1D convolutions,"There is no conv1d method and it would be very welcome.

Or is conv2d implemented in such a way that (h=1, w, channels) would work with no issues and no compute penalties?

It might even be a good idea to implement an arbitrary convxd function, there are instances where high dimensional inputs have spatial relevance.
"
1135,Saver errors in 0.7.0,"In 0.7, there are different errors about Saver, first Warnings in serialization, such as moving averages, or dictionaries:

```
WARNING:tensorflow:Error encountered when serializing moving_average_variables.
Type is unsupported, or the types of the items don't match field type in CollectionDef.
unbound method to_proto() must be called with Variable instance as first argument (got Tensor instance instead)
WARNING:tensorflow:Error encountered when serializing summary_tags.
Type is unsupported, or the types of the items don't match field type in CollectionDef.
'dict' object has no attribute 'name'
```

and then, it throws an OS Error:

```
  File ""/Users/aymeric/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 970, in save
    self.export_meta_graph(meta_graph_file_name)
  File ""/Users/aymeric/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 990, in export_meta_graph
    as_text=as_text)
  File ""/Users/aymeric/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1315, in export_meta_graph
    os.path.basename(filename), as_text=as_text)
  File ""/Users/aymeric/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/training_util.py"", line 70, in write_graph
    gfile.MakeDirs(logdir)
  File ""/Users/aymeric/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/default/_gfile.py"", line 295, in MakeDirs
    os.makedirs(path, mode)
  File ""/Users/aymeric/anaconda2/lib/python2.7/os.py"", line 157, in makedirs
    mkdir(name, mode)
OSError: [Errno 2] No such file or directory: ''
```

But this is working well in 0.6.0, so Saver might have some issues in 0.7.0
"
1134,"Tensorboard graphs broken in v0.7.0 [Fixed in source, will be in next binary release]","After upgrading from `v0.6.0` to `v0.7.0` via

```
pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.0-py2-none-linux_x86_64.whl
```

Tensorboard graph functionality is empty, while the rest of tensorboard seems fine.

Image shows what I get at `http://0.0.0.0:6006/#graphs`

![tensorboard-bug](https://cloud.githubusercontent.com/assets/780341/13100050/767a898e-d4fe-11e5-8f53-da72be8c9e9a.png)
"
1132,Error: Not Found TensorFlow How-Tos,"**TensorFlow Mechanics 101** brings up an error when trying to view tutorial.

![404](https://cloud.githubusercontent.com/assets/7181734/13099384/55fd2918-d4e6-11e5-8cdb-6c7fc698f63b.png)
"
1126,"bug in ""MNIST For ML Beginners""?","Under the heading The MNIST Data (https://www.tensorflow.org/versions/v0.6.0/tutorials/mnist/beginners/index.html) it appears that 

import tensorflow.examples.tutorials.mnist.input_data
mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)

should say

import tensorflow.examples.tutorials.mnist.input_data as input_data
mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)

thx,

--dmm
"
1123,Error encountered when serializing moving_average_variables,"I compiled Tensorflow from source. Running cifar10_train.py outputs the following warning message:

WARNING:tensorflow:Error encountered when serializing moving_average_variables.
Type is unsupported, or the types of the items don't match field type in CollectionDef.
unbound method to_proto() must be called with Variable instance as first argument (got Tensor instance instead)
"
1122,Easy to use batch norm layer.,"Many non-experts are using the following code http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow?answertab=votes#tab-top.

It would be nice to have an official batch norm layer given its importance in training DNNs.
"
1121,"AttributeError: 'module' object has no attribute 'gfile' in 0.7.0 [Fixed in source, will be in next binary release]","I'm on an Ubuntu machine, installed tensorflow and ran the image processing example as in the website but it failed complaining about gFile not found,

Here's the exception stack:

```
    Traceback (most recent call last):
      File ""classify_image.py"", line 213, in <module>
        tf.app.run()
      File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/default/_app.py"", line 30, in run
        sys.exit(main(sys.argv))
      File ""classify_image.py"", line 209, in main
        run_inference_on_image(image)
      File ""classify_image.py"", line 154, in run_inference_on_image
        if not tf.gfile.Exists(image):
    AttributeError: 'module' object has no attribute 'gfile'
```
"
1117,embedding_lookup on multiple dimensions with AdagradOptimizer throwing exception,"I am completing the Udacity course on Tensorflow, and noticed that when embedding_lookup is used in 3 dimensions with AdagradOptimizer, the optimizer throws an error:

CODE (with error):

```
import math
import tensorflow as tf

batch_size = 128
embedding_size = 128 # Dimension of the embedding vector.
skip_window = 1 # How many words to consider left and right.
num_sampled = 64 # Number of negative examples to sample.
vocabulary_size = 50000

graph = tf.Graph()

with graph.as_default():

  # Input data.
  train_dataset = tf.placeholder(tf.int32, shape=[batch_size, 2 * skip_window])
  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])

  # Variables.
  embeddings = tf.Variable(
    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))
  softmax_weights = tf.Variable(
    tf.truncated_normal([vocabulary_size, embedding_size],
                         stddev=1.0 / math.sqrt(embedding_size)))
  softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))

  # Model.
  # Look up embeddings for inputs.
  embed = tf.nn.embedding_lookup(embeddings, train_dataset)
  embed2 = tf.Variable(tf.zeros([batch_size, embedding_size]))
  for i in xrange(2*skip_window):
    embed2 += embed[:, i, :]
  # Compute the softmax loss, using a sample of the negative labels each time.
  loss = tf.reduce_mean(
    tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed2,
                               train_labels, num_sampled, vocabulary_size))
  optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)
```

Error message:

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-32-104452f9cf81> in <module>()
     39     tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed2,
     40                                train_labels, num_sampled, vocabulary_size))
---> 41   optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)

/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.pyc in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, name)
    186         aggregation_method=aggregation_method)
    187     return self.apply_gradients(grads_and_vars, global_step=global_step,
--> 188                                 name=name)
    189 
    190   def compute_gradients(self, loss, var_list=None, gate_gradients=GATE_OP,

/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.pyc in apply_gradients(self, grads_and_vars, global_step, name)
    287             update_ops.append(self._apply_dense(grad, var))
    288           else:
--> 289             update_ops.append(self._apply_sparse(grad, var))
    290       if global_step is None:
    291         return self._finish(update_ops, name)

/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/adagrad.pyc in _apply_sparse(self, grad, var)
     75     return training_ops.sparse_apply_adagrad(
     76         var, acc, self._learning_rate_tensor, grad.values, grad.indices,
---> 77         use_locking=self._use_locking)

/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/gen_training_ops.pyc in sparse_apply_adagrad(var, accum, lr, grad, indices, use_locking, name)
    200   return _op_def_lib.apply_op(""SparseApplyAdagrad"", var=var, accum=accum,
    201                               lr=lr, grad=grad, indices=indices,
--> 202                               use_locking=use_locking, name=name)
    203 
    204 

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.pyc in apply_op(self, op_type_name, g, name, **keywords)
    662         op = g.create_op(op_type_name, inputs, output_types, name=scope,
    663                          input_types=input_types, attrs=attr_protos,
--> 664                          op_def=op_def)
    665         outputs = op.outputs
    666         return _Restructure(ops.convert_n_to_tensor_or_indexed_slices(outputs),

/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes)
   1834                     original_op=self._default_original_op, op_def=op_def)
   1835     if compute_shapes:
-> 1836       set_shapes_for_outputs(ret)
   1837     self._add_op(ret)
   1838     self._record_op_seen_by_control_dependencies(ret)

/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in set_shapes_for_outputs(op)
   1474       raise RuntimeError(""No shape function registered for standard op: %s""
   1475                          % op.type)
-> 1476   shapes = shape_func(op)
   1477   if len(op.outputs) != len(shapes):
   1478     raise RuntimeError(

/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/training_ops.pyc in _SparseApplyAdagradShape(op)
    115   _AssertInputIsScalar(op, 2)  # lr
    116   grad_shape = op.inputs[3].get_shape().merge_with(
--> 117       tensor_shape.TensorShape([None]).concatenate(accum_shape[1:]))
    118   unused_indices_shape = op.inputs[4].get_shape().merge_with(
    119       tensor_shape.vector(grad_shape[0]))

/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.pyc in merge_with(self, other)
    525       return other
    526     else:
--> 527       self.assert_same_rank(other)
    528       new_dims = []
    529       for i, dim in enumerate(self._dims):

/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.pyc in assert_same_rank(self, other)
    568       if self.ndims != other.ndims:
    569         raise ValueError(
--> 570             ""Shapes %s and %s must have the same rank"" % (self, other))
    571 
    572   def assert_has_rank(self, rank):

ValueError: Shapes TensorShape([Dimension(None), Dimension(None), Dimension(None)]) and TensorShape([Dimension(None), Dimension(128)]) must have the same rank
```

Just change it to get embedding look up once for each time for the third dimension:

```
  # Look up embeddings for inputs.
  # embed = tf.nn.embedding_lookup(embeddings, train_dataset)
  # print embed.get_shape()
  embed2 = tf.Variable(tf.zeros([batch_size, embedding_size]))
  for i in xrange(2*skip_window):
    embed2 += tf.nn.embedding_lookup(embeddings, train_dataset[:, i])
```

The code runs smoothly!
"
1108,Basic support for trigonometric operations,"I'd like to request support for basic trigonometric ops. Nothing fancy, but just simple stuff like tan and the arc complement (arcsin, arctan, etc). All that's there right now is sin and cos.
"
1107,model_with_buckets in seq2seq.py,"Hi all,

The online version of https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L873

```
def model_with_buckets(encoder_inputs, decoder_inputs, targets, weights,
                       buckets, seq2seq, softmax_loss_function=None,
                       per_example_loss=False, name=None):
```

is different with the one installed from tensorflow-0.6.0-cp27-none-linux_x86_64.whl

```
def model_with_buckets(encoder_inputs, decoder_inputs, targets, weights,
                       buckets, num_decoder_symbols, seq2seq,
                       softmax_loss_function=None, name=None):
```

(extra **num_decoder_symbols** parameter) which caused 

```
TypeError: model_with_buckets() takes at least 7 arguments (7 given) 
```

when running https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/translate/seq2seq_model.py#L133.

Which one should I use?

Thanks,
"
1105,cifar10 not running,"Execution tracebacks of cyfar10.py and cifar10_train.py:

python cifar10.py:

```
Traceback (most recent call last):
  File ""/media/konet/01D15611945D72C0/Pycharm_ubuntu/tf_models/image/cifar10/cifar10.py"", line 53, in <module>
    """"""Number of images to process in a batch."""""")
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/default/_flags.py"", line 86, in DEFINE_integer
    _define_helper(flag_name, default_value, docstring, int)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/default/_flags.py"", line 60, in _define_helper
    type=flagtype)
  File ""/usr/lib/python2.7/argparse.py"", line 1297, in add_argument
    return self._add_action(action)
  File ""/usr/lib/python2.7/argparse.py"", line 1671, in _add_action
    self._optionals._add_action(action)
  File ""/usr/lib/python2.7/argparse.py"", line 1498, in _add_action
    action = super(_ArgumentGroup, self)._add_action(action)
  File ""/usr/lib/python2.7/argparse.py"", line 1311, in _add_action
    self._check_conflict(action)
  File ""/usr/lib/python2.7/argparse.py"", line 1449, in _check_conflict
    conflict_handler(action, confl_optionals)
  File ""/usr/lib/python2.7/argparse.py"", line 1456, in _handle_conflict_error
    raise ArgumentError(action, message % conflict_string)
argparse.ArgumentError: argument --batch_size: conflicting option string(s): --batch_size
```

python cifar10.py:

```
Traceback (most recent call last):
  File ""/media/konet/01D15611945D72C0/Pycharm_ubuntu/tf_models/image/cifar10/cifar10_train.py"", line 135, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/default/_app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""/media/konet/01D15611945D72C0/Pycharm_ubuntu/tf_models/image/cifar10/cifar10_train.py"", line 128, in main
    if tf.gfile.Exists(FLAGS.train_dir):
AttributeError: 'module' object has no attribute 'gfile'
```

Using:
tensorflow v 0.6 + Titan GPU configured for GPU usage as per instruction in setup.
"
1104,"Failure to build, gcc 5.2.1, cuda 7.0","Hi,

I followed the instructions at https://www.tensorflow.org/versions/v0.6.0/get_started/os_setup.html and tried building from source. I installed bazel 0.1.5 and I have gcc 5.2.1, cuda 7.0 and a GTX 960.

`bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer`
fails with 

> INFO: From Compiling tensorflow/core/kernels/cwise_op_gpu_sub.cu.cc:
> /usr/include/c++/5/bits/stl_iterator_base_types.h(154): error: class ""std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>"" has no member ""iterator_category""
>           detected during:
>             instantiation of class ""std::__iterator_traits<_Iterator, void> [with _Iterator=std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>]"" 
> (163): here
>             instantiation of class ""std::iterator_traits<_Iterator> [with _Iterator=std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>]"" 
> /usr/include/c++/5/sstream(348): here
>             instantiation of class ""std::__cxx11::basic_stringbuf<_CharT, _Traits, _Alloc> [with _CharT=char, _Traits=std::char_traits<char>, _Alloc=std::allocator<char>]"" 
> /usr/include/c++/5/bits/sstream.tcc(272): here
> 
> /usr/include/c++/5/type_traits(1492): error: class ""std::__is_convertible_helper<<error-type>, std::input_iterator_tag, false>"" has no member class ""type""
>           detected during:
>             instantiation of class ""std::is_convertible<_From, _To> [with _From=<error-type>, _To=std::input_iterator_tag]"" 
> /usr/include/c++/5/sstream(348): here
>             instantiation of class ""std::__cxx11::basic_stringbuf<_CharT, _Traits, _Alloc> [with _CharT=char, _Traits=std::char_traits<char>, _Alloc=std::allocator<char>]"" 
> /usr/include/c++/5/bits/sstream.tcc(272): here
> 
> /usr/include/c++/5/type_traits(1492): error: not a class or struct name
>           detected during:
>             instantiation of class ""std::is_convertible<_From, _To> [with _From=<error-type>, _To=std::input_iterator_tag]"" 
> /usr/include/c++/5/sstream(348): here
>             instantiation of class ""std::__cxx11::basic_stringbuf<_CharT, _Traits, _Alloc> [with _CharT=char, _Traits=std::char_traits<char>, _Alloc=std::allocator<char>]"" 
> /usr/include/c++/5/bits/sstream.tcc(272): here
> 
> /usr/include/c++/5/bits/stl_iterator_base_types.h(154): error: class ""std::__cxx11::basic_string<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t>>"" has no member ""iterator_category""
>           detected during:
>             instantiation of class ""std::__iterator_traits<_Iterator, void> [with _Iterator=std::__cxx11::basic_string<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t>>]"" 
> (163): here
>             instantiation of class ""std::iterator_traits<_Iterator> [with _Iterator=std::__cxx11::basic_string<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t>>]"" 
> /usr/include/c++/5/sstream(348): here
>             instantiation of class ""std::__cxx11::basic_stringbuf<_CharT, _Traits, _Alloc> [with _CharT=wchar_t, _Traits=std::char_traits<wchar_t>, _Alloc=std::allocator<wchar_t>]"" 
> /usr/include/c++/5/bits/sstream.tcc(278): here
> 
> /usr/include/c++/5/bits/stl_iterator_base_types.h(154): error: class ""std::vector<std::seed_seq::result_type, std::allocator<std::seed_seq::result_type>>"" has no member ""iterator_category""
>           detected during:
>             instantiation of class ""std::__iterator_traits<_Iterator, void> [with _Iterator=std::vector<std::seed_seq::result_type, std::allocator<std::seed_seq::result_type>>]"" 
> (163): here
>             instantiation of class ""std::iterator_traits<_Iterator> [with _Iterator=std::vector<std::seed_seq::result_type, std::allocator<std::seed_seq::result_type>>]"" 
> /usr/include/c++/5/bits/random.h(6059): here
> 
> /usr/lib/gcc/x86_64-linux-gnu/5/include/emmintrin.h(1294): error: expression must have arithmetic, unscoped enum, or pointer type
> 
> /usr/lib/gcc/x86_64-linux-gnu/5/include/emmintrin.h(1300): error: expression must have arithmetic, unscoped enum, or pointer type
> 
> /usr/lib/gcc/x86_64-linux-gnu/5/include/emmintrin.h(1306): error: expression must have arithmetic, unscoped enum, or pointer type
> 
> /usr/lib/gcc/x86_64-linux-gnu/5/include/emmintrin.h(1312): error: expression must have arithmetic, unscoped enum, or pointer type
> 
> /usr/lib/gcc/x86_64-linux-gnu/5/include/emmintrin.h(1318): error: expression must have arithmetic, unscoped enum, or pointer type
> 
> /usr/lib/gcc/x86_64-linux-gnu/5/include/emmintrin.h(1324): error: expression must have arithmetic, unscoped enum, or pointer type
> 
> /usr/lib/gcc/x86_64-linux-gnu/5/include/emmintrin.h(1330): error: expression must have arithmetic, unscoped enum, or pointer type
> 
> /usr/lib/gcc/x86_64-linux-gnu/5/include/emmintrin.h(1336): error: expression must have arithmetic, unscoped enum, or pointer type
> 
> /usr/lib/gcc/x86_64-linux-gnu/5/include/emmintrin.h(1342): error: expression must have arithmetic, unscoped enum, or pointer type
> 
> /usr/lib/gcc/x86_64-linux-gnu/5/include/smmintrin.h(270): error: expression must have arithmetic, unscoped enum, or pointer type
> 
> /usr/lib/gcc/x86_64-linux-gnu/5/include/smmintrin.h(798): error: expression must have arithmetic, unscoped enum, or pointer type
> 
> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx2intrin.h(233): error: expression must have arithmetic, unscoped enum, or pointer type
> 
> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx2intrin.h(240): error: expression must have arithmetic, unscoped enum, or pointer type
> 
> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx2intrin.h(247): error: expression must have arithmetic, unscoped enum, or pointer type
> 
> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx2intrin.h(254): error: expression must have arithmetic, unscoped enum, or pointer type
> 
> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx2intrin.h(261): error: expression must have arithmetic, unscoped enum, or pointer type
> 
> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx2intrin.h(268): error: expression must have arithmetic, unscoped enum, or pointer type
> 
> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx2intrin.h(275): error: expression must have arithmetic, unscoped enum, or pointer type
> 
> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx2intrin.h(282): error: expression must have arithmetic, unscoped enum, or pointer type
> 
> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(3606): error: identifier ""__builtin_ia32_pbroadcastq512_gpr_mask"" is undefined
> 
> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(3616): error: identifier ""__builtin_ia32_pbroadcastq512_gpr_mask"" is undefined
> 
> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(3625): error: identifier ""__builtin_ia32_pbroadcastq512_gpr_mask"" is undefined
> 
> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(13031): error: identifier ""__builtin_ia32_pd512_pd"" is undefined
> 
> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(13038): error: identifier ""__builtin_ia32_ps512_ps"" is undefined
> 
> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(13045): error: identifier ""__builtin_ia32_si512_si"" is undefined
> 
> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(13052): error: identifier ""__builtin_ia32_pd512_256pd"" is undefined
> 
> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(13059): error: identifier ""__builtin_ia32_ps512_256ps"" is undefined
> 
> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(13066): error: identifier ""__builtin_ia32_si512_256si"" is undefined
> 
> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(52): error: identifier ""__builtin_ia32_movapd256_mask"" is undefined
> 
> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(61): error: identifier ""__builtin_ia32_movapd256_mask"" is undefined
> 
> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(71): error: identifier ""__builtin_ia32_movapd128_mask"" is undefined
> 
> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(80): error: identifier ""__builtin_ia32_movapd128_mask"" is undefined
> 
> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(90): error: identifier ""__builtin_ia32_loadapd256_mask"" is undefined
> 
> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(99): error: identifier ""__builtin_ia32_loadapd256_mask"" is undefined
> 
> lots of errors like that
> 
> Error limit reached.
> 100 errors detected in the compilation of ""/tmp/tmpxft_00004c6c_00000000-7_cwise_op_gpu_sub.cu.cpp1.ii"".
> Compilation terminated.
> ERROR: /home/bernardo/programs/tensorflow/tensorflow/core/BUILD:334:1: output 'tensorflow/core/_objs/gpu_kernels/tensorflow/core/kernels/cwise_op_gpu_sub.cu.o' was not created.
> ERROR: /home/bernardo/programs/tensorflow/tensorflow/core/BUILD:334:1: not all outputs were created.
> Target //tensorflow/cc:tutorials_example_trainer failed to build
"
1103,Inception-v3 only for specific classes,"Hi,

is it possible to use Inception-v3 to classify images only for dogs?The example at tensorflow/examples/label_image/ classify images into 1000 classes.
"
1102,AttributeError: 'module' object has no attribute 'Copy' on fully_connected.py. Built from source on 15th February.,"I built from source on 15th February. I can run cifar10_train.py, with the warning during training as stated here.

https://github.com/tensorflow/tensorflow/issues/1076

I can run the convolutional mnist with no problems. However, when I try fully_connected.py, this error shows up.

Traceback (most recent call last):
  File ""fully_connected_feed.py"", line 228, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/default/_app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""fully_connected_feed.py"", line 224, in main
    run_training()
  File ""fully_connected_feed.py"", line 130, in run_training
    data_sets = input_data.read_data_sets(FLAGS.train_dir, FLAGS.fake_data)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/examples/tutorials/mnist/input_data.py"", line 198, in read_data_sets
    local_file = maybe_download(TRAIN_IMAGES, train_dir)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/examples/tutorials/mnist/input_data.py"", line 42, in maybe_download
    tf.gfile.Copy(temp_file_name, filepath)
AttributeError: 'module' object has no attribute 'Copy'

I have also tried uninstalling and reinstalling TensorFlow. I don't think it's a problem with the installation since some examples run properly. Anyone else facing this too?
"
1100,Protocol messages are limited to 64MB in python,"The classify_image.py example isn't working because inception v3 is larger than 64MB

[libprotobuf ERROR google/protobuf/src/google/protobuf/io/coded_stream.cc:207] A protocol message was rejected because it was too big (more than 67108864 bytes).  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.
"
1097,tensorflow-0.6.0-cp34-none-linux_x86_64.whl is not a supported wheel on this platform.,"INSTALLATION ERROR:
$ sudo pip3 install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.6.0-cp34-none-linux_x86_64.whl
sudo: pip3: command not found

CONDITIONS
I am running iPython using Python 3.4.3 | Anaconda 2.2.0 (64-bit), running on Ubuntu/Linux 14.04 LTS (64-bit) OS.  

DESCRIPTION: 
The Tensorflow installation fails using PIP  for Python 3, as suggested here: (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md).
# Ubuntu/Linux 64-bit

$ sudo apt-get install python-pip python-dev
# Ubuntu/Linux 64-bit, CPU only:

$ sudo pip3 install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.6.0-cp34-none-linux_x86_64.whl

DETAILS:
tensorflow-0.6.0-cp34-none-linux_x86_64.whl is not a supported wheel on this platform.
Exception information:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/dist-packages/pip/basecommand.py"", line 122, in main
    status = self.run(options, args)
  File ""/usr/lib/python2.7/dist-packages/pip/commands/install.py"", line 257, in run
    InstallRequirement.from_line(name, None))
  File ""/usr/lib/python2.7/dist-packages/pip/req.py"", line 168, in from_line
    raise UnsupportedWheel(""%s is not a supported wheel on this platform."" % wheel.filename)
UnsupportedWheel: tensorflow-0.6.0-cp34-none-linux_x86_64.whl is not a supported wheel on this platform.
"
1095,Gradient of tf.py_func and how to define gradients for a custom op only with python,"I know about `tf.py_func` which I can use a python code as an op but how gradient is calculated with it?

Also, is there anyway I can set the gradient only with python? I think one way might be adding a custom op that calculate gradients into an array and use the array as an argument of `apply_gradients`.

For example:

```
x = tf.Variable(3.) # this is an custom op
y = tf.Variable(4.)
loss = 13 - x * y

opt = tf.train.GradientDescentOptimizer(0.1)
[(y_grad, y_val)] = opt.compute_gradients(loss, [y])
clipped_grads_and_vars = [(x * y, x), (y_grad, y_val)] # custom gradient is x * y

optim = opt.apply_gradients(clipped_grads_and_vars)
```

Is this safe way to apply custom gradients only with python?
"
1093,Gradient computation through while loop,"Computation of gradients through a simple while op doesn't seem to work:

``` python
import tensorflow as tf
from tensorflow.python.ops import control_flow_ops

def cond(i, var):
    return tf.less(i, 1)

def body(i, var):
    return [tf.add(i, 1), var]

i = tf.constant(0)
var = tf.Variable(tf.constant(2.))

_, var = control_flow_ops.While(cond, body, [i, var])

out = var*var

optimizer = tf.train.GradientDescentOptimizer(.9)
optimizer_op = optimizer.minimize(out)

with tf.Session() as sess:
    tf.initialize_all_variables().run()
    print sess.run([out, optimizer_op])
    print sess.run([out, optimizer_op])
    print sess.run([out, optimizer_op])
```

gives the following result:

```
[4.0, None]
[4.0, None]
[4.0, None]
```

Whereas commenting out the while op on line 13 gives:

```
[4.0, None]
[2.5599997, None]
[1.6383994, None]
```

Since the while op here only iterates once, and doesn't really do anything, I would expect that the result should be the same with or without it.

I know that the while operator is not officially documented or supported, but the wording in #593 made me believe that computation of gradients through a single simple loop was likely to work. (Why experiment with nested loops if single loops didn't work)
"
1091,Adding summaries changes random number generation,"Here is a simple example:

```
import tensorflow as tf

with tf.Session('') as sess:
    tf.set_random_seed(42)
#    tf.scalar_summary('d', 1.)
    y = tf.get_variable('y', [1, 5])
    sess.run(tf.initialize_all_variables())
    print sess.run(y)
```

Commenting / uncommenting scalar_summary changes results:
`[[ 1.58872545  0.94011414 -1.35924089  1.19359171  1.48940694]]` to `[[ 0.47769845 -1.17613626 -1.06156325 -0.85632306 -0.62591958]]`.

Related to https://github.com/tensorflow/skflow/issues/101
"
1088,cuda error on K40m GPU,"I observed the following error when I used tensorflow on redhat 6.7 system with one K40m GPU. The tensorflow was compiled from the source checked out today. I also tested an older revision of tensorflow (one month back) and saw the same error.

```
+------------------------------------------------------+                       
| NVIDIA-SMI 352.39     Driver Version: 352.39         |                       
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K40m          Off  | 0000:08:00.0     Off |                    0 |
| N/A   20C    P8    19W / 235W |     23MiB / 11519MiB |      0%    E. Thread |
+-------------------------------+----------------------+----------------------+
```

```
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 
name: Tesla K40m
major: 3 minor: 5 memoryClockRate (GHz) 0.745
pciBusID 0000:08:00.0
Total memory: 11.25GiB
Free memory: 11.15GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:718] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40m, pci bus id: 0000:08:00.0)
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 32.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 64.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 128.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 256.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 512.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 32.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 64.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 128.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 256.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 512.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.00GiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.00GiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.00GiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.00GiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.00GiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:73] Allocating 10.60GiB bytes.
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:83] GPU 0 memory begins at 0x13047a0000 extends to 0x15aa9a6334
F tensorflow/stream_executor/cuda/cuda_driver.cc:383] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(context) (0 vs. 216)

```
"
1084,Missing variable on MNIST tutorial,"I'm writing this here because I can't find a better way to contact someone about this.

In short, first Tensorflow tutorial about MNIST:
https://www.tensorflow.org/versions/v0.6.0/tutorials/mnist/beginners/index.html#the-mnist-data

First piece of code:

```
import tensorflow.examples.tutorials.mnist.input_data
mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True) 
```

where it should be:

```
import tensorflow.examples.tutorials.mnist.input_data as input_data
mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True) 
```

**""as input_data""** is missing, so when running the second line of code, python doesn't know whats input_data.whatever.

Hope someone can edit that, because it's a begginer tutorial, and can be very frustrating to fail running the very first piece of code of the first and most basic tutorial.
"
1079,Compiling against Cuda compute environment 3.0 causes TF to take huge virtual memory,"I run tensorflow program with htop in another terminal. Standard installation for cpu or gpu doesn't take more than half a GB of virtual memory. If I compile from source, using 

TF_UNOFFICIAL_SETTING=1 ./configure

and using cuda compute environment 3.0 (not 3.5 or 5.2) then the same program uses 38GB of virtual memory. I don't understand cuda well enough to postulate why this would be. It's not a huge problem unless you have lots of them running at once. 

I only do this bc I want to work on AWS GPU instance and they only have K520 or something like that.
"
1076,tensorboard not working on latest sources 11 th feb 2016,"Hi I have build tensorfow from latest source  on 11 Feb 2016.

I also followed instruction for building tensorboard.

I ran cifar10_train.py. When using tensorboard there is nothing in browser when I  connect to it. 
Browser output is 

No scalar summary tags were found.
Maybe data hasn't loaded yet, or maybe you need to add some tf.scalar_summary ops to your graph, and serialize them using the tf.training.summary_io.SummaryWriter.

I tried adding flush for SummaryWriter. Still same message.  Any suggestion will help me a lot for my work. 

Attaching the console output
# when training this is the Error

/batch)
2016-02-12 11:49:26.445613: step 170, loss = 4.38 (405.8 examples/sec; 0.315 sec/batch)
2016-02-12 11:49:29.575018: step 180, loss = 4.36 (442.2 examples/sec; 0.289 sec/batch)
2016-02-12 11:49:32.828157: step 190, loss = 4.35 (407.5 examples/sec; 0.314 sec/batch)
WARNING:tensorflow:Error encountered when serializing moving_average_variables.
Type is unsupported, or the types of the items don't match field type in CollectionDef.
unbound method to_proto() must be called with Variable instance as first argument (got Tensor instance instead)
# When using tensorboard this is the message on console

./bazel-bin/tensorflow/tensorboard/tensorboard --logdir=/tmp/cifar10_train
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so.7.0 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so.6.5 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so.7.0 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so.7.0 locally
Starting TensorBoard 13 on port 6006
(You can navigate to http://0.0.0.0:6006)
130.253.128.188 - - [12/Feb/2016 11:50:11] ""GET / HTTP/1.1"" 200 -
130.253.128.188 - - [12/Feb/2016 11:50:11] ""GET /lib/css/global.css HTTP/1.1"" 200 -
130.253.128.188 - - [12/Feb/2016 11:50:11] ""GET /external/lodash/lodash.min.js HTTP/1.1"" 200 -
130.253.128.188 - - [12/Feb/2016 11:50:11] ""GET /external/d3/d3.min.js HTTP/1.1"" 200 -
130.253.128.188 - - [12/Feb/2016 11:50:11] ""GET /external/plottable/plottable.min.js HTTP/1.1"" 200 -
130.253.128.188 - - [12/Feb/2016 11:50:11] ""GET /external/plottable/plottable.css HTTP/1.1"" 200 -
130.253.128.188 - - [12/Feb/2016 11:50:11] ""GET /external/graphlib/dist/graphlib.core.min.js HTTP/1.1"" 200 -
130.253.128.188 - - [12/Feb/2016 11:50:11] ""GET /external/dagre/dist/dagre.core.min.js HTTP/1.1"" 200 -
130.253.128.188 - - [12/Feb/2016 11:50:11] ""GET /external/polymer/polymer.html HTTP/1.1"" 200 -
130.253.128.188 - - [12/Feb/2016 11:50:11] ""GET /external/webcomponentsjs/webcomponents-lite.min.js HTTP/1.1"" 200 -
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/iron-ajax/iron-ajax.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/iron-ajax/iron-ajax.html
130.253.128.188 - - [12/Feb/2016 11:50:11] code 404, message Not Found
130.253.128.188 - - [12/Feb/2016 11:50:11] ""GET /external/iron-ajax/iron-ajax.html HTTP/1.1"" 404 -
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/iron-list/iron-list.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/iron-list/iron-list.html
130.253.128.188 - - [12/Feb/2016 11:50:11] code 404, message Not Found
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-button/paper-button.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-button/paper-button.html
130.253.128.188 - - [12/Feb/2016 11:50:11] ""GET /external/iron-list/iron-list.html HTTP/1.1"" 404 -
130.253.128.188 - - [12/Feb/2016 11:50:11] code 404, message Not Found
130.253.128.188 - - [12/Feb/2016 11:50:11] ""GET /external/paper-button/paper-button.html HTTP/1.1"" 404 -
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/iron-collapse/iron-collapse.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/iron-collapse/iron-collapse.html
130.253.128.188 - - [12/Feb/2016 11:50:11] code 404, message Not Found
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-checkbox/paper-checkbox.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-checkbox/paper-checkbox.html
130.253.128.188 - - [12/Feb/2016 11:50:11] code 404, message Not Found
130.253.128.188 - - [12/Feb/2016 11:50:11] ""GET /external/iron-collapse/iron-collapse.html HTTP/1.1"" 404 -
130.253.128.188 - - [12/Feb/2016 11:50:11] ""GET /external/paper-checkbox/paper-checkbox.html HTTP/1.1"" 404 -
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-dropdown-menu/paper-dropdown-menu.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-dropdown-menu/paper-dropdown-menu.html
130.253.128.188 - - [12/Feb/2016 11:50:11] code 404, message Not Found
130.253.128.188 - - [12/Feb/2016 11:50:11] ""GET /external/paper-dropdown-menu/paper-dropdown-menu.html HTTP/1.1"" 404 -
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-header-panel/paper-header-panel.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-header-panel/paper-header-panel.html
130.253.128.188 - - [12/Feb/2016 11:50:11] code 404, message Not Found
130.253.128.188 - - [12/Feb/2016 11:50:11] ""GET /external/paper-header-panel/paper-header-panel.html HTTP/1.1"" 404 -
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-icon-button/paper-icon-button.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-icon-button/paper-icon-button.html
130.253.128.188 - - [12/Feb/2016 11:50:12] code 404, message Not Found
130.253.128.188 - - [12/Feb/2016 11:50:12] ""GET /external/paper-icon-button/paper-icon-button.html HTTP/1.1"" 404 -
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-input/paper-input.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-input/paper-input.html
130.253.128.188 - - [12/Feb/2016 11:50:12] code 404, message Not Found
130.253.128.188 - - [12/Feb/2016 11:50:12] ""GET /external/paper-input/paper-input.html HTTP/1.1"" 404 -
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-item/paper-item.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-item/paper-item.html
130.253.128.188 - - [12/Feb/2016 11:50:12] code 404, message Not Found
130.253.128.188 - - [12/Feb/2016 11:50:12] ""GET /external/paper-item/paper-item.html HTTP/1.1"" 404 -
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-menu/paper-menu.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-menu/paper-menu.html
130.253.128.188 - - [12/Feb/2016 11:50:12] code 404, message Not Found
130.253.128.188 - - [12/Feb/2016 11:50:12] ""GET /external/paper-menu/paper-menu.html HTTP/1.1"" 404 -
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-progress/paper-progress.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-progress/paper-progress.html
130.253.128.188 - - [12/Feb/2016 11:50:12] code 404, message Not Found
130.253.128.188 - - [12/Feb/2016 11:50:12] ""GET /external/paper-progress/paper-progress.html HTTP/1.1"" 404 -
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-radio-button/paper-radio-button.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-radio-button/paper-radio-button.html
130.253.128.188 - - [12/Feb/2016 11:50:12] code 404, message Not Found
130.253.128.188 - - [12/Feb/2016 11:50:12] ""GET /external/paper-radio-button/paper-radio-button.html HTTP/1.1"" 404 -
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-radio-group/paper-radio-group.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-radio-group/paper-radio-group.html
130.253.128.188 - - [12/Feb/2016 11:50:12] code 404, message Not Found
130.253.128.188 - - [12/Feb/2016 11:50:12] ""GET /external/paper-radio-group/paper-radio-group.html HTTP/1.1"" 404 -
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-slider/paper-slider.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-slider/paper-slider.html
130.253.128.188 - - [12/Feb/2016 11:50:12] code 404, message Not Found
130.253.128.188 - - [12/Feb/2016 11:50:12] ""GET /external/paper-slider/paper-slider.html HTTP/1.1"" 404 -
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-styles/paper-styles.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-styles/paper-styles.html
130.253.128.188 - - [12/Feb/2016 11:50:12] code 404, message Not Found
130.253.128.188 - - [12/Feb/2016 11:50:12] ""GET /external/paper-styles/paper-styles.html HTTP/1.1"" 404 -
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-toggle-button/paper-toggle-button.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-toggle-button/paper-toggle-button.html
130.253.128.188 - - [12/Feb/2016 11:50:12] code 404, message Not Found
130.253.128.188 - - [12/Feb/2016 11:50:12] ""GET /external/paper-toggle-button/paper-toggle-button.html HTTP/1.1"" 404 -
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-toolbar/paper-toolbar.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-toolbar/paper-toolbar.html
130.253.128.188 - - [12/Feb/2016 11:50:12] code 404, message Not Found
130.253.128.188 - - [12/Feb/2016 11:50:12] ""GET /external/paper-toolbar/paper-toolbar.html HTTP/1.1"" 404 -
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-tabs/paper-tabs.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-tabs/paper-tabs.html
130.253.128.188 - - [12/Feb/2016 11:50:12] code 404, message Not Found
130.253.128.188 - - [12/Feb/2016 11:50:12] ""GET /external/paper-tabs/paper-tabs.html HTTP/1.1"" 404 -
130.253.128.188 - - [12/Feb/2016 11:50:12] ""GET /dist/tf-tensorboard.html HTTP/1.1"" 200 -
130.253.128.188 - - [12/Feb/2016 11:50:12] ""GET /external/polymer/polymer-mini.html HTTP/1.1"" 200 -
130.253.128.188 - - [12/Feb/2016 11:50:12] ""GET /external/polymer/polymer-micro.html HTTP/1.1"" 200 -
130.253.128.188 - - [12/Feb/2016 11:50:15] ""GET / HTTP/1.1"" 200 -
130.253.128.188 - - [12/Feb/2016 11:50:15] ""GET /lib/css/global.css HTTP/1.1"" 200 -
130.253.128.188 - - [12/Feb/2016 11:50:15] ""GET /external/lodash/lodash.min.js HTTP/1.1"" 200 -
130.253.128.188 - - [12/Feb/2016 11:50:15] ""GET /external/plottable/plottable.css HTTP/1.1"" 200 -
130.253.128.188 - - [12/Feb/2016 11:50:15] ""GET /external/plottable/plottable.min.js HTTP/1.1"" 200 -
130.253.128.188 - - [12/Feb/2016 11:50:15] ""GET /external/d3/d3.min.js HTTP/1.1"" 200 -
130.253.128.188 - - [12/Feb/2016 11:50:15] ""GET /external/graphlib/dist/graphlib.core.min.js HTTP/1.1"" 200 -
130.253.128.188 - - [12/Feb/2016 11:50:15] ""GET /external/dagre/dist/dagre.core.min.js HTTP/1.1"" 200 -
130.253.128.188 - - [12/Feb/2016 11:50:15] ""GET /external/polymer/polymer.html HTTP/1.1"" 200 -
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/iron-ajax/iron-ajax.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/iron-ajax/iron-ajax.html
130.253.128.188 - - [12/Feb/2016 11:50:15] ""GET /external/webcomponentsjs/webcomponents-lite.min.js HTTP/1.1"" 200 -
130.253.128.188 - - [12/Feb/2016 11:50:15] code 404, message Not Found
130.253.128.188 - - [12/Feb/2016 11:50:15] ""GET /external/iron-ajax/iron-ajax.html HTTP/1.1"" 404 -
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/iron-collapse/iron-collapse.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/iron-collapse/iron-collapse.html
130.253.128.188 - - [12/Feb/2016 11:50:15] code 404, message Not Found
130.253.128.188 - - [12/Feb/2016 11:50:15] ""GET /external/iron-collapse/iron-collapse.html HTTP/1.1"" 404 -
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/iron-list/iron-list.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/iron-list/iron-list.html
130.253.128.188 - - [12/Feb/2016 11:50:15] code 404, message Not Found
130.253.128.188 - - [12/Feb/2016 11:50:15] ""GET /external/iron-list/iron-list.html HTTP/1.1"" 404 -
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-button/paper-button.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-button/paper-button.html
130.253.128.188 - - [12/Feb/2016 11:50:15] code 404, message Not Found
130.253.128.188 - - [12/Feb/2016 11:50:15] ""GET /external/paper-button/paper-button.html HTTP/1.1"" 404 -
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-checkbox/paper-checkbox.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-checkbox/paper-checkbox.html
130.253.128.188 - - [12/Feb/2016 11:50:15] code 404, message Not Found
130.253.128.188 - - [12/Feb/2016 11:50:15] ""GET /external/paper-checkbox/paper-checkbox.html HTTP/1.1"" 404 -
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-dropdown-menu/paper-dropdown-menu.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-dropdown-menu/paper-dropdown-menu.html
130.253.128.188 - - [12/Feb/2016 11:50:15] code 404, message Not Found
130.253.128.188 - - [12/Feb/2016 11:50:15] ""GET /external/paper-dropdown-menu/paper-dropdown-menu.html HTTP/1.1"" 404 -
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-header-panel/paper-header-panel.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-header-panel/paper-header-panel.html
130.253.128.188 - - [12/Feb/2016 11:50:15] code 404, message Not Found
130.253.128.188 - - [12/Feb/2016 11:50:15] ""GET /external/paper-header-panel/paper-header-panel.html HTTP/1.1"" 404 -
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-icon-button/paper-icon-button.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-icon-button/paper-icon-button.html
130.253.128.188 - - [12/Feb/2016 11:50:15] code 404, message Not Found
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-input/paper-input.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-input/paper-input.html
130.253.128.188 - - [12/Feb/2016 11:50:15] ""GET /external/paper-icon-button/paper-icon-button.html HTTP/1.1"" 404 -
130.253.128.188 - - [12/Feb/2016 11:50:15] code 404, message Not Found
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-item/paper-item.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-item/paper-item.html
130.253.128.188 - - [12/Feb/2016 11:50:15] code 404, message Not Found
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-menu/paper-menu.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-menu/paper-menu.html
130.253.128.188 - - [12/Feb/2016 11:50:15] ""GET /external/paper-item/paper-item.html HTTP/1.1"" 404 -
130.253.128.188 - - [12/Feb/2016 11:50:15] ""GET /external/paper-input/paper-input.html HTTP/1.1"" 404 -
130.253.128.188 - - [12/Feb/2016 11:50:15] code 404, message Not Found
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-progress/paper-progress.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-progress/paper-progress.html
130.253.128.188 - - [12/Feb/2016 11:50:15] ""GET /external/paper-menu/paper-menu.html HTTP/1.1"" 404 -
130.253.128.188 - - [12/Feb/2016 11:50:15] code 404, message Not Found
130.253.128.188 - - [12/Feb/2016 11:50:15] ""GET /external/paper-progress/paper-progress.html HTTP/1.1"" 404 -
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-radio-button/paper-radio-button.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-radio-button/paper-radio-button.html
130.253.128.188 - - [12/Feb/2016 11:50:15] code 404, message Not Found
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-radio-group/paper-radio-group.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-radio-group/paper-radio-group.html
130.253.128.188 - - [12/Feb/2016 11:50:15] ""GET /external/paper-radio-button/paper-radio-button.html HTTP/1.1"" 404 -
130.253.128.188 - - [12/Feb/2016 11:50:15] code 404, message Not Found
130.253.128.188 - - [12/Feb/2016 11:50:15] ""GET /external/paper-radio-group/paper-radio-group.html HTTP/1.1"" 404 -
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-styles/paper-styles.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-styles/paper-styles.html
130.253.128.188 - - [12/Feb/2016 11:50:15] code 404, message Not Found
130.253.128.188 - - [12/Feb/2016 11:50:15] ""GET /external/paper-styles/paper-styles.html HTTP/1.1"" 404 -
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-toolbar/paper-toolbar.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-toolbar/paper-toolbar.html
130.253.128.188 - - [12/Feb/2016 11:50:15] ""GET /dist/tf-tensorboard.html HTTP/1.1"" 200 -
130.253.128.188 - - [12/Feb/2016 11:50:15] code 404, message Not Found
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-tabs/paper-tabs.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-tabs/paper-tabs.html
130.253.128.188 - - [12/Feb/2016 11:50:15] code 404, message Not Found
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-slider/paper-slider.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-slider/paper-slider.html
130.253.128.188 - - [12/Feb/2016 11:50:15] ""GET /external/paper-toolbar/paper-toolbar.html HTTP/1.1"" 404 -
130.253.128.188 - - [12/Feb/2016 11:50:15] ""GET /external/paper-tabs/paper-tabs.html HTTP/1.1"" 404 -
130.253.128.188 - - [12/Feb/2016 11:50:15] code 404, message Not Found
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-toggle-button/paper-toggle-button.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-toggle-button/paper-toggle-button.html
130.253.128.188 - - [12/Feb/2016 11:50:15] ""GET /external/paper-slider/paper-slider.html HTTP/1.1"" 404 -
130.253.128.188 - - [12/Feb/2016 11:50:15] code 404, message Not Found
130.253.128.188 - - [12/Feb/2016 11:50:15] ""GET /external/paper-toggle-button/paper-toggle-button.html HTTP/1.1"" 404 -
130.253.128.188 - - [12/Feb/2016 11:50:15] ""GET /external/plottable/plottable.min.js HTTP/1.1"" 200 -
130.253.128.188 - - [12/Feb/2016 11:50:15] ""GET /external/polymer/polymer.html HTTP/1.1"" 200 -
130.253.128.188 - - [12/Feb/2016 11:50:15] ""GET /external/polymer/polymer-mini.html HTTP/1.1"" 200 -
130.253.128.188 - - [12/Feb/2016 11:50:15] ""GET /external/polymer/polymer-micro.html HTTP/1.1"" 200 -
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-button/paper-button.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-button/paper-button.html
130.253.128.188 - - [12/Feb/2016 11:50:15] code 404, message Not Found
130.253.128.188 - - [12/Feb/2016 11:50:15] ""GET /external/paper-button/paper-button.html HTTP/1.1"" 404 -
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-checkbox/paper-checkbox.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-checkbox/paper-checkbox.html
130.253.128.188 - - [12/Feb/2016 11:50:15] code 404, message Not Found
130.253.128.188 - - [12/Feb/2016 11:50:15] ""GET /external/paper-checkbox/paper-checkbox.html HTTP/1.1"" 404 -
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-header-panel/paper-header-panel.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-header-panel/paper-header-panel.html
130.253.128.188 - - [12/Feb/2016 11:50:15] code 404, message Not Found
WARNING:tensorflow:IOError [Errno 2] No such file or directory: ' /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-item/paper-item.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-item/paper-item.html
130.253.128.188 - - [12/Feb/2016 11:50:15] ""GET /external/paper-header-panel/paper-header-panel.html HTTP/1.1"" 404 -
130.253.128.188 - - [12/Feb/2016 11:50:15] code 404, message Not Found
130.253.128.188 - - [12/Feb/2016 11:50:15] ""GET /external/paper-item/paper-item.html HTTP/1.1"" 404 -
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-radio-button/paper-radio-button.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-radio-button/paper-radio-button.html
130.253.128.188 - - [12/Feb/2016 11:50:15] code 404, message Not Found
WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-radio-group/paper-radio-group.html' on path /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-radio-group/paper-radio-group.html
130.253.128.188 - - [12/Feb/2016 11:50:15] ""GET /external/paper-radio-button/paper-radio-button.html HTTP/1.1"" 404 -
130.253.128.188 - - [12/Feb/2016 11:50:15] code 404, message Not Found
130.253.128.188 - - [12/Feb/2016 11:50:15] ""GET /external/paper-radio-group/paper-radio-group.html HTTP/1.1"" 404 -
130.253.128.188 - - [12/Feb/2016 11:50:15] ""GET /dist/tf-tensorboard.html HTTP/1.1"" 200 -

These seems to be some issue with path like 

WARNING:tensorflow:IOError [Errno 2] No such file or directory: ' /home/psnegi/Software/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/external/paper-item/paper-item.html'

After checking path on my system , there is an error in path name
**paper-item**. It should be **paper_item**

Thanks,
Pooran
"
1074,"Build from source fails, Ubuntu Precise / GCC 4.9","Attempting to build on TravisCI, still can't get this to complete - any ideas?

Thanks!

https://travis-ci.org/peterbraden/node-tensorflow/builds/108822385

```
INFO: From Compiling external/jpeg_archive/jpeg-9a/jcmaster.c:
external/jpeg_archive/jpeg-9a/jcmaster.c: In function 'void prepare_for_pass(j_compress_ptr)':
external/jpeg_archive/jpeg-9a/jcmaster.c:726:28: error: cannot convert 'bool' to 'boolean' in assignment
   master->pub.is_last_pass = (master->pass_number == master->total_passes-1);
                            ^
ERROR: /home/travis/.cache/bazel/_bazel_travis/2cf95f6ff0f7dc500e0eb5aab5889c29/external/jpeg_archive/BUILD:77:1: C++ compilation of rule '@jpeg_archive//:jpeg' failed: gcc failed: error executing command 
  (cd /home/travis/.cache/bazel/_bazel_travis/2cf95f6ff0f7dc500e0eb5aab5889c29/tensorflow && \
  exec env - \
    PATH=/home/travis/.nvm/versions/node/v4.1.2/lib/node_modules/npm/bin/node-gyp-bin:/home/travis/build/peterbraden/node-tensorflow/node_modules/.bin:/home/travis/.nvm/versions/node/v4.1.2/bin:/home/travis/bin:/home/travis/.local/bin:/home/travis/.gimme/versions/go1.4.2.linux.amd64/bin:/home/travis/.rvm/gems/ruby-1.9.3-p551/bin:/home/travis/.rvm/gems/ruby-1.9.3-p551@global/bin:/home/travis/.rvm/rubies/ruby-1.9.3-p551/bin::/usr/local/phantomjs/bin:./node_modules/.bin:/usr/local/maven-3.2.5/bin:/usr/local/clang-3.4/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/travis/.rvm/bin \
  /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -iquote external/jpeg_archive -iquote bazel-out/local_linux-opt/genfiles/external/jpeg_archive -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/bazel_tools/tools/cpp/gcc3 -no-canonical-prefixes -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' '-frandom-seed=bazel-out/local_linux-opt/bin/external/jpeg_archive/_objs/jpeg/external/jpeg_archive/jpeg-9a/jcmaster.pic.o' -MD -MF bazel-out/local_linux-opt/bin/external/jpeg_archive/_objs/jpeg/external/jpeg_archive/jpeg-9a/jcmaster.pic.d -fPIC -c external/jpeg_archive/jpeg-9a/jcmaster.c -o bazel-out/local_linux-opt/bin/external/jpeg_archive/_objs/jpeg/external/jpeg_archive/jpeg-9a/jcmaster.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: gcc failed: error executing command 
  (cd /home/travis/.cache/bazel/_bazel_travis/2cf95f6ff0f7dc500e0eb5aab5889c29/tensorflow && \
  exec env - \
    PATH=/home/travis/.nvm/versions/node/v4.1.2/lib/node_modules/npm/bin/node-gyp-bin:/home/travis/build/peterbraden/node-tensorflow/node_modules/.bin:/home/travis/.nvm/versions/node/v4.1.2/bin:/home/travis/bin:/home/travis/.local/bin:/home/travis/.gimme/versions/go1.4.2.linux.amd64/bin:/home/travis/.rvm/gems/ruby-1.9.3-p551/bin:/home/travis/.rvm/gems/ruby-1.9.3-p551@global/bin:/home/travis/.rvm/rubies/ruby-1.9.3-p551/bin::/usr/local/phantomjs/bin:./node_modules/.bin:/usr/local/maven-3.2.5/bin:/usr/local/clang-3.4/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/travis/.rvm/bin \
  /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -iquote external/jpeg_archive -iquote bazel-out/local_linux-opt/genfiles/external/jpeg_archive -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/bazel_tools/tools/cpp/gcc3 -no-canonical-prefixes -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' '-frandom-seed=bazel-out/local_linux-opt/bin/external/jpeg_archive/_objs/jpeg/external/jpeg_archive/jpeg-9a/jcmaster.pic.o' -MD -MF bazel-out/local_linux-opt/bin/external/jpeg_archive/_objs/jpeg/external/jpeg_archive/jpeg-9a/jcmaster.pic.d -fPIC -c external/jpeg_archive/jpeg-9a/jcmaster.c -o bazel-out/local_linux-opt/bin/external/jpeg_archive/_objs/jpeg/external/jpeg_archive/jpeg-9a/jcmaster.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
```
"
1071,Local device intra op parallelism,"I followed the virtualenv installation on Mac OS X 10.11.3 using Python 3.5.1 but I'm seeing the following message when verifying setup:

```
$ python
Python 3.5.1 (default, Dec  7 2015, 23:19:42) 
[GCC 4.2.1 Compatible Apple LLVM 7.0.0 (clang-700.1.76)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> hello = tf.constant('Hello, TensorFlow!')
>>> sess = tf.Session()
I tensorflow/core/common_runtime/local_device.cc:40] Local device intra op parallelism threads: 8
I tensorflow/core/common_runtime/direct_session.cc:58] Direct session inter op parallelism threads: 8
>>> ^D
```
"
1070,Request: Equivalent of theano.clone() to dynamically replace subgraph,"I would like to know if this feature exist in the library or if there's any plan to have it in the future.

Thanks!
"
1069,Use external repository rather than git submodule for protobuf dependency,"Currently, TensorFlow vendors its protobuf dependency using a Git submodule due to the [hack](https://github.com/google/protobuf/blob/master/BUILD#L487) that is required for protobuf Python support.

I have made a change (bazelbuild/bazel#702) to add an [`imports` attribute](http://bazel.io/docs/be/python.html#py_library.imports) to Bazel's Python rules, which allows us to add directories to the `PYTHONPATH` and thus allows us to remove the hack in protobuf.

I have a patch to remove the hack for protobuf Python support (see google/protobuf#1230). I am testing this change using a corresponding change to TensorFlow which removes the git submodule and includes the protobuf dependency using a Bazel external repository instead. Once my patch for google/protobuf#1230 has been submitted, I will send out my corresponding patch to TensorFlow for review.
"
1067,problem building,"I'm having trouble building tensorflow from git.

```
$ bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer --verbose_failures
INFO: Found 1 target...
ERROR: /home/nmrp3/devel/oss/tensorflow/google/protobuf/BUILD:64:1: C++ compilation of rule '//google/protobuf:protobuf' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/nmrp3/.cache/bazel/_bazel_nmrp3/af5448b32eaac46105d430d1e2c5b789/tensorflow && \
  exec env - \
    PATH=/home/nmrp3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/jvm/java-8-oracle/bin:/usr/lib/jvm/java-8-oracle/db/bin:/usr/lib/jvm/java-8-oracle/jre/bin \
  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -isystem google/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/google/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -DHAVE_PTHREAD -Wall -Wwrite-strings -Woverloaded-virtual -Wno-sign-compare '-Wno-error=unused-function' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fno-canonical-system-headers '-frandom-seed=bazel-out/local_linux-opt/bin/google/protobuf/_objs/protobuf/google/protobuf/src/google/protobuf/descriptor.o' -MD -MF bazel-out/local_linux-opt/bin/google/protobuf/_objs/protobuf/google/protobuf/src/google/protobuf/descriptor.d -c google/protobuf/src/google/protobuf/descriptor.cc -o bazel-out/local_linux-opt/bin/google/protobuf/_objs/protobuf/google/protobuf/src/google/protobuf/descriptor.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 127: crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/nmrp3/.cache/bazel/_bazel_nmrp3/af5448b32eaac46105d430d1e2c5b789/tensorflow && \
  exec env - \
    PATH=/home/nmrp3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/jvm/java-8-oracle/bin:/usr/lib/jvm/java-8-oracle/db/bin:/usr/lib/jvm/java-8-oracle/jre/bin \
  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -isystem google/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/google/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -DHAVE_PTHREAD -Wall -Wwrite-strings -Woverloaded-virtual -Wno-sign-compare '-Wno-error=unused-function' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fno-canonical-system-headers '-frandom-seed=bazel-out/local_linux-opt/bin/google/protobuf/_objs/protobuf/google/protobuf/src/google/protobuf/descriptor.o' -MD -MF bazel-out/local_linux-opt/bin/google/protobuf/_objs/protobuf/google/protobuf/src/google/protobuf/descriptor.d -c google/protobuf/src/google/protobuf/descriptor.cc -o bazel-out/local_linux-opt/bin/google/protobuf/_objs/protobuf/google/protobuf/src/google/protobuf/descriptor.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 127.                                                
: No such file or directory                                                                                                                               
Target //tensorflow/cc:tutorials_example_trainer failed to build                                                                                          
INFO: Elapsed time: 0.405s, Critical Path: 0.17s    
```

My system is unbuntu

```
$ uname -a                                                                                                            
Linux taiwaif 4.2.0-27-generic #32-Ubuntu SMP Fri Jan 22 04:49:08 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
```

I checked the source out of github today, and am on branch MASTER.
"
1066,"build fail with cuda: identifier ""__builtin_ia32_mwaitx"" is undefined","Using gcc 5 with cuda support results in a compilation error:

```
INFO: From Compiling tensorflow/core/kernels/adjust_contrast_op_gpu.cu.cc:

# Omitting warnings

/usr/lib/gcc/x86_64-unknown-linux-gnu/5.3.0/include/mwaitxintrin.h(36): error: identifier ""__builtin_ia32_monitorx"" is undefined

/usr/lib/gcc/x86_64-unknown-linux-gnu/5.3.0/include/mwaitxintrin.h(42): error: identifier ""__builtin_ia32_mwaitx"" is undefined

2 errors detected in the compilation of ""/tmp/tmpxft_00002a59_00000000-7_adjust_contrast_op_gpu.cu.cpp1.ii"".
```

Build process is as follows:

```
  TF_UNOFFICIAL_SETTING=1 ./configure <<EOF
/usr/bin/python
y
7.5
/opt/cuda
4.0.7
/opt/cuda
5.2
EOF

    bazel build --jobs 4 --config=cuda -c opt --verbose_failures \
        //tensorflow/tools/pip_package:build_pip_package
```

The environnement is:
- bazel 0.1.5-1
- cuda 7.5
- cudnn 4
- gcc 5.3.0
- boost 1.60 (is it even relevant?)
- python3.5 (/usr/bin/python is python3)

This issue might be related to boost 1.60 or gcc 5 as mentioned [here](http://stackoverflow.com/questions/34996295/trying-to-get-cuda-7-5-to-work-with-gcc-5-x) or [there](https://medicineyeh.wordpress.com/2016/01/19/how-to-build-nvidia-caffe-deep-learning-on-arch-linux/).
One suggested fix is to restrict gcc to ansi c++, which can be achieved like so:

``` diff
diff --git a/third_party/gpus/crosstool/CROSSTOOL b/third_party/gpus/crosstool/CROSSTOOL
index dfde7cd..547441f 100644
--- a/third_party/gpus/crosstool/CROSSTOOL
+++ b/third_party/gpus/crosstool/CROSSTOOL
@@ -46,6 +46,8 @@ toolchain {
   # Use ""-std=c++11"" for nvcc. For consistency, force both the host compiler
   # and the device compiler to use ""-std=c++11"".
   cxx_flag: ""-std=c++11""
+  cxx_flag: ""-D_MWAITXINTRIN_H_INCLUDED""
+  cxx_flag: ""-D__STRICT_ANSI__""
   linker_flag: ""-lstdc++""
   linker_flag: ""-B/usr/bin/""
```

(btw. thank you @vrv for telling which file to change).

Could anyone please kindly review this, and integrate it (or not)? Note that I am not quite sure wether both flags are actually required and what the side effects might be.
"
1065,C++ unit test failure under GPU config: //tensorflow/core:ops_array_grad_test,"Using Python 2, Linux. This failure doesn't occur under the CPU config
`bazel test -c opt --config=cuda //tensorflow/core:ops_array_grad_test`

Error log:

> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so.7.0 lo
> cally
> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so.6.5 loc
> ally
> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so.7.0 locally
> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so locally
> I tensorflow/stream_executor/dso_loader.cc:99] Couldn't open CUDA library libcurand.so.7.0. LD_LIBRARY_PATH: 
> I tensorflow/stream_executor/cuda/cuda_rng.cc:333] Unable to load cuRAND DSO.
> Running main() from test_main.cc
> [==========] Running 10 tests from 1 test case.
> [----------] Global test environment set-up.
> [----------] 10 tests from ArrayGradTest
> [ RUN      ] ArrayGradTest.PackGrad
> I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 
> name: Quadro K620
> major: 5 minor: 0 memoryClockRate (GHz) 1.124
> pciBusID 0000:02:00.0
> Total memory: 2.00GiB
> Free memory: 1.57GiB
> I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 
> I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y 
> I tensorflow/core/common_runtime/gpu/gpu_device.cc:713] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro K620, pci bus id: 0000:02:00.0)
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.0KiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.0KiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.0KiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.0KiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.0KiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 32.0KiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 64.0KiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 128.0KiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 256.0KiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 512.0KiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.00MiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.00MiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.00MiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.00MiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.00MiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 32.00MiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 64.00MiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 128.00MiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 256.00MiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 512.00MiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.00GiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.00GiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:73] Allocating 1.37GiB bytes.
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:83] GPU 0 memory begins at 0xb011c0000 extends to 0xb58f68000
> [       OK ] ArrayGradTest.PackGrad (65832 ms)
> [ RUN      ] ArrayGradTest.UnpackGrad
> I tensorflow/core/common_runtime/gpu/gpu_device.cc:713] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro K620, pci bus id: 0000:02:00.0)
> [       OK ] ArrayGradTest.UnpackGrad (4 ms)
> [ RUN      ] ArrayGradTest.ConcatGrad
> I tensorflow/core/common_runtime/gpu/gpu_device.cc:713] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro K620, pci bus id: 0000:02:00.0)
> external/bazel_tools/tools/test/test-setup.sh: line 51: 112821 Segmentation fault      (core dumped) ""$@""
"
1064,Python3 unit test failure: //tensorflow/contrib/layers:summaries_test,"This failure occurs under Python3, but not Python2. 
`bazel test -c opt //tensorflow/contrib/layers:summaries_test`

Other environment info: 
OS: ""Linux"", kernel: ""4.2.0-18-generic"", architecture: ""x86_64"", processor: ""Intel(R) Xeon(R) CPU @ 2.50GHz"", processor_count: ""32"", memory_total: ""61845920 kB"", swap_total: ""0 kB"", Bazel_version: ""Build label: 0.1.4"", Java_version: ""1.8.0_72-internal"", pp_version: ""g++ (Ubuntu 4.8.4-2ubuntu1~14.04) 4.8.4""

> ==================== Test output for //tensorflow/contrib/layers:summaries_test:
> F./var/lib/jenkins/workspace/tensorflow-master-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out/local_linux-py3-opt/bin/tensorflow/contrib/layers/summaries_test.runfiles/tensorflow/contrib/layers/python/layers/summaries_test.py:57: DeprecationWarning: Please use assertEqual instead.
>   self.assertEquals(len(names), 1)
> # ......
> ## FAIL: test_duplicate_tag (**main**.SummariesTest)
> 
> Traceback (most recent call last):
>   File ""/var/lib/jenkins/workspace/tensorflow-master-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out/local_linux-py3-opt/bin/tensorflow/contrib/layers/summaries_test.runfiles/tensorflow/contrib/layers/python/layers/summaries_test.py"", line 35, in test_duplicate_tag
>     tf.contrib.layers.summarize_tensor(var)
> AssertionError: ValueError not raised
> ---
> 
> Ran 8 tests in 0.058s
> 
> FAILED (failures=1)
"
1063,Failed cuda build from master ,"Use Docker 1.10 with official nvidia image: nvidia/cuda:7.0-cudnn2-devel 
Have tried Bazel 1.0.0, 1.0.4, 1.0.5
TF version from current git master
Whats wrong?

`____From Compiling tensorflow/core/kernels/cwise_op_not_equal_to.cc:
gcc: internal compiler error: Killed (program cc1plus)
Please submit a full bug report,
with preprocessed source if appropriate.
See <file:///usr/share/doc/gcc-4.8/README.Bugs> for instructions.
ERROR: /tensorflow/tensorflow/core/BUILD:355:1: C++ compilation of rule '//tenso
rflow/core:kernel_lib' failed: gcc failed: error executing command /usr/bin/gcc
-U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wunused-but-set
-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ff
unction-sections ... (remaining 77 argument(s) skipped): com.google.devtools.bui
ld.lib.shell.BadExitStatusException: Process exited with status 4.
____Building complete.
Target //tensorflow/cc:tutorials_example_trainer failed to build
Use --verbose_failures to see the command lines of failed build steps.
____Elapsed time: 1975.401s, Critical Path: 1894.23s
The command '/bin/sh -c bazel build -c opt --config=cuda //tensorflow/cc:tutoria
ls_example_trainer' returned a non-zero code: 1`
"
1062,python3.5 support and @ operator with  __matmul__  method,"Are there any plans to include Python3.5 support (incl. binaries) and [dedicated matmul `@` operator](https://www.python.org/dev/peps/pep-0465/)?
"
1061,clip_by_value doesn't work with float64,"error message:

```
Traceback (most recent call last):
  File ""_script_nn_skipgram1.py"", line 3, in <module>
    nn_skip = NN_SKIP()
  File ""nn_skipgram1.py"", line 122, in __init__
    self.train_step = optimizer.apply_gradients(zip(grads, tvars))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py"", line 282, in     apply_gradients
    self._assert_valid_dtypes([g, v])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py"", line 354, in _assert_valid_dtypes
    dtype, t.name, [v for v in valid_dtypes]))
ValueError: Invalid type tf.float64 for clip_by_value:0, expected: [tf.float32].
```

original thread:
https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/A_qsCKiEKGE
"
1060,Running Tensorflow on GeForce 940M (Ubuntu 14),"Hi everyone,

I'm running the CIFAR-10 classification from the Tensorflow for the very first time on my laptop with GeForce 940M. I'm running the training with the pre-defined parameters as follows:

`python cifar10_train.py`

after step 1800 I'm getting the following errors:

`E tensorflow/stream_executor/cuda/cuda_event.cc:33] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS
E tensorflow/stream_executor/cuda/cuda_driver.cc:1182] failed to enqueue async memcpy from device to host: CUDA_ERROR_ILLEGAL_ADDRESS; host dst: 0x7ff8e9bf26c0; GPU src: 0x5011c0600; size: 16=0x10
F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:105] Unexpected Event status: 1
I tensorflow/stream_executor/stream.cc:3304] stream 0x35e7190 did not block host until done; was already in an error state
Aborted (core dumped)`

Does anybody have any idea?

By the way, the following fix was required on my side to get the training running:

`from tensorflow.python.platform import gfile
gfile.Exists(...)`

without ""ts."" in front, it didn't work that way... Any ideas why? I've just forked the repository without any changes.

Thanks a lot in advance for your help! Any advice is kindly appreciated!
"
1056,Optimized build across different architecture,"I built a pip package with  ̀-c opt` in one machine and use the same package on another machine. They don't have different version of core i7 processor. With the second machine, Tensorflow runs 10x slower than with the first. Could it be a compilation optimization problem ?
"
1052,failed on build from source,"envy@ub1404envy:~/os_prj/github/tensorflow$ bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer
INFO: Found 1 target...
ERROR: /home/envy/os_prj/github/tensorflow/third_party/gpus/cuda/BUILD:126:1: declared output 'third_party/gpus/cuda/lib64/libcublas.so.7.0' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely).
ERROR: /home/envy/os_prj/github/tensorflow/third_party/gpus/cuda/BUILD:126:1: declared output 'third_party/gpus/cuda/lib64/libcudart.so.7.0' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely).
ERROR: /home/envy/os_prj/github/tensorflow/third_party/gpus/cuda/BUILD:126:1: declared output 'third_party/gpus/cuda/lib64/libcufft.so.7.0' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely).
ERROR: /home/envy/os_prj/github/tensorflow/third_party/gpus/cuda/BUILD:126:1: not all outputs were created.
Target //tensorflow/cc:tutorials_example_trainer failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 31.876s, Critical Path: 1.20s
envy@ub1404envy:~/os_prj/github/tensorflow$ 
"
1049,How to set the max_grad_norm with AdamOptimizer?,"New to the TensorFlow.

I am working closed with RNN. To avoid overfitting, I am trying to control the max_grad_norm when the gradient passes through.

Here is the code I worked with.

```
    # Define optimizer with norm limitation
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred_y, y)) # Softmax loss
tvars = tf.trainable_variables()
grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), max_grad_norm)
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) # Adam Optimizer
optimizer.apply_gradients(zip(grads, tvars))
```

I kept receiving the error that

TypeError: Fetch argument <tensorflow.python.training.adam.AdamOptimizer object at 0x7f0f02b1ce50> of <tensorflow.python.training.adam.AdamOptimizer object at 0x7f0f02b1ce50> has invalid type <class 'tensorflow.python.training.adam.AdamOptimizer'>, must be a string or Tensor. (Can not convert a AdamOptimizer into a Tensor or Operation.)

All suggestions are more than welcome.
Thanks
"
1047,TF 0.6.0 build fails with bazel 0.1.5,"Try to build TF ver. 0.6  with Bazel 0.1.5, got this:

`ERROR: /tensorflow/WORKSPACE:70:1: new_git_repository rule //external:iron-ajax'
s name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:77:1: new_git_repository rule //external:iron-dropd
own's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:84:1: new_git_repository rule //external:accessibil
ity-developer-tools's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:91:1: new_git_repository rule //external:iron-doc-v
iewer's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:98:1: new_git_repository rule //external:iron-icons
's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:105:1: new_git_repository rule //external:paper-ico
n-button's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:119:1: new_git_repository rule //external:paper-dro
pdown-menu's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:126:1: new_git_repository rule //external:iron-flex
-layout's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:133:1: new_git_repository rule //external:iron-auto
grow-textarea's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:147:1: new_git_repository rule //external:iron-comp
onent-page's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:161:1: new_git_repository rule //external:paper-sty
les's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:168:1: new_git_repository rule //external:paper-inp
ut's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:175:1: new_git_repository rule //external:paper-ite
m's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:182:1: new_git_repository rule //external:marked-el
ement's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:196:1: new_git_repository rule //external:paper-pro
gress's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:203:1: new_git_repository rule //external:iron-chec
ked-element-behavior's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:210:1: new_git_repository rule //external:paper-too
lbar's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:224:1: new_git_repository rule //external:es6-promi
se's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:231:1: new_git_repository rule //external:promise-p
olyfill's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:238:1: new_git_repository rule //external:font-robo
to's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:245:1: new_git_repository rule //external:paper-men
u's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:252:1: new_git_repository rule //external:iron-icon
's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:259:1: new_git_repository rule //external:iron-meta
's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:273:1: new_git_repository rule //external:iron-resi
zable-behavior's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:280:1: new_git_repository rule //external:iron-fit-
behavior's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:287:1: new_git_repository rule //external:iron-over
lay-behavior's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:294:1: new_git_repository rule //external:neon-anim
ation's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:301:1: new_git_repository rule //external:iron-a11y
-keys-behavior's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:322:1: new_git_repository rule //external:iron-vali
datable-behavior's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:329:1: new_git_repository rule //external:sinon-cha
i's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:336:1: new_git_repository rule //external:paper-but
ton's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:343:1: new_git_repository rule //external:iron-inpu
t's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:350:1: new_git_repository rule //external:iron-menu
-behavior's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:357:1: new_git_repository rule //external:paper-sli
der's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:364:1: new_git_repository rule //external:iron-list
's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:378:1: new_git_repository rule //external:paper-mat
erial's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:385:1: new_git_repository rule //external:iron-rang
e-behavior's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:392:1: new_git_repository rule //external:svg-typew
riter's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:399:1: new_git_repository rule //external:web-anima
tions-js's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:413:1: new_git_repository rule //external:web-compo
nent-tester's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:420:1: new_git_repository rule //external:paper-tog
gle-button's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:427:1: new_git_repository rule //external:paper-beh
aviors's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:434:1: new_git_repository rule //external:paper-rad
io-group's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:441:1: new_git_repository rule //external:iron-sele
ctor's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:448:1: new_git_repository rule //external:iron-form
-element-behavior's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:469:1: new_git_repository rule //external:iron-beha
viors's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:483:1: new_git_repository rule //external:iron-coll
apse's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:490:1: new_git_repository rule //external:paper-che
ckbox's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:497:1: new_git_repository rule //external:paper-rad
io-button's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:504:1: new_git_repository rule //external:paper-hea
der-panel's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:511:1: new_git_repository rule //external:prism-ele
ment's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:525:1: new_git_repository rule //external:paper-men
u-button's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:539:1: new_git_repository rule //external:paper-rip
ple's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:546:1: new_git_repository rule //external:iron-icon
set-svg's name field must be a legal workspace name.
ERROR: Error evaluating WORKSPACE file.
ERROR: no such package 'external': Package 'external' contains errors.
____Elapsed time: 2.171s`

I think should consider [this commit](https://github.com/bazelbuild/bazel/commit/d21c2d6653a3d9bc3376bcb190ba0ac31f52195b)
"
1046,Graphs tab on TensorBoard failing with a JavaScript error,"Built from TOT today (Feb 10th) with GPU support.  TensorBoard loads and the Events tab works, but when I go to the Graph tab, I see `Uncaught TypeError: Polymer.dom(...).unobserveNodes is not a function`.  

I'm a TensorFlow rookie, so good chance I did something wrong.  
"
1045,tf.train.string_input_producer breaks when num_epochs is set,"I'm using a tf.train.string_input_producer to read in data from a file.  when I set num_epochs=1 instead of None it breaks and I get the following error.  My code is below as well.

Any suggestions for why this is occuring?

``` text
I tensorflow/core/common_runtime/local_device.cc:40] Local device intra op parallelism threads: 8
I tensorflow/core/common_runtime/direct_session.cc:58] Direct session inter op parallelism threads: 8
W tensorflow/core/common_runtime/executor.cc:1076] 0x7fdc4ba6b0f0 Compute status: Out of range: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 10, current size 0)
     [[Node: shuffle_batch = QueueDequeueMany[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]
W tensorflow/core/common_runtime/executor.cc:1076] 0x7fdc48da9170 Compute status: Out of range: FIFOQueue '_0_file_queue' is closed and has insufficient elements (requested 1, current size 0)
     [[Node: ReaderRead = ReaderRead[_device=""/job:localhost/replica:0/task:0/cpu:0""](TFRecordReader, file_queue)]]
W tensorflow/core/common_runtime/executor.cc:1076] 0x7fdc4b8dbe20 Compute status: Aborted: Queue '_2_shuffle_batch/random_shuffle_queue' is already closed.
     [[Node: shuffle_batch/random_shuffle_queue_Close = QueueClose[cancel_pending_enqueues=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](shuffle_batch/random_shuffle_queue)]]
Traceback (most recent call last):
  File ""tensorflow_test.py"", line 51, in <module>
    coord.join(threads)
  File ""/Users/blakec/virtual_envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py"", line 205, in join
    six.reraise(*self._exc_info_to_raise)
  File ""/Users/blakec/virtual_envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/queue_runner.py"", line 112, in _run
    sess.run(enqueue_op)
  File ""/Users/blakec/virtual_envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 368, in run
    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)
  File ""/Users/blakec/virtual_envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 444, in _do_run
    e.code)
tensorflow.python.framework.errors.FailedPreconditionError: Attempting to use uninitialized value file_queue/limit_epochs/epochs
     [[Node: file_queue/limit_epochs/CountUpTo = CountUpTo[T=DT_INT64, limit=2, _device=""/job:localhost/replica:0/task:0/cpu:0""](file_queue/limit_epochs/epochs)]]
Caused by op u'file_queue/limit_epochs/CountUpTo', defined at:
  File ""tensorflow_test.py"", line 6, in <module>
    filename_queue = tf.train.string_input_producer([""datasets_top_10_50k/VALIDATION_testset/part-00000..tf""], num_epochs=2,capacity=10000, name=""file_queue"")
  File ""/Users/blakec/virtual_envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/input.py"", line 135, in string_input_producer
    ""fraction_of_%d_full"" % capacity)
  File ""/Users/blakec/virtual_envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/input.py"", line 86, in _input_producer
    input_tensor = limit_epochs(input_tensor, num_epochs)
  File ""/Users/blakec/virtual_envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/input.py"", line 77, in limit_epochs
    counter = epochs.count_up_to(num_epochs)
  File ""/Users/blakec/virtual_envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/variables.py"", line 414, in count_up_to
    return state_ops.count_up_to(self._variable, limit=limit)
  File ""/Users/blakec/virtual_envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.py"", line 110, in count_up_to
    return _op_def_lib.apply_op(""CountUpTo"", ref=ref, limit=limit, name=name)
  File ""/Users/blakec/virtual_envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py"", line 664, in apply_op
    op_def=op_def)
  File ""/Users/blakec/virtual_envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1834, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/Users/blakec/virtual_envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1043, in __init__
    self._traceback = _extract_stack()
```

Code:

``` python
import tensorflow as tf
import time

filename_queue = tf.train.string_input_producer([""datasets_top_10_50k/VALIDATION_testset/part-00000..tf""], num_epochs=2,capacity=10000, name=""file_queue"")
reader = tf.TFRecordReader()
_, serialized_example = reader.read(filename_queue)

data = tf.parse_single_example(
      serialized_example,
      dense_keys=[""features"",""labels""],
      dense_types=[tf.float32, tf.float32],
      dense_shapes=[(4096),(10)])

features = data[""features""]
labels = data[""labels""]

batch_size=10
capacity=1000
min_after_dequeue=100
example_batch, label_batch = tf.train.shuffle_batch(
      [features, labels], 
      batch_size=batch_size, 
      capacity=capacity,
      min_after_dequeue=min_after_dequeue)
num_examples = 0
with tf.Session() as sess:
    # Start populating the filename queue.
    coord = tf.train.Coordinator()  
    threads = tf.train.start_queue_runners(coord=coord, sess=sess)

    try:
        step = 0
        while not coord.should_stop():
            start_time = time.time()
            e, l = sess.run([example_batch, label_batch])
            print ""grabbing""
            e, l = sess.run([example_batch, label_batch])
            num_examples = num_examples + e.shape[0]
            print ""num_examples = "" + str(num_examples)
            duration = time.time() - start_time

    except tf.errors.OutOfRangeError:
        print('Done training for %d epochs, %d steps.' % (FLAGS.num_epochs, step))
    finally:
        # When done, ask the threads to stop.
        coord.request_stop()

        # Wait for threads to finish.
        coord.join(threads)
        sess.close()
```
"
1043,Alexnet Multi GPU,"I am new to this and would like to run alexnet on multiple GPUs. Can you provide some pointers to that?
"
1041,Build pip package from source failed,"I got the following error while building pip package from the master version 

`tensorflow/core/BUILD:75:1: Target '//google/protobuf:protobuf_python_genproto' is not visible from target '//tensorflow/core:protos_all_py_genproto'. Check the visibility declaration of the former target if you think the dependency is legitimate.`

Bazel version 0.1.4 and 0.1.5
"
1040,Unimplement: Op TopK is not available in GraphDef version 8.,"Hi !

I've just finish to install TensorFlow on my OSX 10.11.13.

When I do: `bazel-bin/tensorflow/examples/label_image`

`E tensorflow/core/common_runtime/executor.cc:275] Executor failed to create kernel. Unimplemented: Op TopK is not available in GraphDef version 8. It has been removed in version 7. Use TopKV2 instead.
     [[Node: top_k = TopK[T=DT_FLOAT, k=5, sorted=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](Const/_0)]]
E tensorflow/examples/label_image/main.cc:321] Running print failed: Unimplemented: Op TopK is not available in GraphDef version 8. It has been removed in version 7. Use TopKV2 instead.
     [[Node: top_k = TopK[T=DT_FLOAT, k=5, sorted=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](Const/_0)]]`

Did someone already seen this ?
"
1038,sequence_loss_by_example()  type error for rnn/ptb,"Hi 

I downloaded 0.6.0 for python 2.7 and I tried running the rnn/ptb example but got a type error:

$ python ptb_word_lm.py --data_path=/this/path/ --model small

I tensorflow/core/common_runtime/local_device.cc:40] Local device intra op parallelism threads: 8
I tensorflow/core/common_runtime/direct_session.cc:58] Direct session inter op parallelism threads: 8
Traceback (most recent call last):
  File ""ptb_word_lm.py"", line 303, in <module>
    tf.app.run()
  File ""/Library/Python/2.7/site-packages/tensorflow/python/platform/default/_app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""ptb_word_lm.py"", line 280, in main
    m = PTBModel(is_training=True, config=config)
  File ""ptb_word_lm.py"", line 135, in __init__
    [tf.ones([batch_size \* num_steps])])
TypeError: sequence_loss_by_example() takes at least 4 arguments (3 given)

It seems that this sequence_loss_by_example() method is imported from seq2seq which imports it from tensorflow.python.ops.seq2seq. Beyond that I'm not really sure how this happened.

Thanks
"
1037,bidirectional_rnn return state,"Hi I am wondering why the bi-directional rnn doesn't return a final state? From a code point of view it would be very easy to return. I.e line 301 in rnn.py  

 output_fw, _ = rnn(cell_fw, inputs, initial_state_fw, dtype,
                       sequence_length)

One could just return _ (named as final_state or whatever) with the concatenated outputs at the end. Is there any theoretical reason why you would never want the final output state? I was planning on feeding the final forward state as the initial state of the forward and backward rnn's on the next pass. The reason I am wanting to do this is the sequences I am working on are much longer than the number of steps I can unroll (seqs are about 15k on average, I'm unrolling 1000 steps) so continuing state is pretty important. It seems weird that the bidirectional rnn would even have the ability to input an initial state but you can't return the output state (what state would you feed in if not the output of the last pass?) 

Let me know what you think, i'm no rnn expert. 

Off topic but does anyone have any advice for super long sequences? What is the maximum you think I could unroll? Are there other techniques I could use besides long unroll and refeeding the outputs? Can LSTM even learn 1000+ step dependencies? 
"
1035,Python3 test failure: //tensorflow/python:saver_test,"Using the Python-3 configure, run command:
`bazel test -c opt //tensorflow/python:saver_test`

See below for the log of the failed test, with the key lines highlighted

........../usr/local/google/home/cais/.cache/bazel/_bazel_cais/10ddfdc323bb20c9bc23987d4925ae7b/tensorflow-caisq/bazel-out/local_linux-py3-opt/bin/tensorflow/python/saver_test.runfiles/tensorflow/python/training/saver.py:848: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logging.warning(""Ignoring: %s"", str(e))
WARNING:tensorflow:Ignoring: [Errno 2] No such file or directory: '/tmp/saver_test/max_to_keep_sharded/s1-00001-of-00002.meta'
WARNING:tensorflow:Ignoring: [Errno 2] No such file or directory: '/tmp/saver_test/max_to_keep_sharded/s1-00000-of-00002.meta'
../usr/local/google/home/cais/.cache/bazel/_bazel_cais/10ddfdc323bb20c9bc23987d4925ae7b/tensorflow-caisq/bazel-out/local_linux-py3-opt/bin/tensorflow/python/saver_test.runfiles/tensorflow/python/training/saver.py:1088: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  ""match field type in CollectionDef.\n%s"" % str(e))
WARNING:tensorflow:Type is unsupported, or the types of the items don't match field type in CollectionDef.
**'hello' has type str, but expected one of: bytes**
WARNING:tensorflow:Type is unsupported, or the types of the items don't match field type in CollectionDef.
name 'long' is not defined
WARNING:tensorflow:Type is unsupported, or the types of the items don't match field type in CollectionDef.
name 'long' is not defined
WARNING:tensorflow:Type is unsupported, or the types of the items don't match field type in CollectionDef.
name 'long' is not defined
WARNING:tensorflow:Type is unsupported, or the types of the items don't match field type in CollectionDef.
**'queue_name: ""test_queue""\n' has type str, but expected one of: bytes**
F/usr/local/google/home/cais/.cache/bazel/_bazel_cais/10ddfdc323bb20c9bc23987d4925ae7b/tensorflow-caisq/bazel-out/local_linux-py3-opt/bin/tensorflow/python/saver_test.runfiles/tensorflow/python/training/saver.py:1063: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  ""serialized. This key has %s"" % type(key))
**WARNING:tensorflow:Only collections with string type keys will be serialized. This key has <class 'tensorflow.python.training.saver.Saver'>**
WARNING:tensorflow:Type is unsupported, or the types of the items don't match field type in CollectionDef.
name 'long' is not defined
.WARNING:tensorflow:Type is unsupported, or the types of the items don't match field type in CollectionDef.
name 'long' is not defined
WARNING:tensorflow:Type is unsupported, or the types of the items don't match field type in CollectionDef.
name 'long' is not defined
E.......W tensorflow/core/common_runtime/executor.cc:1094] 0x33a34d0 Compute status: Not found: Tensor name ""v1"" not found in checkpoint files /tmp/saver_test/basics
         [[Node: save_1/restore_slice_1 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](_recv_save_1/Const_0, save_1/restore_slice_1/tensor_name, save_1/restore_slice_1/shape_and_slice)]]
W tensorflow/core/common_runtime/executor.cc:1094] 0x33a34d0 Compute status: Not found: Tensor name ""v0"" not found in checkpoint files /tmp/saver_test/basics
         [[Node: save_1/restore_slice = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](_recv_save_1/Const_0, save_1/restore_slice/tensor_name, save_1/restore_slice/shape_and_slice)]]
# ...........
## ERROR: testGraphExtension (**main**.MetaGraphTest)

Traceback (most recent call last):
  File ""/usr/local/google/home/cais/.cache/bazel/_bazel_cais/10ddfdc323bb20c9bc23987d4925ae7b/tensorflow-caisq/bazel-out/local_linux-py3-opt/bin/tensorflow/python/saver_test.runfiles/tensorflow/python/training/saver_test.py"", line 968, in testGraphExtension
    self._testGraphExtensionRestore()
  File ""/usr/local/google/home/cais/.cache/bazel/_bazel_cais/10ddfdc323bb20c9bc23987d4925ae7b/tensorflow-caisq/bazel-out/local_linux-py3-opt/bin/tensorflow/python/saver_test.runfiles/tensorflow/python/training/saver_test.py"", line 952, in _testGraphExtensionRestore
    logits = tf.get_collection(""logits"")[0]
**IndexError: list index out of range**
# 
## FAIL: testAddCollectionDef (**main**.MetaGraphTest)

Traceback (most recent call last):
  File ""/usr/local/google/home/cais/.cache/bazel/_bazel_cais/10ddfdc323bb20c9bc23987d4925ae7b/tensorflow-caisq/bazel-out/local_linux-py3-opt/bin/tensorflow/python/saver_test.runfiles/tensorflow/python/training/saver_test.py"", line 760, in testAddCollectionDef
    self.assertEqual(len(collection_def), 9)
**AssertionError: 4 != 9**
"
1033,build_pip_package error on Mac+Python3,"See detailed build log at: http://ci.tensorflow.org/view/Experimental/job/experimental-cais-tensorflow-mac-python3-copt_pip_install-test/1/consoleFull

> In file included from tensorflow/core/kernels/conv_ops.cc:22:
> In file included from ./tensorflow/core/framework/numeric_op.h:19:
> In file included from ./tensorflow/core/framework/op_kernel.h:22:
> In file included from ./tensorflow/core/framework/allocator.h:25:
> In file included from ./tensorflow/core/framework/type_traits.h:22:
> In file included from ./tensorflow/core/framework/types.h:23:
> In file included from third_party/eigen3/unsupported/Eigen/CXX11/Tensor:2:
> In file included from external/eigen_archive/eigen-eigen-8cd7c2c6e9e1/unsupported/Eigen/CXX11/Tensor:92:
> external/eigen_archive/eigen-eigen-8cd7c2c6e9e1/unsupported/Eigen/CXX11/src/Tensor/TensorContractionBlocking.h:29:31: error: no type named 'Scalar' in 'Eigen::internal::TensorContractionInputMapper<float, long, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long>, 16> > >, Eigen::ThreadPoolDevice>, Eigen::array<long, 1>, Eigen::array<long, 1>, 4, false, false, 0>'
>   typedef typename RhsMapper::Scalar RhsScalar;
>           ~~~~~~~~~~~~~~~~~~~~^~~~~~

This error doesn't occur if Python2 is used on Mac.
"
1032,Compilation Error on Travis.ci,"Hi,

I'm getting a gcc error when trying to build on a docker image on travis.ci. Any ideas would be appreciated.

Full Error:

```
ERROR: /home/travis/build/peterbraden/node-tensorflow/lib/tensorflow/google/protobuf/BUILD:64:1: C++ compilation of rule '//google/protobuf:protobuf' failed: gcc failed: error executing command 
  (cd /home/travis/.cache/bazel/_bazel_travis/2cf95f6ff0f7dc500e0eb5aab5889c29/tensorflow && \
  exec env - \
    PATH=/home/travis/.nvm/versions/node/v4.1.2/lib/node_modules/npm/bin/node-gyp-bin:/home/travis/build/peterbraden/node-tensorflow/node_modules/.bin:/home/travis/.nvm/versions/node/v4.1.2/bin:/home/travis/bin:/home/travis/.local/bin:/home/travis/.gimme/versions/go1.4.1.linux.amd64/bin:/usr/local/phantomjs/bin:./node_modules/.bin:/usr/local/maven-3.2.5/bin:/usr/local/clang-3.4/bin:/home/travis/.gimme/versions/go1.4.1.linux.amd64/bin:/usr/local/phantomjs/bin:./node_modules/.bin:/usr/local/maven-3.2.5/bin:/usr/local/clang-3.4/bin:/home/travis/.gimme/versions/go1.4.1.linux.amd64/bin:/home/travis/.rvm/gems/ruby-1.9.3-p551/bin:/home/travis/.rvm/gems/ruby-1.9.3-p551@global/bin:/home/travis/.rvm/rubies/ruby-1.9.3-p551/bin:/usr/local/phantomjs/bin:./node_modules/.bin:/usr/local/maven-3.2.5/bin:/usr/local/clang-3.4/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/travis/.rvm/bin:/home/travis/.rvm/bin:/home/travis/.rvm/bin \
  /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer '-std=c++0x' -iquote . -iquote bazel-out/local_linux-fastbuild/genfiles -iquote external/bazel_tools -iquote bazel-out/local_linux-fastbuild/genfiles/external/bazel_tools -isystem google/protobuf/src -isystem bazel-out/local_linux-fastbuild/genfiles/google/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -DHAVE_PTHREAD -Wall -Wwrite-strings -Woverloaded-virtual -Wno-sign-compare '-Wno-error=unused-function' -no-canonical-prefixes -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' '-frandom-seed=bazel-out/local_linux-fastbuild/bin/google/protobuf/_objs/protobuf/google/protobuf/src/google/protobuf/util/type_resolver_util.pic.o' -MD -MF bazel-out/local_linux-fastbuild/bin/google/protobuf/_objs/protobuf/google/protobuf/src/google/protobuf/util/type_resolver_util.pic.d -fPIC -c google/protobuf/src/google/protobuf/util/type_resolver_util.cc -o bazel-out/local_linux-fastbuild/bin/google/protobuf/_objs/protobuf/google/protobuf/src/google/protobuf/util/type_resolver_util.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: gcc failed: error executing command 
  (cd /home/travis/.cache/bazel/_bazel_travis/2cf95f6ff0f7dc500e0eb5aab5889c29/tensorflow && \
  exec env - \
    PATH=/home/travis/.nvm/versions/node/v4.1.2/lib/node_modules/npm/bin/node-gyp-bin:/home/travis/build/peterbraden/node-tensorflow/node_modules/.bin:/home/travis/.nvm/versions/node/v4.1.2/bin:/home/travis/bin:/home/travis/.local/bin:/home/travis/.gimme/versions/go1.4.1.linux.amd64/bin:/usr/local/phantomjs/bin:./node_modules/.bin:/usr/local/maven-3.2.5/bin:/usr/local/clang-3.4/bin:/home/travis/.gimme/versions/go1.4.1.linux.amd64/bin:/usr/local/phantomjs/bin:./node_modules/.bin:/usr/local/maven-3.2.5/bin:/usr/local/clang-3.4/bin:/home/travis/.gimme/versions/go1.4.1.linux.amd64/bin:/home/travis/.rvm/gems/ruby-1.9.3-p551/bin:/home/travis/.rvm/gems/ruby-1.9.3-p551@global/bin:/home/travis/.rvm/rubies/ruby-1.9.3-p551/bin:/usr/local/phantomjs/bin:./node_modules/.bin:/usr/local/maven-3.2.5/bin:/usr/local/clang-3.4/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/travis/.rvm/bin:/home/travis/.rvm/bin:/home/travis/.rvm/bin \
  /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer '-std=c++0x' -iquote . -iquote bazel-out/local_linux-fastbuild/genfiles -iquote external/bazel_tools -iquote bazel-out/local_linux-fastbuild/genfiles/external/bazel_tools -isystem google/protobuf/src -isystem bazel-out/local_linux-fastbuild/genfiles/google/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -DHAVE_PTHREAD -Wall -Wwrite-strings -Woverloaded-virtual -Wno-sign-compare '-Wno-error=unused-function' -no-canonical-prefixes -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' '-frandom-seed=bazel-out/local_linux-fastbuild/bin/google/protobuf/_objs/protobuf/google/protobuf/src/google/protobuf/util/type_resolver_util.pic.o' -MD -MF bazel-out/local_linux-fastbuild/bin/google/protobuf/_objs/protobuf/google/protobuf/src/google/protobuf/util/type_resolver_util.pic.d -fPIC -c google/protobuf/src/google/protobuf/util/type_resolver_util.cc -o bazel-out/local_linux-fastbuild/bin/google/protobuf/_objs/protobuf/google/protobuf/src/google/protobuf/util/type_resolver_util.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
```

And the offending build: 

https://travis-ci.org/peterbraden/node-tensorflow/builds/108069558
"
1030,problem with rnn comments regarding dynamic seq length?,"I found this in the rnn comments:

```
  Dynamic calculation returns, at time t:
    (t >= max(sequence_length)
        ? (zeros(output_shape), zeros(state_shape))
        : cell(input, state)
```

Is this correct? It seems unreasonable to throw away the final state by overwriting it with 0s after it ran over the maximum sequence length. From reading the later code section for _dynamic_rnn_step there's this comment:

```
  if t >= max_sequence_length:
    return (zero_output, state)
  if t < min_sequence_length:
    return call_cell()
```

that seems to suggest that the final state is actually the last state before we ran over the maximum sequence length, and only the output is zero-ed out, which would make more sense.

I'm not sure which one is actually being done in the implementation, but these 2 views are not consistent.
"
1029,tf.image.resize_images() expects statically known rank,"It looks like the op `tf.image.resize_images()` has no effect on the image shape when the parameters `new_height` and `new_width` are dynamically computed (this is possible since the fix of issue #1001).

For example, this piece of code

```
shape = tf.shape(image)
height = shape[0]
width = shape[1]
new_shorter_edge = tf.constant(400, dtype=tf.int32)

height_smaller_than_width = tf.less_equal(height, width)
new_height_and_width = tf.cond(
    height_smaller_than_width,
    lambda: (new_shorter_edge, _compute_longer_edge(height, width, new_shorter_edge)),
    lambda: (_compute_longer_edge(width, height, new_shorter_edge), new_shorter_edge)
)

image = tf.image.resize_images(image, new_height_and_width[0], new_height_and_width[1])
image = tf.Print(image, [tf.shape(image), height, width, new_height_and_width[0], new_height_and_width[1]])
return tf.image.random_crop(image, [224, 224])
```

throws the following exception:

```
I tensorflow/core/kernels/logging_ops.cc:79] [122 160 3][122][160][400][524]
W tensorflow/core/common_runtime/executor.cc:1096] 0x7f9c08008490 Compute status: Failed precondition: width must be >= target_width: width = 160, target_width = 224
```

So as you can see in the first line of the printout, the shape of the image is not affected by `tf.image.resize_images`. Is this a bug or am I doing something wrong?

---

I also made up a workaround for this:

```
image = tf.expand_dims(image, 0)
image = tf.image.resize_bilinear(image, tf.pack(new_height_and_width))
image = tf.squeeze(image, [0])
image = tf.Print(image, [tf.shape(image), height, width, new_height_and_width[0], new_height_and_width[1]])
return tf.image.random_crop(image, [224, 224])
```

for example prints

```
I tensorflow/core/kernels/logging_ops.cc:79] [400 711 3][281][500][400][711]
```
"
1028,Tensorflow with CUDA build failure in Docker,"Hello) Trying to make dockerfile with ubuntu 14, tensorflow on python3 with cuda and ec2 support ..

TF ver 0.6.0
Bazel ver 0.1.4
Starting from docker image: nvidia/cuda:7.0-cudnn2-devel

In dockerfile have smth like this:

```

RUN TF_UNOFFICIAL_SETTING=1 \
     TF_CUDA_COMPUTE_CAPABILITIES=3.0 \
     PYTHON_BIN_PATH=/usr/bin/python3 ./configure && \
   bazel build \
     --config=cuda \
     --verbose_failures \
     --spawn_strategy=standalone \
     --local_resources 4096,1.0,1.0 \
     tensorflow/tools/pip_package:build_pip_package && \

    bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip && \

    echo $(date) : ""=== Building wheel"" && \

    python3 setup.py bdist_wheel --python-tag py34 >/dev/null && \

    pip3 install --upgrade /tmp/pip/tensorflow-0.6.0-py34-none-any.whl
```

Without --localresources had ""Killed"" error like [here](http://stackoverflow.com/questions/34699069/building-tensorflow-from-source-on-ubuntu-14-04-lts-gcc-internal-compiler-erro)
--spawn_strategy from [here](https://github.com/bazelbuild/bazel/issues/698)

Now have such error, and have no idea what to do next =(

```

`____[577 / 1,175] Compiling tensorflow/python/pywrap_tensorflow.cc
____[580 / 1,175] Compiling tensorflow/core/util/work_sharder.cc [for host]
____[585 / 1,175] Compiling tensorflow/core/lib/hash/crc32c.cc [for host]
____[587 / 1,175] Compiling tensorflow/core/lib/histogram/histogram.cc [for host
]
ERROR: /tensorflow/tensorflow/stream_executor/BUILD:5:1: undeclared inclusion(s)
in rule '//tensorflow/stream_executor:stream_executor':
this rule is missing dependency declarations for the following files included by
'tensorflow/stream_executor/cuda/cuda_rng.cc':
'/tensorflow/tensorflow/stream_executor/cuda/cuda_rng.h'
'/tensorflow/tensorflow/stream_executor/cuda/cuda_activation.h'
'/tensorflow/tensorflow/stream_executor/cuda/multi_op_activation.h'
'/tensorflow/tensorflow/stream_executor/cuda/cuda_gpu_executor.h'
'/tensorflow/tensorflow/stream_executor/cuda/cuda_kernel.h'
'/tensorflow/tensorflow/stream_executor/cuda/cuda_driver.h'
'/tensorflow/tensorflow/stream_executor/cuda/cuda_helpers.h'
'/tensorflow/tensorflow/stream_executor/cuda/cuda_platform_id.h'
'/tensorflow/tensorflow/stream_executor/cuda/cuda_stream.h'.

____Building complete.
Target //tensorflow/tools/pip_package:build_pip_package failed to build

____Elapsed time: 998.708s, Critical Path: 715.53s`
```
"
1026,crosstool_wrapper_driver_is_not_gcc failed: error executing command,"I'm trying to build the master branch, configured with cuda compute capability 3.0,  using the following command:

bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer --verbose_failures --genrule_strategy=standalone --spawn_strategy=standalone

On Ubuntu 12.04 using bazel 0.1.4 and gcc 4.8.1 I get the following error:

ERROR: /opt/tensorflow/tensorflow/core/BUILD:416:1: C++ compilation of rule '//tensorflow/core:kernel_lib' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/#####/.cache/bazel/_bazel_#####/fbc06f9baef46cade6e35d9e4137e37c/tensorflow && \
  exec env - \
    PATH=/usr/local/cuda/bin:/opt/ros/fuerte/bin:/opt/lfd/scripts:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/opt/gurobi650/linux64/bin:/home/#####/bin \
  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -iquote external/jpeg_archive -iquote bazel-out/local_linux-opt/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/local_linux-opt/genfiles/external/png_archive -iquote external/re2 -iquote bazel-out/local_linux-opt/genfiles/external/re2 -iquote external/eigen_archive -iquote bazel-out/local_linux-opt/genfiles/external/eigen_archive -isystem google/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/google/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local_linux-opt/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-fa22401ededc -isystem bazel-out/local_linux-opt/genfiles/external/eigen_archive/eigen-eigen-fa22401ededc -isystem third_party/gpus/cuda -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda -isystem third_party/gpus/cuda/include -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda/include -fno-exceptions -DEIGEN_AVOID_STL_ARRAY '-DGOOGLE_CUDA=1' -pthread '-DGOOGLE_CUDA=1' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fno-canonical-system-headers '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernel_lib/tensorflow/core/kernels/conv_grad_ops.o' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernel_lib/tensorflow/core/kernels/conv_grad_ops.d -c tensorflow/core/kernels/conv_grad_ops.cc -o bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernel_lib/tensorflow/core/kernels/conv_grad_ops.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/#####/.cache/bazel/_bazel_#####/fbc06f9baef46cade6e35d9e4137e37c/tensorflow && \
  exec env - \
    PATH=/usr/local/cuda/bin:/opt/ros/fuerte/bin:/opt/lfd/scripts:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/opt/gurobi650/linux64/bin:/home/#####/bin \
  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -iquote external/jpeg_archive -iquote bazel-out/local_linux-opt/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/local_linux-opt/genfiles/external/png_archive -iquote external/re2 -iquote bazel-out/local_linux-opt/genfiles/external/re2 -iquote external/eigen_archive -iquote bazel-out/local_linux-opt/genfiles/external/eigen_archive -isystem google/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/google/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local_linux-opt/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-fa22401ededc -isystem bazel-out/local_linux-opt/genfiles/external/eigen_archive/eigen-eigen-fa22401ededc -isystem third_party/gpus/cuda -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda -isystem third_party/gpus/cuda/include -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda/include -fno-exceptions -DEIGEN_AVOID_STL_ARRAY '-DGOOGLE_CUDA=1' -pthread '-DGOOGLE_CUDA=1' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fno-canonical-system-headers '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernel_lib/tensorflow/core/kernels/conv_grad_ops.o' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernel_lib/tensorflow/core/kernels/conv_grad_ops.d -c tensorflow/core/kernels/conv_grad_ops.cc -o bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernel_lib/tensorflow/core/kernels/conv_grad_ops.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
Target //tensorflow/cc:tutorials_example_trainer failed to build
"
1021,pre-trained inception v3 model should accept batches as well as singletons,"At present, the inception v3 model seems to only accept size 1 for the batch size.
"
1019,Tensorflow Android not building for non-arm architectures.,"I am trying to build the Tensorflow Android Camera Demo for x86 (or x86_64, or mips) architecture but bazel seems to be ignoring the --android_cpu flag and always builds for armeabi-v7a. 

I have also tried using the --fat_apk_cpu flag but I am getting compilation error about the: unrecognized command line option -mfpu=neon.
"
1016,Incorrect RNN documentation,"In the `tensorflow.python.ops.rnn.rnn` docs:

>   Dynamic calculation returns, at time t:
>     (t >= max(sequence_length)
>         ? (zeros(output_shape), zeros(state_shape))
>         : cell(input, state)

But if we look at `_dynamic_rnn_step`, what seems more reasonable is happening: the old state gets passed through unaltered if `t >= max(sequence_length)`.
"
1015,Error when feeding model,"Hi!

I have downloaded and tried the [RNN tutorial](https://www.tensorflow.org/versions/0.6.0/tutorials/recurrent/index.html), it works great and I want to mod it to use it for another project.

I have managed to write my own ""reader"" which uses my desired data and training the model seems to work, somewhat, I think. I feed my model the following dict:

```
feed_dict = {m.input_data: x, m.targets: y, m.initial_state: state}
cost, state, _ = session.run([m.cost, m.final_state, eval_op], feed_dict=feed_dict)
```

where `x` and `y` are the outputs from the `reader.iterator` method (the one I wrote, but nearly identical to the ptb_iterator used in the tutorial).

I have also created a ""runner"" module which only imports a already trained model and runs it. But I am unable to run this code. I import a saved checkpoint into my session using the saver.restore method and that seems to work. But for the ""runner"" code I changed the `feed_dict` to be:
`feed_dict = {m.input_data: input_id_list, m.initial_state: state}`
since I do not have an output this time, this is what I want the network to come up with.
I also have this in the session.run:
`session.run([m.cost, m.targets, eval_op], feed_dict=feed_dict)`
So this time (in contrast to the ""trainer"" code) I want to have the output of `m.targets`.

But when I try to run the ""runner"" code, I get this output:
[log.txt](https://github.com/tensorflow/tensorflow/files/121384/log.txt)

I have checked so the shape of `input_id_list` is (1, 4). That is also what is printed on the first line in the log file. And from the error message I can see that the Tensorflow placeholder is of shape (1,4) as well. All this makes sense since my config is set to `batch_size=1` and `num_steps=4`.

It also mentions that the input is of wrong type, not a int32, but it is the same input as when I run the ""trainer"" version of the code, so why would it work then but not now?

Thank you for all help and feedback! Let me know if I should submit more code/logs/etc!
"
1013,Cannot import tensorflow in python,"My env is like this:

python 2.7.10
I successfully installed tensorflow, using pip install 
https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl

then I goes into `python`, 
then `import tensorflow as tf`, 
I got this error:

```
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Or\AppData\Local\Enthought\Canopy\User\lib\site-packages\tensorflow\__init__.py"", line 4, in <module>
    from tensorflow.python import *
  File ""C:\Users\Or\AppData\Local\Enthought\Canopy\User\lib\site-packages\tensorflow\python\__init__.py"", line 22, in <module>
    from tensorflow.python.client.client_lib import *
  File ""C:\Users\Or\AppData\Local\Enthought\Canopy\User\lib\site-packages\tensorflow\python\client\client_lib.py"", line 35, in <module>
    from tensorflow.python.client.session import InteractiveSession
  File ""C:\Users\Or\AppData\Local\Enthought\Canopy\User\lib\site-packages\tensorflow\python\client\session.py"", line 11, in <module>
    from tensorflow.python import pywrap_tensorflow as tf_session
  File ""C:\Users\Or\AppData\Local\Enthought\Canopy\User\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\Or\AppData\Local\Enthought\Canopy\User\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow
ImportError: No module named _pywrap_tensorflow
```

would be greatful, thanks!
"
1012,pip installation failing on fedora,"OS info:

> uname -r
> 4.3.4-300.fc23.x86_64
> cat /etc/redhat-release 
> Fedora release 23 (Twenty Three)

Python info:
python3-numpy-1.9.2-2.fc23.x86_64
python3-pip-7.1.0-1.fc23.noarch
python-pip-7.1.0-1.fc23.noarch
numpy-1.9.2-2.fc23.x86_64
python-2.7.10-8.fc23.x86_64
python3-3.4.3-5.fc23.x86_64

Error:
kmulvey@deepthought: ~ (master)> sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl
Collecting tensorflow==0.5.0 from https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl
  Using cached https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl
Requirement already up-to-date: six>=1.10.0 in /usr/lib/python2.7/site-packages (from tensorflow==0.5.0)
Collecting numpy>=1.9.2 (from tensorflow==0.5.0)
  Using cached numpy-1.10.4.tar.gz
Installing collected packages: numpy, tensorflow
  Found existing installation: numpy 1.9.2
    DEPRECATION: Uninstalling a distutils installed project (numpy) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.
    Uninstalling numpy-1.9.2:
      Successfully uninstalled numpy-1.9.2
  Running setup.py install for numpy ... error
    Complete output from command /usr/bin/python -u -c ""import setuptools, tokenize;**file**='/tmp/pip-build-MVpgkh/numpy/setup.py';exec(compile(getattr(tokenize, 'open', open)(**file**).read().replace('\r\n', '\n'), **file**, 'exec'))"" install --record /tmp/pip-Tz7Dc0-record/install-record.txt --single-version-externally-managed --compile:
    Running from numpy source directory.
    blas_opt_info:
    blas_mkl_info:
      libraries mkl,vml,guide not found in ['/usr/local/lib64', '/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/']
      NOT AVAILABLE

```
openblas_info:
  libraries openblas not found in ['/usr/local/lib64', '/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/']
  NOT AVAILABLE

atlas_3_10_blas_threads_info:
Setting PTATLAS=ATLAS
  libraries tatlas not found in ['/usr/local/lib64', '/usr/local/lib', '/usr/lib64/atlas', '/usr/lib64/sse2', '/usr/lib64', '/usr/lib/sse2', '/usr/lib', '/usr/lib/sse2', '/usr/lib/']
  NOT AVAILABLE

atlas_3_10_blas_info:
  libraries satlas not found in ['/usr/local/lib64', '/usr/local/lib', '/usr/lib64/atlas', '/usr/lib64/sse2', '/usr/lib64', '/usr/lib/sse2', '/usr/lib', '/usr/lib/sse2', '/usr/lib/']
  NOT AVAILABLE

atlas_blas_threads_info:
Setting PTATLAS=ATLAS
  libraries ptf77blas,ptcblas,atlas not found in ['/usr/local/lib64', '/usr/local/lib', '/usr/lib64/atlas', '/usr/lib64/sse2', '/usr/lib64', '/usr/lib/sse2', '/usr/lib', '/usr/lib/sse2', '/usr/lib/']
  NOT AVAILABLE

atlas_blas_info:
  libraries f77blas,cblas,atlas not found in ['/usr/local/lib64', '/usr/local/lib', '/usr/lib64/atlas', '/usr/lib64/sse2', '/usr/lib64', '/usr/lib/sse2', '/usr/lib', '/usr/lib/sse2', '/usr/lib/']
  NOT AVAILABLE

/tmp/pip-build-MVpgkh/numpy/numpy/distutils/system_info.py:1651: UserWarning:
    Atlas (http://math-atlas.sourceforge.net/) libraries not found.
    Directories to search for the libraries can be specified in the
    numpy/distutils/site.cfg file (section [atlas]) or by setting
    the ATLAS environment variable.
  warnings.warn(AtlasNotFoundError.__doc__)
blas_info:
C compiler: cc

creating /tmp/tmpQWMe8s/tmp
creating /tmp/tmpQWMe8s/tmp/tmpQWMe8s
compile options: '-I/usr/local/include -I/usr/include -c'
cc: /tmp/tmpQWMe8s/source.c
/tmp/tmpQWMe8s/source.c:1:19: fatal error: cblas.h: No such file or directory
compilation terminated.
/tmp/tmpQWMe8s/source.c:1:19: fatal error: cblas.h: No such file or directory
compilation terminated.
/tmp/pip-build-MVpgkh/numpy/numpy/distutils/system_info.py:635: UserWarning: Specified path  is invalid.
  warnings.warn('Specified path %s is invalid.' % d)
  FOUND:
    libraries = ['blas']
    library_dirs = ['/usr/lib64']

  FOUND:
    libraries = ['blas']
    library_dirs = ['/usr/lib64']
    define_macros = [('NO_ATLAS_INFO', 1)]

/bin/sh: svnversion: command not found
non-existing path in 'numpy/distutils': 'site.cfg'
/bin/sh: svnversion: command not found
F2PY Version 2
lapack_opt_info:
openblas_lapack_info:
  libraries openblas not found in ['/usr/local/lib64', '/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/']
  NOT AVAILABLE

lapack_mkl_info:
mkl_info:
  libraries mkl,vml,guide not found in ['/usr/local/lib64', '/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/']
  NOT AVAILABLE

  NOT AVAILABLE

atlas_3_10_threads_info:
Setting PTATLAS=ATLAS
  libraries tatlas,tatlas not found in /usr/local/lib64
  libraries lapack_atlas not found in /usr/local/lib64
  libraries tatlas,tatlas not found in /usr/local/lib
  libraries lapack_atlas not found in /usr/local/lib
  libraries tatlas,tatlas not found in /usr/lib64/atlas
  libraries lapack_atlas not found in /usr/lib64/atlas
  libraries tatlas,tatlas not found in /usr/lib64/sse2
  libraries lapack_atlas not found in /usr/lib64/sse2
  libraries tatlas,tatlas not found in /usr/lib64
  libraries lapack_atlas not found in /usr/lib64
  libraries tatlas,tatlas not found in /usr/lib/sse2
  libraries lapack_atlas not found in /usr/lib/sse2
  libraries tatlas,tatlas not found in /usr/lib
  libraries lapack_atlas not found in /usr/lib
  libraries tatlas,tatlas not found in /usr/lib/sse2
  libraries lapack_atlas not found in /usr/lib/sse2
  libraries tatlas,tatlas not found in /usr/lib/
  libraries lapack_atlas not found in /usr/lib/
<class 'numpy.distutils.system_info.atlas_3_10_threads_info'>
  NOT AVAILABLE

atlas_3_10_info:
  libraries satlas,satlas not found in /usr/local/lib64
  libraries lapack_atlas not found in /usr/local/lib64
  libraries satlas,satlas not found in /usr/local/lib
  libraries lapack_atlas not found in /usr/local/lib
  libraries satlas,satlas not found in /usr/lib64/atlas
  libraries lapack_atlas not found in /usr/lib64/atlas
  libraries satlas,satlas not found in /usr/lib64/sse2
  libraries lapack_atlas not found in /usr/lib64/sse2
  libraries satlas,satlas not found in /usr/lib64
  libraries lapack_atlas not found in /usr/lib64
  libraries satlas,satlas not found in /usr/lib/sse2
  libraries lapack_atlas not found in /usr/lib/sse2
  libraries satlas,satlas not found in /usr/lib
  libraries lapack_atlas not found in /usr/lib
  libraries satlas,satlas not found in /usr/lib/sse2
  libraries lapack_atlas not found in /usr/lib/sse2
  libraries satlas,satlas not found in /usr/lib/
  libraries lapack_atlas not found in /usr/lib/
<class 'numpy.distutils.system_info.atlas_3_10_info'>
  NOT AVAILABLE

atlas_threads_info:
Setting PTATLAS=ATLAS
  libraries ptf77blas,ptcblas,atlas not found in /usr/local/lib64
  libraries lapack_atlas not found in /usr/local/lib64
  libraries ptf77blas,ptcblas,atlas not found in /usr/local/lib
  libraries lapack_atlas not found in /usr/local/lib
  libraries ptf77blas,ptcblas,atlas not found in /usr/lib64/atlas
  libraries lapack_atlas not found in /usr/lib64/atlas
  libraries ptf77blas,ptcblas,atlas not found in /usr/lib64/sse2
  libraries lapack_atlas not found in /usr/lib64/sse2
  libraries ptf77blas,ptcblas,atlas not found in /usr/lib64
  libraries lapack_atlas not found in /usr/lib64
  libraries ptf77blas,ptcblas,atlas not found in /usr/lib/sse2
  libraries lapack_atlas not found in /usr/lib/sse2
  libraries ptf77blas,ptcblas,atlas not found in /usr/lib
  libraries lapack_atlas not found in /usr/lib
  libraries ptf77blas,ptcblas,atlas not found in /usr/lib/sse2
  libraries lapack_atlas not found in /usr/lib/sse2
  libraries ptf77blas,ptcblas,atlas not found in /usr/lib/
  libraries lapack_atlas not found in /usr/lib/
<class 'numpy.distutils.system_info.atlas_threads_info'>
  NOT AVAILABLE

atlas_info:
  libraries f77blas,cblas,atlas not found in /usr/local/lib64
  libraries lapack_atlas not found in /usr/local/lib64
  libraries f77blas,cblas,atlas not found in /usr/local/lib
  libraries lapack_atlas not found in /usr/local/lib
  libraries f77blas,cblas,atlas not found in /usr/lib64/atlas
  libraries lapack_atlas not found in /usr/lib64/atlas
  libraries f77blas,cblas,atlas not found in /usr/lib64/sse2
  libraries lapack_atlas not found in /usr/lib64/sse2
  libraries f77blas,cblas,atlas not found in /usr/lib64
  libraries lapack_atlas not found in /usr/lib64
  libraries f77blas,cblas,atlas not found in /usr/lib/sse2
  libraries lapack_atlas not found in /usr/lib/sse2
  libraries f77blas,cblas,atlas not found in /usr/lib
  libraries lapack_atlas not found in /usr/lib
  libraries f77blas,cblas,atlas not found in /usr/lib/sse2
  libraries lapack_atlas not found in /usr/lib/sse2
  libraries f77blas,cblas,atlas not found in /usr/lib/
  libraries lapack_atlas not found in /usr/lib/
<class 'numpy.distutils.system_info.atlas_info'>
  NOT AVAILABLE

/tmp/pip-build-MVpgkh/numpy/numpy/distutils/system_info.py:1552: UserWarning:
    Atlas (http://math-atlas.sourceforge.net/) libraries not found.
    Directories to search for the libraries can be specified in the
    numpy/distutils/site.cfg file (section [atlas]) or by setting
    the ATLAS environment variable.
  warnings.warn(AtlasNotFoundError.__doc__)
lapack_info:
  FOUND:
    libraries = ['lapack']
    library_dirs = ['/usr/lib64']
    language = f77

  FOUND:
    libraries = ['lapack', 'blas']
    library_dirs = ['/usr/lib64']
    define_macros = [('NO_ATLAS_INFO', 1)]
    language = f77

/usr/lib64/python2.7/distutils/dist.py:267: UserWarning: Unknown distribution option: 'define_macros'
  warnings.warn(msg)
running install
running build
running config_cc
unifing config_cc, config, build_clib, build_ext, build commands --compiler options
running config_fc
unifing config_fc, config, build_clib, build_ext, build commands --fcompiler options
running build_src
build_src
building py_modules sources
creating build
creating build/src.linux-x86_64-2.7
creating build/src.linux-x86_64-2.7/numpy
creating build/src.linux-x86_64-2.7/numpy/distutils
building library ""npymath"" sources
customize Gnu95FCompiler
Found executable /usr/bin/gfortran
customize Gnu95FCompiler
customize Gnu95FCompiler using config
C compiler: gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -specs=/usr/lib/rpm/redhat/redhat-hardened-cc1 -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -specs=/usr/lib/rpm/redhat/redhat-hardened-cc1 -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC

compile options: '-Inumpy/core/src/private -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/include/python2.7 -c'
gcc: _configtest.c
gcc: error: /usr/lib/rpm/redhat/redhat-hardened-cc1: No such file or directory
gcc: error: /usr/lib/rpm/redhat/redhat-hardened-cc1: No such file or directory
failure.
removing: _configtest.c _configtest.o
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/tmp/pip-build-MVpgkh/numpy/setup.py"", line 263, in <module>
    setup_package()
  File ""/tmp/pip-build-MVpgkh/numpy/setup.py"", line 255, in setup_package
    setup(**metadata)
  File ""/tmp/pip-build-MVpgkh/numpy/numpy/distutils/core.py"", line 169, in setup
    return old_setup(**new_attr)
  File ""/usr/lib64/python2.7/distutils/core.py"", line 151, in setup
    dist.run_commands()
  File ""/usr/lib64/python2.7/distutils/dist.py"", line 953, in run_commands
    self.run_command(cmd)
  File ""/usr/lib64/python2.7/distutils/dist.py"", line 972, in run_command
    cmd_obj.run()
  File ""/tmp/pip-build-MVpgkh/numpy/numpy/distutils/command/install.py"", line 62, in run
    r = self.setuptools_run()
  File ""/tmp/pip-build-MVpgkh/numpy/numpy/distutils/command/install.py"", line 36, in setuptools_run
    return distutils_install.run(self)
  File ""/usr/lib64/python2.7/distutils/command/install.py"", line 563, in run
    self.run_command('build')
  File ""/usr/lib64/python2.7/distutils/cmd.py"", line 326, in run_command
    self.distribution.run_command(command)
  File ""/usr/lib64/python2.7/distutils/dist.py"", line 972, in run_command
    cmd_obj.run()
  File ""/tmp/pip-build-MVpgkh/numpy/numpy/distutils/command/build.py"", line 47, in run
    old_build.run(self)
  File ""/usr/lib64/python2.7/distutils/command/build.py"", line 127, in run
    self.run_command(cmd_name)
  File ""/usr/lib64/python2.7/distutils/cmd.py"", line 326, in run_command
    self.distribution.run_command(command)
  File ""/usr/lib64/python2.7/distutils/dist.py"", line 972, in run_command
    cmd_obj.run()
  File ""/tmp/pip-build-MVpgkh/numpy/numpy/distutils/command/build_src.py"", line 153, in run
    self.build_sources()
  File ""/tmp/pip-build-MVpgkh/numpy/numpy/distutils/command/build_src.py"", line 164, in build_sources
    self.build_library_sources(*libname_info)
  File ""/tmp/pip-build-MVpgkh/numpy/numpy/distutils/command/build_src.py"", line 299, in build_library_sources
    sources = self.generate_sources(sources, (lib_name, build_info))
  File ""/tmp/pip-build-MVpgkh/numpy/numpy/distutils/command/build_src.py"", line 386, in generate_sources
    source = func(extension, build_dir)
  File ""numpy/core/setup.py"", line 669, in get_mathlib_info
    raise RuntimeError(""Broken toolchain: cannot link a simple C program"")
RuntimeError: Broken toolchain: cannot link a simple C program

----------------------------------------
```

  Rolling back uninstall of numpy
Command ""/usr/bin/python -u -c ""import setuptools, tokenize;**file**='/tmp/pip-build-MVpgkh/numpy/setup.py';exec(compile(getattr(tokenize, 'open', open)(**file**).read().replace('\r\n', '\n'), **file**, 'exec'))"" install --record /tmp/pip-Tz7Dc0-record/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /tmp/pip-build-MVpgkh/numpy
"
1001,tf.image.resize_images() does not work with computed height and width,"The wrapper function requires the new height and width arguments to be integers, for build-time checking, even though the underlying ops support Tensor-valued dimensions.
"
1000,tensorboard script fails due to failing import,"It seems commit 8a59748c087a2fee535c0d5067dbabb01920e812 has done this move:  tensorflow/tensorboard/{ → backend}/tensorboard.py but the import in the tensorboard script that gets installed by pip uses the old location (I think that's defined by https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/setup.py#L46 ). Thus, invoking tensorboard fails with a failing import, but if you insert ""backend."" into the script at the appropriate location it works again.
"
999,"ValueError when using tf.nn.moments with axes=[1,2,3]","mean, var = tf.nn.moments(images, [1, 2, 3])

```
/home/cesar/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/nn.pyc in moments(x, axes, name)
    540     # The caller should have a fallback plan, however: this tensor may not be
    541     # available if this function implementation changes.
--> 542     x_centered = math_ops.sub(x, mean, name=""x_centered"")
    543     var = math_ops.mul(math_ops.reduce_sum(math_ops.square(x_centered), axes),
    544                        divisor, name=""variance"")

/home/cesar/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.pyc in sub(x, y, name)
   1398     A `Tensor`. Has the same type as `x`.
   1399   """"""
-> 1400   return _op_def_lib.apply_op(""Sub"", x=x, y=y, name=name)
   1401 
   1402 

/home/cesar/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.pyc in apply_op(self, op_type_name, name, **keywords)
    653         op = g.create_op(op_type_name, inputs, output_types, name=scope,
    654                          input_types=input_types, attrs=attr_protos,
--> 655                          op_def=op_def)
    656         outputs = op.outputs
    657         return _Restructure(ops.convert_n_to_tensor(outputs), output_structure)

/home/cesar/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes)
   2046                     original_op=self._default_original_op, op_def=op_def)
   2047     if compute_shapes:
-> 2048       set_shapes_for_outputs(ret)
   2049     self._add_op(ret)
   2050     self._record_op_seen_by_control_dependencies(ret)

/home/cesar/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in set_shapes_for_outputs(op)
   1526       raise RuntimeError(""No shape function registered for standard op: %s""
   1527                          % op.type)
-> 1528   shapes = shape_func(op)
   1529   if len(op.outputs) != len(shapes):
   1530     raise RuntimeError(

/home/cesar/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.pyc in _BroadcastShape(op)
   1279     else:
   1280       raise ValueError(""Incompatible shapes for broadcasting: %s and %s""
-> 1281                        % (shape_x, shape_y))
   1282   return [tensor_shape.TensorShape(return_dims)]
   1283 

ValueError: Incompatible shapes for broadcasting: (128, 32, 32, 3) and (128,)
```
"
994,Classification model for Imagenet classification tutorial,"Hi!

I'm an engineer looking to work with Tensorflow for computer vision applications. My team was looking into the Imagenet classification tutorial provided via the Tensorflow website and the github files at /models/image/imagenet and have become a little stumped. There's no information on how the classification model was built (like their was for the MNIST tutorials), and there is no information on the graph that has been used for in the classify_image.py file, aside from a convoluted binary .pb file.

Is there any more information on how to define and train the Inception model (or any other classifier) with the Imagenet training set? Or how to create the graph? Or, has this information been made unavailable to the public?

Thanks

Oren 

ps: sorry for writing this in an issue—was having trouble trying to find a better place to put it.
"
993,How to run nvidia-docker with TensorFlow GPU docker,"Thanks for looking at my issue, really appreciate it. 
### What I've Done

I have setup an equivalent of a Nvidia DIGITS machine (running Ubuntu 14.04 server), and am attempting to run everything in docker containers.
1. I have docker installed, and have run `nvidia-docker run nvidia/cuda nvidia-smi` described [here](https://github.com/NVIDIA/nvidia-docker), and I see my 4 TitanX graphic cards. 
2. I have also run the nvidia-docker-plugin described [here](https://github.com/NVIDIA/nvidia-docker/wiki/Using-nvidia-docker-plugin) as `sudo -u nvidia-docker nvidia-docker-plugin -s /var/lib/nvidia-docker` and I get the output:

```
nvidia-docker-plugin | 2016/02/04 12:54:02 Loading NVIDIA management library
nvidia-docker-plugin | 2016/02/04 12:54:04 Loading NVIDIA unified memory
nvidia-docker-plugin | 2016/02/04 12:54:04 Discovering GPU devices
nvidia-docker-plugin | 2016/02/04 12:54:05 Provisioning volumes at /var/lib/nvidia-docker/volumes
nvidia-docker-plugin | 2016/02/04 12:54:05 Serving plugin API at /var/lib/nvidia-docker
nvidia-docker-plugin | 2016/02/04 12:54:05 Serving remote API at localhost:3476
```

which signifies to me that it's working.
1. I ran the tests [here](https://github.com/NVIDIA/nvidia-docker/wiki/Testing-the-samples) and they all passed. 
### My Problem
1. When I try to run the [TensorFlow GPU docker image](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/docker) using nvidia-docker

I first run `sudo -u nvidia-docker nvidia-docker-plugin -s /var/lib/nvidia-docker` in a tmux session. 

Then I run `nvidia-docker run -it -p 8888:8888 b.gcr.io/tensorflow/tensorflow-devel-gpu` it downloads everything and runs the docker container. Next I run ipython and try to import tensorflow but I get the following errors:

```
In [1]: import tensorflow as tf
I tensorflow/stream_executor/dso_loader.cc:92] LD_LIBRARY_PATH: /usr/local/cuda/lib64
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:121] hostname: 16b84b6e71f9
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:146] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:257] driver version file contents: """"""NVRM version: NVIDIA UNIX x86_64 Kernel Module  352.79  Wed Jan 13 16:17:53 PST 2016
GCC version:  gcc version 4.8.4 (Ubuntu 4.8.4-2ubuntu1~14.04)
""""""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:150] kernel reported version is: 352.79
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1054] LD_LIBRARY_PATH: /usr/local/cuda/lib64
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1055] failed to find libcuda.so on this system: Failed precondition: could not dlopen DSO: libcuda.so; dlerror: libcuda.so: cannot open shared object file: No such file or directory
```

**I think I just have a lack in understanding about how I should run the TensorFlow container, or maybe I have to build the container using nvidia-docker. 

Any ideas about how to do this, or general advice about what I'm doing wrong would be amazing. **

Thanks so much.

Brad
"
990,GPU device not found on Ubuntu 14.04,"I have installed tensorflow python3 version using virtual environment on Ubuntu 14.04. There are three GPUs (Tesla K20c) available on the server that i work on.

On the tensorflow webpage it says: ""In general you do not have to specify CPUs or GPUs explicitly. TensorFlow uses your first GPU, if you have one, for as many operations as possible.""

When i checked the examples provided on Using GPUs about using multi-GPUs 

import tensorflow as tf
# Creates a graph.

c = []
for d in ['/gpu:1', '/gpu:2']:
  with tf.device(d):
    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3])
    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2])
    c.append(tf.matmul(a, b))
with tf.device('/cpu:0'):
  sum = tf.add_n(c)
# Creates a session with log_device_placement set to True.

sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
# Runs the op.

print(sess.run(sum))

the following error was displayed:

I tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library                                    libcublas.so.7.0 locally
I tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library                                    libcudnn.so.6.5 locally
I tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library                                    libcufft.so.7.0 locally
I tensorflow/stream_executor/dso_loader.cc:93] Couldn't open CUDA library libcud                                   a.so. LD_LIBRARY_PATH: :/usr/local/cuda/lib64
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:121] hostname: dmlserver
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:146] libcuda reported vers                                   ion is: Not found: was unable to find libcuda.so DSO loaded into this program
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:257] driver version file c                                   ontents: """"""NVRM version: NVIDIA UNIX x86_64 Kernel Module  346.96  Sun Aug 23 2                                   2:29:21 PDT 2015
GCC version:  gcc version 4.8.4 (Ubuntu 4.8.4-2ubuntu1~14.04)
""""""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:150] kernel reported versi                                   on is: 346.96
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1060] LD_LIBRARY_PATH: :/                                   usr/local/cuda/lib64
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1061] failed to find libc                                   uda.so on this system: Failed precondition: could not dlopen DSO: libcuda.so; dl                                   error: libcuda.so: cannot open shared object file: No such file or directory
I tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library                                    libcurand.so.7.0 locally
I tensorflow/core/common_runtime/local_device.cc:40] Local device intra op paral                                   lelism threads: 16
E tensorflow/stream_executor/cuda/cuda_driver.cc:481] failed call to cuInit: CUD                                   A_ERROR_NO_DEVICE
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:114] retrieving CUDA diagn                                   ostic information for host: dmlserver
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:121] hostname: dmlserver
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:146] libcuda reported vers                                   ion is: Not found: was unable to find libcuda.so DSO loaded into this program
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:257] driver version file c                                   ontents: """"""NVRM version: NVIDIA UNIX x86_64 Kernel Module  346.96  Sun Aug 23 2                                   2:29:21 PDT 2015
GCC version:  gcc version 4.8.4 (Ubuntu 4.8.4-2ubuntu1~14.04)
""""""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:150] kernel reported versi                                   on is: 346.96
I tensorflow/core/common_runtime/gpu/gpu_init.cc:127] DMA:
I tensorflow/core/common_runtime/direct_session.cc:58] Direct session inter op p                                   arallelism threads: 16
Device mapping: no known devices.
I tensorflow/core/common_runtime/direct_session.cc:134] Device mapping:

Traceback (most recent call last):
  File ""/usr/lib/python3.4/site-packages/tensorflow/python/client/session.py"", l                                   ine 428, in _do_run
    target_list)
tensorflow.python.pywrap_tensorflow.StatusNotOK: Invalid argument: Cannot assign                                    a device to node 'Const_3': Could not satisfy explicit device specification '/g                                   pu:2'
         [[Node: Const_3 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape:                                    [3,2] values: 1 2 3...>, _device=""/gpu:2""]()]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""checking_2.py"", line 15, in <module>
    print(sess.run(sum))
  File ""/usr/lib/python3.4/site-packages/tensorflow/python/client/session.py"", l                                   ine 368, in run
    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)
  File ""/usr/lib/python3.4/site-packages/tensorflow/python/client/session.py"", l                                   ine 444, in _do_run
    e.code)
tensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device                                    to node 'Const_3': Could not satisfy explicit device specification '/gpu:2'
         [[Node: Const_3 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape:                                    [3,2] values: 1 2 3...>, _device=""/gpu:2""]()]]
Caused by op 'Const_3', defined at:
  File ""checking_2.py"", line 8, in <module>
    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2])
  File ""/usr/lib/python3.4/site-packages/tensorflow/python/ops/constant_op.py"",                                    line 165, in constant
    attrs={""value"": tensor_value, ""dtype"": dtype_value}, name=name).outputs[0]
  File ""/usr/lib/python3.4/site-packages/tensorflow/python/framework/ops.py"", li                                   ne 1834, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/lib/python3.4/site-packages/tensorflow/python/framework/ops.py"", li                                   ne 1043, in __init__
    self._traceback = _extract_stack()

Don't understand what the problem is? Need URGENT help!!! 
"
988,"If download fails, the file downloaded should be deleted",
985,Wrong file dimension when calling maybe_download function,"Inside 1_notmnist.ipynb file maybe_download function is called twice and fails twice because of a wrong file dimension. For notMNIST_large.tar.gz file right dimension should be 74250249, and for notMNIST_small.tar.gz file  right dimension should be 6381568. PS: working on mac
"
977,Word2Vec.py ReluGrad is not finite,"Running the tutorial as downloaded from tensorflow.org results in an exception stating that the ReluGrad is not finite.  This happens with the learning rate set to .00001 and concurrent steps set to 1.  Once this exception is thrown, learning stops.  It happens at epoch 0, roughly step 500,000.
"
976,parse_single_example() should give dense array for VarLenFeature,"On master, the signature of parse_single_example has changed. When testing I noticed something unexpected; a tf.VarLenFeature results in a SparseTensor instead of a dense one.

```
features = tf.parse_single_example(
  serialized_example,
  features = {
    'data': tf.VarLenFeature(tf.float32),
  }
)
```

`features['data']` is now a SparseTensor. I don't see really why, shouldn't this be a (standard dense) Tensor instead?
"
975,the time about gpu,"I have installed the gpu version tf. I'm running a cnn with input size 256_256 and epoch size 32_32, but the speed is very slow,  gpu is 980ti

```
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.00GiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.00GiB
I tensorflow/core/common_runtime/direct_session.cc:45] Direct session inter op parallelism threads: 8
2016-02-03 18:13:09.461207: step 0, loss = 250.84 (0.2 examples/sec; 144.089 sec/batch)
2016-02-03 18:13:59.022429: step 10, loss = 248.88 (7.7 examples/sec; 4.167 sec/batch)
2016-02-03 18:14:40.752574: step 20, loss = 246.82 (7.5 examples/sec; 4.274 sec/batch)
2016-02-03 18:15:24.129283: step 30, loss = 244.89 (7.5 examples/sec; 4.277 sec/batch)
```

the first time running the speed is fast, but after this the speed is slow. What's the reason?
How could I identify the program is running on a gpu or cpu?
"
971,Expose TensorFlow build details,"Requesting addition of API method(s) to enable programmatic checking of TensorFlow version and other build capabilities (such as if TF was compiled with GPU support, for eg.). I would find this useful in deployment scripts that operate in foreign TensorFlow environments.
"
970,Work nvidia-docker rather than using custom docker start script,"Right now the docs recommend using a [custom start command](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/docker#running-the-container) ([or here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#docker-installation)) rather than using the [`nvidia-docker` program published by nvidia](https://github.com/NVIDIA/nvidia-docker). I suggest having the image work with the more standardized approach to starting docker containers with nvidia support.

See: https://github.com/tensorflow/tensorflow/issues/808#issuecomment-175934093.
"
968,"tensorflow.bzl.bzl doesn't exist, unsurprisingly","I get a pile of build errors after pulling.  One of them is that `tensorflow.bzl.bzl` doesn't exist.  Did someone rewrite the build rules recently?

```
bazel build ...
ERROR: /usr/local/google/home/geoffreyi/tensorflow/tensorflow/cc/BUILD:11:6: First argument of load() is a path, not a label. It should start with a single slash if it is an absolute path..
ERROR: /usr/local/google/home/geoffreyi/tensorflow/tensorflow/cc/BUILD:12:6: First argument of load() is a path, not a label. It should start with a single slash if it is an absolute path..
ERROR: /usr/local/google/home/geoffreyi/tensorflow/tensorflow/cc/BUILD:11:6: file '/tensorflow:tensorflow.bzl.bzl' was not correctly loaded. Make sure the 'load' statement appears in the global scope in your file.
ERROR: /usr/local/google/home/geoffreyi/tensorflow/tensorflow/cc/BUILD:12:6: file '/tensorflow:tensorflow.bzl.bzl' was not correctly loaded. Make sure the 'load' statement appears in the global scope in your file.
ERROR: /usr/local/google/home/geoffreyi/tensorflow/tensorflow/cc/BUILD:21:13: Traceback (most recent call last):
  File ""/usr/local/google/home/geoffreyi/tensorflow/tensorflow/cc/BUILD"", line 14
    cc_library(name = ""cc_op_gen_main"", srcs = [""...""], <3 more arguments>)
  File ""/usr/local/google/home/geoffreyi/tensorflow/tensorflow/cc/BUILD"", line 21, in cc_library
    tf_copts
name 'tf_copts' is not defined.
ERROR: /usr/local/google/home/geoffreyi/tensorflow/tensorflow/cc/BUILD:28:1: name 'tf_gen_op_wrappers_cc' is not defined.
ERROR: /usr/local/google/home/geoffreyi/tensorflow/tensorflow/cc/BUILD:66:13: Traceback (most recent call last):
  File ""/usr/local/google/home/geoffreyi/tensorflow/tensorflow/cc/BUILD"", line 63
    cc_binary(name = ""tutorials_example_traine..."", <4 more arguments>)
  File ""/usr/local/google/home/geoffreyi/tensorflow/tensorflow/cc/BUILD"", line 66, in cc_binary
    tf_copts
name 'tf_copts' is not defined.
... (many more)
```
"
965,Test failure on Mac: //tensorflow/core:common_runtime_gpu_gpu_allocator_retry_test,"One of the C++ unit test three method in gpu_allocator_retry_test.cc failed: 
**//tensorflow/core:common_runtime_gpu_gpu_allocator_retry_test**

See below for detailed error log:

```
INFO: From Testing //tensorflow/core:common_runtime_gpu_gpu_allocator_retry_test:
==================== Test output for //tensorflow/core:common_runtime_gpu_gpu_allocator_retry_test:
Running main() from test_main.cc
[==========] Running 3 tests from 1 test case.
[----------] Global test environment set-up.
[----------] 3 tests from GPUAllocatorRetryTest
[ RUN      ] GPUAllocatorRetryTest.RetrySuccess
I tensorflow/core/common_runtime/gpu/gpu_allocator_retry_test.cc:136] Consumer 0 is 1090
I tensorflow/core/common_runtime/gpu/gpu_allocator_retry_test.cc:136] Consumer 1 is 1090
I tensorflow/core/common_runtime/gpu/gpu_allocator_retry_test.cc:136] Consumer 2 is 1089
[       OK ] GPUAllocatorRetryTest.RetrySuccess (55 ms)
[ RUN      ] GPUAllocatorRetryTest.NoRetryFail
I tensorflow/core/common_runtime/gpu/gpu_allocator_retry_test.cc:160] Consumer 0 is 534506
I tensorflow/core/common_runtime/gpu/gpu_allocator_retry_test.cc:160] Consumer 1 is 534511
I tensorflow/core/common_runtime/gpu/gpu_allocator_retry_test.cc:160] Consumer 2 is 534504
**tensorflow/core/common_runtime/gpu/gpu_allocator_retry_test.cc:164: Failure
Value of: has_failed_
  Actual: false
Expected: true**
[  FAILED  ] GPUAllocatorRetryTest.NoRetryFail (10811 ms)
[ RUN      ] GPUAllocatorRetryTest.RetryInsufficientFail
I tensorflow/core/common_runtime/gpu/gpu_allocator_retry_test.cc:183] Consumer 0 is 23
I tensorflow/core/common_runtime/gpu/gpu_allocator_retry_test.cc:183] Consumer 1 is 0
I tensorflow/core/common_runtime/gpu/gpu_allocator_retry_test.cc:183] Consumer 2 is 0
[       OK ] GPUAllocatorRetryTest.RetryInsufficientFail (10001 ms)
[----------] 3 tests from GPUAllocatorRetryTest (20867 ms total)

[----------] Global test environment tear-down
[==========] 3 tests from 1 test case ran. (20867 ms total)
[  PASSED  ] 2 tests.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] GPUAllocatorRetryTest.NoRetryFail

 1 FAILED TEST
```
"
964,Test failure on Mac: //tensorflow/python:directory_watcher_test,"The following Python unit test fails on Mac: **//tensorflow/python:directory_watcher_test**

Below is the log with detailed error info:

```
..F...F...
======================================================================
FAIL: testFinishesLoadingFileWhenSwitchingToNewFile (__main__.DirectoryWatcherTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Users/jenkins/workspace/experimental-cais-tensorflow-mac-python27-copt_pip_install-test/pip_install_tests/directory_watcher_test.py"", line 102, in testFinishesLoadingFileWhenSwitchingToNewFile
    self.assertWatcherYields(['b', 'c'])
  File ""/Users/jenkins/workspace/experimental-cais-tensorflow-mac-python27-copt_pip_install-test/pip_install_tests/directory_watcher_test.py"", line 63, in assertWatcherYields
    self.assertEqual(list(self._watcher.Load()), values)
AssertionError: Lists differ: ['c'] != ['b', 'c']

First differing element 0:
c
b

Second list contains 1 additional elements.
First extra element 1:
c

- ['c']
+ ['b', 'c']

======================================================================
FAIL: testMultipleWrites (__main__.DirectoryWatcherTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Users/jenkins/workspace/experimental-cais-tensorflow-mac-python27-copt_pip_install-test/pip_install_tests/directory_watcher_test.py"", line 82, in testMultipleWrites
    self.assertWatcherYields(['x', 'y', 'z'])
  File ""/Users/jenkins/workspace/experimental-cais-tensorflow-mac-python27-copt_pip_install-test/pip_install_tests/directory_watcher_test.py"", line 63, in assertWatcherYields
    self.assertEqual(list(self._watcher.Load()), values)
AssertionError: Lists differ: [] != ['x', 'y', 'z']

Second list contains 3 additional elements.
First extra element 0:
x

- []
+ ['x', 'y', 'z']

----------------------------------------------------------------------
Ran 10 tests in 0.051s

FAILED (failures=2)
```
"
962,Uninstall Bazel,"Hi!

I ran into an error while running code very similar to the [RNN tutorial](https://www.tensorflow.org/versions/0.6.0/tutorials/recurrent/index.html). 

I believe the issue was the exact [problem](https://github.com/tensorflow/tensorflow/issues/481) that was apparently fixed now. So I wanted to uninstall my version of Tensorflow and install it from source code in order to get the latest updates.

But (!) after installing Bazel and downloading the tensorflow repo, I ran into an error while compiling. A file, ""transpose_op_gpu.cu.d"" was said to be missing and the compilation could not continue. Strange.

So then I wanted to install tensorflow again ""normally"" using pip. That seems to have worked, but when I try to import tensorflow in python it just says 

```
Error importing tensorflow.  Unless you are using bazel,
you should not try to import tensorflow from its source directory;
please exit the tensorflow source tree, and relaunch your python interpreter
from there.
```

So my question is: How do I **uninstall** Bazel!?
"
961,Failed to build pip package for Python 3: `PyString_FromStringAndSize` not declared,"I was building pip package for Python 3.5. bazel reported an error during 

```
bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures
```

The failing command was:

```
cd /home/z/.cache/bazel/_bazel_z/4c5a3f7b23103a2fe3f8cd834eaf9376/tensorflow && \
exec env - \
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games \
third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc \
(lots of flags here)
-c bazel-out/local_linux-py3-opt/bin/tensorflow/python/pywrap_tensorflow.cc \
-o bazel-out/local_linux-py3-opt/bin/tensorflow/python/_objs/_pywrap_tensorflow.so/tensorflow/python/pywrap_tensorflow.pic.o
```

The error was:

```
bazel-out/local_linux-py3-opt/bin/tensorflow/python/pywrap_tensorflow.cc: In function 'PyObject* _wrap_TF_PRunSetup(PyObject*, PyObject*)':
bazel-out/local_linux-py3-opt/bin/tensorflow/python/pywrap_tensorflow.cc:4974:66: error: 'PyString_FromStringAndSize' was not declared in this scope
       resultobj = PyString_FromStringAndSize(*arg6, strlen(*arg6));
                                                                  ^
```

It seemed to be a Python 2/3 compatibility issue, since `PyString_FromStringAndSize` is no longer provided by Python 3 headers.

`pywrap_tensorflow.cc` reads like:

```
  resultobj = SWIG_Py_Void();
  {
    if (!arg5->ok()) {
      RaiseStatusNotOK(*arg5, SWIGTYPE_p_tensorflow__Status);
      SWIG_fail;
    } else {
      resultobj = PyString_FromStringAndSize(*arg6, strlen(*arg6));
      delete *arg6;
    }
  }
  return resultobj;
```

I'm not sure whether it was caused by tensorflow or my configuration, but I did find some suspicious code in `tensorflow/python/client/tf_session.i`, that closely resembles the problematic code:
https://github.com/tensorflow/tensorflow/blob/97f585d506cccc57dc98f234f4d5fcd824dd3c03/tensorflow/python/client/tf_session.i#L211
"
960,bug in tf_session.i came from commit:8a59748,"line 211:
    $result = PyString_FromStringAndSize(_$2, strlen(_$2));
To:
%#if PY_MAJOR_VERSION < 3
    $result = PyString_FromStringAndSize(
%#else
    $result = PyUnicode_FromStringAndSize(
%#endif
        _$2, strlen(_$2));
"
959,"Android demo app crashes ""tensorflow/examples/android/jni/jni_utils.cc:91 'asset' Must be non NULL""","When I try to launch the Android demo app, the app crashes with the following error.

Device: Nexus5x
Android version: 6.0
Build number: MDB08L

I also tried the app on a Nexus5 running Android 5.1, but resulted in the same error.

```
02-02 17:02:28.666 2308-2323/org.tensorflow.demo A/native: tensorflow/examples/android/jni/jni_utils.cc:91 'asset' Must be non NULL
02-02 17:02:28.666 2308-2323/org.tensorflow.demo A/libc: Fatal signal 6 (SIGABRT), code -6 in tid 2323 (ImageListener)
02-02 17:02:28.767 487-487/? A/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
02-02 17:02:28.768 487-487/? A/DEBUG: Build fingerprint: 'google/bullhead/bullhead:6.0/MDB08L/2343525:user/release-keys'
02-02 17:02:28.768 487-487/? A/DEBUG: Revision: 'rev_1.0'
02-02 17:02:28.768 487-487/? A/DEBUG: ABI: 'arm'
02-02 17:02:28.768 487-487/? A/DEBUG: pid: 2308, tid: 2323, name: ImageListener  >>> org.tensorflow.demo <<<
02-02 17:02:28.768 487-487/? A/DEBUG: signal 6 (SIGABRT), code -6 (SI_TKILL), fault addr --------
02-02 17:02:28.792 487-487/? A/DEBUG: Abort message: 'tensorflow/examples/android/jni/jni_utils.cc:91 'asset' Must be non NULL'
02-02 17:02:28.792 487-487/? A/DEBUG:     r0 00000000  r1 00000913  r2 00000006  r3 f3b7f978
02-02 17:02:28.793 487-487/? A/DEBUG:     r4 f3b7f980  r5 f3b7f930  r6 00000000  r7 0000010c
02-02 17:02:28.793 487-487/? A/DEBUG:     r8 f3b7e9ac  r9 f3b7e9a0  sl f3b7e96c  fp f3b7e980
02-02 17:02:28.793 487-487/? A/DEBUG:     ip 00000006  sp f3b7e900  lr f6deef15  pc f6df0c00  cpsr 400f0010
02-02 17:02:28.800 487-487/? A/DEBUG:     #00 pc 00041c00  /system/lib/libc.so (tgkill+12)
02-02 17:02:28.800 487-487/? A/DEBUG:     #01 pc 0003ff11  /system/lib/libc.so (pthread_kill+32)
02-02 17:02:28.800 487-487/? A/DEBUG:     #02 pc 0001c73f  /system/lib/libc.so (raise+10)
02-02 17:02:28.800 487-487/? A/DEBUG:     #03 pc 000198f1  /system/lib/libc.so (__libc_android_abort+34)
02-02 17:02:28.800 487-487/? A/DEBUG:     #04 pc 000174b0  /system/lib/libc.so (abort+4)
02-02 17:02:28.801 487-487/? A/DEBUG:     #05 pc 00b9b500  /data/app/org.tensorflow.demo-2/lib/arm/libtensorflow_demo.so (tensorflow::internal::LogMessage::GenerateLogMessage()+1508)
02-02 17:02:28.801 487-487/? A/DEBUG:     #06 pc 00b9ba18  /data/app/org.tensorflow.demo-2/lib/arm/libtensorflow_demo.so (tensorflow::internal::LogMessageFatal::~LogMessageFatal()+28)
02-02 17:02:28.801 487-487/? A/DEBUG:     #07 pc 00647424  /data/app/org.tensorflow.demo-2/lib/arm/libtensorflow_demo.so (AAsset*&&& tensorflow::internal::CheckNotNull<AAsset*&>(char const*, int, char const*, AAsset*&&&)+160)
02-02 17:02:28.801 487-487/? A/DEBUG:     #08 pc 00648158  /data/app/org.tensorflow.demo-2/lib/arm/libtensorflow_demo.so (ReadFileToProto(AAssetManager*, char const*, google::protobuf::MessageLite*)+316)
02-02 17:02:28.801 487-487/? A/DEBUG:     #09 pc 00649090  /data/app/org.tensorflow.demo-2/lib/arm/libtensorflow_demo.so (Java_org_tensorflow_demo_TensorflowClassifier_initializeTensorflow+804)
02-02 17:02:28.801 487-487/? A/DEBUG:     #10 pc 00013eed  /data/app/org.tensorflow.demo-2/oat/arm/base.odex (offset 0xd000) (int org.tensorflow.demo.TensorflowClassifier.initializeTensorflow(android.content.res.AssetManager, java.lang.String, java.lang.String, int, int, int)+168)
02-02 17:02:28.801 487-487/? A/DEBUG:     #11 pc 0001527b  /data/app/org.tensorflow.demo-2/oat/arm/base.odex (offset 0xd000) (void org.tensorflow.demo.TensorflowImageListener.initialize(android.content.res.AssetManager, org.tensorflow.demo.RecognitionScoreView, android.os.Handler)+166)
02-02 17:02:28.801 487-487/? A/DEBUG:     #12 pc 00010c87  /data/app/org.tensorflow.demo-2/oat/arm/base.odex (offset 0xd000) (void org.tensorflow.demo.CameraConnectionFragment.createCameraPreviewSession()+442)
02-02 17:02:28.802 487-487/? A/DEBUG:     #13 pc 0000f86f  /data/app/org.tensorflow.demo-2/oat/arm/base.odex (offset 0xd000) (void org.tensorflow.demo.CameraConnectionFragment.access$400(org.tensorflow.demo.CameraConnectionFragment)+50)
02-02 17:02:28.802 487-487/? A/DEBUG:     #14 pc 0000dd1b  /data/app/org.tensorflow.demo-2/oat/arm/base.odex (offset 0xd000) (void org.tensorflow.demo.CameraConnectionFragment$2.onOpened(android.hardware.camera2.CameraDevice)+222)
02-02 17:02:28.802 487-487/? A/DEBUG:     #15 pc 734ffe8d  /data/dalvik-cache/arm/system@framework@boot.oat (offset 0x1ec4000)
```
"
958,Generating whl for linux,"Hi all,
I am trying to compile tensorflow for ubuntu 14.04 by following instruction at https://www.tensorflow.org/versions/master/get_started/os_setup.html#create-pip.

Made it as far as:
bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer
 with correct output.  The instructions seems to stop there without any indication of how to generate whl for pip and further how to install with pip.  Any pointers will be appreciated.

Thanks
LEon
"
957,Tutorial ImportError: No module named examples.tutorials.mnist.input_data,"I am beginner in tensorflower

install tensorflow with pip 
    `sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.6.0-cp27-none-linux_x86_64.whl`

I want to start with the tutorial ""MNIST For ML Beginners""
but first import get error...
`import tensorflow.examples.tutorials.mnist.input_data`
`File ""<stdin>"", line 1, in <module>`
`ImportError: No module named examples.tutorials.mnist.input_data`

OS:Ubuntu 14.04
run on virtualbo
"
956,Feature request: other types of padding besides zero-padding.,"I would like to have other options of padding for tf.pad and convolution ops.

Some types that come to my mind right now:
- reflect
- constant value (other than zero)
- maybe implement other options of numpy.pad: http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.pad.html
"
955,Tons of warnings related to GUARDED_BY on Mac,"I get a bunch of these:

```
In file included from tensorflow/core/framework/load_library.cc:20:
./tensorflow/core/framework/op_kernel.h:945:63: warning: 'guarded_by' attribute requires arguments whose type is annotated with 'capability' attribute; type here is 'tensorflow::mutex' [-Wthread-safety-attributes]
  gtl::InlinedVector<WrappedAllocator, 4> wrapped_allocators_ GUARDED_BY(mu_);
                                                              ^
./tensorflow/core/platform/default/thread_annotations.h:52:53: note: expanded from macro 'GUARDED_BY'
#define GUARDED_BY(x) THREAD_ANNOTATION_ATTRIBUTE__(guarded_by(x))
                                                    ^
./tensorflow/core/platform/default/thread_annotations.h:42:57: note: expanded from macro 'THREAD_ANNOTATION_ATTRIBUTE__'
#define THREAD_ANNOTATION_ATTRIBUTE__(x) __attribute__((x))
```
"
953,"Failed to enqueue async memcpy from device to host, with latest code (relapse?)","On a Linux machine with a GPU, with the GPU configuration, I ran the following command: 
`bazel test -c opt --config=cuda //tensorflow/python:framework_function_test`

Then I got the following error, which seems to be similar to the closed issues. I'm using the latest code of the master branch
https://github.com/tensorflow/tensorflow/issues/719
https://github.com/tensorflow/tensorflow/issues/713

I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so.7.0 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so.6.5 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so.7.0 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so locally
I tensorflow/stream_executor/dso_loader.cc:99] Couldn't open CUDA library libcurand.so.7.0. LD_LIBRARY_PATH: 
I tensorflow/stream_executor/cuda/cuda_rng.cc:333] Unable to load cuRAND DSO.
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:909] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 
name: GeForce GTX 980
major: 5 minor: 2 memoryClockRate (GHz) 1.2785
pciBusID 0000:01:00.0
Total memory: 4.00GiB
Free memory: 3.91GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:680] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0)
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 32.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 64.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 128.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 256.0Ki
B
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 512.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 32.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 64.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 128.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 256.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 512.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.00GiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.00GiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.00GiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:73] Allocating 3.62GiB bytes.
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:83] GPU 0 memory begins at 0x704a80000 extends to 0x7ec261000
...I tensorflow/core/common_runtime/gpu/gpu_device.cc:680] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0)
..I tensorflow/core/common_runtime/gpu/gpu_device.cc:680] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0)
**E tensorflow/stream_executor/cuda/cuda_driver.cc:1197] failed to enqueue async memcpy from device to host: CUDA_ERROR_INVALID_VALUE; host dst: 0x7f16e40008c0; GPU src: 0x265d480; size: 4=0x4**
F tensorflow/core/common_runtime/gpu/gpu_util.cc:232] GPU->CPU Memcpy failed
external/bazel_tools/tools/test/test-setup.sh: line 51:   714 Aborted                 (core dumped) ""$@""
"
952,Error in //tensorflow/core:ops_array_grad_test,"On a machine with a GPU, under the GPU configuration, I ran the command: bazel test -c opt --config=cuda //tensorflow/core:ops_array_grad_test

and I got the following error (see highlight in bold font)
## exec ${PAGER:-/usr/bin/less} ""$0"" || exit 1

I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so.7.0 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so.6.5 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so.7.0 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so locally
I tensorflow/stream_executor/dso_loader.cc:99] Couldn't open CUDA library libcurand.so.7.0. LD_LIBRARY_PATH: 
I tensorflow/stream_executor/cuda/cuda_rng.cc:333] Unable to load cuRAND DSO.
Running main() from test_main.cc
[==========] Running 4 tests from 1 test case.
[----------] Global test environment set-up.
[----------] 4 tests from ArrayGradTest
[ RUN      ] ArrayGradTest.PackGrad
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:909] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 
name: GeForce GTX 980
major: 5 minor: 2 memoryClockRate (GHz) 1.2785
pciBusID 0000:01:00.0
Total memory: 4.00GiB
Free memory: 3.91GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:680] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0)
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 32.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 64.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 128.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 256.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 512.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 32.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 64.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 128.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 256.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 512.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.00GiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.00GiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.00GiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:73] Allocating 3.62GiB bytes.
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:83] GPU 0 memory begins at 0x704a80000 extends to 0x7ec261000
[       OK ] ArrayGradTest.PackGrad (481 ms)
[ RUN      ] ArrayGradTest.UnpackGrad
I tensorflow/core/common_runtime/gpu/gpu_device.cc:680] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0)
[       OK ] ArrayGradTest.UnpackGrad (2 ms)
[ RUN      ] ArrayGradTest.ConcatGrad
I tensorflow/core/common_runtime/gpu/gpu_device.cc:680] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0)
**E tensorflow/core/common_runtime/executor.cc:273] Executor failed to create kernel. Not found: No registered 'ZerosLike' OpKernel for GPU devices compatible with node n8 = ZerosLike[T=DT_INT32](n0)**
         [[Node: n8 = ZerosLike[T=DT_INT32](n0)]]
**W tensorflow/core/common_runtime/executor.cc:1096] 0x7f70d3c21f10 Compute status: Not found: **No registered 'ZerosLike' OpKernel for GPU devices compatible with node n8 = ZerosLike[T=DT_INT32](n0)**
         [[Node: n8 = ZerosLike[T=DT_INT32](n0)]]
         [[Node: dx = SymbolicGradient[Tin=[DT_INT32, DT_FLOAT, DT_FLOAT, DT_FLOAT], Tout=[DT_INT32, DT_FLOAT, DT_FLOAT], f=Concat[N=2, T=DT_FLOAT], _device=""/job:localhost/replica:0/task:0/gpu:0""](_recv_dim_0/_1, _recv_x0_0/_3, _recv_x1_0/_5, _recv_dy_0/_7)]]
**W tensorflow/core/common_runtime/executor.cc:1096] 0x7f70d3c224b0 Compute status: Not found: No registered 'ZerosLike' OpKernel for GPU devices compatible with node n8 = ZerosLike[T=DT_INT32](n0)**
         [[Node: n8 = ZerosLike[T=DT_INT32](n0)]]
         [[Node: dx = SymbolicGradient[Tin=[DT_INT32, DT_FLOAT, DT_FLOAT, DT_FLOAT], Tout=[DT_INT32, DT_FLOAT, DT_FLOAT], f=Concat[N=2, T=DT_FLOAT], _device=""/job:localhost/replica:0/task:0/gpu:0""](_recv_dim_0/_1, _recv_x0_0/_3, _recv_x1_0/_5, _recv_dy_0/_7)]]
         [[Node: dx/_9 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_18_dx"", tensor_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
**W tensorflow/core/common_runtime/executor.cc:1096] 0x7f70d3c224b0 Compute status: Not found: No registered 'ZerosLike' OpKernel for GPU devices compatible with node n8 = ZerosLike[T=DT_INT32](n0)
*\*         [[Node: n8 = ZerosLike[T=DT_INT32](n0)]]
         [[Node: dx = SymbolicGradient[Tin=[DT_INT32, DT_FLOAT, DT_FLOAT, DT_FLOAT], Tout=[DT_INT32, DT_FLOAT, DT_FLOAT], f=Concat[N=2, T=DT_FLOAT], _device=""/job:localhost/replica:0/task:0/gpu:0""](_recv_dim_0/_1, _recv_x0_0/_3, _recv_x1_0/_5, _recv_dy_0/_7)]]
         [[Node: dx/_11 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_20_dx"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
**W tensorflow/core/common_runtime/executor.cc:1096] 0x7f70d3c224b0 Compute status: Not found: No registered 'ZerosLike' OpKernel for GPU devices compatible with node n8 = ZerosLike[T=DT_INT32](n0)**
         [[Node: n8 = ZerosLike[T=DT_INT32](n0)]]
         [[Node: dx = SymbolicGradient[Tin=[DT_INT32, DT_FLOAT, DT_FLOAT, DT_FLOAT], Tout=[DT_INT32, DT_FLOAT, DT_FLOAT], f=Concat[N=2, T=DT_FLOAT], _device=""/job:localhost/replica:0/task:0/gpu:0""](_recv_dim_0/_1, _recv_x0_0/_3, _recv_x1_0/_5, _recv_dy_0/_7)]]
         [[Node: dx/_13 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_22_dx"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
**F tensorflow/core/ops/array_grad_test.cc:126] Check failed: ::tensorflow::Status::OK() == (sess->Run( {{""dim"", test::AsScalar(dim)}, {""x0:0"", x0}, {""x1:0"", x1}, {""dy:0"", dy}}, {""dx:0"", ""dx:1"", ""dx:2""}, {}, &out)) (OK vs. Not found: No registered 'ZerosLike' OpKernel for GPU devices compatible with node n8 = ZerosLike[T=DT_INT32](n0)**
         [[Node: n8 = ZerosLike[T=DT_INT32](n0)]]
         [[Node: dx = SymbolicGradient[Tin=[DT_INT32, DT_FLOAT, DT_FLOAT, DT_FLOAT], Tout=[DT_INT32, DT_FLOAT, DT_FLOAT], f=Concat[N=2, T=DT_FLOAT], _device=""/job:localhost/replica:0/task:0/gpu:0""](_recv_dim_0/_1, _recv_x0_0/_3, _recv_x1_0/_5, _recv_dy_0/_7)]]
         [[Node: dx/_9 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_18_dx"", tensor_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]])
external/bazel_tools/tools/test/test-setup.sh: line 51:  9618 Aborted                 (core dumped)
"
950,sigmoid_cross_entropy_loss_with_logits: ReLU input is not finite,"```
W tensorflow/core/common_runtime/executor.cc:1076] 0x7fc328bccbd0 Compute status: Invalid argument: ReluGrad input is not finite. : Tensor had NaN values
     [[Node: nl_1_hs_100_lr_0p1/gradients/nl_1_hs_100_lr_0p1/logistic_loss/Relu_grad/nl_1_hs_100_lr_0p1/logistic_loss/Relu/CheckNumerics = CheckNumerics[T=DT_FLOAT, message=""ReluGrad input is not finite."", _device=""/job:localhost/replica:0/task:0/cpu:0""](nl_1_hs_100_lr_0p1/add)]]
W tensorflow/core/common_runtime/executor.cc:1076] 0x7fc328bccbd0 Compute status: Invalid argument: ReluGrad input is not finite. : Tensor had NaN values
     [[Node: nl_1_hs_100_lr_0p1/gradients/nl_1_hs_100_lr_0p1/logistic_loss/Relu_grad/nl_1_hs_100_lr_0p1/logistic_loss/Relu/CheckNumerics = CheckNumerics[T=DT_FLOAT, message=""ReluGrad input is not finite."", _device=""/job:localhost/replica:0/task:0/cpu:0""](nl_1_hs_100_lr_0p1/add)]]
     [[Node: _send_nl_1_hs_100_lr_0p1/div_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=6247430638048453470, tensor_name=""nl_1_hs_100_lr_0p1/div_1:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](nl_1_hs_100_lr_0p1/div_1)]]
Traceback (most recent call last):
  File ""rnn.py"", line 80, in <module>
    training=True)
  File ""rnn.py"", line 18, in run_epoch
    model.seq_targets: seq_targets
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 368, in run
    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 444, in _do_run
    e.code)
tensorflow.python.framework.errors.InvalidArgumentError: ReluGrad input is not finite. : Tensor had NaN values
     [[Node: nl_1_hs_100_lr_0p1/gradients/nl_1_hs_100_lr_0p1/logistic_loss/Relu_grad/nl_1_hs_100_lr_0p1/logistic_loss/Relu/CheckNumerics = CheckNumerics[T=DT_FLOAT, message=""ReluGrad input is not finite."", _device=""/job:localhost/replica:0/task:0/cpu:0""](nl_1_hs_100_lr_0p1/add)]]
Caused by op u'nl_1_hs_100_lr_0p1/gradients/nl_1_hs_100_lr_0p1/logistic_loss/Relu_grad/nl_1_hs_100_lr_0p1/logistic_loss/Relu/CheckNumerics', defined at:
  File ""rnn.py"", line 70, in <module>
    train_model = Model(set_config(config, ""train""))
  File ""/Users/yoav/projects/music_rnn/model.py"", line 61, in __init__
    .minimize(self.loss)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py"", line 186, in minimize
    aggregation_method=aggregation_method)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py"", line 232, in compute_gradients
    aggregation_method=aggregation_method)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py"", line 445, in gradients
    in_grads = _AsList(grad_fn(op_wrapper, *out_grads))
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_grad.py"", line 126, in _ReluGrad
    t = _VerifyTensor(op.inputs[0], op.name, ""ReluGrad input is not finite."")
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_grad.py"", line 119, in _VerifyTensor
    verify_input = array_ops.check_numerics(t, message=msg)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 48, in check_numerics
    name=name)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py"", line 664, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1834, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1043, in __init__
    self._traceback = _extract_stack()

...which was originally created as op u'nl_1_hs_100_lr_0p1/logistic_loss/Relu', defined at:
  File ""rnn.py"", line 70, in <module>
    train_model = Model(set_config(config, ""train""))
  File ""/Users/yoav/projects/music_rnn/model.py"", line 58, in __init__
    losses = tf.nn.sigmoid_cross_entropy_with_logits(outputs, targets_concat)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/nn.py"", line 273, in sigmoid_cross_entropy_with_logits
    return math_ops.add(nn_ops.relu(logits) - logits * targets,
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 547, in relu
    return _op_def_lib.apply_op(""Relu"", features=features, name=name)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py"", line 664, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1834, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1043, in __init__
    self._traceback = _extract_stack()
```

When training a model using tf.nn.sigmoid_cross_entropy_loss_with_logits and RMSPropOptimizer, I am running into a ReLu gradient error. Since the initial learning rate for this instance of the model is quite high and this is a RNN, I am pretty confident that this is an exploding gradient issue with my model. However, the ReLU operation that my model is exploding on is part of an optimization to ""ensure stability and avoid overflow"", but it seems to be having the opposite effect. 
"
947,sparse_softmax_cross_entropy_with_logits fails when labels is a placeholder,"Running the following results in `ValueError: Shapes (?, 1) and (?,) must have the same rank`:

```
 import tensorflow as tf
 from tensorflow.python.ops.nn import sparse_softmax_cross_entropy_with_logits as sparse_ce

 logits = tf.placeholder('float32', (None, 32))
 labels = tf.placeholder('int64', (None, 1))
 loss = tf.reduce_mean(sparse_ce(logits, labels))
```

Making the minibatch dimension explicit makes no difference: `ValueError: Shapes (32, 1) and (32,) must have the same rank`. 

When I run the unit test for this function there is no problem, but I note that the unit test passes NumPy ndarrays, which are converted to constant tensors. Playing around a bit it seems it fails whenever `labels` is a placeholder.
"
946,running my built tensorflow with cifar10 core dump,"I have followed the instructions to build tensorflow with master branch with gpu.
the only difference is that when I installed cuda7, I can't install the driver because I am not the root.
The admin already installed cuda 7.5 and drivers.
Also, I can't copy cuda 7 so to /usr/local/lib.
when running ./configure I specify the cuda 7 path instead of default value.
After building, I installed it within virtualenv.
$virtualenv --system-site-packages ~/tfenv
$source ~/tfenv/bin/activate
$pip install /tmp/tensorflow_pkg/tensorflow-0.6.0-py2-none-any.whl

$export LD_LIBRARY_PATH=/home/lli/cuda-7.0/lib64
$python cifar10_train.py
.....
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 8.00GiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 16.00GiB
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 1925 get requests, put_count=1701 evicted_count=1000 eviction_rate=0.587889 and unsatisfied allocation rate=0.687792
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 4577 get requests, put_count=4625 evicted_count=1000 eviction_rate=0.216216 and unsatisfied allocation rate=0.213022
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 256 to 281
F tensorflow/stream_executor/cuda/cuda_dnn.cc:207] could not find cudnnCreate in cudnn DSO; dlerror: /home/lli/tfenv/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so: undefined symbol: cudnnCreate
Aborted (core dumped)
"
945,Ops added after calling start_queue_runners() are broken,"(Using the OSX CPU-only  v.0.6 wheel). 

I think this is a minimal example:

```
image = tf.constant(0)
images = tf.train.shuffle_batch([image], batch_size=1, capacity=100, min_after_dequeue=1)

a = tf.constant(1)
with tf.Session():
    b = a+1
    tf.train.start_queue_runners()
    c = a+1

    print(a.eval())
    print(b.eval())
    print(c.eval())
```

It _usually_ crashes on the `c.eval()` giving the following error:

```
Traceback (most recent call last):
  File ""test.py"", line 11, in <module>
    print(c.eval())
  File "".../python2.7/site-packages/tensorflow/python/framework/ops.py"", line 460, in eval
    return _eval_using_default_session(self, feed_dict, self.graph, session)
  File "".../python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2910, in _eval_using_default_session
    return session.run(tensors, feed_dict)
  File "".../python2.7/site-packages/tensorflow/python/client/session.py"", line 368, in run
    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)
  File "".../python2.7/site-packages/tensorflow/python/client/session.py"", line 446, in _do_run
    six.reraise(e_type, e_value, e_traceback)
  File "".../python2.7/site-packages/tensorflow/python/client/session.py"", line 428, in _do_run
    target_list)
tensorflow.python.pywrap_tensorflow.StatusNotOK: Not found: FetchOutputs node add_1:0: not found
```

Even just a clearer error message in this situation would be a big help. 
"
942,build fail with cuda: sparse_xent_op.h,"OS: Debian Stretch 64
Cuda: 7.5
Cudnn: 6.5
Compiler: gcc-4.9 (default gcc-5)
Bazel: 0.1.4

Since I had a problem using gcc-5 building (see at the bottom), I tried forcing it to use **gcc-4.9** by editing third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.

Now the error is about **sparse_xent_op.h**.

Please help, thank you.

```
 $ bazel build -s  -j 1  -c opt  --config=cuda //tensorflow/cc:tutorials_example_trainer                                                                                                                                            [0:28:58]
WARNING: Output base '/auto/master04/weitang114/.cache/bazel/_bazel_weitang114/043c55a899c217c377c622f5ff5a22ca' is on NFS. This may lead to surprising failures and undetermined behavior.
WARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore
_unsupported_sandboxing.
INFO: Found 1 target...
>>>>> # //tensorflow/core:gpu_kernels [action 'Compiling tensorflow/core/kernels/sparse_xent_op_gpu.cu.cc']
(cd /auto/master04/weitang114/.cache/bazel/_bazel_weitang114/043c55a899c217c377c622f5ff5a22ca/tensorflow && \
  exec env - \
    PATH=/home/master/04/weitang114//bin/:/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/local/cuda-7.5/bin:/home/master/04/weitang114//bin \
  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG
-ffunction-sections -fdata-sections '-std=c++11' -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -iquote external/jpeg_archive -iquote bazel
-out/local_linux-opt/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/local_linux-opt/genfiles/external/png_archive -iquote external/re2 -iquote bazel-out/local_linux-opt/genfiles/external/re2 -iquote external
/eigen_archive -iquote bazel-out/local_linux-opt/genfiles/external/eigen_archive -isystem third_party/gpus/cuda/include -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda/include -isystem external/bazel_tools/tools/cpp/gcc3
 -isystem google/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/google/protobuf/src -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/l
ibpng-1.2.53 -isystem bazel-out/local_linux-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local_linux-opt/genf
iles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-b45554449873 -isystem bazel-out/local_linux-opt/genfiles/external/eigen_archive/eigen-eigen-b45554449873 -isystem third_party/gpus/cuda -isystem bazel-out/local_linux-opt
/genfiles/third_party/gpus/cuda -x cuda '-DGOOGLE_CUDA=1' '-nvcc_options=relaxed-constexpr' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fno-canonical-sy
stem-headers '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/core/_objs/gpu_kernels/tensorflow/core/kernels/sparse_xent_op_gpu.cu.o' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/core/_objs/gpu_kernels/tensorflow/core/kernels/s
parse_xent_op_gpu.cu.d -c tensorflow/core/kernels/sparse_xent_op_gpu.cu.cc -o bazel-out/local_linux-opt/bin/tensorflow/core/_objs/gpu_kernels/tensorflow/core/kernels/sparse_xent_op_gpu.cu.o)
INFO: From Compiling tensorflow/core/kernels/sparse_xent_op_gpu.cu.cc:
./tensorflow/core/kernels/sparse_xent_op.h(58): error: explicit type is missing (""int"" assumed)

./tensorflow/core/kernels/sparse_xent_op.h(58): error: expected a "">""

./tensorflow/core/kernels/sparse_xent_op.h(58): error: expected a type specifier

./tensorflow/core/kernels/sparse_xent_op.h(73): error: explicit type is missing (""int"" assumed)

./tensorflow/core/kernels/sparse_xent_op.h(73): error: expected a "">""

./tensorflow/core/kernels/sparse_xent_op.h(90): error: explicit type is missing (""int"" assumed)

./tensorflow/core/kernels/sparse_xent_op.h(90): error: expected a "">""

./tensorflow/core/kernels/sparse_xent_op.h(90): error: expected a type specifier

./tensorflow/core/kernels/sparse_xent_op.h(104): error: explicit type is missing (""int"" assumed)

./tensorflow/core/kernels/sparse_xent_op.h(104): error: expected a "">""

./tensorflow/core/kernels/sparse_xent_op.h(122): error: identifier ""int64"" is undefined

./tensorflow/core/kernels/sparse_xent_op.h(133): error: identifier ""int64"" is undefined

./tensorflow/core/kernels/sparse_xent_op.h(185): error: no instance of constructor ""tensorflow::generator::SparseXentLossGenerator<T>::SparseXentLossGenerator [with T=float]"" matches the argument list
            argument types are: (Eigen::TensorMap<Eigen::Tensor<const float, 2, 1, int>, 16>, Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, int>, 16>, <error-type>)
          detected during:
            instantiation of ""void tensorflow::functor::SparseXentEigenImpl<Device, T>::Compute(const Device &, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::ConstMatrix, tensorflow::TTypes<<error-type>, 1, Eigen::DenseIndex>::ConstVec, te
nsorflow::TTypes<T, 1, Eigen::DenseIndex>::Vec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Vec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Matrix) [with Device=tensorflow::GPUDevice, T=float]""
tensorflow/core/kernels/sparse_xent_op_gpu.cu.cc(39): here
            instantiation of ""void tensorflow::functor::SparseXentFunctor<tensorflow::GPUDevice, T>::operator()(const tensorflow::GPUDevice &, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::ConstMatrix, tensorflow::TTypes<tensorflow::int64,
 1, Eigen::DenseIndex>::ConstVec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Vec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Vec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Matrix) [with T=float]""
tensorflow/core/kernels/sparse_xent_op_gpu.cu.cc(46): here

./tensorflow/core/kernels/sparse_xent_op.h(193): error: no instance of constructor ""tensorflow::generator::SparseXentGradGenerator<T>::SparseXentGradGenerator [with T=float]"" matches the argument list
            argument types are: (Eigen::TensorMap<Eigen::Tensor<const float, 2, 1, int>, 16>, Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, int>, 16>, <error-type>)
          detected during:
            instantiation of ""void tensorflow::functor::SparseXentEigenImpl<Device, T>::Compute(const Device &, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::ConstMatrix, tensorflow::TTypes<<error-type>, 1, Eigen::DenseIndex>::ConstVec, te
nsorflow::TTypes<T, 1, Eigen::DenseIndex>::Vec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Vec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Matrix) [with Device=tensorflow::GPUDevice, T=float]""
tensorflow/core/kernels/sparse_xent_op_gpu.cu.cc(39): here
            instantiation of ""void tensorflow::functor::SparseXentFunctor<tensorflow::GPUDevice, T>::operator()(const tensorflow::GPUDevice &, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::ConstMatrix, tensorflow::TTypes<tensorflow::int64,
 1, Eigen::DenseIndex>::ConstVec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Vec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Vec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Matrix) [with T=float]""
tensorflow/core/kernels/sparse_xent_op_gpu.cu.cc(46): here

tensorflow/core/kernels/sparse_xent_op_gpu.cu.cc(39): error: no suitable user-defined conversion from ""Eigen::TensorMap<Eigen::Tensor<const tensorflow::int64, 1, 1, Eigen::DenseIndex>, 16>"" to ""Eigen::TensorMap<Eigen::Tensor<const <error-
type>, 1, 1, Eigen::DenseIndex>, 16>"" exists
          detected during instantiation of ""void tensorflow::functor::SparseXentFunctor<tensorflow::GPUDevice, T>::operator()(const tensorflow::GPUDevice &, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::ConstMatrix, tensorflow::TTypes<tens
orflow::int64, 1, Eigen::DenseIndex>::ConstVec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Vec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Vec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Matrix) [with T=float]""
(46): here

./tensorflow/core/kernels/sparse_xent_op.h(91): error: identifier ""labels"" is undefined

16 errors detected in the compilation of ""/tmp/tmpxft_00007ac3_00000000-10_sparse_xent_op_gpu.cu.compute_52.cpp1.ii"".
ERROR: /auto/master04/weitang114/tensorflow/tensorflow/core/BUILD:321:1: output 'tensorflow/core/_objs/gpu_kernels/tensorflow/core/kernels/sparse_xent_op_gpu.cu.o' was not created.
ERROR: /auto/master04/weitang114/tensorflow/tensorflow/core/BUILD:321:1: not all outputs were created.
Target //tensorflow/cc:tutorials_example_trainer failed to build
```

<br>
## <br>

(For the details how I changed to gcc-4.9, in case I did something wrong)
At first I used gcc-5(by default), nvcc compiling **core/kernels/bias_op_gpu.cu.cc**: 

```
/usr/lib/gcc/x86_64-linux-gnu/5/include/mwaitxintrin.h(36): error: identifier ""__builtin_ia32_monitorx"" is undefined.
/usr/lib/gcc/x86_64-linux-gnu/5/include/mwaitxintrin.h(42): error: identifier ""__builtin_ia32_mwaitx"" is undefined.
```

Then I followed #8 to modify **crosstool_wrapper_driver_is_not_gcc** into

```
 46 CURRENT_DIR = os.path.dirname(sys.argv[0])
 47 CPU_COMPILER = ('/usr/bin/gcc-4.9')
 48 NVCC_PATH = CURRENT_DIR + '/../../../cuda/bin/nvcc'
 49 GCC_HOST_COMPILER_PATH = ('/usr/bin/gcc-4.9')
 50 LLVM_HOST_COMPILER_PATH = ('/usr/bin/gcc-4.9')
 51 PREFIX_DIR = os.path.dirname(GCC_HOST_COMPILER_PATH)
```
"
941,build fail on eigen,"On Ubuntu 14.04 in a VirtualBox

Followed setup and install instructions and ran:
bazel build -c opt //tensorflow/tools/pip_package:build_pip_package

Stops on error:

INFO: From Compiling tensorflow/core/kernels/conv_grad_ops.cc:
In file included from ./external/eigen_archive/eigen-eigen-b45554449873/unsupported/Eigen/CXX11/Core:35:0,
                 from ./external/eigen_archive/eigen-eigen-b45554449873/unsupported/Eigen/CXX11/Tensor:14,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/core/framework/types.h:23,
                 from ./tensorflow/core/framework/type_traits.h:22,
                 from ./tensorflow/core/framework/allocator.h:25,
                 from ./tensorflow/core/framework/op_kernel.h:22,
                 from ./tensorflow/core/framework/numeric_op.h:19,
                 from tensorflow/core/kernels/conv_grad_ops.cc:22:
./external/eigen_archive/eigen-eigen-b45554449873/unsupported/Eigen/CXX11/src/Core/util/EmulateArray.h: In static member function 'static void Eigen::internal::TensorExecutor<Expression, Eigen::ThreadPoolDevice, Vectorizable>::run(const Expression&, const Eigen::ThreadPoolDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long int>, 16>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 4>, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long int>, 2ul>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 3>, const Eigen::TensorImagePatchOp<-1l, -1l, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 3>, const Eigen::TensorReverseOp<const Eigen::array<bool, 4ul>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16> > > > > >; bool Vectorizable = true]':
./external/eigen_archive/eigen-eigen-b45554449873/unsupported/Eigen/CXX11/src/Core/util/EmulateArray.h:24:67: warning: array subscript is above array bounds [-Warray-bounds]
   EIGEN_STRONG_INLINE T& operator[](size_t index) { return values[index]; }
                                                                   ^
INFO: From Compiling tensorflow/core/kernels/cwise_op_add.cc:
gcc: internal compiler error: Killed (program cc1plus)
Please submit a full bug report,
with preprocessed source if appropriate.
See file:///usr/share/doc/gcc-4.8/README.Bugs for instructions.
ERROR: /home/sandra/deeplearning/tensorflow/tensorflow/core/BUILD:342:1: C++ compilation of rule '//tensorflow/core:kernels' failed: gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections ... (remaining 78 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 4.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 815.245s, Critical Path: 772.81s

I tried running the gcc call and it said somethine like eigen_archive/... didn't exist. I'm not sure where it's mean to be or whether it's generated by this step, but it's not under bazel-bin/external like the other ""archive"" folders.

$ ls bazel-bin/external/
jpeg_archive/ png_archive/  re2/          

Any help?
"
938,apply_gradients.run seems to leak memory,"I'm running a large scale network with tensorflow and I keep getting memory errors, so I pull out memory_profiler and found this 

Filename: parameterservermodel.py

Line    Memtotal    Increment   Line Contents
   110   2121.9 MiB     39.2 MiB            self.apply_gradients.run(session=self.session, feed_dict=feed_dict)

If I'm not mistaken, it looks like like apply_gradients.run allocates almost 40MB and doesn't return anything for me to free. I might be missing something here. 
"
937,Build fail on OSX (@ 2cb25ab),"I recently refreshed my master and now build fails on my laptop. Also tried building from a fresh clone. That fails too.

```
bazel build -c opt //tensorflow/tools/pip_package:build_pip_package
.........
ERROR: /Users/delip/Softwares/tensorflow/tensorflow/models/embedding/BUILD:10:6: syntax error at '""//tensorflow:tensorflow.bzl""': First argument of load() is a path, not a label. It should start with a single slash if it is an absolute path..
...
```

I just noticed now that Jenkins status is red for OSX too. Until this gets fixed, what is the latest commit for which OSX builds were working? Also, any tips on fixing this locally will help too. I'm not conversant with Bazel, unfortunately.
"
936,Reduce 1_notmnist.ipynb memory requirements.,"Lots of people are having trouble downloading and pickling the data. need to look into ways to make that easier.
"
933,thread pool does not execute all remaining tasks on shutdown,"ThreadPool claims to finish all remaining work on shutdown. This is not the case. If remaining tasks submit new tasks, the new tasks do not get executed. This happens because the sentinel tasks are fetched before the newly submitted real tasks.

ThreadPool must either execute all pending work, or execute nothing on shutdown. It does not make sense to execute half of work.

Here is a repro:

``` c
static void BM_ParallelDivide(int iters, const char* impl) {
  THREAD_POOL_IMPL_NAME = impl;
  for (int i = 0; i < iters; i++) {
    const int kTasks = 10;
    const int kLevels = 22;
    std::atomic<unsigned> count(kTasks * (1 << kLevels));
    mutex done_lock;
    condition_variable done;
    bool done_flag = false;
    std::function<void(int)> work;
    ThreadPool pool(Env::Default(), ""test"", kNumThreads);
    work = [&pool, &work, &count, &done_lock, &done, &done_flag](int level) {
      if (level-- > 0) {
        pool.Schedule([&work, level]() { work(level); });
        pool.Schedule([&work, level]() { work(level); });
        return;
      }
      delay();
    };
    for (int t = 0; t < kTasks; ++t) {
      pool.Schedule([&work]() {
        work(kLevels);
      });
    }
  }
}
```
"
932,thread pool deadlocks on shutdown,"ThreadPool dtor does not pop waiters from waiters_ list. As the result dead waiters are left on the list. If remaining tasks submit new tasks, thread pool deadlocks because some notifications are consumed by the leftover dead waiters instead of alive threads that should receive the notifications.

Here is a simple test that does classical parallel decomposition and reliably deadlocks:

``` c
static void BM_ParallelDivide(int iters, const char* impl) {
  THREAD_POOL_IMPL_NAME = impl;
  for (int i = 0; i < iters; i++) {
    const int kTasks = 10;
    const int kLevels = 22;
    std::atomic<unsigned> count(kTasks * (1 << kLevels));
    mutex done_lock;
    condition_variable done;
    bool done_flag = false;
    std::function<void(int)> work;
    ThreadPool pool(Env::Default(), ""test"", kNumThreads);
    work = [&pool, &work, &count, &done_lock, &done, &done_flag](int level) {
      if (level-- > 0) {
        pool.Schedule([&work, level]() { work(level); });
        pool.Schedule([&work, level]() { work(level); });
        return;
      }
      delay();
    };
    for (int t = 0; t < kTasks; ++t) {
      pool.Schedule([&work]() {
        work(kLevels);
      });
    }
  }
}
```
"
931,Text window slider not working on TensorFlow.org Download and Setup page,"On TensorFlow.org - Download and Setup page

https://www.tensorflow.org/versions/master/get_started/os_setup.html

Under Pip Installation
For python3:

the text window slider does not allow you to slide to the right. 

If works for the text window slider above under Install TensorFlow:

As a work around you can visit the instructions on the GitHub page

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md
"
929,label_image deadlocks,"I am on commit 14cd77baeb0ee6f4b40816afc1f8c9c5d186bb8e.

I've added the following change that randomizes order of execution of tasks in thread pool:

```
diff --git a/tensorflow/core/lib/core/threadpool.cc b/tensorflow/core/lib/core/threadpool.cc
index 50aec3e..03c4485 100644
--- a/tensorflow/core/lib/core/threadpool.cc
+++ b/tensorflow/core/lib/core/threadpool.cc
@@ -20,6 +20,8 @@ limitations under the License.
 #include ""tensorflow/core/platform/tracing.h""
 #include ""tensorflow/core/platform/types.h""

+#include <unistd.h>
+
 namespace tensorflow {
 namespace thread {

@@ -34,6 +36,7 @@ ThreadPool::ThreadPool(Env* env, const string& name, int num_threads)
 ThreadPool::ThreadPool(Env* env, const ThreadOptions& thread_options,
                        const string& name, int num_threads)
     : name_(name) {
+  rand_ = getpid();
   CHECK_GE(num_threads, 1);
   string name_prefix = ""tf_"" + name_;
   for (int i = 0; i < num_threads; i++) {
@@ -102,6 +105,7 @@ void ThreadPool::WorkerLoop() {
         w.cv.wait(l);
       }
     }
+    std::swap(pending_[0], pending_[rand_r(&rand_) % pending_.size()]);
     // Pick up pending work
     Item item = pending_.front();
     pending_.pop_front();
diff --git a/tensorflow/core/lib/core/threadpool.h b/tensorflow/core/lib/core/threadpool.h
index ef37dcf..ae1eef3 100644
--- a/tensorflow/core/lib/core/threadpool.h
+++ b/tensorflow/core/lib/core/threadpool.h
@@ -66,6 +66,7 @@ class ThreadPool {
   std::vector<Thread*> threads_;  // All threads
   std::vector<Waiter*> waiters_;  // Stack of waiting threads.
   std::deque<Item> pending_;      // Queue of pending work
+  unsigned rand_;

   TF_DISALLOW_COPY_AND_ASSIGN(ThreadPool);
 };
```

Then run label_image as:

```
while echo OK; do bazel-bin/tensorflow/examples/label_image/label_image; done
```

after few iterations it deadlocks. All threads are blocked on condition variables, but tasks they are waiting for can't run because all thread pool threads are busy:

```
(gdb) info threads
  Id   Target Id         Frame 
  97   Thread 0x7f0d04038700 (LWP 74836) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  96   Thread 0x7f0d03837700 (LWP 74837) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  95   Thread 0x7f0d03036700 (LWP 74838) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  94   Thread 0x7f0d02835700 (LWP 74839) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  93   Thread 0x7f0d02034700 (LWP 74840) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  92   Thread 0x7f0d01833700 (LWP 74841) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  91   Thread 0x7f0d01032700 (LWP 74842) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  90   Thread 0x7f0d00831700 (LWP 74843) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  89   Thread 0x7f0cf3fff700 (LWP 74844) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  88   Thread 0x7f0cf37fe700 (LWP 74845) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  87   Thread 0x7f0cf2ffd700 (LWP 74846) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  86   Thread 0x7f0cf27fc700 (LWP 74847) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  85   Thread 0x7f0cf1ffb700 (LWP 74848) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  84   Thread 0x7f0cf17fa700 (LWP 74849) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  83   Thread 0x7f0cf0ff9700 (LWP 74850) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  82   Thread 0x7f0ce7fff700 (LWP 74851) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  81   Thread 0x7f0ce77fe700 (LWP 74852) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  80   Thread 0x7f0ce6ffd700 (LWP 74853) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  79   Thread 0x7f0ce67fc700 (LWP 74854) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  78   Thread 0x7f0ce5ffb700 (LWP 74855) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  77   Thread 0x7f0ce57fa700 (LWP 74856) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  76   Thread 0x7f0ce4ff9700 (LWP 74857) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  75   Thread 0x7f0cdffff700 (LWP 74858) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  74   Thread 0x7f0cdf7fe700 (LWP 74859) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  73   Thread 0x7f0cdeffd700 (LWP 74860) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  72   Thread 0x7f0cde7fc700 (LWP 74861) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  71   Thread 0x7f0cddffb700 (LWP 74862) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  70   Thread 0x7f0cdd7fa700 (LWP 74863) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  69   Thread 0x7f0cdcff9700 (LWP 74864) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  68   Thread 0x7f0cdc7f8700 (LWP 74865) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  67   Thread 0x7f0cdbff7700 (LWP 74866) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  66   Thread 0x7f0cdb7f6700 (LWP 74867) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  65   Thread 0x7f0cdaff5700 (LWP 74868) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  64   Thread 0x7f0cda7f4700 (LWP 74869) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  63   Thread 0x7f0cd9ff3700 (LWP 74870) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  62   Thread 0x7f0cd97f2700 (LWP 74871) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  61   Thread 0x7f0cd8ff1700 (LWP 74872) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  60   Thread 0x7f0cd3fff700 (LWP 74873) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  59   Thread 0x7f0cd37fe700 (LWP 74874) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  58   Thread 0x7f0cd2ffd700 (LWP 74875) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  57   Thread 0x7f0cd27fc700 (LWP 74876) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  56   Thread 0x7f0cd1ffb700 (LWP 74877) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  55   Thread 0x7f0cd17fa700 (LWP 74878) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  54   Thread 0x7f0cd0ff9700 (LWP 74879) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  53   Thread 0x7f0cd07f8700 (LWP 74880) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  52   Thread 0x7f0ccfff7700 (LWP 74881) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  51   Thread 0x7f0ccf7f6700 (LWP 74882) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  50   Thread 0x7f0cceff5700 (LWP 74883) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  49   Thread 0x7f0cce7f4700 (LWP 74884) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  48   Thread 0x7f0ccdff3700 (LWP 74885) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  47   Thread 0x7f0ccd7f2700 (LWP 74886) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  46   Thread 0x7f0cccff1700 (LWP 74887) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  45   Thread 0x7f0cc7fff700 (LWP 74888) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  44   Thread 0x7f0cc77fe700 (LWP 74889) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  43   Thread 0x7f0cc6ffd700 (LWP 74890) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  42   Thread 0x7f0cc67fc700 (LWP 74891) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  41   Thread 0x7f0cc5ffb700 (LWP 74892) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  40   Thread 0x7f0cc57fa700 (LWP 74893) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  39   Thread 0x7f0cc4ff9700 (LWP 74894) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  38   Thread 0x7f0cb7fff700 (LWP 74895) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  37   Thread 0x7f0cb77fe700 (LWP 74896) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  36   Thread 0x7f0cb6ffd700 (LWP 74897) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
---Type <return> to continue, or q <return> to quit---
  35   Thread 0x7f0cb67fc700 (LWP 74898) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  34   Thread 0x7f0cb5ffb700 (LWP 74899) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  33   Thread 0x7f0cb57fa700 (LWP 74900) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  32   Thread 0x7f0cb4ff9700 (LWP 74901) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  31   Thread 0x7f0caffff700 (LWP 74902) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  30   Thread 0x7f0caf7fe700 (LWP 74903) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  29   Thread 0x7f0caeffd700 (LWP 74904) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  28   Thread 0x7f0cae7fc700 (LWP 74905) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  27   Thread 0x7f0cadffb700 (LWP 74906) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  26   Thread 0x7f0cad7fa700 (LWP 74907) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  25   Thread 0x7f0cacff9700 (LWP 74908) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  24   Thread 0x7f0ca7fff700 (LWP 74909) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  23   Thread 0x7f0ca77fe700 (LWP 74910) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  22   Thread 0x7f0ca6ffd700 (LWP 74911) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  21   Thread 0x7f0ca67fc700 (LWP 74912) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  20   Thread 0x7f0ca5ffb700 (LWP 74913) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  19   Thread 0x7f0ca57fa700 (LWP 74914) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  18   Thread 0x7f0ca4ff9700 (LWP 74915) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  17   Thread 0x7f0ca47f8700 (LWP 74916) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  16   Thread 0x7f0ca3ff7700 (LWP 74917) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  15   Thread 0x7f0ca37f6700 (LWP 74918) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  14   Thread 0x7f0ca2ff5700 (LWP 74919) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  13   Thread 0x7f0ca27f4700 (LWP 74920) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  12   Thread 0x7f0ca1ff3700 (LWP 74921) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  11   Thread 0x7f0ca17f2700 (LWP 74922) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  10   Thread 0x7f0ca0ff1700 (LWP 74923) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  9    Thread 0x7f0c9bfff700 (LWP 74924) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  8    Thread 0x7f0c9b7fe700 (LWP 74925) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  7    Thread 0x7f0c9affd700 (LWP 74926) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  6    Thread 0x7f0c9a7fc700 (LWP 74927) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  5    Thread 0x7f0c99ffb700 (LWP 74928) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  4    Thread 0x7f0c997fa700 (LWP 74929) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  3    Thread 0x7f0c98ff9700 (LWP 74930) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  2    Thread 0x7f0c987f8700 (LWP 74931) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
* 1    Thread 0x7f0d0ad4c780 (LWP 74835) ""label_image"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
(gdb) thread 53
[Switching to thread 53 (Thread 0x7f0cd07f8700 (LWP 74880))]
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
185 in ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S
(gdb) bt
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00007f0d09fb94bc in __gthread_cond_wait (__mutex=<optimized out>, __cond=<optimized out>)
    at /build/buildd/gcc-4.8-4.8.4/build/x86_64-linux-gnu/libstdc++-v3/include/x86_64-linux-gnu/bits/gthr-default.h:864
#2  std::condition_variable::wait (this=<optimized out>, __lock=...) at ../../../../../src/libstdc++-v3/src/c++11/condition_variable.cc:52
#3  0x00000000004464fc in Eigen::Notification::WaitForNotification() ()
#4  0x000000000064f49a in void Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16> const> const> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16> const> const> const, Eigen::ThreadPoolDevice>::packRhsAndKernel<Eigen::internal::packRhsAndKernelArg<float, float, Eigen::internal::TensorContractionInputMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16> const> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 4, true, false, 0>, Eigen::internal::blas_data_mapper<float, long, 0, 0>, long>, Eigen::internal::gemm_pack_rhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16> const> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 4, true, false, 0>, 4, 0, false, false>, Eigen::internal::gebp_kernel<float, float, long, Eigen::internal::blas_data_mapper<float, long, 0, 0>, 8, 4, false, false> >(Eigen::internal::packRhsAndKernelArg<float, float, Eigen::internal::TensorContractionInputMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16> const> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 4, true, false, 0>, Eigen::internal::blas_data_mapper<float, long, 0, 0>, long>) ()
#5  0x0000000000634b1f in std::_Function_handler<void (), std::_Bind<void (*(Eigen::internal::packRhsAndKernelArg<float, float, Eigen::internal::TensorContractionInputMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16> const> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 4, true, false, 0>, Eigen::internal::blas_data_mapper<float, long, 0, 0>, long>))(Eigen::internal::packRhsAndKernelArg<float, float, Eigen::internal::TensorContractionInputMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16> const> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 4, true, false, 0>, Eigen::internal::blas_data_mapper<float, long, 0, 0>, long>)> >::_M_invoke(std::_Any_data const&) ()
#6  0x0000000000e44cef in std::_Function_handler<void (), tensorflow::thread::ThreadPoolDefaultImpl::ThreadPoolDefaultImpl(tensorflow::Env*, tensorflow::ThreadOptions const&, std::string const&, int)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()
#7  0x00007f0d09fbca40 in std::(anonymous namespace)::execute_native_thread_routine (__p=<optimized out>) at ../../../../../src/libstdc++-v3/src/c++11/thread.cc:84
#8  0x00007f0d0a217184 in start_thread (arg=0x7f0cd07f8700) at pthread_create.c:312
#9  0x00007f0d09a2a34d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111
```

This come up during testing of a faster thread pool implementation that has distributed queues and so does not preserve FIFO order. But I think it can come up with current pool as well (maybe after some unrelated code changes, or maybe just due to unlucky scheduling order).

Computational tasks running on a fixed-size thread pool must not ever block. Instead they should schedule continuations. Besides deadlocks blocking leads to serious CPU underutilization. I.e. on my machine label_image utilizes only about half of cores, because half of threads in pool are blocked waiting for other tasks.
"
927,cannot compile pip package after update to the HEAD,"error as follows:

$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

ERROR:./tensorflow/models/embedding/BUILD:10:6: First argument of load() is a path, not a label. It should start with a single slash if it is an absolute path..
ERROR: ./tensorflow/models/embedding/BUILD:10:6: file '/tensorflow:tensorflow.bzl.bzl' was not correctly loaded. Make sure the 'load' statement appears in the global scope in your file.
ERROR: ./tensorflow/models/embedding/BUILD:104:1: name 'tf_gen_op_wrapper_py' is not defined.
ERROR: ./tensorflow/tools/pip_package/BUILD:13:1: Target '//tensorflow/models/embedding:package' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:build_pip_package'.
ERROR: Loading failed; build aborted.

which work well before update to the Newest version.
"
925,Android demo crashes on Android 6 devices,"When I tried to run the Android demo, it instantly crashed when I opened the app.
The CameraConnectionFragment doesn't handle the runtime permission for camera.

The code needs to handle the runtime permission.

```
01-29 17:23:43.694 15962-15962/org.tensorflow.demo E/AndroidRuntime: FATAL EXCEPTION: main
Process: org.tensorflow.demo, PID: 15962
java.lang.SecurityException
    at android.os.BinderProxy.transactNative(Native Method)
    at android.os.BinderProxy.transact(Binder.java:503)
    at android.hardware.ICameraService$Stub$Proxy.connectDevice(ICameraService.java:364)
    at java.lang.reflect.Method.invoke(Native Method)
    at android.hardware.camera2.utils.Decorator.invoke(Decorator.java:80)
    at java.lang.reflect.Proxy.invoke(Proxy.java:393)
    at $Proxy0.connectDevice(Unknown Source)
    at android.hardware.camera2.CameraManager.openCameraDeviceUserAsync(CameraManager.java:321)
    at android.hardware.camera2.CameraManager.openCamera(CameraManager.java:457)
    at org.tensorflow.demo.CameraConnectionFragment.openCamera(CameraConnectionFragment.java:380)
    at org.tensorflow.demo.CameraConnectionFragment.access$000(CameraConnectionFragment.java:63)
    at org.tensorflow.demo.CameraConnectionFragment$1.onSurfaceTextureAvailable(CameraConnectionFragment.java:96)
    at android.view.TextureView.getHardwareLayer(TextureView.java:368)
    at android.view.View.updateDisplayListIfDirty(View.java:15151)
    at android.view.View.draw(View.java:15948)
    at android.view.ViewGroup.drawChild(ViewGroup.java:3609)
    at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3399)
    at android.view.View.updateDisplayListIfDirty(View.java:15169)
    at android.view.View.draw(View.java:15948)
    at android.view.ViewGroup.drawChild(ViewGroup.java:3609)
    at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3399)
    at android.view.View.draw(View.java:16181)
    at android.view.View.updateDisplayListIfDirty(View.java:15174)
    at android.view.View.draw(View.java:15948)
    at android.view.ViewGroup.drawChild(ViewGroup.java:3609)
    at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3399)
    at android.view.View.updateDisplayListIfDirty(View.java:15169)
    at android.view.View.draw(View.java:15948)
    at android.view.ViewGroup.drawChild(ViewGroup.java:3609)
    at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3399)
    at android.view.View.updateDisplayListIfDirty(View.java:15169)
    at android.view.View.draw(View.java:15948)
    at android.view.ViewGroup.drawChild(ViewGroup.java:3609)
    at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3399)
    at android.view.View.draw(View.java:16181)
    at com.android.internal.policy.PhoneWindow$DecorView.draw(PhoneWindow.java:2690)
    at android.view.View.updateDisplayListIfDirty(View.java:15174)
    at android.view.ThreadedRenderer.updateViewTreeDisplayList(ThreadedRenderer.java:281)
    at android.view.ThreadedRenderer.updateRootDisplayList(ThreadedRenderer.java:287)
    at android.view.ThreadedRenderer.draw(ThreadedRenderer.java:322)
    at android.view.ViewRootImpl.draw(ViewRootImpl.java:2615)
    at android.view.ViewRootImpl.performDraw(ViewRootImpl.java:2434)
    at android.view.ViewRootImpl.performTraversals(ViewRootImpl.java:2067)
    at android.view.ViewRootImpl.doTraversal(ViewRootImpl.java:1107)
    at android.view.ViewRootImpl$TraversalRunnable.run(ViewRootImpl.java:6013)
    at android.view.Choreographer$CallbackRecord.run(Choreographer.java:858)
    at android.view.Choreographer.doCallbacks(Choreographer.java:670)
    at android.view.Choreographer.doFrame(Choreographer.java:606)
    at android.view.Choreographer$FrameDisplayEventReceiver.run(Choreographer.java:844)
    at android.os.Handler.handleCallback(Handler.java:739)
    at android.os.Handler.dispatchMessage(Handler.java:95)
    at android.os.Looper.loop(Looper.java:148)
    at android.app.ActivityThread.main(ActivityThread.java:5417)
    at java.lang.reflect.Method.invoke(Native Method)
    at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:726)
    at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:616)
```
"
924,git clone hangs when building from source,"$bazel build -c opt --config=cuda --spawn_strategy=standalone --verbose_failures //tensorflow/tools/pip_package:build_pip_package
..........
[4 / 4] Cloning https://github.com/Polymer/polymer.git: Resolving deltas (13948 / 15071)

it hangs here for a few hours.
"
923,Out of range: FIFOQueue '_0_fifo_queue',"When I test  [cifar10_input_test.py](tensorflow/tensorflow/models/image/cifar10/cifar10_input_test.py).I got the following information, how could I solve this error.

```
W tensorflow/core/common_runtime/executor.cc:1052] 0x1fe5ea0 Compute status: Out of range: FIFOQueue '_0_fifo_queue' is closed and has insufficient elements (requested 1, current size 0)
     [[Node: ReaderRead = ReaderRead[_device=""/job:localhost/replica:0/task:0/cpu:0""](FixedLengthRecordReader, fifo_queue)]]
```
"
921,tensorflow.Session() Hangs on 24-Logical Core Machine,"We have a newish (<6 months old) server with two hyperthreaded 6-core processors, and when we try to create a tensorflow.Session(), it hangs indefinitely in what `top` reports is state D (uninterruptible sleep), unresponsive even to SIGKILL. Relevant output is below.

```
>>> sess = tf.Session() 
I tensorflow/core/common_runtime/local_device.cc:40] Local device intra op parallelism threads: 24
[...indefinite hang...]
```
"
918,bug in machine translation example - learning rate randomly changes,"I'm running into a strange issue with translate.py. I'm getting NaNs in the learning rate, or at least that's what my debug printouts suggest. 

My code is bleeding edge (5572b1a9205d94a0de8dc843f51197ce8bbedf7a) and unmodified except I added print statements like this at the start, and after each model.get_batch and model.step call:
print('got a batch: lr = %.9f'% model.learning_rate.eval())

Each run is different, but here are is one example:

CUDA_VISIBLE_DEVICES=1 python translate.py --data_dir enfr_data --train_dir foo --en_vocab_size=50000 --fr_vocab_size=50000 --size=600 --num_layers=3 --batch_size 128 --steps_per_checkpoint 200 --max_train_data_size 110000
...

```
Creating 3 layers of 600 units.
Created model with fresh parameters.
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:73] Allocating 10.57GiB bytes.
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:83] GPU 0 memory begins at 0x2047a0000 extends to 0x4a8b24400
before loop: lr=0.500000000
Reading development and training data (limit: 110000).
  reading data line 100000
got a batch: lr = 0.500000000
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 7642 get requests, put_count=3359 evicted_count=1000 eviction_rate=0.297708 and unsatisfied allocation rate=0.704397
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110
did a step: lr = 0.500000000
got a batch: lr = 0.500000000
did a step: lr = 0.500000000
got a batch: lr = 0.500000000
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 22 get requests, put_count=2034 evicted_count=2000 eviction_rate=0.983284 and unsatisfied allocation rate=0
did a step: lr = 0.500000000
got a batch: lr = 0.500000000
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 46 get requests, put_count=7059 evicted_count=7000 eviction_rate=0.991642 and unsatisfied allocation rate=0
did a step: lr = 0.967365444
got a batch: lr = 0.967365444
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 44 get requests, put_count=6060 evicted_count=6000 eviction_rate=0.990099 and unsatisfied allocation rate=0
did a step: lr = nan
got a batch: lr = nan
did a step: lr = nan
got a batch: lr = nan
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 6 get requests, put_count=3025 evicted_count=3000 eviction_rate=0.991736 and unsatisfied allocation rate=0
```

And here is another:

CUDA_VISIBLE_DEVICES=1 python translate.py --data_dir enfr_data --train_dir foo --en_vocab_size=50000 --fr_vocab_size=50000 --size=600 --num_layers=3 --batch_size 128 --steps_per_checkpoint 2 --max_train_data_size 110000

```

Creating 3 layers of 600 units.
Created model with fresh parameters.
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:73] Allocating 10.57GiB bytes.
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:83] GPU 0 memory begins at 0x2047a0000 extends to 0x4a8b24400
before loop: lr=0.500000000
Reading development and training data (limit: 110000).
  reading data line 100000
got a batch: lr = 0.500000000
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 7642 get requests, put_count=3316 evicted_count=1000 eviction_rate=0.301568 and unsatisfied allocation rate=0.710024
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110
did a step: lr = 0.500000000
got a batch: lr = 0.500000000
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 8 get requests, put_count=5019 evicted_count=5000 eviction_rate=0.996214 and unsatisfied allocation rate=0
did a step: lr = 0.000000000
global step 2 learning rate 0.0000 step-time 4.49 perplexity 14766.28
  eval: bucket 0 perplexity inf
  eval: bucket 1 perplexity inf
  eval: bucket 2 perplexity inf
  eval: bucket 3 perplexity inf
```

Can anyone else replicate this, or have any idea how to fix it? Seems like a buffer overflow...
"
917,Sequence-Wise Batch Normalization for RNN's,"Hey TF,

Recently, for deep RNN's, sequence wise batch normalization has proven to be very helpful. [Deep Speech 2](http://arxiv.org/pdf/1512.02595v1.pdf) in section 3.2 explains this in more detail. Sequence-wise batch normalization is described in section 4.1 in [Batch Normalized RNNs](http://arxiv.org/pdf/1510.01378v1.pdf).

`tf.nn.moments` is very useful for batch normalization because it gives you the mean and variance. However, in seq2seq setups, we need to do a batch normalization across all timesteps (the entire sequence). Unfortunately, the way `seq2seq.py` is written, each timestep is within a separate 2D matrix. This means that there is a list of 2d tensors for the entire sequence.

If somehow, `tf.nn.moments` could be modified to accept lists of tensors (all of the same dimensions), it would be incredibly helpful. This way, we could input the entire list of tensors, and compute the resulting mean and variance for that sequence. 

Thanks!
"
916,a CUDA runtime call was likely performed without using a StreamExecutor context,"I want use my pylearn2 dataset code with tensorflow. But when I add `from pylearn2.datasets.dense_design_matrix import DefaultViewConverter` into mnist convolutional.py and run it in my server, I get this errors:

`F tensorflow/stream_executor/cuda/cuda_driver.cc:302] current context was not created by the StreamExecutor cuda_driver API: 0x38e0330; a CUDA runtime call was likely performed without using a StreamExecutor context
Aborted (core dumped)`

My server is GTX980 with 8 core CPU, I will get that errors in my server. But I can run it in my desktop PC with GTX960/GTX970 and 4 core CPU and I don't get any errors. All the cuda version is same in my machices.
Does anyone can help me? 

the full traceback is:

```
I tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcublas.so.7.0 locally
I tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcudnn.so.6.5 locally
I tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcufft.so.7.0 locally
I tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcuda.so locally
I tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcurand.so.7.0 locally
Using gpu device 0: GeForce GTX 980 (CNMeM is disabled)
/home/b3432/ShareCache/houzhi/data
train-images-idx3-ubyte.gz
/home/b3432/ShareCache/houzhi/data
train-labels-idx1-ubyte.gz
/home/b3432/ShareCache/houzhi/data
t10k-images-idx3-ubyte.gz
/home/b3432/ShareCache/houzhi/data
t10k-labels-idx1-ubyte.gz
Extracting /home/b3432/ShareCache/houzhi/data/train-images-idx3-ubyte.gz
Extracting /home/b3432/ShareCache/houzhi/data/train-labels-idx1-ubyte.gz
Extracting /home/b3432/ShareCache/houzhi/data/t10k-images-idx3-ubyte.gz
Extracting /home/b3432/ShareCache/houzhi/data/t10k-labels-idx1-ubyte.gz
I tensorflow/core/common_runtime/local_device.cc:40] Local device intra op parallelism threads: 8
F tensorflow/stream_executor/cuda/cuda_driver.cc:302] current context was not created by the StreamExecutor cuda_driver API: 0x38e0330; a CUDA runtime call was likely performed without using a StreamExecutor context
Aborted (core dumped)
```
"
914,label_image example failing due to missing types.h,"```
In file included from tensorflow/examples/label_image/command_line_flags.cc:16:0:
./tensorflow/examples/label_image/command_line_flags.h:20:44: fatal error: tensorflow/core/platform/types.h: No such file or directory
 #include ""tensorflow/core/platform/types.h""
```

there is no such file.  there is a `tensorflow/core/framework/types.h`
"
913,"First assignment on Google Deep Learning, Exception raised on download","This is the first time taking a course on udacity. With atmost interest opened up the first assignment and ended up in Exception:
The screenshot attached
![screenshot from 2016-01-28 10 17 30](https://cloud.githubusercontent.com/assets/16772566/12635403/c45f8344-c5a8-11e5-9aa2-bb641a451d60.png)
"
912,"""Cannot allocate memory"" error on first Udacity assignment","![screenshot](http://i.imgur.com/r1PTe4C.png)

Though, as the screenshot shows, I have allocated 5GB of memory to the Docker container running [the course's first Jupyter notebook](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/1_notmnist.ipynb), I can't seem to get past this cell because of Python's `cannot allocate memory` error. Sometimes it happens with `notMNIST_large/B` and sometimes with `notMNIST_large/C`. I don't see why this error is thrown when the container reaches 1.4GB, since it should be able to go up to 5GB...

I doubt this is a problem with TensorFlow but, since it's part of the course, I figured this issue might be helpful to other students as well.

This is the Docker command I'm running (differs from [the original command](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/udacity#running-the-docker-container-from-the-google-cloud-repository) in that this gets rid of `--rm` and adds `--memory=5gb`):

``` shell
docker run --name udacity_deep_learning --memory=5g -p 8888:8888 -it b.gcr.io/tensorflow-udacity/assignments
```
"
911,MNIST 0.6.0 Tutorial Documentation bug,"At this URL:

https://www.tensorflow.org/versions/0.6.0/tutorials/mnist/beginners/index.html

The 2nd line of syntax is wrong:

`mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)`

It should be:

`mnist = tensorflow.examples.tutorials.mnist.input_data.read_data_sets(""MNIST_data/"", one_hot=True)`
"
910,Error downloading jpegsrc.v9a.tar.gz,"I'm getting the following build error:

```
$ bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer
ERROR:  tensorflow/tensorflow/core/platform/default/build_config/BUILD:36:1: 
no such package '@jpeg_archive//': 
Error downloading from http://www.ijg.org/files/jpegsrc.v9a.tar.gz
```

It turns out that bazel cannot download directly from the `ijg.org` in my network, due to security restrictions. However, I am able to download the file by hand. 

I tried just putting it into the required `.cache/bazel` location by hand and run the bazel command again. This used to work, but it won't work in the latest version, and the same error comes up again.

Would it be possible to enable bazel to pick up a locally downloaded `jpegsrc.v9a.tar.gz` file?

Or, if I don't work on JPEGs, how do I get rid of this dependency altogether?
"
908,Visualizing CNNs with TensorBoard,"I am a huge fan of Tensorboard visualization however in my opinion it may lack good documentation when the user wants to implement non trivial visualization for example visualizing weights for a CNNs (or even features) (and also on how everything is stored and processed but that is another question).  
I am runing a CNN on a set of color images the structure and the code of my cnn is almost entirely based on Deep MNIST tutorial/MNIST+tensorboard tutorial.
I have several problems :
I want to run tf.image_summary(W_conv1) every 100 steps/epochs of the training with W_conv1 being of shape (3,5,5,32) (first conv layer after an RGB image)
However the desired shape is (32,5,5,3) so I reshape using `tf.reshape(W_conv1;shape=(32,5,5,3))`
I am only using reshape because I could not find a numpy swapaxes equivalent in tf (and from the definition of reshape in numpy doc I am not sure wether or not it does exactly what I wanted but I agree it is my problem...).
I call a `merge_all_summaries()` that I feed every 100 steps.
I run 1000 epochs so I should get 320 images, instead I get 2 or 3 actually displayed !!!
On stackoverflow I found out that even if all the images are loaded into the memory (where?!) it does not display them all and that I should do something like initialize an empty tf.summary() and call `.MergefromString(image_summary)` but I am confused on exactly where in the code I should do that and at what time ?
My second issue is I also want to see the weights for the second conv layer which is of shape (32,5,5,64) so I would like to split it into (32x64) 5x5 images does that seem reasonnable ?
How would I go about displaying them ?
Same for features obtained at different layers of the CNN ?
I understand my questions are trivial but I think a lot of people would benefit (I would certainly) from an example of a clean and efficient code in tensorflow of a CNN on whatever image dataset WITH tensorboard visualization of the weights/features.
Thanks for the work TF is amazing !
"
905,ValueError: All shapes must be fully defined.,"Hi,
I am trying to create a TFRecord file where each picture is saved by its dimensions, label and image content. 
Since every picture has different dimensions, so it is necessary to read the info from the file and reshape them. When I did something as the following:

features = tf.parse_single_example(
      serialized_example,
      dense_keys=['image_raw', 'label', 'height', 'width', 'depth'],
      # Defaults are not specified since both keys are required.
      dense_types=[tf.string, tf.int64, tf.int64, tf.int64, tf.int64])

  image = tf.decode_raw(features['image_raw'], tf.uint8)
  height = tf.cast(features['height'], tf.int32)
  width = tf.cast(features['width'], tf.int32)
  depth = tf.cast(features['depth'], tf.int32)

  im_shape = tf.pack([height, width, depth])
  new_image = tf.reshape(image, im_shape)

I got the error ""ValueError: All shapes must be fully defined."" Is this because I used the tensors representing dimensions instead of specific numbers? I have searched through the internet, all the tutorials and discussions are about reading fixed sized images. I do think reading various sized images is important. Any solutions to this issue? I would really appreciate it. 
"
903,Wrong Momentum Equation in Tensorflow's Kernel,"I believe I found a slight error in the multiplication of the learning rate

Momentum Optimization has the form:
![brahms_cpmc_columbia_edu_publications_momentum_pdf](https://cloud.githubusercontent.com/assets/5315899/12606885/98bc9276-c496-11e5-8bfc-0b887bfaef71.png)
Where epsilon is the learning rate and p is the momentum.
Reference: Ning Qian's Momentum Paper (1999) http://brahms.cpmc.columbia.edu/publications/momentum.pdf

In tensorflow/core/kernel/training_ops.cc, lines 68 and 69 (ApplyMomentum) we find:

``` c++
accum = accum * momentum() + grad;
var -= accum * lr();
```

Expanding this, we get:

``` c++
var -= (accum*momentum() + grad) * lr();
// =>
var -= accum*momentum()*lr() + grad*lr();
```

Notice how the momentum variable is being multiplied by the learning rate, but this should not be the case. The correct form is:

``` c++
accum = accum * momentum() + grad * lr();
var -= accum;
```

Here's my pull request fixing this: #904 
"
899,"MatMul ""flops"" statistic looks incorrect","I think the flops estimate for MatMul is incorrect. In a vanilla matrix multiply AB=C with dimensions `shape_A=(i,j)`, `shape_B=(j,k)`, and `shape_C=(i,k)`, we should get ~ `2*i*j*k`. It appears `_calc_mat_mul_flops` is giving `shape_A[0] * prod(shape_C)`, or `2*i*i*k` for an un-transposed multiply, ignoring the internal dimensions. I believe you meant `shape_A[0] * prod(shape_B)`. Or if it helps,

```
weights_shape = graph_util.tensor_shape_from_node_def_name(graph,node.input[1])
weights_shape.assert_is_fully_defined()
weight_count = np.prod(weight_shape.as_list())
return ops.OpStats(""flops"", (k * weight_count * 2))
```

Also, here's a minimum working example that demonstrates the problem.

```
import tensorflow as tf
import tensorflow.python.framework.ops as ops 
g = tf.Graph()
with g.as_default():
  A = tf.Variable(tf.random_normal( [25,16] ))
  B = tf.Variable(tf.random_normal( [16,9] ))
  C = tf.matmul(A,B) # shape=[25,9]
for op in g.get_operations():
  flops = ops.get_stats_for_node_def(g, op.node_def, 'flops').value
  if flops is not None:
    print 'Flops should be ~',2*25*16*9
    print '25 x 25 x 9 would be',2*25*25*9 # ignores internal dim, repeats first
    print 'TF stats gives',flops
```

And it's output:

```
Flops should be ~ 7200
25 x 25 x 9 would be 11250
TF stats gives 11250
```
"
898,HEAD does not load the inception-v3 graph def,"The code:

```
graph_def = tf.GraphDef()
graph_def.ParseFromString(open(PB_PATH).read())
```

The error:

```
google.protobuf.message.DecodeError: Error parsing message
```

The SHA: `f2a42d48a885ce8956c3ea69ef900cf484a6e873`

Thanks.
"
897,gradient of tf.floor,"``` python
import numpy as np
import tensorflow as tf
f = np.array([3.8], dtype='float32')
vif = tf.placeholder(tf.float32, shape=[1])

#out = vif + tf.cast(tf.cast(vif, tf.int32), tf.float32)
out = vif + tf.floor(vif)
grad = tf.gradients(tf.reduce_sum(out), vif)

sess = tf.Session()
sess.run(tf.initialize_all_variables())
print sess.run(grad, feed_dict={vif: f})  # print 2

import theano
import theano.tensor as T
vif = T.fvector()
out = vif + T.floor(vif)
grad = T.grad(out.sum(), vif)
func = theano.function([vif], grad)
print func(f) # print 1
```

It looks like the gradient of `tf.floor(x)` w.r.t `x` is 1. But I'm expecting it to be 0, as in theano.
Right now I could use a double casting as a work around, but why is it different?
"
896,Missing --recurse-submodules flag in ci_build instructions,"In the file https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/ci_build/README.md, the git clone command is missing the --recurse-submodules option flag, which will cause Bazel builds to fail on users' machines.
"
890,"""ImportError: No module named protobuf"" on El Capitan with VirtualEnv","Hello, 
When I try importing tensorflow in Python I get ""ImportError: No module named protobuf"".

I followed instructions from here https://www.tensorflow.org/versions/0.6.0/get_started/os_setup.html#virtualenv_install and also updated protobuf:

brew reinstall --devel protobuf

The version I have is 3.0.0a3

I looked at the issues here and all remedies don't seem to work. 

Any clue?

Thx
"
889,Large Strides for 1x1 Convolutions,"When a conv2d operator has a stride greater than the kernel size, the following error is thrown:
`ValueError: ('stride must be less than or equal to filter size', 'stride: [2x2] filter: [Dimension(1)xDimension(1)]')`
This makes implementing the 1x1 convolutions used to reduce spatial resolution in several papers (MSRA 2015 among others) awkward. Also, a convolution with stride greater than kernel size may be unusual but is it still well defined. Fixing this may be as simple as removing this assertion.
Thanks to everyone who developed TensorFlow, it's a fascinating tool.
"
888,multiple GPU training out of memory ,"When I run the cifar10_mutil_gpu_train.py example with 2 gpus, I got error message like the below, does this mean there is out of memory problem? How to solve or work around this?
My GPU is K80 so there are two K40 actually. 

PoolAllocator: After 2573 get requests, put_count=2268 evicted_count=1000 eviction_rate=0.440917 and unsatisfied allocation rate=0.546055
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110
"
883,Build fails on Mac OSX 10.11.3 with `no type  named 'Scalar' in...`,"Using the most recent commit ""a27d844e05447e65aa279ae5269a2d75590f46f6"" on Jan 25, 2016, the build fails with 

```
./external/eigen_archive/eigen-eigen-
c8e5d094f3a9/unsupported/Eigen/CXX11/src/Tensor/TensorContractionBlocking.h:29:31: error: no type
 named 'Scalar' in 'Eigen::internal::TensorContractionInputMapper<float, long, 0,
 Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long, 2>, const 
Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long>, 
16> > >, Eigen::ThreadPoolDevice>, Eigen::array<long, 1>, Eigen::array<long, 1>, 4, true, true, 0>'
  typedef typename RhsMapper::Scalar RhsScalar;
```

using an older commit from Jan 21, 2016  ""e14090d217fc4e7e49ac04ccbc50acdba8b9f120"" completes normally.
"
881,Git branch 0.6.0 not showing,"Hi,
I've cloned the tensorflow repository (git clone https://github.com/tensorflow/tensorflow.git)
and when I do 
git branch -a 
it only appears master branch (\* master) instead of 
*master
0.6.0 
as i though it should appear 
Can someone please help me?
Thanks
"
880,Unable to run example convolutional.py,"Hello,
I did the three differents installations
based on pip
based on virtualenv 
based on docker.
i am on Virtual Machine (VM Virtual Box) with Ubuntu.

and i have these errors when trying to run :

root@b6782358bd01:/usr/local/lib/python2.7/dist-packages/tensorflow/models/image/mnist# python -m tensorflow.models.image.mnist.convolutional
Traceback (most recent call last):
  File ""/usr/lib/python2.7/runpy.py"", line 162, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
  File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code
    exec code in run_globals
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/models/image/mnist/convolutional.py"", line 290, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/default/_app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/models/image/mnist/convolutional.py"", line 121, in main
    train_data_filename = maybe_download('train-images-idx3-ubyte.gz')
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/models/image/mnist/convolutional.py"", line 59, in maybe_download
    filepath, _ = urllib.request.urlretrieve(SOURCE_URL + filename, filepath)
  File ""/usr/lib/python2.7/urllib.py"", line 94, in urlretrieve
    return _urlopener.retrieve(url, filename, reporthook, data)
  File ""/usr/lib/python2.7/urllib.py"", line 240, in retrieve
    fp = self.open(url, data)
  File ""/usr/lib/python2.7/urllib.py"", line 208, in open
    return getattr(self, name)(url)
  File ""/usr/lib/python2.7/urllib.py"", line 352, in open_http
    'got a bad status line', None)
IOError: ('http protocol error', 0, 'got a bad status line', None)

i did not suceed with the three methods. can anyone who has found these issues help me. Thanks in advance. Best Regards.
"
878,Tensorflow installer,"Hi TensorFlow Team,

I would like to propose a simple bash script to automate the installation of required tools(based on os_setup.md instructions), I think it will help many newbies. It is available under :  https://github.com/bellaj/tensorflow/blob/master/setup_Tensor.sh
This script will help users to install Tensorflow for many Linux distributions .

I have tested it for ubuntu 64, and it works fine.
I am working now on GPU support

any feedback is welcome
thank you
"
877,word2vec.py error,"I download the tensorflow-master version.

When I execute the word2vec.py on pycharm, it shows the error:

from tensorflow.models.embedding import gen_word2vec as word2vec
ImportError: No module named embedding

In **init**.py, I see ""Import generated word2vec optimized ops into embedding package.""

Does it means it needs a file about ""generated word2vec optimized ops""? But I can' t find it. How can I fix it?

The environment is Mac OS X, and I install tensorflow in this way: $ pip install https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl.
"
876,How to set weight cost strength in TensorFlow,"When i use Momentum Gradient Descent, how to set weight cost strength?( The λ in this formula.)
![img](http://i.stack.imgur.com/305RE.jpg)
"
875,GPU device not found on OSX,"Hello,

I'm trying to install TensorFlow with GPU support for a CUDA Capability 3.0 device. I have followed the instructions found here: https://gist.github.com/Mistobaan/dd32287eeb6859c6668d, and everything compiles without error, but TensorFlow doesn't recognize my GPU device. The steps that I'm taking are below, as is the error output.

I've modified bazel to use gcc-4.8, so these are the commands I'm running:

```
bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer ---crosstool_top=//tools/cpp:toolchain
bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --crosstool_top=//tools/cpp:toolchain
```

Running the example with:

```
bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu
```

produces the following output:

```
F tensorflow/cc/tutorials/example_trainer.cc:118] Check failed: ::tensorflow::Status::OK() == (session->Run({{""x"", x}}, {""y:0"", ""y_normalized:0""}, {}, &outputs)) (OK vs. Invalid argument: Cannot assign a device to node 'Const/_2': Could not satisfy explicit device specification '/gpu:0'
     [[Node: Const/_2 = Const[dtype=DT_INT32, value=Tensor<type: int32 shape: [] values: 0>, _device=""/gpu:0""]()]])
F tensorflow/cc/tutorials/example_trainer.cc:118] Check failed: ::tensorflow::Status::OK() == (session->Run({{""x"", x}}, {""y:0"", ""y_normalized:0""}, {}, &outputs)) (OK vs. Invalid argument: Cannot assign a device to node 'Const/_2': Could not satisfy explicit device specification '/gpu:0'
     [[Node: Const/_2 = Const[dtype=DT_INT32, value=Tensor<type: in
```

I can also confirm that after installing the pip package and running a test script, no devices are found.

A few things to note:
- Running deviceQuery confirms that a CUDA device (Device 0: GeForce GT 650M) is found. 
- I've made sure to run bazel clean before compiling, and have tried compiling using sudo just to see if this was a permissions issue.

Thanks in advance for your help. 
"
870,2d gradients have a second dimension of size None,"When calling compute_gradients with 2d variables, the first dimension has the correct size while the second dimension has a size of None.  Is this intentional?  It makes it somewhat difficult when processing the gradients, as a dimension size of None isn't really compatible with anything else.

Tested with:
2d variables - second dimension is size None, first dimension is size of first dimension of variable
1d variables - dimension is size of dimension of variable
Tested using compute_gradients in AdamOptimizer
"
869,Error in description of outputs of rnn_decoder,"rnn_decoder is described as follows:

```
def rnn_decoder(decoder_inputs, initial_state, cell, loop_function=None,
                scope=None):
  """"""RNN decoder for the sequence-to-sequence model.
...
Returns:
...
    states: The state of each cell in each time-step. This is a list with
      length len(decoder_inputs) -- one item for each time-step.
```

States output does not appear to have the length as the decoder inputs. For example, the following script --

```
import tensorflow as tf
from tensorflow.models.rnn import rnn_cell
from tensorflow.models.rnn import seq2seq
batch_size = 4
seq_len = 5
num_layers = 3
vocab_size = 26
rnn_size = 100
cell = rnn_cell.MultiRNNCell([rnn_cell.BasicLSTMCell(rnn_size)] * num_layers)
input_data = tf.placeholder(tf.int32, [batch_size, seq_len])
embedding = tf.get_variable(""embedding"", [vocab_size, rnn_size])
decoder_inputs = tf.split(1, seq_len, tf.nn.embedding_lookup(embedding, input_data))
decoder_inputs = [tf.squeeze(input_, [1]) for input_ in decoder_inputs]
outputs, states = seq2seq.rnn_decoder(decoder_inputs, cell.zero_state(batch_size, tf.float32), cell)
print ""Length of states is {}, but length of decoder inputs is {}"".format(len(states), len(decoder_inputs))
```

gives the following output:

```
Length of states is 6, but length of decoder inputs is 5
```

This is still true if you replace BasicLSTMCell with GRUCell.

Is it correct to say that the length of states is equal to _one greater than_ the length of decoder inputs? Seems to be true from playing around with it. Is this a fencepost error, where the states include the zeroth state _plus_ the state following each decoder input?
"
868,Udacity not_mnist notebook broken double quotes,"When trying to run tensorflow/examples/udacity/1_notmnist.ipynb in my ipython notebook (tried versions 4.0 and 3.2.1), I got a NotJSONError:

[W 22:14:13.940 NotebookApp] Unreadable Notebook: /home/pzelasko/tensorflow/tensorflow/examples/udacity/1_notmnist.ipynb NotJSONError('Notebook does not appear to be JSON: u\'{\n  ""worksheets"": [\n    {\n      ""ce...',)

I found the culprit by loading the file using json module in python:

In [11]: s = open('1_notmnist.ipynb').read()
In [12]: json.loads(s)
(...)
ValueError: Expecting , delimiter: line 187 column 431 (char 9558)

It turned out to be non-escaped double quotes inside json string. After chaning them to single quotes the notebook loaded correctly.
"
867,Failed to build master with GPU support,"with bazel 0.1.4

```
INFO: From Compiling tensorflow/core/kernels/cwise_op_complex.cc:
In file included from ./tensorflow/core/public/partial_tensor_shape.h:19:0,
                 from ./tensorflow/core/framework/attr_value_util.h:25,
                 from ./tensorflow/core/framework/function.h:22,
                 from ./tensorflow/core/framework/op_kernel.h:26,
                 from ./tensorflow/core/kernels/cwise_ops_common.h:26,
                 from tensorflow/core/kernels/cwise_op_complex.cc:16:
./tensorflow/core/framework/partial_tensor_shape.h: In static member function 'static bool tensorflow::PartialTensorShapeUtils::AreCompatible(const tensorflow::gtl::ArraySlice<tensorflow::PartialTensorShape>&, const tensorflow::gtl::ArraySlice<tensorflow::PartialTensorShape>&)':
./tensorflow/core/framework/partial_tensor_shape.h:156:40: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
       for (int i = 0; i < shapes0.size(); ++i) {
                                        ^
In file included from ./tensorflow/core/framework/op_kernel.h:38:0,
                 from ./tensorflow/core/kernels/cwise_ops_common.h:26,
                 from tensorflow/core/kernels/cwise_op_complex.cc:16:
./tensorflow/core/framework/unique_tensor_references.h: In member function 'void tensorflow::UniqueTensorReferences::Add(const tensorflow::Tensor&)':
./tensorflow/core/framework/unique_tensor_references.h:63:61: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
         for (int i = 0; i < referenced_tensors_vector_.size(); ++i) {
                                                             ^
INFO: From Compiling tensorflow/core/kernels/tile_ops.cc:
In file included from ./tensorflow/core/public/partial_tensor_shape.h:19:0,
                 from ./tensorflow/core/framework/attr_value_util.h:25,
                 from ./tensorflow/core/framework/function.h:22,
                 from ./tensorflow/core/framework/op_kernel.h:26,
                 from ./tensorflow/core/framework/numeric_op.h:19,
                 from tensorflow/core/kernels/tile_ops.cc:27:
./tensorflow/core/framework/partial_tensor_shape.h: In static member function 'static bool tensorflow::PartialTensorShapeUtils::AreCompatible(const tensorflow::gtl::ArraySlice<tensorflow::PartialTensorShape>&, const tensorflow::gtl::ArraySlice<tensorflow::PartialTensorShape>&)':
./tensorflow/core/framework/partial_tensor_shape.h:156:40: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
       for (int i = 0; i < shapes0.size(); ++i) {
                                        ^
In file included from ./tensorflow/core/framework/op_kernel.h:38:0,
                 from ./tensorflow/core/framework/numeric_op.h:19,
                 from tensorflow/core/kernels/tile_ops.cc:27:
./tensorflow/core/framework/unique_tensor_references.h: In member function 'void tensorflow::UniqueTensorReferences::Add(const tensorflow::Tensor&)':
./tensorflow/core/framework/unique_tensor_references.h:63:61: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
         for (int i = 0; i < referenced_tensors_vector_.size(); ++i) {
                                                             ^
INFO: From Compiling tensorflow/core/kernels/conv_grad_ops.cc:
In file included from ./tensorflow/core/public/partial_tensor_shape.h:19:0,
                 from ./tensorflow/core/framework/attr_value_util.h:25,
                 from ./tensorflow/core/framework/function.h:22,
                 from ./tensorflow/core/framework/op_kernel.h:26,
                 from ./tensorflow/core/framework/numeric_op.h:19,
                 from tensorflow/core/kernels/conv_grad_ops.cc:22:
./tensorflow/core/framework/partial_tensor_shape.h: In static member function 'static bool tensorflow::PartialTensorShapeUtils::AreCompatible(const tensorflow::gtl::ArraySlice<tensorflow::PartialTensorShape>&, const tensorflow::gtl::ArraySlice<tensorflow::PartialTensorShape>&)':
./tensorflow/core/framework/partial_tensor_shape.h:156:40: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
       for (int i = 0; i < shapes0.size(); ++i) {
                                        ^
In file included from ./tensorflow/core/framework/op_kernel.h:38:0,
                 from ./tensorflow/core/framework/numeric_op.h:19,
                 from tensorflow/core/kernels/conv_grad_ops.cc:22:
./tensorflow/core/framework/unique_tensor_references.h: In member function 'void tensorflow::UniqueTensorReferences::Add(const tensorflow::Tensor&)':
./tensorflow/core/framework/unique_tensor_references.h:63:61: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
         for (int i = 0; i < referenced_tensors_vector_.size(); ++i) {
                                                             ^
tensorflow/core/kernels/conv_grad_ops.cc: In instantiation of 'tensorflow::Conv2DCustomBackpropFilterOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda8':
tensorflow/core/kernels/conv_grad_ops.cc:718:22:   required from 'struct tensorflow::Conv2DCustomBackpropFilterOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda8'
tensorflow/core/kernels/conv_grad_ops.cc:729:7:   required from 'void tensorflow::Conv2DCustomBackpropFilterOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]'
tensorflow/core/kernels/conv_grad_ops.cc:1422:1:   required from here
tensorflow/core/kernels/conv_grad_ops.cc:718:56: error: use of 'tensorflow::Conv2DCustomBackpropFilterOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda8::__col_buffer_data' before deduction of 'auto'
                     &size_A](int64 start, int64 limit) {
                                                        ^
tensorflow/core/kernels/conv_grad_ops.cc:718:56: error: use of 'tensorflow::Conv2DCustomBackpropFilterOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda8::__col_buffer_data' before deduction of 'auto'
tensorflow/core/kernels/conv_grad_ops.cc:718:56: error: use of 'tensorflow::Conv2DCustomBackpropFilterOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda8::__input_data' before deduction of 'auto'
tensorflow/core/kernels/conv_grad_ops.cc:718:56: error: use of 'tensorflow::Conv2DCustomBackpropFilterOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda8::__input_data' before deduction of 'auto'
tensorflow/core/kernels/conv_grad_ops.cc:720:50: error: use of 'input_data' before deduction of 'auto'
           const T* input_data_shard = input_data + shard_id * input_offset;
                                                  ^
tensorflow/core/kernels/conv_grad_ops.cc:720:50: error: invalid use of 'auto'
tensorflow/core/kernels/conv_grad_ops.cc:720:63: error: cannot convert 'auto*' to 'const float*' in initialization
           const T* input_data_shard = input_data + shard_id * input_offset;
                                                               ^
tensorflow/core/kernels/conv_grad_ops.cc:721:47: error: use of 'col_buffer_data' before deduction of 'auto'
           T* col_data_shard = col_buffer_data + shard_id * size_A;
                                               ^
tensorflow/core/kernels/conv_grad_ops.cc:721:47: error: invalid use of 'auto'
tensorflow/core/kernels/conv_grad_ops.cc:721:60: error: cannot convert 'auto*' to 'float*' in initialization
           T* col_data_shard = col_buffer_data + shard_id * size_A;
                                                            ^
tensorflow/core/kernels/conv_grad_ops.cc: In instantiation of 'void tensorflow::Conv2DCustomBackpropFilterOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]':
tensorflow/core/kernels/conv_grad_ops.cc:1422:1:   required from here
tensorflow/core/kernels/conv_grad_ops.cc:715:20: error: invalid initialization of reference of type 'auto*&' from expression of type 'const float*'
       auto shard = [&input_data, &col_buffer_data, &in_depth, &input_rows,
                    ^
tensorflow/core/kernels/conv_grad_ops.cc:715:20: error: invalid initialization of reference of type 'auto*&' from expression of type 'float*'
tensorflow/core/kernels/conv_grad_ops.cc: In instantiation of 'tensorflow::Conv2DCustomBackpropInputOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda4':
tensorflow/core/kernels/conv_grad_ops.cc:496:24:   required from 'struct tensorflow::Conv2DCustomBackpropInputOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda4'
tensorflow/core/kernels/conv_grad_ops.cc:514:9:   required from 'void tensorflow::Conv2DCustomBackpropInputOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]'
tensorflow/core/kernels/conv_grad_ops.cc:1422:1:   required from here
tensorflow/core/kernels/conv_grad_ops.cc:496:58: error: use of 'tensorflow::Conv2DCustomBackpropInputOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda4::__filter_data' before deduction of 'auto'
                       &size_C](int64 start, int64 limit) {
                                                          ^
tensorflow/core/kernels/conv_grad_ops.cc:496:58: error: use of 'tensorflow::Conv2DCustomBackpropInputOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda4::__filter_data' before deduction of 'auto'
tensorflow/core/kernels/conv_grad_ops.cc:496:58: error: use of 'tensorflow::Conv2DCustomBackpropInputOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda4::__out_backprop_data' before deduction of 'auto'
tensorflow/core/kernels/conv_grad_ops.cc:496:58: error: use of 'tensorflow::Conv2DCustomBackpropInputOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda4::__out_backprop_data' before deduction of 'auto'
tensorflow/core/kernels/conv_grad_ops.cc:496:58: error: use of 'tensorflow::Conv2DCustomBackpropInputOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda4::__col_buffer_data' before deduction of 'auto'
tensorflow/core/kernels/conv_grad_ops.cc:496:58: error: use of 'tensorflow::Conv2DCustomBackpropInputOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda4::__col_buffer_data' before deduction of 'auto'
tensorflow/core/kernels/conv_grad_ops.cc:496:58: error: use of 'tensorflow::Conv2DCustomBackpropInputOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda4::__input_backprop_data' before deduction of 'auto'
tensorflow/core/kernels/conv_grad_ops.cc:496:58: error: use of 'tensorflow::Conv2DCustomBackpropInputOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda4::__input_backprop_data' before deduction of 'auto'
tensorflow/core/kernels/conv_grad_ops.cc:498:45: error: use of 'col_buffer_data' before deduction of 'auto'
             T* im2col_buf = col_buffer_data + shard_id * size_C;
                                             ^
tensorflow/core/kernels/conv_grad_ops.cc:498:45: error: invalid use of 'auto'
tensorflow/core/kernels/conv_grad_ops.cc:498:58: error: cannot convert 'auto*' to 'float*' in initialization
             T* im2col_buf = col_buffer_data + shard_id * size_C;
                                                          ^
tensorflow/core/kernels/conv_grad_ops.cc:499:49: error: use of 'input_backprop_data' before deduction of 'auto'
             T* input_data = input_backprop_data + shard_id * input_offset;
                                                 ^
tensorflow/core/kernels/conv_grad_ops.cc:499:49: error: invalid use of 'auto'
tensorflow/core/kernels/conv_grad_ops.cc:499:62: error: cannot convert 'auto*' to 'float*' in initialization
             T* input_data = input_backprop_data + shard_id * input_offset;
                                                              ^
tensorflow/core/kernels/conv_grad_ops.cc:500:51: error: use of 'out_backprop_data' before deduction of 'auto'
             const T* out_data = out_backprop_data + shard_id * output_offset;
                                                   ^
tensorflow/core/kernels/conv_grad_ops.cc:500:51: error: invalid use of 'auto'
tensorflow/core/kernels/conv_grad_ops.cc:500:64: error: cannot convert 'auto*' to 'const float*' in initialization
             const T* out_data = out_backprop_data + shard_id * output_offset;
                                                                ^
tensorflow/core/kernels/conv_grad_ops.cc:506:71: error: use of 'filter_data' before deduction of 'auto'
             ConstMatrixMap B(filter_data, filter_total_size, out_depth);
                                                                       ^
tensorflow/core/kernels/conv_grad_ops.cc:506:71: error: no matching function for call to 'Eigen::Map<const Eigen::Matrix<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >::Map(auto*&, const int&, const int64&)'
tensorflow/core/kernels/conv_grad_ops.cc:506:71: note: candidates are:
In file included from ./external/eigen_archive/eigen-eigen-c8e5d094f3a9/Eigen/Core:378:0,
                 from third_party/eigen3/Eigen/Core:1,
                 from ./external/eigen_archive/eigen-eigen-c8e5d094f3a9/unsupported/Eigen/CXX11/Core:14,
                 from ./external/eigen_archive/eigen-eigen-c8e5d094f3a9/unsupported/Eigen/CXX11/Tensor:14,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/core/framework/types.h:23,
                 from ./tensorflow/core/framework/type_traits.h:22,
                 from ./tensorflow/core/framework/allocator.h:25,
                 from ./tensorflow/core/framework/op_kernel.h:22,
                 from ./tensorflow/core/framework/numeric_op.h:19,
                 from tensorflow/core/kernels/conv_grad_ops.cc:22:
./external/eigen_archive/eigen-eigen-c8e5d094f3a9/Eigen/src/Core/Map.h:149:12: note: Eigen::Map<MatrixType, MapOptions, StrideType>::Map(Eigen::Map<MatrixType, MapOptions, StrideType>::PointerArgType, Eigen::Index, Eigen::Index, const StrideType&) [with PlainObjectType = const Eigen::Matrix<float, -1, -1, 1, -1, -1>; int MapOptions = 0; StrideType = Eigen::Stride<0, 0>; Eigen::Map<MatrixType, MapOptions, StrideType>::PointerArgType = const float*; Eigen::Index = long int]
     inline Map(PointerArgType dataPtr, Index rows, Index cols, const StrideType& stride = StrideType())
            ^
./external/eigen_archive/eigen-eigen-c8e5d094f3a9/Eigen/src/Core/Map.h:149:12: note:   no known conversion for argument 1 from 'auto*' to 'Eigen::Map<const Eigen::Matrix<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >::PointerArgType {aka const float*}'
./external/eigen_archive/eigen-eigen-c8e5d094f3a9/Eigen/src/Core/Map.h:135:12: note: Eigen::Map<MatrixType, MapOptions, StrideType>::Map(Eigen::Map<MatrixType, MapOptions, StrideType>::PointerArgType, Eigen::Index, const StrideType&) [with PlainObjectType = const Eigen::Matrix<float, -1, -1, 1, -1, -1>; int MapOptions = 0; StrideType = Eigen::Stride<0, 0>; Eigen::Map<MatrixType, MapOptions, StrideType>::PointerArgType = const float*; Eigen::Index = long int]
     inline Map(PointerArgType dataPtr, Index size, const StrideType& stride = StrideType())
            ^
./external/eigen_archive/eigen-eigen-c8e5d094f3a9/Eigen/src/Core/Map.h:135:12: note:   no known conversion for argument 1 from 'auto*' to 'Eigen::Map<const Eigen::Matrix<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >::PointerArgType {aka const float*}'
./external/eigen_archive/eigen-eigen-c8e5d094f3a9/Eigen/src/Core/Map.h:122:21: note: Eigen::Map<MatrixType, MapOptions, StrideType>::Map(Eigen::Map<MatrixType, MapOptions, StrideType>::PointerArgType, const StrideType&) [with PlainObjectType = const Eigen::Matrix<float, -1, -1, 1, -1, -1>; int MapOptions = 0; StrideType = Eigen::Stride<0, 0>; Eigen::Map<MatrixType, MapOptions, StrideType>::PointerArgType = const float*]
     explicit inline Map(PointerArgType dataPtr, const StrideType& stride = StrideType())
                     ^
./external/eigen_archive/eigen-eigen-c8e5d094f3a9/Eigen/src/Core/Map.h:122:21: note:   candidate expects 2 arguments, 3 provided
./external/eigen_archive/eigen-eigen-c8e5d094f3a9/Eigen/src/Core/Map.h:88:79: note: Eigen::Map<const Eigen::Matrix<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >::Map(const Eigen::Map<const Eigen::Matrix<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >&)
 template<typename PlainObjectType, int MapOptions, typename StrideType> class Map
                                                                               ^
./external/eigen_archive/eigen-eigen-c8e5d094f3a9/Eigen/src/Core/Map.h:88:79: note:   candidate expects 1 argument, 3 provided
tensorflow/core/kernels/conv_grad_ops.cc: In instantiation of 'void tensorflow::Conv2DCustomBackpropInputOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]':
tensorflow/core/kernels/conv_grad_ops.cc:1422:1:   required from here
tensorflow/core/kernels/conv_grad_ops.cc:490:22: error: invalid initialization of reference of type 'auto*&' from expression of type 'float*'
         auto shard = [&in_depth, &input_rows, &input_cols, &filter_rows,
                      ^
tensorflow/core/kernels/conv_grad_ops.cc:490:22: error: invalid initialization of reference of type 'auto*&' from expression of type 'float*'
tensorflow/core/kernels/conv_grad_ops.cc:490:22: error: invalid initialization of reference of type 'auto*&' from expression of type 'const float*'
tensorflow/core/kernels/conv_grad_ops.cc:490:22: error: invalid initialization of reference of type 'auto*&' from expression of type 'const float*'
In file included from ./tensorflow/core/framework/op_kernel.h:22:0,
                 from ./tensorflow/core/framework/numeric_op.h:19,
                 from tensorflow/core/kernels/conv_grad_ops.cc:22:
./tensorflow/core/framework/allocator.h: In member function 'virtual std::size_t tensorflow::Allocator::RequestedSize(void*)':
./tensorflow/core/framework/allocator.h:122:3: warning: control reaches end of non-void function [-Wreturn-type]
   }
   ^
In file included from ./tensorflow/core/framework/op_kernel.h:25:0,
                 from ./tensorflow/core/framework/numeric_op.h:19,
                 from tensorflow/core/kernels/conv_grad_ops.cc:22:
./tensorflow/core/framework/device_base.h: In member function 'virtual tensorflow::Allocator* tensorflow::DeviceBase::GetAllocator(tensorflow::AllocatorAttributes)':
./tensorflow/core/framework/device_base.h:147:3: warning: control reaches end of non-void function [-Wreturn-type]
   }
   ^
./tensorflow/core/framework/device_base.h: In member function 'virtual const tensorflow::DeviceAttributes& tensorflow::DeviceBase::attributes() const':
./tensorflow/core/framework/device_base.h:175:3: warning: control reaches end of non-void function [-Wreturn-type]
   }
   ^
ERROR: /home/phill/tensorflow/tensorflow/core/BUILD:341:1: C++ compilation of rule '//tensorflow/core:kernels' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command
  (cd /home/phill/.cache/bazel/_bazel_phill/124dc7c235a2311dca36360045bf7452/tensorflow && \
  exec env - \
    PATH=/home/phill/bin:/usr/local/cuda-7.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games \
  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -iquote external/jpeg_archive -iquote bazel-out/local_linux-opt/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/local_linux-opt/genfiles/external/png_archive -iquote external/re2 -iquote bazel-out/local_linux-opt/genfiles/external/re2 -iquote external/eigen_archive -iquote bazel-out/local_linux-opt/genfiles/external/eigen_archive -isystem google/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/google/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local_linux-opt/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-c8e5d094f3a9 -isystem bazel-out/local_linux-opt/genfiles/external/eigen_archive/eigen-eigen-c8e5d094f3a9 -isystem third_party/gpus/cuda -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda -isystem third_party/gpus/cuda/include -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda/include -pthread -fno-exceptions -DEIGEN_AVOID_STL_ARRAY '-DGOOGLE_CUDA=1' '-DGOOGLE_CUDA=1' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fno-canonical-system-headers '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernels/tensorflow/core/kernels/conv_grad_ops.pic.o' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernels/tensorflow/core/kernels/conv_grad_ops.pic.d -fPIC -c tensorflow/core/kernels/conv_grad_ops.cc -o bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernels/tensorflow/core/kernels/conv_grad_ops.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: crosstool_wrapper_driver_is_not_gcc failed: error executing command
  (cd /home/phill/.cache/bazel/_bazel_phill/124dc7c235a2311dca36360045bf7452/tensorflow && \
  exec env - \
    PATH=/home/phill/bin:/usr/local/cuda-7.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games \
  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -iquote external/jpeg_archive -iquote bazel-out/local_linux-opt/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/local_linux-opt/genfiles/external/png_archive -iquote external/re2 -iquote bazel-out/local_linux-opt/genfiles/external/re2 -iquote external/eigen_archive -iquote bazel-out/local_linux-opt/genfiles/external/eigen_archive -isystem google/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/google/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local_linux-opt/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-c8e5d094f3a9 -isystem bazel-out/local_linux-opt/genfiles/external/eigen_archive/eigen-eigen-c8e5d094f3a9 -isystem third_party/gpus/cuda -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda -isystem third_party/gpus/cuda/include -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda/include -pthread -fno-exceptions -DEIGEN_AVOID_STL_ARRAY '-DGOOGLE_CUDA=1' '-DGOOGLE_CUDA=1' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fno-canonical-system-headers '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernels/tensorflow/core/kernels/conv_grad_ops.pic.o' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernels/tensorflow/core/kernels/conv_grad_ops.pic.d -fPIC -c tensorflow/core/kernels/conv_grad_ops.cc -o bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernels/tensorflow/core/kernels/conv_grad_ops.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```
"
865,Latest commit (524595) for Udacity Assignment 1 breaks IPython Notebook,"Hi, I attempted to open the latest IPython Notebook for the Udacity course Assignment 1 from commit 524595 (23 Jan 2016), and it fails with the error:

```
Unreadable Notebook: ~/tensorflow/tensorflow/examples/udacity/1_notmnist.ipynb
NotJSONError('Notebook does not appear to be JSON: u\'{\\n ""worksheets"": [\\n {\\n ""ce...',)
```

The notebook also fails to load here on Github. If I checkout the previous commit 42154f, the notebook loads correctly.

It's possible this is just me having bad luck, so if important, I'm running Anaconda Python on Ubuntu 14.04.3 LTS.
"
862, How to Fine-tuning a Pretrained Network?,"Any one can give an example code of how to fine tuning a pretrained imagenet network with new data and different classes num. Just like this:
https://github.com/BVLC/caffe/blob/master/examples/03-fine-tuning.ipynb
"
861,Noise in Variable assignment,"I am writing a code to use an LSTM in the following manner:
1. Given an input _x_ of dimensionality _d_ at time _t_, two quantities are computed:
   a. The difference _x(t) - x(t-1)_. I call this the first derivative of _x_: _x'(t)_.
   b. The difference _x'(t) - x'(t-1)_. I call this the second derivative of _x_: _x''(t)_.
2. _x(t)_, _x'(t)_, _x''(t)_ are concatenated as the total input to an LSTM of state size _3xd_.
3. The output from the LSTM, say _L(t)_ is then passed onto a _d_-dimensional densely connected linear layer. So the final output out is _f(t) = W x L(t) + b_

I am using the _AdamOptimizer_ with a learning rate of 0.0005.

The problem I am facing is, when I do steps 1 and 2 as given above _outside_ TensorFlow, the model works well. However, when I try to do the computation of derivatives inside a TensorFlow Graph, there seems to be some noise seeping into the values of the variables. Heres the code:

```
##The Input Layer as a Placeholder
#Since we will provide data sequentially, the 'batch size'
#is 1.
input_layer = tf.placeholder(tf.float32, [1, input_dim])

##First Order Derivative Layer
#This will store the last recorded value
last_value1 = tf.Variable(tf.zeros([1, input_dim]))
#Subtract last value from current
sub_value1 = tf.sub(input_layer, last_value1)
#Update last recorded value
last_assign_op1 = last_value1.assign(input_layer)

##Second Order Derivative Layer
#This will store the last recorded derivative
last_value2 = tf.Variable(tf.zeros([1, input_dim]))
#Subtract last value from current
sub_value2 = tf.sub(sub_value1, last_value2)
#Update last recorded value
last_assign_op2 = last_value2.assign(sub_value1)

##Overall input to the LSTM
#x and its first and second order derivatives as outputs of
#earlier layers
zero_order = input_layer
first_order = sub_value1
second_order = sub_value2
#Concatenated
total_input = tf.concat(1, [zero_order, first_order, second_order])
```

While running the model, I run in order: final_output (which is dependent on total_input), then last_assign_op1 and then last_assign_op2.
However, there always seems to be a difference of about 0.001 between what the actual values of _last_value1_ and _last_value2_ should be, and what they really are. This is causing a lot of noise in the final output.

What is the reason for this? Is this somehow because of the Optimizer (I can't figure out why that would be)? Or am I doing something wrong?
"
859,Feature request: clear all variable tensors,"When using ipython notebooks, one frequently runs the same cell without restarting the kernel. If the cell contains code of the form:

```
x = tf.get_variable('x', (1,))
```

the cell will run correctly the first time, but throw an error the second. Obviously this error is appropriate in python files, but really just gets in the way for notebooks. It would be great if something like tf.clear_all_variables() were implemented, such that I could have a cell with contents:

```
tf.clear_all_variables()
x = tf.get_variable('x', (1,))
```

that I could run over and over again. On a larger scale, tensorflow seems to be rather unfriendly to notebooks in general, frequently requiring kernel restarts for small changes. But this issue for definitely contributes to the greatest number of forced kernel restarts for me.
"
857,Assignment 2 of Udacity course,"Lecture one covers logistic regression and stochastic gradient descent. Assignment 2, assignment of this lecture, requests students to implement a neural network which is covered in later chapter.
I suggest the question should be related to the topic of lecture.
"
856,How to invoke tf.initialize_all_variables() in C++?,"Hi, and thank you for your job)
I'm newbie in tensorflow.
Btw, how to execute ops in C++ such as tf.initialize_all_variables() or some ops returned by optimizers
self.train_op = self.optimizer.apply_gradients(gradients)
?

I've tried 
init_all_vars_op = tf.initialize_variables(tf.all_variables(), name='init_all_vars_op')
...
tf.train.write_graph(session.graph_def, 'models/', 'graph.pb', as_text=False)
in python and then in C++:

```
    status = session->Run(inputs, {""init_all_vars_op""}, {}, &outputs);
    if (!status.ok()) {
        std::cout << ""tf error: "" << status.ToString() << ""\n"";
        return;
    }
```

so I got

tf error: Invalid argument: FetchOutputs init_all_vars_op: output index too large, must be < 0

here is stackowerflow question
http://stackoverflow.com/questions/34975884/how-to-invoke-tf-initialize-all-variables-in-c-tensorflow
"
852,Add new Tensorflow Udacity Course too the main Readme or Readme in master/tensorflow/examples/udacity,"Dear Tensorflow-Team,

please add a link of the new Tensorflow Udacity course to your Readme and Docs.
https://www.udacity.com/course/deep-learning--ud730

Thanks and best Wishes,
Patrick
"
851,tensorflow for Nvidia TX1,"Hello,

@maxcuda has recently got tensorflow running on the tk1 as documented in blogpost http://cudamusing.blogspot.de/2015/11/building-tensorflow-for-jetson-tk1.html but since then been unable to repeatedly build it. I am now trying to get tensorflow running on a tx1 tegra platform and need some support.

Much trouble seems to come from Eigen variadic templates and using C++11 initializer lists, both of wich could work according to http://devblogs.nvidia.com/parallelforall/cplusplus-11-in-cuda-variadic-templates/.
In theory std=c++11 should be set according to crosstool. Nevertheless, nvcc crashes happily on all of them. This smells as if the  ""-std=c++11""  flag is not properly set. 
How can I verify/enforce this?

Also in tensorflow.bzl, variadic templates in Eigen are said to be disabled
`We have to disable variadic templates in Eigen for NVCC even though std=c++11 are enabled`
is that still necessary?

Here is my build workflow:

```
git clone —recurse-submodules git@github.com:jmtatsch/tensorflow.git
cd tensorflow
grep -Rl ""lib64""| xargs sed -i 's/lib64/lib/g' # no lib64 for tx1 yet 
./configure
bazel build -c opt --local_resources 2048,0.5,1.0 --verbose_failures --config=cuda //tensorflow/cc:tutorials_example_trainer
```
"
848,"tf use all gpus in one machine, and killed my other training job, how to fix this?","Hi, all,

i installed the gpu vision of tf which refer to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#virtualenv-installation, and when i run the third line of:

``` python
import tensorflow as tf
hello = tf.constant('Hello, TensorFlow!')
sess = tf.Session()
```

then it will start all of my k80s in one machine, because there is som traning job of some gpus with almost full memeory, then when tf starts, it killed all of the other job. so how to fix this probem?that to say, when i start tf, it use the gpu 0 default?
thanks very much!
"
843,build issue: Bazel paths,"As of 0db986d25335a767855e92fc5748038f6fb585c4, I'm getting the error below when I try to build from source. Is my Bazel out of date or something?
# 

ERROR: /home/bjt/git_repos/tensorflow2/tensorflow/cc/BUILD:11:6: First argument of load() is a path, not a label. It should start with a single slash if it is an absolute path..
ERROR: /home/bjt/git_repos/tensorflow2/tensorflow/cc/BUILD:12:6: First argument of load() is a path, not a label. It should start with a single slash if it is an absolute path..
ERROR: /home/bjt/git_repos/tensorflow2/tensorflow/cc/BUILD:11:6: file '/tensorflow:tensorflow.bzl.bzl' was not correctly loaded. Make sure the 'load' statement appears in the global scope in your file.
ERROR: /home/bjt/git_repos/tensorflow2/tensorflow/cc/BUILD:12:6: file '/tensorflow:tensorflow.bzl.bzl' was not correctly loaded. Make sure the 'load' statement appears in the global scope in your file.
ERROR: /home/bjt/git_repos/tensorflow2/tensorflow/cc/BUILD:21:13: Traceback (most recent call last):
        File ""/home/bjt/git_repos/tensorflow2/tensorflow/cc/BUILD"", line 14
                cc_library(name = ""cc_op_gen_main"", srcs = [""ops/cc_op_gen.cc"", ""ops/cc_op_gen_main.cc""], hdrs = [""ops/cc_op_gen.h""], copts = tf_copts(), deps = [""//tensorflow/core:framework""])
        File ""/home/bjt/git_repos/tensorflow2/tensorflow/cc/BUILD"", line 21, in cc_library
                tf_copts
name 'tf_copts' is not defined.
ERROR: /home/bjt/git_repos/tensorflow2/tensorflow/cc/BUILD:28:1: name 'tf_gen_op_wrappers_cc' is not defined.
ERROR: /home/bjt/git_repos/tensorflow2/tensorflow/cc/BUILD:66:13: Traceback (most recent call last):
        File ""/home/bjt/git_repos/tensorflow2/tensorflow/cc/BUILD"", line 63
                cc_binary(name = ""tutorials_example_trainer"", srcs = [""tutorials/example_trainer.cc""], copts = tf_copts(), linkopts = [""-lpthread"", ""-lm""], deps = ["":cc_ops"", ""//tensorflow/core:kernels"", ""//tensorflow/core:tensorflow""])
        File ""/home/bjt/git_repos/tensorflow2/tensorflow/cc/BUILD"", line 66, in cc_binary
                tf_copts
name 'tf_copts' is not defined.
ERROR: no such target '//tensorflow/cc:tutorials_example_trainer': target 'tutorials_example_trainer' not declared in package 'tensorflow/cc' defined by /home/bjt/git_repos/tensorflow2/tensorflow/cc/BUILD.
INFO: Elapsed time: 2.232s
# 

bazel version
Build label: 0.1.1
Build target: bazel-out/local_linux-fastbuild/bin/src/main/java/bazel-main_deploy.jar
Build time: Thu Oct 15 20:15:14 2015 (1444940114)
Build timestamp: 1444940114
Build timestamp as int: 1444940114
"
842,Tool request: Deep Visualization Toolbox for TensorFlow,"I would like TensorFlow to have an official tool similar to this https://github.com/yosinski/deep-visualization-toolbox.

It would be helpful to check if our CNNs were learning useful features, to debug, and to have better insights.

Check this video: https://www.youtube.com/watch?v=AgkfIQ4IGaM

Thoughts?
"
841,py_func() is not getting imported,
838,fully_connected_preloaded.py on GPU trains slower then on CPU,"while running `examples/how_tos/reading_data/fully_connected_preloaded.py` on CPU the speed of training is the following:

```
Step 0: loss = 2.31 (0.586 sec)
Step 100: loss = 2.22 (0.012 sec)
```

But, when I run this code on GPU, it hangs, seems like some blocking occurs:

```
Step 0: loss = 2.30 (3.961 sec)
Step 100: loss = 2.13 (2.797 sec)
```

Timings are meaningless here, since actually I wait more than ~3 sec for next output.
"
837,reading_data/fully_connected_reader.py VERY slow relative to fully_connected_feed.py,"I noticed that when using a data reader to provide minibatches of examples to a model that performance is greatly reduced relative to just supplying the examples via `feed_dict`. For instance, when running `reading_data/fully_connected_reader.py` with the following flags::

```
--hidden1 512 --hidden2 512 --batch_size 128
```

it takes 28.7 seconds to process 600 minibatches with a GPU utilization of 13%. If I edit the code so that `num_threads=16` (instead of `num_threads=2`) when `shuffle_batch` is called, these numbers improve to 14.9 seconds and 23% GPU utilization. However, training the same model via `fully_connected_feed.py` takes only 2.63 seconds and achieves a GPU utilization of 55%. This is hardly rigorous, but it seems that the overhead involved in reading the Example protos from the TFRecords file, putting them into a queue, etc is much higher than I would expect. 

These numbers were compiled using 039981f5a382ce9dc1e97dc3bd25aeba7fd82ade and running on a Titan X card with no other background processes running.
"
836,tensorboard not able to read large event file (~600MB),"I am training a network on 4 GPUs. The event file is too large to be parsed by tensorboard. How can I increase the limit?

```
RNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/pc/anaconda2/lib/python2.7/site-packages/tensorflow/tensorboard/TAG' on path /home/pc/anaconda2/lib/python2.7/site-packages/tensorflow/tensorboard/TAG
WARNING:tensorflow:Unable to read TensorBoard tag
Starting TensorBoard  on port 6006
(You can navigate to http://0.0.0.0:6006)
[libprotobuf ERROR google/protobuf/src/google/protobuf/io/coded_stream.cc:207] A protocol message was rejected because it was too big (more than 67108864 bytes).  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.
Exception in thread Thread-2:
Traceback (most recent call last):
  File ""/home/pc/anaconda2/lib/python2.7/threading.py"", line 801, in __bootstrap_inner
    self.run()
  File ""/home/pc/anaconda2/lib/python2.7/threading.py"", line 1073, in run
    self.function(*self.args, **self.kwargs)
  File ""/home/pc/anaconda2/lib/python2.7/site-packages/tensorflow/python/summary/event_accumulator.py"", line 242, in Update
    self.Reload()
  File ""/home/pc/anaconda2/lib/python2.7/site-packages/tensorflow/python/summary/event_accumulator.py"", line 175, in Reload
    for event in self._generator.Load():
  File ""/home/pc/anaconda2/lib/python2.7/site-packages/tensorflow/python/summary/impl/directory_watcher.py"", line 82, in Load 
    for event in self._loader.Load():
  File ""/home/pc/anaconda2/lib/python2.7/site-packages/tensorflow/python/summary/impl/event_file_loader.py"", line 53, in Load 
    event.ParseFromString(self._reader.record())
DecodeError: Error parsing message
```
"
834,Optimizing over only some parameters,"Currently the optimizers in TensorFlow appear to act simultaneously upon all tf.Variables in the function graph. However, commonly, it's useful to optimize only over certain Variables. For example, given a pre-trained network, I might only want to optimize wrt a new set of output weights. Am I misreading the examples and optimizer code or is this presently not supported?
"
833,conv2d_transpose cannot be used in network with variable batch size,"I'm building a network that requires an approximate deconvolution using conv2d_transpose and I'm running into this error when constructing the network:

```
...
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py"", line 80, in conv2d_transpose
    output_shape_ = ops.convert_to_tensor(output_shape, name=""output_shape"")
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 530, in convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/constant_op.py"", line 195, in _tensor_shape_tensor_conversion_function
    ""Cannot convert a partially known TensorShape to a Tensor: %s"" % s)
ValueError: Cannot convert a partially known TensorShape to a Tensor: (?, 40, 40, 32)
```

It appears that the operation doesn't support shape objects with none values as they cannot be converted to tensors. Is there a workaround? Or should I simply explicitly set my batch size for now?

edit: It appears that reshape has the same issue.
"
830,fully_connected_preloaded.py fails with exception on python3.5,"There are some errors while handling exception `tf.errors.OutOfRangeError`:
On python2 no issues with handling `tf.errors.OutOfRangeError`.

During handling of the above exception, another exception occurred.
log:

``` python
Traceback (most recent call last):
  File ""fully_connected_preloaded.py"", line 158, in <module>
    tf.app.run()
  File ""/Users/home/tensorflow_python3/lib/python3.5/site-packages/tensorflow/python/platform/default/_app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""fully_connected_preloaded.py"", line 154, in main
    run_training()
  File ""fully_connected_preloaded.py"", line 120, in run_training
    _, loss_value = sess.run([train_op, loss])
  File ""/Users/home/tensorflow_python3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 368, in run
    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)
  File ""/Users/home/tensorflow_python3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 428, in _do_run
    target_list)
SystemError: <built-in function delete_Status> returned a result with an error set
```
"
829,GradientDescentOptimizer requires explicit variable listing after being called once in a different session,"When I run 
[debugger.txt](https://github.com/tensorflow/tensorflow/files/99486/debugger.txt), I get a TypeError because the operation gradstep yields an empty list. However, when I comment out the first attempt to gather a gradient, or add the argument `var_list=[W,b]` to the second attempt, this error does not occur. It seems like somehow there's an issue with identifying the var_list implicitly, after having run (and closed) the previous session. 
"
827,Making a composite assignment + optimization op,"I want to make an op that, when run, changes a `Variable`'s value and then updates it by optimizing some dependent scalar `Tensor`.

I came up with this, which runs successfully:

``` python
v = tf.Variable(tf.zeros([3]), name='v')
v_input = tf.placeholder(tf.float32, [3], name='v_input')
loss = tf.reduce_sum(tf.square(v))

with tf.control_dependencies([v.assign(v_input)]):
  op = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(loss)

session = tf.Session()
session.run(tf.initialize_all_variables())
session.run(op, feed_dict={v_input: [1., 2., 3.]})
```

However, if I change `GradientDescentOptimizer` to `AdagradOptimizer` (or `RMSProp`, `Adam`, ...), it fails in the variable-initialization (second-to-last) line, complaining that `v_input` must be fed:

```
W tensorflow/core/common_runtime/executor.cc:1076] 0x15cb160 Compute status: Invalid argument: You must feed a value for placeholder tensor 'v_input' with dtype float and shape dim { size: 3 }
```

Obviously to actually execute the training op I need to feed in a value for `v_input` due to the control dependency. But just to initialize variables? I don't understand why I would need to feed anything in...

I guess since `AdagradOptimizer` creates its own `Variable`s (accumulated gradient) when you call `minimize`? I really only want the optimizer's update step to be dependent on the assignment op I create, but I'm not sure how to do that. Any help would be appreciated!
"
824,Feature parity of tf.diag with np.diag,"np.diag can

1) take 2d array and produce 1d vector of diagonal entries
2) take 1d array and produce diagonal 2d array (inverse of case 1)

tf.diag only supports 2) and the behavior for case 1) is undocumented

Currently to do 1) you need to do something like this:

```
def identity_matrix(n):
  """"""Returns nxn identity matrix.""""""
  # note, if n is a constant node, this assert node won't be executed,
  # this error will be caught during shape analysis 
  assert_op = tf.Assert(tf.greater(n, 0), [""Matrix size must be positive""])
  with tf.control_dependencies([assert_op]):
    ones = tf.fill(n, 1)
    diag = tf.diag(ones)
  return diag

def extract_diagonal(tensor):
  """"""Extract diagonal of a square matrix.""""""

  shape = tf.shape(tensor)
  n = shape[0]
  assert_op = tf.Assert(tf.equal(shape[0], shape[1]), [""Can't get diagonal of ""
                                                       ""a non-square matrix""])

  with tf.control_dependencies([assert_op]):
  return tf.reduce_sum(tf.mul(tensor, identity_matrix(n)), [0])
```
"
823,Give a convenient way to retrieve shadow variables created by ExponentialMovingAverage.,"At first I was under the impression, the variables returned by `tf.moving_average_variables()` would be shadow variables created, but that does not seem to be the case (they didn't save/restore correctly for me). 

Now that I'm reading the code, that seems to be the intention, though? Shouldn't [this line](https://github.com/tensorflow/tensorflow/blame/f2bd0fc399606d14b55f3f7d732d013f32b33dd5/tensorflow/python/training/moving_averages.py#L227) read 

```
ops.add_to_collection(ops.GraphKeys.MOVING_AVERAGE_VARIABLES, avg)
```

? 

If not, it would be great to save the averages under some other key and make them easily retrievable for the saver in a similar way as `tf.moving_average_variables()`, maybe as  `tf.moving_average_shadow_variables()`.
"
822,Often PR are closed instead of merged,"@vrv In the majority of the cases seems that your are following [this merging process](http://stackoverflow.com/questions/23015168/how-to-close-a-github-pull-request-with-a-commit-comment). This alter the github pulse and other stats. Is there a way in your process to let this PRs result as merged?
"
821,String input producer creates trainable variable.,"Using tf.train.string_input_producer function, creates trainable variable. In addition its tensor of int type. Example:

filename_queue = tf.train.string_input_producer([""test""], num_epochs=1, name='input_producer_test')
assert tf.trainable_variables()[0].dtype.is_integer

It becomes problem, when uses variable_averages.apply(...), which require, that each trainable variable must be float type.
"
819,./configure: line 26: ./util/python/python_config.sh: no such file or directory ,"While running ./configure and then comes to an issue, ./configure: line 26: ./util/python/python_config.sh: no such file or directory. Could somebody give me a little help?
"
817,crosstool_wrapper_driver_is_not_gcc failed: error executing command,"I am using the master branch of tensorflow and also build bazel master 
/home/xxxxx/bazel/output/bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer --verbose_failures

ERROR: /home/xxxxx/.cache/bazel/_bazel_lli/716dd78c923ec9441d6903ce812e1d6e/external/gemmlowp/BUILD:77:1: C++ compilation of rule '@gemmlowp//:eight_bit_int_gemm' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/lli/.cache/bazel/_bazel_lli/716dd78c923ec9441d6903ce812e1d6e/tensorflow && \
  exec env - \
    PATH='~/.local/bin:~/.local/bin:~/.local/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/lli/bin:/home/lli/bin:/home/lli/bin' \
  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -iquote external/gemmlowp -iquote bazel-out/local_linux-opt/genfiles/external/gemmlowp -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -isystem external/bazel_tools/tools/cpp/gcc3 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fno-canonical-system-headers '-frandom-seed=bazel-out/local_linux-opt/bin/external/gemmlowp/_objs/eight_bit_int_gemm/external/gemmlowp/eight_bit_int_gemm/eight_bit_int_gemm.o' -MD -MF bazel-out/local_linux-opt/bin/external/gemmlowp/_objs/eight_bit_int_gemm/external/gemmlowp/eight_bit_int_gemm/eight_bit_int_gemm.d -c external/gemmlowp/eight_bit_int_gemm/eight_bit_int_gemm.cc -o bazel-out/local_linux-opt/bin/external/gemmlowp/_objs/eight_bit_int_gemm/external/gemmlowp/eight_bit_int_gemm/eight_bit_int_gemm.o): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/lli/.cache/bazel/_bazel_lli/716dd78c923ec9441d6903ce812e1d6e/tensorflow && \
  exec env - \
    PATH='~/.local/bin:~/.local/bin:~/.local/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/lli/bin:/home/lli/bin:/home/lli/bin' \
  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -iquote external/gemmlowp -iquote bazel-out/local_linux-opt/genfiles/external/gemmlowp -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -isystem external/bazel_tools/tools/cpp/gcc3 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fno-canonical-system-headers '-frandom-seed=bazel-out/local_linux-opt/bin/external/gemmlowp/_objs/eight_bit_int_gemm/external/gemmlowp/eight_bit_int_gemm/eight_bit_int_gemm.o' -MD -MF bazel-out/local_linux-opt/bin/external/gemmlowp/_objs/eight_bit_int_gemm/external/gemmlowp/eight_bit_int_gemm/eight_bit_int_gemm.d -c external/gemmlowp/eight_bit_int_gemm/eight_bit_int_gemm.cc -o bazel-out/local_linux-opt/bin/external/gemmlowp/_objs/eight_bit_int_gemm/external/gemmlowp/eight_bit_int_gemm/eight_bit_int_gemm.o).
src/main/tools/namespace-sandbox.c:645: execvp(argv[0], argv): No such file or directory
Target //tensorflow/cc:tutorials_example_trainer failed to build
"
816,Can tensorflow run on freescale ubuntu system?,"I have one board called udoo neo (from www.udoo.org) ,which run ubuntu system .
I want to port tensorflow on it. How to do it step by step?
"
813,Cumulative sum (cumsum) op needed,"C.f. [`numpy.cumsum`](http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.cumsum.html)
"
812,Feature suggestion: expose weights for RNNCell,"I realize that the API for RNNs hasn't been finalized yet but eventually it would be nice if the weights of the RNNCell are exposed so that they can be tracked using the summary ops like `histogram_summary`. Maybe just a property that returns all the internal weights.
"
811,relu_layer doesn't appear in tensorflow.org/versions/master/api_docs,"relu_layer has a docstring in the code, but it doesn’t appear in the tensorflow online documentation for some reason.

In case it is not public yet, it should be removed from the cifar example.
"
810,Error importing tensorflow,"I had a problem with the cifar10 example. When running cifar10_eval.py I got: 
""AttributeError: 'ExponentialMovingAverage' object has no attribute 'variables_to_restore'""

I then saw #802 and tried to follow the instructions for ""Installing from sources"" on the website. Now I get a new error when trying to import tensorflow:

Error importing tensorflow.  Unless you are using bazel,
you should not try to import tensorflow from its source directory;
please exit the tensorflow source tree, and relaunch your python interpreter
from there.

How do I solve this? I'm using ubuntu 15.04.
"
808,CUDA_ERROR_NO_DEVICE inside docker with GTX Titan X,"Running `b.gcr.io/tensorflow/tensorflow:latest-gpu` having CUDA 7.0 installed on the host when I try to create the session it returns `CUDA_ERROR_NO_DEVICE` and `was unable to find libcuda.so DSO loaded into this program` but when the strange thing is that when the module is imported all the libraries are loaded correctly.
#### Log:

```
root@5b1e79697b49:~# python
Python 2.7.6 (default, Jun 22 2015, 17:58:13) 
[GCC 4.8.2] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
I tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcublas.so.7.0 locally
I tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcudnn.so.6.5 locally
I tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcufft.so.7.0 locally
I tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcuda.so locally
I tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcurand.so.7.0 locally
>>> 
>>> with tf.Session() as sess:
...   with tf.device(""/gpu:0""):
...     matrix1 = tf.constant([[3., 3.]])
...     matrix2 = tf.constant([[2.],[2.]])
...     product = tf.matmul(matrix1, matrix2)
...     sess.run(product)
... 
I tensorflow/core/common_runtime/local_device.cc:40] Local device intra op parallelism threads: 8
E tensorflow/stream_executor/cuda/cuda_driver.cc:481] failed call to cuInit: CUDA_ERROR_NO_DEVICE
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:114] retrieving CUDA diagnostic information for host: 5b1e79697b49
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:121] hostname: 5b1e79697b49
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:146] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:257] driver version file contents: """"""NVRM version: NVIDIA UNIX x86_64 Kernel Module  358.16  Mon Nov 16 19:25:55 PST 2015
GCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-4) (GCC) 
""""""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:150] kernel reported version is: 358.16
I tensorflow/core/common_runtime/gpu/gpu_init.cc:127] DMA: 
I tensorflow/core/common_runtime/direct_session.cc:58] Direct session inter op parallelism threads: 8
Traceback (most recent call last):
  File ""<stdin>"", line 6, in <module>
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 368, in run
    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 444, in _do_run
    e.code)
tensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'Const_1': Could not satisfy explicit device specification '/gpu:0'
         [[Node: Const_1 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [2,1] values: 2 2>, _device=""/gpu:0""]()]]
Caused by op u'Const_1', defined at:
  File ""<stdin>"", line 4, in <module>
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/constant_op.py"", line 165, in constant
    attrs={""value"": tensor_value, ""dtype"": dtype_value}, name=name).outputs[0]
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1834, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1043, in __init__
    self._traceback = _extract_stack()

>>> 
```
#### Versions:
- OS: `CentOS Linux elease.2.1511(Core)                                                                                                        
  `
- Kernel: `Linux localhost.localdomain 3.10.0-327.el7.x86_64 #1 SMP Thu Nov 19 22:10:57 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux` 
- GPU: `NVIDIA Corporation GM200 [GeForce GTX TITAN X] (rev a1)`
- Docker: `1.9.1`
#### Env:

```
CUDA_HOME=/usr/local/cuda-7.0
LD_LIBRARY_PATH=/usr/local/cuda-7.0/lib64:
```
#### Command:

`docker run -it -v /usr/lib/x86_64-linux-gnu/libcudadevrt.a:/usr/lib/x86_64-linux-gnu/libcudadevrt.a -v /usr/lib/x86_64-linux-gnu/libcudart.so:/usr/lib/x86_64-linux-gnu/libcudart.so -v /usr/lib/x86_64-linux-gnu/libcudart.so.7.0:/usr/lib/x86_64-linux-gnu/libcudart.so.7.0 -v /usr/lib/x86_64-linux-gnu/libcudart.so.7.0.28:/usr/lib/x86_64-linux-gnu/libcudart.so.7.0.28 -v /usr/lib/x86_64-linux-gnu/libcudart_static.a:/usr/lib/x86_64-linux-gnu/libcudart_static.a -v /usr/lib/x86_64-linux-gnu/libcuda.so:/usr/lib/x86_64-linux-gnu/libcuda.so --device /dev/nvidia0:/dev/nvidia0 --device /dev/nvidiactl:/dev/nvidiactl --device /dev/nvidia-modeset:/dev/nvidia-modeset b.gcr.io/tensorflow/tensorflow:latest-gpu`
"
806,google-tensorflow,
804,Applying batch normalization to a non-convolutional layer fails due to restriction of input to rank 4,"Starting from [this top answer of stackoverflow](http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow?answertab=votes#tab-top), I tried to make batch normalization work for a fully connected layer. This is the relevant part of the code:

```
            if self.convolutional:
              mean, variance = tf.nn.moments(x, [0, 1, 2])
            else:
              mean, variance = tf.nn.moments(x, [0])
            assign_mean = self.mean.assign(mean)
            assign_variance = self.variance.assign(variance)
            with tf.control_dependencies([assign_mean, assign_variance]):
                return tf.nn.batch_norm_with_global_normalization(
                    x, mean, variance, self.beta, self.gamma, self.epsilon,
                    scale_after_normalization=self.scale_after_normalization)
```

The call to tf.nn.batch_norm_with_global_normalization fails however:

```
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py"", line 104, in batch_norm_with_global_normalization
    name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py"", line 659, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1896, in create_op
    set_shapes_for_outputs(ret)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1524, in set_shapes_for_outputs
    shapes = shape_func(op)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.py"", line 313, in _BatchNormShape
    input_shape = op.inputs[0].get_shape().with_rank(4)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.py"", line 614, in with_rank
    return self.merge_with(unknown_shape(ndims=rank))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.py"", line 542, in merge_with
    self.assert_same_rank(other)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.py"", line 585, in assert_same_rank
    ""Shapes %s and %s must have the same rank"" % (self, other))
ValueError: Shapes (64, 512) and (?, ?, ?, ?) must have the same rank
```

since, as far as I can tell, the input is constraint to have rank 4 in in [nn_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_ops.py#L313). Is this really a necessary constraint?
"
803,Error in cifar10_eval.py,"Hi,
i'm trying to run the cifar10 network and getting problems when evaluating the trained variables. I've runned the cifar_10 train with a maximum steps of 100 and then it should theoretically be able to be evaluated, but i get this error:
.../tensorflow/models/image/cifar10/cifar10_eval.py"", line 138, in evaluate
    variables_to_restore = variable_averages.variables_to_restore()
AttributeError: 'ExponentialMovingAverage' object has no attribute 'variables_to_restore'
Can someone help me?
Thanks
"
802,AttributeError: 'ExponentialMovingAverage' object has no attribute 'variables_to_restore',"I am running the code provided with the tutorial Convolutional Neural Networks https://www.tensorflow.org/versions/master/tutorials/deep_cnn/index.html 

The training works fine, but when I run the evaluation I get the following error. I have not modified in any way the code, I am running it as is.

Code snippet:
...
# Restore the moving average version of the learned variables for eval.

```
variable_averages = tf.train.ExponentialMovingAverage(
    cifar10.MOVING_AVERAGE_DECAY)
variables_to_restore = variable_averages.variables_to_restore()
saver = tf.train.Saver(variables_to_restore)
```

...

Error:

Traceback (most recent call last):
  File ""cifar10_eval.py"", line 161, in <module>
    tf.app.run()
  File ""/Library/Python/2.7/site-packages/tensorflow/python/platform/default/_app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""cifar10_eval.py"", line 157, in main
    evaluate()
  File ""cifar10_eval.py"", line 135, in evaluate
    variables_to_restore = variable_averages.variables_to_restore()
AttributeError: 'ExponentialMovingAverage' object has no attribute 'variables_to_restore'

Please help,
"
800,"Sudden Loss Explosion, followed by total missevaluation","Hello altogether =)

I'm testing Tensorflow 0.60 (GPU Version) for Python 3.4 on Ubuntu 15.10. The used GPU is a GT 960m. Testing the CIFAR10 Demo for over two days in a row gave me the ~86.x% everbody gets after evaluation. After transforming my own data into batches (cifar10 demo like) and adjusting the cifar-python files accordingly, i achieved the following results for my binary classification task (always stopped training, then evaluating, then restarting).

after 240 steps (of 128 size batches)  -- >> 92 %
after 420 steps (of 128 size batches)  -- >> 92 %
after 1500 steps (of 128 size batches)  -- >> 94 %
after 2000 steps (of 128 size batches)  -- >> 95 %

Over the course of these tests, the loss value - printed on the console and retrieved via tensorboard, went from about 12 to about 3. Over the (last) weekend, i raised the maximum steps to 10 million, and let things just run. Today in the morning i did an evaluation on the network, which arrived at a loss of 0.6 but evaluated with 50.4 % which looks like totally random (binary task). Scrolling through the logs i realized, there was a sudden loss explosion at around step 70000, jumping from like 2 to 2000 (!!), and - after slowly recovering from that, again up to about 2 000 000 (!!) million, and also slowly recovering from that downto said 0.6, but with the said total missevaluation. Also noteworthy, the speed from almost constantly 100 examples per second broke downto 30 when the loss spikes occured. The eventfile (one of them) went up to 9 Gigabytes in size, and re-checking with tensorboard gave strange results (showing sometimes 4 million, and sometimes 900 million learn steps, appeared to be totally broken somehow).

Appart from just a different batch file, and adjusted image size parameters, the network is exactly the same as in CIFAR 10.

I'm sorry, i can't provide the logs, but right now i am relearning the whole thing and monitoring if this happens again.

Did anybody of experience something like this before?
"
799,Got error when initialize bidirectional rnn with LSTM cell ,"I want to build a bi-rnn model with tensorflow with LSTM cell, when i try to initialize bidirectional_rnn,
it gives: `ValueError: Over-sharing: Variable BiRNN_FW/RNN/BasicLSTMCell/Linear/Matrix already exists, disallowed. Did you mean to set reuse=True in VarScope?`

```
import tensorflow as tf
from tensorflow.models.rnn import rnn, rnn_cell
from tensorflow.python.ops.constant_op import constant
import numpy as np

class Model(object):
    def __init__(self, batch_size, len_word, num_chars, dim_embed, dim_hidden):
        self.batch_size = batch_size
        self.dim_embed = dim_embed
        self.dim_hidden = dim_hidden
        self.num_chars = num_chars
        self.len_word = len_word

        with tf.device(""/cpu:0""):
            self.embedding = tf.Variable(tf.random_uniform([num_chars, dim_embed], -0.1, 0.1), name='embedding')

        self.W_emb = tf.Variable(tf.random_uniform([dim_hidden*2, dim_embed], -0.1, 0.1), name='W_emb')
        self.b_emb = tf.Variable(tf.zeros([dim_embed]), name='b_emb')
        self.lstm_fw_cell = rnn_cell.BasicLSTMCell(dim_hidden)
        self.lstm_bw_cell = rnn_cell.BasicLSTMCell(dim_hidden)

    def build_model(self):
        inputs = tf.placeholder(tf.int32, [self.batch_size, self.len_word])
        input_length = tf.placeholder(tf.int64, [self.batch_size])
        lstm_state_fw = self.lstm_fw_cell.zero_state(self.batch_size, tf.float32)
        lstm_state_bw = self.lstm_bw_cell.zero_state(self.batch_size, tf.float32)

        with tf.device(""/cpu:0""):
            embedded_input = tf.nn.embedding_lookup(self.embedding, tf.transpose(inputs))

        brnn_output = rnn.bidirectional_rnn(
            self.lstm_fw_cell, self.lstm_bw_cell,
            tf.unpack(embedded_input),
            sequence_length=input_length,
            initial_state_fw=lstm_state_fw,
            initial_state_bw=lstm_state_bw,
        )

        pooled_output = tf.reduce_sum( tf.pack(brnn_output), 0 )
        pooled_output = pooled_output / tf.expand_dims( tf.to_float(input_length) + 1e-6, 1)
        final_emb = tf.nn.xw_plus_b(pooled_output, self.W_emb, self.b_emb)
        final_emb = tf.nn.l2_normalize(final_emb, dim=1, epsilon=1e-7)

        return final_emb
```
"
798,Converting Tensor to ndarray,"```
>>> a = tf.random_uniform((3,3))
>>> b = tf.tensor_util.MakeNdarray(a)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py"", line 414, in MakeNdarray
    shape = [d.size for d in tensor.tensor_shape.dim]
AttributeError: 'Tensor' object has no attribute 'tensor_shape'
```

It looks like `MakeNdarray` expects a `TensorProto` instead of a `Tensor`, so this works fine:

```
a = tf.tensor_util.make_tensor_proto([1,2,3])
```

Is there any way to convert a `Tensor` to a `TensorProto`? If not, is there any other way to convert a `Tensor` to an `ndarray`?
"
796,Problem running Convolutional.py,"I am trying to run the convolutional.py through the tensorflow virtualenv i am getting an error as follows 
Traceback (most recent call last):
  File ""convolutional.py"", line 290, in <module>
    tf.app.run()
  File ""/home/akshay/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/platform/default/_app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""convolutional.py"", line 121, in main
    train_data_filename = maybe_download('train-images-idx3-ubyte.gz')
  File ""convolutional.py"", line 56, in maybe_download
    os.mkdir(WORK_DIRECTORY)
OSError: [Errno 13] Permission denied: 'data'

I am getting this error even when i am running as super user.
"
795,Tensor Flow IN Android,"Please someone help me how i set the tensor flow library in android i try to follow the steps explain in the android example but stuck in first and i do not know how to do it which is Get the recommended Bazel version
"
794,Docker versus direct install?,"I picked up a TitanX card and I'm about to install Linux 14.04.3 Desktop.  Are there any performance considerations or Nvidia GPU driver considerations between choosing a Docker install or direct install?
"
791,Building TF on archlinux with python3,"Hi

Sounds like the python3 building script have some issues:

```
PC% bazel build -c opt //tensorflow/tools/pip_package:build_pip_package
...........
WARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.
INFO: Found 1 target...
ERROR: /home/zenol/tensorflow/tensorflow/tensorflow/python/BUILD:247:1: Converting to Python 3: tensorflow/python/ops/functional_ops.py failed: 2to3 failed: error executing command bazel-out/host/bin/external/bazel_tools/tools/python/2to3 --no-diffs --nobackups --write --output-dir bazel-out/local_linux-py3-opt/genfiles/python3/tensorflow/python/ops --write-unchanged-files ... (remaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 18.140s, Critical Path: 2.02s
```

With verbose_faillures : 

```
ERROR: /home/zenol/tensorflow/tensorflow/tensorflow/python/BUILD:247:1: Converting to Python 3: tensorflow/python/ops/functional_ops.py failed: 2to3 failed: error executing command 
  (cd /home/zenol/.cache/bazel/_bazel_zenol/67d52188e906c7f900d4635d7d1ff821/tensorflow && \
  exec env - \
  bazel-out/host/bin/external/bazel_tools/tools/python/2to3 --no-diffs --nobackups --write --output-dir bazel-out/local_linux-py3-opt/genfiles/python3/tensorflow/python/ops --write-unchanged-files tensorflow/python/ops/functional_ops.py): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: 2to3 failed: error executing command 
  (cd /home/zenol/.cache/bazel/_bazel_zenol/67d52188e906c7f900d4635d7d1ff821/tensorflow && \
  exec env - \
  bazel-out/host/bin/external/bazel_tools/tools/python/2to3 --no-diffs --nobackups --write --output-dir bazel-out/local_linux-py3-opt/genfiles/python3/tensorflow/python/ops --write-unchanged-files tensorflow/python/ops/functional_ops.py): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 30.007s, Critical Path: 29.28s
```

Nb : The python2 build seems to work.
"
785,OOM error when using dropout,"I thought I might have been going crazy so I only got around to posting this just now. I've got a GPU that can handle a batch size of 160 on my network, but I run it at 128. Thats just to show that I've got about 20% space left on my GPU ram when the network starts training. If I am using dropout, after about 100,000 iterations i get an OOM error.

```
tensorflow.python.framework.errors.ResourceExhaustedError: OOM when allocating tensor with shapedim { size: 128 } dim { size: 40 } dim { size: 40 } dim { size: 64 }
         [[Node: range_1/_1993 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_25333_range_1"", tensor_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
```

Is it possible that there could be a memory leak somewhere?
"
784,TensorBoard cannot feed to placeholder x-input on MNIST (for merged only),"I am running a slightly different version of mnist+relog because I recently installed TF on my computer and it does not have access to Internet (and I wanted to do the Image processing part myself).
It runs completely fine without TensorBoard and gives me 0.9166 accuracy on Test set which is not too bad for simple relog on unrolled images.
However when I add in the for loop the part where you feed feed_dist to merged (which is the fusion of all the summaries) I got the following error message:  

```
InvalidArgumentError: You must feed a value for placeholder tensor 'x-input_4' with dtype float
     [[Node: x-input_4 = Placeholder[dtype=DT_FLOAT, shape=[], _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
     [[Node: x-input_7/_23 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_152_x-input_7"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]
Caused by op u'x-input_4', defined at:
  File ""/home/jeandut/anaconda/lib/python2.7/site-packages/spyderlib/widgets/externalshell/start_ipython_kernel.py"", line 188, in <module>
    __ipythonkernel__.start()
  File ""/home/jeandut/anaconda/lib/python2.7/site-packages/IPython/kernel/zmq/kernelapp.py"", line 374, in start
    ioloop.IOLoop.instance().start()
  File ""/home/jeandut/anaconda/lib/python2.7/site-packages/zmq/eventloop/ioloop.py"", line 151, in start
    super(ZMQIOLoop, self).start()
  File ""/home/jeandut/anaconda/lib/python2.7/site-packages/tornado/ioloop.py"", line 840, in start
    handler_func(fd_obj, events)
  File ""/home/jeandut/anaconda/lib/python2.7/site-packages/tornado/stack_context.py"", line 275, in null_wrapper
    return fn(*args, **kwargs)
  File ""/home/jeandut/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py"", line 433, in _handle_events
    self._handle_recv()
  File ""/home/jeandut/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py"", line 465, in _handle_recv
    self._run_callback(callback, msg)
  File ""/home/jeandut/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py"", line 407, in _run_callback
    callback(*args, **kwargs)
  File ""/home/jeandut/anaconda/lib/python2.7/site-packages/tornado/stack_context.py"", line 275, in null_wrapper
    return fn(*args, **kwargs)
  File ""/home/jeandut/anaconda/lib/python2.7/site-packages/IPython/kernel/zmq/kernelbase.py"", line 252, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""/home/jeandut/anaconda/lib/python2.7/site-packages/IPython/kernel/zmq/kernelbase.py"", line 213, in dispatch_shell
    handler(stream, idents, msg)
  File ""/home/jeandut/anaconda/lib/python2.7/site-packages/IPython/kernel/zmq/kernelbase.py"", line 362, in execute_request
    user_expressions, allow_stdin)
  File ""/home/jeandut/anaconda/lib/python2.7/site-packages/IPython/kernel/zmq/ipkernel.py"", line 181, in do_execute
    shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/home/jeandut/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2868, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/home/jeandut/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2978, in run_ast_nodes
    if self.run_code(code, result):
  File ""/home/jeandut/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 3032, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-48-28d84e6d5ef3>"", line 1, in <module>
    runfile('/home/jeandut/mnist_jean_tf.py', wdir='/home/jeandut')
  File ""/home/jeandut/anaconda/lib/python2.7/site-packages/spyderlib/widgets/externalshell/sitecustomize.py"", line 682, in runfile
    execfile(filename, namespace)
  File ""/home/jeandut/anaconda/lib/python2.7/site-packages/spyderlib/widgets/externalshell/sitecustomize.py"", line 78, in execfile
    builtins.execfile(filename, *where)
  File ""/home/jeandut/mnist_jean_tf.py"", line 64, in <module>
    x=tf.placeholder(tf.float32, [None, 784],name=""x-input"")
  File ""/home/jeandut/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py"", line 673, in placeholder
    name=name)
  File ""/home/jeandut/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 463, in _placeholder
    name=name)
  File ""/home/jeandut/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py"", line 664, in apply_op
    op_def=op_def)
  File ""/home/jeandut/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1834, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/jeandut/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1043, in __init__
    self._traceback = _extract_stack()
```

It is therefore :   
- me badly implementing tensorboard part (but as I copied almost exactly what you did in the mnist tutorial I cannot find where it the bug is)  
- a bug in my installation of tensorflow somehow...  
- something else  

My code:

```
# -*- coding: utf-8 -*-
""""""
Created on Thu Jan 14 13:06:44 2016

@author: jeandut
""""""
#from tensorflow.examples.tutorials.mnist import input_data
#mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)

import tensorflow as tf
import os
import random
import numpy as np
from array import array
import struct
import matplotlib.pyplot as plt
import time

#I imported myself the decompressed -ubyte files from mnist cause I do not have internet they are at the path I wrote replace with appropriate path to reproduce
os.chdir('/home/jeandut/Bureau/Step1/')

with open(""train-labels.idx1-ubyte"") as file:
    magic, size = struct.unpack("">II"",file.read(8))
    train_labels_data=np.asarray(array(""B"",file.read()))
with open(""t10k-labels.idx1-ubyte"") as file:
    magic, size = struct.unpack("">II"",file.read(8))
    test_labels_data=np.asarray(array(""B"",file.read()))
with open(""train-images.idx3-ubyte"") as file:
    magic, size, rows, cols =struct.unpack("">IIII"",file.read(16))
    train_images_data=np.reshape(np.asarray(array(""B"",file.read())),(size,rows,cols))
with open(""t10k-images.idx3-ubyte"") as file:
    magic, size, rows, cols =struct.unpack("">IIII"",file.read(16))
    test_images_data=np.reshape(np.asarray(array(""B"",file.read())),(size,rows,cols))





for i in range(10):
   plt.imshow(train_images_data[i,:])
   plt.show()
   print(train_labels_data[i])


train_images=np.reshape(train_images_data,(60000,28*28)).astype(np.float32)*1/255
test_images=np.reshape(test_images_data,(10000,28*28)).astype(np.float32)*1/255

train_labels=np.zeros((60000,10),dtype=np.float32)
test_labels=np.zeros((10000,10),dtype=np.float32)

for i in range(60000):
    a=train_labels_data[i]
    train_labels[i,a]=1.

for j in range(10000):
    b=test_labels_data[j]
    test_labels[j,b]=1.



sess=tf.Session()


x=tf.placeholder(tf.float32, [None, 784],name=""x-input"")
W=tf.Variable(tf.zeros([784, 10]),name=""weights"")
b=tf.Variable(tf.zeros([10]),name=""bias"")


with tf.name_scope(""Wx_b"") as scope:
    y=tf.nn.softmax(tf.matmul(x,W) + b)


w_hist=tf.histogram_summary(""weights"",W)
b_hist=tf.histogram_summary(""bias"",b)
y_hist=tf.histogram_summary(""y"",y)


y_ =tf.placeholder(tf.float32, [None, 10], name=""y-input"")

y_hist=tf.histogram_summary(""y"",y)
with tf.name_scope(""xent"") as scope:

    cross_entropy= -tf.reduce_sum(y_*tf.log(y))
    ce_summ=tf.scalar_summary(""cross_entropy"", cross_entropy)


with tf.name_scope(""train"") as scope:
    train_step=tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)


with tf.name_scope(""train"") as scope:
    correct_prediction =tf.equal(tf.argmax(y,1), tf.argmax(y_,1))


    accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))
    accuracy_summary=tf.scalar_summary(""accuracy"",accuracy)

merged=tf.merge_all_summaries()
writer=tf.train.SummaryWriter(""tmp/mnist_logs"",sess.graph_def)






init=tf.initialize_all_variables()
sess.run(init)


for i in range(1000):
    if i % 10 == 0:
        feed={x:test_images, y_: test_labels}
        result=sess.run([merged, accuracy],feed_dict=feed)
        summary_str=result[0]
        acc=result[1]
        writer.add_summary(summary_str, i)
        print(""Accuracy at step %s: %s"" % (i,acc))
    else:
        index=np.random.randint(60000-1,size=100)
        batch_xs, batch_ys = train_images[index,:], train_labels[index]
        sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})





print(sess.run(accuracy, feed_dict={x: train_images, y_: train_labels}))



```

I posted the question on stack but nobody seems to be able to solve it ! Please help !
"
783,Gradient computation erroneously returns None,"``` python
In [5]: tf.gradients(tf.constant(5), tf.Variable(0))
Out[5]: [None]
```

The derivative of 5 with respect to x should be 0.
"
